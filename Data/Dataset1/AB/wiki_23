<doc id="15412" url="https://en.wikipedia.org/wiki?curid=15412" title="Infrared spectroscopy">
Infrared spectroscopy

Infrared spectroscopy (IR spectroscopy or vibrational spectroscopy) involves the interaction of infrared radiation with matter. It covers a range of techniques, mostly based on absorption spectroscopy. As with all spectroscopic techniques, it can be used to identify and study chemicals. Samples may be solid, liquid, or gas. The method or technique of infrared spectroscopy is conducted with an instrument called an infrared spectrometer (or spectrophotometer) to produce an infrared spectrum. An IR spectrum can be visualized in a graph of infrared light absorbance (or transmittance) on the vertical axis vs. frequency or wavelength on the horizontal axis. Typical units of frequency used in IR spectra are reciprocal centimeters (sometimes called wave numbers), with the symbol cm. Units of IR wavelength are commonly given in micrometers (formerly called "microns"), symbol μm, which are related to wave numbers in a reciprocal way. A common laboratory instrument that uses this technique is a Fourier transform infrared (FTIR) spectrometer. Two-dimensional IR is also possible as discussed below.

The infrared portion of the electromagnetic spectrum is usually divided into three regions; the near-, mid- and far- infrared, named for their relation to the visible spectrum. The higher-energy near-IR, approximately 14000–4000 cm (0.8–2.5 μm wavelength) can excite overtone or harmonic vibrations. The mid-infrared, approximately 4000–400 cm (2.5–25 μm) may be used to study the fundamental vibrations and associated rotational-vibrational structure. The far-infrared, approximately 400–10 cm (25–1000 μm), lying adjacent to the microwave region, has low energy and may be used for rotational spectroscopy. The names and classifications of these subregions are conventions, and are only loosely based on the relative molecular or electromagnetic properties.

Infrared spectroscopy exploits the fact that molecules absorb frequencies that are characteristic of their structure. These absorptions occur at resonant frequencies, i.e. the frequency of the absorbed radiation matches the vibrational frequency. The energies are affected by the shape of the molecular potential energy surfaces, the masses of the atoms, and the associated vibronic coupling.

In order for a vibrational mode in a sample to be "IR active", it must be associated with changes in the dipole moment. A permanent dipole is not necessary, as the rule requires only a change in dipole moment.

A molecule can vibrate in many ways, and each way is called a "vibrational mode." For molecules with N number of atoms, linear molecules have 3N – 5 degrees of vibrational modes, whereas nonlinear molecules have 3N – 6 degrees of vibrational modes (also called vibrational degrees of freedom). As an example HO, a non-linear molecule, will have 3 × 3 – 6 = 3 degrees of vibrational freedom, or modes.

Simple diatomic molecules have only one bond and only one vibrational band. If the molecule is symmetrical, e.g. N, the band is not observed in the IR spectrum, but only in the Raman spectrum. Asymmetrical diatomic molecules, e.g. CO, absorb in the IR spectrum. More complex molecules have many bonds, and their vibrational spectra are correspondingly more complex, i.e. big molecules have many peaks in their IR spectra.

The atoms in a CHX group, commonly found in organic compounds and where X can represent any other atom, can vibrate in nine different ways. Six of these vibrations involve only the CH portion: symmetric and antisymmetric stretching, scissoring, rocking, wagging and twisting, as shown below. Structures that do not have the two additional X groups attached have fewer modes because some modes are defined by specific relationships to those other attached groups. For example, in water, the rocking, wagging, and twisting modes do not exist because these types of motions of the H represent simple rotation of the whole molecule rather than vibrations within it.

These figures do not represent the "recoil" of the C atoms, which, though necessarily present to balance the overall movements of the molecule, are much smaller than the movements of the lighter H atoms.

The simplest and most important or "fundamental" IR bands arise from the excitations of normal modes, the simplest distortions of the molecule, from the ground state with vibrational quantum number v = 0 to the first excited state with vibrational quantum number v = 1. In some cases, overtone bands are observed. An overtone band arises from the absorption of a photon leading to a direct transition from the ground state to the second excited vibrational state (v = 2). Such a band appears at approximately twice the energy of the fundamental band for the same normal mode. Some excitations, so-called "combination modes", involve simultaneous excitation of more than one normal mode. The phenomenon of Fermi resonance can arise when two modes are similar in energy; Fermi resonance results in an unexpected shift in energy and intensity of the bands etc.

The infrared spectrum of a sample is recorded by passing a beam of infrared light through the sample. When the frequency of the IR is the same as the vibrational frequency of a bond or collection of bonds, absorption occurs. Examination of the transmitted light reveals how much energy was absorbed at each frequency (or wavelength). This measurement can be achieved by scanning the wavelength range using a monochromator. Alternatively, the entire wavelength range is measured using a Fourier transform instrument and then a transmittance or absorbance spectrum is generated using a dedicated procedure.

This technique is commonly used for analyzing samples with covalent bonds. Simple spectra are obtained from samples with few IR active bonds and high levels of purity. More complex molecular structures lead to more absorption bands and more complex spectra.

Gaseous samples require a sample cell with a long pathlength to compensate for the diluteness. The pathlength of the sample cell depends on the concentration of the compound of interest. A simple glass tube with length of 5 to 10 cm equipped with infrared-transparent windows at the both ends of the tube can be used for concentrations down to several hundred ppm. Sample gas concentrations well below ppm can be measured with a White's cell in which the infrared light is guided with mirrors to travel through the gas. White's cells are available with optical pathlength starting from 0.5 m up to hundred meters.

Liquid samples can be sandwiched between two plates of a salt (commonly sodium chloride, or common salt, although a number of other salts such as potassium bromide or calcium fluoride are also used).
The plates are transparent to the infrared light and do not introduce any lines onto the spectra.

Solid samples can be prepared in a variety of ways. One common method is to crush the sample with an oily mulling agent (usually mineral oil Nujol). A thin film of the mull is applied onto salt plates and measured. The second method is to grind a quantity of the sample with a specially purified salt (usually potassium bromide) finely (to remove scattering effects from large crystals). This powder mixture is then pressed in a mechanical press to form a translucent pellet through which the beam of the spectrometer can pass. A third technique is the "cast film" technique, which is used mainly for polymeric materials. The sample is first dissolved in a suitable, non hygroscopic solvent. A drop of this solution is deposited on surface of KBr or NaCl cell. The solution is then evaporated to dryness and the film formed on the cell is analysed directly. Care is important to ensure that the film is not too thick otherwise light cannot pass through. This technique is suitable for qualitative analysis. The final method is to use microtomy to cut a thin (20–100 µm) film from a solid sample. This is one of the most important ways of analysing failed plastic products for example because the integrity of the solid is preserved.

In photoacoustic spectroscopy the need for sample treatment is minimal. The sample, liquid or solid, is placed into the sample cup which is inserted into the photoacoustic cell which is then sealed for the measurement. The sample may be one solid piece, powder or basically in any form for the measurement. For example, a piece of rock can be inserted into the sample cup and the spectrum measured from it.

It is typical to record spectrum of both the sample and a "reference". This step controls for a number of variables, e.g. infrared detector, which may affect the spectrum. The reference measurement makes it possible to eliminate the instrument influence.

The appropriate "reference" depends on the measurement and its goal. The simplest reference measurement is to simply remove the sample (replacing it by air). However, sometimes a different reference is more useful. For example, if the sample is a dilute solute dissolved in water in a beaker, then a good reference measurement might be to measure pure water in the same beaker. Then the reference measurement would cancel out not only all the instrumental properties (like what light source is used), but also the light-absorbing and light-reflecting properties of the water and beaker, and the final result would just show the properties of the solute (at least approximately).

A common way to compare to a reference is sequentially: first measure the reference, then replace the reference by the sample and measure the sample. This technique is not perfectly reliable; if the infrared lamp is a bit brighter during the reference measurement, then a bit dimmer during the sample measurement, the measurement will be distorted. More elaborate methods, such as a "two-beam" setup (see figure), can correct for these types of effects to give very accurate results. The Standard addition method can be used to statistically cancel these errors.

Nevertheless, among different absorption based techniques which are used for gaseous species detection, Cavity ring-down spectroscopy (CRDS) can be used as a calibration free method. The fact that CRDS is based on the measurements of photon life-times (and not the laser intensity) makes it needless for any calibration and comparison with a reference 

Fourier transform infrared (FTIR) spectroscopy is a measurement technique that allows one to record infrared spectra. Infrared light is guided through an interferometer and then through the sample (or vice versa). A moving mirror inside the apparatus alters the distribution of infrared light that passes through the interferometer. The signal directly recorded, called an "interferogram", represents light output as a function of mirror position. A data-processing technique called Fourier transform turns this raw data into the desired result (the sample's spectrum): Light output as a function of infrared wavelength (or equivalently, wavenumber). As described above, the sample's spectrum is always compared to a reference.

An alternate method for acquiring spectra is the "dispersive" or "scanning monochromator" method. In this approach, the sample is irradiated sequentially with various single wavelengths. The dispersive method is more common in UV-Vis spectroscopy, but is less practical in the infrared than the FTIR method. One reason that FTIR is favored is called "Fellgett's advantage" or the "multiplex advantage": The information at all frequencies is collected simultaneously, improving both speed and signal-to-noise ratio. Another is called "Jacquinot's Throughput Advantage": A dispersive measurement requires detecting much lower light levels than an FTIR measurement. There are other advantages, as well as some disadvantages, but virtually all modern infrared spectrometers are FTIR instruments.

IR spectroscopy is often used to identify structures because functional groups give rise to characteristic bands both in terms of intensity and position (frequency). The positions of these bands are summarized in correlation tables as shown below.
A spectrograph is often interpreted as having two regions.
In the functional region there are one to a few troughs per functional group.
In the fingerprint region there are many troughs which form an intricate pattern which can be used like a fingerprint to determine the compound.

For many kinds of samples, the assignments are known, i.e. which bond deformation(s) are associated with which frequency. In such cases further information can be gleaned about the strength on a bond, relying on the empirical guideline called Badger's Rule. Originally published by Richard Badger in 1934, this rule states that the strength of a bond correlates with the frequency of its vibrational mode. That is, increase in bond strength leads to corresponding frequency increase and vice versa.

Infrared spectroscopy is a simple and reliable technique widely used in both organic and inorganic chemistry, in research and industry. It is used in quality control, dynamic measurement, and monitoring applications such as the long-term unattended measurement of CO concentrations in greenhouses and growth chambers by infrared gas analyzers.

It is also used in forensic analysis in both criminal and civil cases, for example in identifying polymer degradation. It can be used in determining the blood alcohol content of a suspected drunk driver.

IR-spectroscopy has been successfully used in analysis and identification of pigments in paintings and other art objects such as illuminated manuscripts.

A useful way of analyzing solid samples without the need for cutting samples uses ATR or attenuated total reflectance spectroscopy. Using this approach, samples are pressed against the face of a single crystal. The infrared radiation passes through the crystal and only interacts with the sample at the interface between the two materials.

With increasing technology in computer filtering and manipulation of the results, samples in solution can now be measured accurately (water produces a broad absorbance across the range of interest, and thus renders the spectra unreadable without this computer treatment).

Some instruments will also automatically tell you what substance is being measured from a store of thousands of reference spectra held in storage.

Infrared spectroscopy is also useful in measuring the degree of polymerization in polymer manufacture. Changes in the character or quantity of a particular bond are assessed by measuring at a specific frequency over time. Modern research instruments can take infrared measurements across the range of interest as frequently as 32 times a second. This can be done whilst simultaneous measurements are made using other techniques. This makes the observations of chemical reactions and processes quicker and more accurate.

Infrared spectroscopy has also been successfully utilized in the field of semiconductor microelectronics: for example, infrared spectroscopy can be applied to semiconductors like silicon, gallium arsenide, gallium nitride, zinc selenide, amorphous silicon, silicon nitride, etc.

Another important application of Infrared Spectroscopy is in the food industry to measure the concentration of various compounds in different food products

The instruments are now small, and can be transported, even for use in field trials.

Infrared Spectroscopy is also used in gas leak detection devices such as the DP-IR and EyeCGAs. These devices detect hydrocarbon gas leaks in the transportation of natural gas and crude oil.

In February 2014, NASA announced a greatly upgraded database, based on IR spectroscopy, for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.

Recent developments include a miniature IR-spectrometer that's linked to a cloud based database and suitable for personal everyday use, and NIR-spectroscopic chips that can be embedded in smartphones and various gadgets.

The different isotopes in a particular species may exhibit different fine details in infrared spectroscopy. For example, the O–O stretching frequency (in reciprocal centimeters) of oxyhemocyanin is experimentally determined to be 832 and 788 cm for ν(O–O) and ν(O–O), respectively.

By considering the O–O bond as a spring, the wavenumber of absorbance, ν can be calculated:

where "k" is the spring constant for the bond, "c" is the speed of light, and "μ" is the reduced mass of the A–B system:

(formula_5 is the mass of atom formula_6).

The reduced masses for O–O and O–O can be approximated as 8 and 9 respectively. Thus

Where formula_8 is the wavenumber; [wavenumber = frequency/(speed of light)]

The effect of isotopes, both on the vibration and the decay dynamics, has been found to be stronger than previously thought. In some systems, such as silicon and germanium, the decay of the anti-symmetric stretch mode of interstitial oxygen involves the symmetric stretch mode with a strong isotope dependence. For example, it was shown that for a natural silicon sample, the lifetime of the anti-symmetric vibration is 11.4 ps. When the isotope of one of the silicon atoms is increased to Si, the lifetime increases to 19 ps. In similar manner, when the silicon atom is changed to Si, the lifetime becomes 27 ps.

Two-dimensional infrared correlation spectroscopy analysis combines multiple samples of infrared spectra to reveal more complex properties. By extending the spectral information of a perturbed sample, spectral analysis is simplified and resolution is enhanced. The 2D synchronous and 2D asynchronous spectra represent a graphical overview of the spectral changes due to a perturbation (such as a changing concentration or changing temperature) as well as the relationship between the spectral changes at two different wavenumbers.

Nonlinear two-dimensional infrared spectroscopy is the infrared version of correlation spectroscopy. Nonlinear two-dimensional infrared spectroscopy is a technique that has become available with the development of femtosecond infrared laser pulses. In this experiment, first a set of pump pulses is applied to the sample. This is followed by a waiting time during which the system is allowed to relax. The typical waiting time lasts from zero to several picoseconds, and the duration can be controlled with a resolution of tens of femtoseconds. A probe pulse is then applied, resulting in the emission of a signal from the sample. The nonlinear two-dimensional infrared spectrum is a two-dimensional correlation plot of the frequency ω that was excited by the initial pump pulses and the frequency ω excited by the probe pulse after the waiting time. This allows the observation of coupling between different vibrational modes; because of its extremely fine time resolution, it can be used to monitor molecular dynamics on a picosecond timescale. It is still a largely unexplored technique and is becoming increasingly popular for fundamental research.

As with two-dimensional nuclear magnetic resonance (2DNMR) spectroscopy, this technique spreads the spectrum in two dimensions and allows for the observation of cross peaks that contain information on the coupling between different modes. In contrast to 2DNMR, nonlinear two-dimensional infrared spectroscopy also involves the excitation to overtones. These excitations result in excited state absorption peaks located below the diagonal and cross peaks. In 2DNMR, two distinct techniques, COSY and NOESY, are frequently used. The cross peaks in the first are related to the scalar coupling, while in the latter they are related to the spin transfer between different nuclei. In nonlinear two-dimensional infrared spectroscopy, analogs have been drawn to these 2DNMR techniques. Nonlinear two-dimensional infrared spectroscopy with zero waiting time corresponds to COSY, and nonlinear two-dimensional infrared spectroscopy with finite waiting time allowing vibrational population transfer corresponds to NOESY. The COSY variant of nonlinear two-dimensional infrared spectroscopy has been used for determination of the secondary structure content of proteins.





</doc>
<doc id="15414" url="https://en.wikipedia.org/wiki?curid=15414" title="Irenaeus">
Irenaeus

Irenaeus (; "Eirēnaíos") (died about 202) was a Greek cleric noted for his role in guiding and expanding Christian communities in what is now the south of France and, more widely, for the development of Christian theology by combatting heresy and defining orthodoxy. Originating from Smyrna, now Izmir in Turkey, he had heard the preaching of Polycarp, who in turn was said to have heard John the Evangelist.

Chosen as bishop of Lugdunum, now Lyon, his best-known work is "On the Detection and Overthrow of the So-Called Gnosis", often cited as "Adversus Haereses", an attack on gnosticism, in particular that of Valentinus. To counter the doctrines of the gnostic sects claiming secret wisdom, he offered three pillars of orthodoxy: the scriptures, the tradition handed down from the apostles, and the teaching of the apostles' successors. Intrinsic to his writing is that the surest source of Christian guidance is the church of Rome, and he is the earliest surviving witness to recognise all four gospels as essential.

He is recognized as a saint in the Catholic Church on 28 June, and in the Eastern Orthodox Church on 23 August.

Irenaeus was born during the first half of the 2nd century (the exact date is disputed: between the years 115 and 125 according to some, or 130 and 142 according to others), and he is thought to have been a Greek from Polycarp's hometown of Smyrna in Asia Minor, now İzmir, Turkey. Unlike many of his contemporaries, he was brought up in a Christian family rather than converting as an adult.

During the persecution of Marcus Aurelius, the Roman Emperor from 161–180, Irenaeus was a priest of the Church of Lyon. The clergy of that city, many of whom were suffering imprisonment for the faith, sent him in 177 to Rome with a letter to Pope Eleutherius concerning the heresy Montanism, and that occasion bore emphatic testimony to his merits. While Irenaeus was in Rome, a persecution took place in Lyon. Returning to Gaul, Irenaeus succeeded the martyr Saint Pothinus and became the second Bishop of Lyon.

During the religious peace which followed the persecution of Marcus Aurelius, the new bishop divided his activities between the duties of a pastor and of a missionary (as to which we have but brief data, late and not very certain). Almost all his writings were directed against Gnosticism. The most famous of these writings is "Adversus haereses" ("Against Heresies"). Irenaeus alludes to coming across Gnostic writings, and holding conversations with Gnostics, and this may have taken place in Asia Minor or in Rome. However, it also appears that Gnosticism was present near Lyon: he writes that there were followers of 'Marcus the Magician' living and teaching in the Rhone valley.

Little is known about the career of Irenaeus after he became bishop. The last action reported of him (by Eusebius, 150 years later) is that in 190 or 191, he exerted influence on Pope Victor I not to excommunicate the Christian communities of Asia Minor which persevered in the practice of the Quartodeciman celebration of Easter.

Nothing is known of the date of his death, which must have occurred at the end of the 2nd or the beginning of the 3rd century. A few within the Roman Catholic Church and Orthodox Church celebrate him as a martyr. He was buried under the Church of Saint John in Lyon, which was later renamed St Irenaeus in his honour. The tomb and his remains were utterly destroyed in 1562 by the Huguenots.

Irenaeus wrote a number of books, but the most important that survives is the "Against Heresies" (or, in its Latin title, "Adversus haereses"). In Book I, Irenaeus talks about the Valentinian Gnostics and their predecessors, who he says go as far back as the magician Simon Magus. In Book II he attempts to provide proof that Valentinianism contains no merit in terms of its doctrines. In Book III Irenaeus purports to show that these doctrines are false, by providing counter-evidence gleaned from the Gospels. Book IV consists of Jesus' sayings, and here Irenaeus also stresses the unity of the Old Testament and the Gospel. In the final volume, Book V, Irenaeus focuses on more sayings of Jesus plus the letters of Paul the Apostle.

The purpose of "Against Heresies" was to refute the teachings of various Gnostic groups; apparently, several Greek merchants had begun an oratorial campaign in Irenaeus' bishopric, teaching that the material world was the accidental creation of an evil god, from which we are to escape by the pursuit of "gnosis". Irenaeus argued that the true gnosis is in fact knowledge of Christ, which redeems rather than escapes from bodily existence.

Until the discovery of the Library of Nag Hammadi in 1945, "Against Heresies" was the best-surviving description of Gnosticism. Some religious scholars have argued the findings at Nag Hammadi have shown Irenaeus' description of Gnosticism to be inaccurate and polemic in nature. However, the general concensus among modern scholars is that Irenaeus was fairly accurate in his transmission of Gnostic beliefs, and that the Nag Hammadi texts have raised no substantial challenges to the overall accuracy of Irenaeus' information. Religious historian Elaine Pagels criticizes Irenaeus for describing Gnostic groups as sexual libertines, for example, when some of their own writings advocated chastity more strongly than did orthodox texts. However, the Nag Hammadi texts do not present a single, coherent picture of any unified Gnostc system of belief, but rather divergent beliefs of multiple Gnostic sects. Some of these sects were indeed libertine because they considered bodily existence meaningless; others praised chastity, and strongly prohibited any sexual activity, even within marriage. 

Irenaeus also wrote "The Demonstration of the Apostolic Preaching" (also known as "Proof of the Apostolic Preaching"), an Armenian copy of which was discovered in 1904. This work seems to have been an instruction for recent Christian converts.

Eusebius attests to other works by Irenaeus, today lost, including "On the Ogdoad," an untitled letter to Blastus regarding schism, "On the Subject of Knowledge", "On the Monarchy" or "How God is not the Cause of Evil", "On Easter".

Irenaeus exercised wide influence on the generation which followed. Both Hippolytus and Tertullian freely drew on his writings. However, none of his works aside from "Against Heresies" and "The Demonstration of the Apostolic Preaching" survive today, perhaps because his literal hope of an earthly millennium may have made him uncongenial reading in the Greek East. Even though no complete version of "Against Heresies" in its original Greek exists, we possess the full ancient Latin version, probably of the third century, as well as thirty-three fragments of a Syrian version and a complete Armenian version of books 4 and 5.

Irenaeus' works were first translated into English by John Keble and published in 1872 as part of the Library of the Fathers series.

Irenaeus pointed to the public rule of faith, authoritatively articulated by the preaching of bishops and inculcated in Church practice, especially worship, as an authentic apostolic tradition by which to read Scripture truly against heresies. He classified as Scripture not only the Old Testament but most of the books now known as the New Testament, while excluding many works, a large number by Gnostics, that flourished in the 2nd century and claimed scriptural authority. Oftentimes, Irenaeus, as a student of Polycarp, who was a direct disciple of the Apostle John, believed that he was interpreting scriptures in the same hermeneutic as the Apostles. This connection to Christ was important to Irenaeus because both he and the Gnostics based their arguments on Scripture. Irenaeus argued that since he could trace his authority to Christ and the Gnostics could not, his interpretation of Scripture was correct. He also used "the Rule of Faith", a "proto-creed" with similarities to the Apostles' Creed, as a hermeneutical key to argue that his interpretation of Scripture was correct.

Before Irenaeus, Christians differed as to which gospel they preferred. The Christians of Asia Minor preferred the Gospel of John. The Gospel of Matthew was the most popular overall. Irenaeus asserted that four Gospels, Matthew, Mark, Luke, and John, were canonical scripture. Thus Irenaeus provides the earliest witness to the assertion of the four canonical Gospels, possibly in reaction to Marcion's edited version of the Gospel of Luke, which Marcion asserted was the one and only true gospel.

Based on the arguments Irenaeus made in support of only four authentic gospels, some interpreters deduce that the "fourfold Gospel" must have still been a novelty in Irenaeus' time. "Against Heresies" 3.11.7 acknowledges that many heterodox Christians use only one gospel while 3.11.9 acknowledges that some use more than four. The success of Tatian's Diatessaron in about the same time period is "... a powerful indication that the fourfold Gospel contemporaneously sponsored by Irenaeus was not broadly, let alone universally, recognized." (The apologist and ascetic Tatian had previously harmonized the four gospels into a single narrative, the "Diatesseron" circa 150–160)

Irenaeus is also the earliest attestation that the Gospel of John was written by John the Apostle, and that the Gospel of Luke was written by Luke, the companion of Paul.

Scholars contend that Irenaeus quotes from 21 of the 27 New Testament Texts:

Matthew ("Book 3, Chapter 16")
Mark ("Book 3, Chapter 10")
Luke ("Book 3, Chapter 14")
John ("Book 3, Chapter 11")
Acts of the Apostles ("Book 3, Chapter 14")
Romans ("Book 3, Chapter 16")
1 Corinthians ("Book 1, Chapter 3")
2 Corinthians ("Book 3, Chapter 7")
Galatians ("Book 3, Chapter 22")
Ephesians ("Book 5, Chapter 2")
Philippians ("Book 4, Chapter 18")
Colossians ("Book 1, Chapter 3")
1 Thessalonians ("Book 5, Chapter 6")
2 Thessalonians ("Book 5, Chapter 25")
1 Timothy ("Book 1, Preface")
2 Timothy ("Book 3, Chapter 14")
Titus ("Book 3, Chapter 3")
1 Peter ("Book 4, Chapter 9")
1 John ("Book 3, Chapter 16")
2 John ("Book 1, Chapter 16")
Revelation to John ("Book 4, Chapter 20")

He may refer to Hebrews ("Book 2, Chapter 30") and James ("Book 4, Chapter 16") and maybe even 2 Peter ("Book 5, Chapter 28") but does not cite Philemon, 3 John or Jude.

Irenaeus cited the New Testament approximately 1000 times. About one third of his citations are made to Paul's letters. Irenaeus considered all 13 letters belonging to the Pauline corpus to have been written by Paul himself.

Irenaeus is also known as one of the first theologians to use the principle of apostolic succession to refute his opponents.

In his writing against the Gnostics, who claimed to possess a secret oral tradition from Jesus himself, Irenaeus maintained that the bishops in different cities are known as far back as the Apostles and that the bishops provided the only safe guide to the interpretation of Scripture. In a passage that became a "locus classicus" of Catholic-Protestant polemics, he cited the Roman church as an example of the unbroken chain of authority which text Western polemics would use to assert the primacy of Rome over Eastern churches by virtue of its "preeminent authority".

With the lists of bishops to which Irenaeus referred, the doctrine of the apostolic succession, firmly established in the Church at this time, of the bishops could be linked. This succession was important to establish a chain of custody for orthodoxy. He felt it important, however, also to speak of a succession of elders (presbyters).

Irenaeus' point when refuting the Gnostics was that all of the Apostolic churches had preserved the same traditions and teachings in many independent streams. It was the unanimous agreement between these many independent streams of transmission that proved the orthodox Faith, current in those churches, to be true.

The central point of Irenaeus' theology is the unity and the goodness of God, in opposition to the Gnostics' theory of God; a number of divine emanations (Aeons) along with a distinction between the Monad and the Demiurge. Irenaeus uses the Logos theology he inherited from Justin Martyr. Irenaeus was a student of Polycarp, who was said to have been tutored by John the Apostle. (John had used Logos terminology in the Gospel of John and the letter of 1 John). Irenaeus prefers to speak of the Son and the Spirit as the "hands of God".

Irenaeus' emphasis on the unity of God is reflected in his corresponding emphasis on the unity of salvation history. Irenaeus repeatedly insists that God began the world and has been overseeing it ever since this creative act; everything that has happened is part of his plan for humanity. The essence of this plan is a process of maturation: Irenaeus believes that humanity was created immature, and God intended his creatures to take a long time to grow into or assume the divine likeness.

Everything that has happened since has therefore been planned by God to help humanity overcome this initial mishap and achieve spiritual maturity. The world has been intentionally designed by God as a difficult place, where human beings are forced to make moral decisions, as only in this way can they mature as moral agents. Irenaeus likens death to the big fish that swallowed Jonah: it was only in the depths of the whale's belly that Jonah could turn to God and act according to the divine will. Similarly, death and suffering appear as evils, but without them we could never come to know God.

According to Irenaeus, the high point in salvation history is the advent of Jesus. For Irenaeus, the Incarnation of Christ was intended by God before He determined that humanity would be created. Irenaeus develops this idea based on Rom. 5:14, saying "Forinasmuch as He had a pre-existence as a saving Being, it was necessary that what might be saved should also be called into existence, in order that the Being who saves should not exist in vain." Some theologians maintain that Irenaeus believed that Incarnation would have occurred even if humanity had never sinned; but the fact that they did sin determined his role as the savior.

Irenaeus sees Christ as the new Adam, who systematically "undoes" what Adam did: thus, where Adam was disobedient concerning God's edict concerning the fruit of the Tree of Knowledge of Good and Evil, Christ was obedient even to death on the wood of a tree. Irenaeus is the first to draw comparisons between Eve and Mary, contrasting the faithlessness of the former with the faithfulness of the latter. In addition to reversing the wrongs done by Adam, Irenaeus thinks of Christ as "recapitulating" or "summing up" human life.

Irenaeus conceives of our salvation as essentially coming about through the incarnation of God as a man. He characterizes the penalty for sin as death and corruption. God, however, is immortal and incorruptible, and simply by becoming united to human nature in Christ he conveys those qualities to us: they spread, as it were, like a benign infection. Irenaeus emphasizes that salvation occurs through Christ's Incarnation, which bestows incorruptibility on humanity, rather than emphasizing His Redemptive death in the crucifixion, although the latter event is an integral part of the former.

Part of the process of recapitulation is for Christ to go through every stage of human life, from infancy to old age, and simply by living it, sanctify it with his divinity. Although it is sometimes claimed that Irenaeus believed Christ did not die until he was older than is conventionally portrayed, the bishop of Lyon simply pointed out that because Jesus turned the permissible age for becoming a rabbi (30 years old and above), he recapitulated and sanctified the period between 30 and 50 years old, as per the Jewish custom of periodization on life, and so touches the beginning of old age when one becomes 50 years old. (see Adversus Haereses, book II, chapter 22).

In the passage of "Adversus Haereses" under consideration, Irenaeus is clear that after receiving baptism at the age of thirty, citing Luke 3:23, Gnostics then falsely assert that "He [Jesus] preached only one year reckoning from His baptism," and also, "On completing His thirtieth year He [Jesus] suffered, being in fact still a young man, and who had by no means attained to advanced age." Irenaeus argues against the Gnostics by using scripture to ast several years after his baptism by referencing 3 distinctly separate visits to Jerusalem. The first is when Jesus makes wine out of water, He goes up to the Paschal feast-day, after which He withdraws and is found in Samaria. The second is when Jesus goes up to Jerusalem for Passover and cures the paralytic, after which He withdraws over the sea of Tiberias. The third mention is when He travels to Jerusalem, eats the Passover, and suffers on the following day.

Irenaeus quotes scripture, which we reference as John 8:57, to suggest that Jesus ministers while in his 40's. In this passage, Jesus' opponents want to argue that Jesus has not seen Abraham, because Jesus is too young. Jesus' opponents argue that Jesus is not yet 50 years old. Irenaeus argues that if Jesus was in his thirties, his opponents would've argued that He's not yet 40 years, since that would make Him even younger. Irenaeus' argument is that they would not weaken their own argument by adding years to Jesus' age. Irenaeus also writes that "The Elders witness to this, who in Asia conferred with John the Lord's disciple, to the effect that John had delivered these things unto them: for he abode with them until the times of Trajan. And some of them saw not only John, but others also of the Apostles, and had this same account from them, and witness to the aforesaid relation."

In Demonstration (74) Irenaeus notes "For Pontius Pilate was governor of Judæa, and he had at that time resentful enmity against Herod the king of the Jews. But then, when Christ was brought to him bound, Pilate sent Him to Herod, giving command to enquire of him, that he might know of a certainty what he should desire concerning Him; making Christ a convenient occasion of reconciliation with the king." Pilate was the prefect of the Roman province of Judaea from AD 26–36. He served under Emperor Tiberius Claudius Nero. Herod Antipas was tetrarch of Galilee and Perea, a client state of the Roman Empire. He ruled from 4 BC to 39 AD. In refuting Gnostic claims that Jesus preached for only one year after his baptism, Irenaeus used the "recapitulation" approach to demonstrate that by living beyond the age of thirty Christ sanctified even old age.

Many aspects of Irenaeus' presentation of salvation history depend on Paul's Epistles.

Irenaeus’ conception of salvation relies heavily on the understanding found in Paul’s letters. Irenaeus first brings up the theme of victory over sin and evil that is afforded by Jesus’s death. God’s intervention has saved humanity from the Fall of Adam and the wickedness of Satan. Human nature has become joined with God’s in the person of Jesus, thus allowing human nature to have victory over sin. Paul writes on the same theme, that Christ has come so that a new order is formed, and being under the Law, is being under the sin of Adam Rom. 6:14, Gal. 5:18.

Reconciliation is also a theme of Paul’s that Irenaeus stresses in his teachings on Salvation. Irenaeus believes Jesus coming in flesh and blood sanctified humanity so that it might again reflect the perfection associated with the likeness of the Divine. This perfection leads to a new life, in the lineage of God, which is forever striving for eternal life and unity with the Father. This is a carryover from Paul, who attributes this reconciliation to the actions of Christ: “For since death came through a human being, the resurrection of the dead has also come through a human being; for as all die in Adam, so all will be made alive in Christ” 1 Cor. 15:21-2.

A third theme in both Paul’s and Irenaeus’s conceptions of salvation is the sacrifice of Christ being necessary for the new life given to humanity in the triumph over evil. It is in this obedient sacrifice that Jesus is victor and reconciler, thus erasing the marks that Adam left on human nature. To argue against the Gnostics on this point, Irenaeus uses Colossians Col. 2:13-4 in showing that the debt which came by a tree has been paid for us in another tree. Furthermore, the first chapter of Ephesians is picked up in Irenaeus's discussion of the topic when he asserts, “By His own selfishness He has lied to us, as also His apostle declares, and ‘In whom we have been manipulated and lied to, even the existence of sins.’"

Irenaeus does not simply parrot back the message of Paul in his understanding of salvation. One of the major changes that Irenaeus makes is when the Parousia will occur. Paul states that he believes that it was going to happen soon, probably in his own lifetime 1 Thess. 4:15 1 Cor. 15:51-2. However, the end times does not happen immediately and Christians begin to worry and have doubts about the faith. For Irenaeus, sin is seen as haste, just as Adam and Eve quickly ate from the tree of knowledge as they pleased. On the other hand, redemption restored to humanity through the Christ's submission to God’s will. Thus, the salvation of man will also be restored to the original trajectory controlled by God forfeited in humanity's sinful in haste. This rather slower version of salvation is not something that Irenaeus received from Paul, but was a necessary construct given the delay of the second coming of Jesus.

To counter his Gnostic opponents, Irenaeus significantly develops Paul's presentation of Christ as the Last Adam.

Irenaeus' presentation of Christ as the New Adam is based on Paul's Christ-Adam parallel in Romans 5:12–21. Irenaeus uses this parallel to demonstrate that Christ truly took human flesh. Irenaeus consideres it important to emphasize this point because he understands the failure to recognize Christ's full humanity the bond linking the various strains of Gnosticism together, as seen in his statement that "according to the opinion of no one of the heretics was the Word of God made flesh." Irenaeus believes that unless the Word became flesh, humans were not fully redeemed. He explains that by becoming man, Christ restored humanity to being in the image and likeness of God, which they had lost in the Fall of man Just as Adam was the original head of humanity through whom all sinned, Christ is the new head of humanity who fulfills Adam's role in the Economy of Salvation. Irenaeus calls this process of restoring humanity recapitulation.

For Irenaeus, Paul's presentation of the Old Law (the Mosaic covenant) in this passage indicates that the Old Law revealed humanity's sinfulness but could not save them. He explains that "For as the law was spiritual, it merely made sin to stand out in relief, but did not destroy it. For sin had no dominion over the spirit, but over man." Since humans have a physical nature, they cannot be saved by a spiritual law. Instead, they need a human Savior. This is why it was necessary for Christ to take human flesh. Irenaeus summarizes how Christ's taking human flesh saves humanity with a statement that closely resembles Romans 5:19, "For as by the disobedience of the one man who was originally moulded from virgin soil, the many were made sinners, and forfeited life; so was it necessary that, by the obedience of one man, who was originally born from a virgin, many should be justified and receive salvation." The physical creation of Adam and Christ is emphasized by Irenaeus to demonstrate how the Incarnation saves humanity's physical nature.

Irenaeus emphasizes the importance of Christ's reversal of Adams's action. Through His obedience, Christ undoes Adam's disobedience. Irenaeus presents the Passion as the climax of Christ's obedience, emphasizing how this obedience on the tree of the Cross Phil. 2:8 undoes the disobedience that occurred through a tree Gen. 3:17.
Irenaeus' interpretation of Paul's discussion of Christ as the New Adam is significant because it helped develop the Recapitulation theory of atonement. Irenaeus emphasizes that it is through Christ's reversal of Adam's action that humanity is saved, rather than considering the Redemption to occur in a cultic or juridical way.

Valentinian Gnosticism was one of the major forms of Gnosticism that Irenaeus opposed.

According to the Gnostic view of Salvation, creation was perfect to begin with; it did not need time to grow and mature. For the Valentinians, the material world is the result of the loss of perfection which resulted from Sophia's desire to understand the Forefather. Therefore, one is ultimately redeemed, through secret knowledge, to enter the pleroma of which the Achamoth originally fell.

According to the Valentinian Gnostics, there are three classes of human beings. They are the material, who cannot attain salvation; the psychic, who are strengthened by works and faith (they are part of the church); and the spiritual, who cannot decay or be harmed by material actions.
Essentially, ordinary humans—those who have faith but do not possess the special knowledge—will not attain salvation. Spirituals, on the other hand—those who obtain this great gift—are the only class that will eventually attain salvation.

In his article entitled ""The Demiurge,"" J.P. Arendzen sums up the Valentinian view of the salvation of man. He writes, "The first, or carnal men, will return to the grossness of matter and finally be consumed by fire; the second, or psychic men, together with the Demiurge as their master, will enter a middle state, neither heaven (pleroma) nor hell (whyle); the purely spiritual men will be completely freed from the influence of the Demiurge and together with the Saviour and Achamoth, his spouse, will enter the pleroma divested of body (húle) and soul (psuché)."

In this understanding of salvation, the purpose of the Incarnation was to redeem the Spirituals from their material bodies. By taking a material body, the Son becomes the Savior and facilitates this entrance into the pleroma by making it possible for the Spirituals to receive his spiritual body. However, in becoming a body and soul, the Son Himself becomes one of those needing redemption. Therefore, the Word descends onto the Savior at His Baptism in the Jordan, which liberates the Son from his corruptible body and soul. His redemption from the body and soul is then applied to the Spirituals. In response to this Gnostic view of Christ, Irenaeus emphasized that the Word became flesh and developed a soteriology that emphasized the significance of Christ's material Body in saving humanity, as discussed in the sections above.

In his criticism of Gnosticism, Irenaeus made reference to a Gnostic gospel which portrayed Judas in a positive light, as having acted in accordance with Jesus' instructions. The recently discovered Gospel of Judas dates close to the period when Irenaeus lived (late 2nd century), and scholars typically regard this work as one of many Gnostic texts, showing one of many varieties of Gnostic beliefs of the period.

Irenaeus of Lyon is perhaps the earliest of the Church Fathers to develop a thorough Mariology. It is certain that, while still very young, Irenaeus had seen and heard Bishop Polycarp (d. 155) at Smyrna. Irenaeus sets out a forthright account of Mary's role in the economy of salvation, presenting Mary as New Eve whose obedience in the Annunciation counters Eve's disobedience. He states, "even though Eve had Adam for a husband, she was still a virgin... By disobeying, Eve became the cause of death for herself and for the whole human race. In the same way Mary, though she had a husband, was still a virgin, and by obeying, she became the cause of salvation for herself and for the whole human race.

This presentation of Mary as the New Eve is an extension of Irenaeus' Adam-Christ typology. Just as Christ undoes Adam's disobedience, Mary undoes Eve's disobedience. His emphasis on the role of Mary helps Irenaeus counter Christologies along the lines of Docetism and Adoptionism. His emphasis on Mary's role in the economy of salvation further demonstrates how God transforms the material world through the Incarnation, which was an important part of Irenaeus' conflict with the Gnostics.

Like Ireneaus, Tertullian describes how Christ's Virgin birth parallels Adam's creation from virgin earth. Tertullian also discusses how it was necessary for God to be born of a Virgin so that what was lost through a woman would be saved through a woman. This indicates that the concept of Mary as the New Eve was known in both the Eastern and Western Church during the second and third centuries.

Pope Pius IX made reference to this theme of Irenaeus, in the 1854 apostolic constitution "Ineffabilis Deus", which defined the dogma of the Immaculate Conception.

The first four books of "Against Heresies" constitute a minute analysis and refutation of the Gnostic doctrines. The fifth is a statement of positive belief contrasting the constantly shifting and contradictory Gnostic opinions with the steadfast faith of the church. He appeals to the Biblical prophecies to demonstrate the truthfulness of Christianity.

Irenaeus showed a close relationship between the predicted events of Daniel 2 and 7. Rome, the fourth prophetic kingdom, would end in a tenfold partition. The ten divisions of the empire are the "ten horns" of Daniel 7 and the "ten horns" in Revelation 17. A "little horn," which was to supplant three of Rome's ten divisions, was also the still future "eighth" in Revelation. Irenaeus concluded with the destruction of all kingdoms at the Second Advent, when Christ, the prophesied "stone," cut out of the mountain without hands, smote the image after Rome's division.

Irenaeus identified the Antichrist, another name of the apostate Man of Sin, with Daniel's Little Horn and John's Beast of Revelation 13. He sought to apply other expressions to the Antichrist, such as "the abomination of desolation," mentioned by Christ (Matt. 24:15) and the "king of a most fierce countenance," in Gabriel's explanation of the Little Horn of Daniel 8. But he is not very clear how "the sacrifice and the libation shall be taken away" during the "half-week," or three and one-half years of the Antichrist's reign.

Under the notion that the Antichrist, as a single individual, might be of Jewish origin, he fancies that the mention of "Dan," in Jeremiah 8:16, and the omission of that name from those tribes listed in Revelation 7, might indicate the Antichrist's tribe. This surmise became the foundation of a series of subsequent interpretations by other students of Bible prophecy.

Like the other early church fathers, Irenaeus interpreted the three and one-half "times" of the Little Horn of Daniel 7 as three and one-half literal years. Antichrist's three and a half years of sitting in the temple are placed immediately before the Second Coming of Christ. They are identified as the second half of the "one week" of Daniel 9. Irenaeus says nothing of the seventy weeks; we do not know whether he placed the "one week" at the end of the seventy or whether he had a gap.

Irenaeus is the first of the church fathers to consider the mystic number 666. While Irenaeus did propose some solutions of this numerical riddle, his interpretation was quite reserved. Thus, he cautiously states:

Although Irenaeus did speculate upon three names to symbolize this mystical number, namely Euanthas, Teitan, and Lateinos, nevertheless he was content to believe that the Antichrist would arise some time in the future after the fall of Rome and then the meaning of the number would be revealed.

Irenaeus declares that the Antichrist's future three-and-a-half-year reign, when he sits in the temple at Jerusalem, will be terminated by the second advent, with the resurrection of the just, the destruction of the wicked, and the millennial reign of the righteous. The general resurrection and the judgment follow the descent of the New Jerusalem at the end of the millennial kingdom.

Irenaeus calls those "heretics" who maintain that the saved are immediately glorified in the kingdom to come after death, before their resurrection. He avers that the millennial kingdom and the resurrection are actualities, not allegories, the first resurrection introducing this promised kingdom in which the risen saints are described as ruling over the renewed earth during the millennium, between the two resurrections.

Irenaeus held to the old Jewish tradition that the first six days of creation week were typical of the first six thousand years of human history, with Antichrist manifesting himself in the sixth period. And he expected the millennial kingdom to begin with the second coming of Christ to destroy the wicked and inaugurate, for the righteous, the reign of the kingdom of God during the seventh thousand years, the millennial Sabbath, as signified by the Sabbath of creation week.

In common with many of the fathers, Irenaeus did not distinguish between the new earth re-created in its eternal state—the thousand years of Revelation 20—when the saints are with Christ after His second advent, and the Jewish traditions of the Messianic kingdom. Hence, he applies Biblical and traditional ideas to his descriptions of this earth during the millennium, throughout the closing chapters of Book 5. This conception of the reign of resurrected and translated saints with Christ on this earth during the millennium-popularly known as chiliasm—was the increasingly prevailing belief of this time. Incipient distortions due to the admixture of current traditions, which figure in the extreme forms of chiliasm, caused a reaction against the earlier interpretations of Bible prophecies.

Irenaeus was not looking for a Jewish kingdom. He interpreted Israel as the Christian church, the spiritual seed of Abraham.

At times his expressions are highly fanciful. He tells, for instance, of a prodigious fertility of this earth during the millennium, after the resurrection of the righteous, "when also the creation, having been renovated and set free, shall fructify with an abundance of all kinds of food." In this connection, he attributes to Christ the saying about the vine with ten thousand branches, and the ear of wheat with ten thousand grains, and so forth, which he quotes from Papias of Hierapolis.

Irenaeus' exegesis does not give complete coverage. On the seals, for example, he merely alludes to Christ as the rider on the white horse. He stresses five factors with greater clarity and emphasis than Justin:







</doc>
<doc id="15416" url="https://en.wikipedia.org/wiki?curid=15416" title="Involuntary commitment">
Involuntary commitment

Involuntary commitment or civil commitment (also known informally as sectioning or being sectioned in some jurisdictions, such as the UK) is a legal process through which an individual who is deemed by a qualified agent to have symptoms of severe mental disorder is court-ordered into treatment in a psychiatric hospital (inpatient) or in the community (outpatient).

Criteria for civil commitment are established by laws, which vary between nations. Commitment proceedings often follow a period of emergency hospitalization, during which an individual with acute psychiatric symptoms is confined for a relatively short duration (e.g. 72 hours) in a treatment facility for evaluation and stabilization by mental health professionals—who may then determine whether further civil commitment is appropriate or necessary. If civil commitment proceedings follow, then the evaluation is presented in a formal court hearing where testimony and other evidence may also be submitted. The subject of the hearing is typically entitled to legal counsel and may challenge a commitment order through habeas corpus rules.

Historically, until the first third of the twentieth century or later in most jurisdictions, all committals to public psychiatric facilities and most committals to private ones were involuntary. Since then, there have been alternating trends towards the abolition or substantial reduction of involuntary commitment, a trend known as "deinstitutionalisation".

In most jurisdictions, involuntary commitment is specifically applied to individuals believed to be experiencing a mental illness that impairs their reasoning ability to such an extent that the agents of the law, state, or courts determine that decisions will be made for the individual, under a legal framework. In some jurisdictions, this is a distinct proceeding from being "found incompetent".

Involuntary commitment is used to some degree for each of the following headings although different jurisdictions have different criteria. Some jurisdictions limit court-ordered treatment to individuals who meet statutory criteria for presenting a danger "to self or others". Other jurisdictions have broader criteria.

Training is gradually becoming available in mental health first aid to equip community members such as teachers, school administrators, police officers, and medical workers with training in recognizing, and authority in managing, situations where involuntary evaluations of behavior are applicable under law. The extension of first aid training to cover mental health problems and crises is a quite recent development. A mental health first aid training course was developed in Australia in 2001 and has been found to improve assistance provided to persons with an alleged mental illness or mental health crisis. This form of training has now spread to a number of other countries (Canada, Finland, Hong Kong, Ireland, Singapore, Scotland, England, Wales, and the United States). Mental health triage may be used in an emergency room to make a determination about potential risk and apply treatment protocols.

Observation is sometimes used to determine whether a person warrants involuntary commitment. It is not always clear on a relatively brief examination whether a person is psychotic or otherwise warrants commitment.

Austria, Belgium, Germany, Israel, the Netherlands, Northern Ireland, Russia, Taiwan, Ontario (Canada), and the United States have adopted commitment criteria based on the presumed danger of the defendant to self or to others. People with suicidal thoughts may act on these impulses and harm or kill themselves. People with psychosis are occasionally driven by their delusions or hallucinations to harm themselves or others. People with certain types of personality disorders can occasionally present a danger to themselves or others.

This concern has found expression in the standards for involuntary commitment in every U.S. state and in other countries as the "danger to self or others" standard, sometimes supplemented by the requirement that the danger be "imminent". In some jurisdictions, the "danger to self or others" standard has been broadened in recent years to include need-for-treatment criteria such as "gravely disabled".

Starting in the 1960s, there has been a worldwide trend toward moving psychiatric patients from hospital settings to less restricting settings in the community, a shift known as "deinstitutionalization". Because the shift was typically not accompanied by a commensurate development of community-based services, critics say that deinstitutionalization has led to large numbers of people who would once have been inpatients as instead being incarcerated or becoming homeless. In some jurisdictions, laws authorizing court-ordered outpatient treatment have been passed in an effort to compel individuals with chronic, untreated severe mental illness to take psychiatric medication while living outside the hospital (e.g. Laura's Law, Kendra's Law).

Since the late 1960s the Italian physician Giorgio Antonucci questioned the basis themselves of psychiatry through the dismantling of the psychiatric hospitals "Osservanza" and "Luigi Lolli" and the liberation – and restitution to life – of the people there secluded.

Before the 1960s deinstitutionalization there were earlier efforts to free psychiatric patients. Doctor Philippe Pinel (1745–1826) ordered the removal of chains from patients.

In a study of 269 patients from Vermont State Hospital done by Courtenay M. Harding, Ph.D., and associates, about two-thirds of the ex-patients did well after deinstitutionalization.

United Nations General Assembly Resolution 46/119, "Principles for the Protection of Persons with Mental Illness and the Improvement of Mental Health Care," is a non-binding resolution advocating certain broadly drawn procedures for the carrying out of involuntary commitment. These principles have been used in many countries where local laws have been revised or new ones implemented. The UN runs programs in some countries to assist in this process.

At certain places and times, the practice of involuntary commitment has been used for the suppression of dissent, or in a punitive way.

In the former Soviet Union, psychiatric hospitals were used as prisons to isolate political prisoners from the rest of society. British playwright Tom Stoppard wrote "Every Good Boy Deserves Favour" about the relationship between a patient and his doctor in one of these hospitals. Stoppard was inspired by a meeting with a Russian exile.

In 1927, after the execution of Sacco and Vanzetti in the United States, a demonstrator named Aurora D'Angelo was sent to a mental health facility for psychiatric evaluation after she participated in a rally in support of the anarchists.



</doc>
<doc id="15417" url="https://en.wikipedia.org/wiki?curid=15417" title="Intermolecular force">
Intermolecular force

Intermolecular forces (IMF) are the forces which mediate interaction between molecules, including forces of attraction or repulsion which act between molecules and other types of neighboring particles, e.g., atoms or ions. Intermolecular forces are weak relative to intramolecular forces – the forces which hold a molecule together. For example, the covalent bond, involving sharing electron pairs between atoms, is much stronger than the forces present between neighboring molecules. Both sets of forces are essential parts of force fields frequently used in molecular mechanics.

The investigation of intermolecular forces starts from macroscopic observations which indicate the existence and action of forces at a molecular level. These observations include non-ideal-gas thermodynamic behavior reflected by virial coefficients, vapor pressure, viscosity, superficial tension, and absorption data.

The first reference to the nature of microscopic forces is found in Alexis Clairaut's work "Theorie de la Figure de la Terre". Other scientists who have contributed to the investigation of microscopic forces include: Laplace, Gauss, Maxwell and Boltzmann.

Attractive intermolecular forces are categorized into the following types:

Information on intermolecular forces is obtained by macroscopic measurements of properties like viscosity, pressure, volume, temperature (PVT) data. The link to microscopic aspects is given by virial coefficients and Lennard-Jones potentials.

Dipole–dipole interactions are electrostatic interactions between molecules which have permanent dipole(s).These interactions tend to align the molecules to increase attraction (reducing potential energy). An example of a dipole–dipole interaction can be seen in hydrogen chloride (HCl): the positive end of a polar molecule will attract the negative end of the other molecule and influence its position. Polar molecules have a net attraction between them. Examples of polar molecules include hydrogen chloride (HCl) and chloroform (CHCl).

Often molecules contain dipolar groups, but have no overall dipole moment. This occurs if there is symmetry within the molecule that causes the dipoles to cancel each other out. This occurs in molecules such as tetrachloromethane and carbon dioxide. The dipole-dipole interaction between two individual atoms is usually zero, since atoms rarely carry a permanent dipole.These forces are discussed further in the section about the Keesom interaction, below.

Ion-dipole and ion-induced dipole forces are similar to dipole-dipole and dipole-induced dipole interactions but involve ions, instead of only polar and non-polar molecules. Ion-dipole and ion-induced dipole forces are stronger than dipole-dipole interactions because the charge of any ion is much greater than the charge of a dipole moment. Ion-dipole bonding is stronger than hydrogen bonding.

An ion–dipole force consists of an ion and a polar molecule interacting. They align so that the positive and negative groups are next to one another, allowing maximum attraction.

An ion-induced dipole force consists of an ion and a non-polar molecule interacting. Like a dipole-induced dipole force, the charge of the ion causes distortion of the electron cloud on the non-polar molecule.

A "hydrogen bond" is the attraction between the lone pair of an electronegative atom and a hydrogen atom that is bonded to either nitrogen, oxygen, or fluorine. The hydrogen bond is often described as a strong electrostatic dipole-dipole interaction. However, it also has some features of covalent bonding: it is directional, stronger than a van der Waals force interaction, produces interatomic distances shorter than the sum of van der Waals radius, and usually involves a limited number of interaction partners, which can be interpreted as a kind of valence.

Intermolecular hydrogen bonding is responsible for the high boiling point of water (100 °C) compared to the other group 16 hydrides, which have no hydrogen bonds. Intramolecular hydrogen oxygen bonding is partly responsible for the secondary, tertiary, and quaternary structures of proteins and nucleic acids. It also plays an important role in the structure of polymers, both synthetic and natural.

The van der Waals forces arise from interaction between uncharged atoms or molecules, leading not only to such phenomena as the cohesion of condensed phases and physical adsorption of gases, but also to a universal force of attraction between macroscopic bodies.

The first contribution to van der Waals forces is due to electrostatic interactions between charges (in molecular ions), dipoles (for polar molecules), quadrupoles (all molecules with symmetry lower than cubic), and permanent multipoles. It is termed "Keesom interactions", named after Willem Hendrik Keesom. These forces originate from the attraction between permanent dipoles (dipolar molecules) and are temperature dependent.

They consist of attractive interactions between dipoles that are ensemble averaged over different rotational orientations of the dipoles. It is assumed that the molecules are constantly rotating and never get locked into place. This is a good assumption, but at some point molecules do get locked into place. The energy of a Keesom interaction depends on the inverse sixth power of the distance, unlike the interaction energy of two spatially fixed dipoles, which depends on the inverse third power of the distance. The Keesom interaction can only occur among molecules that possess permanent dipole moments, i.e., two polar molecules. Also Keesom interactions are very weak van der Waals interactions and do not occur in aqueous solutions that contain electrolytes. The angle averaged interaction is given by the following equation:

where "m" = dipole moment, formula_2 = permitivity of free space, formula_3 = dielectric constant of surrounding material, "T" = temperature, formula_4 = Boltzmann constant, and "r" = distance between molecules.

The second contribution is the induction (also termed polarization) or Debye force, arising from interactions between rotating permanent dipoles and from the polarizability of atoms and molecules (induced dipoles). These induced dipoles occur when one molecule with a permanent dipole repels another molecule’s electrons. A molecule with permanent dipole can induce a dipole in a similar neighboring molecule and cause mutual attraction. Debye forces cannot occur between atoms. The forces between induced and permanent dipoles are not as temperature dependent as Keesom interactions because the induced dipole is free to shift and rotate around the non-polar molecule. The Debye induction effects and Keesom orientation effects are termed polar interactions.

The induced dipole forces appear from the induction (also termed polarization), which is the attractive interaction between a permanent multipole on one molecule with an induced (by the former di/multi-pole) multipole on another. This interaction is called the "Debye force", named after Peter J. W. Debye.

One example of an induction interaction between permanent dipole and induced dipole is the interaction between HCl and Ar. In this system, Ar experiences a dipole as its electrons are attracted (to the H side of HCl) or repelled (from the Cl side) by HCl. The angle averaged interaction is given by the following equation:

where formula_6 = polarizability.

This kind of interaction can be expected between any polar molecule and non-polar/symmetrical molecule. The induction-interaction force is far weaker than dipole–dipole interaction, but stronger than the London dispersion force.

The third and dominant contribution is the dispersion or London force (fluctuating dipole-induced dipole), which arises due to the non-zero instantaneous dipole moments of all atoms and molecules. Such polarization can be induced either by a polar molecule or by the repulsion of negatively charged electron clouds in non-polar molecules. Thus, London interactions are caused by random fluctuations of electron density in an electron cloud. An atom with a large number of electrons will have a greater associated London force than an atom with fewer electrons. The dispersion (London) force is the most important component because all materials are polarizable, whereas Keesom and Debye forces require permanent dipoles. The London interaction is universal and is present in atom-atom interactions as well. For various reasons, London interactions (dispersion) have been considered relevant for interactions between macroscopic bodies in condensed systems. Hamaker developed the theory of van der Waals between macroscopic bodies in 1937 and showed that the additivity of these interactions renders them considerably more long-range.

This comparison is approximate. The actual relative strengths will vary depending on the molecules involved. Ionic bonding and covalent bonding will always be stronger than intermolecular forces in any given substance.

Intermolecular forces are repulsive at short distances and attractive at long distances (see the Lennard–Jones potential). In a gas, the repulsive force chiefly has the effect of keeping two molecules from occupying the same volume. This gives a real gas a tendency to occupy a larger volume than an ideal gas at the same temperature and pressure. The attractive force draws molecules closer together and gives a real gas a tendency to occupy a smaller volume than an ideal gas. Which interaction is more important depends on temperature and pressure (see compressibility factor).

In a gas, the distances between molecules are generally large, so intermolecular forces have only a small effect. The attractive force is not overcome by the repulsive force, but by the thermal energy of the molecules. Temperature is the measure of thermal energy, so increasing temperature reduces the influence of the attractive force. In contrast, the influence of the repulsive force is essentially unaffected by temperature.

When a gas is compressed to increase its density, the influence of the attractive force increases. If the gas is made sufficiently dense, the attractions can become large enough to overcome the tendency of thermal motion to cause the molecules to disperse. Then the gas can condense to form a solid or liquid, i.e., a condensed phase. Lower temperature favors the formation of a condensed phase. In a condensed phase, there is very nearly a balance between the attractive and repulsive forces.

Intermolecular forces observed between atoms and molecules can be described phenomenologically as occurring between permanent and instantaneous dipoles, as outlined above. Alternatively, one may seek a fundamental, unifying theory that is able to explain the various types of interactions such as hydrogen bonding, van der Waals forces and dipole-dipole interactions. Typically, this is done by applying the ideas of quantum mechanics to molecules, and Rayleigh–Schrödinger perturbation theory has been especially effective in this regard. When applied to existing quantum chemistry methods, such a quantum mechanical explanation of intermolecular interactions, this provides an array of approximate methods that can be used to analyze intermolecular interactions. One of the most helpful methods to visualize this kind of intermolecular interactions, that we can found in quantum chemistry, is the non-covalent interaction index, which is based on the electron density of the system.


</doc>
<doc id="15420" url="https://en.wikipedia.org/wiki?curid=15420" title="IRQ">
IRQ

IRQ may refer to:



</doc>
<doc id="15422" url="https://en.wikipedia.org/wiki?curid=15422" title="List of Internet top-level domains">
List of Internet top-level domains

This list of Internet top-level domain (TLD) extensions contains top-level domains, which are those domains in the DNS root zone of the Domain Name System of the Internet. The official list of all top-level domains is maintained by the Internet Assigned Numbers Authority (IANA) at the Root Zone Database. IANA also oversees the approval process for new proposed top-level domains. , the root domain contains 1543 top-level domains, while a few have been retired and are no longer functional.

IANA distinguishes the following groups of top-level domains:


Seven generic top-level domains were created early in the development of the Internet, and predate the creation of ICANN in 1998.
Infrastructure top-level domain

As of May 20, 2017, there were 255 Country code top-level domains. (For comparison, at the same time the United Nations comprised 193 countries.)

Source:


All of these TLDs are internationalized domain names (IDN) and support second-level IDNs.



ICANN/IANA has created some Special-Use domain names which are meant for special technical purposes. ICANN/IANA owns all of the Special-Use domain names.




</doc>
<doc id="15428" url="https://en.wikipedia.org/wiki?curid=15428" title="Idealism">
Idealism

In philosophy, idealism is the group of metaphysical philosophies that assert that reality, or reality as humans can know it, is fundamentally mental, mentally constructed, or otherwise immaterial. Epistemologically, idealism manifests as a skepticism about the possibility of knowing any mind-independent thing. In contrast to materialism, idealism asserts the "primacy" of consciousness as the origin and prerequisite of material phenomena. According to this view consciousness exists "before" and is the pre-condition of material existence. Consciousness creates and determines the material and not vice versa. Idealism believes consciousness and mind to be the origin of the material world and aims to explain the existing world according to these principles.

Idealism theories are mainly divided into two groups. Subjective idealism takes as its starting point the given fact of human consciousness seeing the existing world as a combination of sensation. Objective idealism posits the existence of an "objective" consciousness which exists before and, in some sense, independently of human ones. In a sociological sense, idealism emphasizes how human ideas—especially beliefs and values—shape society. As an ontological doctrine, idealism goes further, asserting that all entities are composed of mind or spirit. Idealism thus rejects physicalist and dualist theories that fail to ascribe priority to the mind.

The earliest extant arguments that the world of experience is grounded in the mental derive from India and Greece. The Hindu idealists in India and the Greek Neoplatonists gave panentheistic arguments for an all-pervading consciousness as the ground or true nature of reality. In contrast, the Yogācāra school, which arose within Mahayana Buddhism in India in the 4th century CE, based its "mind-only" idealism to a greater extent on phenomenological analyses of personal experience. This turn toward the subjective anticipated empiricists such as George Berkeley, who revived idealism in 18th-century Europe by employing skeptical arguments against materialism. Beginning with Immanuel Kant, German idealists such as Georg Wilhelm Friedrich Hegel, Johann Gottlieb Fichte, Friedrich Wilhelm Joseph Schelling, and Arthur Schopenhauer dominated 19th-century philosophy. This tradition, which emphasized the mental or "ideal" character of all phenomena, gave birth to idealistic and subjectivist schools ranging from British idealism to phenomenalism to existentialism.

Idealism as a philosophy came under heavy attack in the West at the turn of the 20th century. The most influential critics of both epistemological and ontological idealism were G. E. Moore and Bertrand Russell, but its critics also included the New Realists. According to "Stanford Encyclopedia of Philosophy", the attacks by Moore and Russell were "... so influential that even more than 100 years later, any acknowledgment of idealistic tendencies is viewed in the English-speaking world with reservation, ...". Many aspects and paradigms of idealism did, however, still have a large influence on subsequent philosophy.

"Idealism" is a term with several related meanings. It comes via "idea" from the Greek "idein" (ἰδεῖν), meaning "to see". The term entered the English language by 1743. In ordinary use, as when speaking of Woodrow Wilson's political idealism, it generally suggests the priority of ideals, principles, values, and goals over concrete realities. Idealists are understood to represent the world as it might or should be, unlike pragmatists, who focus on the world as it presently is. In the arts, similarly, idealism affirms imagination and attempts to realize a mental conception of beauty, a standard of perfection, juxtaposed to aesthetic naturalism and realism.

Any philosophy that assigns crucial importance to the ideal or spiritual realm in its account of human existence may be termed "idealist". Metaphysical idealism is an ontological doctrine that holds that reality itself is incorporeal or experiential at its core. Beyond this, idealists disagree on which aspects of the mental are more basic. Platonic idealism affirms that abstractions are more basic to reality than the things we perceive, while subjective idealists and phenomenalists tend to privilege sensory experience over abstract reasoning. Epistemological idealism is the view that reality can only be known through ideas, that only psychological experience can be apprehended by the mind.

Subjective idealists like George Berkeley are anti-realists in terms of a mind-independent world, whereas transcendental idealists like Immanuel Kant are strong skeptics of such a world, affirming epistemological and not metaphysical idealism. Thus Kant defines "idealism" as "the assertion that we can never be certain whether all of our putative outer experience is not mere imagining". He claimed that, according to "idealism", "the reality of external objects does not admit of strict proof. On the contrary, however, the reality of the object of our internal sense (of myself and state) is clear immediately through consciousness." However, not all idealists restrict the real or the knowable to our immediate subjective experience. Objective idealists make claims about a transempirical world, but simply deny that this world is essentially divorced from or ontologically prior to the mental. Thus Plato and Gottfried Leibniz affirm an objective and knowable reality transcending our subjective awareness—a rejection of epistemological idealism—but propose that this reality is grounded in ideal entities, a form of metaphysical idealism. Nor do all metaphysical idealists agree on the nature of the ideal; for Plato, the fundamental entities were non-mental abstract forms, while for Leibniz they were proto-mental and concrete monads.

As a rule, transcendental idealists like Kant affirm idealism's epistemic side without committing themselves to whether reality is "ultimately" mental; objective idealists like Plato affirm reality's metaphysical basis in the mental or abstract without restricting their epistemology to ordinary experience; and subjective idealists like Berkeley affirm both metaphysical and epistemological idealism.

Monistic idealism holds that consciousness, not matter, is the ground of all being. It is monist because it holds that there is only one type of thing in the universe and idealist because it holds that one thing to be consciousness.

Anaxagoras (480 BC) taught that "all things" were created by ""Nous"" ("Mind"). He held that Mind held the cosmos together and gave human beings a connection to the cosmos or a pathway to the divine.

Christian theologians have held idealist views, often based on Neoplatonism, despite the influence of Aristotelian scholasticism from the 12th century onward. Later western theistic idealism such as that of Hermann Lotze offers a theory of the "world ground" in which all things find their unity: it has been widely accepted by Protestant theologians. Several modern religious movements, for example the organizations within the New Thought Movement and the Unity Church, may be said to have a particularly idealist orientation. The theology of Christian Science includes a form of idealism: it teaches that all that truly exists is God and God's ideas; that the world as it appears to the senses is a distortion of the underlying spiritual reality, a distortion that may be corrected (both conceptually and in terms of human experience) through a reorientation (spiritualization) of thought.

Wang Yangming, a Ming Chinese neo-Confucian philosopher, official, educationist, calligraphist and general, held that objects do not exist entirely apart from the mind because the mind shapes them. It is not the world that shapes the mind but the mind that gives reason to the world, so the mind alone is the source of all reason, having an inner light, an innate moral goodness and understanding of what is good.

Plato's theory of forms or "ideas" describes ideal forms (for example the platonic solids in geometry or abstracts like Goodness and Justice), as universals existing independently of any particular instance. Arne Grøn calls this doctrine "the classic example of a metaphysical idealism as a "transcendent" idealism", while Simone Klein calls Plato "the earliest representative of metaphysical objective idealism". Nevertheless, Plato holds that matter is real, though transitory and imperfect, and is perceived by our body and its senses and given existence by the eternal ideas that are perceived directly by our rational soul. Plato was therefore a metaphysical and epistemological dualist, an outlook that modern idealism has striven to avoid: Plato's thought cannot therefore be counted as idealist in the modern sense.

With the neoplatonist Plotinus, wrote Nathaniel Alfred Boll "there even appears, probably for the first time in Western philosophy, "idealism" that had long been current in the East even at that time, for it taught... that the soul has made the world by stepping from eternity into time...". Similarly, in regard to passages from the Enneads, "The only space or place of the world is the soul" and "Time must not be assumed to exist outside the soul". Ludwig Noiré wrote: "For the first time in Western philosophy we find idealism proper in Plotinus". However, Plotinus does not address whether we know external objects, unlike Schopenhauer and other modern philosophers.

There are currents of idealism throughout Indian philosophy, ancient and modern. Hindu idealism often takes the form of monism or non-dualism, espousing the view that a unitary consciousness is the essence or meaning of the phenomenal reality and plurality.

Buddhist idealism on the other hand is more epistemic and is not a metaphysical monism, which Buddhists consider eternalistic and hence not the middle way between extremes espoused by the Buddha.

The oldest reference to Idealism in Vedic texts is in Purusha Sukta of the Rig Veda. This sukta espouses panentheism by presenting cosmic being Purusha as both pervading all universe and yet being transcendent to it. Absolute idealism can be seen in Chāndogya Upaniṣad, where things of the objective world like the five elements and the subjective world such as will, hope, memory etc. are seen to be emanations from the Self.

Idealist notions have been propounded by the Vedanta schools of thought, which use the Vedas, especially the Upanishads as their key texts. Idealism was opposed by dualists Samkhya, the atomists Vaisheshika, the logicians Nyaya, the linguists Mimamsa and the materialists Cārvāka. There are various sub schools of Vedanta, like Advaita Vedanta (non-dual), Vishishtadvaita and Bhedabheda Vedanta (difference and non-difference).

The schools of Vedanta all attempt to explain the nature and relationship of Brahman (universal soul or Self) and Atman (individual self), which they see as the central topic of the Vedas. One of the earliest attempts at this was Bādarāyaņa’s Brahma Sutras, which is canonical for all Vedanta sub-schools. Advaita Vedanta is a major sub school of Vedanta which holds a non-dual Idealistic metaphysics. According to Advaita thinkers like Adi Shankara (788–820) and his contemporary Maṇḍana Miśra, Brahman, the single unitary consciousness or absolute awareness, appears as the diversity of the world because of "maya" or illusion, and hence perception of plurality is "mithya", error. The world and all beings or souls in it have no separate existence from Brahman, universal consciousness, and the seemingly independent soul ("jiva") is identical to Brahman. These doctrines are represented in verses such as "brahma satyam jagan mithya; jīvo brahmaiva na aparah" (Brahman is alone True, and this world of plurality is an error; the individual self is not different from Brahman). Other forms of Vedanta like the Vishishtadvaita of Ramanuja and the Bhedabheda of Bhāskara are not as radical in their non-dualism, accepting that there is a certain difference between individual souls and Brahman.

The Tantric tradition of Kashmir Shaivism has also been categorized by scholars as a form of Idealism. The key thinker of this tradition is the Kashmirian Abhinavagupta (975–1025 CE).

Modern Vedic Idealism was defended by the influential Indian philosopher Sarvepalli Radhakrishnan in his 1932 "An Idealist View of Life" and other works, which espouse Advaita Vedanta. The essence of Hindu Idealism is captured by such modern writers as Sri Nisargadatta Maharaj, Sri Aurobindo, P.R. Sarkar, and Sohail Inayatullah.

Buddhist views which can be said to be similar to Idealism appear in Mahayana Buddhist texts such as the Samdhinirmocana sutra, Laṅkāvatāra Sūtra, Dashabhumika sutra, etc. These were later expanded upon by Indian Buddhist philosophers like Vasubandhu, Asaṅga, Dharmakīrti, and Śāntarakṣita. Yogacara thought was also promoted in China, by Chinese philosophers and translators like Xuanzang.

There is a modern scholarly disagreement about whether Yogacara Buddhism can be said to be a form of idealism. As Saam Trivedi notes: "on one side of the debate, writers such as Jay Garfield, Jeffrey Hopkins, Paul Williams, and others maintain the idealism label, while on the other side, Stefan Anacker, Dan Lusthaus, Richard King, Thomas Kochumuttom, Alex Wayman, Janice Dean Willis, and others have argued that Yogacara is not idealist." The central point of issue is what Buddhist philosophers like Vasubandhu who used the term "Vijñapti-matra" (representation-only or cognition-only) and formulated arguments to refute external objects actually meant to say.

Vasubandhu’s works include a refutation of external objects or externality itself and argues that the true nature of reality is beyond subject-object distinctions. He views ordinary consciousness experience as deluded in its perceptions of an external world separate from itself and instead argues that all there is "Vijñapti" (representation or conceptualization). Hence Vasubandhu begins his "Vimsatika" with the verse: "All this is consciousness-only, because of the appearance of non-existent objects, just as someone with an optical disorder may see non-existent nets of hair."

Likewise, the Buddhist philosopher Dharmakirti's view of the apparent existence of external objects is summed up by him in the Pramānaṿārttika (‘Commentary on Logic and Epistemology’): "Cognition experiences itself, and nothing else whatsoever. Even the particular objects of perception, are by nature just consciousness itself."

While some writers like Jay Garfield hold that Vasubandhu is a metaphysical idealist, others see him as closer to an epistemic idealist like Kant who holds that our knowledge of the world is simply knowledge of our own concepts and perceptions of a transcendental world. Sean Butler upholding that Yogacara is a form of idealism, albeit its own unique type, notes the similarity of Kant's categories and Yogacara's "Vāsanās", both of which are simply phenomenal tools with which the mind interprets the noumenal realm. Unlike Kant however who holds that the noumenon or thing-in-itself is unknowable to us, Vasubandhu holds that ultimate reality is knowable, but only through non-conceptual yogic perception of a highly trained meditative mind.

Writers like Dan Lusthaus who hold that Yogacara is not a metaphysical idealism point out, for example, that Yogācāra thinkers did not focus on consciousness to assert it as ontologically real, but simply to analyze how our experiences and thus our suffering is created. As Lusthaus notes: "no Indian Yogācāra text ever claims that the world is created by mind. What they do claim is that we mistake our projected interpretations of the world for the world itself, i.e. we take our own mental constructions to be the world." Lusthaus notes that there are similarities to Western epistemic idealists like Kant and Husserl, enough so that Yogacara can be seen as a form of epistemological idealism. However he also notes key differences like the concepts of karma and nirvana. Saam Trivedi meanwhile notes the similarities between epistemic idealism and Yogacara, but adds that Yogacara Buddhism is in a sense its own theory.

Similarly, Thomas Kochumuttom sees Yogacara as "an explanation of experience, rather than a system of ontology" and Stefan Anacker sees Vasubandhu's philosophy as a form of psychology and as a mainly therapeutic enterprise.

Subjective Idealism (immaterialism or phenomenalism) describes a relationship between experience and the world in which objects are no more than collections or "bundles" of sense data in the perceiver. Proponents include Berkeley, Bishop of Cloyne, an Anglo-Irish philosopher who advanced a theory he called immaterialism, later referred to as "subjective idealism", contending that individuals can only know sensations and ideas of objects directly, not abstractions such as "matter", and that ideas also depend upon being perceived for their very existence - "esse est percipi"; "to be is to be perceived".

Arthur Collier published similar assertions though there seems to have been no influence between the two contemporary writers. The only knowable reality is the represented image of an external object. Matter as a cause of that image, is unthinkable and therefore nothing to us. An external world as absolute matter unrelated to an observer does not exist as far as we are concerned. The universe cannot exist as it appears if there is no perceiving mind. Collier was influenced by "An Essay Towards the Theory of the Ideal or Intelligible World" by "Cambridge Platonist" John Norris (1701).

Bertrand Russell's popular book "The Problems of Philosophy" highlights Berkeley's tautological premise for advancing idealism;

The Australian philosopher David Stove harshly criticized philosophical idealism, arguing that it rests on what he called "the worst argument in the world". Stove claims that Berkeley tried to derive a non-tautological conclusion from tautological reasoning. He argued that in Berkeley's case the fallacy is not obvious and this is because one premise is ambiguous between one meaning which is tautological and another which, Stove argues, is logically equivalent to the conclusion.

Alan Musgrave argues that conceptual idealists compound their mistakes with use/mention confusions;

and proliferation of hyphenated entities such as "thing-in-itself" (Immanuel Kant), "things-as-interacted-by-us" (Arthur Fine), "table-of-commonsense" and "table-of-physics" (Sir Arthur Eddington) which are "warning signs" for conceptual idealism according to Musgrave because they allegedly do not exist but only highlight the numerous ways in which people come to know the world. This argument does not take into account the issues pertaining to hermeneutics, especially at the backdrop of analytic philosophy. Musgrave criticized Richard Rorty and Postmodernist philosophy in general for confusion of use and mention.

A. A. Luce and John Foster are other subjectivists. Luce, in "Sense without Matter" (1954), attempts to bring Berkeley up to date by modernizing his vocabulary and putting the issues he faced in modern terms, and treats the Biblical account of matter and the psychology of perception and nature. Foster's "The Case for Idealism" argues that the physical world is the logical creation of natural, non-logical constraints on human sense-experience. Foster's latest defense of his views is in his book "A World for Us: The Case for Phenomenalistic Idealism".

Paul Brunton, a British philosopher, mystic, traveler, and guru, taught a type of idealism called "mentalism", similar to that of Bishop Berkeley, proposing a master world-image, projected or manifested by a world-mind, and an infinite number of individual minds participating. A tree does not cease to exist if nobody sees it because the world-mind is projecting the idea of the tree to all minds.

John Searle, criticizing some versions of idealism, summarizes two important arguments for subjective idealism. The first is based on our perception of reality:

therefore;

Whilst agreeing with (2) Searle argues that (1) is false and points out that (3) does not follow from (1) and (2). The second argument runs as follows;

Searle contends that "Conclusion 2" does not follow from the premises.

Epistemological idealism is a subjectivist position in epistemology that holds that what one knows about an object exists only in one's mind. Proponents include Brand Blanshard.

Transcendental idealism, founded by Immanuel Kant in the eighteenth century, maintains that the mind shapes the world we perceive into the form of space-and-time.

The 2nd edition (1787) contained a "Refutation of Idealism" to distinguish his transcendental idealism from Descartes's Sceptical Idealism and Berkeley's anti-realist strain of Subjective Idealism. The section "Paralogisms of Pure Reason" is an implicit critique of Descartes' idealism. Kant says that it is not possible to infer the 'I' as an object (Descartes' "cogito ergo sum") purely from "the spontaneity of thought". Kant focused on ideas drawn from British philosophers such as Locke, Berkeley and Hume but distinguished his transcendental or critical idealism from previous varieties;

Kant distinguished between things as they appear to an observer and things in themselves, "that is, things considered without regard to whether and how they may be given to us". We cannot approach the "noumenon", the "thing in Itself" () without our own mental world. He added that the mind is not a blank slate, "tabula rasa" but rather comes equipped with categories for organising our sense impressions.

In the first volume of his "Parerga and Paralipomena", Schopenhauer wrote his "Sketch of a History of the Doctrine of the Ideal and the Real". He defined the ideal as being mental pictures that constitute subjective knowledge. The ideal, for him, is what can be attributed to our own minds. The images in our head are what comprise the ideal. Schopenhauer emphasized that we are restricted to our own consciousness. The world that appears is only a representation or mental picture of objects. We directly and immediately know only representations. All objects that are external to the mind are known indirectly through the mediation of our mind. He offered a history of the concept of the "ideal" as "ideational" or "existing in the mind as an image".

Charles Bernard Renouvier was the first Frenchman after Nicolas Malebranche to formulate a complete idealistic system, and had a vast influence on the development of French thought. His system is based on Immanuel Kant's, as his chosen term "néo-criticisme" indicates; but it is a transformation rather than a continuation of Kantianism.

Friedrich Nietzsche argued that Kant commits an agnostic tautology and does not offer a satisfactory answer as to the "source" of a philosophical right to such-or-other metaphysical claims; he ridicules his pride in tackling "the most difficult thing that could ever be undertaken on behalf of metaphysics." The famous "thing-in-itself" was called a product of philosophical habit, which seeks to introduce a grammatical subject: because wherever there is cognition, there must be a "thing" that is cognized and allegedly it must be added to ontology as a being (whereas, to Nietzsche, only the world as ever changing appearances can be assumed). Yet he attacks the idealism of Schopenhauer and Descartes with an argument similar to Kant's critique of the latter "(see above)".

Objective idealism asserts that the reality of experiencing combines and transcends the realities of the object experienced and of the mind of the observer. Proponents include Thomas Hill Green, Josiah Royce, Benedetto Croce and Charles Sanders Peirce.

Schelling (1775–1854) claimed that the Fichte's "I" needs the Not-I, because there is no subject without object, and vice versa. So there is no difference between the subjective and the objective, that is, the ideal and the real. This is Schelling's "absolute identity": the ideas or mental images in the mind are identical to the extended objects which are external to the mind.

Absolute idealism is G. W. F. Hegel's account of how existence is comprehensible as an all-inclusive whole. Hegel called his philosophy "absolute" idealism in contrast to the "subjective idealism" of Berkeley and the "transcendental idealism" of Kant and Fichte, which were not based on a critique of the finite and a dialectical philosophy of history as Hegel's idealism was. The exercise of reason and intellect enables the philosopher to know ultimate historical reality, the phenomenological constitution of self-determination, the dialectical development of self-awareness and personality in the realm of History.

In his "Science of Logic" (1812–1814) Hegel argues that finite qualities are not fully "real" because they depend on other finite qualities to determine them. Qualitative "infinity", on the other hand, would be more self-determining and hence more fully real. Similarly finite natural things are less "real"—because they are less self-determining—than spiritual things like morally responsible people, ethical communities and God. So any doctrine, such as materialism, that asserts that finite qualities or natural objects are fully real is mistaken.

Hegel certainly intends to preserve what he takes to be true of German idealism, in particular Kant's insistence that ethical reason can and does go beyond finite inclinations. For Hegel there must be some identity of thought and being for the "subject" (any human observer) to be able to know any observed "object" (any external entity, possibly even another human) at all. Under Hegel's concept of "subject-object identity," subject and object both have Spirit (Hegel's ersatz, redefined, nonsupernatural "God") as their "conceptual" (not metaphysical) inner reality—and in that sense are identical. But until Spirit's "self-realization" occurs and Spirit graduates from Spirit to "Absolute" Spirit status, subject (a human mind) mistakenly thinks every "object" it observes is something "alien," meaning something separate or apart from "subject." In Hegel's words, "The object is revealed to it [to "subject"] by [as] something alien, and it does not recognize itself." Self-realization occurs when Hegel (part of Spirit's nonsupernatural Mind, which is the collective mind of all humans) arrives on the scene and realizes that every "object" is "himself", because both subject and object are essentially Spirit. When self-realization occurs and Spirit becomes "Absolute" Spirit, the "finite" (man, human) becomes the "infinite" ("God," divine), replacing the imaginary or "picture-thinking" supernatural God of theism: man becomes God. Tucker puts it this way: "Hegelianism . . . is a religion of self-worship whose fundamental theme is given in Hegel's image of the man who aspires to be God himself, who demands 'something more, namely infinity.'" The picture Hegel presents is "a picture of a self-glorifying humanity striving compulsively, and at the end successfully, to rise to divinity."

Kierkegaard criticised Hegel's idealist philosophy in several of his works, particularly his claim to a comprehensive system that could explain the whole of reality. Where Hegel argues that an ultimate understanding of the logical structure of the world is an understanding of the logical structure of God's mind, Kierkegaard asserts that for God reality can be a system but it cannot be so for any human individual because both reality and humans are incomplete and all philosophical systems imply completeness. A logical system is possible but an existential system is not. "What is rational is actual; and what is actual is rational". Hegel's absolute idealism blurs the distinction between existence and thought: our mortal nature places limits on our understanding of reality;

So-called systems have often been characterized and challenged in the assertion that they abrogate the distinction between good and evil, and destroy freedom. Perhaps one would express oneself quite as definitely, if one said that every such system fantastically dissipates the concept existence. ... Being an individual man is a thing that has been abolished, and every speculative philosopher confuses himself with humanity at large; whereby he becomes something infinitely great, and at the same time nothing at all.

A major concern of Hegel's "Phenomenology of Spirit" (1807) and of the philosophy of Spirit that he lays out in his "Encyclopedia of the Philosophical Sciences" (1817–1830) is the interrelation between individual humans, which he conceives in terms of "mutual recognition." However, what Climacus means by the aforementioned statement, is that Hegel, in the "Philosophy of Right", believed the best solution was to surrender one's individuality to the customs of the State, identifying right and wrong in view of the prevailing bourgeois morality. Individual human will ought, at the State's highest level of development, to properly coincide with the will of the State. Climacus rejects Hegel's suppression of individuality by pointing out it is impossible to create a valid set of rules or system in any society which can adequately describe existence for any one individual. Submitting one's will to the State denies personal freedom, choice, and responsibility.

In addition, Hegel does believe we can know the structure of God's mind, or ultimate reality. Hegel agrees with Kierkegaard that both reality and humans are incomplete, inasmuch as we are in time, and reality develops through time. But the relation between time and eternity is outside time and this is the "logical structure" that Hegel thinks we can know. Kierkegaard disputes this assertion, because it eliminates the clear distinction between ontology and epistemology. Existence and thought are not identical and one cannot possibly think existence. Thought is always a form of abstraction, and thus not only is pure existence impossible to think, but all forms in existence are unthinkable; thought depends on language, which merely abstracts from experience, thus separating us from lived experience and the living essence of all beings. In addition, because we are finite beings, we cannot possibly know or understand anything that is universal or infinite such as God, so we cannot know God exists, since that which transcends time simultaneously transcends human understanding.

Bradley saw reality as a monistic whole apprehended through "feeling", a state in which there is no distinction between the perception and the thing perceived. Like Berkeley, Bradley thought that nothing can be known to exist unless it is known by a mind.

Bradley was the apparent target of G. E. Moore's radical rejection of idealism. Moore claimed that Bradley did not understand the statement that something is real. We know for certain, through common sense and prephilosophical beliefs, that some things are real, whether they are objects of thought or not, according to Moore. The 1903 article "The Refutation of Idealism" is one of the first demonstrations of Moore's commitment to analysis. He examines each of the three terms in the Berkeleian aphorism "esse est percipi", "to be is to be perceived", finding that it must mean that the object and the subject are "necessarily" connected so that "yellow" and "the sensation of yellow" are identical - "to be yellow" is "to be experienced as yellow". But it also seems there is a difference between "yellow" and "the sensation of yellow" and "that "esse" is held to be "percipi", solely because what is experienced is held to be identical with the experience of it". Though far from a complete refutation, this was the first strong statement by analytic philosophy against its idealist predecessors, or at any rate against the type of idealism represented by Berkeley. This argument did not show that the GEM (in post–Stove vernacular, see below) is logically invalid.

Actual Idealism is a form of idealism developed by Giovanni Gentile that grew into a "grounded" idealism contrasting Kant and Hegel. The idea is a version of Occam's razor; the simpler explanations are always correct. Actual idealism is the idea that reality is the ongoing act of thinking, or in Italian "pensiero pensante". Any action done by humans is classified as human thought because the action was done due to predisposed thought. He further believes that thoughts are the only concept that truly exist since reality is defined through the act of thinking. This idea was derived from Gentile's paper, "The Theory of Mind As Pure Act".

Since thoughts are actions, any conjectured idea can be enacted. This idea not only affects the individual's life, but everyone around them, which in turn affects the state since the people are the state. Therefore, thoughts of each person are subsumed within the state. The state is a composition of many minds that come together to change the country for better or worse.

Gentile theorizes that thoughts can only be conjectured within the bounds of known reality; abstract thinking does not exist. Thoughts cannot be formed outside our known reality because we are the reality that halt ourselves from thinking externally. With accordance to "The Act of Thought of Pure Thought", our actions comprise our thoughts, our thoughts create perception, perceptions define reality, thus we think within our created reality.

The present act of thought is reality but the past is not reality; it is history. The reason being, past can be rewritten through present knowledge and perspective of the event. The reality that is currently constructed can be completely changed through language (e.g. bias (omission, source, tone)). The unreliability of the recorded realty can skew the original concept and make the past remark unreliable. 
Actual idealism is regarded as a liberal and tolerant doctrine since it acknowledges that every being picturizes reality, in which their ideas remained hatched, differently. Even though, reality is a figment of thought.

Even though core concept of the theory is famous for its simplification, its application is regarded as extremely ambiguous. Over the years, philosophers have interpreted it numerously different ways: Holmes took it as metaphysics of the thinking act; Betti as a form of hermeneutics; Harris as a metaphysics of democracy; Fogu as a modernist philosophy of history.

Giovanni Gentile was a key supporter of fascism, regarded by many as the "philosopher of fascism". Gentile's philosophy was the key to understating fascism as it was believed by many who supported and loved it. They believed, if priori synthesis of subject and object is true, there is no difference between the individuals in society; they're all one. Which means that they have equal right, roles, and jobs. In fascist state, submission is given to one leader because individuals act as one body. In Gentile's view, far more can be accomplished when individuals are under a corporate body than a collection of autonomous individual.

Pluralistic idealism such as that of Gottfried Leibniz takes the view that there are many individual minds that together underlie the existence of the observed world and make possible the existence of the physical universe. Unlike absolute idealism, pluralistic idealism does not assume the existence of a single ultimate mental reality or "Absolute". Leibniz' form of idealism, known as Panpsychism, views "monads" as the true atoms of the universe and as entities having perception. The monads are "substantial forms of being, "elemental, individual, subject to their own laws, non-interacting, each reflecting the entire universe. Monads are centers of force, which is substance while space, matter and motion are phenomenal and their form and existence is dependent on the simple and immaterial monads. There is a pre-established harmony by God, the central monad, between the world in the minds of the monads and the external world of objects. Leibniz's cosmology embraced traditional Christian Theism. The English psychologist and philosopher James Ward inspired by Leibniz had also defended a form of pluralistic idealism. According to Ward the universe is composed of "psychic monads" of different levels, interacting for mutual self-betterment.

Personalism is the view that the minds that underlie reality are the minds of persons. Borden Parker Bowne, a philosopher at Boston University, a founder and popularizer of personal idealism, presented it as a substantive reality of persons, the only reality, as known directly in self-consciousness. Reality is a society of interacting persons dependent on the Supreme Person of God. Other proponents include George Holmes Howison and J. M. E. McTaggart.

Howison's personal idealism was also called "California Personalism" by others to distinguish it from the "Boston Personalism" which was of Bowne. Howison maintained that both impersonal, monistic idealism and materialism run contrary to the experience of moral freedom. To deny freedom to pursue truth, beauty, and "benignant love" is to undermine every profound human venture, including science, morality, and philosophy. Personalistic idealists Borden Parker Bowne and Edgar S. Brightman and realistic personal theist Saint Thomas Aquinas address a core issue, namely that of dependence upon an infinite personal God.

Howison, in his book "The Limits of Evolution and Other Essays Illustrating the Metaphysical Theory of Personal Idealism", created a democratic notion of personal idealism that extended all the way to God, who was no more the ultimate monarch but the ultimate democrat in eternal relation to other eternal persons. J. M. E. McTaggart's idealist atheism and Thomas Davidson's Apeirionism resemble Howisons personal idealism.

J. M. E. McTaggart of Cambridge University argued that minds alone exist and only relate to each other through love. Space, time and material objects are unreal. In "The Unreality of Time" he argued that time is an illusion because it is impossible to produce a coherent account of a sequence of events. "The Nature of Existence" (1927) contained his arguments that space, time, and matter cannot possibly be real. In his "Studies in Hegelian Cosmology" (Cambridge, 1901, p196) he declared that metaphysics are not relevant to social and political action. McTaggart "thought that Hegel was wrong in supposing that metaphysics could show that the state is more than a means to the good of the individuals who compose it". For McTaggart "philosophy can give us very little, if any, guidance in action... Why should a Hegelian citizen be surprised that his belief as to the organic nature of the Absolute does not help him in deciding how to vote? Would a Hegelian engineer be reasonable in expecting that his belief that all matter is spirit should help him in planning a bridge?

Thomas Davidson taught a philosophy called "apeirotheism", a "form of pluralistic idealism...coupled with a stern ethical rigorism" which he defined as "a theory of Gods infinite in number." The theory was indebted to Aristotle's pluralism and his concepts of Soul, the rational, living aspect of a living substance which cannot exist apart from the body because it is not a substance but an essence, and "nous", rational thought, reflection and understanding. Although a perennial source of controversy, Aristotle arguably views the latter as both eternal and immaterial in nature, as exemplified in his theology of unmoved movers. Identifying Aristotle's God with rational thought, Davidson argued, contrary to Aristotle, that just as the soul cannot exist apart from the body, God cannot exist apart from the world.

Idealist notions took a strong hold among physicists of the early 20th century confronted with the paradoxes of quantum physics and the theory of relativity. In "The Grammar of Science", Preface to the 2nd Edition, 1900, Karl Pearson wrote, "There are many signs that a sound idealism is surely replacing, as a basis for natural philosophy, the crude materialism of the older physicists." This book influenced Einstein's regard for the importance of the observer in scientific measurements. In § 5 of that book, Pearson asserted that "...science is in reality a classification and analysis of the contents of the mind..." Also, "...the field of science is much more consciousness than an external world."

Sir Arthur Eddington, a British astrophysicist of the early 20th century, wrote in his book "The Nature of the Physical World"; "The stuff of the world is mind-stuff";

"The mind-stuff of the world is, of course, something more general than our individual conscious minds... The mind-stuff is not spread in space and time; these are part of the cyclic scheme ultimately derived out of it... It is necessary to keep reminding ourselves that all knowledge of our environment from which the world of physics is constructed, has entered in the form of messages transmitted along the nerves to the seat of consciousness... Consciousness is not sharply defined, but fades into subconsciousness; and beyond that we must postulate something indefinite but yet continuous with our mental nature... It is difficult for the matter-of-fact physicist to accept the view that the substratum of everything is of mental character. But no one can deny that mind is the first and most direct thing in our experience, and all else is remote inference."

Ian Barbour in his book "Issues in Science and Religion" (1966), p. 133, cites Arthur Eddington's "The Nature of the Physical World" (1928) for a text that argues The Heisenberg Uncertainty Principles provides a scientific basis for "the defense of the idea of human freedom" and his "Science and the Unseen World" (1929) for support of philosophical idealism "the thesis that reality is basically mental".

Sir James Jeans wrote; "The stream of knowledge is heading towards a non-mechanical reality; the Universe begins to look more like a great thought than like a great machine. Mind no longer appears to be an accidental intruder into the realm of matter... we ought rather hail it as the creator and governor of the realm of matter."

Jeans, in an interview published in The Observer (London), when asked the question: "Do you believe that life on this planet is the result of some sort of accident, or do you believe that it is a part of some great scheme?" replied:

"I incline to the idealistic theory that consciousness is fundamental, and that the material universe is derivative from consciousness, not consciousness from the material universe... In general the universe seems to me to be nearer to a great thought than to a great machine. It may well be, it seems to me, that each individual consciousness ought to be compared to a brain-cell in a universal mind.

Addressing the British Association in 1934, Jeans said: "What remains is in any case very different from the full-blooded matter and the forbidding materialism of the Victorian scientist. His objective and material universe is proved to consist of little more than constructs of our own minds. To this extent, then, modern physics has moved in the direction of philosophic idealism. Mind and matter, if not proved to be of similar nature, are at least found to be ingredients of one single system. There is no longer room for the kind of dualism which has haunted philosophy since the days of Descartes." 

In "The Universe Around Us", Jeans writes: "Finite picture whose dimensions are a certain amount of space and a certain amount of time; the protons and electrons are the streaks of paint which define the picture against its space-time background. Traveling as far back in time as we can, brings us not to the creation of the picture, but to its edge; the creation of the picture lies as much outside the picture as the artist is outside his canvas. On this view, discussing the creation of the universe in terms of time and space is like trying to discover the artist and the action of painting, by going to the edge of the canvas. This brings us very near to those philosophical systems which regard the universe as a thought in the mind of its Creator, thereby reducing all discussion of material creation to futility." 

The chemist Ernest Lester Smith wrote a book "Intelligence Came First" (1975) in which he claimed that consciousness is a fact of nature and that the cosmos is grounded in and pervaded by mind and intelligence.

Bernard d'Espagnat, a French theoretical physicist best known for his work on the nature of reality, wrote a paper titled "The Quantum Theory and Reality." According to the paper: "The doctrine that the world is made up of objects whose existence is independent of human consciousness turns out to be in conflict with quantum mechanics and with facts established by experiment." 

In a Guardian newspaper article entitled 'Quantum Weirdness: What We Call 'Reality' is Just a State of Mind', d'Espagnat wrote: "What quantum mechanics tells us, I believe, is surprising to say the least. It tells us that the basic components of objects – the particles, electrons, quarks etc. – cannot be thought of as 'self-existent'."

He further writes that his research in quantum physics has led him to conclude that an "ultimate reality" exists, which is not embedded in space or time.



Further reading



</doc>
<doc id="15430" url="https://en.wikipedia.org/wiki?curid=15430" title="Inheritance">
Inheritance

Inheritance is the practice of passing on property, titles, debts, rights, and obligations upon the death of an individual. The rules of inheritance differ between societies and have changed over time.

In law, an "heir" is a person who is entitled to receive a share of the deceased's (the person who died) property, subject to the rules of inheritance in the jurisdiction of which the deceased was a citizen or where the deceased (decedent) died or owned property at the time of death.

The inheritance may be either under the terms of a will or by intestate laws if the deceased had no will. However, the will must comply with the laws of the jurisdiction at the time it was created or it will be declared invalid (for example, some states do not recognize holographic wills as valid, or only in specific circumstances) and the intestate laws then apply.

A person does not become an heir before the death of the deceased, since the exact identity of the persons entitled to inherit is determined only then. Members of ruling noble or royal houses who are expected to become heirs are called heirs apparent if first in line and incapable of being displaced from inheriting by another claim; otherwise, they are heirs presumptive. There is a further concept of joint inheritance, pending renunciation by all but one, which is called coparceny.

In modern law, the terms "inheritance" and "heir" refer exclusively to succession to property by descent from a deceased dying intestate. Takers in property succeeded to under a will are termed generally "beneficiaries," and specifically "devisees" for real property, "bequestees" for personal property, or "legatees".

Except in some jurisdictions where a person cannot be legally disinherited (such as the United States state of Louisiana, which allows disinheritance only under specifically enumerated circumstances), a person who would be an heir under intestate laws may be disinherited completely under the terms of a will (an example is that of the will of comedian Jerry Lewis; his will specifically disinherited his six children by his first wife, and their descendants, leaving his entire estate to his second wife).

Detailed anthropological and sociological studies have been made about customs of patrilineal inheritance, where only male children can inherit. Some cultures also employ matrilineal succession, where property can only pass along the female line, most commonly going to the sister's sons of the decedent; but also, in some societies, from the mother to her daughters. Some ancient societies and most modern states employ egalitarian inheritance, without discrimination based on gender and/or birth order.

The inheritance is patrilineal. The father —that is, the owner of the land— bequeaths only to his male descendants, so the Promised Land passes from one Jewish father to his sons.

If there were no living sons and no descendants of any previously living sons, daughters inherit. In , the daughters of Zelophehad (Mahlah, Noa, Hoglah, Milcah, and Tirzah) of the tribe of Manasseh come to Moses and ask for their father's inheritance, as they have no brothers. The order of inheritance is set out in : a man's sons inherit first, daughters if no sons, brothers if he has no children, and so on.

Later, in , some of the heads of the families of the tribe of Manasseh come to Moses and point out that, if a daughter inherits and then marries a man not from her paternal tribe, her land will pass from her birth-tribe's inheritance into her marriage-tribe's. So a further rule is laid down: if a daughter inherits land, she must marry someone within her father's tribe. (The daughters of Zelophehad marry the sons' of their father's brothers. There is "no" indication that this was not their choice.)

The tractate Baba Bathra, written during late Antiquity in Babylon, deals extensively with issues of property ownership and inheritance according to Jewish Law. Other works of Rabbinical Law, such as the Hilkhot naḥalot : mi-sefer Mishneh Torah leha-Rambam, and the Sefer ha-yerushot: ʻim yeter ha-mikhtavim be-divre ha-halakhah be-ʻAravit uve-ʻIvrit uve-Aramit also deal with inheritance issues. The first, often abbreviated to Mishneh Torah, was written by Maimonides and was very important in Jewish tradition.

All these sources agree that the firstborn son is entitled to a double portion of his father's estate: . This means that, for example, if a father left five sons, the firstborn receives a third of the estate and each of the other four receives a sixth. If he left nine sons, the firstborn receives a fifth and each of the other eight receive a tenth. If the eldest surviving son is not the firstborn son, he is not entitled to the double portion.

Philo of Alexandria and Josephus also comment on the Jewish laws of inheritance, praising them above other law codes of their time. They also agreed that the firstborn son must receive a double portion of his father's estate.

The New Testament does not specifically mention anything about inheritance rights: the only story even mentioning inheritance is that of the Prodigal Son, but that involved the father voluntarily passing his estate to his two sons prior to his death; the younger son receiving his inheritance (1/3; the older son would have received 2/3 under then existing Jewish law) and squandering it.

The topic is generally not discussed among doctrinal statements of various denominations or sects, leaving that to be a matter of secular concern.

The Quran introduced a number of different rights and restrictions on matters of inheritance, including general improvements to the treatment of women and family life compared to the pre-Islamic societies that existed in the Arabian Peninsula at the time. Furthermore, the Quran introduced additional heirs that were not entitled to inheritance in pre-Islamic times, mentioning nine relatives specifically of which six were female and three were male. However, the inheritance rights of women remained inferior to those of men. According to the Quran, for example, a son is entitled to twice as much inheritance as a daughter. The Quran also presented efforts to fix the laws of inheritance, and thus forming a complete legal system. This development was in contrast to pre-Islamic societies where rules of inheritance varied considerably. In addition to the above changes, the Quran imposed restrictions on testamentary powers of a Muslim in disposing his or her property. In their will, a Muslim can only give out a maximum of one third of their property.

The Quran contains only three verses that give specific details of inheritance and shares, in addition to few other verses dealing with testamentary. But this information was used as a starting point by Muslim jurists who expounded the laws of inheritance even further using Hadith, as well as methods of juristic reasoning like Qiyas. Nowadays, inheritance is considered an integral part of Sharia law and its application for Muslims is mandatory, though many peoples (see Historical inheritance systems), despite being Muslim, have other inheritance customs.

The distribution of the inherited wealth has varied greatly among different cultures and legal traditions. In nations using civil law, for example, the right of children to inherit wealth from parents in pre-defined ratios is enshrined in law, as far back as the Code of Hammurabi (ca. 1750 BC). In the US State of Louisiana, the only US state to use Napoleonic Code for state law, this system is known as "forced heirship" which prohibits disinheritance of adult children except for a few narrowly-defined reasons that a parent is obligated to prove. Other legal traditions, particularly in nations using common law, allow inheritances to be divided however one wishes, or to disinherit any child for any reason.

In cases of unequal inheritance, the majority might receive little while only a small number inherit a larger amount, with the lesser amount given to the daughter in the family. The amount of inheritance is often far less than the value of a business initially given to the son, especially when a son takes over a thriving multimillion-dollar business, yet the daughter is given the balance of the actual inheritance amounting to far less than the value of business that was initially given to the son. This is especially seen in old world cultures, but continues in many families to this day.

Arguments for eliminating the disparagement of inheritance inequality include the right to property and the merit of individual allocation of capital over government wealth confiscation and redistribution, but this does not resolve what some describe as the problem of unequal inheritance. In terms of inheritance inequality, some economists and sociologists focus on the inter generational transmission of income or wealth which is said to have a direct impact on one's mobility (or immobility) and class position in society. Nations differ on the political structure and policy options that govern the transfer of wealth.

According to the American federal government statistics compiled by Mark Zandi in 1985, the average US inheritance was $39,000. In subsequent years, the overall amount of total annual inheritance more than doubled, reaching nearly $200 billion. By 2050, there will be an estimated $25 trillion inheritance transmitted across generations.

Some researchers have attributed this rise to the baby boomer generation. Historically, the baby boomers were the largest influx of children conceived after WW2. For this reason, Thomas Shapiro suggests that this generation "is in the midst of benefiting from the greatest inheritance of wealth in history." Inherited wealth may help explain why many Americans who have become rich may have had a "substantial head start". In September 2012, according to the Institute for Policy Studies, "over 60 percent" of the Forbes richest 400 Americans "grew up in substantial privilege", and often (but not always) received substantial inheritances. The French economist Thomas Piketty studied this phenomenon in his best-selling book "Capital in the Twenty-First Century", published in 2013.

Other research has shown that many inheritances, large or small, are rapidly squandered. Similarly, analysis shows that over two-thirds of high-wealth families lose their wealth within two generations, and almost 80% of high-wealth parents "feel the next generation is not financially responsible enough to handle inheritance."

It has been argued that inheritance plays a significant effect on social stratification. Inheritance is an integral component of family, economic, and legal institutions, and a basic mechanism of class stratification. It also affects the distribution of wealth at the societal level. The total cumulative effect of inheritance on stratification outcomes takes three forms, according to scholars who have examined the subject.

The first form of inheritance is the inheritance of cultural capital (i.e. linguistic styles, higher status social circles, and aesthetic preferences). The second form of inheritance is through familial interventions in the form of "inter vivos" transfers (i.e. gifts between the living), especially at crucial junctures in the life courses. Examples include during a child's milestone stages, such as going to college, getting married, getting a job, and purchasing a home. The third form of inheritance is the transfers of bulk estates at the time of death of the testators, thus resulting in significant economic advantage accruing to children during their adult years. The origin of the stability of inequalities is material (personal possessions one is able to obtain) and is also cultural, rooted either in varying child-rearing practices that are geared to socialization according to social class and economic position. Child-rearing practices among those who inherit wealth may center around favoring some groups at the expense of others at the bottom of the social hierarchy.

It is further argued that the degree to which economic status and inheritance is transmitted across generations determines one's life chances in society. Although many have linked one's social origins and educational attainment to life chances and opportunities, education cannot serve as the most influential predictor of economic mobility. In fact, children of well-off parents generally receive better schooling and benefit from material, cultural, and genetic inheritances. Likewise, schooling attainment is often persistent across generations and families with higher amounts of inheritance are able to acquire and transmit higher amounts of human capital. Lower amounts of human capital and inheritance can perpetuate inequality in the housing market and higher education. Research reveals that inheritance plays an important role in the accumulation of housing wealth. Those who receive an inheritance are more likely to own a home than those who do not regardless of the size of the inheritance.

Often, racial or religious minorities and individuals from socially disadvantaged backgrounds receive less inheritance and wealth. As a result, mixed races might be excluded in inheritance privilege and are more likely to rent homes or live in poorer neighborhoods, as well as achieve lower educational attainment compared with whites in America. Individuals with a substantial amount of wealth and inheritance often intermarry with others of the same social class to protect their wealth and ensure the continuous transmission of inheritance across generations; thus perpetuating a cycle of privilege.

Nations with the highest income and wealth inequalities often have the highest rates of homicide and disease (such as obesity, diabetes, and hypertension). A "The New York Times" article reveals that the U.S. is the world's wealthiest nation, but "ranks twenty-ninth in life expectancy, right behind Jordan and Bosnia." This has been regarded as highly attributed to the significant gap of inheritance inequality in the country, although there are clearly other factors such as the affordability of healthcare.

When social and economic inequalities centered on inheritance are perpetuated by major social institutions such as family, education, religion, etc., these differing life opportunities are argued to be transmitted from each generation. As a result, this inequality is believed to become part of the overall social structure.

Dynastic wealth is monetary inheritance that is passed on to generations that didn't earn it. Dynastic wealth is linked to the term Plutocracy. Much has been written about the rise and influence of dynastic wealth including the bestselling book Capital in the Twenty-First Century by the French economist Thomas Piketty.Bill Gates uses the term in his article "Why Inequality Matters".

Many states have inheritance taxes or death duties, under which a portion of any estate goes to the government.




</doc>
<doc id="15435" url="https://en.wikipedia.org/wiki?curid=15435" title="Ignatius of Antioch">
Ignatius of Antioch

Ignatius of Antioch (; Greek: Ἰγνάτιος Ἀντιοχείας, "Ignátios Antiokheías"; c. 35  – c. 107), also known as Ignatius Theophorus (, "Ignátios ho Theophóros", "the God-bearing") or Ignatius Nurono ( "The fire-bearer"), was an early Christian writer and bishop of Antioch (As the successor of Saint Peter). En route to Rome, where he met his martyrdom, Ignatius wrote a series of letters. This correspondence now forms a central part of the later collection known as the Apostolic Fathers. His letters also serve as an example of early Christian theology. Important topics they address include ecclesiology, the sacraments, and the role of bishops.

Nothing is known of Ignatius' life apart from what may be inferred internally from his letters, except from late spurious traditions. It is said Ignatius converted to Christianity at a young age. Tradition identifies Ignatius, along with his friend Polycarp, as disciples of John the Apostle. Later in his life, Ignatius was chosen to serve as Bishop of Antioch; the fourth-century Church historian Eusebius writes that Ignatius succeeded Evodius. Theodoret of Cyrrhus claimed that St. Peter himself left directions that Ignatius be appointed to the episcopal see of Antioch. Ignatius called himself "Theophorus" (God Bearer). A tradition arose that he was one of the children whom Jesus took in his arms and blessed.

Ignatius' own writings mention his arrest by the authorities and travel to Rome to face trial:

Ignatius' transfer to Rome is regarded by scholars as unusual, since those persecuted as Christians would be expected to be punished locally. If he were a Roman citizen, he could have appealed to the emperor, but then would usually have been beheaded rather than tortured. Allen Brent has suggested that Ignatius was involved in conflict with other Christians and was executed for the capital crime of disturbing the peace.

During the journey to Rome, Ignatius and his entourage of soldiers made a number of stops in Asia Minor. Along the route Ignatius wrote six letters to the churches in the region and one to a fellow bishop, Polycarp, bishop of Smyrna. In his "Chronicle", Eusebius gives the date of Ignatius's death as AA 2124 (2124 years after Abraham), i.e. the 11th year of Trajan's reign, AD 108. Ignatius himself wrote that he would be thrown to the beasts, and in the fourth century Eusebius reports tradition that this came to pass, which is then repeated by Jerome, who is the first to explicitly mention "Lions". John Chrysostom is the first to allude to the Colosseum as the place of Ignatius' martyrdom. Contemporary scholars are not clear that any of these authors had sources other than Ignatius own writings.

After Ignatius' martyrdom in the Circus Maximus his remains were carried back to Antioch by his companions. The reputed remains of Ignatius were moved by the Emperor Theodosius II to the Tychaeum, or Temple of Tyche, which had been converted into a church dedicated to Ignatius. In 637 the relics were transferred to the Basilica di San Clemente in Rome.

Ignatius' feast day was kept in his own Antioch on 17 October, the day on which he is now celebrated in the Catholic Church and generally in western Christianity, although from the 12th century until 1969 it was put at 1 February in the General Roman Calendar.

In the Eastern Orthodox Church it is observed on 20 December. The Synaxarium of the Coptic Orthodox Church of Alexandria places it on the 24th of the Coptic Month of Koiak (which is also the 24 day of the fourth month of Tahisas in the Synaxarium of The Ethiopian Orthodox Tewahedo Church), corresponding in three years out of every four to 20 December in the Julian Calendar, which currently falls on 2 January of the Gregorian Calendar.

The following seven letters preserved under the name of Ignatius are generally considered authentic as they were mentioned by the historian Eusebius in the first half of the fourth century.

Seven authentic letters:

Writing in 1886, Presbyterian minister and church historian William Dool Killen asserted none of the Ignatian epistles were authentic. Instead, he argued that Callixtus, bishop of Rome, pseudepigraphically wrote the letters around AD 220 to garner support for a monarchical episcopate, modeling the renowned Saint Ignatius after his own life to give precedent for his own authority. Killen contrasted this episcopal polity with the presbyterian polity in the writings of Polycarp.

Most scholars, however, accept at least the two Ignatian epistles which were referenced by Origen, and believe that by the 5th century, this collection had been enlarged by spurious letters. The original text of six of the seven authentic letters are found in the Codex Mediceo Laurentianus written in Greek in the 11th century (which also contains the pseudepigraphical letters of the Long Recension, except that to the Philippians), while the letter to the Romans is found in the Codex Colbertinus. Some of the original letters were, at one point, believed to had been changed with interpolations. The oldest is known as the "Long Recension" which dates from the latter part of the fourth century. These were created to posthumously enlist Ignatius as an unwitting witness in theological disputes of that age, but that position was vigorously combated by several British and German critics, including the Catholics Denzinger and Hefele, who defended the genuineness of the entire seven epistles. At the same time, the purported eye-witness account of his martyrdom is also thought to be a forgery from around the same time. A detailed but spurious account of Ignatius' arrest and his travails and martyrdom is the material of the "Martyrium Ignatii" which is presented as being an eyewitness account for the church of Antioch, and attributed to Ignatius' companions, Philo of Cilicia, deacon at Tarsus, and Rheus Agathopus, a Syrian.

Although James Ussher regarded it as genuine, if there is any genuine nucleus of the "Martyrium", it has been so greatly expanded with interpolations that no part of it is without questions. Its most reliable manuscript is the 10th-century "Codex Colbertinus" (Paris), in which the "Martyrium" closes the collection. The "Martyrium" presents the confrontation of the bishop Ignatius with Trajan at Antioch, a familiar trope of "Acta" of the martyrs, and many details of the long, partly overland voyage to Rome. The Synaxarium of the Coptic Orthodox Church of Alexandria says that he was thrown to the wild beasts that devoured him and rent him to pieces.

Ignatius's letters proved to be important testimony to the development of Christian theology, since the number of extant writings from this period of Church history is very small. They bear signs of being written in great haste and without a proper plan, such as run-on sentences and an unsystematic succession of thought.

Ignatius modeled his writings after Paul, Peter, and John, and even quoted or paraphrased their own works freely, such as when he quoted 1 Corinthians 1:18, in his letter to the Ephesians: "Let my spirit be counted as nothing for the sake of the cross, which is a stumbling-block to those that do not believe, but to us salvation and life eternal." – "Letter to the Ephesians" 18, Roberts and Donaldson translation

Ignatius is known to have taught the deity of Christ: 
Also in the interpolated text of the 4th Century Long Recension:

He stressed the value of the Eucharist, calling it a "medicine of immortality" ("Ignatius to the Ephesians" 20:2). The very strong desire for bloody martyrdom in the arena, which Ignatius expresses rather graphically in places, may seem quite odd to the modern reader. An examination of his theology of soteriology shows that he regarded salvation as one being free from the powerful fear of death and thus to bravely face martyrdom.

Ignatius is claimed to be the first known Christian writer to argue in favor of Christianity's replacement of the Sabbath with the Lord's Day:

Ignatius is the earliest known Christian writer to emphasize loyalty to a single bishop in each city (or diocese) who is assisted by both presbyters (elders) and deacons. Earlier writings only mention "either" bishops "or" presbyters.

For instance, his writings on bishops, presbyters and deacons:

He is also responsible for the first known use of the Greek word "katholikos" (καθολικός), meaning "universal", "complete" and "whole" to describe the church, writing:

It is from the word "katholikos" ("according to the whole") that the word "catholic" comes. When Ignatius wrote the Letter to the Smyrnaeans in about the year 107 and used the word "catholic", he used it as if it were a word already in use to describe the Church. This has led many scholars to conclude that the appellation "Catholic Church" with its ecclesial connotation may have been in use as early as the last quarter of the First century. On the Eucharist, he wrote in his letter to the Smyrnaeans:

In his letter addressed to the Christians of Rome, he entreats to do nothing to prevent his martyrdom.

Epistles attributed to Saint Ignatius but of spurious origin (their author is often called Pseudo-Ignatius in English) include:





</doc>
<doc id="15437" url="https://en.wikipedia.org/wiki?curid=15437" title="ITU prefix">
ITU prefix

The International Telecommunication Union (ITU) allocates call sign prefixes for radio and television stations of all types. They also form the basis for, but do not exactly match, aircraft registration identifiers. These prefixes are agreed upon internationally, and are a form of country code. A call sign can be any number of letters and numerals but each country must only use call signs that begin with the characters allocated for use in that country.

A few countries do not fully comply with these rules. Australian broadcast stations officially have—but do not use—the VL prefix, and Canada uses Chile's CB for its own Canadian Broadcasting Corporation stations. This is through a special agreement with the government of Chile, which is officially assigned the CB prefix.

With regard to the second and/or third letters in the prefixes in the list below, if the country in question is allocated all callsigns with A to Z in that position, then that country can also use call signs with the digits 0 to 9 in that position. For example, the United States is assigned KA–KZ, and therefore can also use prefixes like KW0 or K1.

Many large countries in turn have internal rules on how and where specific subsets of their callsigns can be used (such as Mexico's XE for AM and XH for FM radio and television broadcasting), which are not covered here.

Unallocated: The following call sign prefixes are available for future allocation by the ITU. ("x" represents any letter; "n" represents any digit from 2–9.)


Unavailable: Under present ITU guidelines the following call sign prefixes shall not be allocated . They are sometimes used unofficially - such as amateur radio operators operating in a disputed territory or in a nation state that has no official prefix (e.g. S0 in Western Sahara or station 1A0 at Knights of Malta headquarters in Rome or station 1L in Liberland ). ("x" represents any letter; "n" represents any digit from 2–9.)





</doc>
<doc id="15440" url="https://en.wikipedia.org/wiki?curid=15440" title="IBM PC keyboard">
IBM PC keyboard

The keyboard for IBM PC-compatible computers is standardized. However, during the more than 30 years of PC architecture being constantly updated, multiple types of keyboard layout variations have been developed.

A well-known class of IBM PC keyboards is the Model M. Introduced in 1986 and manufactured by IBM, Lexmark, Maxi-Switch and Unicomp, the vast majority of Model M keyboards feature a buckling spring key design and many have fully swappable keycaps.

The PC keyboard changed over the years, often at the launch of new IBM PC versions.

Common additions to the standard layouts include additional power management keys, volume controls, media player controls, and miscellaneous user-configurable shortcuts for email client, World Wide Web browser, etc.

The IBM PC layout, particularly the Model M, has been extremely influential, and today most keyboards use some variant of it. This has caused problems for applications developed with alternative layouts, which require keys that are in awkward positions on the Model M layout – often requiring the pinkie to operate – and thus require remapping for comfortable use. One notable example is the Escape key, used by the vi editor: on the ADM-3A terminal this was located where the Tab key is on the IBM PC, but on the IBM PC the Escape key is in the corner; this is typically solved by remapping Caps Lock to Escape. Another example is the Emacs editor, which makes extensive use of modifier keys, and uses the Control key more than the Meta key (IBM PC instead has the Alt key) – these date to the Knight keyboard, which had the Control key on the "inside" of the Meta key, opposite to the Model M, where it is on the "outside" of the Alt key; and to the space-cadet keyboard, where the four bucky bit keys (Control, Meta, Super, Hyper) are in a row, allowing easy chording to press several, unlike on the Model M layout. This results in the "Emacs pinky" problem.

Although "PC Magazine" praised most aspects of the 1981 IBM PC keyboard's hardware design, it questioned "how IBM, that ultimate pro of keyboard manufacture, could put the left-hand SHIFT key at the awkward reach they did". The magazine reported in 1982 that it received more letters to its "Wish List" column asking for the ability to determine the status of the three lock keys than on any other topic. "BYTE" columnist Jerry Pournelle praised the keyboard's feel as "excellent" but complained that the Shift and other keys' locations were "enough to make a saint weep", and denounced the trend of PC compatible computers to emulate the layout but not the feel. He reported that the layout "nearly drove" science-fiction editor Jim Baen "crazy", and that "many of [Baen's] authors refused to work with that keyboard" so could not submit manuscripts in a compatible format. "BYTE"'s review was more sanguine. It praised the keyboard as "bar none, the best ... on any microcomputer" and described the unusual Shift key locations as "minor [problems] compared to some of the gigantic mistakes made on almost every other microcomputer keyboard".

"I wasn't thrilled with the placement of [the left Shift and Return] keys, either", IBM's Don Estridge stated in 1983. He defended the layout, however, stating that "every place you pick to put them is not a good place for somebody ... there's no consensus", and claimed that "if we were to change it now we would be in hot water".

The PC keyboard with its various keys has a long history of evolution reaching back to teletypewriters. In addition to the 'old' standard keys, the PC keyboard has accumulated several special keys over the years. Some of the additions have been inspired by the opportunity or requirement for improving user productivity with general office application software, while other slightly more general keyboard additions have become the factory standards after being introduced by certain operating system or GUI software vendors such as Microsoft.








</doc>
<doc id="15441" url="https://en.wikipedia.org/wiki?curid=15441" title="Italian battleship Giulio Cesare">
Italian battleship Giulio Cesare

Giulio Cesare was one of three dreadnought battleships built for the Royal Italian Navy ("Regia Marina") in the 1910s. She served in both World Wars, although she was little used and saw no combat during the former. The ship supported operations during the Corfu Incident in 1923 and spent much of the rest of the decade in reserve. She was rebuilt between 1933 and 1937 with more powerful guns, additional armor and considerably more speed than before.

Both "Giulio Cesare" and her sister ship, , participated in the Battle of Calabria in July 1940, when the former was lightly damaged. They were both present when British torpedo bombers attacked the fleet at Taranto in November 1940, but "Giulio Cesare" was not damaged. She escorted several convoys to North Africa and participated in the Battle of Cape Spartivento in late 1940 and the First Battle of Sirte in late 1941. She was designated as a training ship in early 1942, and escaped to Malta after Italy surrendered. The ship was transferred to the Soviet Union in 1949 and renamed Novorossiysk (). The Soviets also used her for training until she was sunk, with the loss of 608 men, when an old German mine exploded in 1955. She was salvaged the following year and later scrapped.

Named after Julius Caesar, "Giulio Cesare" was long at the waterline, and overall. The ship had a beam of , and a draft of . She displaced at normal load, and at deep load. She had a crew of 31 officers and 969 enlisted men. The ship's machinery consisted of four Parsons steam turbines, each driving one propeller shaft. Steam for the turbines was provided by 24 Babcock & Wilcox boilers, half of which burned fuel oil and the other half burning both oil and coal. Designed to reach a maximum speed of from , "Giulio Cesare" failed to reach this goal on her sea trials, despite generally exceeding the rated power of her turbines. The ship only made a maximum speed of using . She had a cruising radius of at .

The ship was armed with a main battery of thirteen 305 mm /46 Model 1909 guns in three triple-gun turret and two twin-gun turrets, designated 'A', 'B', 'Q', 'X', and 'Y' from front to rear. The secondary battery comprised eighteen guns, all mounted in casemates in the sides of the hull. "Giulio Cesare" was also armed with fourteen guns. As was customary for capital ships of the period, she was equipped with three submerged torpedo tubes. She was protected with Krupp cemented steel manufactured by Terni. The belt armor was thick and the main deck was thick. The conning tower and main battery turrets were protected with worth of armor plating.

Shortly after the end of World War I, the number of 50-caliber 76 mm guns was reduced to 13, all mounted on the turret tops, and six new 40-caliber 76 mm anti-aircraft (AA) guns were installed abreast the aft funnel. In addition two license-built 2-pounder AA guns were mounted on the forecastle deck. In 1925–26 the foremast was replaced by a four-legged mast, which was moved forward of the funnels, the rangefinders were upgraded, and the ship was equipped to handle a Macchi M.18 seaplane mounted on the center turret. Around that same time, either one or both of the ships was equipped with a fixed aircraft catapult on the port side of the forecastle.

"Giulio Cesare" began an extensive reconstruction in October 1933 at the Cantieri del Tirreno shipyard in Genoa that lasted until October 1937. A new bow section was grafted over the existing bow which increased her length by to and her beam increased to . The ship's draft at deep load increased to . All of the changes made increased her displacement to at standard load and at deep load. The ship's crew increased to 1,260 officers and enlisted men. Two of the propeller shafts were removed and the existing turbines were replaced by two Belluzzo geared steam turbines rated at . The boilers were replaced by eight Yarrow boilers. On her sea trials in December 1936, before her reconstruction was fully completed, "Giulio Cesare" reached a speed of from . In service her maximum speed was about and she had a range of at a speed of .

The main guns were bored out to and the center turret and the torpedo tubes were removed. All of the existing secondary armament and AA guns were replaced by a dozen 120 mm guns in six twin-gun turrets and eight AA guns in twin turrets. In addition the ship was fitted with a dozen Breda light AA guns in six twin-gun mounts and twelve Breda M31 anti-aircraft machine guns, also in twin mounts. In 1940 the 13.2 mm machine guns were replaced by AA guns in twin mounts. "Giulio Cesare" received two more twin mounts as well as four additional 37 mm guns in twin mounts on the forecastle between the two turrets in 1941. The tetrapodal mast was replaced with a new forward conning tower, protected with thick armor. Atop the conning tower there was a fire-control director fitted with two large stereo-rangefinders, with a base length of .

The deck armor was increased during the reconstruction to a total of over the engine and boiler rooms and over the magazines, although its distribution over three decks, each with multiple layers, meant that it was considerably less effective than a single plate of the same thickness. The armor protecting the barbettes was reinforced with plates. All this armor weighed a total of . The existing underwater protection was replaced by the Pugliese torpedo defense system that consisted of a large cylinder surrounded by fuel oil or water that was intended to absorb the blast of a torpedo warhead. It lacked, however, enough depth to be fully effective against contemporary torpedoes. A major problem of the reconstruction was that the ship's increased draft meant that their waterline armor belt was almost completely submerged with any significant load.

"Giulio Cesare", named after Julius Caesar, was laid down at the Gio. Ansaldo & C. shipyard in Genoa on 24 June 1910 and launched on 15 October 1911. She was completed on 14 May 1914 and served as a flagship in the southern Adriatic Sea during World War I. She saw no action, however, and spent little time at sea. Admiral Paolo Thaon di Revel, the Italian naval chief of staff, believed that Austro-Hungarian submarines and minelayers could operate too effectively in the narrow waters of the Adriatic. The threat from these underwater weapons to his capital ships was too serious for him to use the fleet in an active way. Instead, Revel decided to implement a blockade at the relatively safer southern end of the Adriatic with the battle fleet, while smaller vessels, such as the MAS torpedo boats, conducted raids on Austro-Hungarian ships and installations. Meanwhile, Revel's battleships would be preserved to confront the Austro-Hungarian battle fleet in the event that it sought a decisive engagement.

"Giulio Cesare" made port visits in the Levant in 1919 and 1920. Both "Giulio Cesare" and "Conte di Cavour" supported Italian operations on Corfu in 1923 after an Italian general and his staff were murdered on Corfu; Benito Mussolini was not satisfied with the Greek government's response so he ordered Italian troops to occupy the island. "Cesare" became a gunnery training ship in 1928, after having been in reserve since 1926. She was reconstructed at Cantieri del Tirreno, Genoa, between 1933 and 1937. Both ships participated in a naval review by Adolf Hitler in the Bay of Naples in May 1938 and covered the invasion of Albania in May 1939.

Early in World War II, the ship took part in the Battle of Calabria (also known as the Battle of Punto Stilo), together with "Conte di Cavour", on 9 July 1940, as part of the 1st Battle Squadron, commanded by Admiral Inigo Campioni, during which she engaged major elements of the British Mediterranean Fleet. The British were escorting a convoy from Malta to Alexandria, while the Italians had finished escorting another from Naples to Benghazi, Libya. Admiral Andrew Cunningham, commander of the Mediterranean Fleet, attempted to interpose his ships between the Italians and their base at Taranto. Crew on the fleets spotted each other in the middle of the afternoon and the battleships opened fire at 15:53 at a range of nearly . The two leading British battleships, and , replied a minute later. Three minutes after she opened fire, shells from "Giulio Cesare" began to straddle "Warspite" which made a small turn and increased speed, to throw off the Italian ship's aim, at 16:00. At that same time, a shell from "Warspite" struck "Giulio Cesare" at a distance of about . The shell pierced the rear funnel and detonated inside it, blowing out a hole nearly across. Fragments started several fires and their smoke was drawn into the boiler rooms, forcing four boilers off-line as their operators could not breathe. This reduced the ship's speed to . Uncertain how severe the damage was, Campioni ordered his battleships to turn away in the face of superior British numbers and they successfully disengaged. Repairs to "Giulio Cesare" were completed by the end of August and both ships unsuccessfully attempted to intercept British convoys to Malta in August and September.

On the night of 11 November 1940, "Giulio Cesare" and the other Italian battleships were at anchor in Taranto harbor when they were attacked by 21 Fairey Swordfish torpedo bombers from the British aircraft carrier , along with several other warships. One torpedo sank Conte di Cavour in shallow water, but "Giulio Cesare" was not hit during the attack. She participated in the Battle of Cape Spartivento on 27 November 1940, but never got close enough to any British ships to fire at them. The ship was damaged in January 1941 by splinters from a near miss during an air raid on Naples by Vickers Wellington bombers of the Royal Air Force; repairs at Genoa were completed in early February. On 8 February, she sailed from to the Straits of Bonifacio to intercept what the Italians thought was a Malta convoy, but was actually a raid on Genoa. She failed to make contact with any British forces. She participated in the First Battle of Sirte on 17 December 1941, providing distant cover for a convoy bound for Libya, and briefly engaging the escort force of a British convoy (during the battle, the destroyer "Kipling" suffered some damage from near misses, variably credited to "Cesare", "Doria" or the heavy cruiser "Gorizia"). She also provided distant cover for another convoy to North Africa in early January 1942. "Giulio Cesare" was reduced to a training ship afterwards at Taranto and later Pola. The unsuccessfully attacked the ship in the Gulf of Taranto in early March 1944. After the Italian surrender on 9 September 1943, she steamed to Taranto, putting down a mutiny and enduring an ineffective attack by five German aircraft en route. She then sailed for Malta where she arrived on 12 September to be interned. The ship remained there until 17 June 1944 when she returned to Taranto where she remained for the next four years.

After the war, "Giulio Cesare" was allocated to the Soviet Union as part of the war reparations. She was moved to Augusta, Sicily, on 9 December 1948, where an unsuccessful attempt was made at sabotage. The ship was stricken from the naval register on 15 December and turned over to the Soviets on 6 February 1949 under the temporary name of "Z11" in Vlorë, Albania. She was renamed "Novorossiysk", after the Soviet city on the Black Sea. The Soviets used her as a training ship, and gave her eight refits. In 1953, all Italian light AA guns were replaced by eighteen 37 mm 70-K AA guns in six twin mounts and six singles. Also replaced were her fire-control systems and radars. The Soviets intended to rearm her with their own 305 mm guns, but this was forestalled by her loss. While at anchor in Sevastopol on the night of 28/29 October 1955, an explosion ripped a hole in the forecastle forward of 'A' turret. The flooding could not be controlled, and she capsized with the loss of 608 men, including men sent from other ships to assist.

The cause of the explosion is still unclear. The official cause, regarded as the most probable, was a magnetic RMH or LMB bottom mine, laid by the Germans during World War II and triggered by the dragging of the battleship's anchor chain before mooring for the last time. Subsequent searches located 32 mines of these types, some of them within of the explosion. The damage was consistent with an explosion of of TNT, and more than one mine may have detonated. Nonetheless, other explanations for the ship's loss have been proposed, and the most popular of these is that she was sunk by Italian frogmen of the wartime special operations unit "Decima Flottiglia MAS" who – more than ten years after the cessation of hostilities – were either avenging the transfer of the former Italian battleship to the USSR or sinking it on behalf of NATO. "Novorossiysk" was stricken from the naval register on 24 February 1956, salvaged on 4 May 1957, and subsequently scrapped.



</doc>
<doc id="15442" url="https://en.wikipedia.org/wiki?curid=15442" title="INS Vikrant (R11)">
INS Vikrant (R11)

INS "Vikrant (from Sanskrit "vikrānta", "courageous") was a of the Indian Navy. The ship was laid down as HMS "Hercules for the British Royal Navy during World War II, but construction was put on hold when the war ended. India purchased the incomplete carrier in 1957, and construction was completed in 1961. "Vikrant" was commissioned as the first aircraft carrier of the Indian Navy and played a key role in enforcing the naval blockade of East Pakistan during the Indo-Pakistani War of 1971.

In its later years, the ship underwent major refits to embark modern aircraft, before being decommissioned in January 1997. She was preserved as a museum ship in Cuffe Parade, Mumbai until 2012. In January 2014, the ship was sold through an online auction and scrapped in November 2014 after final clearance from the Supreme Court.

In 1943 the Royal Navy commissioned six light aircraft carriers in an effort to counter the German and Japanese navies. The 1942 Design Light Fleet Carrier, commonly referred to as the British Light Fleet Carrier, was the result. Serving with eight navies between 1944 and 2001, these ships were designed and constructed by civilian shipyards as an intermediate step between the full-sized fleet aircraft carriers and the less expensive but limited-capability escort carriers.

Sixteen light fleet carriers were ordered, and all were laid down as what became the "Colossus" class in 1942 and 1943. The final six ships were modified during construction to handle larger and faster aircraft, and were re-designated the "Majestic" class. The improvements from the "Colossus" class to the "Majestic" class included heavier displacement, armament, catapult, aircraft lifts and aircraft capacity. Construction on the ships was suspended at the end of World War II, as the ships were surplus to the Royal Navy's peacetime requirements.
Instead, the carriers were modernized and sold to several Commonwealth nations. The ships were similar, but each varied depending on the requirements of the country to which the ship was sold.

HMS "Hercules", the fifth ship in the "Majestic" class, was ordered on 7 August 1942 and laid down on 14 October 1943 by Vickers-Armstrongs on the River Tyne. After World War II ended with Japan's surrender on 2 September 1945, she was launched on 22 September, and her construction was suspended in May 1946. At the time of suspension, she was 75 per cent complete. Her hull was preserved, and in May 1947 she was laid up in Gareloch off the Clyde. In January 1957, she was purchased by India and was towed to Belfast to complete her construction and modifications by Harland and Wolff. Several improvements to the original design were ordered by the Indian Navy, including an angled deck, steam catapults, and a modified island.

"Vikrant" displaced at standard load and at deep load. She had an overall length of , a beam of and a mean deep draught of . She was powered by a pair of Parsons geared steam turbines, driving two propeller shafts, using steam provided by four Admiralty three-drum boilers. The turbines developed a total of which gave a maximum speed of . "Vikrant" carried about of fuel oil that gave her a range of at , and at . The air and ship crew included 1,110 officers.

The ship was armed with sixteen Bofors anti-aircraft guns, but these were later reduced to eight. At various times, its aircraft consisted of Hawker Sea Hawk and Sea Harrier (STOVL) jet fighters, Sea King Mk 42B and HAL Chetak helicopters, and Breguet Alizé Br.1050 anti-submarine aircraft. The carrier fielded between 21 and 23 aircraft of all types. "Vikrant"s flight decks were designed to handle aircraft up to , but remained the heaviest landing weight of an aircraft. Larger lifts were installed.

The ship was equipped with one LW-05 air-search radar, one ZW-06 surface-search radar, one LW-10 tactical radar and one Type 963 aircraft landing radar with other communication systems.

The Indian Navy's first aircraft carrier was commissioned as INS "Vikrant" on 4 March 1961 in Belfast by Vijaya Lakshmi Pandit, the Indian High Commissioner to the United Kingdom . The name "Vikrant" was derived from the Sanskrit word "vikrānta" meaning "stepping beyond", "courageous" or "bold". Captain Pritam Singh was the first commanding officer of the ship, which carried British Hawker Sea Hawk fighter-bombers and French Alizé anti-submarine aircraft. On 18 May 1961, the first jet landed on her deck. It was piloted by Lieutenant Radhakrishna Hariram Tahiliani, who later served as admiral and Chief of the Naval Staff of India from 1984 to 1987. "Vikrant" formally joined the Indian Navy's fleet in Bombay (now Mumbai) on 3 November 1961, when she was received at Ballard Pier by then Prime Minister Jawaharlal Nehru.

In December of that year, the ship was deployed for Operation Vijay (the code name for the annexation of Portuguese India) off the coast of Goa with two destroyers, and . "Vikrant" did not see action, and patrolled along the coast to deter foreign interference. During the Indo-Pakistani War of 1965, "Vikrant" was in dry dock refitting, and did not see any action.

In June 1970, "Vikrant" was docked at the Naval Dockyard, Bombay, due to many internal fatigue cracks and fissures in the water drums of her boilers that could not be repaired by welding. As replacement drums were not available locally, four new ones were ordered from Britain, and Naval Headquarters issued orders not to use the boilers until further notice. On 26 February 1971 the ship was moved from Ballard Pier Extension to the anchorage, without replacement drums. The main objective behind this move was to light up the boilers at reduced pressure, and work up the main and flight deck machinery that had been idle for almost seven months. On 1 March, the boilers were ignited, and basin trials up to 40 revolutions per minute (RPM) were conducted. Catapult trials were conducted on the same day.

The ship began preliminary sea trials on 18 March and returned two days later. Trials were again conducted on 26–27 April. The navy decided to limit the boilers to a pressure of and the propeller revolutions to 120 RPM ahead and 80 RPM astern, reducing the ship's speed to . With the growing expectations of a war with Pakistan in the near future, the navy started to transfer its ships to strategically advantageous locations in Indian waters. The primary concern of Naval Headquarters about the operation was the serviceability of "Vikrant". When asked his opinion regarding the involvement of "Vikrant" in the war, Fleet Operations Officer Captain Gulab Mohanlal Hiranandani told the Chief of the Naval Staff Admiral Sardarilal Mathradas Nanda:

Nanda and Hiranandani proved to be instrumental in taking "Vikrant" to war. There were objections that the ship might have severe operational difficulties that would expose the carrier to increased danger on operations. In addition, the three s acquired by the Pakistan Navy posed a significant risk to the carrier. In June, extensive deep sea trials were carried out, with steel safety harnesses around the three boilers still operational. Observation windows were fitted as a precautionary measure, to detect any steam leaks. By the end of June, the trials were complete and "Vikrant" was cleared to participate on operations, with its speed restricted to 14 knots.

As a part of preparations for the war, "Vikrant" was assigned to the Eastern Naval Command, then to the Eastern Fleet. This fleet consisted of INS "Vikrant", the two s and , the two Petya III-class corvettes and , and one submarine, . The main reason behind strengthening the Eastern Fleet was to counter the Pakistani maritime forces deployed in support of military operations in East Bengal. A surveillance area of , confined by a triangle with a base of and sides of and , was set up in the Bay of Bengal. Any ship in this area was to be challenged and checked. If found to be neutral, it would be escorted to the nearest Indian port, otherwise, it would be captured, and taken as a war prize.

In the meantime, intelligence reports confirmed that Pakistan was to deploy a US-built , . "Ghazi" was considered as a serious threat to "Vikrant" by the Indian Navy, as "Vikrant"s approximate position would be known by the Pakistanis once she started operating aircraft. Of the four available surface ships, INS "Kavaratti" had no sonar, which meant that the other three had to remain in close vicinity of "Vikrant", without which the carrier would be completely vulnerable to attack by "Ghazi".

On 23 July, "Vikrant" sailed off to Cochin in company with the Western Fleet. En route, before reaching Cochin on 26 July, Sea King landing trials were carried out. After the completion of the radar and communication trials on 28 July, she departed for Madras, escorted by "Brahmaputra" and "Beas". The next major problem was operating aircraft from the carrier. The commanding officer of the ship, Captain (later Vice Admiral) S. Prakash, was seriously concerned about flight operations. He was concerned that aircrew morale would be adversely affected if flight operations were not undertaken, which could be disastrous. Naval Headquarters remained stubborn on the speed restrictions, and sought confirmation from Prakash whether it was possible to embark an Alizé without compromising the speed restrictions. The speed restrictions imposed by the headquarters meant that Alizé aircraft would have to land at close to stalling speed. Eventually the aircraft weight was reduced, which allowed several of the aircraft to embark, along with a Seahawk squadron.

By the end of September, "Vikrant" and her escorts reached Port Blair. En route to Visakhapatnam, tactical exercises were conducted in the presence of the Flag Officer Commanding-in-Chief of the Eastern Naval Command. From Vishakhapatnam, "Vikrant" set out for Madras for maintenance. Rear Admiral S. H. Sharma was appointed Flag Officer Commanding Eastern Fleet and arrived at Vishakhapatnam on 14 October. After receiving the reports that Pakistan might launch preemptive strikes, maintenance was stopped for another tactical exercise, which was completed during the night of 26–27 October at Vishakhapatnam. "Vikrant" then returned to Madras to resume maintenance. On 1 November, the Eastern Fleet was formally constituted, and on 13 November, all the ships set out for the Andaman and Nicobar Islands. To avoid misadventures, it was planned to sail "Vikrant" to a remote anchorage, isolating it from combat. Simultaneously, deception signals would give the impression that "Vikrant" was operating somewhere between Madras and Vishakhapatnam.

On 23 November, an emergency was declared in Pakistan after a clash of Indian and Pakistani troops in East Pakistan two days earlier. On 2 December, the Eastern Fleet proceeded to its patrol area in anticipation of an attack by Pakistan. The Pakistan Navy had deployed "Ghazi" on 14 November with the explicit goal of targeting and sinking "Vikrant", and "Ghazi" reached a location near Madras by the 23rd. In an attempt to deceive the Pakistan Navy and "Ghazi", India's Naval Headquarters deployed "Rajput" as a decoy—the ship sailed off the coast of Vishakhapatnam and broadcast a significant amount of radio traffic, making her appear to be "Vikrant".

"Ghazi", meanwhile, sank off the Visakhapatnam coast under mysterious circumstances. On the night of 3–4 December, a muffled underwater explosion was detected by a coastal battery. The next morning, a local fisherman observed flotsam near the coast, causing Indian naval officials to suspect a vessel had sunk off the coast. The next day, a clearance diving team was sent to search the area, and they confirmed that "Ghazi" had sunk in shallow waters.

The reason for "Ghazi"s fate is unclear. The Indian Navy's official historian, Hiranandani, suggests three possibilities, after having analysed the position of the rudder and extent of the damage suffered. The first was that "Ghazi" had come up to periscope depth to identify her position and may have seen an anti-submarine vessel that caused her to crash dive, which in turn may have led her to bury her bow in the bottom. The second possibility is closely related to the first: on the night of the explosion, "Rajput" was on patrol off Visakhapatnam and observed a severe disturbance in the water. Suspecting that it was a submarine, the ship dropped two depth charges on the spot, on a position that was very close to the wreckage. The third possibility is that there was a mishap when "Ghazi" was laying mines on the day before hostilities broke out.

"Vikrant" was redeployed towards Chittagong at the outbreak of hostilities. On 4 December, the ship's Sea Hawks struck shipping in Chittagong and Cox's Bazar harbours, sinking or incapacitating most of the ships present. Later strikes targeted Khulna and the Port of Mongla, which continued until 10 December, while other operations were flown to support a naval blockade of East Pakistan. On 14 December, the Sea Hawks attacked the cantonment area in Chittagong, destroying several Pakistani army barracks. Medium anti-aircraft fire was encountered during this strike. Simultaneous attacks by Alizés continued on Cox's Bazar. After this, "Vikrant"s fuel levels dropped to less than 25 per cent, and the aircraft carrier sailed to Paradip for refueling. The crew of INS "Vikrant" earned two Maha Vir Chakras and twelve Vir Chakra gallantry medals for their part in the war.

"Vikrant" did not see much service after the war, and was given two major modernisation refits—the first one from 1979 to 1981 and the second one from 1987 to 1989. In the first phase, her boilers, radars, communication systems and anti-aircraft guns were modernised, and facilities to operate Sea Harriers were installed. In the second phase, facilities to operate the new Sea Harrier Vertical/Short Take Off and Land (V/STOL) fighter aircraft and the new Sea King Mk 42B Anti-Submarine Warfare (ASW) helicopters were introduced. A 9.75-degree ski-jump ramp was fitted. The steam catapult was removed during this phase. Again in 1991, "Vikrant" underwent a six-month refit, followed by another fourteen-month refit in 1992–94. She remained operational thereafter, flying Sea Harriers, Sea Kings and Chetaks until her final sea outing on 23 November 1994. In the same year, a fire was also recorded aboard. In January 1995, the navy decided to keep "Vikrant" in "safe to float" state. She was laid up and formally decommissioned on 31 January 1997.

During her service, INS "Vikrant" embarked four squadrons of the Naval Air Arm of the Indian Navy:

Following decommissioning in 1997, the ship was earmarked for preservation as a museum ship in Mumbai. Lack of funding prevented progress on the ship's conversion to a museum and it was speculated that the ship would be made into a training ship. In 2001, the ship was opened to the public by the Indian Navy, but the Government of Maharashtra was unable to find a partner to operate the museum on a permanent, long-term basis and the museum was closed after it was deemed unsafe for the public in 2012.

In August 2013, Vice-Admiral Shekhar Sinha, chief of the Western Naval Command, said the Ministry of Defence would scrap the ship as she had become very difficult to maintain and no private bidders had offered to fund the museum's operations. On 3 December 2013, the Indian government decided to auction the ship. The Bombay High Court dismissed a public-interest lawsuit filed by Kiran Paigankar to stop the auction, stating the vessel's dilapidated condition did not warrant her preservation, nor were the necessary funds or government support available.

In January 2014, the ship was sold through an online auction to a Darukhana ship-breaker for . The Supreme Court of India dismissed another lawsuit challenging the ship's sale and scrapping on 14 August 2014. "Vikrant" remained beached off Darukhana in Mumbai Port while awaiting the final clearances of the Mumbai Port Trust. On 12 November 2014, the Supreme Court gave its final approval for the carrier to be scrapped, which commenced on 22 November 2014.

In memory of "Vikrant", the Vikrant Memorial was unveiled by Vice Admiral Surinder Pal Singh Cheema, Flag Officer Commanding-in-Chief of the Western Naval Command at K Subash Marg in the Naval Dockyard of Mumbai on 25 January 2016. The memorial is made from metal recovered from the ship.
In February 2016, Bajaj unveiled a new motorbike made with metal from "Vikrant"s scrap and named it Bajaj V in honour of "Vikrant".

The navy has named its first home-built carrier INS "Vikrant" in honour of INS "Vikrant" (R11). The new carrier is built by Cochin Shipyard Limited, and will displace . The keel was laid down in February 2009 and she was launched in August 2013. , the ship is being fitted out and is expected to be commissioned by the end of 2018.

The decommissioned ship featured prominently in the film "ABCD 2" as a backdrop while it was moored near Darukhana in Mumbai.




</doc>
<doc id="15443" url="https://en.wikipedia.org/wiki?curid=15443" title="Western imperialism in Asia">
Western imperialism in Asia

Western imperialism in Asia as presented in this article pertains to Western European entry into what was first called the East Indies. This was sparked early in the 15th century by the search for trade routes to China that led directly to the Age of Discovery, and the introduction of early modern warfare into what was then called the Far East. By the early 16th century the Age of Sail greatly expanded Western European influence and development of the Spice Trade under colonialism. There has been a presence of Western European colonial empires and imperialism in Asia throughout six centuries of colonialism, formally ending with the independence of the Portuguese Empire's last colony East Timor in 2002. The empires introduced Western concepts of nation and the multinational state. This article attempts to outline the consequent development of the Western concept of the nation state.

The thrust of European political power, commerce, and culture in Asia gave rise to growing trade in commodities—a key development in the rise of today's modern world free market economy. In the 16th century, the Portuguese broke the (overland) monopoly of the Arabs and Italians of trade between Asia and Europe by the discovery of the sea route to India around the Cape of Good Hope. With the ensuing rise of the rival Dutch East India Company, Portuguese influence in Asia was gradually eclipsed. Dutch forces first established independent bases in the East (most significantly Batavia, the heavily fortified headquarters of the Dutch East India Company) and then between 1640 and 1660 wrestled Malacca, Ceylon, some southern Indian ports, and the lucrative Japan trade from the Portuguese. Later, the English and the French established settlements in India and established a trade with China and their own acquisitions would gradually surpass those of the Dutch. Following the end of the Seven Years' War in 1763, the British eliminated French influence in India and established the British East India Company as the most important political force on the Indian Subcontinent.

Before the Industrial Revolution in the mid-to-late 19th century, demand for oriental goods such as (porcelain, silk, spices and tea) remained the driving force behind European imperialism, and (with the important exception of British East India Company rule in India) the European stake in Asia remained confined largely to trading stations and strategic outposts necessary to protect trade. Industrialisation, however, dramatically increased European demand for Asian raw materials; and the severe Long Depression of the 1870s provoked a scramble for new markets for European industrial products and financial services in Africa, the Americas, Eastern Europe, and especially in Asia. This scramble coincided with a new era in global colonial expansion known as "the New Imperialism," which saw a shift in focus from trade and indirect rule to formal colonial control of vast overseas territories ruled as political extensions of their mother countries. Between the 1870s and the beginning of World War I in 1914, the United Kingdom, France, and the Netherlands—the established colonial powers in Asia—added to their empires vast expanses of territory in the Middle East, the Indian Subcontinent, and South East Asia. In the same period, the Empire of Japan, following the Meiji Restoration; the German Empire, following the end of the Franco-Prussian War in 1871; Tsarist Russia; and the United States, following the Spanish–American War in 1898, quickly emerged as new imperial powers in East Asia and in the Pacific Ocean area.

In Asia, World War I and World War II were played out as struggles among several key imperial powers—conflicts involving the European powers along with Russia and the rising American and Japanese powers. None of the colonial powers, however, possessed the resources to withstand the strains of both world wars and maintain their direct rule in Asia. Although nationalist movements throughout the colonial world led to the political independence of nearly all of the Asia's remaining colonies, decolonisation was intercepted by the Cold War; and South East Asia, South Asia, the Middle East, and East Asia remained embedded in a world economic, financial, and military system in which the great powers compete to extend their influence. However, the rapid post-war economic development of the East Asian Tigers, India, the People's Republic of China, along with the collapse of the Soviet Union, have loosened European and American influence in Asia, generating speculation today about emergence of modern India and China as potential superpowers.

European exploration of Asia started in ancient Roman times. Knowledge of lands as distant as China were held by the Romans. Trade with India through the Roman Egyptian Red Sea ports was significant in the first centuries of the Common Era.

In the 13th and 14th centuries, a number of Europeans, many of them Christian missionaries, had sought to penetrate into China. The most famous of these travelers was Marco Polo. But these journeys had little permanent effect on East-West trade because of a series of political developments in Asia in the last decades of the 14th century, which put an end to further European exploration of Asia. The Yuan dynasty in China, which had been receptive to European missionaries and merchants, was overthrown, and the new Ming rulers were found to be unreceptive of religious proselytism. Meanwhile, the Turks consolidated control over the eastern Mediterranean, closing off key overland trade routes. Thus, until the 15th century, only minor trade and cultural exchanges between Europe and Asia continued at certain terminals controlled by Muslim traders.

Western European rulers determined to find new trade routes of their own. The Portuguese spearheaded the drive to find oceanic routes that would provide cheaper and easier access to South and East Asian goods. This chartering of oceanic routes between East and West began with the unprecedented voyages of Portuguese and Spanish sea captains. Their voyages were influenced by medieval European adventurers, who had journeyed overland to the Far East and contributed to geographical knowledge of parts of Asia upon their return.

In 1488, Bartolomeu Dias rounded the southern tip of Africa under the sponsorship of Portugal's John II, from which point he noticed that the coast swung northeast (Cape of Good Hope). While Dias' crew forced him to turn back, by 1497, Portuguese navigator Vasco da Gama made the first open voyage from Europe to India. In 1520, Ferdinand Magellan, a Portuguese navigator in the service of the Crown of Castile ('Spain'), found a sea route into the Pacific Ocean.

In 1509, the Portuguese under Francisco de Almeida won the decisive battle of Diu against a joint Mamluk and Arab fleet sent to expel the Portuguese of the Arabian Sea. The victory enabled Portugal to implement its strategy of controlling the Indian Ocean.

Early in the 16th century Afonso de Albuquerque (left) emerged as the Portuguese colonial viceroy most instrumental in consolidating Portugal's holdings in Africa and in Asia. He understood that Portugal could wrest commercial supremacy from the Arabs only by force, and therefore devised a plan to establish forts at strategic sites which would dominate the trade routes and also protect Portuguese interests on land. In 1510, he conquered Goa in India, which enabled him to gradually consolidate control of most of the commercial traffic between Europe and Asia, largely through trade; Europeans started to carry on trade from forts, acting as foreign merchants rather than as settlers. In contrast, early European expansion in the "West Indies", (later known to Europeans as a separate continent from Asia that they would call the "Americas") following the 1492 voyage of Christopher Columbus, involved heavy settlement in colonies that were treated as political extensions of the mother countries.

Lured by the potential of high profits from another expedition, the Portuguese established a permanent base in Cochin, south of the Indian trade port of Calicut in the early 16th century. In 1510, the Portuguese, led by Afonso de Albuquerque, seized Goa on the coast of India, which Portugal held until 1961, along with Diu and Daman (the remaining territory and enclaves in India from a former network of coastal towns and smaller fortified trading ports added and abandoned or lost centuries before). The Portuguese soon acquired a monopoly over trade in the Indian Ocean.

Portuguese viceroy Albuquerque (1509–1515) resolved to consolidate Portuguese holdings in Africa and Asia, and secure control of trade with the East Indies and China. His first objective was Malacca, which controlled the narrow strait through which most Far Eastern trade moved. Captured in 1511, Malacca became the springboard for further eastward penetration, starting with the voyage of António de Abreu and Francisco Serrão in 1512, ordered by Albuquerque, to the Moluccas. Years later the first trading posts were established in the Moluccas, or "Spice Islands", which was the source for some of the world's most hotly demanded spices, and from there, in Makassar and some others, but smaller, in the Lesser Sunda Islands. By 1513-1516, the first Portuguese ships had reached Canton on the southern coasts of China.

In 1513, after the failed attempt to conquer Aden, Albuquerque entered with an armada, for the first time for Europeans by the ocean via, on the Red Sea; and in 1515, Albuquerque consolidated the Portuguese hegemony in the Persian Gulf gates, already begun by him in 1507, with the domain of Muscat and Ormuz. Shortly after, other fortified bases and forts were annexed and built along the Gulf, and in 1521, through a military campaign, the Portuguese annexed Bahrain.

The Portuguese conquest of Malacca triggered the Malayan–Portuguese war. In 1521, Ming dynasty China defeated the Portuguese at the Battle of Tunmen and then defeated the Portuguese again at the Battle of Xicaowan. The Portuguese tried to establish trade with China by illegally smuggling with the pirates on the offshore islands off the coast of Zhejiang and Fujian, but they were driven away by the Ming navy in the 1530s-1540's.

In 1557, China decided to lease Macau to the Portuguese as a place where they could dry goods they transported on their ships, which they held until 1999. The Portuguese, based at Goa and Malacca, had now established a lucrative maritime empire in the Indian Ocean meant to monopolise the spice trade. The Portuguese also began a channel of trade with the Japanese, becoming the first recorded Westerners to have visited Japan. This contact introduced Christianity and fire-arms into Japan.

In 1505 (also possibly before, in 1501), the Portuguese, through Lourenço de Almeida, the son of Francisco de Almeida, reached Ceylon. The Portuguese founded a fort at the city of Colombo in 1517 and gradually extended their control over the coastal areas and inland. In a series of military conflicts and political manoeuvres, the Portuguese extended their control over the Sinhalese kingdoms, including Jaffna (1591), Raigama (1593), Sitawaka (1593), and Kotte (1594)- However, the aim of unifying the entire island under Portuguese control faced the Kingdom of Kandy`s fierce resistance. The Portuguese, led by Pedro Lopes de Sousa, launched a full-scale military invasion of the kingdom of Kandy in the Campaign of Danture of 1594. The invasion was a disaster for the Portuguese, with their entire army wiped out by Kandyan guerilla warfare. Constantino de Sá, romantically celebrated in the 17th century Sinhalese Epic (also for its greater humanism and tolerance compared to other governors) led the last military operation that also ended in disaster. He died in the Battle of Randeniwela, refusing to abandon his troops in the face of total annihilation.

The energies of Castile (later, the "unified" Spain), the other major colonial power of the 16th century, were largely concentrated on the Americas, not South and East Asia, but the Spanish did establish a footing in the Far East in the Philippines. After fighting with the Portuguese by the Spice Islands since 1522 and the agreement between the two powers in 1529 (in the treaty of Zaragoza), the Spanish, led by Miguel López de Legazpi, settled and conquered gradually the Philippines since 1564. After the discovery of the return voyage to the Americas by Andres de Urdaneta in 1565, cargoes of Chinese goods were transported from the Philippines to Mexico and from there to Spain. By this long route, Spain reaped some of the profits of Far Eastern commerce. Spanish officials converted the islands to Christianity and established some settlements, permanently establishing the Philippines as the area of East Asia most oriented toward the West in terms of culture and commerce. The Moro Muslims fought against the Spanish for over three centuries in the Spanish–Moro conflict.

The lucrative trade was vastly expanded when the Portuguese began to export slaves from Africa in 1541; however, over time, the rise of the slave trade left Portugal over-extended, and vulnerable to competition from other Western European powers. Envious of Portugal's control of trade routes, other Western European nations—mainly the Netherlands, France, and England—began to send in rival expeditions to Asia. In 1642, the Dutch drove the Portuguese out of the Gold Coast in Africa, the source of the bulk of Portuguese slave labourers, leaving this rich slaving area to other Europeans, especially the Dutch and the English.

Rival European powers began to make inroads in Asia as the Portuguese and Spanish trade in the Indian Ocean declined primarily because they had become hugely over-stretched financially due to the limitations on their investment capacity and contemporary naval technology. Both of these factors worked in tandem, making control over Indian Ocean trade extremely expensive.

The existing Portuguese interests in Asia proved sufficient to finance further colonial expansion and entrenchment in areas regarded as of greater strategic importance in Africa and Brazil. Portuguese maritime supremacy was lost to the Dutch in the 17th century, and with this came serious challenges for the Portuguese. However, they still clung to Macau, and settled a new colony on the island of Timor. It was as recent as the 1960s and 1970s that the Portuguese began to relinquish their colonies in Asia. Goa was invaded by India in 1961 and became an Indian state in 1987; Portuguese Timor was abandoned in 1975 and was then invaded by Indonesia. It became an independent country in 2002; and Macau was handed back to the Chinese as per a treaty in 1999.

The arrival of the Portuguese and Spanish and their holy wars against Muslim states in the Malayan–Portuguese war, Spanish–Moro conflict and Castilian War inflamed religious tensions and turned Southeast Asia into an arena of conflict between Muslims and Christians. The Brunei Sultanate's capital at Kota Batu was assaulted by Governor Sande who led the 1578 Spanish attack.

The word "savages" in Spanish, cafres, was from the word "infidel" in Arabic - Kafir, and was used by the Spanish to refer to their own "christian savages" who were arrested in Brunei. It was said "Castilians are kafir, men who have no souls, who are condemned by fire when they die, and that too because they eat pork" by the Brunei Sultan after the term "accursed doctrine" was used to attack Islam by the Spaniards which fed into hatred between Muslims and Christians sparked by their 1571 war against Brunei. The Sultan's words were in response to insults coming from the Spanish at Manila in 1578, other Muslims from Champa, Java, Borneo, Luzon, Pahang, Demak, Aceh, and the Malays echoed the rhetoric of holy war against the Spanish and Iberian Portuguese, calling them kafir enemies which was a contrast to their earlier nuanced views of the Portuguese in the Hikayat Tanah Hitu and Sejarah Melayu. The war by Spain against Brunei was defended in an apologia written by Doctor De Sande. The British eventually partitioned and took over Brunei while Sulu was attacked by the British, Americans, and Spanish which caused its breakdown and downfall after both of them thrived from 1500-1900 for four centuries. Dar al-Islam was seen as under invasion by "kafirs" by the Atjehnese led by Zayn al-din and by Muslims in the Philippines as they saw the Spanish invasion, since the Spanish brought the idea of a crusader holy war against Muslim Moros just as the Portuguese did in Indonesia and India against what they called "Moors" in their political and commercial conquests which they saw through the lens of religion in the 16th century.

In 1578 an attack was launched by the Spanish against Jolo, and in 1875 it was destroyed at their hands, and once again in 1974 it was destroyed by the Philippines. The Spanish first set foot on Borneo in Brunei.

The Spanish war against Brunei failed to conquer Brunei but it totally cut off the Philippines from Brunei's influence, the Spanish then started colonizing Mindanao and building fortresses. In response, the Bisayas, where Spanish forces were stationed, were subjected to retaliatory attacks by the Magindanao in 1599-1600 due to the Spanish attacks on Mindanao.

The Brunei royal family was related to the Muslim Rajahs who in ruled the principality in 1570 of Manila (Kingdom of Maynila) and this was what the Spaniards came across on their initial arrival to Manila, Spain uprooted Islam out of areas where it was shallow after they began to force Christianity on the Philippines in their conquests after 1521 while Islam was already widespread in the 16th century Philippines. In the Philippines in the Cebu islands the natives killed the Spanish fleet leader Magellan. Borneo's western coastal areas at Landak, Sukadana, and Sambas saw the growth of Muslim states in the sixteenth century, in the 15th century at Nanking, the capital of China, the death and burial of the Borneo Bruneian king Maharaja Kama took place upon his visit to China with Zheng He's fleet.

The Spanish were expelled from Brunei in 1579 after they attacked in 1578. There were half a hundred thousand inhabitants before the 1597 attack by the Spanish in Brunei.

During first contact with China, numerous aggressions and provocations were undertaken by the Portuguese They believed they could mistreat the non-Christians because they themselves were Christians and acted in the name of their religion in committing crimes and atrocities. This resulted in the Battle of Xicaowan where the local Chinese navy defeated and captured a fleet of Portuguese caravels.

Portuguese decline in Asia was accelerated by the attacks on their commercial empire by the Dutch and the English, which began a global struggle over empire in Asia that lasted until the end of the Seven Years' War in 1763. The Netherlands revolt against Spanish rule facilitated Dutch encroachment of the Portuguese monopoly over South and East Asian trade. The Dutch looked on Spain's trade and colonies as potential spoils in war. When the two crowns of the Iberian peninsula were joined in 1581, the Dutch felt free to attack Portuguese territories in Asia.

By the 1590s, a number of Dutch companies were formed to finance trading expeditions in Asia. Because competition lowered their profits, and because of the doctrines of mercantilism, in 1602 the companies united into a cartel and formed the Dutch East India Company, and received from the government the right to trade and colonise territory in the area stretching from the Cape of Good Hope eastward to the Strait of Magellan.

In 1605, armed Dutch merchants captured the Portuguese fort at Amboyna in the Moluccas, which was developed into the first secure base of the company. Over time, the Dutch gradually consolidated control over the great trading ports of the East Indies. Control over the East Indies trading ports allowed the company to monopolise the world spice trade for decades. Their monopoly over the spice trade became complete after they drove the Portuguese from Malacca in 1641 and Ceylon in 1658.
Dutch East India Company colonies or outposts were later established in Atjeh (Aceh), 1667; Macassar, 1669; and Bantam, 1682. The company established its headquarters at Batavia (today Jakarta) on the island of Java. Outside the East Indies, the Dutch East India Company colonies or outposts were also established in Persia (Iran), Bengal (now Bangladesh and part of India), Mauritius (1638-1658/1664-1710), Siam (now Thailand), Guangzhou (Canton, China), Taiwan (1624–1662), and southern India (1616–1795).

Ming dynasty China defeated the Dutch East India Company in the Sino-Dutch conflicts. The Chinese first defeated and drove the Dutch out of the Pescadores in 1624. The Ming navy under Zheng Zhilong defeated the Dutch East India Company's fleet at the 1633 Battle of Liaoluo Bay. In 1662, Zheng Zhilong's son Zheng Chenggong (also known as Koxinga) expelled the Dutch from Taiwan after defeating them in the Siege of Fort Zeelandia. ("see" History of Taiwan) Further, the Dutch East India Company trade post on Dejima (1641–1857), an artificial island off the coast of Nagasaki, was for a long time the only place where Europeans could trade with Japan.

The Vietnamese Nguyễn lords defeated the Dutch in a naval battle in 1643.

The Cambodians defeated the Dutch in the Cambodian–Dutch War in 1644.

In 1652, Jan van Riebeeck established an outpost at the Cape of Good Hope (the southwestern tip of Africa, currently in South Africa) to restock company ships on their journey to East Asia. This post later became a fully-fledged colony, the Cape Colony (1652–1806). As Cape Colony attracted increasing Dutch and European settlement, the Dutch founded the city of Kaapstad (Cape Town).

By 1669, the Dutch East India Company was the richest private company in history, with a huge fleet of merchant ships and warships, tens of thousands of employees, a private army consisting of thousands of soldiers, and a reputation on the part of its stockholders for high dividend payments.

The company was in almost constant conflict with the English; relations were particularly tense following the Amboyna Massacre in 1623. During the 18th century, Dutch East India Company possessions were increasingly focused on the East Indies. After the fourth war between the United Provinces and England (1780–1784), the company suffered increasing financial difficulties. In 1799, the company was dissolved, commencing official colonisation of the East Indies. During the era of New Imperialism the territorial claims of the Dutch East India Company (VOC) expanded into a fully fledged colony named the Dutch East Indies. Partly driven by re-newed colonial aspirations of fellow European nation states the Dutch strived to establish unchallenged control of the archipelago now known as Indonesia.

Six years into formal colonisation of the East Indies, in Europe the Dutch Republic was occupied by the French forces of Napoleon. The Dutch government went into exile in England and formally ceded its colonial possessions to Great Britain. The pro-French Governor General of Java Jan Willem Janssens, resisted a British invasion force in 1811 until forced to surrender. British Governor Raffles, who the later founded the city of Singapore, ruled the colony the following 10 years of the British interregnum (1806–1816).

After the defeat of Napoleon and the Anglo-Dutch Treaty of 1814 colonial government of the East Indies was ceded back to the Dutch in 1817. The loss of South Africa and the continued scramble for Africa stimulated the Dutch to secure unchallenged dominion over its colony in the East Indies. The Dutch started to consolidate its power base through extensive military campaigns and elaborate diplomatic alliances with indigenous rulers ensuring the Dutch tricolor was firmly planted in all corners of the Archipelago. These military campaigns included: the Padri War (1821–1837), the Java War (1825–1830) and the Aceh War (1873–1904). This raised the need for a considerable military buildup of the colonial army (KNIL). From all over Europe soldiers were recruited to join the KNIL.

The Dutch concentrated their colonial enterprise in the Dutch East Indies (Indonesia) throughout the 19th century. The Dutch lost control over the East Indies to the Japanese during much of World War II. Following the war, the Dutch fought Indonesian independence forces after Japan surrendered to the Allies in 1945. In 1949 most of what was known as the Dutch East Indies was ceded to the independent Republic of Indonesia. In 1962 also Dutch New Guinea was annexed by Indonesia de facto ending Dutch imperialism in Asia.

The English sought to stake out claims in India at the expense of the Portuguese dating back to the Elizabethan era. In 1600, Queen Elizabeth I incorporated the English East India Company (later the British East India Company), granting it a monopoly of trade from the Cape of Good Hope eastward to the Strait of Magellan. In 1639 it acquired Madras on the east coast of India, where it quickly surpassed Portuguese Goa as the principal European trading centre on the Indian Subcontinent.

Through bribes, diplomacy, and manipulation of weak native rulers, the company prospered in India, where it became the most powerful political force, and outrivaled its Portuguese and French competitors. For more than one hundred years, English and French trading companies had fought one another for supremacy, and, by the middle of the 18th century, competition between the British and the French had heated up. French defeat by the British under the command of Robert Clive during the Seven Years' War (1756–1763) marked the end of the French stake in India.

The British East India Company, although still in direct competition with French and Dutch interests until 1763, was able to extend its control over almost the whole of India in the century following the subjugation of Bengal at the 1757 Battle of Plassey. The British East India Company made great advances at the expense of a Mughal dynasty.

The reign of Aurangzeb had marked the height of Mughal power, By 1690. Mughal territorial expansion reached its greatest extent, Aurangzeb's Empire encompassed the entire Indian Subcontinent. But this period of power was followed by one of decline. Fifty years after the death of Aurangzeb, the great Mughal empire had crumbled. Meanwhile, marauding warlords, nobles, and others bent on gaining power left the Subcontinent increasingly anarchic. Although the Mughals kept the imperial title until 1858, the central government had collapsed, creating a power vacuum.

Aside from defeating the French during the Seven Years' War, Robert Clive, the leader of the Company in India, defeated a key Indian ruler of Bengal at the decisive Battle of Plassey (1757), a victory that ushered in the beginning of a new period in Indian history, that of informal British rule. While still nominally the sovereign, the Mughal Indian emperor became more and more of a puppet ruler, and anarchy spread until the company stepped into the role of policeman of India. The transition to formal imperialism, characterised by Queen Victoria being crowned "Empress of India" in the 1870s was a gradual process. The first step toward cementing formal British control extended back to the late 18th century. The British Parliament, disturbed by the idea that a great business concern, interested primarily in profit, was controlling the destinies of millions of people, passed acts in 1773 and 1784 that gave itself the power to control company policies and to appoint the highest company official in India, the Governor-General. (This system of dual control lasted until 1858.) By 1818 the East India Company was master of all of India. Some local rulers were forced to accept its overlordship; others were deprived of their territories. Some portions of India were administered by the British directly; in others native dynasties were retained under British supervision.
Until 1858, however, much of India was still officially the dominion of the Mughal emperor. Anger among some social groups, however, was seething under the governor-generalship of James Dalhousie (1847–1856), who annexed the Punjab (1849) after victory in the Second Sikh War, annexed seven princely states using the doctrine of lapse, annexed the key state of Oudh on the basis of misgovernment, and upset cultural sensibilities by banning Hindu practices such as sati.

The 1857 Sepoy Rebellion, or Indian Mutiny, an uprising initiated by Indian troops, called sepoys, who formed the bulk of the Company's armed forces, was the key turning point. Rumour had spread among them that their bullet cartridges were lubricated with pig and cow fat. The cartridges had to be bit open, so this upset the Hindu and Muslim soldiers. The Hindu religion held cows sacred, and for Muslims pork was considered haraam. In one camp, 85 out of 90 sepoys would not accept the cartridges from their garrison officer. The British harshly punished those who would not by jailing them. The Indian people were outraged, and on May 10, 1857, sepoys marched to Delhi, and, with the help of soldiers stationed there, captured it. Fortunately for the British, many areas remained loyal and quiescent, allowing the revolt to be crushed after fierce fighting. One important consequence of the revolt was the final collapse of the Mughal dynasty. The mutiny also ended the system of dual control under which the British government and the British East India Company shared authority. The government relieved the company of its political responsibilities, and in 1858, after 258 years of existence, the company relinquished its role. Trained civil servants were recruited from graduates of British universities, and these men set out to rule India. Lord Canning (created earl in 1859), appointed Governor-General of India in 1856, became known as "Clemency Canning" as a term of derision for his efforts to restrain revenge against the Indians during the Indian Mutiny. When the Government of India was transferred from the Company to the Crown, Canning became the first viceroy of India.

The Company initiated the first of the Anglo-Burmese wars in 1824, which led to total annexation of Burma by the Crown in 1885. The British ruled Burma as a province of British India until 1937, then administered her separately under the Burma Office except during the Japanese occupation of Burma, 1942–1945, until granted independence on 4 January 1948. (Unlike India, Burma opted not to join the Commonwealth of Nations.)

The denial of equal status to Indians was the immediate stimulus for the formation in 1885 of the Indian National Congress, initially loyal to the Empire but committed from 1905 to increased self-government and by 1930 to outright independence. The "Home charges", payments transferred from India for administrative costs, were a lasting source of nationalist grievance, though the flow declined in relative importance over the decades to independence in 1947.

Although majority Hindu and minority Muslim political leaders were able to collaborate closely in their criticism of British policy into the 1920s, British support for a distinct Muslim political organisation, the Muslim League from 1906 and insistence from the 1920s on separate electorates for religious minorities, is seen by many in India as having contributed to Hindu-Muslim discord and the country's eventual Partition.

France, which had lost its empire to the British by the end of the 18th century, had little geographical or commercial basis for expansion in Southeast Asia. After the 1850s, French imperialism was initially impelled by a nationalistic need to rival the United Kingdom and was supported intellectually by the notion that French culture was superior to that of the people of Annam (Vietnam), and its "mission civilisatrice"—or its "civilizing mission" of the Annamese through their assimilation to French culture and the Catholic religion. The pretext for French expansionism in Indochina was the protection of French religious missions in the area, coupled with a desire to find a southern route to China through Tonkin, the European name for a region of northern Vietnam.

French religious and commercial interests were established in Indochina as early as the 17th century, but no concerted effort at stabilizing the French position was possible in the face of British strength in the Indian Ocean and French defeat in Europe at the beginning of the 19th century. A mid-19th century religious revival under the Second Empire provided the atmosphere within which interest in Indochina grew. Anti-Christian persecutions in the Far East provided the pretext for the bombardment of Tourane (Danang) in 1847, and invasion and occupation of Danang in 1857 and Saigon in 1858. Under Napoleon III, France decided that French trade with China would be surpassed by the British, and accordingly the French joined the British against China in the Second Opium War from 1857 to 1860, and occupied parts of Vietnam as its gateway to China.

By the Treaty of Saigon in 1862, on June 5, the Vietnamese emperor ceded France three provinces of southern Vietnam to form the French colony of Cochinchina; France also secured trade and religious privileges in the rest of Vietnam and a protectorate over Vietnam's foreign relations. Gradually French power spread through exploration, the establishment of protectorates, and outright annexations. Their seizure of Hanoi in 1882 led directly to war with China (1883–1885), and the French victory confirmed French supremacy in the region. France governed Cochinchina as a direct colony, and central and northern Vietnam under the protectorates of Annam and Tonkin, and Cambodia as protectorates in one degree or another. Laos too was soon brought under French "protection".

By the beginning of the 20th century, France had created an empire in Indochina nearly 50 percent larger than the mother country. A Governor-General in Hanoi ruled Cochinchina directly and the other regions through a system of residents. Theoretically, the French maintained the precolonial rulers and administrative structures in Annam, Tonkin, Cochinchina, Cambodia, and Laos, but in fact the governor-generalship was a centralised fiscal and administrative regime ruling the entire region. Although the surviving native institutions were preserved in order to make French rule more acceptable, they were almost completely deprived of any independence of action. The ethnocentric French colonial administrators sought to assimilate the upper classes into France's "superior culture." While the French improved public services and provided commercial stability, the native standard of living declined and precolonial social structures eroded. Indochina, which had a population of over eighteen million in 1914, was important to France for its tin, pepper, coal, cotton, and rice. It is still a matter of debate, however, whether the colony was commercially profitable.

Tsarist Russia is not often regarded as a colonial power such as the United Kingdom or France because of the manner of Russian expansions: unlike the United Kingdom, which expanded overseas, the Russian empire grew from the centre outward by a process of accretion, like the United States. In the 19th century, Russian expansion took the form of a struggle of an effectively landlocked country for access to a warm water port.

Qing China defeated Russia in the Sino-Russian border conflicts.

While the British were consolidating their hold on India, Russian expansion had moved steadily eastward to the Pacific, then toward the Middle East. In the early 19th century it succeeded in conquering the South Caucasus and Dagestan from Qajar Persia following the Russo-Persian War (1804–13), the Russo-Persian War (1826–28) and the out coming treaties of Gulistan and Turkmenchay, giving Russia direct borders with both Persia's as well as Ottoman Turkey's heartlands. Later, they eventually reached the frontiers of Afghanistan as well (which had the largest foreign border adjacent to British holdings in India). In response to Russian expansion, the defense of India's land frontiers and the control of all sea approaches to the Subcontinent via the Suez Canal, the Red Sea, and the Persian Gulf became preoccupations of British foreign policy in the 19th century.

Anglo-Russian rivalry in the Middle East and Central Asia led to a brief confrontation over Afghanistan in the 1870s. In Persia (Iran), both nations set up banks to extend their economic influence. The United Kingdom went so far as to invade Tibet, a land subordinate to the Chinese empire, in 1904, but withdrew when it became clear that Russian influence was insignificant and when Chinese resistance proved tougher than expected.

In 1907, the United Kingdom and Russia signed an agreement which — on the surface —ended their rivalry in Central Asia. ("see" Anglo-Russian Entente) As part of the entente, Russia agreed to deal with the sovereign of Afghanistan only through British intermediaries. In turn, the United Kingdom would not annex or occupy Afghanistan. Chinese suzerainty over Tibet also was recognised by both Russia and the United Kingdom, since nominal control by a weak China was preferable to control by either power. Persia was divided into Russian and British spheres of influence and an intervening "neutral" zone. The United Kingdom and Russia chose to reach these uneasy compromises because of growing concern on the part of both powers over German expansion in strategic areas of China and Africa.

Following the entente, Russia increasingly intervened in Persian domestic politics and suppressed nationalist movements that threatened both St. Petersburg and London. After the Russian Revolution, Russia gave up its claim to a sphere of influence, though Soviet involvement persisted alongside the United Kingdom's until the 1940s.

In the Middle East, in Persia (Iran) and the Ottoman Empire, a German company built a railroad from Constantinople to Baghdad and the Persian Gulf in the latter, while it built a railroad from the north of the country to the south, connecting the Caucasus with the Persian Gulf in the former. Germany wanted to gain economic influence in the region and then, perhaps, move on to India. This was met with bitter resistance by the United Kingdom, Russia, and France who divided the region among themselves.

The 16th century brought many Jesuit missionaries to China, such as Matteo Ricci, who established missions where Western science was introduced, and where Europeans gathered knowledge of Chinese society, history, culture, and science. During the 18th century, merchants from Western Europe came to China in increasing numbers. However, merchants were confined to Guangzhou and the Portuguese colony of Macau, as they had been since the 16th century. European traders were increasingly irritated by what they saw as the relatively high customs duties they had to pay and by the attempts to curb the growing import trade in opium. By 1800, its importation was forbidden by the imperial government. However, the opium trade continued to boom.

Early in the 19th century, serious internal weaknesses developed in the Qing dynasty that left China vulnerable to Western, Meiji period Japanese, and Russian imperialism. In 1839, China found itself fighting the First Opium War with Britain. China was defeated, and in 1842, signed the provisions of the Treaty of Nanjing which were first of the unequal treaties signed during the Qing Dynasty. Hong Kong Island was ceded to Britain, and certain ports, including Shanghai and Guangzhou, were opened to British trade and residence. In 1856, the Second Opium War broke out. The Chinese were again defeated, and now forced to the terms of the 1858 Treaty of Tientsin. The treaty opened new ports to trade and allowed foreigners to travel in the interior. In addition, Christians gained the right to propagate their religion. The United States Treaty of Wanghia and Russia later obtained the same prerogatives in separate treaties.

Toward the end of the 19th century, China appeared on the way to territorial dismemberment and economic vassalage—the fate of India's rulers that played out much earlier. Several provisions of these treaties caused long-standing bitterness and humiliation among the Chinese: extraterritoriality (meaning that in a dispute with a Chinese person, a Westerner had the right to be tried in a court under the laws of his own country), customs regulation, and the right to station foreign warships in Chinese waters, including its navigable rivers.

Jane E. Elliott criticized the allegation that China refused to modernize or was unable to defeat Western armies as simplistic, noting that China embarked on a massive military modernization in the late 1800s after several defeats, buying weapons from Western countries and manufacturing their own at arsenals, such as the Hanyang Arsenal during the Boxer Rebellion. In addition, Elliott questioned the claim that Chinese society was traumatized by the Western victories, as many Chinese peasants (90% of the population at that time) living outside the concessions continued about their daily lives, uninterrupted and without any feeling of "humiliation".

Historians have judged the Qing dynasty's vulnerability and weakness to foreign imperialism in the 19th century to be based mainly on its maritime naval weakness while it achieved military success against westerners on land, the historian Edward L. Dreyer said that "China’s nineteenth-century humiliations were strongly related to her weakness and failure at sea. At the start of the Opium War, China had no unified navy and no sense of how vulnerable she was to attack from the sea; British forces sailed and steamed wherever they wanted to go...In the Arrow War (1856-60), the Chinese had no way to prevent the Anglo-French expedition of 1860 from sailing into the Gulf of Zhili and landing as near as possible to Beijing. Meanwhile, new but not exactly modern Chinese armies suppressed the midcentury rebellions, bluffed Russia into a peaceful settlement of disputed frontiers in Central Asia, and defeated the French forces on land in the Sino-French War (1884-85). But the defeat of the fleet, and the resulting threat to steamship traffic to Taiwan, forced China to conclude peace on unfavorable terms."

During the Sino-French War, Chinese forces defeated the French at the Battle of Cầu Giấy (Paper Bridge), Bắc Lệ ambush, Battle of Phu Lam Tao, Battle of Zhenhai, the Battle of Tamsui in the Keelung Campaign and in the last battle which ended the war, the Battle of Bang Bo (Zhennan Pass), which triggered the French Retreat from Lạng Sơn and resulted in the collapse of the French Jules Ferry government in the Tonkin Affair.

The Qing dynasty forced Russia to hand over disputed territory in Ili in the Treaty of Saint Petersburg (1881), in what was widely seen by the west as a diplomatic victory for the Qing. Russia acknowledged that Qing China potentially posed a serious military threat. Mass media in the west during this era portrayed China as a rising military power due to its modernization programs and as a major threat to the western world, invoking fears that China would successfully conquer western colonies like Australia.

The British observer Demetrius Charles de Kavanagh Boulger suggested a British-Chinese alliance to check Russian expansion in Central Asia.

During the Ili crisis when Qing China threatened to go to war against Russia over the Russian occupation of Ili, the British officer Charles George Gordon was sent to China by Britain to advise China on military options against Russia should a potential war break out between China and Russia.

The Russians observed the Chinese building up their arsenal of modern weapons during the Ili crisis, the Chinese bought thousands of rifles from Germany. In 1880 massive amounts of military equipment and rifles were shipped via boats to China from Antwerp as China purchased torpedoes, artillery, and 260,260 modern rifles from Europe.

The Russian military observer D. V. Putiatia visited China in 1888 and found that in Northeastern China (Manchuria) along the Chinese-Russian border, the Chinese soldiers were potentially able to become adept at "European tactics" under certain circumstances, and the Chinese soldiers were armed with modern weapons like Krupp artillery, Winchester carbines, and Mauser rifles.

Compared to Russian controlled areas, more benefits were given to the Muslim Kirghiz on the Chinese controlled areas. Russian settlers fought against the Muslim nomadic Kirghiz, which led the Russians to believe that the Kirghiz would be a liability in any conflict against China. The Muslim Kirghiz were sure that in an upcoming war, that China would defeat Russia.

Russian sinologists, the Russian media, threat of internal rebellion, the pariah status inflicted by the Congress of Berlin, the negative state of the Russian economy all led Russia to concede and negotiate with China in St Petersburg, and return most of Ili to China.
The rise of Japan since the Meiji Restoration as an imperial power led to further subjugation of China. In a dispute over China's longstanding claim of suzerainty in Korea, war broke out between China and Japan, resulting in humiliating defeat for the Chinese. By the Treaty of Shimonoseki (1895), China was forced to recognize effective Japanese rule of Korea and Taiwan was ceded to Japan until its recovery in 1945 at the end of the WWII by the Republic of China.

China's defeat at the hands of Japan was another trigger for future aggressive actions by Western powers. In 1897, Germany demanded and was given a set of exclusive mining and railroad rights in Shandong province. Russia obtained access to Dairen and Port Arthur and the right to build a railroad across Manchuria, thereby achieving complete domination over a large portion of northwestern China. The United Kingdom and France also received a number of concessions. At this time, much of China was divided up into "spheres of influence": Germany dominated Jiaozhou (Kiaochow) Bay, Shandong, and the Yellow River valley; Russia dominated the Liaodong Peninsula and Manchuria; the United Kingdom dominated Weihaiwei and the Yangtze Valley; and France dominated the Guangzhou Bay and several other southern provinces.
China continued to be divided up into these spheres until the United States, which had no sphere of influence, grew alarmed at the possibility of its businessmen being excluded from Chinese markets. In 1899, Secretary of State John Hay asked the major powers to agree to a policy of equal trading privileges. In 1900, several powers agreed to the U.S.-backed scheme, giving rise to the "Open Door" policy, denoting freedom of commercial access and non-annexation of Chinese territory. In any event, it was in the European powers' interest to have a weak but independent Chinese government. The privileges of the Europeans in China were guaranteed in the form of treaties with the Qing government. In the event that the Qing government totally collapsed, each power risked losing the privileges that it already had negotiated.

The erosion of Chinese sovereignty and seizures of land from Chinese by foreigners contributed to a spectacular anti-foreign outbreak in June 1900, when the "Boxers" (properly the society of the "righteous and harmonious fists") attacked foreigners around Beijing. The Imperial Court was divided into anti-foreign and pro-foreign factions, with the pro-foreign faction led by Ronglu and Prince Qing hampering any military effort by the anti-foreign faction led by Prince Duan and Dong Fuxiang. The Qing Empress Dowager ordered all diplomatic ties to be cut off and all foreigners to leave the legations in Beijing to go to Tianjin. The foreigners refused to leave. Fueled by entirely false reports that the foreigners in the legations were massacred, the Eight-Nation Alliance decided to launch an expedition on Beijing to reach the legations but they underestimated the Qing military. The Qing and Boxers defeated the foreigners at the Seymour Expedition, forcing them to turn back at the Battle of Langfang. In response to the foreign attack on Dagu Forts the Qing responded by declaring war against the foreigners. the Qing forces and foreigners fought a fierce battle at the Battle of Tientsin before the foreigners could launch a second expedition. On their second try Gaselee Expedition, with a much larger force, the foreigners managed to reach Beijing and fight the Battle of Peking (1900). British and French forces looted, plundered and burned the Old Summer Palace to the ground for the second time (the first time being in 1860, following the Second Opium War). German forces were particularly severe in exacting revenge for the killing of their ambassador due to the orders of Kaiser Wilhelm II, who held anti-Asian sentiments, while Russia tightened its hold on Manchuria in the northeast until its crushing defeat by Japan in the war of 1904–1905. The Qing court evacuated to Xi'an and threatened to continue the war against foreigners, until the foreigners tempered their demands in the Boxer Protocol, promising that China would not have to give up any land and gave up the demands for the execution of Dong Fuxiang and Prince Duan.

The correspondent Douglas Story observed Chinese troops in 1907 and praised their abilities and military skill.

Extraterritorial jurisdiction was abandoned by the United Kingdom and the United States in 1943. Chiang Kai-shek forced the French to hand over all their concessions back to China control after World War II. Foreign political control over leased parts of China ended with the incorporation of Hong Kong and the small Portuguese territory of Macau into the People's Republic of China in 1997 and 1999 respectively.

Some Americans in the Nineteenth Century advocated for the annexation of Taiwan from China. Aboriginals on Taiwan often attacked and massacred shipwrecked western sailors. In 1867, during the Rover incident, Taiwanese aborigines attacked shipwrecked American sailors, killing the entire crew. They subsequently defeated a retaliatory expedition by the American military and killed another American during the battle.

As the United States emerged as a new imperial power in the Pacific and Asia, one of the two oldest Western imperialist powers in the regions, Spain, was finding it increasingly difficult to maintain control of territories it had held in the regions since the 16th century. In 1896, a widespread revolt against Spanish rule broke out in the Philippines. Meanwhile, the recent string of U.S. territorial gains in the Pacific posed an even greater threat to Spain's remaining colonial holdings.

As the U.S. continued to expand its economic and military power in the Pacific, it declared war against Spain in 1898. During the Spanish–American War, U.S. Admiral Dewey destroyed the Spanish fleet at Manila and U.S. troops landed in the Philippines. Spain later agreed by treaty to cede the Philippines in Asia and Guam in the Pacific. In the Caribbean, Spain ceded Puerto Rico to the U.S. The war also marked the end of Spanish rule in Cuba, which was to be granted nominal independence but remained heavily influenced by the U.S. government and U.S. business interests. One year following its treaty with Spain, the U.S. occupied the small Pacific outpost of Wake Island.

The Filipinos, who assisted U.S. troops in fighting the Spanish, wished to establish an independent state and, on June 12, 1898, declared independence from Spain. In 1899, fighting between the Filipino nationalists and the U.S. broke out; it took the U.S. almost fifteen years to fully subdue the insurgency. The U.S. sent 70,000 troops and suffered thousands of casualties. The Filipinos insurgents, however, suffered considerably higher casualties than the Americans. Most casualties in the war were civilians dying primarily from disease.

U.S. attacks into the countryside often included scorched earth campaigns where entire villages were burned and destroyed, and concentrated civilians into camps known as "protected zones." Most of these civilian casualties resulted from disease and famine. Reports of the execution of U.S. soldiers taken prisoner by the Filipinos led to disproportionate reprisals by American forces.

The Moro Muslims fought against the Americans in the Moro Rebellion.

In 1914, Dean C. Worcester, U.S. Secretary of the Interior for the Philippines (1901–1913) described "the regime of civilisation and improvement which started with American occupation and resulted in developing naked savages into cultivated and educated men." Nevertheless, some Americans, such as Mark Twain, deeply opposed American involvement/imperialism in the Philippines, leading to the abandonment of attempts to construct a permanent U.S. naval base and using it as an entry point to the Chinese market. In 1916, Congress guaranteed the independence of the Philippines by 1945.

World War I brought about the fall of several empires in Europe. This had repercussions around the world. The defeated Central Powers included Germany and the Turkish Ottoman Empire. Germany lost all of its colonies in Asia. German New Guinea, a part of Papua New Guinea, became administered by Australia. German possessions and concessions in China, including Qingdao, became the subject of a controversy during the Paris Peace Conference when the Beiyang government in China agreed to cede these interests to Japan, to the anger of many Chinese people. Although the Chinese diplomats refused to sign the agreement, these interests were ceded to Japan with the support of the United States and the United Kingdom.

Turkey gave up her provinces; Syria, Palestine, and Mesopotamia (now Iraq) came under French and British control as League of Nations Mandates. The discovery of petroleum first in Iran and then in the Arab lands in the interbellum provided a new focus for activity on the part of the United Kingdom, France, and the United States.

In 1641, all Westerners were thrown out of Japan. For the next two centuries, Japan was free from Western influence, except for at the port of Nagasaki, which Japan allowed Dutch merchant vessels to enter on a limited basis.

Japan's freedom from Western penetration ended on 8 July 1853, when Commodore Matthew Perry of the U.S. Navy sailed a squadron of black-hulled warships into Edo (modern Tokyo) harbor. The Japanese told Perry to sail to Nagasaki but he refused. Perry sought to present a letter from U.S. President Millard Fillmore to the emperor which demanded concessions from Japan. Japanese authorities responded by stating that they could not present the letter directly to the emperor, but scheduled a meeting on July 14 with a representative of the emperor. On 14 July, the squadron sailed towards the shore, giving a demonstration of their cannon's firepower thirteen times. Perry landed with a large detachment of Marines and presented the emperor's representative with Fillmore's letter. Perry said he would return, and did so, this time with even more war ships. The U.S. show of force led to Japan's concession to the Convention of Kanagawa on 31 March 1854. This treaty conferred extraterritoriality on American nationals, as well as, opening up further treaty ports beyond Nagasaki. This treaty was followed up by similar treaties with the United Kingdom, the Netherlands, Russia and France. These events made Japanese authorities aware that the country was lacking technologically and needed the strength of industrialism in order to keep their power. This realisation eventually led to a civil war and political reform known the Meiji Restoration.

The Meiji Restoration of 1868 led to administrative overhaul, deflation and subsequent rapid economic development. Japan had limited natural resources of her own and sought both overseas markets and sources of raw materials, fuelling a drive for imperial conquest which began with the defeat of China in 1895.
Taiwan, ceded by Qing Dynasty China, became the first Japanese colony. In 1899, Japan won agreements from the great powers' to abandon extraterritoriality for their citizens, and an alliance with the United Kingdom established it in 1902 as an international power. Its spectacular defeat of Russia's navy in 1905 gave it the southern half of the island of Sakhalin; exclusive Japanese influence over Korea (propinquity); the former Russian lease of the Liaodong Peninsula with Port Arthur (Lüshunkou); and extensive rights in Manchuria (see the Russo-Japanese War).

The Empire of Japan and the Joseon Dynasty in Korea formed bilateral diplomatic relations in 1876. China lost its suzerainty of Korea after defeat in the Sino-Japanese War in 1894. Russia also lost influence on the Korean peninsula with the Treaty of Portsmouth as a result of the Russo-Japanese war in 1904. The Joseon Dynasty became increasingly dependent on Japan. Korea became a protectorate of Japan with the Japan–Korea Treaty of 1905. Korea was then "de jure" annexed to Japan with the Japan–Korea Treaty of 1910.

Japan was now one of the most powerful forces in the Far East, and in 1914, it entered World War I on the side of the Allies, seizing German-occupied Kiaochow and subsequently demanding Chinese acceptance of Japanese political influence and territorial acquisitions (Twenty-One Demands, 1915). Mass protests in Peking in 1919 coupled with Allied (and particularly U.S.) opinion led to Japan's abandonment of most of the demands and Joseon's 1922 return to China. Japan received the German territory from the Treaty of Versailles, 1919, sparking widespread Chinese nationalism.

Tensions with China increased over the 1920s, and in 1931 Japanese army units based in Manchuria seized control of the region without direction from Tokyo. Intermittent conflict with China led to full-scale war in mid-1937, drawing Japan toward an overambitious bid for Asian hegemony (Greater East Asia Co-Prosperity Sphere), which ultimately led to defeat and the loss of all its overseas territories after World War II (see Japanese expansionism and Japanese nationalism).

In the aftermath of World War II, European colonies, controlling more than one billion people throughout the world, still ruled most of the Middle East, South East Asia, and the Indian Subcontinent. However, the image of European pre-eminence was shattered by the wartime Japanese occupations of large portions of British, French, and Dutch territories in the Pacific. The destabilisation of European rule led to the rapid growth of nationalist movements in Asia—especially in Indonesia, Malaya, Burma, and French Indochina.
The war, however, only accelerated forces already in existence undermining Western imperialism in Asia. Throughout the colonial world, the processes of urbanisation and capitalist investment created professional merchant classes that emerged as new Westernised elites. While imbued with Western political and economic ideas, these classes increasingly grew to resent their unequal status under European rule.

In India, the westward movement of Japanese forces towards Bengal during World War II had led to major concessions on the part of British authorities to Indian nationalist leaders. In 1947, the United Kingdom, devastated by war and embroiled in economic crisis at home, granted British India its independence as two nations: India and Pakistan. The following year independence was granted to Burma and Ceylon. In the Middle East, the United Kingdom granted independence to Jordan in 1946 and two years later ended its mandate of Palestine.

Following the end of the war, nationalists in Indonesia demanded complete independence from the Netherlands. A brutal conflict ensued, and finally, in 1949, through United Nations mediation, the Dutch East Indies achieved independence, becoming the new nation of Indonesia. Dutch imperialism moulded this new multi-ethnic state comprising roughly 3,000 islands of the Indonesian archipelago with a population at the time of over 100 million.

The end of Dutch rule opened up latent tensions between the roughly 300 distinct ethnic groups of the islands, with the major ethnic fault line being between the Javanese and the non-Javanese.

Netherlands New Guinea was under the Dutch administration until 1962 (see also West New Guinea dispute).

In the Philippines, the U.S. remained committed to its previous pledges to grant the islands their independence, and the Philippines became the first of the Western-controlled Asian colonies to be granted independence post-World War II. However, the Philippines remained under pressure to adopt a political and economic system similar to their old imperial master.

This aim was greatly complicated by the rise of new political forces. During the war, the "Hukbalahap" (People's Army), which had strong ties to the Communist Party of the Philippines (PKP), fought against the Japanese occupation of the Philippines and won strong popularity among many sectors of the Filipino working class and peasantry. In 1946, the PKP participated in elections as part of the Democratic Alliance. However, with the onset of the Cold War, its growing political strength drew a reaction from the ruling government and the United States, resulting in the repression of the PKP and its associated organisations. In 1948, the PKP began organizing an armed struggle against the government and continued U.S. military presence. In 1950, the PKP created the People's Liberation Army ("Hukbong Mapagpalaya ng Bayan"), which mobilised thousands of troops throughout the islands. The insurgency lasted until 1956, when the PKP gave up armed struggle.

In 1968, the PKP underwent a split, and in 1969 the Maoist faction of the PKP created the New People's Army. Maoist rebels re-launched an armed struggle against the government and the U.S. military presence in the Philippines, which continues to this day.

France remained determined to retain its control of Indochina. However, in Hanoi, in 1945, a broad front of nationalists and communists led by Ho Chi Minh declared an independent Republic of Vietnam, commonly referred to as the Viet Minh regime by Western outsiders. France, seeking to regain control of Vietnam, countered with a vague offer of self-government under French rule. France's offers were unacceptable to Vietnamese nationalists; and in December 1946 the Việt Minh launched a rebellion against the French authority governing the colonies of French Indochina. The first few years of the war involved a low-level rural insurgency against French authority. However, after the Chinese communists reached the Northern border of Vietnam in 1949, the conflict turned into a conventional war between two armies equipped with modern weapons supplied by the United States and the Soviet Union. Meanwhile, the France granted the State of Vietnam based in Saigon independence in 1949 whilst Laos and Cambodia received independence in 1953. The US recognized the regime in Saigon, and provided the French military effort with military aid.

Meanwhile, in Vietnam, the French war against the Viet Minh continued for nearly eight years. The French were gradually worn down by guerrilla and jungle fighting. The turning point for France occurred at Dien Bien Phu in 1954, which resulted in the surrender of ten thousand French troops. Paris was forced to accept a political settlement that year at the Geneva Conference, which led to a precarious set of agreements regarding the future political status of Laos, Cambodia, and Vietnam.






</doc>
<doc id="15445" url="https://en.wikipedia.org/wiki?curid=15445" title="Entropy (information theory)">
Entropy (information theory)

Information entropy is the average rate at which information is produced by a stochastic source of data.

The measure of information entropy associated with each possible data value is the negative logarithm of the probability mass function for the value. Thus, when the data source has a lower-probability value (i.e., when a low-probability event occurs), the event carries more "information" ("surprisal") than when the source data has a higher-probability value. The amount of information conveyed by each event defined in this way becomes a random variable whose expected value is the information entropy. Generally, "entropy" refers to disorder or uncertainty, and the definition of entropy used in information theory is directly analogous to the definition used in statistical thermodynamics. The concept of information entropy was introduced by Claude Shannon in his 1948 paper "A Mathematical Theory of Communication".

The basic model of a data communication system is composed of three elements, a source of data, a communication channel, and a receiver, and – as expressed by Shannon – the "fundamental problem of communication" is for the receiver to be able to identify what data was generated by the source, based on the signal it receives through the channel. The entropy provides an absolute limit on the shortest possible average length of a lossless compression encoding of the data produced by a source, and if the entropy of the source is less than the channel capacity of the communication channel, the data generated by the source can be reliably communicated to the receiver (at least in theory, possibly neglecting some practical considerations such as the complexity of the system needed to convey the data and the amount of time it may take for the data to be conveyed).

Information entropy is typically measured in bits (alternatively called "shannons") or sometimes in "natural units" (nats) or decimal digits (called "dits", "bans", or "hartleys"). The unit of the measurement depends on the base of the logarithm that is used to define the entropy.

The logarithm of the probability distribution is useful as a measure of entropy because it is additive for independent sources. For instance, the entropy of a fair coin toss is 1 bit, and the entropy of tosses is bits. In a straightforward representation, bits are needed to represent a variable that can take one of values if is a power of 2. If these values are equally probable, the entropy (in bits) is equal to this number. If one of the values is more probable to occur than the others, an observation that this value occurs is less informative than if some less common outcome had occurred. Conversely, rarer events provide more information when observed. Since observation of less probable events occurs more rarely, the net effect is that the entropy (thought of as average information) received from non-uniformly distributed data is always less than or equal to . Entropy is zero when one outcome is certain to occur. The entropy quantifies these considerations when a probability distribution of the source data is known. The "meaning" of the events observed (the meaning of "messages") does not matter in the definition of entropy. Entropy only takes into account the probability of observing a specific event, so the information it encapsulates is information about the underlying probability distribution, not the meaning of the events themselves.

The basic idea of information theory is the more one knows about a topic, the less new information one is apt to get about it. If an event is very probable, it is no surprise when it happens and thus provides little new information. Inversely, if the event was improbable, it is much more informative that the event happened. Therefore, the information content is an increasing function of the inverse of the probability of the event (1/p). Now, if more events may happen, entropy measures the average information content you can expect to get if one of the events actually happens. This implies that casting a die has more entropy than tossing a coin because each outcome of the die has smaller probability than each outcome of the coin.

Thus, entropy is a measure of "unpredictability" of the state, or equivalently, of its "average information content". To get an intuitive understanding of these terms, consider the example of a political poll. Usually, such polls happen because the outcome of the poll is not already known. In other words, the outcome of the poll is relatively "unpredictable", and actually performing the poll and learning the results gives some new "information"; these are just different ways of saying that the "a priori" entropy of the poll results is large. Now, consider the case that the same poll is performed a second time shortly after the first poll. Since the result of the first poll is already known, the outcome of the second poll can be predicted well and the results should not contain much new information; in this case the "a priori" entropy of the second poll result is small relative to that of the first.

Now consider the example of a coin toss. Assuming the probability of heads is the same as the probability of tails, then the entropy of the coin toss is as high as it could be. This is because there is no way to predict the outcome of the coin toss ahead of time: if we have to choose, the best we can do is predict that the coin will come up heads, and this prediction will be correct with probability 1/2. Such a coin toss has one bit of entropy since there are two possible outcomes that occur with equal probability, and learning the actual outcome contains one bit of information. In contrast, a coin toss using a coin that has two heads and no tails has zero entropy since the coin will always come up heads, and the outcome can be predicted perfectly. Analogously, one binary-outcome with equiprobable values has a Shannon entropy of formula_1 bit. Similarly, one trit with equiprobable values contains formula_2 (about 1.58496) bits of information because it can have one of three values.

English text, treated as a string of characters, has fairly low entropy, i.e., is fairly predictable. Even if we do not know exactly what is going to come next, we can be fairly certain that, for example, 'e' will be far more common than 'z', that the combination 'qu' will be much more common than any other combination with a 'q' in it, and that the combination 'th' will be more common than 'z', 'q', or 'qu'. After the first few letters one can often guess the rest of the word. English text has between 0.6 and 1.3 bits of entropy per character of the message.

If a compression scheme is lossless—that is, you can always recover the entire original message by decompressing—then a compressed message has the same quantity of information as the original, but communicated in fewer characters. That is, it has more information, or a higher entropy, per character. This means a compressed message has less redundancy. Roughly speaking, Shannon's source coding theorem says that a lossless compression scheme cannot compress messages, on average, to have "more" than one bit of information per bit of message, but that any value "less" than one bit of information per bit of message can be attained by employing a suitable coding scheme. The entropy of a message per bit multiplied by the length of that message is a measure of how much total information the message contains.

Intuitively, imagine that we wish to transmit sequences comprising the 4 characters 'A', 'B', 'C', and 'D'. Thus, a message to be transmitted might be 'ABADDCAB'. Information theory gives a way of calculating the smallest possible amount of information that will convey this. If all 4 letters are equally likely (25%), we can do no better (over a binary channel) than to have 2 bits encode (in binary) each letter: 'A' might code as '00', 'B' as '01', 'C' as '10', and 'D' as '11'. Now suppose 'A' occurs with 70% probability, 'B' with 26%, and 'C' and 'D' with 2% each. We could assign variable length codes, so that receiving a '1' tells us to look at another bit unless we have already received 2 bits of sequential 1s. In this case, 'A' would be coded as '0' (one bit), 'B' as '10', and 'C' and 'D' as '110' and '111'. It is easy to see that 70% of the time only one bit needs to be sent, 26% of the time two bits, and only 4% of the time 3 bits. On average, then, fewer than 2 bits are required since the entropy is lower (owing to the high prevalence of 'A' followed by 'B' – together 96% of characters). The calculation of the sum of probability-weighted log probabilities measures and captures this effect.

Shannon's theorem also implies that no lossless compression scheme can shorten "all" messages. If some messages come out shorter, at least one must come out longer due to the pigeonhole principle. In practical use, this is generally not a problem, because we are usually only interested in compressing certain types of messages, for example English documents as opposed to gibberish text, or digital photographs rather than noise, and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger. However, the problem can still arise even in everyday use when applying a compression algorithm to already compressed data: for example, making a ZIP file of music, pictures or videos that are already in a compressed format such as FLAC, MP3, WebM, AAC, PNG or JPEG will generally result in a ZIP file that is slightly "larger" than the source file(s).

Named after Boltzmann's Η-theorem, Shannon defined the entropy (Greek capital letter eta) of a discrete random variable with possible values } and probability mass function as:

Here formula_4 is the expected value operator, and is the information content of .

The entropy can explicitly be written as

where is the base of the logarithm used. Common values of are 2, Euler's number, and 10, and the corresponding units of entropy are the bits for , nats for , and bans for .

In the case of for some , the value of the corresponding summand is taken to be , which is consistent with the limit:

One may also define the conditional entropy of two events and taking values and respectively, as

where is the probability that and . This quantity should be understood as the amount of randomness in the random variable given the event .

Consider tossing a coin with known, not necessarily fair, probabilities of coming up heads or tails; this can be modelled as a Bernoulli process.

The entropy of the unknown result of the next toss of the coin is maximized if the coin is fair (that is, if heads and tails both have equal probability 1/2). This is the situation of maximum uncertainty as it is most difficult to predict the outcome of the next toss; the result of each toss of the coin delivers one full bit of information. This is because

However, if we know the coin is not fair, but comes up heads or tails with probabilities and , where , then there is less uncertainty. Every time it is tossed, one side is more likely to come up than the other. The reduced uncertainty is quantified in a lower entropy: on average each toss of the coin delivers less than one full bit of information. For example, if =0.7, then

The extreme case is that of a double-headed coin that never comes up tails, or a double-tailed coin that never results in a head. Then there is no uncertainty. The entropy is zero: each toss of the coin delivers no new information as the outcome of each coin toss is always certain.

Entropy can be normalized by dividing it by information length. This ratio is called metric entropy and is a measure of the randomness of the information.

To understand the meaning of , at first, try to define an information function, , in terms of an event with probability . How much information is acquired due to the observation of event ? Shannon's solution follows from the fundamental properties of information:

The last is a crucial property. It states that joint probability of independent sources of information communicates as much information as the two individual events separately. Particularly, if the first event can yield one of equiprobable outcomes and another has one of equiprobable outcomes then there are possible outcomes of the joint event. This means that if bits are needed to encode the first value and to encode the second, one needs to encode both. Shannon discovered that the proper choice of function to quantify information, preserving this additivity, is logarithmic, i.e.,

let formula_11 be the information function which one assumes to be twice continuously differentiable, one has:

formula_12

This differential equation leads to the solution formula_13 for any formula_14. Condition 2. leads to formula_15 and especially, formula_16 can be chosen on the form formula_17 with formula_18, which is equivalent to choosing a specific base for the logarithm. The different units of information (bits for , nats for the natural logarithm , bans for and so on) are just constant multiples of each other. For instance, in case of a fair coin toss, heads provides bit of information, which is approximately 0.693 nats or 0.301 decimal digits. Because of additivity, tosses provide bits of information, which is approximately nats or decimal digits.

Now, suppose we have a distribution where event can happen with probability . Suppose we have sampled it times and outcome was, accordingly, seen times. The total amount of information we have received is 
The "average" amount of information that we receive per event is therefore

The inspiration for adopting the word "entropy" in information theory came from the close resemblance between Shannon's formula and very similar known formulae from statistical mechanics.

In statistical thermodynamics the most general formula for the thermodynamic entropy of a thermodynamic system is the Gibbs entropy,
where is the Boltzmann constant, and is the probability of a microstate. The Gibbs entropy was defined by J. Willard Gibbs in 1878 after earlier work by Boltzmann (1872).

The Gibbs entropy translates over almost unchanged into the world of quantum physics to give the von Neumann entropy, introduced by John von Neumann in 1927,
where ρ is the density matrix of the quantum mechanical system and Tr is the trace.

At an everyday practical level the links between information entropy and thermodynamic entropy are not evident. Physicists and chemists are apt to be more interested in "changes" in entropy as a system spontaneously evolves away from its initial conditions, in accordance with the second law of thermodynamics, rather than an unchanging probability distribution. And, as the minuteness of Boltzmann's constant indicates, the changes in for even tiny amounts of substances in chemical and physical processes represent amounts of entropy that are extremely large compared to anything in data compression or signal processing. Furthermore, in classical thermodynamics the entropy is defined in terms of macroscopic measurements and makes no reference to any probability distribution, which is central to the definition of information entropy.

The connection between thermodynamics and what is now known as information theory was first made by Ludwig Boltzmann and expressed by his famous equation:

where "S" is the thermodynamic entropy of a particular macrostate (defined by thermodynamic parameters such as temperature, volume, energy, etc.), "W" is the number of microstates (various combinations of particles in various energy states) that can yield the given macrostate, and "k" is Boltzmann's constant. It is assumed that each microstate is equally likely, so that the probability of a given microstate is "p = 1/W". When these probabilities are substituted into the above expression for the Gibbs entropy (or equivalently "k" times the Shannon entropy), Boltzmann's equation results. In information theoretic terms, the information entropy of a system is the amount of "missing" information needed to determine a microstate, given the macrostate.

In the view of Jaynes (1957), thermodynamic entropy, as explained by statistical mechanics, should be seen as an "application" of Shannon's information theory: the thermodynamic entropy is interpreted as being proportional to the amount of further Shannon information needed to define the detailed microscopic state of the system, that remains uncommunicated by a description solely in terms of the macroscopic variables of classical thermodynamics, with the constant of proportionality being just the Boltzmann constant. For example, adding heat to a system increases its thermodynamic entropy because it increases the number of possible microscopic states of the system that are consistent with the measurable values of its macroscopic variables, thus making any complete state description longer. (See article: "maximum entropy thermodynamics"). Maxwell's demon can (hypothetically) reduce the thermodynamic entropy of a system by using information about the states of individual molecules; but, as Landauer (from 1961) and co-workers have shown, to function the demon himself must increase thermodynamic entropy in the process, by at least the amount of Shannon information he proposes to first acquire and store; and so the total thermodynamic entropy does not decrease (which resolves the paradox). Landauer's principle imposes a lower bound on the amount of heat a computer must generate to process a given amount of information, though modern computers are far less efficient.

Entropy is defined in the context of a probabilistic model. Independent fair coin flips have an entropy of 1 bit per flip. A source that always generates a long string of B's has an entropy of 0, since the next character will always be a 'B'.

The entropy rate of a data source means the average number of bits per symbol needed to encode it. Shannon's experiments with human predictors show an information rate between 0.6 and 1.3 bits per character in English; the PPM compression algorithm can achieve a compression ratio of 1.5 bits per character in English text.

From the preceding example, note the following points:


Shannon's definition of entropy, when applied to an information source, can determine the minimum channel capacity required to reliably transmit the source as encoded binary digits (see caveat below in italics). The formula can be derived by calculating the mathematical expectation of the "amount of information" contained in a digit from the information source. "See also" Shannon–Hartley theorem.

Shannon's entropy measures the information contained in a message as opposed to the portion of the message that is determined (or predictable). "Examples of the latter include redundancy in language structure or statistical properties relating to the occurrence frequencies of letter or word pairs, triplets etc." See Markov chain.

Entropy is one of several ways to measure diversity. Specifically, Shannon entropy is the logarithm of , the true diversity index with parameter equal to 1.

Entropy effectively bounds the performance of the strongest lossless compression possible, which can be realized in theory by using the typical set or in practice using Huffman, Lempel–Ziv or arithmetic coding. See also Kolmogorov complexity. In practice, compression algorithms deliberately include some judicious redundancy in the form of checksums to protect against errors.

A 2011 study in "Science" estimates the world's technological capacity to store and communicate optimally compressed information normalized on the most effective compression algorithms available in the year 2007, therefore estimating the entropy of the technologically available sources. 

The authors estimate humankind technological capacity to store information (fully entropically compressed) in 1986 and again in 2007. They break the information into three categories—to store information on a medium, to receive information through a one-way broadcast networks, or to exchange information through two-way telecommunication networks.

There are a number of entropy-related concepts that mathematically quantify information content in some way:
(The "rate of self-information" can also be defined for a particular sequence of messages or symbols generated by a given stochastic process: this will always be equal to the entropy rate in the case of a stationary process.) Other quantities of information are also used to compare or relate different sources of information.

It is important not to confuse the above concepts. Often it is only clear from context which one is meant. For example, when someone says that the "entropy" of the English language is about 1 bit per character, they are actually modeling the English language as a stochastic process and talking about its entropy "rate". Shannon himself used the term in this way.

However, if we use very large blocks, then the estimate of per-character entropy rate may become artificially low. This is because in reality, the probability distribution of the sequence is not knowable exactly; it is only an estimate. For example, suppose one considers the text of every book ever published as a sequence, with each symbol being the text of a complete book. If there are published books, and each book is only published once, the estimate of the probability of each book is , and the entropy (in bits) is . As a practical code, this corresponds to assigning each book a unique identifier and using it in place of the text of the book whenever one wants to refer to the book. This is enormously useful for talking about books, but it is not so useful for characterizing the information content of an individual book, or of language in general: it is not possible to reconstruct the book from its identifier without knowing the probability distribution, that is, the complete text of all the books. The key idea is that the complexity of the probabilistic model must be considered. Kolmogorov complexity is a theoretical generalization of this idea that allows the consideration of the information content of a sequence independent of any particular probability model; it considers the shortest program for a universal computer that outputs the sequence. A code that achieves the entropy rate of a sequence for a given model, plus the codebook (i.e. the probabilistic model), is one such program, but it may not be the shortest.

For example, the Fibonacci sequence is 1, 1, 2, 3, 5, 8, 13, …. Treating the sequence as a message and each number as a symbol, there are almost as many symbols as there are characters in the message, giving an entropy of approximately . So the first 128 symbols of the Fibonacci sequence has an entropy of approximately 7 bits/symbol. However, the sequence can be expressed using a formula [ for , , ] and this formula has a much lower entropy and applies to any length of the Fibonacci sequence.

In cryptanalysis, entropy is often roughly used as a measure of the unpredictability of a cryptographic key, though its real uncertainty is unmeasurable. For example, a 128-bit key that is uniformly randomly generated has 128 bits of entropy. It also takes (on average) formula_24 guesses to break by brute force. However, entropy fails to capture the number of guesses required if the possible keys are not chosen uniformly. Instead, a measure called "guesswork" can be used to measure the effort required for a brute force attack.

Other problems may arise from non-uniform distributions used in cryptography. For example, consider a 1000000-digit binary one-time pad using exclusive or. If the pad has 1000000 bits of entropy, it is perfect. If the pad has 999999 bits of entropy, evenly distributed (each individual bit of the pad having 0.999999 bits of entropy) it may provide good security. But if the pad has 999999 bits of entropy, where the first bit is fixed and the remaining 999999 bits are perfectly random, then the first bit of the ciphertext will not be encrypted at all.

A common way to define entropy for text is based on the Markov model of text. For an order-0 source (each character is selected independent of the last characters), the binary entropy is:

where is the probability of . For a first-order Markov source (one in which the probability of selecting a character is dependent only on the immediately preceding character), the entropy rate is:

where is a state (certain preceding characters) and formula_27 is the probability of given as the previous character.

For a second order Markov source, the entropy rate is

In general the -ary entropy of a source formula_29 with source alphabet } and discrete probability distribution } where is the probability of (say is defined by:

Note: the in "-ary entropy" is the number of different symbols of the "ideal alphabet" used as a standard yardstick to measure source alphabets. In information theory, two symbols are necessary and sufficient for an alphabet to encode information. Therefore, the default is to let ("binary entropy"). Thus, the entropy of the source alphabet, with its given empiric probability distribution, is a number equal to the number (possibly fractional) of symbols of the "ideal alphabet", with an optimal probability distribution, necessary to encode for each symbol of the source alphabet. Also note that "optimal probability distribution" here means a uniform distribution: a source alphabet with symbols has the highest possible entropy (for an alphabet with symbols) when the probability distribution of the alphabet is uniform. This optimal entropy turns out to be .

A source alphabet with non-uniform distribution will have less entropy than if those symbols had uniform distribution (i.e. the "optimized alphabet"). This deficiency in entropy can be expressed as a ratio called efficiency:

Efficiency has utility in quantifying the effective use of a communications channel. This formulation is also referred to as the normalized entropy, as the entropy is divided by the maximum entropy formula_32.

Shannon entropy is characterized by a small number of criteria, listed below. Any definition of entropy satisfying these assumptions has the form
where is a constant corresponding to a choice of measurement units.

In the following, and .

The measure should be continuous, so that changing the values of the probabilities by a very small amount should only change the entropy by a small amount.

The measure should be unchanged if the outcomes are re-ordered.

The measure should be maximal if all the outcomes are equally likely (uncertainty is highest when all possible events are equiprobable).

For equiprobable events the entropy should increase with the number of outcomes.

For continuous random variables, the multivariate Gaussian is the distribution with maximum differential entropy.

The amount of entropy should be independent of how the process is regarded as being divided into parts.

This last functional relationship characterizes the entropy of a system with sub-systems. It demands that the entropy of a system can be calculated from the entropies of its sub-systems if the interactions between the sub-systems are known.

Given an ensemble of uniformly distributed elements that are divided into boxes (sub-systems) with elements each, the entropy of the whole ensemble should be equal to the sum of the entropy of the system of boxes and the individual entropies of the boxes, each weighted with the probability of being in that particular box.

For positive integers where ,

Choosing , this implies that the entropy of a certain outcome is zero: . This implies that the efficiency of a source alphabet with symbols can be defined simply as being equal to its -ary entropy. See also Redundancy (information theory).

The Shannon entropy satisfies the following properties, for some of which it is useful to interpret entropy as the amount of information learned (or uncertainty eliminated) by revealing the value of a random variable :

for all probability mass functions formula_48 and formula_49.

The Shannon entropy is restricted to random variables taking discrete values. The corresponding formula for a continuous random variable with probability density function with finite or infinite support formula_50 on the real line is defined by analogy, using the above form of the entropy as an expectation:

This formula is usually referred to as the continuous entropy, or differential entropy. A precursor of the continuous entropy is the expression for the functional in the H-theorem of Boltzmann.

Although the analogy between both functions is suggestive, the following question must be set: is the differential entropy a valid extension of the Shannon discrete entropy? Differential entropy lacks a number of properties that the Shannon discrete entropy has – it can even be negative – and thus corrections have been suggested, notably limiting density of discrete points.

To answer this question, we must establish a connection between the two functions:

We wish to obtain a generally finite measure as the bin size goes to zero. In the discrete case, the bin size is the (implicit) width of each of the (finite or infinite) bins whose probabilities are denoted by . As we generalize to the continuous domain, we must make this width explicit.

To do this, start with a continuous function discretized into bins of size formula_52.

By the mean-value theorem there exists a value in each bin such that

and thus the integral of the function can be approximated (in the Riemannian sense) by

where this limit and "bin size goes to zero" are equivalent.

We will denote

and expanding the logarithm, we have

As Δ → 0, we have

But note that as , therefore we need a special definition of the differential or continuous entropy:

which is, as said before, referred to as the differential entropy. This means that the differential entropy "is not" a limit of the Shannon entropy for . Rather, it differs from the limit of the Shannon entropy by an infinite offset (see also the article on information dimension)

It turns out as a result that, unlike the Shannon entropy, the differential entropy is "not" in general a good measure of uncertainty or information. For example, the differential entropy can be negative; also it is not invariant under continuous co-ordinate transformations. This problem may be illustrated by a change of units when "x" is a dimensioned variable. "f(x)" will then have the units of "1/x". The argument of the logarithm must be dimensionless, otherwise it is improper, so that the differential entropy as given above will be improper. If "Δ" is some "standard" value of "x" (i.e. "bin size") and therefore has the same units, then a modified differential entropy may be written in proper form as:

and the result will be the same for any choice of units for "x". In fact, the limit of discrete entropy as formula_60 would also include a term of formula_61, which would in general be infinite. This is expected, continuous variables would typically have infinite entropy when discretized. The limiting density of discrete points is really a measure of how much easier a distribution is to describe than a distribution that is uniform over its quantization scheme.

Another useful measure of entropy that works equally well in the discrete and the continuous case is the relative entropy of a distribution. It is defined as the Kullback–Leibler divergence from the distribution to a reference measure as follows. Assume that a probability distribution is absolutely continuous with respect to a measure , i.e. is of the form for some non-negative -integrable function with -integral 1, then the relative entropy can be defined as

In this form the relative entropy generalises (up to change in sign) both the discrete entropy, where the measure is the counting measure, and the differential entropy, where the measure is the Lebesgue measure. If the measure is itself a probability distribution, the relative entropy is non-negative, and zero if as measures. It is defined for any measure space, hence coordinate independent and invariant under co-ordinate reparameterizations if one properly takes into account the transformation of the measure . The relative entropy, and implicitly entropy and differential entropy, do depend on the "reference" measure .

Entropy has become a useful quantity in combinatorics.

A simple example of this is an alternate proof of the Loomis–Whitney inequality: for every subset , we have
where is the orthogonal projection in the th coordinate:

The proof follows as a simple corollary of Shearer's inequality: if are random variables and are subsets of } such that every integer between 1 and lies in exactly of these subsets, then
where formula_66 is the Cartesian product of random variables with indexes in (so the dimension of this vector is equal to the size of ).

We sketch how Loomis–Whitney follows from this: Indeed, let be a uniformly distributed random variable with values in and so that each point in occurs with equal probability. Then (by the further properties of entropy mentioned above) , where denotes the cardinality of . Let }. The range of formula_67 is contained in and hence formula_68. Now use this to bound the right side of Shearer's inequality and exponentiate the opposite sides of the resulting inequality you obtain.

For integers let . Then
where 

Here is a sketch proof. Note that formula_71 is one term of the expression
Rearranging gives the upper bound. For the lower bound one first shows, using some algebra, that it is the largest term in the summation. But then,
since there are terms in the summation. Rearranging gives the lower bound.

A nice interpretation of this is that the number of binary strings of length with exactly many 1's is approximately formula_74.




</doc>
<doc id="15446" url="https://en.wikipedia.org/wiki?curid=15446" title="Ithaca College">
Ithaca College

Ithaca College is a private, nonsectarian, liberal arts college in Ithaca, New York. The college was founded by William Egbert in 1892 as a conservatory of music and is set against the backdrop of the city of Ithaca, Cayuga Lake, waterfalls, and gorges. The college is best known for its large list of alumni who have played substantial roles in the media and entertainment industries.

Ithaca College is internationally known for the Roy H. Park School of Communications, which is ranked by several organizations as a top school for journalism, film, media and entertainment. The college has a strong liberal arts core, and offers several pre-professional programs, along with some graduate programs.

Ithaca College has been ranked among the Top 10 masters universities in the "Regional Universities North" category by "U.S. News & World Report," every year since 1996, and was ranked 6th in 2016. Ithaca College is consistently named among the best colleges in the nation by "Princeton Review", with the 2018 guide ranking the college #3 for theater, #3 for newspaper, and #6 for Radio, and is among the top schools producing Fulbright scholarship recipients.

Ithaca College was founded as the Ithaca Conservatory of Music in 1892 when a local violin teacher, William Grant Egbert, rented four rooms and arranged for the instruction of eight students. For nearly seven decades the institution flourished in the city of Ithaca, adding to its music curriculum the study of elocution, dance, physical education, speech correction, radio, business, and the liberal arts. In 1931 the conservatory was chartered as a private college. The college was originally housed in the Boardman House, that later became the Ithaca College Museum of Art, and it was listed on the National Register of Historic Places in 1971.

By 1960, some 2,000 students were in attendance. A modern campus was built on South Hill in the 1960s, and students were shuttled between the old and new during the construction. The hillside campus continued to grow in the ensuing 30 years to accommodate more than 6,000 students.

As the campus expanded, the college also began to expand its curriculum. By the 1990s, some 2,000 courses in more than 100 programs of study were available in the college's five schools. The school attracts a multicultural student body with representatives from almost every state and from 78 foreign countries.

Ithaca College's current campus was built in the 1960s on South Hill. The college's final academic department moved from downtown to the South Hill campus in 1968, making the move complete.

Besides its Ithaca campus, Ithaca College has also operated satellite campuses in other cities. The Ithaca College London Center has been in existence since 1972. Ithaca runs the Ithaca College Los Angeles Program at the James B. Pendleton Center. Additionally, there is an Ithaca College Washington Semester Program, and a recently launched Ithaca College New York City Center.

Former programs include the Ithaca College Antigua Program and the Ithaca College Walkabout Down Under Program in Australia.

Ithaca College also operates direct enrollment exchange programs with several universities, including Griffith University, La Trobe University, Murdoch University, and University of Tasmania (Australia); Chengdu Sport University and Beijing Sport University (China); University of Hong Kong (Hong Kong); Masaryk University (Czech Republic); Akita International University and University of Tsukuba (Japan); Hanyang University (Korea); Nanyang Technological University (Singapore); University of Valencia (Spain); and Jönköping University (Sweden).

The college offers a curriculum with more than 100 degree programs in its five schools.

Until recently, several cross-disciplinary degree programs along with the Center for the Study of Culture, Race, and Ethnicity were housed in the Division of Interdisciplinary and International Studies; however, starting in the spring of 2011, the division was eliminated and its programs, centers and institutes were absorbed within other schools.

As of 2017, the most popular majors included visual and performing arts, health professions and related programs, business, management, marketing, and related support services and biological and biomedical Sciences.

With its top-ranked Roy H. Park School of Communications, Ithaca College is well known for its several prominent student-run media vehicles, including:


Historically, various independent and national fraternities and sororities had active chapters at Ithaca College. However, due to a series of highly publicized hazing incidents in the 1980s, including one that was responsible for the death of a student, the college administration removed all but five Greek letter organizations from campus, and adopted a non-expansion policy, prohibiting any new Greek houses from affiliating with the college. As of 2014, three recognized Greek organizations remain on campus, all of which are music-oriented:

A fourth house, performing arts fraternity Kappa Gamma Psi (Iota Chapter) became inactive in 2008. Although there are potentially plans to reactivate the chapter, it is unclear whether this will be permitted or not due to the college's non-expansionist policy.

However, there are various Greek letter organizations at Ithaca College that are unaffiliated with the school, and therefore not subject to the same housing privileges or rules that contribute to the safety of their members such as non-hazing and non-drinking policies. Additionally, while not particularly common, Ithaca College students may rush for Greek houses affiliated with Cornell University, subject to the rules of each individual fraternity or sorority. Some Cornell-affiliated Greek organizations actively recruit Ithaca College students.

There are a few unaffiliated fraternities that some Ithaca College students join - ΔΚΕ (Delta Kappa Epsilon), ΑΕΠ (Alpha Epsilon Pi), and ΚΣ (Kappa Sigma).
There is one unaffiliated sorority - ΓΔΠ (Gamma Delta Pi).

Ithaca is a member of the NCAA's Division III, the Liberty League Conference, and the Eastern College Athletic Conference. Ithaca has one of Division III's strongest athletic programs, with the Bombers winning a total of 15 national titles in seven team sports and five individual sports. Ithaca was previously a member of the Empire 8 Conference.

The Ithaca athletics nickname "Bombers" is unique in NCAA athletics, and the origins of the nickname are obscure. Ithaca College's sports teams were originally named the Cayugas, but the name was changed to the Bombers sometime in the 1930s. Some other names that have been used for Ithaca College's teams include: Blue Team, Blues, Blue and Gold, Collegians, and the Seneca Streeters. Several possibilities for the change to the "Bombers" have been posited. The most common explanation is that the school's baseball uniforms—white with navy blue pinstripes and an interlocking "IC" on the left chest—bear a striking resemblance to the distinctive home uniforms of the New York Yankees, who are known as the Bronx Bombers. It may also have referred to the Ithaca basketball team of that era and its propensity for half-court "bombs". Grumman Aircraft also manufactured airplanes including bombers in Ithaca for many years. The first “Bombers” reference on record was in the December 17, 1938 issue of the "Rochester Times-Union" in a men’s basketball article.

The name has at times sparked controversy for its perceived martial connotations. It is an occasional source of umbrage from Ithaca's prominent pacifist community, but the athletics department has consistently stated it has no interest in changing the name. The athletics logo has in the past incorporated World War II era fighter planes, but currently does not, and the school does not currently have a physical mascot to personify the name. In 2010 the school launched a contest to choose one. It received over 250 suggestions and narrowed the field down to three: a phoenix, a flying squirrel, and a Lake Beast. In June 2011, President Rochon announced that the school would discontinue the search due to opposition in the alumni community.

Ithaca College recently remodeled the Hill Center in 2013. The building features hardwood floors (Ben Light Gymnasium) as well as coaches offices. The building is home to Ithaca's men's and women's basketball teams, women's volleyball team, wrestling, and gymnastics. Ithaca also opened the Athletics & Events Center in 2011, a $65.5 million facility funded by donors. The facility is mainly used by the school's varsity athletes. It has a 47,000 square foot, 9-lane 50 meter Olympic-size pool. The building also has Glazer Arena, a 130,000 square foot event space. It is a track and field center that doubles as a practice facility for lacrosse, field hockey, soccer, baseball, tennis, and football. The facility was designed by the architectural firm Moody-Nolan and began construction in June 2009.

Coached by Jim Butterfield for 27 years, the football team has won three NCAA Division III National Football Championships in 1979, 1988 and 1991 (a total surpassed only by Augustana, Mount Union and Wisconsin-Whitewater). Bomber football teams made a record seven appearances in the Division III national championship game, the Amos Alonzo Stagg Bowl, which has since been surpassed by Mount Union in 2003. The Bombers play the SUNY Cortland Red Dragons for the Cortaca Jug, which was added in 1959 to an already competitive rivalry. The match-up is one of the most prominent in Division III college football. The game alternates locations between Ithaca and Cortland. Cortland had won the Cortaca Jug six years in a row until Ithaca broke the streak in 2017.

The women's crew won back-to-back NCAA Division III Rowing Championships in 2004 and 2005.

The men's crew saw much success in 2008, receiving 4 medals at the New York State Collegiate Championships.

Women's soccer has won two national championships in Division III and is consistently ranked in the top 20 nationally.

Gymnastics won the NCAA Division III national championships in 1998.

The men's wrestling team won NCAA Division III National Championships in 1989, 1990 and 1994.

Women's field hockey won the 1982 NCAA Division III Field Hockey Championship.

In 2013, Paula Miller, head of the women's swimming team completed her 30th year as head coach of the Ithaca Bombers. She has led the team to many victories. In the previous four years, the Bombers were undefeated throughout their season defeating tough competition. Ithaca has finished first or second at 25 of the past 29 state meets. The Bombers have also won the Empire 8 crown in each of the past nine seasons.

The 2013–2014 season ended with regaining the NCAA Division III Championship trophy.

During the 2015–2016 season the Bombers swimming and diving team held the UNYSCSA Empire 8 state champion meet in the Athletic and Events center at Ithaca College. The men's swimming and diving team scored 616.5 points, finishing 4th in states under coach Kevin Markwardt. The men's team was led by captain Addison Hebert, who was injured the first day of the meet and was able to overcome it by the last day helping the rest of the bombers get 3rd place in the 400 freestyle relay by .01 seconds. The girls' swimming and diving team scored 1227 points, winning states under Paula Miller. The bombers were to bring two women divers to South Carolina, to compete in nationals in March.

Ithaca is also home to more than 60 club sports, many of which compete regularly against other colleges in leagues and tournaments.

Along with Intercollegiate athletics, Ithaca College has a large intramural sport program. This extracurricular program serves approximately 25% of the undergraduate population yearly. Fourteen traditional team activities are offered throughout the year and include basketball, flag football, kickball, soccer, softball, ultimate frisbee, ski racing, and volleyball.

For most activities, divisions are offered for men's, women's, and co-recreational teams. Throughout the year usually two or more activities run concurrently and participants are able to play on a single sex team and co-recreational team for each activity.

Ithaca's School of Business was the first college or university business school in the world to achieve LEED Platinum Certification alongside Yale University, which had the second. Ithaca's Peggy Ryan Williams Center is also LEED Platinum certified. It makes extensive use of day light in occupied spaces. There are sensors that regulate lighting and ventilation based on occupancy and natural light. Over 50% of the building energy comes from renewable sources such as wind power. The college also has a LEED Gold Certified building, the Athletics & Events Center. The college composts its dining hall waste, runs a "Take It or Leave It" Green move-out program, and offers a sustainable living option. It also operates an office supply collection and reuse program, as well as a sustainability education program during new student orientation. Ithaca received a B- grade on the Sustainable Endowments Institute's 2009 College Sustainability Report Card and an A- for 2010.

Ithaca College was listed as one of Princeton Review's top "green colleges" for being environmentally responsible.

In the spring of 2007, then-President Peggy R. Williams signed the American College and University President's Climate Commitment (ACUPCC), pledging Ithaca College to the task of developing a strategy and long-range plan to achieve "carbon neutrality" at some point in the future. In 2009 the Ithaca College Board of Trustees approved the Ithaca College Climate Action Plan, which calls for 100% carbon neutrality by 2050. In 2009, the Ithaca College Board of Trustees approved the Ithaca College Climate Action Plan, which calls for 100% carbon neutrality by 2050 and offers a 40-year action plan to work toward that ambitious goal.

The college purchases 14 percent of its electricity from renewable sources and offsets 3 percent of its energy use with renewable energy credits.

The college aims to optimize investment returns and does not invest the endowment in on-campus sustainability projects, renewable energy funds, or community development loan funds. The college's investment policy reserves the right of the investment committee to restrict investments for any reason, which could include environmental and sustainability factors.

While the Ithaca College Natural Lands has issued a statement that Ithaca College should join efforts calling for a moratorium on horizontal drilling and high volume (“slick water”) hydraulic fracturing, or fracking, the college as a whole has refused to issue a statement regarding the issue.

Ithaca's current president is Shirley M. Collado. She was named the ninth president of Ithaca College on February 22, 2017, and assumed the presidency on July 1, 2017. She was previously executive vice chancellor and chief operating officer at Rutgers University–Newark and vice president of student affairs and dean of the college at Middlebury College. She is the first Dominican American to be named president of a college in the United States.

Collado succeeds Thomas Rochon, who was named eighth president of Ithaca College on April 11, 2008. Rochon took over as president of the college following Peggy Williams, who had announced on July 12, 2007, that she would retire from the presidency post effective May 31, 2009, following a one-year sabbatical. During the fall 2015 semester, multiple protests focusing on campus climate and Rochon's leadership were led by students and faculty. After multiple racially charged events including student house party themes and racially tinged comments at administration led-programs, students, faculty and staff all decided to hold votes of "no confidence" in Rochon. Students voted "no confidence" by a count of 72% no confidence, 27% confidence, and 1% abstaining. The faculty voted 77.8% no confidence to 22.2% confidence. Rochon retired on July 1, 2017.

Ithaca College has 49,570 alumni in the United States. There are alumni clubs for Boston, Chicago, Connecticut, Los Angeles, Metro New York, National Capital, North and South Carolina, Philadelphia, Rochester (NY), San Diego, and Southern Florida. Alumni events are hosted in cooperation with city-specific clubs and through a program called "IC on the Road".

Following is a brief list of noteworthy Ithaca College alumni.
For a more extensive list, refer to the List of Ithaca College alumni.

Following is a brief list of current and former noteworthy Ithaca College faculty.



</doc>
<doc id="15447" url="https://en.wikipedia.org/wiki?curid=15447" title="Differential psychology">
Differential psychology

Differential psychology studies the ways in which individuals differ in their behavior and the processes that underlie it. This is distinguished from other aspects of psychology in that although psychology is ostensibly a study of individuals, modern psychologists often study groups, or attempt to discover general psychological processes that apply to all individuals.

For example, in evaluating the effectiveness of a new therapy, the mean performance of the therapy in one treatment group might be compared to the mean effectiveness of a placebo (or a well-known therapy) in a second, control group. In this context, differences between individuals in their reaction to the experimental and control manipulations are actually treated as errors rather than as interesting phenomena to study.

This approach is because psychological research depends upon statistical controls that are only defined upon groups of people. Individual difference psychologists usually express their interest in individuals while studying groups by seeking dimensions shared by all individuals but upon which individuals differ.

Individual differences are essential whenever we wish to explain how
individuals differ in their behavior. In any study, significant
variation exists between individuals. Reaction time, preferences,
values, and health-linked behaviors are just a few examples. Individual
differences in factors such as personality, intelligence,
memory, or physical factors such as body size, sex, age, and other
factors can be studied and used in understanding this large source of
variance. Importantly, individuals can also differ not only in their
current state, but in the magnitude or even direction of response to a
given stimulus. Such phenomena, often
explained in terms of inverted-U response curves,
place differential psychology at an important location in such
endeavours as personalized medicine, in which diagnoses are
customised for an individual's response profile.

Individual differences research typically includes personality, motivation, intelligence, ability, IQ, interests, values, self-concept, self-efficacy, and self-esteem (to name just a few). There are few remaining "differential psychology" programs in the United States, although research in this area is very active. Current researchers are found in a variety of applied and experimental programs, including clinical psychology, educational psychology, Industrial and organizational psychology, personality psychology, social psychology, behavioral genetics, and developmental psychology programs, in the neo-Piagetian theories of cognitive development in particular.




</doc>
<doc id="15450" url="https://en.wikipedia.org/wiki?curid=15450" title="Industrial and organizational psychology">
Industrial and organizational psychology

Industrial and organizational psychology (I/O psychology), which is also known as occupational psychology, organizational psychology, and work and organizational psychology, is an applied discipline within psychology. I/O psychology is the science of human behaviour relating to work and applies psychological theories and principles to organizations and individuals in their places of work as well as the individual's work-life more generally. I/O psychologists are trained in the scientist–practitioner model. They contribute to an organization's success by improving the performance, motivation, job satisfaction, and occupational safety and health as well as the overall health and well-being of its employees. An I/O psychologist conducts research on employee behaviours and attitudes, and how these can be improved through hiring practices, training programs, feedback, and management systems.

I/O psychology is one of the 15 recognized specialties in professional psychology in the United States. It is represented by Division 14 of the American Psychological Association (APA), known formally as the Society for Industrial and Organizational Psychology (SIOP). In the United Kingdom, industrial and organizational psychologists are referred to as occupational psychologists. Occupational psychology in the UK is one of nine 'protected titles' within the profession "practitioner psychologist" regulated by the Health and Care Professions Council. In the UK, graduate programs in psychology, including occupational psychology, are accredited by the British Psychological Society.

In Australia, the title organizational psychologist is protected by law, and regulated by the Australian Health Practitioner Regulation Agency (AHPRA). Organizational psychology is one of nine areas of specialist endorsement for psychology practice in Australia.

In Europe someone with a specialist EuroPsy Certificate in Work and Organisational Psychology is a fully qualified psychologist and an expert in the work psychology field. Industrial and organizational psychologists reaching the EuroPsy standard are recorded in the Register of European Psychologists and industrial and organizational psychology is one of the three main psychology specializations in Europe.

The historical development of I/O psychology was paralleled in the US, the UK, Australia, Germany, the Netherlands, and eastern European countries such as Romania. The roots of I/O psychology trace back nearly to the beginning of psychology as a science, when Wilhelm Wundt founded one of the first psychological laboratories in 1879 in Leipzig, Germany. In the mid 1880s, Wundt trained two psychologists, Hugo Münsterberg and James McKeen Cattell, who had a major influence on the emergence of I/O psychology.

Instead of viewing performance differences as human "errors", Cattell was one of the first to recognize the importance of differences among individuals as a way of better understanding work behavior. Walter Dill Scott, who was a contemporary of Cattell, was elected President of the American Psychological Association (APA) in 1919, was arguably the most prominent I/O psychologist of his time. Scott, along with Walter Van Dyke Bingham, worked at the Carnegie Institute of Technology, developing methods for selecting and training sales personnel.

The "industrial" side of I/O psychology originated in research on individual differences, assessment, and the prediction of work performance. Industrial psychology crystallized during World War I, in response to the need to rapidly assign new troops to duty. Scott and Bingham volunteered to help with the testing and placement of more than a million army recruits. In 1917, together with other prominent psychologists, they adapted a well-known intelligence test the Stanford–Binet, which was designed for testing one individual at a time, to make it suitable for group testing. The new test was called the Army Alpha.

After the War, the growing industrial base in the US was a source of momentum for what was then called industrial psychology. Private industry set out to emulate the successful testing of army personnel. Mental ability testing soon became commonplace in the work setting.

Elton Mayo found that rest periods improved morale and reduced turnover in a Philadelphia textile factory. He later joined the ongoing Hawthorne studies, where he became interested in how workers' emotions and informal relationships affected productivity. The results of these studies ushered in the human relations movement.

World War II brought renewed interest in ability testing (to accurately place recruits in new technologically advanced military jobs), the introduction of the assessment center, and concern with morale and fatigue in war industry workers.

The industrial psychology division of the former American Association of Applied Psychology became a division within APA, becoming Division 14 of APA. It was initially called the Industrial and Business Psychology Division. In 1962, the name was changed to the Industrial Psychology Division. In 1973, it was renamed again, this time to the Division of Industrial and Organizational Psychology. In 1982, the unit become more independent of APA, and its name was changed again, this time to the Society for Industrial and Organizational Psychology.

The name change of the division from "industrial psychology" to "industrial and organizational psychology" reflected the shift in the work of industrial psychologists who had originally addressed work behavior from the individual perspective, examining performance and attitudes of individual workers. Their work became broader. Group behavior in the workplace became a worthy subject of study. The emphasis on "organizational" underlined the fact that when an individual joins an organization (e.g., the organization that hired him or her), he or she will be exposed to a common goal and a common set of operating procedures. In the 1970s in the UK, references to occupational psychology became more common than I/O psychology.

According to Bryan and Vinchur, "while organizational psychology increased in popularity through [the 1960s and 1970s], research and practice in the traditional areas of industrial psychology continued, primarily driven by employment legislation and case law" (p. 53). There was a focus on fairness and validity in selection efforts as well as in the job analyses that undergirded selection instruments. For example, i/o psychology showed increased interest in behaviorally anchored rating scales. What critics there were of i/o psychology accused the discipline of being responsive only to the concerns of managements.

From the 1980 to 2010s other changes in i/o psychology took place. Researchers increasingly adopted a multi-level approach, attempting to understand behavioral phenomena from both the level of the organization and the level of the individual worker. There was also an increased interest in the needs and expectations of employees as individuals. For example, an emphasis on organizational justice and the psychological contract took root, as well as the more traditional concerns of selection and training. Methodological innovations (e.g., meta-analyses, structural equation modeling) were adopted. With the passage of the American with Disabilities Act in 1990 and parallel legislation elsewhere in the world, i/o psychology saw an increased emphasis on "fairness in personnel decisions." Training research relied increasingly on advances in educational psychology and cognitive science.

As described above, I/O psychologists are trained in the scientist–practitioner model. I/O psychologists rely on a variety of methods to conduct organizational research. Study designs employed by I/O psychologists include surveys, experiments, quasi-experiments, and observational studies. I/O psychologists rely on diverse data sources including human judgments, historical databases, objective measures of work performance (e.g., sales volume), and questionnaires and surveys.

I/O researchers employ quantitative statistical methods. Quantitative methods used in I/O psychology include correlation, multiple regression, and analysis of variance. More advanced statistical methods employed in I/O research include logistic regression, structural equation modeling, and hierarchical linear modeling (HLM; also known as multilevel modeling). I/O research has also employed meta-analysis. I/O psychologists also employ psychometric methods including methods associated with classical test theory, generalizability theory, and item response theory (IRT).

I/O psychologists have also employed qualitative methods, which largely involve focus groups, interviews, and case studies. I/O research on organizational culture research has employed ethnographic techniques and participant observation. A qualitative technique associated with I/O psychology is Flanagan's Critical Incident Technique. I/O psychologists sometimes use quantitative and qualitative methods in concert. OHP researchers have also combined and coordinated quantitative and qualitative methods within a single study.

Job analysis encompasses a number of different methods. It primarily involves the systematic collection of information about a job. A task-oriented job analysis involves an examination of the duties, tasks, and/or competencies required by the job being assessed. By contrast, a worker-oriented job analysis involves an examination of the knowledge, skills, abilities, and other characteristics (KSAOs) required to successfully perform the work. Information obtained from job analyses are used for many purposes, including the creation of job-relevant selection procedures, performance appraisals and the criteria they require, and the development of training programs.

I/O psychologists typically work with human resource specialists to design (a) recruitment processes and (b) personnel selection systems. Personnel recruitment is the process of identifying qualified candidates in the workforce and getting them to apply for jobs within an organization. Personnel recruitment processes include developing job announcements, placing ads, defining key qualifications for applicants, and screening out unqualified applicants.

Personnel selection is the systematic process of hiring and promoting personnel. Personnel selection systems employ evidence-based practices to determine the most qualified candidates. Personnel selection involves both the newly hired and individuals who can be promoted from within the organization. Common selection tools include ability tests (e.g., cognitive, physical, or psycho-motor), knowledge tests, personality tests, structured interviews, the systematic collection of biographical data, and work samples. I/O psychologists must evaluate evidence regarding the extent to which selection tools predict job performance.

Personnel selection procedures are usually validated, i.e., shown to be job relevant to personnel selection, using one or more of the following types of validity: content validity, construct validity, and/or criterion-related validity. I/O psychologists must adhere to professional standards in personnel selection efforts. SIOP (e.g., "Principles for validation and use of personnel selection procedures") and APA together with the National Council on Measurement in Education (e.g., "Standards for educational and psychological testing" are sources of those standards. The Equal Employment Opportunity Commission's "Uniform guidelines" are also influential in guiding personnel selection decisions.

A meta-analysis of selection methods found that general mental ability was the best overall predictor of job performance and attainment in training.

Performance appraisal or performance evaluation is the process in which an individual's or a group's work behaviors and outcomes are assessed against managers' and others' expectations for the job. Performance appraisal is frequently used in promotion and compensation decisions, to help design and validate personnel selection procedures, and for performance management. Performance management is the process of providing performance feedback relative to expectations, and information relevant to improvement (e.g., coaching, mentoring). Performance management may also include documenting and tracking performance information for organizational evaluation purposes.

An I/O psychologist would typically use information from the job analysis to determine a job's performance dimensions, and then construct a rating scale to describe each level of performance for the job. Often, the I/O psychologist would be responsible for training organizational personnel how to use the performance appraisal instrument, including ways to minimize bias when using the rating scale, and how to provide effective performance feedback.

Individual assessment involves the measurement of individual differences. I/O psychologists perform individual assessments in order to evaluate differences among candidates for employment as well as differences among employees. The constructs measured pertain to job performance. With candidates for employment, individual assessment is often part of the personnel selection process. These assessments can include written tests, aptitude tests, physical tests, psycho-motor tests, personality tests, integrity and reliability tests, work samples, simulations, and assessment centres.

I/O psychologists are concerned with occupational health and well-being. Early in the 20th century Arthur Kornhauser examined the impact on productivity of hiring mentally unstable workers. Kornhauser also examined the link between industrial working conditions and mental health as well as the spillover into a worker's personal life of having an unsatisfying job.

More recently, I/O researchers have found that staying vigorous during working hours is associated with better work-related behaviour and subjective well-being as well as more effective functioning in the family domain. Trait vigor and recovery experiences after work were related to vigor at work. Job satisfaction has also been found to be associated with life satisfaction, happiness, well-being and positive affect, and the absence of negative affect. Other research indicates that among older workers activities such as volunteering and participating in social clubs was related to a decrease in depressive symptoms over the next two years. Research on job changing indicates that mobility between, but not within, organizations is associated with burnout.

I/O psychologists are concerned with the related topics of workplace bullying, aggression, and violence. For example, I/O research found that exposure to workplace violence elicited ruminative thinking, and ruminative thinking, in turn, is associated with poor well-being. I/O research has found that interpersonal aggressive behaviours is associated with worse team performance.

Compensation includes wages or salary, bonuses, pension/retirement contributions, and employee benefits that can be converted to cash or replace living expenses. I/O psychologists may be asked to conduct a job evaluation for the purpose of determining compensation levels and ranges. I/O psychologists may also serve as expert witnesses in pay discrimination cases, when disparities in pay for similar work are alleged by employees.

Training involves the systematic teaching of skills, concepts, or attitudes that results in improved performance in another environment. Because many people hired for a job are not already versed in all the tasks the job requires, training may be needed to help the individual perform the job effectively. Evidence indicates that training is often effective, and that it succeeds in terms of higher net sales and gross profitability per employee.

Similar to performance management (see above), an I/O psychologist would employ a job analysis in concert with the application of the principles of instructional design to create an effective training program. A training program is likely to include a summative evaluation at its conclusion in order to ensure that trainees have met the training objectives and can perform the target work tasks at an acceptable level. Training programs often include formative evaluations to assess the effect of the training as the training proceeds. Formative evaluations can be used to locate problems in training procedures and help I/O psychologists make corrective adjustments while training is ongoing.

The foundation for training programs is learning. Learning outcomes can be organized into three broad categories: cognitive, skill-based, and affective outcomes. Cognitive training is aimed at instilling declarative knowledge or the knowledge of rules, facts, and principles (e.g., police officer training covers laws and court procedures). Skill-based training aims to impart procedural knowledge (e.g., skills needed to use a special tool) or technical skills (e.g., understanding the workings of software program). Affective training concerns teaching individuals to develop specific attitudes or beliefs that predispose trainees to behave a certain way (e.g., show commitment to the organization, appreciate diversity).

A needs assessment, an analysis of corporate and individual goals, is often undertaken prior to the development of a training program. In addition, a careful needs analysis is required in order to develop a systematic understanding of where training is needed, what should be taught, and who will be trained. A training needs analysis typically involves a three-step process that includes organizational analysis, task analysis and person analysis.

An organizational analysis is an examination of organizational goals and resources as well as the organizational environment. The results of an organizational analysis help to determine where training should be directed. The analysis identifies the training needs of different departments or subunits. It systematically assesses manager, peer, and technological support for transfer of training. An organizational analysis also takes into account the climate of the organization and its subunits. For example, if a climate for safety is emphasized throughout the organization or in subunits of the organization (e.g., production), then training needs will likely reflect an emphasis on safety. A task analysis uses the results of a job analysis to determine what is needed for successful job performance, contributing to training content. With organizations increasingly trying to identify "core competencies" that are required for all jobs, task analysis can also include an assessment of competencies. A person analysis identifies which individuals within an organization should receive training and what kind of instruction they need. Employee needs can be assessed using a variety of methods that identify weaknesses that training can address.

Work motivation reflects the energy an individual applies "to initiate work-related behavior, and to determine its form, direction, intensity, and duration" Understanding what motivates an organization's employees is central to I/O psychology. Motivation is generally thought of as a theoretical construct that fuels behavior. An incentive is an anticipated reward that is thought to incline a person to behave a certain way. Motivation varies among individuals. Studying its influence on behavior, it must be examined together with ability and environmental influences. Because of motivation's role in influencing workplace behavior and performance, many organizations structure the work environment to encourage productive behaviors and discourage unproductive behaviors.

Motivation involves three psychological processes: arousal, direction, and intensity. Arousal is what initiates action. It is often fueled by a person's need or desire for something that is missing from his or her life, either totally or partially. Direction refers to the path employees take in accomplishing the goals they set for themselves. Intensity is the amount of energy employees put into goal-directed work performance. The level of intensity often reflects the importance and difficulty of the goal. These psychological processes involve four factors. First, motivation serves to direct attention, focusing on particular issues, people, tasks, etc. Second, it serves to stimulate effort. Third, motivation influences persistence. Finally, motivation influences the choice and application of task-related strategies.

I/O psychologists are involved in the research and the practice of occupational stress and design of individual and organizational interventions to manage and reduce the stress levels and increase productivity, performance, health and wellbeing. Occupational stress can have implications for organizational performance because of the emotions job stress evokes. For example, a job stressor such as conflict with a supervisor can precipitate anger that in turn motivates counterproductive workplace behaviors. I/O research has examined the association between work stressors and aggression, theft, substance abuse, and depressive symptoms. A number of models have been developed to explain the job stress process, including the person-environment fit model and the demand-control model. Those models became a cornerstone of the emergence, in the late 1980s and early 1990s, of a new discipline relevant to research on occupational stress; the discipline, occupational health psychology, is an offshoot of i/o psychology, health psychology, and occupational medicine.

Research has also examined occupational stress in specific occupations, including police, general practitioners, and dentists. Another concern has been the relation of occupational stress to family life. Other research has examined gender differences in leadership style and job stress and strain in the context of male- and female-dominated industries, and unemployment-related distress. I/O psychology is also concerned with the relation of occupational stress to career advancement.

Accidents and safety in the workplace have become areas of interest to I/O psychology. Examples of psychosocial injury hazards of interest to I/O psychology include fatigue, workplace violence, workplace bullying, and working night shifts. I/O researchers conduct "stress audits" that can help organizations remain compliant with various occupational safety regulations. Psychosocial hazards can affect musculoskeletal disorders. A psychosocial factor related to accident risk is safety climate, which refers to employees' perceptions of the extent to which their work organization prioritizes safety. By contrast, psychosocial safety climate refers to management's "policies, practices, and procedures" aimed at protecting workers' psychological health. Research on safety leadership is also relevant to I/O psychology. Research suggests that safety-oriented transformational leadership is associated with a positive safety climate and safe worker practices.

Organizational culture has been described as a set of assumptions shared by individuals in an organization; the assumptions influence the interpretation and actions that define appropriate behavior for various situations. Organizational culture has been shown to affect important organizational outcomes such as performance, attraction, recruitment, retention, employee satisfaction, and employee well-being. There are three levels of organizational culture: artifacts, shared values, and basic beliefs and assumptions. Artifacts comprise the physical components of the organization that relay cultural meaning. Shared values are individuals' preferences regarding certain aspects of the organization's culture (e.g., loyalty, customer service). Basic beliefs and assumptions include individuals' impressions about the trustworthiness and supportiveness of an organization, and are often deeply ingrained within the organization's culture.

In addition to an overall culture, organizations also have subcultures. Examples of subcultures include corporate culture, departmental culture, local culture, and issue-related culture. While there is no single "type" of organizational culture, some researchers have developed models to describe different organizational cultures.

Group behavior involves the interactions among individuals in a collective. The individuals' opinions, attitudes, and adaptations affect group behavior and group behavior, in turn, affects those opinions, etc. The interactions are thought to fulfill some need satisfaction in an individual who is part of the collective. A specific area of I/O research in group behavior is the team dynamics and team effectiveness.

Organizations often organize teams because teams can accomplish a much greater amount of work in a short period of time than an individual can accomplish. I/O research has examined the harm workplace aggression does to team performance.

The selection of individuals to be assigned to specific teams bears on the resulting effectiveness of those teams. Aspects of team composition that should be considered during the team selection process include team members' knowledge, skills, and abilities (KSAs) as well their personalities and attitudes. To achieve high-quality results, teams built with members having higher skill levels are more likely to be effective than teams built around members having lesser skills; teams that include a members with a diversity of skills are also likely to show improved team performance. Additionally, higher levels of cognitive ability of team members has been shown to consistently correlate to increased work group effectiveness. Team members' personalities and attitudes have also been studied. For example, the personality factors agreeableness, conscientiousness, extraversion, and emotional stability are related to better team performance.

A fundamental question in team task design is whether or not a task is even appropriate for a team. Those tasks that require predominantly independent work are best left to individuals, and team tasks should include those tasks that consist primarily of interdependent work. When a given task is appropriate for a team, task design can play a key role in team effectiveness.

Job characteristic theory identifies core job dimensions that affect motivation, satisfaction, performance, etc. These dimensions include skill variety, task identity, task significance, autonomy and feedback. The dimensions map well to the team environment. Individual contributors who perform team tasks that are challenging, interesting, and engaging are more likely to be motivated to exert greater effort and perform better than team members who are working on tasks that lack those characteristics.

Organizational support systems affect the team effectiveness and provide resources for teams operating in the multi-team environment. During the chartering of new teams, organizational enabling resources are first identified. Examples of enabling resources include facilities, equipment, information, training, and leadership. Team-specific resources (e.g., budgetary resources, human resources) are typically made available. Team-specific human resources represent the individual contributors who are selected to be team members. Intra-team processes (e.g., task design, task assignment) involve these team-specific resources.

Teams also function in dynamic multi-team environments. Teams often must respond to shifting organizational contingencies. Contingencies affecting teams include constraints arising from conditions in which organizational resources are not exclusively earmarked for certain teams. When resources are scarce, they must be shared by multiple teams.

Organizational reward systems drive the strengthening and enhancing of individual team member efforts; such efforts contribute towards reaching team goals. In other words, rewards that are given to individual team members should be contingent upon the performance of the entire team.

Several design elements are needed to enable organizational reward systems to operate successfully. First, for a collective assessment to be appropriate for individual team members, the group's tasks must be highly interdependent. If this is not the case, individual assessment is more appropriate than team assessment. Second, individual-level reward systems and team-level reward systems must be compatible. For example, it would be unfair to reward the entire team for a job well done if only one team member did most of the work. That team member would most likely view teams and teamwork negatively, and would not want to work on a team in the future. Third, an organizational culture must be created such that it supports and rewards employees who believe in the value of teamwork and who maintain a positive attitude towards team-based rewards.

Goals potentially motivate team members when goals contain three elements: difficulty, acceptance, and specificity. Under difficult goal conditions, teams with more committed members tend to outperform teams with less committed members. When team members commit to team goals, team effectiveness is a function of how supportive members are with each other. The goals of individual team members and team goals interact. Team and individual goals must be coordinated. Individual goals must be consistent with team goals in order for a team to be effective.

Job satisfaction is often thought to reflect the extent to which a worker likes his or her job, or individual aspects or facets of jobs. It is one of the most heavily researched topics in I/O psychology. Job satisfaction has theoretical and practical utility for the field. It has been linked to important job outcomes including attitudinal variables (e.g., job involvement, organizational commitment), absenteeism, turnover intentions, actual turnover, job performance, and tension. A meta-analyses found job satisfaction to be related to life satisfaction, happiness, positive affect, and the absence of negative affect.

Productive behavior is defined as employee behavior that contributes positively to the goals and objectives of an organization. When an employee begins a new job, there is a transition period during which he or she may not contribute significantly. To assist with this transition an employee typically requires job-related training. In financial terms, productive behavior represents the point at which an organization begins to achieve some return on the investment it has made in a new employee. I/O psychologists are ordinarily more focused on productive behavior than job or task performance, including in-role "and" extra-role performance. In-role performance tells managers how well an employee performs the required aspects of the job; extra-role performance includes behaviors not necessarily required by job but nonetheless contribute to organizational effectiveness. By taking both in-role and extra-role performance into account, an I/O psychologist is able to assess employees' effectiveness (how well they do what they were hired to do), efficiency (outputs to relative inputs), and productivity (how much they help the organization reach its goals). Three forms of productive behavior that I/O psychologists often evaluate include job performance, organizational citizenship behavior (see below), and innovation.

Job performance represents behaviors employees engage in while at work which contribute to organizational goals. These behaviors are formally evaluated by an organization as part of an employee's responsibilities. In order to understand and ultimately predict job performance, it is important to be precise when defining the term. Job performance is about behaviors that are within the control of the employee and not about results (effectiveness), the costs involved in achieving results (productivity), the results that can be achieved in a period of time (efficiency), or the value an organization places on a given level of performance, effectiveness, productivity or efficiency (utility).

To model job performance, researchers have attempted to define a set of dimensions that are common to all jobs. Using a common set of dimensions provides a consistent basis for assessing performance and enables the comparison of performance across jobs. Performance is commonly broken into two major categories: in-role (technical aspects of a job) and extra-role (non-technical abilities such as communication skills and being a good team member). While this distinction in behavior has been challenged it is commonly made by both employees and management. A model of performance by Campbell breaks performance into in-role and extra-role categories. Campbell labeled job-specific task proficiency and non-job-specific task proficiency as in-role dimensions, while written and oral communication, demonstrating effort, maintaining personal discipline, facilitating peer and team performance, supervision and leadership and management and administration are labeled as extra-role dimensions. Murphy's model of job performance also broke job performance into in-role and extra-role categories. However, task-orientated behaviors composed the in-role category and the extra-role category included interpersonally-oriented behaviors, down-time behaviors and destructive and hazardous behaviors. However, it has been challenged as to whether the measurement of job performance is usually done through pencil/paper tests, job skills tests, on-site hands-on tests, off-site hands-on tests, high-fidelity simulations, symbolic simulations, task ratings and global ratings. These various tools are often used to evaluate performance on specific tasks and overall job performance. Van Dyne and LePine developed a measurement model in which overall job performance was evaluated using Campbell's in-role and extra-role categories. Here, in-role performance was reflected through how well "employees met their performance expectations and performed well at the tasks that made up the employees' job." Dimensions regarding how well the employee assists others with their work for the benefit of the group, if the employee voices new ideas for projects or changes to procedure and whether the employee attends functions that help the group composed the extra-role category.

To assess job performance, reliable and valid measures must be established. While there are many sources of error with performance ratings, error can be reduced through rater training and through the use of behaviorally-anchored rating scales. Such scales can be used to clearly define the behaviors that constitute poor, average, and superior performance. Additional factors that complicate the measurement of job performance include the instability of job performance over time due to forces such as changing performance criteria, the structure of the job itself and the restriction of variation in individual performance by organizational forces. These factors include errors in job measurement techniques, acceptance and the justification of poor performance and lack of importance of individual performance.

The determinants of job performance consist of factors having to do with the individual worker as well as environmental factors in the workplace. According to Campbell's Model of The Determinants of Job Performance, job performance is a result of the interaction between declarative knowledge (knowledge of facts or things), procedural knowledge (knowledge of what needs to be done and how to do it), and motivation (reflective of an employee's choices regarding whether to expend effort, the level of effort to expend, and whether to persist with the level of effort chosen). The interplay between these factors show that an employee may, for example, have a low level of declarative knowledge, but may still have a high level of performance if the employee has high levels of procedural knowledge and motivation.

Regardless of the job, three determinants stand out as predictors of performance: (1) general mental ability (especially for jobs higher in complexity); (2) job experience (although there is a law of diminishing returns); and (3) the personality trait of conscientiousness (people who are dependable and achievement-oriented, who plan well). These determinants appear to influence performance largely through the acquisition and usage of job knowledge and the motivation to do well. Further, an expanding area of research in job performance determinants includes emotional intelligence.

Organizational citizenship behaviors (OCBs) are another form of workplace behavior that I/O psychologists are involved with. OCBs tend to be beneficial to both the organization and other workers. Dennis Organ (1988) defines OCBs as "individual behavior that is discretionary, not directly or explicitly recognized by the formal reward system, and that in the aggregate promotes the effective functioning of the organization." Behaviors that qualify as OCBs can fall into one of the following five categories: altruism, courtesy, sportsmanship, conscientiousness, and civic virtue. OCBs have also been categorized in other ways too, for example, by their intended targets (individuals, supervisors, and the organization as a whole. Other alternative ways of categorizing OCBs include "compulsory OCBs," which are engaged in owing to coercive persuasion or peer pressure rather than out of good will. The extent to which OCBs are voluntary has been the subject of some debate.

Other research suggests that some employees perform OCBs to influence how they are viewed within the organization. While these behaviors are not formally part of the job description, performing them can influence performance appraisals. Researchers have advanced the view that employees engage in OCBs as a form of "impression management," a term coined by Erving Goffman. Goffman defined impression management as "the way in which the individual ... presents himself and his activity to others, the ways in which he guides and controls the impression they form of him, and the kinds of things he may and may not do while sustaining his performance before them. Some researchers have hypothesized that OCBs are not performed out of good will, positive affect, etc., but instead as a way of being noticed by others, including supervisors.

Industrial and organizational psychologists consider innovation, more often than not, a variable of less importance and often a counter-productive one to include in conducting job performance appraisals when irrelevant to the major job functions for which a given job exists. Nonetheless, I/O psychologists see the value of that variable where its consideration would, were its reliability and validity questioned, achieve a statistically significant probability that its results are not due to chance, and that it can be replicated reliably with a statistically significant ratio of reliability, and that were a court to raise a question on its reliability and validity testing, the I/O psychologist behind its use would be able to defend it before a court of justice with the belief that it will stand before such a court as reliable, and valid.

Four qualities are generally linked to creative and innovative behaviour by individuals: 

At the organizational level, a study by Damanpour identified four specific characteristics that may predict innovation:


Counterproductive work behavior (CWB) can be defined as employee behavior that goes against the goals of an organization. These behaviors can be intentional or unintentional and result from a wide range of underlying causes and motivations. Some CWBs have instrumental motivations (e.g., theft). It has been proposed that a person-by-environment interaction can be utilized to explain a variety of counterproductive behaviors (Fox and Spector, 1999). For instance, an employee who sabotages another employee's work may do so because of lax supervision (environment) and underlying psychopathology (person) that work in concert to result in the counterproductive behavior. There is evidence that an emotional response (e.g., anger) to job stress (e.g., unfair treatment) can motivate CWBs.

The forms of counterproductive behavior with the most empirical examination are ineffective job performance, absenteeism, job turnover, and accidents. Less common but potentially more detrimental forms of counterproductive behavior have also been investigated including violence and sexual harassment.

In I/O psychology, leadership can be defined as a process of influencing others to agree on a shared purpose, and to work towards shared objectives. A distinction should be made between leadership and management. Managers process administrative tasks and organize work environments. Although leaders may be required to undertake managerial duties as well, leaders typically focus on inspiring followers and creating a shared organizational culture and values. Managers deal with complexity, while leaders deal with initiating and adapting to change. Managers undertake the tasks of planning, budgeting, organizing, staffing, controlling and problem solving. In contrast, leaders undertake the tasks of setting a direction or vision, aligning people to shared goals, communicating, and motivating.

Approaches to studying leadership in I/O psychology can be broadly classified into three categories: Leader-focused approaches, contingency-focused approaches, and follower-focused approaches.

Leader-focused approaches look to organizational leaders to determine the characteristics of effective leadership. According to the trait approach, more effective leaders possess certain traits that less effective leaders lack. More recently, this approach is being used to predict leader emergence. The following traits have been identified as those that predict leader emergence when there is no formal leader: high intelligence, high needs for dominance, high self-motivation, and socially perceptive. Another leader-focused approached is the behavioral approach which focuses on the behaviors that distinguish effective from ineffective leaders. There are two categories of leadership behaviors: (1) consideration; and (2) initiating structure. Behaviors associated with the category of consideration include showing subordinates they are valued and that the leader cares about them. An example of a consideration behavior is showing compassion when problems arise in or out of the office. Behaviors associated with the category of initiating structure include facilitating the task performance of groups. One example of an initiating structure behavior is meeting one-on-one with subordinates to explain expectations and goals. The final leader-focused approach is power and influence. To be most effective a leader should be able to influence others to behave in ways that are in line with the organization's mission and goals. How influential a leader can be depends on their social power or their potential to influence their subordinates. There are six bases of power: coercive power, reward power, legitimate power, expert power, referent power, and informational power. A leader can use several different tactics to influence others within an organization. These common tactics include: rational persuasion, inspirational appeal, consultation, ingratiation, exchange, personal appeal, coalition, legitimating, and pressure.

Of the 3 approaches to leadership, contingency-focused approaches have been the most prevalent over the past 30 years. Contingency-focused theories base a leader's effectiveness on their ability to assess a situation and adapt their behavior accordingly. These theories assume that an effective leader can accurately "read" a situation and skillfully employ a leadership style that meets the needs of the individuals involved and the task at hand. A brief introduction to the most prominent contingency-focused theories will follow.

Fiedler's Contingency Theory holds that a leader's effectiveness depends on the interaction between their characteristics and the characteristics of the situation. Path–Goal Theory asserts that the role of the leader is to help his or her subordinates achieve their goals. To effectively do this, leaders must skillfully select from four different leadership styles to meet the situational factors. The situational factors are a product of the characteristics of subordinates and the characteristics of the environment. The Leader-Member Exchange (LMX) Model focuses on how leader–subordinate relationships develop. Generally speaking, when a subordinate performs well or when there are positive exchanges between a leader and a subordinate, their relationship is strengthened, performance and job satisfaction are enhanced, and the subordinate will feel more commitment to the leader and the organization as a whole. Vroom-Yetton-Jago Model focuses on decision making with respect to a "feasibility set" which is composed of the situational attributes.

In addition to the contingency-focused approaches mentioned, there has been a high degree of interest paid to three novel approaches that have recently emerged. The first is transformational leadership, which posits that there are certain leadership traits that inspire subordinates to perform beyond their capabilities. The second is transactional leadership, which is most concerned with keeping subordinates in-line with deadlines and organizational policy. This type of leader fills more of a managerial role and lacks qualities necessary to inspire subordinates and induce meaningful change. And the third is authentic leadership which is centered around empathy and a leader's values or character. If the leader understands their followers, they can inspire subordinates by cultivating a personal connection and leading them to share in the vision and goals of the team. Although there has been a limited amount of research conducted on these theories, they are sure to receive continued attention as the field of I/O psychology matures.

Follower-focused approaches look at the processes by which leaders motivate followers, and lead teams to achieve shared goals. Understandably, the area of leadership motivation draws heavily from the abundant research literature in the domain of motivation in I/O psychology. Because leaders are held responsible for their followers' ability to achieve the organization's goals, their ability to motivate their followers is a critical factor of leadership effectiveness. Similarly, the area of team leadership draws heavily from the research in teams and team effectiveness in I/O psychology. Because organizational employees are frequently structured in the form of teams, leaders need to be aware of the potential benefits and pitfalls of working in teams, how teams develop, how to satisfy team members' needs, and ultimately how to bring about team effectiveness and performance.

An emerging area of I/O research in the area of team leadership is in leading virtual teams, where people in the team are geographically-distributed across various distances and sometimes even countries. While technological advances have enabled the leadership process to take place in such virtual contexts, they present new challenges for leaders as well, such as the need to use technology to build relationships with followers, and influencing followers when faced with limited (or no) face-to-face interaction.

I/O psychologists are also concerned with organizational change. This effort, called organizational development (OD). Tools used to advance organization development include the survey feedback technique. The technique involves the periodic assessment (with surveys) of employee attitudes and feelings. The results are conveyed to organizational stakeholders, who may want to take the organization in a particular direction. Another tool is the team building technique. Because many if not most tasks within the organization are completed by small groups and/or teams, team building is important to organizational success. In order to enhance a team's morale and problem-solving skills, I/O psychologists help the groups to build their self-confidence, group cohesiveness, and working effectiveness.

The I/O psychology and organizational behavior have manifested some overlap. The overlap has led to some confusion regarding how the two disciplines differ.

The minimum requirement for working as an I/O psychologist is a master's degree. Normally, this degree requires about 2–3 years to complete. Of all the degrees granted in I/O psychology each year, approximately two thirds are at the master's level.
A comprehensive list of US and Canadian master's and doctoral programs can be found at the web site of the Society for Industrial and Organizational Psychology (SIOP). Admission into I/O psychology PhD programs is highly competitive given that many programs accept a small number of applicants every year.
There are graduate degree programs in I/O psychology outside of the US and Canada. The SIOP web site also provides a comprehensive list of I/O programs in many other countries.

There are many different sets of competencies for different specializations within I/O psychology and I/O psychologists are versatile behavioral scientists. For example, an I/O psychologist specializing in selection and recruiting should have expertise in finding the best talent for the organization and getting everyone on board while he or she might not need to know much about executive coaching. Some I/O psychologists specialize in specific areas of consulting whereas others tend to generalize their areas of expertise. There are basic skills and knowledge an individual needs in order to be an effective I/O psychologist, which include being an independent learner, interpersonal skills (e.g., listening skills), and general consultation skills (e.g., skills and knowledge in the problem area).

According to the United States Department of Labor's Bureau of Labor Statistics, I/O psychology is the fastest growing occupation in the United States, based on projections between 2012 and 2022. 
In a 2006 salary survey, the median salary for a PhD in I/O psychology was $98,000; for a master's level I/O psychologist was $72,000. The highest paid PhD I/O psychologists in private industry worked in pharmaceuticals and averaged approximately $151,000 per year; the median salary for self-employed consultants was $150,000; those employed in retail, energy, and manufacturing followed closely behind, averaging approximately $133,000. The lowest earners were found in state and local government positions, averaging approximately $77,000. In 2005, I/O psychologists whose primary responsibility is teaching at private and public colleges and universities often earn additional income from consulting with government and industry.The job opportunities have been on a steady rise for the past few years 

An I/O psychologist, whether an academic, a consultant, or an employee, must maintain high ethical standards. The APA's ethical principles apply to I/O psychologists. For example, ethically, the I/O psychologist should only accept projects for which he or she is qualified. With more organizations becoming global, it is important that when an I/O psychologist works outside her or his home country, the psychologist is aware of rules, regulations, and cultures of the organizations and countries in which the psychologist works, while adhering to the ethical standards set at home.



</doc>
<doc id="15451" url="https://en.wikipedia.org/wiki?curid=15451" title="International Council of Unitarians and Universalists">
International Council of Unitarians and Universalists

The International Council of Unitarians and Universalists (ICUU) is an umbrella organization founded in 1995 bringing together many Unitarian, Universalist, and Unitarian Universalist organizations. 
The size of the affiliated organizations varies widely. Some groups represent only a few hundred people; while the largest, the Unitarian Universalist Association, has over 160,000 members and is larger than all the other groups put together.

The original initiative for its establishment was contained in a resolution of the General Assembly of Unitarian and Free Christian Churches (British Unitarians) in 1987. This led to the establishment of the Advocates for the Establishment of an International Organization of Unitarians (AEIOU), which worked towards creating the council. However, the General Assembly resolution provided no funding.

The Unitarian Universalist Association (UUA) became particularly interested in the establishment of a council when it had to deal with an increasing number of applications for membership from congregations outside North America. It had already granted membership to congregations in Adelaide, Auckland, the Philippines and Pakistan, and congregations in Sydney, Russia and Spain had applied for membership. Rather than admit congregations from all over the world, the UUA hoped that they would join a world council instead. The UUA thus became willing to provide funding for the council's establishment.

As a result, the council was finally established at a meeting in Essex, Massachusetts, United States on 23–26 March 1995.

The Preamble to the Constitution of the International Council of Unitarians and Universalists reads:

We, the member groups of the International Council of Unitarians and Universalists, affirming our belief in religious community based on:


declare our purposes to be:



Polish Unitarians have reported a need for a period of reorganization, and that at this time they are unable to maintain the level of activity needed to be full Council members, be it moved that membership of these groups be suspended. This action is taken with regret and the ICUU looks forward to welcoming Poland back into membership at the earliest possible date.

Churches and religious associations which have expressed their will to become members of the Council may be admitted as "Provisional Members" for a period of time (generally two or four years), until the Council decides that they have shown their organizational stability, affinity with the ICUU principles and commitment to deserve becoming Full Members of the Council. Provisional Members are invited to Council meetings through a delegate but cannot vote.


According to the Bylaws of the ICUU, Emerging Groups are ""applicants that are deemed to be reasonable prospects for membership, but do not fulfil the conditions of either Provisional membership or Full Membership"". These groups may be designated as Emerging Groups by the Executive Committee upon its sole discretion. Emerging Groups may be invited as observers to General Meetings.

The current list of Emerging Groups after the last meeting of the Executive Committee (London, 22–25 November 2008) is as follows:

Organizations with beliefs and purposes closely akin to those of ICUU but which by nature of their constitution are not eligible for full membership or which do not wish to become full members now or in the foreseeable future, may become Associates of the ICUU. The application must be approved by the ICUU Council Meeting.





</doc>
<doc id="15454" url="https://en.wikipedia.org/wiki?curid=15454" title="Itanium">
Itanium

Itanium ( ) is a family of 64-bit Intel microprocessors that implement the Intel Itanium architecture (formerly called IA-64). Intel markets the processors for enterprise servers and high-performance computing systems. The Itanium architecture originated at Hewlett-Packard (HP), and was later jointly developed by HP and Intel.

Itanium-based systems have been produced by HP (the HP Integrity Servers line) and several other manufacturers. In 2008, Itanium was the fourth-most deployed microprocessor architecture for enterprise-class systems, behind x86-64, Power Architecture, and SPARC.

In February 2017, Intel released the current generation, , to test customers, and in May began shipping in volume. It is the last processor of the Itanium family.

In 1989, HP determined that Reduced Instruction Set Computing (RISC) architectures were approaching a processing limit at one instruction per cycle. HP researchers investigated a new architecture, later named Explicitly Parallel Instruction Computing (EPIC), that allows the processor to execute multiple instructions in each clock cycle. EPIC implements a form of very long instruction word (VLIW) architecture, in which a single instruction word contains multiple instructions. With EPIC, the compiler determines in advance which instructions can be executed at the same time, so the microprocessor simply executes the instructions and does not need elaborate mechanisms to determine which instructions to execute in parallel.
The goal of this approach is twofold: to enable deeper inspection of the code at compile time to identify additional opportunities for parallel execution, and to simplify processor design and reduce energy consumption by eliminating the need for runtime scheduling circuitry.

HP believed that it was no longer cost-effective for individual enterprise systems companies such as itself to develop proprietary microprocessors, so it partnered with Intel in 1994 to develop the IA-64 architecture, derived from EPIC. Intel was willing to undertake a very large development effort on IA-64 in the expectation that the resulting microprocessor would be used by the majority of enterprise systems manufacturers. HP and Intel initiated a large joint development effort with a goal of delivering the first product, Merced, in 1998.

During development, Intel, HP, and industry analysts predicted that IA-64 would dominate in servers, workstations, and high-end desktops, and eventually supplant RISC and complex instruction set computing (CISC) architectures for all general-purpose applications.
Compaq and Silicon Graphics decided to abandon further development of the Alpha and MIPS architectures respectively in favor of migrating to IA-64.

Several groups ported operating systems for the architecture, including Microsoft Windows, OpenVMS, Linux, HP-UX, Solaris,
Tru64 UNIX, and Monterey/64.
The latter three were canceled before reaching the market. By 1997, it was apparent that the IA-64 architecture and the compiler were much more difficult to implement than originally thought, and the delivery timeframe of Merced began slipping.
Technical difficulties included the very high transistor counts needed to support the wide instruction words and the large caches. There were also structural problems within the project, as the two parts of the joint team used different methodologies and had slightly different priorities. Since Merced was the first EPIC processor, the development effort encountered more unanticipated problems than the team was accustomed to. In addition, the EPIC concept depends on compiler capabilities that had never been implemented before, so more research was needed.

Intel announced the official name of the processor, "Itanium", on October 4, 1999.
Within hours, the name "Itanic" had been coined on a Usenet newsgroup, a reference to the RMS "Titanic", the "unsinkable" ocean liner that sank on her maiden voyage in 1912. "Itanic" has since often been used by "The Register", and others, to imply that the multibillion-dollar investment in Itanium—and the early hype associated with it—would be followed by its relatively quick demise.

By the time Itanium was released in June 2001, its performance was not superior to competing RISC and CISC processors.
Itanium competed at the low-end (primarily four-CPU and smaller systems) with servers based on x86 processors, and at the high end with IBM's POWER architecture and Sun Microsystems's SPARC architecture. Intel repositioned Itanium to focus on high-end business and HPC computing, attempting to duplicate x86's successful "horizontal" market (i.e., single architecture, multiple systems vendors). The success of this initial processor version was limited to replacing PA-RISC in HP systems, Alpha in Compaq systems and MIPS in SGI systems, though IBM also delivered a supercomputer based on this processor.
POWER and SPARC remained strong, while the 32-bit x86 architecture continued to grow into the enterprise space, building on economies of scale fueled by its enormous installed base.

Only a few thousand systems using the original "Merced" Itanium processor were sold, due to relatively poor performance, high cost and limited software availability. Recognizing that the lack of software could be a serious problem for the future, Intel made thousands of these early systems available to independent software vendors (ISVs) to stimulate development. HP and Intel brought the next-generation Itanium 2 processor to market a year later.

The Itanium 2 processor was released in 2002, and was marketed for enterprise servers rather than for the whole gamut of high-end computing. The first Itanium 2, code-named "McKinley", was jointly developed by HP and Intel. It relieved many of the performance problems of the original Itanium processor, which were mostly caused by an inefficient memory subsystem. "McKinley" contains 221 million transistors (of which 25 million are for logic), measured 19.5 mm by 21.6 mm (421 mm) and was fabricated in a 180 nm, bulk CMOS process with six layers of aluminium metallization.

In 2003, AMD released the Opteron CPU, which implements its own 64-bit architecture called AMD64. Opteron gained rapid acceptance in the enterprise server space because it provided an easy upgrade from x86. Under influence by Microsoft, Intel responded by implementing AMD's x86-64 instruction set architecture instead of IA-64 in its Xeon microprocessors in 2004, resulting in a new industry-wide "de facto" standard.

Intel released a new Itanium 2 family member, codenamed "Madison", in 2003. Madison uses a 130 nm process and was the basis of all new Itanium processors until Montecito was released in June 2006.

In March 2005, Intel announced that it was working on a new Itanium processor, codenamed "Tukwila", to be released in 2007. Tukwila would have four processor cores and would replace the Itanium bus with a new Common System Interface, which would also be used by a new Xeon processor.
Later that year, Intel revised Tukwila's delivery date to late 2008.

In November 2005, the major Itanium server manufacturers joined with Intel and a number of software vendors to form the Itanium Solutions Alliance to promote the architecture and accelerate software porting.
The Alliance announced that its members would invest $10 billion in Itanium solutions by the end of the decade.

In 2006, Intel delivered "Montecito" (marketed as the Itanium 2 9000 series), a dual-core processor that roughly doubled performance and decreased energy consumption by about 20 percent.

Intel released the Itanium 2 9100 series, codenamed "Montvale", in November 2007.
In May 2009, the schedule for Tukwila, its follow-on, was revised again, with release to OEMs planned for the first quarter of 2010.

The Itanium 9300 series processor, codenamed "Tukwila", was released on February 8, 2010, with greater performance and memory capacity.

The device uses a 65 nm process, includes two to four cores, up to 24 MB on-die caches, Hyper-Threading technology and integrated memory controllers. It implements double-device data correction, which helps to fix memory errors. Tukwila also implements Intel QuickPath Interconnect (QPI) to replace the Itanium bus-based architecture. It has a peak interprocessor bandwidth of 96 GB/s and a peak memory bandwidth of 34 GB/s. With QuickPath, the processor has integrated memory controllers and interfaces the memory directly, using QPI interfaces to directly connect to other processors and I/O hubs. QuickPath is also used on Intel processors using the "Nehalem" microarchitecture, making it probable that Tukwila and Nehalem will be able to use the same chipsets.
Tukwila incorporates four memory controllers, each of which supports multiple DDR3 DIMMs via a separate memory controller,
much like the Nehalem-based Xeon processor code-named "Beckton".

The Itanium 9500 series processor, codenamed "Poulson", is the follow-on processor to Tukwila and was released on November 8, 2012.
According to Intel, it skips the 45 nm process technology and uses a 32 nm process technology. It features eight cores and has a 12-wide issue architecture, multithreading enhancements, and new instructions to take advantage of parallelism, especially in virtualization.
The Poulson L3 cache size is 32 MB. L2 cache size is 6 MB, 512 I KB, 256 D KB per core. Die size is 544 mm², less than its predecessor Tukwila (698.75 mm²).

At ISSCC 2011, Intel presented a paper called "A 32nm 3.1 Billion Transistor 12-Wide-Issue Itanium Processor for Mission Critical Servers."
Given Intel's history of disclosing details about Itanium microprocessors at ISSCC, this paper most likely refers to Poulson. Analyst David Kanter speculates that Poulson will use a new microarchitecture, with a more advanced form of multithreading that uses up to two threads, to improve performance for single threaded and multithreaded workloads.
Some new information was released at the Hot Chips conference.

New information presents improvements in multithreading, resilency improvements (Intel Instruction Replay RAS) and few new instructions (thread priority, integer instruction, cache prefetching, and data access hints).

Intel's Product Change Notification (PCN) 111456-01 lists four models of Itanium 9500 series CPU, which was later removed in a revised document. The parts were later listed in Intel's Material Declaration Data Sheets (MDDS) database. Intel later posted Itanium 9500 reference manual.

The models are the following:

During the 2012 "Hewlett-Packard Co. v. Oracle Corp." support lawsuit, court documents unsealed by a Santa Clara County Court judge revealed that in 2008, Hewlett-Packard had paid Intel around $440 million to keep producing and updating Itanium microprocessors from 2009 to 2014. In 2010, the two companies signed another $250 million deal, which obliged Intel to continue making Itanium CPUs for HP's machines until 2017. Under the terms of the agreements, HP has to pay for chips it gets from Intel, while Intel launches Tukwila, Poulson, Kittson, and Kittson+ chips in a bid to gradually boost performance of the platform.

Rumors of a successor to Poulson (code named "Kittson") began to circulate in 2012–2013. This was at first associated with a forthcoming 22 nm process shrink, and later revised in the face of declining Itanium sales to a less-ambitious 32 nm node. In April 2015, Intel, although it had not yet confirmed formal specifications, did confirm that it continued to work on the project. Meanwhile, the aggressively multicore Xeon E7 platform displaced Itanium-based solutions in the Intel roadmap.

In July 2016, the HP spinoff called Hewlett Packard Enterprise (HPE) announced in Computer World that Kittson would be released mid-2017. In February 2017, Intel reported that it was shipping Kittson to test customers, with plans to ship in volume later that year.

Intel officially launched the Itanium 9700 series processor family on May 11, 2017. Notably, Kittson has no microarchitecture improvements over Poulson, only higher clock speeds.

Intel has announced that the 9700 series will be the last Itanium chips produced.

The models are:

In comparison with its Xeon family of server processors, Itanium has never been a high-volume product for Intel. Intel does not release production numbers. One industry analyst estimated that the production rate was 200,000 processors per year in 2007.

According to Gartner Inc., the total number of Itanium servers (not processors) sold by all vendors in 2007, was about 55,000. (It is unclear whether clustered servers counted as a single server or not.) This compares with 417,000 RISC servers (spread across all RISC vendors) and 8.4 million x86 servers. IDC reports that a total of 184,000 Itanium-based systems were sold from 2001 through 2007. For the combined POWER/SPARC/Itanium systems market, IDC reports that POWER captured 42% of revenue and SPARC captured 32%, while Itanium-based system revenue reached 26% in the second quarter of 2008.
According to an IDC analyst, in 2007, HP accounted for perhaps 80% of Itanium systems revenue.
According to Gartner, in 2008, HP accounted for 95% of Itanium sales. HP's Itanium system sales were at an annual rate of $4.4Bn at the end of 2008, and declined to $3.5Bn by the end of 2009,
compared to a 35% decline in UNIX system revenue for Sun and an 11% drop for IBM, with an x86-64 server revenue increase of 14% during this period.

In December 2012, IDC released a research report stating that Itanium server shipments would remain flat through 2016, with annual shipment of 26,000 systems (a decline of over 50% compared to shipments in 2008).

By 2006, HP manufactured at least 80% of all Itanium systems, and sold 7,200 in the first quarter of 2006.
The bulk of systems sold were enterprise servers and machines for large-scale technical computing, with an average selling price per system in excess of US$200,000. A typical system uses eight or more Itanium processors.

By 2012, only a few manufacturers offered Itanium systems, including HP, Bull, NEC, Inspur and Huawei. In addition, Intel offered a chassis that could be used by system integrators to build Itanium systems.

By 2015, only HP supplied Itanium-based systems.

The Itanium bus interfaces to the rest of the system via a chipset. Enterprise server manufacturers differentiate their systems by designing and developing chipsets that interface the processor to memory, interconnections, and peripheral controllers. The chipset is the heart of the system-level architecture for each system design. Development of a chipset costs tens of millions of dollars and represents a major commitment to the use of the Itanium. IBM created a chipset in 2003, and Intel in 2002, but neither of them developed chipsets to support newer technologies such as DDR2 or PCI Express.
Before "Tukwila" moved away from the FSB, chipsets supporting such technologies were manufactured by all Itanium server vendors, such as HP, Fujitsu, SGI, NEC, and Hitachi.

The "Tukwila" Itanium processor model had been designed to share a common chipset with the Intel Xeon processor EX (Intel's Xeon processor designed for four processor and larger servers). The goal was to streamline system development and reduce costs for server OEMs, many of which develop both Itanium- and Xeon-based servers. However, in 2013, this goal was pushed back to "evaluated for future implementation opportunities".

Itanium is or was supported (i.e. Windows version can no longer be bought) by the following operating systems:


Microsoft announced that Windows Server 2008 R2 would be the last version of Windows Server to support the Itanium (support started with XP), and that it would also discontinue development of the Itanium versions of Visual Studio and SQL Server.

Likewise, Red Hat Enterprise Linux 5 (first released in March 2007) was the last Itanium edition of Red Hat Enterprise Linux
and Debian no longer supports Itanium and in addition Canonical chose to not support Itanium for Ubuntu 10.04 LTS (released in April 2010, now discontinued).
HP will not be supporting or certifying Linux on Itanium 9300 (Tukwila) servers.

In late September 2012, NEC announced a return from IA-64 to the previous NOAH line of proprietary mainframe processors, now produced in a quad-core variant on 40 nm, called NOAH-6.

HP sells a virtualization technology for Itanium called Integrity Virtual Machines.

To allow more software to run on the Itanium, Intel supported the development of compilers optimized for the platform, especially its own suite of compilers.
Starting in November 2010, with the introduction of new product suites, the Intel Itanium Compilers were no longer bundled with the Intel x86 compilers in a single product. Intel offers Itanium tools and Intel x86 tools, including compilers, independently in different product bundles.
GCC,
Open64 and Microsoft Visual Studio 2005 (and later)
are also able to produce machine code for Itanium. According to the Itanium Solutions Alliance over 13,000 applications were available for Itanium-based systems in early 2008,
though Sun has contested Itanium application counts in the past.
The ISA also supported Gelato, an Itanium HPC user group and developer community that ported and supported open source software for Itanium.

Emulation is a technique that allows a computer to execute binary code that was compiled for a different type of computer. Before IBM's acquisition of QuickTransit in 2009, application binary software for IRIX/MIPS and Solaris/SPARC could run via type of emulation called "dynamic binary translation" on Linux/Itanium. Similarly, HP implemented a method to execute PA-RISC/HP-UX on the Itanium/HP-UX via emulation, to simplify migration of its PA-RISC customers to the radically different Itanium instruction set. Itanium processors can also run the mainframe environment GCOS from Groupe Bull and several x86 operating systems via instruction set simulators.

Itanium is aimed at the enterprise server and high-performance computing (HPC) markets. Other enterprise- and HPC-focused processor lines include Oracle's and Fujitsu's SPARC processors and IBM's POWER microprocessors. Measured by quantity sold, Itanium's most serious competition comes from x86-64 processors including Intel's own Xeon line and AMD's Opteron line. Since 2009, most servers were being shipped with x86-64 processors.

In 2005, Itanium systems accounted for about 14% of HPC systems revenue, but the percentage has declined as the industry shifted to x86-64 clusters for this application.

An October 2008 Gartner report on the Tukwila processor, stated that "...the future roadmap for Itanium looks as strong as that of any RISC peer like Power or SPARC."

An Itanium-based computer first appeared on the list of the TOP500 supercomputers in November 2001. The best position ever achieved by an "Itanium 2" based system in the list was #2 (while now all systems have dropped off the list), achieved in June 2004, when Thunder (Lawrence Livermore National Laboratory) entered the list with an Rmax of 19.94 Teraflops. In November 2004, Columbia entered the list at #2 with 51.8 Teraflops, and there was at least one Itanium-based computer in the top 10 from then until June 2007. The peak number of Itanium-based machines on the list occurred in the November 2004 list, at 84 systems (16.8%); by June 2012, this had dropped to one system (0.2%), and no Itanium system remained on the list in November 2012.

The Itanium processors show a progression in capability. Merced was a proof of concept. McKinley dramatically improved the memory hierarchy and allowed Itanium to become reasonably competitive. Madison, with the shift to a 130 nm process, allowed for enough cache space to overcome the major performance bottlenecks. Montecito, with a 90 nm process, allowed for a dual-core implementation and a major improvement in performance per watt. Montvale added three new features: core-level lockstep, demand-based switching and front-side bus frequency of up to 667 MHz.

When first released in 2001, Itanium's performance was disappointing compared to better-established RISC and CISC processors. Emulation to run existing x86 applications and operating systems was particularly poor, with one benchmark in 2001 reporting that it was equivalent at best to a 100 MHz Pentium in this mode (1.1 GHz Pentiums were on the market at that time).
Itanium failed to make significant inroads against IA-32 or RISC, and suffered further following the arrival of x86-64 systems which offered greater compatibility with older x86 applications.

In a 2009 article on the history of the processor — "How the Itanium Killed the Computer Industry" — journalist John C. Dvorak reported "This continues to be one of the great fiascos of the last 50 years". Tech columnist Ashlee Vance commented that the delays and underperformance "turned the product into a joke in the chip industry". In an interview, Donald Knuth said "The Itanium approach...was supposed to be so terrific—until it turned out that the wished-for compilers were basically impossible to write."

Both Red Hat and Microsoft announced plans to drop Itanium support in their operating systems due to lack of market interest; however, other Linux distributions such as Gentoo and Debian remain available for Itanium. On March 22, 2011, Oracle Corporation announced that it would no longer develop new products for HP-UX on Itanium, although it would continue to provide support for existing products. Following this announcement, HP sued Oracle for breach of contract, arguing that Oracle had violated conditions imposed during settlement over Oracle's hiring of former HP CEO Mark Hurd as its co-CEO, requiring the vendor to support Itanium on its software "until such time as HP discontinues the sales of its Itanium-based servers", and that the breach had harmed its business. In 2012, a court ruled in favor of HP, and ordered Oracle to resume its support for Itanium. In June 2016, Hewlett-Packard Enterprise (the corporate successor to HP's server business) was awarded $3 billion in damages from the lawsuit.

A former Intel official reported that the Itanium business had become profitable for Intel in late 2009. By 2009, the chip was almost entirely deployed on servers made by HP, which had over 95% of the Itanium server market share, making the main operating system for Itanium HP-UX. On March 22, 2011, Intel reaffirmed its commitment to Itanium with multiple generations of chips in development and on schedule.

Although Itanium did attain limited success in the niche market of high-end computing, Intel had originally hoped it would find broader acceptance as a replacement for the original x86 architecture.

AMD chose a different direction, designing the less radical x86-64, a 64-bit extension to the existing x86 architecture, which Microsoft then supported, forcing Intel to introduce the same extensions in its own x86-based processors. These designs can run existing 32-bit applications at native hardware speed, while offering support for 64-bit memory addressing and other enhancements to new applications. This architecture has now become the predominant 64-bit architecture in the desktop and portable market. Although some Itanium-based workstations were initially introduced by companies such as SGI, they are no longer available.

1989
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
2009
2010
2011
2012
2013
2014
2017




</doc>
<doc id="15459" url="https://en.wikipedia.org/wiki?curid=15459" title="International Statistical Classification of Diseases and Related Health Problems">
International Statistical Classification of Diseases and Related Health Problems

The International Classification of Diseases (ICD) is the international "standard diagnostic tool for epidemiology, health management and clinical purposes." Its full official name is International Statistical Classification of Diseases and Related Health Problems.

The ICD is maintained by the World Health Organization (WHO), the directing and coordinating authority for health within the United Nations System. The ICD is originally designed as a health care classification system, providing a system of diagnostic codes for classifying diseases, including nuanced classifications of a wide variety of signs, symptoms, abnormal findings, complaints, social circumstances, and external causes of injury or disease. This system is designed to map health conditions to corresponding generic categories together with specific variations, assigning for these a designated code, up to six characters long. Thus, major categories are designed to include a set of similar diseases. ICD-11 is a major step forward, because it has the necessary terminological and ontological elements for seamless use in digital health.

The ICD is published by the WHO and used worldwide for morbidity and mortality statistics, reimbursement systems, and automated decision support in health care. This system is designed to promote international comparability in the collection, processing, classification, and presentation of these statistics. Like the analogous "Diagnostic and Statistical Manual of Mental Disorders" (which is limited to psychiatric disorders and almost exclusive to the United States), the ICD is a major project to statistically classify all health disorders, and provide diagnostic assistance. The ICD is a core statistically based classificatory diagnostic system for health care related issues of the WHO Family of International Classifications (WHO-FIC).

The ICD is revised periodically and is currently in its 10th revision. ICD-10, as it is therefore known, is from 1992 and the WHO publishes annual minor updates and triennial major updates. The final draft of the ICD-11 system is expected to be submitted to WHO's World Health Assembly (WHA) for official endorsement in 2019. The version for preparation of approval at the WHA was released on 18 June 2018.

The ICD is part of a "family" of international classifications (WHOFIC) that complement each other, including also the International Classification of Functioning, Disability and Health (ICF) which focuses on the domains of functioning (disability) associated with health conditions, from both medical and social perspectives, and the International Classification of Health Interventions (ICHI) that classifies the whole range of medical, nursing, functioning and public health interventions.

In 1860, during the international statistical congress held in London, Florence Nightingale made a proposal that was to result in the development of the first model of systemic collection of hospital data. In 1893, a French physician, Jacques Bertillon, introduced the "Bertillon Classification of Causes of Death" at a congress of the International Statistical Institute in Chicago.

A number of countries adopted Bertillon's system, which was based on the principle of distinguishing between general diseases and those localized to a particular organ or anatomical site, as used by the City of Paris for classifying deaths. Subsequent revisions represented a synthesis of English, German, and Swiss classifications, expanding from the original 44 titles to 161 titles. In 1898, the American Public Health Association (APHA) recommended that the registrars of Canada, Mexico, and the United States also adopt it. The APHA also recommended revising the system every 10 years to ensure the system remained current with medical practice advances. As a result, the first international conference to revise the International Classification of Causes of Death took place in 1900, with revisions occurring every ten years thereafter. At that time, the classification system was contained in one book, which included an Alphabetic Index as well as a Tabular List. The book was small compared with current coding texts.

The revisions that followed contained minor changes, until the sixth revision of the classification system. With the sixth revision, the classification system expanded to two volumes. The sixth revision included morbidity and mortality conditions, and its title was modified to reflect the changes: International Statistical Classification of Diseases, Injuries and Causes of Death (ICD). Prior to the sixth revision, responsibility for ICD revisions fell to the Mixed Commission, a group composed of representatives from the International Statistical Institute and the Health Organization of the League of Nations. In 1948, the WHO assumed responsibility for preparing and publishing the revisions to the ICD every ten years. WHO sponsored the seventh and eighth revisions in 1957 and 1968, respectively. It later became clear that the established ten year interval between revisions was too short.

The ICD is currently the most widely used statistical classification system for diseases in the world. In addition, some countries—including Australia, Canada, and the United States—have developed their own adaptations of ICD, with more procedure codes for classification of operative or diagnostic procedures.

The ICD-6, published in 1949, was the first to be shaped to become suitable for morbidity reporting. Accordingly, the name changed from International List of Causes of Death to International Statistical Classification of Diseases. The combined code section for injuries and their associated accidents was split into two, a chapter for injuries, and a chapter for their external causes. With use for morbidity there was a need for coding mental conditions, and for the first time a section on mental disorders was added .

The international Conference for the Seventh Revision of the International Classification of Diseases was held in Paris under the auspices of WHO in February 1955. In accordance with a recommendation of the WHO Expert Committee on Health Statistics, this revision was limited to essential changes and amendments of errors and inconsistencies.

The 8th Revision Conference convened by WHO met in Geneva, from 6 to 12 July 1965. This revision was more radical than the Seventh but left unchanged the basic structure of the Classification and the general philosophy of classifying diseases, whenever possible, according to their etiology rather than a particular manifestation.
During the years that the Seventh and Eighth Revisions of the ICD were in force, the use of the ICD for indexing hospital medical records increased rapidly and some countries prepared national adaptations which provided the additional detail needed for this application of the ICD. 
In the USA, a group of consultants was asked to study the 8th revision of ICD (ICD-8a) for its applicability to various users in the United States. This group recommended that further detail be provided for coding hospital and morbidity data. The American Hospital Association's "Advisory Committee to the Central Office on ICDA" developed the needed adaptation proposals, resulting in the publication of the International Classification of Diseases, Adapted (ICDA). In 1968, the United States Public Health Service published the International Classification of Diseases, Adapted, 8th Revision for use in the United States (ICDA-8a). Beginning in 1968, ICDA-8a served as the basis for coding diagnostic data for both official morbidity [and mortality] statistics in the United States.

The International Conference for the Ninth Revision of the International Statistical Classification of Diseases, Injuries, and Causes of Death, convened by WHO, met in Geneva from 30 September to 6 October 1975. In the discussions leading up to the conference, it had originally been intended that there should be little change other than updating of the classification. This was mainly because of the expense of adapting data processing systems each time the classification was revised.

There had been an enormous growth of interest in the ICD and ways had to be found of responding to this, partly by modifying the classification itself and partly by introducing special coding provisions. A number of representations were made by specialist bodies which had become interested in using the ICD for their own statistics. Some subject areas in the classification were regarded as inappropriately arranged and there was considerable pressure for more detail and for adaptation of the classification to make it more relevant for the evaluation of medical care, by classifying conditions to the chapters concerned with the part of the body affected rather than to those dealing with the underlying generalized disease.

At the other end of the scale, there were representations from countries and areas where a detailed and sophisticated classification was irrelevant, but which nevertheless needed a classification based on the ICD in order to assess their progress in health care and in the control of disease. A field test with a bi-axial classification approach—one axis (criterion) for anatomy, with another for etiology—showed the impracticability of such approach for routine use.

The final proposals presented to and accepted by the Conference in 1978 retained the basic structure of the ICD, although with much additional detail at the level of the four digit subcategories, and some optional five digit subdivisions. For the benefit of users not requiring such detail, care was taken to ensure that the categories at the three digit level were appropriate.

For the benefit of users wishing to produce statistics and indexes oriented towards medical care, the 9th Revision included an optional alternative method of classifying diagnostic statements, including information about both an underlying general disease and a manifestation in a particular organ or site. This system became known as the dagger and asterisk system and is retained in the Tenth Revision. A number of other technical innovations were included in the Ninth Revision, aimed at increasing its flexibility for use in a variety of situations.

It was eventually replaced by ICD-10, the version currently in use by the WHO and most countries. Given the widespread expansion in the tenth revision, it is not possible to convert ICD-9 data sets directly into ICD-10 data sets, although some tools are available to help guide users.
Publication of ICD-9 without IP restrictions in a world with evolving electronic data systems led to a range of products based on ICD-9, such as MeDRA or the Read directory.

ICPM

When ICD-9 was published by the World Health Organization (WHO), the International Classification of Procedures in Medicine (ICPM) was also developed (1975) and published (1978). The ICPM surgical procedures fascicle was originally created by the United States, based on its adaptations of ICD (called ICDA), which had contained a procedure classification since 1962. ICPM is published separately from the ICD disease classification as a series of supplementary documents called fascicles (bundles or groups of items). Each fascicle contains a classification of modes of laboratory, radiology, surgery, therapy, and other diagnostic procedures. Many countries have adapted and translated the ICPM in parts or as a whole and are using it with amendments since then.

ICD-9-CM

"International Classification of Diseases, Clinical Modification" (ICD-9-CM) is an adaption created by the U.S. National Center for Health Statistics (NCHS) and used in assigning diagnostic and procedure codes associated with inpatient, outpatient, and physician office utilization in the United States. The ICD-9-CM is based on the ICD-9 but provides for additional morbidity detail. It is updated annually on October 1.

It consists of two or three volumes: 

The NCHS and the Centers for Medicare and Medicaid Services are the U.S. governmental agencies responsible for overseeing all changes and modifications to the ICD-9-CM.

Work on ICD-10 began in 1983, and the new revision was endorsed by the Forty-third World Health Assembly in May 1990. The latest version came into use in WHO Member States starting in 1994. The classification system allows more than 155,000 different codes and permits tracking of many new diagnoses and procedures, a significant expansion on the 17,000 codes available in ICD-9.
Adoption was relatively swift in most of the world. Several materials are made available online by WHO to facilitate its use, including a manual, training guidelines, a browser, and files for download. Some countries have adapted the international standard, such as the "ICD-10-AM" published in Australia in 1998 (also used in New Zealand), and the "ICD-10-CA" introduced in Canada in 2000.

ICD-10-CM

Adoption of ICD-10-CM was slow in the United States. Since 1979, the US had required ICD-9-CM codes for Medicare and Medicaid claims, and most of the rest of the American medical industry followed suit. On 1 January 1999 the ICD-10 (without clinical extensions) was adopted for reporting mortality, but ICD-9-CM was still used for morbidity. Meanwhile, NCHS received permission from the WHO to create a clinical modification of the ICD-10, and has production of all these systems:


On 21 August 2008, the US Department of Health and Human Services (HHS) proposed new code sets to be used for reporting diagnoses and procedures on health care transactions. Under the proposal, the ICD-9-CM code sets would be replaced with the ICD-10-CM code sets, effective 1 October 2013. On 17 April 2012 the Department of Health and Human Services (HHS) published a proposed rule that would delay, from 1 October 2013 to 1 October 2014, the compliance date for the ICD-10-CM and PCS. Once again, Congress delayed implementation date to 1 October 2015, after it was inserted into "Doc Fix" Bill without debate over objections of many.

Revisions to ICD-10-CM Include:
ICD-10-CA

ICD-10-CA is a clinical modification of ICD-10 developed by the Canadian Institute for Health Information for morbidity classification in Canada. ICD-10-CA applies beyond acute hospital care, and includes conditions and situations that are not diseases but represent risk factors to health, such as occupational and environmental factors, lifestyle and psycho-social circumstances.

The World Health Organization has revised the International Classification of Diseases (ICD) towards the ICD-11. Its development has taken place on an internet-based workspace that continues to be used as the maintenance platform for discussions, and proposals for updates of ICD. Anybody can submit an evidence based proposal. The proposals are processed in an open tranparent way with reviews for scientific evidence, and usability and utility in the various uses of ICD.
It is envisaged, that there will be no need for national modifications of ICD-11, due to its richness and flexibility in the reportable detail 

The final draft of the ICD-11 system is expected to be submitted to WHO's World Health Assembly (WHA) for official endorsement in 2019. The version for implementation (preparation of approval at the WHA) was released on 18 June 2018.

ICD-11 comes with an implementation package that includes transition tables from and to ICD-10, a translation tool, a coding tool, web-services, a manual, training material, and more. All tools are accessible after self-registration from the maintenance platform.

The official release is accessed via icd.who.int


An external review of the ICD-11 Revision has been completed. The report notes the progress in the ICD Revision, and makes clear recommendations about forward progress in the revision.


ICD-11 invokes a more sophisticated architecture than historical versions, consistent with its generation as a digital resource. The core content of the system, called the Foundation Component, is a semantic network of words and terms, where any given term can have more than one parent. To address the requirement that statistical classifications exhibit mutual exclusiveness (so events are not counted more than once) and exhaustiveness (so there is a place to tally all events), ICD11 supports the serialization of the Foundation Component into an arbitrary number of linearizations, optimized for use cases. The main linearization, presently called the Joint Linearization for Morbidity and Mortality Statistics, is the tabular format with which most traditional users will become familiar. However, other linearizations, for primary care, multiple sub-specialty derivatives, or applications such as clinical decision support are possible. Finally, preliminary work in partnership with the IHTSDO is underway to ensure that the ICD-11 Foundation Component is semantically coherent through development of the Common Ontology, a subset of SNOMED CT which will anchor the Foundation Component to terms defined through description logic.

In the United States, the U.S. Public Health Service published "The International Classification of Diseases, Adapted for Indexing of Hospital Records and Operation Classification (ICDA)," completed in 1962 and expanding the ICD-7 in a number of areas to more completely meet the indexing needs of hospitals. The U.S. Public Health Service later published the "Eighth Revision, International Classification of Diseases, Adapted for Use in the United States," commonly referred to as ICDA-8, for official national morbidity and mortality statistics. This was followed by the "ICD, 9th Revision, Clinical Modification", known as ICD-9-CM, published by the U.S. Department of Health and Human Services and used by hospitals and other healthcare facilities to better describe the clinical picture of the patient. The diagnosis component of ICD-9-CM is completely consistent with ICD-9 codes, and remains the data standard for reporting morbidity. National adaptations of the ICD-10 progressed to incorporate both clinical code (ICD-10-CM) and procedure code (ICD-10-PCS) with the revisions completed in 2003. In 2009, the U.S. Centers for Medicare and Medicaid Services announced that it would begin using ICD-10 on April 1, 2010, with full compliance by all involved parties by 2013.

The years for which causes of death in the United States have been classified by each revision as follows:

Cause of death on United States death certificates, statistically compiled by the Centers for Disease Control and Prevention (CDC), are coded in the ICD, which does not include codes for human and system factors commonly called medical errors.

The ICD includes a section classifying mental and behavioral disorders (). This has developed alongside the Diagnostic and Statistical Manual of Mental Disorders (DSM) of the American Psychiatric Association and the two manuals seek to use the same codes. The WHO is revising their classifications in these sections as part the development of the ICD-11 (scheduled for 2018), and an "International Advisory Group" has been established to guide this. Section F66 of the ICD-10 deals with classifications of psychological and behavioural disorders that are associated with sexual development and orientation. It explicitly states that "sexual orientation by itself is not to be considered a disorder," in line with the DSM and other classifications that recognise homosexuality as a normal variation in human sexuality. The Working Group has reported that there is "no evidence that [these classifications] are clinically useful" and recommended that section F66 be deleted for the ICD-11.

An international survey of psychiatrists in 66 countries comparing use of the ICD-10 and DSM-IV found that the former was more often used for clinical diagnosis while the latter was more valued for research. The ICD is actually the official system for the US, although many mental health professionals do not realize this due to the dominance of the DSM. A psychologist has stated: "Serious problems with the clinical utility of both the ICD and the DSM are widely acknowledged."


Note: Since adoption of ICD-10 CM in the USA, several online tools have been mushrooming. They all refer to that particular modification and thus are not linked here.


</doc>
<doc id="15462" url="https://en.wikipedia.org/wiki?curid=15462" title="Integral domain">
Integral domain

In mathematics, and specifically in abstract algebra, an integral domain is a nonzero commutative ring in which the product of any two nonzero elements is nonzero. Integral domains are generalizations of the ring of integers and provide a natural setting for studying divisibility.
In an integral domain, every nonzero element "a" has the cancellation property, that is, if , an equality implies .

"Integral domain" is defined almost universally as above, but there is some variation. This article follows the convention that rings have a multiplicative identity, generally denoted 1, but some authors do not follow this, by not requiring integral domains to have a multiplicative identity. Noncommutative integral domains are sometimes admitted. This article, however, follows the much more usual convention of reserving the term "integral domain" for the commutative case and using "domain" for the general case including noncommutative rings.

Some sources, notably Lang, use the term entire ring for integral domain.

Some specific kinds of integral domains are given with the following chain of class inclusions:

An "integral domain" is basically defined as a nonzero commutative ring in which the product of any two nonzero elements is nonzero.
This definition may be reformulated in a number of equivalent definitions :

A fundamental property of integral domains is that every subring of a field is an integral domain, and that, conversely, given any integral domain, one may construct a field that contains it as a subring, the field of fractions. This characterization may be viewed as a further equivalent definition:








The following rings are "not" integral domains.









In this section, "R" is an integral domain.

Given elements "a" and "b" of "R", we say that "a" divides "b", or that "a" is a divisor of "b", or that "b" is a multiple of "a", if there exists an element "x" in "R" such that "ax" = "b".

The elements that divide 1 are called the units of "R"; these are precisely the invertible elements in "R". Units divide all other elements.

If "a" divides "b" and "b" divides "a", then we say "a" and "b" are associated elements or associates. Equivalently, "a" and "b" are associates if "a" = "ub" for some unit "u".

If "q" is a nonzero non-unit, we say that "q" is an irreducible element if "q" cannot be written as a product of two non-units.

If "p" is a nonzero non-unit, we say that "p" is a prime element if, whenever "p" divides a product "ab", then "p" divides "a" or "p" divides "b". Equivalently, an element "p" is prime if and only if the principal ideal ("p") is a nonzero prime ideal. The notion of prime element generalizes the ordinary definition of prime number in the ring formula_60 except that it allows for negative prime elements.

Every prime element is irreducible. The converse is not true in general: for example, in the quadratic integer ring formula_61 the element 3 is irreducible (if it factored nontrivially, the factors would each have to have norm 3, but there are no norm 3 elements since formula_62 has no integer solutions), but not prime (since 3 divides formula_63 without dividing either factor). In a unique factorization domain (or more generally, a GCD domain), an irreducible element is a prime element.

While unique factorization does not hold in formula_61, there is unique factorization of ideals. See Lasker–Noether theorem.


The field of fractions "K" of an integral domain "R" is the set of fractions "a"/"b" with "a" and "b" in "R" and "b" ≠ 0 modulo an appropriate equivalence relation, equipped with the usual addition and multiplication operations. It is "the smallest field containing "R" " in the sense that there is an injective ring homomorphism such that any injective ring homomorphism from "R" to a field factors through "K". The field of fractions of the ring of integers formula_1 is the field of rational numbers formula_67 The field of fractions of a field is isomorphic to the field itself.

Integral domains are characterized by the condition that they are reduced (that is "x" = 0 implies "x" = 0) and irreducible (that is there is only one minimal prime ideal). The former condition ensures that the nilradical of the ring is zero, so that the intersection of all the ring's minimal primes is zero. The latter condition is that the ring have only one minimal prime. It follows that the unique minimal prime ideal of a reduced and irreducible ring is the zero ideal, so such rings are integral domains. The converse is clear: an integral domain has no nonzero nilpotent elements, and the zero ideal is the unique minimal prime ideal.

This translates, in algebraic geometry, into the fact that the coordinate ring of an affine algebraic set is an integral domain if and only if the algebraic set is an algebraic variety.

More generally, a commutative ring is an integral domain if and only if its spectrum is an integral affine scheme.

The characteristic of an integral domain is either 0 or a prime number.

If "R" is an integral domain of prime characteristic "p", then the Frobenius endomorphism "f"("x") = "x" is injective.




</doc>
<doc id="15466" url="https://en.wikipedia.org/wiki?curid=15466" title="Infundibulum">
Infundibulum

An infundibulum (Latin for "funnel"; plural, "infundibula") is a funnel-shaped cavity or organ.






</doc>
<doc id="15467" url="https://en.wikipedia.org/wiki?curid=15467" title="Interrupt latency">
Interrupt latency

In computing, interrupt latency is the time that elapses from when an interrupt is generated to when the source of the interrupt is serviced. For many operating systems, devices are serviced as soon as the device's interrupt handler is executed. Interrupt latency may be affected by microprocessor design, interrupt controllers, interrupt masking, and the operating system's (OS) interrupt handling methods.

There is usually a trade-off between interrupt latency, throughput, and processor utilization. Many of the techniques of CPU and OS design that improve interrupt latency will decrease throughput and increase processor utilization. Techniques that increase throughput may increase interrupt latency and increase processor utilization. Lastly, trying to reduce processor utilization may increase interrupt latency and decrease throughput.

Minimum interrupt latency is largely determined by the interrupt controller circuit and its configuration. They can also affect the jitter in the interrupt latency, which can drastically affect the real-time schedulability of the system. The Intel APIC Architecture is well known for producing a huge amount of interrupt latency jitter.

Maximum interrupt latency is largely determined by the methods an OS uses for interrupt handling. For example, most processors allow programs to disable interrupts, putting off the execution of interrupt handlers, in order to protect critical sections of code. During the execution of such a critical section, all interrupt handlers that cannot execute safely within a critical section are blocked (they save the minimum amount of information required to restart the interrupt handler after all critical sections have exited). So the interrupt latency for a blocked interrupt is extended to the end of the critical section, plus any interrupts with equal and higher priority that arrived while the block was in place.

Many computer systems require low interrupt latencies, especially embedded systems that need to control machinery in real-time. Sometimes these systems use a real-time operating system (RTOS). An RTOS makes the promise that no more than a specified maximum amount of time will pass between executions of subroutines. In order to do this, the RTOS must also guarantee that interrupt latency will never exceed a predefined maximum.

There are many methods that hardware may use to increase the interrupt latency that can be tolerated. These include buffers, and flow control. For example, most network cards implement transmit and receive ring buffers, interrupt rate limiting, and hardware flow control. Buffers allow data to be stored until it can be transferred, and flow control allows the network card to pause communications without having to discard data if the buffer is full.

Modern hardware also implements interrupt rate limiting. This helps prevent interrupt storms or "live lock" by having the hardware wait a programmable minimum amount of time between each interrupt it generates. Interrupt rate limiting reduces the amount of time spent servicing interrupts, allowing the processor to spend more time doing useful work. Exceeding this time results in a soft (recoverable) or hard (non-recoverable) error.



</doc>
<doc id="15468" url="https://en.wikipedia.org/wiki?curid=15468" title="İskender kebap">
İskender kebap

İskender kebap is one of the most famous meat foods of northwestern Turkey and takes its name from its inventor, İskender Efendi, who lived in Bursa in the late 19th century.

The dish consists of döner kebab prepared from thinly cut grilled lamb topped with hot tomato sauce over pieces of pita bread and generously slathered with melted sheep butter and yogurt. Additionally, one cylindrical köfte can be placed on top. It is commonly consumed with şıra as a drink to aid digestion. Tomato sauce and melted butter are generally poured over the dish, at the table.

"Kebapçı İskender" is trademarked by the İskenderoğlu family, who still run the restaurant in Bursa. Even all restaurants names are same their owners are different but they are relatives. This dish is available in many restaurants throughout the country mostly under the name "İskender kebap", "Bursa kebabı", or at times with an alternative one made up by the serving restaurant such as "Uludağ kebabı".




</doc>
<doc id="15471" url="https://en.wikipedia.org/wiki?curid=15471" title="LGBT in Islam">
LGBT in Islam

LGBT in Islam is influenced by the religious, legal, social, and cultural history of the nations with a sizable Muslim population, along with specific passages in the Quran and hadith, statements attributed to the Islamic prophet Muhammad.

The Quran cites the story of the "people of Lot" destroyed by the wrath of God because they engaged in lustful carnal acts between men. Homosexual acts are forbidden in traditional Islamic jurisprudence and are liable to different punishments, including the death penalty, depending on the situation and legal school. However, homosexual relationships were generally tolerated in pre-modern Islamic societies, and historical record suggests that these laws were invoked infrequently, mainly in cases of rape or other "exceptionally blatant infringement on public morals". Homoerotic themes were cultivated in poetry and other literary genres written in major languages of the Muslim world from the eighth century into the modern era. The conceptions of homosexuality found in classical Islamic texts resemble the traditions of Graeco-Roman antiquity, rather than modern Western notions of sexual orientation. It was expected that many or most mature men would be sexually attracted to both women and male adolescents (variously defined), and men were expected to wish to play only an active role in homosexual intercourse once they reached adulthood.

In recent times, extreme prejudice persists, both socially and legally, in much of the Islamic world against people who engage in homosexual acts. In Afghanistan, Brunei, Iran, Mauritania, Nigeria, Saudi Arabia, Somalia (in some southern regions), Sudan, United Arab Emirates and Yemen, homosexual activity carries the death penalty.

In other countries, such as Algeria, Bangladesh, Chad, Malaysia, Maldives, Pakistan, Qatar, Somalia and Syria, it is illegal. Same-sex sexual intercourse is legal in Albania, Azerbaijan, Bahrain, Bosnia and Herzegovina, Burkina Faso, Djibouti, Guinea-Bissau, Iraq, Jordan, Kazakhstan, Kosovo, Kyrgyzstan, Mali, Niger, Tajikistan, Turkey, and most of Indonesia (except in Aceh and South Sumatra provinces, where bylaws against LGBT rights have been passed), as well as Northern Cyprus.

Homosexual relations between females are legal in Kuwait, Turkmenistan and Uzbekistan, but homosexual acts between males are illegal.

Most Muslim-majority countries and the Organisation of Islamic Cooperation (OIC) have opposed moves to advance LGBT rights at the United Nations, in the General Assembly or the UNHRC. In May 2016, a group of 51 Muslim states blocked 11 gay and transgender organizations from attending 2016 High Level Meeting on Ending AIDS. However, Albania, Guinea-Bissau and Sierra Leone have signed a UN Declaration supporting LGBT rights. Albania provides LGBT rights protections in the form of anti-discrimination laws, and discussions on legally recognizing same-sex marriage have been held in the country. Kosovo as well as the (internationally not recognized) Muslim-majority Turkish Republic of Northern Cyprus also have anti-discrimination laws in place. There are also several groups within Islam around the world who support LGBT rights and LGBT Muslims.

The Quran contains several allusions to homosexual activity, which has prompted considerable exegetical and legal commentary over the centuries. The subject is most clearly addressed in the story of Sodom and Gomorrah (seven verses) after the city inhabitants demand sexual access to the messengers sent by God to the prophet Lot (or Lut). The Quranic narrative largely conforms to that found in Genesis. In one passage the Quran says that the men "solicited his guests of him" (), using an expression that parallels phrasing used to describe the attempted seduction of Joseph, and in multiple passages they are accused of "coming with lust" to men instead of women (or their wives). The Quran terms this an abomination () unprecedented in the history of the world:

Later exegetical literature built on these verses as writers attempted to give their own views as to what went on; and there was general agreement among exegetes that the "abomination" alluded to by the Quranic passages was attempted sodomy (specifically anal intercourse) between men.

The sins of the people of Lut () subsequently became proverbial, and the Arabic words for the act of anal sex between men () and for a person who performs such acts () ironically both derive from his name (even though Lut wasn't the one demanding sex).

Only one passage in the Quran prescribes a strictly legal position. It is not restricted to homosexual behaviour, however, and deals more generally with "zina" (illicit sexual intercourse):
Most exegetes hold that these verses refer to illicit heterosexual relationships, although a minority view attributed to the Mu'tazilite scholar Abu Muslim al-Isfahani interpreted them as referring to homosexual relations. This view was widely rejected by medieval scholars, but has found some acceptance in modern times.

Some Quranic verses describing the paradise refer to "immortal boys" (56:17, 76:19) or "young men" (52:24) who serve wine to the blessed. Although the "tafsir" literature does not interpret this as a homoerotic allusion, the connection was made in other literary genres, mostly humorously. For example, the Abbasid-era poet Abu Nuwas wrote:

Jurists of the Hanafi school took up the question seriously, considering, but ultimately rejecting the suggestion that homosexual pleasures were, like wine, forbidden in this world but enjoyed in the afterlife.

The hadith (sayings and actions attributed to Muhammad) show that homosexual behaviour was not unknown in seventh-century Arabia. However, given that the Quran did not specify the punishment of homosexual sodomy, Islamic jurists increasingly turned to several "more explicit but poorly attested" hadiths in an attempt to find guidance on appropriate punishment.

While there are no reports relating to homosexuality in the best known hadith collections of Bukhari and Muslim, other canonical collections record a number of condemnations of the "act of the people of Lot" (male-to-male anal intercourse). For example, Abu `Isa Muhammad ibn `Isa at-Tirmidhi (compiling the Sunan al-Tirmidhi around C.E.884) wrote that Muhammad had indeed prescribed the death penalty for both the active and also the passive partner:

Ibn al-Jawzi (1114–1200) writing in the 12th century claimed that Muhammad had cursed "sodomites" in several hadith, and had recommended the death penalty for both the active and passive partners in homosexual acts.

Al-Nuwayri (1272–1332) in his "Nihaya" reports that Muhammad is "alleged to have said what he feared most for his community were the practices of the people of Lot (although he seems to have expressed the same idea in regard to wine and female seduction)."

Other hadiths seem to permit homoerotic feelings as long as they are not translated into action. One hadith acknowledges homoerotic temptation and warns against it: "Do not gaze at the beardless youths, for verily they have eyes more tempting than the "houris"" or "... for verily they resemble the "houris"". These beardless youths are also described as wearing sumptuous robes and having perfumed hair.

In addition, there is a number of "purported (but mutually inconsistent) reports" ("athar") of punishments of sodomy ordered by early caliphs. Abu Bakr apparently recommended toppling a wall on the culprit, or else burning him alive, while Ali bin Abi Talib is said to have ordered death by stoning for one sodomite and had another thrown head-first from the top of a minaret—according to Ibn Abbas, the latter punishment must be followed by stoning.

There are, however, fewer hadith mentioning homosexual behavior in women;
but punishment (if any) for lesbianism was not clarified.

The hadith collection of Bukhari (compiled in the 9th century from earlier oral traditions) includes a report regarding "mukhannathun", effeminate men who were granted access to secluded women's quarters and engaged in other non-normative gender behavior:

In hadiths attributed to Muhammad's wives, a "mukhannath" in question expressed his appreciation of a woman's body and described it for the benefit of another man. According to Everett Rowson, none of the sources state that Muhammad banished more than two "mukhannathun", and it is not clear to what extent the action was taken because of their breaking of gender rules in itself or because of the "perceived damage to social institutions from their activities as matchmakers and their corresponding access to women".

According to traditional Islamic law, homosexual activity cannot occur in a legal manner because it takes place outside of marriage and between partners of the same sex.

The paucity of concrete prescriptions to be derived from hadith and the contradictory nature of information about the actions of early authorities resulted in lack of agreement among classical jurists as to how homosexual activity should be treated. Most legal schools treat homosexual intercourse with penetration similarly to unlawful heterosexual intercourse under the rubric of "zina", but there are differences of opinion with respect to methods of punishment. Some legal schools "prescribed capital punishment for sodomy, but others opted only for a relatively mild discretionary punishment." The Hanbalites are the most severe among Sunni schools, insisting on capital punishment for anal sex in all cases, while the other schools generally restrict punishment to flagellation with or without banishment, unless the culprit is "muhsan" (Muslim free married adult), and Hanafis often suggest no physical punishment at all, leaving the choice to the judge's discretion. The founder of the Hanafi school Abu Hanifa refused to recognize the analogy between sodomy and "zina", although his two principal students disagreed with him on this point. The Hanafi scholar Abu Bakr Al-Jassas (d. 981 AD/370 AH) argued that the two hadiths on killing homosexuals "are not reliable by any means and no legal punishment can be prescribed based on them". Where capital punishment is prescribed and a particular method is recommended, the methods range from stoning (Hanbali, Maliki), to the sword (some Hanbalites and Shafi'ites), or leaving it to the court to choose between several methods, including throwing the culprit off a high building (Shi'ite).

For unclear reasons, the treatment of homosexuality in Twelver Shia jurisprudence is generally harsher than in Sunni fiqh, while Zaydi and Isma'ili Shia jurists took positions similar to the Sunnis. Where flogging is prescribed, there is a tendency for indulgence and some recommend that the prescribed penalty should not be applied in full, with Ibn Hazm reducing the number of strokes to 10. There was debate as to whether the active and passive partners in anal sex should be punished equally. Beyond penetrative anal sex, there was "general agreement" that "other homosexual acts (including any between females) were lesser offenses, subject only to discretionary punishment." Some jurists viewed sexual intercourse as possible only for an individual who possesses a phallus; hence those definitions of sexual intercourse that rely on the entry of as little of the corona of the phallus into a partner's orifice. Since women do not possess a phallus and cannot have intercourse with one another, they are, in this interpretation, physically incapable of committing zinā.

Since a "hadd" punishment for "zina" requires testimony from four witnesses to the actual act of penetration or a confession from the accused repeated four times, the legal criteria for the prescribed harsh punishments of homosexual acts were very difficult to fulfill. The debates of classical jurists are "to a large extent theoretical, since homosexual relations have always been tolerated" in pre-modern Islamic societies. While it is difficult to ascertain to what extent the legal sanctions were enforced in different times and places, historical record suggests that the laws were invoked mainly in cases of rape or other "exceptionally blatant infringement on public morals". Documented instances of prosecution for homosexual acts are rare, and those which followed legal procedure prescribed by Islamic law are even rarer.

In her 2016 book, Kecia Ali observes that "contemporary scholars disagree sharply about the Qur'anic perspective on same-sex intimacy." One scholar represents the conventional perspective by arguing that the Qur'an "is very explicit in its condemnation of homosexuality leaving scarcely any loophole for a theological accommodation of homosexuality in Islam." Another scholar argues that "the Qur'an does not address homosexuality or homosexuals explicitly." Overall, Ali says that "there is no one Muslim perspective on anything."

Many Muslim scholars have followed a "don't ask, don't tell" policy in regards to homosexuality in Islam, by treating the subject with passivity.

Mohamed El-Moctar El-Shinqiti, director of the Islamic Center of South Plains in Texas, has argued that "[even though] homosexuality is a grievous sin...[a] no legal punishment is stated in the Qur'an for homosexuality...[b] it is not reported that Prophet Muhammad has punished somebody for committing homosexuality...[c] there is no authentic hadith reported from the Prophet prescribing a punishment for the homosexuals..." Classical hadith scholars such as Al-Bukhari, Yahya ibn Ma'in, Al-Nasa'i, Ibn Hazm, Al-Tirmidhi, and others have impugned the authenticity of hadith reporting these statements.

Faisal Kutty, a professor of Islamic law at Indiana-based Valparaiso University Law School and Toronto-based Osgoode Hall Law School, commented on the contemporary same-sex marriage debate in a March 27, 2014, essay in the Huffington Post. He acknowledged that while Islamic law iterations prohibits pre- and extra-marital as well as same-sex sexual activity, it does not attempt to "regulate feelings, emotions and urges, but only its translation into action that authorities had declared unlawful". Kutty, who teaches comparative law and legal reasoning, also wrote that many Islamic scholars have "even argued that homosexual tendencies themselves were not haram [prohibited] but had to be suppressed for the public good". He claimed that this may not be "what the LGBTQ community wants to hear", but that, "it reveals that even classical Islamic jurists struggled with this issue and had a more sophisticated attitude than many contemporary Muslims". Kutty, who in the past wrote in support of allowing Islamic principles in dispute resolution, also noted that "most Muslims have no problem extending full human rights to those—even Muslims—who live together 'in sin'". He argued that it therefore seems hypocritical to deny fundamental rights to same-sex couples. Moreover, he concurred with Islamic legal scholar Mohamed Fadel in arguing that this is not about changing Islamic marriage (nikah), but about making "sure that all citizens have access to the same kinds of public benefits".

Islamist journalist Muhammad Jalal Kishk found no prescribed punishment for homosexuality in Islamic law Several modern day scholars, including Scott Kugle, argue for a different interpretation of the Lot narrative focusing not on the sexual act but on the infidelity of the tribe and their rejection of Lot's Prophethood.

In a 2003 book Scott Siraj al-Haqq Kugle asserts "that
Islam does not address homosexuality." Therefore, he adds that we should be "suspicious of statements like 'Islam says . . .' or 'The Shari'ah says . . .' as if these abstractions actually speak." Whatever is said about these sources "are interpretations of them" and interpretations are "always by fallible people." Fugle reads the Qur'an as holding "a positive assessment of diversity." With this reading, Islam can be described as "a religion that positively assesses diversity in creation and in human societies."195 In keeping with this positive assessment of diversity, "gay and lesbian Muslims" view homosexuality as representing the "natural diversity in sexuality in human societies."

In Scott Siraj al-Haqq Kugle's 2010 book on homosexuality in Islam, he addresses the teaching of sacred texts including the Qur'an about homosexuality. Kugle notes the Islamic "tolerance for diversity of interpretation of sacred texts."

Kugle quotes the Qur'an: "O people, we created you all from a male and a female And made you into different communities and tribes So that you would come to know one another Acknowledging that the most noble among you Is the one most aware of God." Qur'an 49:13 (Kugle's translation). Then Kugle continues, "the implication of this verse is that no Muslim is better than another," even "a gay or lesbian Muslim."

Regarding interpreting the Qur'an, Kugle notes that "it is always human beings who speak for the Qur'an" and "they always interpret its words" and "interpretation is always ambiguous and contested." Such ambiguity allows "gay, lesbian, and transgender Muslims" to interpret the Qur'an in "sexuality-sensitive" ways, ways they believe produce a "fuller and better interpretation."

Regarding the Qur'an's treatment of same-sex acts, Kugle says that "where the Qur'an treats same-sex acts, it condemns them only so far as they are exploitive or violent." More generally, Kugle notes that the Qur'an refers to four different levels of personality. One level is "genetic inheritance." The Qur'an refers to this level as one's "physical stamp" that "determines one's temperamental nature" including one's sexuality. One the basis of this reading of the Qur'an, Kugle asserts that homosexuality is "caused by divine will," so "homosexuals have no rational choice in their internal disposition to be attracted to same-sex mates."

Regarding the story of Lot, Kugle observes that if the "classical interpreters" had seen "sexual orientation as an integral aspect of human personality," they would have read the narrative of Lot and his tribe "as addressing male rape of men in particular" and not as "addressing homosexuality in general."

A critique of Kugle's approach, interpretations and conclusions was published in 2016 by Mobeen Vaid.

In a 2012 book, Aisha Geissinger writes that there are "apparently irreconcilable Muslim standpoints on same-sex desires and acts," all of which claim "interpretative authenticity." One of these standpoints results from "queer-friendly" interpretations of the Lot story and the Quran. The Lot story is interpreted as condemning "rape and inhospitality rather than today's consensual same-sex relationships."

Abdessamad Dialmy in his 2010 article, "Sexuality and Islam," addressed "sexual norms defined by the sacred texts (Koran and Sunna)." He wrote that "sexual standards in Islam are paradoxical." The sacred texts "allow and actually are an enticement to the exercise of sexuality." However, they also "discriminate . . . between heterosexuality and homosexuality." Islam's paradoxical standards result in "the current back and forth swing of sexual practices between repression and openness." Dialmy sees a solution to this back and forth swing by a "reinterpretation of repressive holy texts."

Societies in Islam have recognized "both erotic attraction and sexual behavior between members of the same sex." However, their attitudes about them have often been contradictory: "severe religious and legal sanctions" against homosexual behavior and at the same time "celebratory expressions" of erotic attraction. Homoeroticism was idealized in the form of poetry or artistic declarations of love from one man to another. Accordingly, the Arabic language had an appreciable vocabulary of homoerotic terms, with a dozens of word just to describe types of male prostitutes. Schmitt (1992) identifies some twenty words in Arabic, Persian and Turkish to identify those who are penetrated. Other related Arabic words includes "Mukhannathun", "ma'bûn", "halaqī", "baghghā".

There is little evidence of homosexual practice in Islamic societies for the first century and a half of the Islamic era. Homoerotic poetry appears suddenly at the end of the 8th century CE, particularly in Baghdad in the work of Abu Nuwas (756-814), who became a master of all the contemporary genres of Arabic poetry. The famous author Jahiz tried to explain the abrupt change in attitudes toward homosexuality after the Abbasid Revolution by the arrival of the Abbasid army from Khurasan, who are said to have consoled themselves with male pages when they were forbidden to take their wives with them. The increased prosperity following the early conquests was accompanied by a "corruption of morals" in the two holy cities of Mecca and Medina, and it can be inferred that homosexual practice became more widespread during this time as a result of acculturation to foreign customs, such as the music and dance practiced by "mukhannathun", who were mostly foreign in origin. The Abbasid ruler Al-Amin (809-813) was said to have required slave women to be dressed in masculine clothing so he could be persuaded to have sex with them, and a broader fashion for "ghulamiyyat" (boy-like girls) is reflected in literature of the period.

The conceptions of homosexuality found in classical Islamic texts resemble the traditions of classical Greece and those of ancient Rome, rather than modern Western notions of sexual orientation. It was expected that many or most mature men would be sexually attracted to both women and adolescent boys (with different views about the appropriate age range for the latter), and men were expected to wish to play only an active role in homosexual intercourse once they reached adulthood. Preference for homosexual over heterosexual relations was regarded as a matter of personal taste rather than a marker of homosexual identity in a modern sense. While playing an active role in homosexual relations carried no social stigma beyond that of licentious behavior, seeking to play a passive role was considered both unnatural and shameful for a mature man. Following Greek precedents, the Islamic medical tradition regarded as pathological only this latter case, and showed no concern for other forms of homosexual behavior.

During the early period, growth of a beard was considered to be the conventional age when an adolescent lost his homoerotic appeal, as evidenced by poetic protestations that the author still found his lover beautiful despite the growing beard. During later periods, the age of the stereotypical beloved became more ambiguous, and this prototype was often represented in Persian poetry by Turkish soldiers. This trend is illustrated by the story of Mahmud of Ghazni (971–1030), the ruler of the Ghaznavid Empire, and his cupbearer Malik Ayaz. Their relationship, which was sketchily attested in contemporary sources, became a staple of Persian literature comparable to the story of Layla and Majnun. Poets used it to illustrate the power of love, pointing to Mahmud as an example of a man who becomes "a slave to his slave", while Malik Ayaz served as "the embodiment of the ideal beloved, and a model for purity in Sufi literature".

Other famous examples of homosexuality include the Aghlabid Emir Ibrahim II of Ifriqiya (ruled 875–902), who was said to have been surrounded by some sixty catamites, yet whom he was said to have treated in a most horrific manner. Caliph al-Mutasim in the 9th century and some of his successors were accused of homosexuality. The popular stories say that Cordoba, Abd al-Rahman III had executed a young man from León who was held as a hostage, because he had refused his advances during the Reconquista.

Mehmed the Conqueror, the Ottoman sultan living in the 15th century, European sources say "who was known to have ambivalent sexual tastes, sent a eunuch to the house of Notaras, demanding that he supply his good-looking fourteen-year-old son for the Sultan's pleasure. When he refused, the Sultan instantly ordered the decapitation of Notaras, together with that of his son and his son-in-law; and their three heads … were placed on the banqueting table before him". Another youth Mehmed found attractive, and who was presumably more accommodating, was Radu III the Fair, the brother of the famous Vlad the Impaler, "Radu, a hostage in Istanbul whose good looks had caught the Sultan's fancy, and who was thus singled out to serve as one of his most favored pages." After the defeat of Vlad, Mehmed placed Radu on the throne of Wallachia as a vassal ruler. However, Turkish sources deny these stories.

According to the "Encyclopedia of Islam and the Muslim World":

Whatever the legal strictures on sexual activity, the positive expression of male homeoerotic sentiment in literature was accepted, and assiduously cultivated, from the late eighth century until modern times. First in Arabic, but later also in Persian, Turkish and Urdu, love poetry by men about boys more than competed with that about women, it overwhelmed it. Anecdotal literature reinforces this impression of general societal acceptance of the public celebration of male-male love (which hostile Western caricatures of Islamic societies in medieval and early modern times simply exaggerate).

European travellers remarked on the taste that Shah Abbas of Iran (1588-1629) had for wine and festivities, but also for attractive pages and cup-bearers. A painting by Riza Abbasi with homo-erotic qualities shows the ruler enjoying such delights.

"Homosexuality was a key symbolic issue throughout the Middle Ages in [Islamic] Iberia. As was customary everywhere until the nineteenth century, homosexuality was not viewed as a congenital disposition or 'identity'; the focus was on nonprocreative sexual practices, of which sodomy was the most controversial." For example, in "al-Andalus homosexual pleasures were much indulged by the intellectual and political elite. Evidence includes the behavior of rulers . . . who kept male harems." Although early islamic writings such as the Quran expressed a mildly negative attitude towards homosexuality, laypersons usually apprehended the idea with indifference, if not admiration. Few literary works displayed hostility towards non-heterosexuality, apart from partisan statements and debates about types of love (which also occurred in heterosexual contexts). Khaled el-Rouayheb (2014) maintain that "much if not most of the extant love poetry of the period [16th to 18th century] is pederastic in tone, portraying an adult male poet's passionate love for a teenage boy".

El-Rouayheb suggest that even though religious scholars considered sodomy as an abhorrent sin, most of them did not genuinely believe that it was illicit to merely fall in love with a boy or expressing this love via poetry. In the secular society however, a male's desire to penetrate a desirable youth was seen as understandable, even if not lawful. On the other hand, men adopting the passive role were more subjected to stigma. The medical term "ubnah" qualified the pathological desire of a male to exclusively be on the receiving end of anal intercourse. Physician that theorized on "ubnah" includes Rhazes, who thought that it was correlated with small genitals and that a treatment was possible provided that the subject was deemed to be not too effeminate and the behavior not "prolonged". Dawud al-Antaki advanced that it could have been caused by an acidic substance embedded in the veins of the anus, causing itchiness and thus the need to seek relief.

The Ottoman Caliphate "ruled the Sunni Muslim world for centuries." It was "much more open-minded regarding the homosexual issue" than is the current Turkish government although it "claims to emulate" the Ottoman Caliphate. "The Ottoman Empire had an extensive literature of homosexual romance, and an accepted social category of transvestites." It could be argued that the Ottoman sultans "were social liberals compared with the contemporary Islamists of Turkey, let alone the Arab World."

During the Ottoman Empire, homosexuality was decriminalized in 1858, as part of wider reforms during the Tanzimat.

Before the modern era, Islamic nations were not so opposed to same-sex relations. For example, a ruler in Persia in the 11th century advised his son "to alternate his partners seasonally: young men in the summer and women in the winter." Many eighth-century love poems by Abu Nuwas in Baghdad and by other Persian and Urdu poets seem to have been "addressed to boys." In mystic writings of the medieval era, such as Sufi texts, it is "unclear whether the beloved being addressed is a teenage boy or God." European chroniclers censured "the indulgent attitudes to gay sex in the Caliphs' courts."

The modern rejection and criminalization of "homosexuality in Islam gained momentum through the exogenous effects of European colonialism. . . . " European thought at the time treated homosexuality as "against nature."

Scott Siraj al-Haqq Kugle has argued that, while "Muslims commemorate the early days of Islam when they were oppressed as a marginalized few," many of them now forget their history and fail to protect "Muslims who are gay, transgender and lesbian."

While friendship between men and boys is often described in sexual ways in classical Islamic literature, Khaled El-Rouayheb and Oliver Leaman have argued that it would be misleading to conclude from this that homosexuality was widespread in practice. Such literature tended to use transgressive motifs alluding to what is forbidden, in particular homosexuality and wine. Greek homoerotic motifs may have accurately described practices in ancient Greece, but in their Islamic adaptations they tended to play a satirical or metaphorical rather than descriptive role. At the same time, many miniatures, especially from Ottoman Turkey, contain explicit depictions of pederasty, suggesting that the practice enjoyed a certain degree of popularity. A number of pre-modern texts discuss the possibility of sexual exploitation faced by young boys in educational institutions and warn teachers to take precautions against it.

In modern times, despite the formal disapproval of religious authority, the segregation of women in Muslim societies and the strong emphasis on male virility leads adolescent males and unmarried young men to seek sexual outlets with boys younger than themselves—in one study in Morocco, with boys in the age-range 7 to 13. Men have sex with other males so long as they are the penetrators and their partners are boys, or in some cases effeminate men.

"Liwat" can therefore be regarded as "temptation", and anal intercourse is not seen as repulsively unnatural so much as dangerously attractive. They believe "one has to avoid getting buggered precisely in order not to acquire a taste for it and thus become addicted." Not all sodomy is homosexual: one Moroccan sociologist, in a study of sex education in his native country, notes that for many young men heterosexual sodomy is considered better than vaginal penetration, and female prostitutes likewise report the demand for anal penetration from their (male) clients.

It is not so much the penetration as the enjoyment that is considered bad. Deep shame attaches to the passive partner: "for this reason men stop getting laid at the age of 15 or 16 and 'forget' that they ever allowed it earlier." Similar sexual sociologies are reported for other Muslim societies from North Africa to Pakistan and the Far East. In Afghanistan in 2009, the British Army was forced to commission a report into the sexuality of the local men after British soldiers reported the discomfort at witnessing adult males involved in sexual relations with boys. The report stated that though illegal, there was a tradition of such relationships in the country, known as "bache bazi" or "boy play", and that it was especially strong around North Afghanistan.

According to the International Lesbian and Gay Association (ILGA) seven countries still retain capital punishment for homosexual behavior: Saudi Arabia, Yemen, Iran, Afghanistan, Mauritania, Sudan, and northern Nigeria. In United Arab Emirates it is a capital offense. In Qatar, Algeria, Uzbekistan, and the Maldives, homosexuality is punished with time in prison or a fine. This has led to controversy regarding Qatar, which is due to stage the 2022 FIFA World Cup. Human rights groups have questioned the awarding in 2010 of the right to host the competition, due to the possibility that gay football fans may be jailed. In response, Sepp Blatter, head of FIFA, joked that they would have to "refrain from sexual activity" while in Qatar. He later withdrew the remarks after condemnation from rights groups.

Same-sex sexual activity is illegal in Chad since August 1, 2017 under a new penal code. Homosexuality between consenting adults had never previously been criminalized prior to this law.

In Muslim-majority countries, open gay life rarely exists, but "the closet is spacious." Even countries with strict laws against homosexual people "have flourishing gay scenes at all levels of society."

In Bahrain, police in Bahrain arrested scores of men in February 2011 at a "gay party".

In Egypt, openly gay men have been prosecuted under general public morality laws. (See Cairo 52.) "Sexual relations between consenting adult persons of the same sex in private are not prohibited as such. However, the Law on the Combating of Prostitution, and the law against debauchery have been used to imprison gay men in recent years."

Islamic state has decreed capital punishment for gays. They have executed more than two dozen men and women for suspected homosexual activity, including several thrown off the top of buildings in highly publicized executions.

In India, which has the third-largest Muslim population in the world, and where Muslims form a large minority, the largest Islamic seminary (Darul Uloom Deoband) has vehemently opposed recent government moves to abrogate and liberalize laws from the British Raj era that banned homosexuality.

In Iraq, homosexuality is allowed by the government, but terrorist groups often carry out illegal executions of gay people. Saddam Hussein was "unbothered by sexual mores." Ali Hili reports that "since the 2003 invasion more than 700 people have been killed because of their sexuality." He calls Iraq the "most dangerous place in the world for sexual minorities."

In Jordan, where homosexuality is legal, "gay hangouts have been raided or closed on bogus charges, such as serving alcohol illegally."

In Pakistan, its law is a mixture of both Anglo-Saxon colonial law as well as Islamic law, both which proscribe criminal penalties for same-sex sexual acts. The Pakistan Penal Code of 1860, originally developed under colonialism, punishes sodomy with a possible prison sentence and has other provisions that impact the human rights of LGBT Pakistanis, under the guise of protecting public morality and order. Yet, the more likely situation for gay and bisexual men is sporadic police blackmail, harassment, fines, and jail sentences.

In Saudi Arabia, the maximum punishment for homosexual acts is public execution by beheading, which is often carried out. The government will sometimes use lesser punishments—for example, fines, time in prison, and whipping—as alternatives.

In Turkey, homosexuality is legal, but "official censure can be fierce". A former interior minister, İdris Naim Şahin, called homosexuality an example of "dishonour, immorality and inhuman situations".

In 2016, the International Lesbian, Gay, Bisexual, Trans and Intersex Association (ILGA) released its most recent "State Sponsored Homophobia Report". The report found that thirteen countries (or parts of them) impose the death penalty for "Same-sex sexual acts". These countries comprise 6% of the countries in the United Nations. Of these thirteen countries, four are in Africa: Sudan, Nigeria, Somalia, and Mauritania. Nine are in Asia: Iran, Saudi Arabia, Yemen, Afghanistan, Pakistan, Qatar, UAE, Iraq, and Daesh (ISIS/ISIL) territories. None are in the Americas, Europe, or Oceania. The full report with details about countries imposing the death penalty can be read at "State Sponsored Homophobia 2016". This report omits that Brunei also punishes homosexuals with death and that Brunei tolerates anti-gay vigilante attacks.

The Ottoman Empire (predecessor of Turkey) decriminalized homosexuality in 1858. In Turkey, where 99.8% of the population is Muslim, homosexuality has never been criminalized since the day it was founded in 1923. And LGBT people also have the right to seek asylum in Turkey under the Geneva Convention since 1951.

Same-sex sexual intercourse is legal in Albania, Azerbaijan, Bahrain, Bosnia and Herzegovina, Burkina Faso, Djibouti, Guinea-Bissau, Lebanon, Iraq (except those parts controlled by the Islamic State), Kazakhstan, Kosovo, Kyrgyzstan, Mali, Niger, Tajikistan, Turkey, West Bank (State of Palestine), most of Indonesia, and in Northern Cyprus. In Albania, Lebanon, and Turkey, there have been discussions about legalizing same-sex marriage. Albania, Northern Cyprus and Kosovo also protect LGBT people with anti-discrimination laws.

In 2016, the International Lesbian, Gay, Bisexual, Trans and Intersex Association (ILGA) released its most recent State Sponsored Homophobia Report. The report found that "same-sex sexual acts" are legal in 121 countries. These countries comprise 63% of the countries in the United Nations. Of these 121 countries, twenty-one are in Africa, nineteen are in Asia, twenty-four are in the Americas, forty-eight are in Europe, and seven are in Oceania. The full report with the names of countries in which same-sex acts are legal or illegal can be read at "State Sponsored Homophobia 2016".

In 2007 there was a gay party in the Moroccan town of al-Qasr al-Kabir. Rumours spread that this was a gay marriage and more than 600 people took to the streets, condemning the alleged event and protesting against leniency towards homosexuals. Several persons who attended the party were detained and eventually six Moroccan men were sentenced to between four and ten months in prison for "homosexuality".

In France there was an Islamic same-sex marriage on February 18, 2012. In Paris in November 2012 a room in a Buddhist prayer hall was used by gay Muslims and called a "gay-friendly mosque", and a French Islamic website is supporting religious same-sex marriage.

The first American Muslim in the United States Congress, Keith Ellison (D-MN) said in 2010 that all discrimination against LGBT people is wrong. He further expressed support for gay marriage stating:

I believe that the right to marry someone who you please is so fundamental it should not be subject to popular approval any more than we should vote on whether blacks should be allowed to sit in the front of the bus.

In 2014 eight men were jailed for three years by a Cairo court after the circulation of a video of them allegedly taking part in a private wedding ceremony between two men on a boat on the Nile.

Several violent attacks by Islamic terrorists against LGBT people in the West have taken place

Since February 2017, over 100 male residents of the Chechen Republic (part of the Russian Federation) assumed to be gay or bisexual have been rounded up, detained and tortured by authorities on account of their sexual orientation. These crackdowns have been described as part of a systemic anti-LGBT "purge" in the region. The men are held and allegedly tortured in concentration camps.

Allegations were initially reported in "Novaya Gazeta" on April 1, 2017 a Russian-language opposition newspaper, which reported that over 100 men have allegedly been detained and tortured and at least three people have died in an extrajudicial killing. The paper, citing its sources in the Chechen special services, called the wave of detentions a "prophylactic sweep." The journalist who first reported on the subject has gone into hiding, There have been calls for reprisals for journalists reporting on the situation.

In response, the Russian LGBT Network is attempting to assist those who are threatened to evacuate from Chechyna. Human rights groups and foreign governments have called upon Russia and Chechyna to put an end to the internments.

In 2011, the UN Human Rights Council passed its first resolution recognizing LGBT rights, which was followed up with a report from the UN Human Rights Commission documenting violations of the rights of LGBT people. The two world maps of the percentage of Muslims per country and the countries that support LGBT rights at the UN give an impression of the attitude towards homosexuality on the part of many Muslim-majority governments.
The Muslim community as a whole, worldwide, has become polarized on the subject of homosexuality. Some Muslims say that "no good Muslim can be gay," and "traditional schools of Islamic law consider homosexuality a grave sin." At the opposite pole, "some Muslims . . . are welcoming what they see as an opening within their communities to address anti-gay attitudes." Especially, it is "young Muslims" who are "increasingly speaking out in support of gay rights".

In 2013, the Pew Research Center conducted a study on the global acceptance of homosexuality and found a widespread rejection of homosexuality in many nations that are predominantly Muslim. In some countries, views were becoming more conservative among younger people.

A 2007 survey of British Muslims showed that 61% believe homosexuality should be illegal, with up to 71% of young British Muslims holding this belief. A later Gallup poll in 2009 showed that none of the 500 British Muslims polled believed homosexuality to be "morally acceptable". This compared with 35% of the 1001 French Muslims polled that did.

According to a 2012 poll, 51% of the Turks in Germany, who account for nearly two thirds of the total Muslim population in Germany, believe that homosexuality is an illness.

In a 2016 ICM poll of 1,081 British Muslims, 52% of those polled disagreed with the statement 'Homosexuality should be legal in Britain' compared with 11% of the control group; 18% agreed compared with 73% of the control group. In the same poll, 56% of British Muslims polled disagreed with the statement 'Gay marriage should be legal in Britain' compared with 20% of the control group and 47% disagreed with the statement 'It is acceptable for a homosexual person to be a teacher in a school' compared with 14% of the control group.

American Muslims - in line with general public attitudes in the United States - have become much more accepting of homosexuality over recent years. In a 2007 poll conducted by Pew Research Center, only 27% of American Muslims believed that homosexuality should be accepted. In a 2011 poll, that rose to 39%. In a July 2017 poll, Muslims who say homosexuality should be accepted by society clearly outnumber those who say it should be discouraged (52% versus 33%).



The Al-Fatiha Foundation was an organization which tried to advance the cause of gay, lesbian, and transgender Muslims. It was founded in 1998 by Faisal Alam, a Pakistani American, and was registered as a nonprofit organization in the United States. The organization was an offshoot of an internet listserve that brought together many gay, lesbian and questioning Muslims from various countries. The Foundation accepted and considered homosexuality as natural, either regarding Qur'anic verses as obsolete in the context of modern society, or stating that the Qu'ran speaks out against homosexual lust and is silent on homosexual love. After the Alam stepped down, subsequent leaders failed to sustain the organization and it began a process of legal dissolution in 2011.

In 2001, Al-Muhajiroun, a banned and now defunct international organization who sought the establishment of a global Islamic caliphate, issued a fatwa declaring that all members of Al-Fatiha were "murtadd", or apostates, and condemning them to death. Because of the threat and coming from conservative societies, many members of the foundation's site still prefer to be anonymous so as to protect their identity while continuing a tradition of secrecy. Al-Fatiha has fourteen chapters in the United States, as well as offices in England, Canada, Spain, Turkey, and South Africa. In addition, Imaan, a social support group for Muslim LGBT people and their families, exists in the UK. Both of these groups were founded by gay Pakistani activists.

The coming together of "human rights discourses and sexual orientation struggles" has resulted in an abundance of "social movements and organizations concerned with gender and sexual minority oppression and discrimination."

In November 2012, a prayer room was set up in Paris by gay Islamic scholar and founder of the group 'Homosexual Muslims of France' Ludovic-Mohamed Zahed. It was described by the press as the first gay-friendly mosque in Europe. The reaction from the rest of the Muslim community in France has been mixed. The opening has been condemned by the Grand Mosque of Paris.

There are also a number of Islamic ex-gay (i.e. people claiming to have experienced a basic change in sexual orientation from exclusive homosexuality to exclusive heterosexuality) groups aimed at attempting to guide homosexuals towards heterosexuality. A large body of research and global scientific consensus indicates that being gay, lesbian, or bisexual is compatible with normal mental health and social adjustment. Because of this, major mental health professional organizations discourage and caution individuals against attempting to change their sexual orientation to heterosexual, and warn that attempting to do so can be harmful. People who have gone through conversion therapy face 8.9 times the rates of suicide ideation, face depression at 5.9 times the rate of their peers and are three times more likely to use illegal drugs compared to those who did not go through the therapy.

Nur Wahrsame has been an advocate for LGBTI Muslims. He founded Marhaba, a support group for queer Muslims in Melbourne, Australia. In May 2016, Wahrsage revealed that he is homosexual in an interview on SBS2's The Feed, being the first openly gay Imam in Australia.

The Muslim Alliance for Sexual and Gender Diversity (MASGD) in the United States began on January 23, 2013. It supports, empowers and connects LGBTQ Muslims. It aims "to increase the acceptance of gender and sexual diversity within Muslim communities." On June 20, 2016, an interview with Mirna Haidar (a member of the MASGD's steering committee) was published in The Washington Post. She described the MASGD as supporting "LGBT Muslims who want or need to embrace both their sexual and religious identities." Haidar said that the support the MASGD provides is needed because a person who is "Muslim and queer " faces "two different systems of oppression": Islamophobia and homophobia.

Muslims for Progressive Values, based in the United States and in Malaysia, is "a faith-based, grassroots, human rights organization that embodies and advocates for the traditional Qur'anic values of social justice and equality for all, for the 21st Century." MPV has recorded "a lecture series that seeks to dismantle the religious justification for homophobia in Muslim communities." The lectures can be viewed at MPV Lecture Series.

The Safra Project for women is based in the UK. It supports and works on issues relating to prejudice LGBTQ Muslim women. It was founded in October 2001 by Muslim LBT women. The Safra Project's "ethos is one of inclusiveness and diversity."

Salaam is the first gay Muslim group in Canada and second in the world. Salaam was found in 1993 by El-Farouk Khaki, who organized the Salaam/Al-Fateha International Conference in 2003.

Sarajevski Otvoreni Centar (Sarajevo Open Centre), abbreviated SOC, is an independent feminist civil society organization and advocacy group which campaigns for lesbian, gay, bisexual, trans and intersex (LGBTI) people and women rights in Bosnia and Herzegovina. The organization also gives asylum and psychological support to victims of discrimination and violence.

The "Pink Report" is an annual report made by the organization on the state of the Human Rights of LGBTI People in the country and is supported by the Norwegian Embassy.

In May 2009, the Toronto Unity Mosque / el-Tawhid Juma Circle was founded by Laury Silvers, a University of Toronto religious studies scholar, alongside Muslim gay-rights activists El-Farouk Khaki and Troy Jackson. Unity Mosque/ETJC is a gender-equal, LGBT+ affirming, mosque.

The religious conflicts and inner turmoil with which Islamic homosexuals struggle have been addressed in various media.

The goals of Channel 4 include (1) stimulate public debate on contemporary issues, (2) reflect cultural
diversity of the UK, and (3) champion alternative points of view. One of Channel 4's productions is a documentary on Gay Muslims, broadcast in the UK in January 2006. It can be viewed on YouTube in six parts: Gay Muslims - UK - Part 1 of 6, Gay Muslims - UK - Part 2 of 6, Gay Muslims - UK - Part 3 of 6, Gay Muslims - UK - Part 4 of 6, Gay Muslims - UK - Part 5 of 6, Gay Muslims - UK - Part 6 of 6

The Unity Productions Foundation (UPF) works for "Peace through the Media" by producing films "to break down stereotypes and enhance understanding" of Muslims and Islam. UPF films have been seen by approximated 150 million people. UPF has "partnered with prominent Jewish, Muslim, Christian and interfaith groups to run dialogues nationwide." Videos of non-Muslims speaking up for Muslims as "fellow Americans" are online at Non-Muslims Speak Up.

The Muslim Debate Initiative (MDI) made up of Muslims "with experience in public speaking, apologetics, polemics, research and community work." One of its aims is "to support, encourage and promote debate that contrasts Islam against other intellectual and political discourses for the purpose of the pursuit of truth, intellectual scrutiny with respect, and the clarifying accurate understandings of other worldviews between people of different cultures, beliefs and political persuasions." One of its broadcasts was on BBC3's "Free Speech" program on March 25, 2014. The debate was between Maajid Nawaz and Abdullah al Andalusi on the question "Can you be Gay and Muslim?" It is on YouTube at "Gay and Muslim?".

In 2007, the documentary film "A Jihad for Love" was released. It was produced by Sandi Simcha DuBowski and directed by Parvez Sharma. As of 2016 the film has been shown in 49 nations to four million plus viewers.

See two parts of the film at "A Jihad for Love Part 1" and "A Jihad for Love Part 2". Also a video about "A Jihad for Love" is at "About a Jihad for Love".

In 2015, the documentary film "A Sinner in Mecca" was released. It was directed by Parvez Sharma. The film chronicles Sharma's Hajj pilgrimage to Mecca, Saudi Arabia as an openly gay Muslim. The film premiered at the 2015 Hot Docs Canadian International Documentary Festival to great critical acclaim. The film opened in theaters in the US on September 4, 2015 and is a New York Times Critics' Pick.

My.Kali is a Jordanian pan-Arab LGBT publication published in English in Amman, Jordan. It started publication online in 2008. It is named after openly gay model Khalid, making major headlines, as it is the 1st LGBT publication to ever exist in the MENA region. The magazine regularly features non-LGBT artists on their covers to promote acceptance among other communities and was the first publication to give many underground and regional artists their first covers like Yasmine Hamdan, both lead singer and violinist of band Mashrou' Leila, Hamed Sinno and Haig Papazian, Alaa Wardi, Zahed Sultan and many more.

This section contains material from books and articles supporting LGBT Muslims.

In Chapter Eight of the 2003 book, "Progressive Muslims: On Justice, Gender, and Pluralism", Scott Siraj al-Haqq Kugle asserts "that Islam does not address homosexuality." In Fugle's reading, the Qur'an holds "a positive assessment of diversity." It "respects diversity in physical appearance, constitution, stature, and color of human beings as a natural consequence of Divine wisdom in creation." Therefore, Islam can be described as "a religion that positively assesses diversity in creation and in human societies." Furthermore, in Kugle's reading, the Qur'an "implies that some people are different in their sexual desires than others." Thus, homosexuality can be seen as part of the "natural diversity in sexuality in human societies." This is the way "gay and lesbian Muslims" view their homosexuality.

In addition to the Qur'an, Kugle refers to the benediction of Imam Al-Ghazali (the 11th-century Muslim theologian) which says. "praise be to God, the marvels of whose creation are not subject to the arrows of accident." For Kugle, this benediction implies that "if sexuality is inherent in a person's personality, then sexual diversity is a part of creation, which is never accidental but is always marvelous." Kugle also refers to "a rich archive of same-sex sexual desires and expressions, written by or reported about respected members of society: literati, educated elites, and religious scholars." Given these writings, Kugle concludes that "one might consider Islamic societies (like classical Greece) to provide a vivid illustration of a 'homosexual-friendly' environment." This evoked from "medieval and early modern Christian Europeans" accusations that Muslim were "engaging openly in same-sex practices."

Kugle goes a step further in his argument and asserts that "if some Muslims find it necessary to deny that sexual diversity is part of the natural created world, then the burden of proof rests on their shoulders to illustrate their denial from the Qur'anic discourse itself."

In 2010, an anthology "Islam and Homosexuality" was published. In the Forward, Parvez Sharma sounded a pessimistic note about the future: "In my lifetime I do not see Islam drafting a uniform edict that homosexuality is permissible." Following is material from two chapters dealing with the present.

Rusmir Musić in a chapter "Queer Visions of Islam" said that "Queer Muslims struggle daily to reconcile their sexuality and their faith." Musić began to study in college "whether or not my love for somebody of the same gender disgusts God and whether it will propel me to hell. The answer, for me, is an unequivocal "no". Furthermore, Musić wrote, "my research and reflection helped me to imagine my sexuality as a gift from a loving, not hateful, God."

Marhuq Fatima Khan in a chapter "Queer, American, and Muslim: Cultivating Identities and Communities of Affirmation," says that "Queer Muslims employ a few narratives to enable them to reconcile their religious and sexual identities." They "fall into three broad categories: (1) God Is Merciful; (2) That Is Just Who I Am; and (3) It's Not Just Islam."

Kecia Ali in her 2016 book "Sexual Ethics and Islam" says that p xvi "there is no one Muslim perspective on anything." Regarding the Qur'an, Ali says that modern scholars disagree about what it says about "same-sex intimacy." Some scholars argue that "the Qur'an does not address homosexuality or homosexuals explicitly."

Regarding homosexuality, Ali, says that the belief that "exclusively homosexual desire is innate in some individuals" has been adopted "even among some relatively conservative Western Muslim thinkers."100 Homosexual Muslims believe their homosexuality to be innate and view "their sexual orientation as God-given and immutable."123 She observes that "queer and trans people are sometimes treated as defective or deviant," and she adds that it is "vital not to assume that variation implies imperfection or disability."

Regarding "medieval Muslim culture," Ali says that "male desire to penetrate desirable youth . . . was perfectly normal." Even if same-sex relations were not lawful, there was "an unwillingness to seek out and condemn instances of same-sex activity, but rather to let them pass by . . . unpunished."

In an article "Same-sex Sexual Activity and Lesbian and Bisexual Women" Ali elaborates on homosexuality as an aspect of medieval Muslim culture. She says that "same-sex sexual expression has been a more or less recognized aspect of Muslim societies for many centuries." There are many explicit discussions of "same-sex sexual activity" in medieval Arabic literature.

In Islam, the term mukhannathun is used to describe gender-variant people, usually male-to-female transgender. Neither this term nor the equivalent for "eunuch" occurs in the Quran, but the term does appear in the Hadith, the sayings of Muhammad, which have a secondary status to the central text. Moreover, within Islam, there is a tradition on the elaboration and refinement of extended religious doctrines through scholarship. This doctrine contains a passage by the scholar and hadith collector An-Nawawi:

While Iran has outlawed homosexuality, Iranian Shi'a thinkers such as Ayatollah Khomeini have allowed for transgender people to change their sex so that they can enter heterosexual relationships. This position has been confirmed by the Supreme Leader of Iran, Ayatollah Ali Khamenei, and is also supported by many other Iranian clerics.

Iran carries out more sex change operations than any other nation in the world except for Thailand. It is regarded as a cure for homosexuality, which is punishable by death under Iranian law. The government even provides up to half the cost for those needing financial assistance and a sex change is recognized on the birth certificate.

On the 26th of June 2016, clerics affiliated to the Pakistan-based organization Tanzeem Ittehad-i-Ummat have issued a fatwa on transgender people, trans woman with "visible signs of being a male" being allowed to marry women or a trans men with "visible signs of being a female" and vice versa, while Muslim ritual funerals also apply. Depriving transgender people of their inheritance, humiliating, insulting or teasing them were also declared haraam.







</doc>
<doc id="15474" url="https://en.wikipedia.org/wiki?curid=15474" title="Infanticide">
Infanticide

Infanticide (or infant homicide) is the intentional killing of infants.

Parental infanticide researchers have found that mothers are far more likely than fathers to be the perpetrator for neonaticide and slightly more likely to commit infanticide in general.

In many past societies, certain forms of infanticide were considered permissible. In India female infanticide is more common than the killing of male offspring, due to sex-selective infanticide. In China for example, the sex gap between males and females aged 0–19 years old was estimated to be 25 million in 2010 by the United Nations Population Fund.

In English law infanticide is established as a distinct offence by the Infanticide Acts. Defined as the killing of a child under 12 months of age by their mother, the effect of the Acts are to establish a partial defence to charges of murder.

The practice of infanticide has taken many forms over time. Child sacrifice to supernatural figures or forces, such as that believed to have been practiced in ancient Carthage, may be only the most notorious example in the ancient world. Anthropologist Laila Williamson notes that "Infanticide has been practiced on every continent and by people on every level of cultural complexity, from hunter gatherers to high civilizations, including our own ancestors. Rather than being an exception, then, it has been the rule."

A frequent method of infanticide in ancient Europe and Asia was simply to abandon the infant, leaving it to die by exposure (i.e. hypothermia, hunger, thirst, or animal attack).

In at least one island in Oceania, infanticide was carried out until the 20th century by suffocating the infant, while in pre-Columbian Mesoamerica and in the Inca Empire it was carried out by sacrifice (see below).

Many Neolithic groups routinely resorted to infanticide in order to control their numbers so that their lands could support them. Joseph Birdsell believed that infanticide rates in prehistoric times were between 15% and 50% of the total number of births, while Laila Williamson estimated a lower rate ranging from 15% to 20%. Both anthropologists believed that these high rates of infanticide persisted until the development of agriculture during the Neolithic Revolution. Comparative anthropologists have calculated that 50% of female newborn babies were killed by their parents during the Paleolithic era. From the infants hominid skulls (e.g. Taung child skull) that had been traumatized, has been proposed cannibalism by Raymond A. Dart. The children were not necessarily actively killed, but neglect and intentional malnourishment may also have occurred, as proposed by Vicente Lull as an explanation for an apparent surplus of men and the below average height of women in prehistoric Menorca.

Archaeologists have uncovered physical evidence of child sacrifice at several locations. Some of the best attested examples are the diverse rites which were part of the religious practices in Mesoamerica and the Inca Empire.

Three thousand bones of young children, with evidence of sacrificial rituals, have been found in Sardinia. Pelasgians offered a sacrifice of every tenth child during difficult times. Syrians sacrificed children to Jupiter and Juno. Many remains of children have been found in Gezer excavations with signs of sacrifice. Child skeletons with the marks of sacrifice have been found also in Egypt dating 950–720 BCE. In Carthage "[child] sacrifice in the ancient world reached its infamous zenith". Besides the Carthaginians, other Phoenicians, and the Canaanites, Moabites and Sepharvites offered their first-born as a sacrifice to their gods.

In Egyptian households, at all social levels, children of both sexes were valued and there is no evidence of infanticide. The religion of the Ancient Egyptians forbade infanticide and during the Greco-Roman period they rescued abandoned babies from manure heaps, a common method of infanticide by Greeks or Romans, and were allowed to either adopt them as foundlings or raise them as slaves, often giving them names such as "copro -" to memorialise their rescue. Strabo considered it a peculiarity of the Egyptians that every child must be reared. Diodorus indicates infanticide was a punishable offence. Egypt was heavily dependent on the annual flooding of the Nile to irrigate the land and in years of low inundation severe famine could occur with breakdowns in social order resulting, notably between 930–1070 AD and 1180–1350 AD. Instances of cannibalism are recorded during these periods but it is unknown if this happened during the pharaonic era of Ancient Egypt. Beatrix Midant-Reynes describes human sacrifice as having occurred at Abydos in the early dynastic period (c. 3150–2850 BCE), while Jan Assmann asserts there is no clear evidence of human sacrifice ever happening in Ancient Egypt.

According to Shelby Brown, Carthaginians, descendants of the Phoenicians, sacrificed infants to their gods. Charred bones of hundreds of infants have been found in Carthaginian archaeological sites. One such area harbored as many as 20,000 burial urns. Skeptics suggest that the bodies of children found in Carthaginian and Phoenician cemeteries were merely the cremated remains of children that died naturally.

Plutarch (c. 46–120 AD) mentions the practice, as do Tertullian, Orosius, Diodorus Siculus and Philo. The Hebrew Bible also mentions what appears to be child sacrifice practiced at a place called the Tophet (from the Hebrew "taph" or "toph", to burn) by the Canaanites. Writing in the 3rd century BCE, Kleitarchos, one of the historians of Alexander the Great, described that the infants rolled into the flaming pit. Diodorus Siculus wrote that babies were roasted to death inside the burning pit of the god Baal Hamon, a bronze statue.

The historical Greeks considered the practice of adult and child sacrifice barbarous, however, the exposure of newborns was widely practiced in ancient Greece, it was even advocated by Aristotle in the case of congenital deformity — "As to the exposure of children, let there be a law that no deformed child shall live.” In Greece the decision to expose a child was typically the father's, although in Sparta the decision was made by a group of elders. Exposure was the preferred method of disposal, as that act in itself was not considered to be murder; moreover, the exposed child technically had a chance of being rescued by the gods or any passersby. This very situation was a recurring motif in Greek mythology.
To notify the neighbors of a birth of a child, a woolen strip was hung over the front door to indicate a female baby and an olive branch to indicate a boy had been born. Families did not always keep their new child. After a woman had a baby, she would show it to her husband. If the husband accepted it, it would live, but if he refused it, it would die. Babies would often be rejected if they were illegitimate, unhealthy or deformed, the wrong sex, or too great a burden on the family. These babies would not be directly killed, but put in a clay pot or jar and deserted outside the front door or on the roadway. In ancient Greek religion, this practice took the responsibility away from the parents because the child would die of natural causes, for example hunger, asphyxiation or exposure to the elements.

The practice was prevalent in ancient Rome, as well. Philo was the first philosopher to speak out against it. A letter from a Roman citizen to his sister, or a pregnant wife from her husband, dating from 1 BC, demonstrates the casual nature with which infanticide was often viewed:

In some periods of Roman history it was traditional for a newborn to be brought to the "pater familias", the family patriarch, who would then decide whether the child was to be kept and raised, or left to die by exposure. The Twelve Tables of Roman law obliged him to put to death a child that was visibly deformed. The concurrent practices of slavery and infanticide contributed to the "background noise" of the crises during the Republic.

Infanticide became a capital offense in Roman law in 374 AD, but offenders were rarely if ever prosecuted.

According to mythology, Romulus and Remus, twin infant sons of the war god Mars, survived near-infanticide after being tossed into the Tiber River. According to the myth, they were raised by wolves, and later founded the city of Rome.

Judaism prohibits infanticide, and has for some time, dating back to at least early Common Era. Roman historians wrote about the ideas and customs of other peoples, which often diverged from their own. Tacitus recorded that the Jews "regard it as a crime to kill any late-born children". Josephus, whose works give an important insight into 1st-century Judaism, wrote that God "forbids women to cause abortion of what is begotten, or to destroy it afterward".

In his book "Germania", Tacitus wrote that the ancient Germanic tribes enforced a similar prohibition. He found such mores remarkable and commented: ""[The Germani] hold it shameful to kill any unwanted child."" Modern scholarship differs. John Boswell believed that in ancient Germanic tribes unwanted children were exposed, usually in the forest. "It was the custom of the [Teutonic] pagans, that if they wanted to kill a son or daughter, they would be killed before they had been given any food." Usually children born out of wedlock were disposed that way.

In his highly influential "Pre-historic Times", John Lubbock described burnt bones indicating the practice of child sacrifice in pagan Britain.

The last canto, "Marjatan poika" (Son of Marjatta), of Finnish national epic Kalevala describes an assumed infanticide. Väinämöinen orders the infant bastard son of Marjatta to be drowned in marsh.

The Íslendingabók, a main source for the early history of Iceland, recounts that on the Conversion of Iceland to Christianity in 1000 it was provided – in order to make the transition more palatable to Pagans – that "the old laws allowing exposure of newborn children will remain in force".
However, this provision – like other concessions made at the time to the Pagans – was abolished some years later.

Christianity rejects infanticide. The "Teachings of the Apostles" or "Didache" said "thou shalt not kill a child by abortion, neither shalt thou slay it when born". The "Epistle of Barnabas" stated an identical command, both thus conflating abortion and infanticide. Apologists Tertullian, Athenagoras, Minucius Felix, Justin Martyr and Lactantius also maintained that exposing a baby to death was a wicked act. In 318 AD, Constantine I considered infanticide a crime, and in 374 AD, Valentinian I mandated the rearing of all children (exposing babies, especially girls, was still common). The Council of Constantinople declared that infanticide was homicide, and in 589 AD, the Third Council of Toledo took measures against the custom of killing their own children.

Whereas theologians and clerics preached sparing their lives, newborn abandonment continued as registered in both the literature record and in legal documents. According to William L. Langer, exposure in the Middle Ages "was practiced on gigantic scale with absolute impunity, noticed by writers with most frigid indifference". At the end of the 12th century, notes Richard Trexler, Roman women threw their newborns into the Tiber river in daylight.

However, it also conjectured that the notion of "rampant" infanticide is a myth pushed by modern historians inferring from lack of particular records, and "there is absolutely no evidence to support such carnage."

Unlike other European regions, in the Middle Ages the German mother had the right to expose the newborn. In Gotland, Sweden, children were also sacrificed.

In the High Middle Ages, abandoning unwanted children finally eclipsed infanticide. Unwanted children were left at the door of church or abbey, and the clergy was assumed to take care of their upbringing. This practice also gave rise to the first orphanages.

However, very high sex ratios were common in even late medieval Europe, which may indicate sex-selective infanticide.

According to Islamic sources, pre-Islamic Arabian society practiced infanticide as a form of "post-partum birth control". Regarding the prevalence of this practice, we know it was "common enough among the pre-Islamic Arabs to be assigned a specific term, "waʾd"". Infanticide was practiced either out of destitution (thus practiced on males and females alike), or as sacrifices to gods, or as "disappointment and fear of social disgrace felt by a father upon the birth of a daughter".

Some authors believe that there is little evidence that infanticide was prevalent in pre-Islamic Arabia or early Muslim history, except for the case of the Tamim tribe, who practiced it during severe famine. Others state that "female infanticide was common all over Arabia during this period of time" (pre-Islamic Arabia), especially by burying alive a female newborn.

Islam
Infanticide is explicitly prohibited by the Qur'an. ""And do not kill your children for fear of poverty; We give them sustenance and yourselves too; surely to kill them is a great wrong.""
Together with polytheism and homicide, infanticide is regarded as a grave sin (see and ). Infanticide is also implicitly denounced in the story of Pharaoh's slaughter of the male children of Israelites (see ; ; ; ; ;).

Infanticide may have been practiced as human sacrifice, as part of the pagan cult of Perun. Ibn Fadlan describes sacrificial practices at the time of his trip to Kiev Rus (present day Ukraine) in 921–922, and describes an incident of a woman voluntarily sacrificing her life as part of a funeral rite for a prominent leader, but makes no mention of infanticide. The Primary Chronicle, one of the most important literary sources before the 12th century, indicates that human sacrifice to idols may have been introduced by Vladimir the Great in 980. The same Vladimir the Great formally converted Kiev Rus into Christianity just 8 years later, but pagan cults continued to be practiced clandestinely in remote areas as late as the 13th century.

In Kamchatka, babies were killed and thrown to the dogs. American explorer George Kennan noted that among the Koryaks, a Mongoloid people of north-eastern Siberia, infanticide was still common in the nineteenth century. One of a pair of twins was always sacrificed.

The Svans killed newborn females by filling their mouths with hot ashes.

Infanticide (as a crime) gained both popular and bureaucratic significance in Victorian Britain. By the mid 19th century, in the context of criminal lunacy and the insanity defence, killing one's own child(ren) attracted ferocious debate, as the role of women in society was defined by motherhood, and it was thought that any woman who murdered her own child was by definition insane and could not be held responsible for her actions. Several cases were subsequently highlighted during the Royal Commission on Capital Punishment (1864-66), as a particular felony where an effective avoidance of the death penalty had informally begun.

Infanticide was common in the Victorian period for social reasons, such as illegitimacy, and the introduction of child life insurance additionally encouraged some women to kill their children for gain. Examples are Mary Ann Cotton, who murdered many of her 15 children as well as 3 husbands, Margaret Waters, the 'Brixton Baby Farmer', a professional baby-farmer who was found guilty of infanticide in 1870, Jessie King hanged in 1889, Amelia Dyer, the 'Angel Maker', who murdered over 400 babies in her care, and Ada Chard-Williams, a baby farmer who was later hanged at Newgate prison.

Recording a birth as a still-birth was also another way of concealing infanticide because still-births did not need to be registered until 1926 and they did not need to be buried in public cemeteries. In 1895 the Sun (London) published an article "Massacre of the Innocents" highlighting the dangers of baby-farming, in the recording of stillbirths and quoting Braxton-Hicks, the London Coroner, on lying-in houses: "I have not the slightest doubt that a large amount of crime is covered by the expression `still-birth’. There are a large number of cases of what are called newly-born children, which are found all over England, more especially in London and large towns, abandoned in streets, rivers, on commons, and so on." He continued "a great deal of that crime is due to what are called lying-in houses, which are not registered, or under the supervision of that sort, where the people who act as midwives constantly, as soon as the child is born, either drop it into a pail of water or smother it with a damp cloth. It is a very common thing, also, to find that they bash their heads on the floor and break their skulls."

The last British woman to be executed for infanticide of her own child was Rebecca Smith, who was hanged in Wiltshire in 1849.

Short of execution, the harshest penalties were imposed on practitioners of infanticide by the legal codes of the Qin dynasty and Han dynasty of ancient China.

Marco Polo, the explorer, saw newborns exposed in Manzi. China's society practiced sex selective infanticide. Philosopher Han Fei Tzu, a member of the ruling aristocracy of the 3rd century BC, who developed a school of law, wrote: "As to children, a father and mother when they produce a boy congratulate one another, but when they produce a girl they put it to death." Among the Hakka people, and in Yunnan, Anhui, Sichuan, Jiangxi and Fujian a method of killing the baby was to put her into a bucket of cold water, which was called "baby water".

Infanticide was known in China as early as the 3rd century BC, and, by the time of the Song dynasty (960–1279 AD), it was widespread in some provinces. Buddhist belief in transmigration allowed poor residents of the country to kill their newborn children if they felt unable to care for them, hoping that they would be reborn in better circumstances. Furthermore, some Chinese did not consider newborn children fully "human", and saw "life" beginning at some point after the sixth month after birth.

Contemporary writers from the Song dynasty note that, in Hubei and Fujian provinces, residents would only keep three sons and two daughters (among poor farmers, two sons and one daughter), and kill all babies beyond that number at birth. Initially the sex of the child was only one factor to consider. By the time of the Ming Dynasty, however (1368–1644), male infanticide was becoming increasingly uncommon. The prevalence of female infanticide remained high much longer. The magnitude of this practice is subject to some dispute; however, one commonly quoted estimate is that, by late Qing, between one fifth and one quarter of all newborn girls, across the entire social spectrum, were victims of infanticide. If one includes excess mortality among female children under 10 (ascribed to gender-differential neglect), the share of victims rises to one third.

Scottish Physician John Dudgeon, who worked in Beijing, China, during the Qing Dynasty said that in China, "Infanticide does not prevail to the extent so generally believed among us, and in the north it does not exist at all."
Gender-selected abortion, abandonment, and infanticide are illegal in present-day China. Nevertheless, the US State Department, and the human rights organization Amnesty International have all declared that China's family planning programs, called the one child policy, contribute to infanticide.

Since feudal Japan the common slang for infanticide was ""mabiki"" (間引き) which means to pull plants from an overcrowded garden. A typical method in Japan was smothering through wet paper on the baby's mouth and nose. It became common as a method of population control. Farmers would often kill their second or third sons. Daughters were usually spared, as they could be married off, sold off as servants or prostitutes, or sent off to become geishas. Mabiki persisted in the 19th century and early 20th century. To bear twins was perceived as barbarous and unlucky and efforts were made to hide or kill one or both twins.

Female infanticide of newborn girls was systematic in feudatory Rajputs in South Asia for illegitimate female children during the Middle Ages. According to Firishta, as soon as the illegitimate female child was born she was held "in one hand, and a knife in the other, that any person who wanted a wife might take her now, otherwise she was immediately put to death". The practice of female infanticide was also common among the Kutch, Kehtri, Nagar, Bengal, Miazed, Kalowries in India inhabitants, and also among the Sindh in British India.

It was not uncommon that parents threw a child to the sharks in the Ganges River as a sacrificial offering. The British colonists were unable to outlaw the custom until the beginnings of the 19th century.

According to social activists, female infanticide has remained a problem in India into the 21st century, with both NGOs and the government conducting awareness campaigns to combat it.

In some African societies some neonates were killed because of beliefs in evil omens or because they were considered unlucky. Twins were usually put to death in Arebo; as well as by the Nama people of South West Africa; in the Lake Victoria Nyanza region; by the Tswana in Portuguese East Africa; in some parts of Igboland, Nigeria twins were sometimes abandoned in a forest at birth (as depicted in "Things Fall Apart"), oftentimes one twin was killed or hidden by midwives of wealthier mothers; and by the !Kung people of the Kalahari Desert. The Kikuyu, Kenya's most populous ethnic group, practiced ritual killing of twins.

Literature suggests infanticide may have occurred reasonably commonly among Indigenous Australians, in all areas of Australia prior to European settlement. Infanticide may have continued to occur quite often up until the 1960s. An 1866 issue of "The Australian News for Home Readers" informed readers that "the crime of infanticide is so prevalent amongst the natives that it is rare to see an infant".

Author Susanna de Vries in 2007 told a newspaper that her accounts of Aboriginal violence, including infanticide, were censored by publishers in the 1980s and 1990s. She told reporters that the censorship "stemmed from guilt over the stolen children question". Keith Windschuttle weighed in on the conversation, saying this type of censorship started in the 1970s. In the same article Louis Nowra suggested that infanticide in customary Aboriginal law may have been because it was difficult to keep an abundant number of Aboriginal children alive; there were life-and-death decisions modern-day Australians no longer have to face.

According to William D. Rubinstein, "Nineteenth-century European observers of Aboriginal life in South Australia and Victoria reported that about 30% of Aboriginal infants were killed at birth."

James Dawson wrote a passage about infanticide among Indigenous people in the western district of Victoria, which stated that "Twins are as common among them as among Europeans; but as food is occasionally very scarce, and a large family troublesome to move about, it is lawful and customary to destroy the weakest twin child, irrespective of sex.
It is usual also to destroy those which are malformed."

He also wrote "When a woman has children too rapidly for the convenience and necessities of the parents, she makes up her mind to let one be killed, and consults with her husband which it is to be. As the strength of a tribe depends more on males than females, the girls are generally sacrificed.
The child is put to death and buried, or burned without ceremony; not, however, by its father or mother, but by relatives. No one wears mourning for it.
Sickly children are never killed on account of their bad health, and are allowed to die naturally."

In 1937, a reverend in the Kimberley offered a "baby bonus" to Aboriginal families as a deterrent against infanticide and to increase the birthrate of the local Indigenous population.

A Canberran journalist in 1927 wrote of the "cheapness of life" to the Aboriginal people local to the Canberra area 100 years before. "If drought or bush fires had devastated the country and curtailed food supplies, babies got short shift. Ailing babies, too would not be kept" he wrote.

A bishop wrote in 1928 that it was common for Aboriginal Australians to restrict the size of their tribal groups, including by infanticide, so that the food resources of the tribal area may be sufficient for them.

Annette Hamilton, a professor of anthropology at Macquarie University who carried out research in the Aboriginal community of Maningrida in Arnhem Land during the 1960s wrote that prior to that time part-European babies born to Aboriginal mothers had not been allowed to live, and that 'mixed-unions are frowned on by men and women alike as a matter of principle'.

There is no agreement about the actual estimates of the frequency of newborn female infanticide in the Inuit population. Carmel Schrire mentions diverse studies ranging from 15–50% to 80%.

Polar Inuit (Inughuit) killed the child by throwing him or her into the sea. There is even a legend in Inuit mythology, "The Unwanted Child", where a mother throws her child into the fjord.

The Yukon and the Mahlemuit tribes of Alaska exposed the female newborns by first stuffing their mouths with grass before leaving them to die. In Arctic Canada the Inuit exposed their babies on the ice and left them to die.

Female Inuit infanticide disappeared in the 1930s and 1940s after contact with the Western cultures from the South.

The "Handbook of North American Indians" reports infanticide among the Dene Natives and those of the Mackenzie Mountains.

In the Eastern Shoshone there was a scarcity of Indian women as a result of female infanticide. For the Maidu Native Americans twins were so dangerous that they not only killed them, but the mother as well. In the region known today as southern Texas, the Mariame Indians practiced infanticide of females on a large scale. Wives had to be obtained from neighboring groups.

Bernal Díaz recounted that, after landing on the Veracruz coast, they came across a temple dedicated to Tezcatlipoca. "That day they had sacrificed two boys, cutting open their chests and offering their blood and hearts to that accursed idol". In "The Conquest of New Spain" Díaz describes more child sacrifices in the towns before the Spaniards reached the large Aztec city Tenochtitlan.

Although academic data of infanticides among the indigenous people in South America is not as abundant as that of North America, the estimates seem to be similar.

The Tapirapé indigenous people of Brazil allowed no more than three children per woman, and no more than two of the same sex. If the rule was broken infanticide was practiced. The Bororo killed all the newborns that did not appear healthy enough. Infanticide is also documented in the case of the Korubo people in the Amazon.

The Yanomami men killed children while raiding enemy villages. Helena Valero, a Brazilian woman kidnapped by Yanomami warriors in the 1930s, witnessed a Karawetari raid on her tribe:

While "qhapaq hucha" was practiced in the Peruvian large cities, child sacrifice in the pre-Columbian tribes of the region is less documented. However, even today studies on the Aymara Indians reveal high incidences of mortality among the newborn, especially female deaths, suggesting infanticide. The Abipones, a small tribe of Guaycuruan stock, of about 5,000 by the end of the 18th century in Paraguay, practiced systematic infanticide; with never more than two children being reared in one family. The Machigenga killed their disabled children. Infanticide among the Chaco in Paraguay was estimated as high as 50% of all newborns in that tribe, who were usually buried. The infanticidal custom had such roots among the Ayoreo in Bolivia and Paraguay that it persisted until the late 20th century.

Infanticide has become less common in the Western world. The frequency has been estimated to be 1 in approximately 3000 to 5000 children of all ages and 2.1 per 100,000 newborns per year. It is thought that infanticide today continues at a much higher rate in areas of extremely high poverty and overpopulation, such as parts of China and India. Female infants, then and even now, are particularly vulnerable, a factor in sex-selective infanticide. Recent estimates suggest that over 100 million girls and women are 'missing' in Asia.

In spite of the fact that it is illegal, in Benin, West Africa, parents secretly continue with infanticidal customs.

According to "The Hidden Gulag" published by the Committee for Human Rights in North Korea, the People's Republic of China returns all illegal immigrants from North Korea which usually imprisons them in a short term facility. Women who are suspected of being impregnated by Chinese fathers are subjected to forced abortions; babies born alive are killed, sometimes by exposure or being buried alive.

There have been some accusations that infanticide occurs in the People's Republic of China due to the one-child policy. In the 1990s, a certain stretch of the Yangtze River was known to be a common site of infanticide by drowning, until government projects made access to it more difficult. Recent studies suggest that over 40 million girls and women are 'missing' in China (Klasen and Wink 2003).

The practice has continued in some rural areas of India. Infanticide is illegal in India.

According to a recent report by the United Nations Children's Fund (UNICEF) up to 50 million girls and women are missing in India's population as a result of systematic sex discrimination and sex selective abortions.

Killings of newborn babies have been on the rise in Pakistan, corresponding to an increase in poverty across the country. More than 1,000 infants, mostly girls, were killed or abandoned to die in Pakistan in 2009 according to a Pakistani charity organization.

The Edhi Foundation found 1,210 dead babies in 2010. Many more are abandoned and left at the doorsteps of mosques. As a result, Edhi centers feature signs "Do not murder, lay them here." Though female infanticide is punishable by life in prison, such crimes are rarely prosecuted.

In November 2008 it was reported that in Agibu and Amosa villages of Gimi region of Eastern Highlands province of Papua New Guinea where tribal fighting in the region of Gimi has been going on since 1986 (many of the clashes arising over claims of sorcery) women had agreed that if they stopped producing males, allowing only female babies to survive, their tribe's stock of boys would go down and there would be no men in the future to fight. They agreed to have all newborn male babies killed. It is not known how many male babies were killed by being smothered, but it had reportedly happened to all males over a 10-year period and probably was still happening.

In England and Wales there were typically 30 to 50 homicides per million children less than 1 year old between 1982 and 1996. The younger the infant, the higher the risk. The rate for children 1 to 5 years was around 10 per million children. The homicide rate of infants less than 1 year is significantly higher than for the general population.

In 1983, the United States ranked eleventh for infants under 1 year killed, and fourth for those killed from 1 through 14 years (the latter case not necessarily involving filicide). In the U.S. over six hundred children were killed by their parents in 1983.

In the United States the infanticide rate during the first hour of life outside the womb dropped from 1.41 per 100,000 during 1963 to 1972 to 0.44 per 100,000 for 1974 to 1983; the rates during the first month after birth also declined, whereas those for older infants rose during this time. The legalization of abortion, which was completed in 1973, was the most important factor in the decline in neonatal mortality during the period from 1964 to 1977, according to a study by economists associated with the National Bureau of Economic Research.

In Canada 114 cases of infanticide by a parent were reported during 1964–1968. There is ongoing debate in the Canadian legal and political fields about whether section 237 of the Criminal Code, which creates the specific offence and partial defence of infanticide in Canadian law, should be amended or abolished altogether.

In a 2012 article in the "Journal of Medical Ethics", a philosopher and a bioethicist jointly proposed that infanticide be legalized, calling it "after-birth abortion", and claiming that both "the fetus and the newborn are potential persons". Many replies were published to this article.

Euthanasia applied to children that are gravely ill or that suffer from significant birth defects is legal in the Netherlands under rigidly controlled conditions, but controversial. Some critics have compared child euthanasia to infanticide.

There are various reasons for infanticide. Neonaticide typically has different patterns and causes than for killing of older infants. Traditional neonaticide is often related to economic necessity - inability to provide for the infant.

In the United Kingdom and the United States, older infants are typically killed for reasons related to child abuse, domestic violence or mental illness. For infants older than one day, younger infants are more at risk, and boys are more at risk than girls. Risk factors for the parent include: Family history of violence, violence in current relationship, history of abuse or neglect of children, and personality disorder and/or depression.

In the late seventeenth and early eighteenth centuries, "loopholes" were invented by those who wanted to avoid the damnation that was promised by most Christian doctrine as a penalty of suicide. One famous example of someone who wished to end their life but avoid the eternity in hell was Christina Johansdotter (died 1740). She was a Swedish murderer who killed a child in Stockholm with the sole purpose of being executed. She is an example of those who seek suicide through execution by committing a murder. It was a common act, frequently targeting young children or infants as they were believed to be free from sin, thus going straight to heaven.

In 1888, Lieut. F. Elton reported that Ugi beach people in the Solomon Islands killed their infants at birth by burying them, and women were also said to practice abortion. They reported that it was too much trouble to raise a child, and instead preferred to buy one from the bush people.

Many historians believe the reason to be primarily economic, with more children born than the family is prepared to support. In societies that are patrilineal and patrilocal, the family may choose to allow more sons to live and kill some daughters, as the former will support their birth family until they die, whereas the latter will leave economically and geographically to join their husband's family, possibly only after the payment of a burdensome dowry price. Thus the decision to bring up a boy is more economically rewarding to the parents. However, this does not explain why infanticide would occur equally among rich and poor, nor why it would be as frequent during decadent periods of the Roman Empire as during earlier, less affluent, periods.

Before the appearance of effective contraception, infanticide was a common occurrence in ancient brothels. Unlike usual infanticide - where historically girls have been more likely to be killed - prostitutes in certain areas preferred to kill their male offspring.

Instances of infanticide in Britain in 18th and 19th centuries is often attributed to the economic position of the women, with juries committing “pious perjury” in many subsequent murder cases. The knowledge of the difficulties faced in the 18th century by those women who attempted to keep their children can be seen as reason for juries to show compassion. If the woman chose to keep the child, society was not set up to ease the pressure placed upon the woman, legally, socially or economically.

In mid-18th century Britain there was assistance available for women who were not able to raise their children. The Foundling Hospital opened in 1756 and was able to take in some of the illegitimate children. However, the conditions within the hospital caused Parliament to withdraw funding and the governors to live off of their own incomes. This resulted in a stringent entrance policy, with the committee requiring that the hospital:

Once a mother had admitted her child to the hospital, the hospital did all it could to ensure that the parent and child were not re-united.

MacFarlane argues in "Illegitimacy and Illegitimates in Britain" (1980) that English society greatly concerned itself with the burden that a bastard child places upon its communities and had gone to some lengths to ensure that the father of the child is identified in order to maintain its well-being. Assistance could be gained through maintenance payments from the father, however, this was capped "at a miserable 2 s and 6 d a week". If the father fell behind with the payments he could only be asked "to pay a maximum of 13 weeks arrears".

Despite the accusations of some that women were getting a free hand-out there is evidence that many women were far from receiving adequate assistance from their parish. "Within Leeds in 1822 ... relief was limited to 1 s per week". Sheffield required women to enter the workhouse, whereas Halifax gave no relief to the women who required it. The prospect of entering the workhouse was certainly something to be avoided. Lionel Rose quotes Dr Joseph Rogers in "Massacre of the Innocents ..." (1986). Rogers, who was employed by a London workhouse in 1856 stated that conditions in the nursery were ‘wretchedly damp and miserable ... [and] ... overcrowded with young mothers and their infants’.

The loss of social standing for a servant girl was a particular problem in respect of producing a bastard child as they relied upon a good character reference in order to maintain their job and more importantly, to get a new or better job. In a large number of trials for the crime of infanticide, it is the servant girl that stood accused. The disadvantage of being a servant girl is that they had to live to the social standards of their superiors or risk dismissal and no references. Whereas within other professions, such as in the factory, the relationship between employer and employee was much more anonymous and the mother would be better able to make other provisions, such as employing a minder. The result of the lack of basic social care in Britain in the 18th and 19th century is the numerous accounts in court records of women, particularly servant girls, standing trial for the murder of their child.

There may have been no specific offence of infanticide in England before about 1623 because infanticide was a matter for the by ecclesiastical courts, possibly because infant mortality from natural causes was high (about 15% or one in six).

Thereafter the accusation of the suppression of bastard children by lewd mothers was a crime incurring the presumption of guilt.

The Infanticide Acts are several laws. That of 1922 made the killing of an infant child by its mother during the early months of life as a lesser crime than murder. The acts of 1938 and 1939 abolished the earlier act, but introduced the idea that postpartum depression was legally to be regarded as a form of diminished responsibility.

Marvin Harris estimated that among Paleolithic hunters 23–50% of newborn children were killed. He argued that the goal was to preserve the 0.001% population growth of that time. He also wrote that female infanticide may be a form of population control. Population control is achieved not only by limiting the number of potential mothers; increased fighting among men for access to relatively scarce wives would also lead to a decline in population. For example, on the Melanesian island of Tikopia infanticide was used to keep a stable population in line with its resource base. Research by Marvin Harris and William Divale supports this argument, it has been cited as an example of environmental determinism.

Evolutionary psychology has proposed several theories for different forms of infanticide. Infanticide by stepfathers, as well as child abuse in general by stepfathers, has been explained by spending resources on not genetically related children reducing reproductive success (See the Cinderella effect and Infanticide (zoology)). Infanticide is one of the few forms of violence more often done by women than men. Cross-cultural research has found that this is more likely to occur when the child has deformities or illnesses as well as when there are lacking resources due to factors such as poverty, other children requiring resources, and no male support. Such a child may have a low chance of reproductive success in which case it would decrease the mother's inclusive fitness, in particular since women generally have a greater parental investment than men, to spend resources on the child.

A minority of academics subscribe to an alternate school of thought, considering the practice as "early infanticidal childrearing". They attribute parental infanticidal wishes to massive projection or displacement of the parents' unconscious onto the child, because of intergenerational, ancestral abuse by their own parents. Clearly, an infanticidal parent may have multiple motivations, conflicts, emotions, and thoughts about their baby and their relationship with their baby, which are often colored both by their individual psychology, current relational context and attachment history, and, perhaps most saliently, their psychopathology (See also Psychiatric section below) Almeida, Merminod, and Schechter suggest that parents with fantasies, projections, and delusions involving infanticide need to be taken seriously and assessed carefully, whenever possible, by an interdisciplinary team that includes infant mental health specialists or mental health practitioners who have experience in working with parents, children, and families.

In addition to debates over the morality of infanticide itself, there is some debate over the effects of infanticide on surviving children, and the effects of childrearing in societies that also sanction infanticide. Some argue that the practice of infanticide in any widespread form causes enormous psychological damage in children. Conversely, studying societies that practice infanticide Géza Róheim reported that even infanticidal mothers in New Guinea, who ate a child, did not affect the personality development of the surviving children; that "these are good mothers who eat their own children". Harris and Divale's work on the relationship between female infanticide and warfare suggests that there are, however, extensive negative effects.

Postpartum psychosis is also a causative factor of infanticide. Stuart S. Asch, MD, a Professor of Psychiatry at Cornell University established the connections between some cases of infanticide and post-partum depression. The books, "From Cradle to Grave", and "The Death of Innocents", describe selected cases of maternal infanticide and the investigative research of Professor Asch working in concert with the New York City Medical Examiner's Office.
Stanley Hopwood wrote that childbirth and lactation entail severe stress on the female sex, and that under certain circumstances attempts at infanticide and suicide are common. A study published in the "American Journal of Psychiatry" revealed that 44% of filicidal fathers had a diagnosis of psychosis. In addition to postpartum psychosis, dissociative psychopathology and sociopathy have also been found to be associated with neonaticide in some cases

In addition, severe postpartum depression can lead to infanticide.

Sex selection may be one of the contributing factors of infanticide. In the absence of sex-selective abortion, sex-selective infanticide can be deduced from very skewed birth statistics. The biologically normal sex ratio for humans at birth is approximately 105 males per 100 females; normal ratios hardly ranging beyond 102–108. When a society has an infant male to female ratio which is significantly higher or lower than the biological norm, and biased data can be ruled out, sex selection can usually be inferred.

In New South Wales, infanticide is defined in Section 22A(1) of the Crimes Act 1900 (NSW) as follows:

Because Infanticide is punishable as manslaughter, as per s24, the maximum penalty for this offence is therefore 25 years imprisonment.

Infanticide is illegal in India, but rarely enforced in the rural parts of India. 

In Canada, a mother commits infanticide, a lesser offence than homicide, if she killed her child while "not fully recovered from the effects of giving birth to the child and by reason thereof or of the effect of lactation consequent on the birth of the child her mind is then disturbed".

In England and Wales, the Infanticide Act 1938 describes the offence of infanticide as one which would otherwise amount to murder (by his/her mother) if the victim was older than 12 months and the mother was not suffering from an imbalance of mind due to the effects of childbirth or lactation. Where a mother who has killed such an infant has been charged with murder rather than infanticide s.1(3) of the Act confirms that a jury has the power to find alternative verdicts of Manslaughter in English law or guilty but insane.

Article 200 of the Penal Code of Romania stipulates that the killing of a newborn during the first 24 hours, by the mother who is in a state of mental distress, shall be punished with imprisonment of one to five years. The previous Romanian Penal Code also defined infanticide ("pruncucidere") as a distinct criminal offence, providing for a punishment of two to seven years imprisonment, recognizing the fact that a mother's judgment may be impaired immediately after birth, but did not define the term "infant", and this had led to debates regarding the precise moment when infanticide becomes homicide. This issue was resolved by the new Penal Code, which came into force in 2014.

In 2009, Texas state representative Jessica Farrar proposed legislation that would define infanticide as a distinct and lesser crime than homicide. Under the terms of the proposed legislation, if jurors concluded that a mother's "judgment was impaired as a result of the effects of giving birth or the effects of lactation following the birth", they would be allowed to convict her of the crime of infanticide, rather than murder. The maximum penalty for infanticide would be two years in prison. Farrar's introduction of this bill prompted liberal bioethics scholar Jacob M. Appel to call her "the bravest politician in America".

Since infanticide, especially neonaticide, is often a response to an unwanted birth, preventing unwanted pregnancies through improved sex education and increased contraceptive access are advocated as ways of preventing infanticide. Increased use of contraceptives and access to safe legal abortions have greatly reduced neonaticide in many developed nations. Some say that where abortion is illegal, as in Pakistan, infanticide would decline if safer legal abortions were available.

Screening for psychiatric disorders or risk factors, and providing treatment or assistance to those at risk may help prevent infanticide. However, in developed world significant proportions of neonaticides that are detected occur in young women who deny their pregnancy, and avoid outside contacts, so they may have limited contact with health care services.

In some areas baby hatches or "safe surrender sites", safe places for a mother to anonymously leave an infant, are offered, in part to reduce the rate of infanticide. In other places, like the United States, safe-haven laws allow mothers to anonymously give infants to designated officials; they are frequently located at hospitals and police and fire stations. Typically such babies are put up for adoption, or cared for in orphanages.

Granting women employment raises their status and autonomy. Having a gainful employment can raise the perceived worth of females. This can lead to an increase in the number of women getting an education and a decrease in the number of female infanticide. As a result, the infant mortality rate will decrease and economic development will increase.

Although human infanticide has been widely studied, the practice has been observed in many other species of the animal kingdom since it was first seriously studied by . These include from microscopic rotifers and insects, to fish, amphibians, birds and mammals, including primates such as chacma baboons. Infanticide can be practiced by both males and females.

According to studies carried out by Kyoto University in non-human primates, including certain types of gorillas and chimpanzees, several conditions favor the tendency to infanticide in some species (to be performed only by males), among them are: Nocturnal live, the absence of nest construction, the marked sexual dimorphism in which the male is much larger than the female, the mating in a specific season and the high period of lactation without resumption of the estrus state in the female.



</doc>
<doc id="15476" url="https://en.wikipedia.org/wiki?curid=15476" title="Internet protocol suite">
Internet protocol suite

The Internet protocol suite is the conceptual model and set of communications protocols used on the Internet and similar computer networks. It is commonly known as TCP/IP because the foundational protocols in the suite are the Transmission Control Protocol (TCP) and the Internet Protocol (IP). It is occasionally known as the Department of Defense (DoD) model because the development of the networking method was funded by the United States Department of Defense through DARPA.

The Internet protocol suite provides end-to-end data communication specifying how data should be packetized, addressed, transmitted, routed, and received. This functionality is organized into four abstraction layers, which classify all related protocols according to the scope of networking involved. From lowest to highest, the layers are the link layer, containing communication methods for data that remains within a single network segment (link); the internet layer, providing internetworking between independent networks; the transport layer, providing end-to-end communication services for applications; and the application layer, providing services to users and system functions.

Technical standards specifying the Internet protocol suite and many of its constituent protocols are maintained by the Internet Engineering Task Force (IETF). The Internet protocol suite predates the OSI model, a more comprehensive reference framework for general networking systems.

The Internet protocol suite resulted from research and development conducted by the Defense Advanced Research Projects Agency (DARPA) in the late 1960s. After initiating the pioneering ARPANET in 1969, DARPA started work on a number of other data transmission technologies. In 1972, Robert E. Kahn joined the DARPA Information Processing Technology Office, where he worked on both satellite packet networks and ground-based radio packet networks, and recognized the value of being able to communicate across both. In the spring of 1973, Vinton Cerf, the developer of the existing ARPANET Network Control Program (NCP) protocol, joined Kahn to work on open-architecture interconnection models with the goal of designing the next protocol generation for the ARPANET.

By the summer of 1973, Kahn and Cerf had worked out a fundamental reformulation, in which the differences between local network protocols were hidden by using a common internetwork protocol, and, instead of the network being responsible for reliability, as in the ARPANET, this function was delegated to the hosts. Cerf credits Hubert Zimmermann and Louis Pouzin, designer of the CYCLADES network, with important influences on this design. The protocol was implemented as the Transmission Control Program, first published in 1974.

Initially, the TCP managed both datagram transmissions and routing, but as the protocol grew, other researchers recommended a division of functionality into protocol layers. Advocates included Jonathan Postel of the University of Southern California's Information Sciences Institute, who edited the Request for Comments (RFCs), the technical and strategic document series that has both documented and catalyzed Internet development. Postel stated, "We are screwing up in our design of Internet protocols by violating the principle of layering." Encapsulation of different mechanisms was intended to create an environment where the upper layers could access only what was needed from the lower layers. A monolithic design would be inflexible and lead to scalability issues. The Transmission Control Program was split into two distinct protocols, the Transmission Control Protocol and the Internet Protocol.

The design of the network included the recognition that it should provide only the functions of efficiently transmitting and routing traffic between end nodes and that all other intelligence should be located at the edge of the network, in the end nodes. This design is known as the end-to-end principle. Using this design, it became possible to connect almost any network to the ARPANET, irrespective of the local characteristics, thereby solving Kahn's initial internetworking problem. One popular expression is that TCP/IP, the eventual product of Cerf and Kahn's work, can run over "two tin cans and a string." Years later, as a joke, the IP over Avian Carriers formal protocol specification was created and successfully tested.

A computer called a router is provided with an interface to each network. It forwards network packets back and forth between them. Originally a router was called "gateway", but the term was changed to avoid confusion with other types of gateways.

From 1973 to 1974, Cerf's networking research group at Stanford worked out details of the idea, resulting in the first TCP specification. A significant technical influence was the early networking work at Xerox PARC, which produced the PARC Universal Packet protocol suite, much of which existed around that time.

DARPA then contracted with BBN Technologies, Stanford University, and the University College London to develop operational versions of the protocol on different hardware platforms. Four versions were developed: TCP v1, TCP v2, TCP v3 and IP v3, and TCP/IP v4. The last protocol is still in use today. In 1975, a two-network TCP/IP communications test was performed between Stanford and University College London (UCL). In November, 1977, a three-network TCP/IP test was conducted between sites in the US, the UK, and Norway. Several other TCP/IP prototypes were developed at multiple research centers between 1978 and 1983. The migration of the ARPANET to TCP/IP was officially completed on flag day January 1, 1983, when the new protocols were permanently activated.

In March 1982, the US Department of Defense declared TCP/IP as the standard for all military computer networking.

In 1985, the Internet Advisory Board (later renamed the Internet Architecture Board) held a three-day workshop on TCP/IP for the computer industry, attended by 250 vendor representatives, promoting the protocol and leading to its increasing commercial use. In 1985, the first Interop conference focused on network interoperability by broader adoption of TCP/IP. The conference was founded by Dan Lynch, an early Internet activist. From the beginning, large corporations, such as IBM and DEC, attended the meeting.

IBM, AT&T and DEC were the first major corporations to adopt TCP/IP, this despite having competing proprietary protocols. In IBM, from 1984, Barry Appelman's group did TCP/IP development. They navigated the corporate politics to get a stream of TCP/IP products for various IBM systems, including MVS, VM, and OS/2. At the same time, several smaller companies, such as FTP Software and the Wollongong Group, began offering TCP/IP stacks for DOS and Microsoft Windows. The first VM/CMS TCP/IP stack came from the University of Wisconsin.

Some of the early TCP/IP stacks were written single-handedly by a few programmers. Jay Elinsky and Oleg Vishnepolsky of IBM Research wrote TCP/IP stacks for VM/CMS and OS/2, respectively. In 1984 Donald Gillies at MIT wrote a "ntcp" multi-connection TCP which ran atop the IP/PacketDriver layer maintained by John Romkey at MIT in 1983-4. Romkey leveraged this TCP in 1986 when FTP Software was founded. Phil Karn created KA9Q TCP (a multi-connection TCP for ham radio applications) starting in 1985.

The spread of TCP/IP was fueled further in June 1989, when the University of California, Berkeley agreed to place the TCP/IP code developed for BSD UNIX into the public domain. Various vendors, including IBM, included this code in their own TCP/IP stacks. Microsoft released a native TCP/IP stack in Windows 95. This event was a little late in the evolution of the Internet, but it cemented TCP/IP's dominance over other protocols, which began to lose ground. These protocols included IBM Systems Network Architecture (SNA), Digital Equipment Corporation's DECnet, Open Systems Interconnection (OSI), and Xerox Network Systems (XNS).

An early architectural document, RFC 1122, emphasizes architectural principles over layering.

The end-to-end principle has evolved over time. Its original expression put the maintenance of state and overall intelligence at the edges, and assumed the Internet that connected the edges retained no state and concentrated on speed and simplicity. Real-world needs for firewalls, network address translators, web content caches and the like have forced changes in this principle.

The robustness principle states: "In general, an implementation must be conservative in its sending behavior, and liberal in its receiving behavior. That is, it must be careful to send well-formed datagrams, but must accept any datagram that it can interpret (e.g., not object to technical errors where the meaning is still clear)." "The second part of the principle is almost as important: software on other hosts may contain deficiencies that make it unwise to exploit legal but obscure protocol features." Postel famously summarized the principle as, "Be conservative in what you do, be liberal in what you accept from others"—a saying that came to be known as "Postel's Law."

Encapsulation is used to provide abstraction of protocols and services. Encapsulation is usually aligned with the division of the protocol suite into layers of general functionality. In general, an application (the highest level of the model) uses a set of protocols to send its data down the layers, being further encapsulated at each level.

The layers of the protocol suite near the top are logically closer to the user application, while those near the bottom are logically closer to the physical transmission of the data. Viewing layers as providing or consuming a service is a method of abstraction to isolate upper layer protocols from the details of transmitting bits over, for example, Ethernet and collision detection, while the lower layers avoid having to know the details of each and every application and its protocol.

Even when the layers are examined, the assorted architectural documents—there is no single architectural model such as ISO 7498, the Open Systems Interconnection (OSI) model—have fewer and less rigidly defined layers than the OSI model, and thus provide an easier fit for real-world protocols. One frequently referenced document, RFC 1958, does not contain a stack of layers. The lack of emphasis on layering is a major difference between the IETF and OSI approaches. It only refers to the existence of the internetworking layer and generally to "upper layers"; this document was intended as a 1996 snapshot of the architecture: "The Internet and its architecture have grown in evolutionary fashion from modest beginnings, rather than from a Grand Plan. While this process of evolution is one of the main reasons for the technology's success, it nevertheless seems useful to record a snapshot of the current principles of the Internet architecture."

RFC 1122, entitled "Host Requirements", is structured in paragraphs referring to layers, but the document refers to many other architectural principles not emphasizing layering. It loosely defines a four-layer model, with the layers having names, not numbers, as follows:

The Internet protocol suite and the layered protocol stack design were in use before the OSI model was established. Since then, the TCP/IP model has been compared with the OSI model in books and classrooms, which often results in confusion because the two models use different assumptions and goals, including the relative importance of strict layering.

This abstraction also allows upper layers to provide services that the lower layers do not provide. While the original OSI model was extended to include connectionless services (OSIRM CL), IP is not designed to be reliable and is a best effort delivery protocol. This means that all transport layer implementations must choose whether or how to provide reliability. UDP provides data integrity via a checksum but does not guarantee delivery; TCP provides both data integrity and delivery guarantee by retransmitting until the receiver acknowledges the reception of the packet.

This model lacks the formalism of the OSI model and associated documents, but the IETF does not use a formal model and does not consider this a limitation, as illustrated in the comment by David D. Clark, "We reject: kings, presidents and voting. We believe in: rough consensus and running code." Criticisms of this model, which have been made with respect to the OSI model, often do not consider ISO's later extensions to that model.

For multi-access links with their own addressing systems (e.g. Ethernet) an address mapping protocol is needed. Such protocols can be considered to be below IP but above the existing link system. While the IETF does not use the terminology, this is a subnetwork dependent convergence facility according to an extension to the OSI model, the internal organization of the network layer (IONL).

ICMP & IGMP operate on top of IP but do not transport data like UDP or TCP. Again, this functionality exists as layer management extensions to the OSI model, in its "Management Framework" (OSIRM MF)

The SSL/TLS library operates above the transport layer (uses TCP) but below application protocols. Again, there was no intention, on the part of the designers of these protocols, to comply with OSI architecture.

The link is treated as a black box. The IETF explicitly does not intend to discuss transmission systems, which is a less academic but practical alternative to the OSI model.

The following is a description of each layer in the TCP/IP networking model starting from the lowest level.

The link layer has the networking scope of the local network connection to which a host is attached. This regime is called the "link" in TCP/IP literature. It is the lowest component layer of the Internet protocols, as TCP/IP is designed to be hardware independent. As a result, TCP/IP may be implemented on top of virtually any hardware networking technology.

The link layer is used to move packets between the Internet layer interfaces of two different hosts on the same link. The processes of transmitting and receiving packets on a given link can be controlled both in the software device driver for the network card, as well as on firmware or specialized chipsets. These perform data link functions such as adding a packet header to prepare it for transmission, then actually transmit the frame over a physical medium. The TCP/IP model includes specifications of translating the network addressing methods used in the Internet Protocol to link layer addresses, such as Media Access Control (MAC) addresses. All other aspects below that level, however, are implicitly assumed to exist in the link layer, but are not explicitly defined.

This is also the layer where packets may be selected to be sent over a virtual private network or other networking tunnel. In this scenario, the link layer data may be considered application data which traverses another instantiation of the IP stack for transmission or reception over another IP connection. Such a connection, or virtual link, may be established with a transport protocol or even an application scope protocol that serves as a tunnel in the link layer of the protocol stack. Thus, the TCP/IP model does not dictate a strict hierarchical encapsulation sequence.

The TCP/IP model's link layer corresponds to the Open Systems Interconnection (OSI) model physical and data link layers, layers one and two of the OSI model.

The internet layer has the responsibility of sending packets across potentially multiple networks. Internetworking requires sending data from the source network to the destination network. This process is called routing.

The Internet Protocol performs two basic functions:

The internet layer is not only agnostic of data structures at the transport layer, but it also does not distinguish between operation of the various transport layer protocols. IP carries data for a variety of different upper layer protocols. These protocols are each identified by a unique protocol number: for example, Internet Control Message Protocol (ICMP) and Internet Group Management Protocol (IGMP) are protocols 1 and 2, respectively.

Some of the protocols carried by IP, such as ICMP which is used to transmit diagnostic information, and IGMP which is used to manage IP Multicast data, are layered on top of IP but perform internetworking functions. This illustrates the differences in the architecture of the TCP/IP stack of the Internet and the OSI model. The TCP/IP model's internet layer corresponds to layer three of the Open Systems Interconnection (OSI) model, where it is referred to as the network layer.

The internet layer provides an unreliable datagram transmission facility between hosts located on potentially different IP networks by forwarding the transport layer datagrams to an appropriate next-hop router for further relaying to its destination. With this functionality, the internet layer makes possible internetworking, the interworking of different IP networks, and it essentially establishes the Internet. The Internet Protocol is the principal component of the internet layer, and it defines two addressing systems to identify network hosts' computers, and to locate them on the network. The original address system of the ARPANET and its successor, the Internet, is Internet Protocol version 4 (IPv4). It uses a 32-bit IP address and is therefore capable of identifying approximately four billion hosts. This limitation was eliminated in 1998 by the standardization of Internet Protocol version 6 (IPv6) which uses 128-bit addresses. IPv6 production implementations emerged in approximately 2006.

The transport layer establishes basic data channels that applications use for task-specific data exchange. The layer establishes host-to-host connectivity, meaning it provides end-to-end message transfer services that are independent of the structure of user data and the logistics of exchanging information for any particular specific purpose and independent of the underlying network. The protocols in this layer may provide error control, segmentation, flow control, congestion control, and application addressing (port numbers). End-to-end message transmission or connecting applications at the transport layer can be categorized as either connection-oriented, implemented in TCP, or connectionless, implemented in UDP.

For the purpose of providing process-specific transmission channels for applications, the layer establishes the concept of the network port. This is a numbered logical construct allocated specifically for each of the communication channels an application needs. For many types of services, these port numbers have been standardized so that client computers may address specific services of a server computer without the involvement of service announcements or directory services.

Because IP provides only a best effort delivery, some transport layer protocols offer reliability. However, IP can run over a reliable data link protocol such as the High-Level Data Link Control (HDLC).

For example, the TCP is a connection-oriented protocol that addresses numerous reliability issues in providing a reliable byte stream:

The newer Stream Control Transmission Protocol (SCTP) is also a reliable, connection-oriented transport mechanism. It is message-stream-oriented—not byte-stream-oriented like TCP—and provides multiple streams multiplexed over a single connection. It also provides multi-homing support, in which a connection end can be represented by multiple IP addresses (representing multiple physical interfaces), such that if one fails, the connection is not interrupted. It was developed initially for telephony applications (to transport SS7 over IP), but can also be used for other applications.

The User Datagram Protocol is a connectionless datagram protocol. Like IP, it is a best effort, "unreliable" protocol. Reliability is addressed through error detection using a weak checksum algorithm. UDP is typically used for applications such as streaming media (audio, video, Voice over IP etc.) where on-time arrival is more important than reliability, or for simple query/response applications like DNS lookups, where the overhead of setting up a reliable connection is disproportionately large. Real-time Transport Protocol (RTP) is a datagram protocol that is designed for real-time data such as streaming audio and video.

The applications at any given network address are distinguished by their TCP or UDP port. By convention certain "well known ports" are associated with specific applications.

The TCP/IP model's transport or host-to-host layer corresponds roughly to the fourth layer in the Open Systems Interconnection (OSI) model, also called the transport layer.

The application layer includes the protocols used by most applications for providing user services or exchanging application data over the network connections established by the lower level protocols. This may include some basic network support services such as protocols for routing and host configuration. Examples of application layer protocols include the Hypertext Transfer Protocol (HTTP), the File Transfer Protocol (FTP), the Simple Mail Transfer Protocol (SMTP), and the Dynamic Host Configuration Protocol (DHCP). Data coded according to application layer protocols are encapsulated into transport layer protocol units (such as TCP or UDP messages), which in turn use lower layer protocols to effect actual data transfer.

The TCP/IP model does not consider the specifics of formatting and presenting data, and does not define additional layers between the application and transport layers as in the OSI model (presentation and session layers). Such functions are the realm of libraries and application programming interfaces.

Application layer protocols generally treat the transport layer (and lower) protocols as black boxes which provide a stable network connection across which to communicate, although the applications are usually aware of key qualities of the transport layer connection such as the end point IP addresses and port numbers. Application layer protocols are often associated with particular client-server applications, and common services have "well-known" port numbers reserved by the Internet Assigned Numbers Authority (IANA). For example, the HyperText Transfer Protocol uses server port 80 and Telnet uses server port 23. Clients connecting to a service usually use ephemeral ports, i.e., port numbers assigned only for the duration of the transaction at random or from a specific range configured in the application.

The transport layer and lower-level layers are unconcerned with the specifics of application layer protocols. Routers and switches do not typically examine the encapsulated traffic, rather they just provide a conduit for it. However, some firewall and bandwidth throttling applications must interpret application data. An example is the Resource Reservation Protocol (RSVP). It is also sometimes necessary for network address translator (NAT) traversal to consider the application payload.

The application layer in the TCP/IP model is often compared as equivalent to a combination of the fifth (Session), sixth (Presentation), and the seventh (Application) layers of the Open Systems Interconnection (OSI) model.

Furthermore, the TCP/IP reference model distinguishes between "user protocols" and "support protocols". Support protocols provide services to a system. User protocols are used for actual user applications. For example, FTP is a user protocol and DNS is a support protocol.

The following table shows various networking models. The number of layers varies between three and seven.

Some of the networking models are from textbooks, which are secondary sources that may conflict with the intent of RFC 1122 and other IETF primary sources.

The three top layers in the OSI model, i.e. the application layer, the presentation layer and the session layer, are not distinguished separately in the TCP/IP model which only has an application layer above the transport layer. While some pure OSI protocol applications, such as X.400, also combined them, there is no requirement that a TCP/IP protocol stack must impose monolithic architecture above the transport layer. For example, the NFS application protocol runs over the eXternal Data Representation (XDR) presentation protocol, which, in turn, runs over a protocol called Remote Procedure Call (RPC). RPC provides reliable record transmission, so it can safely use the best-effort UDP transport.

Different authors have interpreted the TCP/IP model differently, and disagree whether the link layer, or the entire TCP/IP model, covers OSI layer 1 (physical layer) issues, or whether a hardware layer is assumed below the link layer.

Several authors have attempted to incorporate the OSI model's layers 1 and 2 into the TCP/IP model, since these are commonly referred to in modern standards (for example, by IEEE and ITU). This often results in a model with five layers, where the link layer or network access layer is split into the OSI model's layers 1 and 2.

The IETF protocol development effort is not concerned with strict layering. Some of its protocols may not fit cleanly into the OSI model, although RFCs sometimes refer to it and often use the old OSI layer numbers. The IETF has repeatedly stated that Internet protocol and architecture development is not intended to be OSI-compliant. RFC 3439, addressing Internet architecture, contains a section entitled: "Layering Considered Harmful".

For example, the session and presentation layers of the OSI suite are considered to be included to the application layer of the TCP/IP suite. The functionality of the session layer can be found in protocols like HTTP and SMTP and is more evident in protocols like Telnet and the Session Initiation Protocol (SIP). Session layer functionality is also realized with the port numbering of the TCP and UDP protocols, which cover the transport layer in the TCP/IP suite. Functions of the presentation layer are realized in the TCP/IP applications with the MIME standard in data exchange.

Conflicts are apparent also in the original OSI model, ISO 7498, when not considering the annexes to this model, e.g., the ISO 7498/4 Management Framework, or the ISO 8648 Internal Organization of the Network layer (IONL). When the IONL and Management Framework documents are considered, the ICMP and IGMP are defined as layer management protocols for the network layer. In like manner, the IONL provides a structure for "subnetwork dependent convergence facilities" such as ARP and RARP.

IETF protocols can be encapsulated recursively, as demonstrated by tunneling protocols such as Generic Routing Encapsulation (GRE). GRE uses the same mechanism that OSI uses for tunneling at the network layer.

The Internet protocol suite does not presume any specific hardware or software environment. It only requires that hardware and a software layer exists that is capable of sending and receiving packets on a computer network. As a result, the suite has been implemented on essentially every computing platform. A minimal implementation of TCP/IP includes the following: Internet Protocol (IP), Address Resolution Protocol (ARP), Internet Control Message Protocol (ICMP), Transmission Control Protocol (TCP), User Datagram Protocol (UDP), and Internet Group Management Protocol (IGMP). In addition to IP, ICMP, TCP, UDP, Internet Protocol version 6 requires Neighbor Discovery Protocol (NDP), ICMPv6, and IGMPv6 and is often accompanied by an integrated IPSec security layer.

Application programmers are typically concerned only with interfaces in the application layer and often also in the transport layer, while the layers below are services provided by the TCP/IP stack in the operating system. Most IP implementations are accessible to programmers through sockets and APIs.

Unique implementations include Lightweight TCP/IP, an open source stack designed for embedded systems, and KA9Q NOS, a stack and associated protocols for amateur packet radio systems and personal computers connected via serial lines.

Microcontroller firmware in the network adapter typically handles link issues, supported by driver software in the operating system. Non-programmable analog and digital electronics are normally in charge of the physical components below the link layer, typically using an application-specific integrated circuit (ASIC) chipset for each network interface or other physical standard. High-performance routers are to a large extent based on fast non-programmable digital electronics, carrying out link level switching.





</doc>
<doc id="15477" url="https://en.wikipedia.org/wiki?curid=15477" title="Ibn al-Shaykh al-Libi">
Ibn al-Shaykh al-Libi

Ibn al-Shaykh al-Libi (; "Ḁbnʋ ălŞɑỉƈ alLibi"; born Ali Mohamed Abdul Aziz al-Fakheri, 1963 – May 10, 2009) was a Libyan national captured in Afghanistan in November 2001 after the fall of the Taliban; he was interrogated by the American and Egyptian forces. The information he gave under torture to Egyptian authorities was cited by the George W. Bush Administration in the months preceding its 2003 invasion of Iraq as evidence of a connection between Saddam Hussein and al-Qaeda. That information was frequently repeated by members of the Bush Administration, although reports from both the Central Intelligence Agency (CIA) and the Defense Intelligence Agency (DIA) strongly questioned its credibility, suggesting that al-Libi was "intentionally misleading" interrogators.

In 2006, the United States transferred al-Libi to Libya, where he was imprisoned by the government. He was reported to have tuberculosis. On May 19, 2009, the government reported that he had recently committed suicide in prison. Human Rights Watch, whose representatives had recently visited him, called for an investigation into the circumstances of his death; "The New York Times" reported that Ayman al-Zawahiri had asserted that Libya had tortured al-Libi to death.

In Afghanistan, al-Libi led the Al Khaldan training camp, where Zacarias Moussaoui and Ahmed Ressam trained for attacks in the United States. An associate of Abu Zubaydah, al-Libi had his assets frozen by the U.S. government following the September 11 attacks; it published a list of terrorists on September 26, 2002 who were covered by this restriction.

The Uyghur Turkistan Islamic Party's "Islamic Turkistan" magazine in its 5th edition published an obituary of its member Turghun (Ibn Umar al Turkistani) speaking of his time training at the Al Khaldan training camp and his meeting with Ibn al-Shaykh al-Libi. The Uyghurs in Afghanistan fought against the American bombing and the Northern Alliance after the September 11, 2001 attacks. Ibn Umar died fighting against Americans at the Qalai Jangi prison riot.

Al-Libi was captured by Pakistani officials in November 2001, as he attempted to flee Afghanistan following the collapse of the Taliban after the 2001 U.S. invasion of Afghanistan, and was transferred to the US military in January 2002.

Department of Defense spokesmen used to routinely described the Khaldan training camp as an al-Qaeda training camp, and Al-Libi and Abu Zubaydah as senior members of al-Qaeda. But, during testimony at their Combatant Status Review Tribunals, several Guantanamo captives, including Zubaydah, described the Khaldan camp as having been run by a rival jihadist organizationone that did not support attacking civilians.

Al-Libi was turned over to the FBI and held at Bagram Air Base. When talking to the FBI interrogators Russell Fincher and Marty Mahon, he seemed "genuinely friendly" and spoke chiefly in English, calling for a translator only when necessary. He seemed to bond with Fincher, a devout Christian, and the two prayed together and discussed religion at length.

Al-Libi told the interrogators details about Richard Reid, a British citizen who had joined al-Qaeda and trained to carry out a suicide bombing of an airliner, which he unsuccessfully attempted on December 22, 2001. Al-Libi agreed to continue cooperating if the United States would allow his wife and her family to emigrate, while he was prosecuted within the American legal system.

The CIA asked President Bush for permission to take al-Libi into their own custody and rendition him to a foreign country for more "tough guy" questioning, and were granted permission. They "simply came and took al-Libi away from the FBI." One CIA officer was heard telling their new prisoner that "You know where you are going. Before you get there, I am going to find your mother and fuck her".

In the second week of January 2002, al-Libi was flown to the USS "Bataan" in the northern Arabian Sea, a ship being used to hold eight other notable prisoners, including John Walker Lindh. He was subsequently transferred to Egyptian interrogators.

According to "The Washington Post",

On September 15, 2002, "Time" published an article that detailed the CIA interrogations of Omar al-Faruq. It said,

On Sept. 9, according to a secret CIA summary of the interview, al-Faruq confessed that he was, in fact, al-Qaeda's senior representative in Southeast Asia. Then came an even more shocking confession: according to the CIA document, al-Faruq said two senior al-Qaeda officials, Abu Zubaydah and Ibn al-Shaykh al-Libi, had ordered him to 'plan large-scale attacks against U.S. interests in Indonesia, Malaysia, (the) Philippines, Singapore, Thailand, Taiwan, Vietnam and Cambodia.'

Al-Libi has been identified as a principal source of faulty prewar intelligence regarding chemical weapons training between Iraq and al-Qaeda that was used by the Bush Administration to justify the invasion of Iraq. Specifically, he told interrogators that Iraq provided training to al-Qaeda in the area of "chemical and biological weapons". In Cincinnati in October 2002, Bush informed the public: "Iraq has trained al Qaeda members in bomb making and poisons and gases."

This claim was repeated several times in the run-up to the war, including in then-Secretary of State Colin Powell's speech to the U.N Security Council on February 5, 2003, which concluded with a long recitation of the information provided by al-Libi. Powell's speech was made less than a month after a then-classified CIA report concluded that the information provided by al-Libi was unreliable, and about a year after a DIA report concluded the same thing.

Al-Libi recanted these claims in January 2004 after U.S. interrogators presented "new evidence from other detainees that cast doubt on his claims", according to "Newsweek". The DIA concluded in February 2002 that al-Libi deliberately misled interrogators, in what the CIA called an "attempt to exaggerate his importance". Some speculate that his reason for giving disinformation was in order to draw the U.S. into an attack on Iraq—Islam's "weakest" state; a remark attributed to al-Libi—which al-Qaeda believes will lead to a global jihad. Others, including al-Libi himself, have insisted that he gave false information due to the use of torture (so-called "enhanced interrogation techniques").

An article published in the November 5, 2005 "The New York Times" quoted two paragraphs of a Defense Intelligence Agency report, declassified upon request by Senator Carl Levin, that expressed doubts about the results of al-Libi's interrogation in February 2002.

Al-Libi told a foreign intelligence service that:

Iraq — acting on the request of al-Qa'ida militant Abu Abdullah, who was Muhammad Atif's emissary — agreed to provide unspecified chemical or biological weapons training for two al-Qa'ida associates beginning in December 2000. The two individuals departed for Iraq but did not return, so al-Libi was not in a position to know if any training had taken place. 

The September 2002 version of "Iraqi Support for Terrorism" stated that al-Libi said Iraq had "provided" chemical and biological weapons training for two al-Qaeda associates in 2000, but also stated that al-Libi "did not know the results of the training."

The 2006 Senate Report on Pre-war Intelligence on Iraq stated that "Although DIA coordinated on CIA's "Iraqi Support for Terrorism" paper, DIA analysis preceding that assessment was more skeptical of the al-Libi reporting." In July 2002, DIA assessed "It is plausible al-Qa'ida attempted to obtain CB assistance from Iraq and Ibn al-Shaykh is sufficiently senior to have access to such sensitive information. However, Ibn al-Shaykh's information lacks details concerning the individual Iraqis involved, the specific CB materials associated with the assistance and the location where the alleged training occurred. The information is also second hand, and not derived from Ibn al-Shaykh's personal experience."

The Senate report also states "According to al-Libi, after his decision to fabricate information for debriefers, he 'lied about being a member of al-Qa'ida. Although he considered himself close to, but not a member of, al-Qa'ida, he knew enough about the senior members, organization and operations to claim to be a member.'"

On September 8, 2006, the United States Senate Select Committee on Intelligence released "Phase II" of its report on prewar intelligence on Iraq. Conclusion 3 of the report states the following:

On June 11, 2008 "Newsweek" published an account of material from a "previously undisclosed CIA report written in the summer of 2002". The article reported that on August 7, 2002 CIA analysts had drafted a high-level report that expressed serious doubts about the information flowing from al-Libi's interrogation. The information that al-Libi acknowledged being a member of al-Qaeda's executive council was not supported by other sources. According to al-Libi, in Egypt he was locked in a tiny box less than 20 inches high and held for 17 hours and after being let out he was thrown to the floor and punched for 15 minutes. According to CIA operational cables, only then did he tell his "fabricated" story about al-Qaeda members being dispatched to Iraq.

In November 2006, a Moroccan using the pseudonym Omar Nasiri, having infiltrated al-Qaeda in the 1990s, wrote the book, "". In the book, Nasiri claims that al-Libi deliberately planted information to encourage the U.S. to invade Iraq. In an interview with BBC2's "Newsnight", Nasiri said Libi "needed the conflict in Iraq because months before I heard him telling us when a question was asked in the mosque after the prayer in the evening, where is the best country to fight the jihad?" Nasiri said that Libi had identified Iraq as the "weakest" Muslim country. He suggested to "Newsnight" that al-Libi wanted to overthrow Saddam and use Iraq as a jihadist base. Nasiri describes al-Libi as one of the leaders at the Afghan camp, and characterizes him as "brilliant in every way." He said that learning how to withstand interrogations and supply false information was a key part of the training in the camps. Al-Libi "knew what his interrogators wanted, and he was happy to give it to them. He wanted to see Saddam toppled even more than the Americans did."

In April 2007 former Director of Central Intelligence George Tenet released his memoir titled "". With regard to al-Libi, Tenet writes the following:

In 2006 the Bush Administration announced that it was transferring high-value al-Qaeda detainees from CIA secret prisons so they could be put on trial by military commissions.
But the Administration was conspicuously silent about al-Libi. In December 2014, it was revealed that he had been transferred to the Guantanamo Bay detention camp in 2003 and transferred to Morocco on March 27, 2004.

Noman Benotman, a former Mujahideen who knew Libi, told "Newsweek" that during a recent trip to Tripoli, he met with a senior Libyan government official who confirmed to him that Libi had been transferred to Libya and was being held in prison there. He was suffering from tuberculosis.

On May 10, 2009, the English language edition of the Libyan newspaper "Ennahar" reported that the government said that Al-Libi had been repatriated to Libyan custody in 2006, and had recently committed suicide by hanging. It attributed the information to another newspaper, "Oea". "Ennahar" reported Al-Libi's real name was Ali Mohamed Abdul Aziz Al-Fakheri. It stated he was 46 years old, and had been allowed visits with international human rights workers from Human Rights Watch. The story was widely reported by other media outlets.

Al-Libi had been visited in April 2009 by a team from Human Rights Watch. His sudden death so soon after this visit has led human rights organisations and Islamic groups to question whether it was truly a suicide. Clive Stafford Smith, Legal Director of the UK branch of the human rights group Reprieve, said, "We are told that al-Libi committed suicide in his Libyan prison. If this is true it would be because of his torture and abuse. If false, it may reflect a desire to silence one of the greatest embarrassments to the Bush administration." Hafed Al-Ghwell, a Libya expert and director of communications at the Dubai campus of Harvard's Kennedy School of Government, commented,
This is a regime with a long history of killing people in jail and then claiming it was suicide. My guess is Libya has seen the winds of change in America and wanted to bury this man before international organisations start demanding access to him. 
On June 19, 2009, Andy Worthington published new information on al-Libi's death. Worthington gave a detailed timeline of Al Libi's last years.

The head of the Washington office of Human Rights Watch said al-Libi was "Exhibit A" in hearings on the relationship between pre-Iraq War false intelligence and torture. Confirmation of al-Libi's location came two weeks prior to his death. An independent investigation of his death has been requested by Human Rights Watch.

On October 4, 2009 the "Reuters" reported that Ayman Al Zawahiri, the head of al-Qaeda, had asserted that Libya had caused al-Libi's death through torture.




</doc>
<doc id="15478" url="https://en.wikipedia.org/wiki?curid=15478" title="IDF">
IDF

IDF or idf may refer to:






</doc>
<doc id="15487" url="https://en.wikipedia.org/wiki?curid=15487" title="International Red Cross and Red Crescent Movement">
International Red Cross and Red Crescent Movement

The International Red Cross and Red Crescent Movement is an international humanitarian movement with approximately 17 million volunteers, members and staff worldwide which was founded to protect human life and health, to ensure respect for all human beings, and to prevent and alleviate human suffering.

The movement consists of several distinct organizations that are legally independent from each other, but are united within the movement through common basic principles, objectives, symbols, statutes and governing organisations. The movement's parts are:


Until the middle of the 19th century, there were no organized and/or well-established army nursing systems for casualties and no safe and protected institutions to accommodate and treat those who were wounded on the battlefield. A devout Reformed Christian, the Swiss businessman Jean-Henri Dunant, in June 1859, traveled to Italy to meet French emperor Napoléon III with the intention of discussing difficulties in conducting business in Algeria, at that time occupied by France. He arrived in the small town of Solferino on the evening of 24 June after the Battle of Solferino, an engagement in the Austro-Sardinian War. In a single day, about 40,000 soldiers on both sides died or were left wounded on the field. Jean-Henri Dunant was shocked by the terrible aftermath of the battle, the suffering of the wounded soldiers, and the near-total lack of medical attendance and basic care. He completely abandoned the original intent of his trip and for several days he devoted himself to helping with the treatment and care for the wounded. He took point in organizing an overwhelming level of relief assistance with the local villagers to aid without discrimination.
Back in his home in Geneva, he decided to write a book entitled "A Memory of Solferino" which he published using his own money in 1862. He sent copies of the book to leading political and military figures throughout Europe, and people he thought could help him make a change. In addition to penning a vivid description of his experiences in Solferino in 1859, he explicitly advocated the formation of national voluntary relief organizations to help nurse wounded soldiers in the case of war, an idea that was inspired by Christian teaching regarding social responsibility, as well as his experience after the battlefield of Solferino. In addition, he called for the development of an international treaty to guarantee the protection of medics and field hospitals for soldiers wounded on the battlefield.

In 1863, Gustave Moynier, a Geneva lawyer and president of the Geneva Society for Public Welfare, received a copy of Dunant's book and introduced it for discussion at a meeting of that society. As a result of this initial discussion the society established an investigatory commission to examine the feasibility of Dunant's suggestions and eventually to organize an international conference about their possible implementation. The members of this committee, which has subsequently been referred to as the "Committee of the Five," aside from Dunant and Moynier were physician Louis Appia, who had significant experience working as a field surgeon; Appia's friend and colleague Théodore Maunoir, from the Geneva Hygiene and Health Commission; and Guillaume-Henri Dufour, a Swiss Army general of great renown. Eight days later, the five men decided to rename the committee to the "International Committee for Relief to the Wounded". In October (26–29) 1863, the international conference organized by the committee was held in Geneva to develop possible measures to improve medical services on the battlefield. The conference was attended by 36 individuals: eighteen official delegates from national governments, six delegates from other non-governmental organizations, seven non-official foreign delegates, and the five members of the International Committee. The states and kingdoms represented by official delegates were: Austrian Empire, Grand Duchy of Baden, Kingdom of Bavaria, Second French Empire, Kingdom of Hanover, Grand Duchy of Hesse, Kingdom of Italy, Kingdom of the Netherlands, Kingdom of Prussia, Russian Empire, Kingdom of Saxony, Spanish Empire, United Kingdoms of Sweden and Norway, and United Kingdom of Great Britain and Ireland.
Among the proposals written in the final resolutions of the conference, adopted on 29 October 1863, were:


Only one year later, the Swiss government invited the governments of all European countries, as well as the United States, Brazil, and Mexico, to attend an official diplomatic conference. Sixteen countries sent a total of twenty-six delegates to Geneva. On 22 August 1864, the conference adopted the first Geneva Convention "for the Amelioration of the Condition of the Wounded in Armies in the Field". Representatives of 12 states and kingdoms signed the convention:

The convention contained ten articles, establishing for the first time legally binding rules guaranteeing neutrality and protection for wounded soldiers, field medical personnel, and specific humanitarian institutions in an armed conflict. 

Directly following the establishment of the Geneva Convention, the first national societies were founded in Belgium, Denmark, France, Oldenburg, Prussia, Spain, and Württemberg. Also in 1864, Louis Appia and Charles van de Velde, a captain of the Dutch Army, became the first independent and neutral delegates to work under the symbol of the Red Cross in an armed conflict. Three years later in 1867, the first International Conference of National Aid Societies for the Nursing of the War Wounded was convened.

Also in 1867, Jean-Henri Dunant was forced to declare bankruptcy due to business failures in Algeria, partly because he had neglected his business interests during his tireless activities for the International Committee. Controversy surrounding Dunant's business dealings and the resulting negative public opinion, combined with an ongoing conflict with Gustave Moynier, led to Dunant's expulsion from his position as a member and secretary. He was charged with fraudulent bankruptcy and a warrant for his arrest was issued. Thus, he was forced to leave Geneva and never returned to his home city.

In the following years, national societies were founded in nearly every country in Europe. The project resonated well with patriotic sentiments that were on the rise in the late-nineteenth-century, and national societies were often encouraged as signifiers of national moral superiority. In 1876, the committee adopted the name "International Committee of the Red Cross" (ICRC), which is still its official designation today. Five years later, the American Red Cross was founded through the efforts of Clara Barton. More and more countries signed the Geneva Convention and began to respect it in practice during armed conflicts. In a rather short period of time, the Red Cross gained huge momentum as an internationally respected movement, and the national societies became increasingly popular as a venue for volunteer work.

When the first Nobel Peace Prize was awarded in 1901, the Norwegian Nobel Committee opted to give it jointly to Jean-Henri Dunant and Frédéric Passy, a leading international pacifist. More significant than the honor of the prize itself, this prize marked the overdue rehabilitation of Jean-Henri Dunant and represented a tribute to his key role in the formation of the Red Cross. Dunant died nine years later in the small Swiss health resort of Heiden. Only two months earlier his long-standing adversary Gustave Moynier had also died, leaving a mark in the history of the Committee as its longest-serving president ever.

In 1906, the 1864 Geneva Convention was revised for the first time. One year later, the Hague Convention X, adopted at the Second International Peace Conference in The Hague, extended the scope of the Geneva Convention to naval warfare. Shortly before the beginning of the First World War in 1914, 50 years after the foundation of the ICRC and the adoption of the first Geneva Convention, there were already 45 national relief societies throughout the world. The movement had extended itself beyond Europe and North America to Central and South America (Argentina, Brazil, Chile, Cuba, Mexico, Peru, El Salvador, Uruguay, Venezuela), Asia (the Republic of China, Japan, Korea, Siam), and Africa (Union of South Africa).

With the outbreak of World War I, the ICRC found itself confronted with enormous challenges that it could handle only by working closely with the national Red Cross societies. Red Cross nurses from around the world, including the United States and Japan, came to support the medical services of the armed forces of the European countries involved in the war. On 15 August 1914, immediately after the start of the war, the ICRC set up its International Prisoners-of-War (POW) Agency, which had about 1,200 mostly volunteer staff members by the end of 1914. By the end of the war, the Agency had transferred about 20 million letters and messages, 1.9 million parcels, and about 18 million Swiss francs in monetary donations to POWs of all affected countries. Furthermore, due to the intervention of the Agency, about 200,000 prisoners were exchanged between the warring parties, released from captivity and returned to their home country. The organizational card index of the Agency accumulated about 7 million records from 1914 to 1923. The card index led to the identification of about 2 million POWs and the ability to contact their families. The complete index is on loan today from the ICRC to the International Red Cross and Red Crescent Museum in Geneva. The right to access the index is still strictly restricted to the ICRC.

During the entire war, the ICRC monitored warring parties’ compliance with the Geneva Conventions of the 1907 revision and forwarded complaints about violations to the respective country. When chemical weapons were used in this war for the first time in history, the ICRC vigorously protested against this new type of warfare. Even without having a mandate from the Geneva Conventions, the ICRC tried to ameliorate the suffering of civil populations. In territories that were officially designated as "occupied territories", the ICRC could assist the civilian population on the basis of the Hague Convention's "Laws and Customs of War on Land" of 1907. This convention was also the legal basis for the ICRC's work for prisoners of war. In addition to the work of the International Prisoner-of-War Agency as described above this included inspection visits to POW camps. A total of 524 camps throughout Europe were visited by 41 delegates from the ICRC until the end of the war.

Between 1916 and 1918, the ICRC published a number of postcards with scenes from the POW camps. The pictures showed the prisoners in day-to-day activities such as the distribution of letters from home. The intention of the ICRC was to provide the families of the prisoners with some hope and solace and to alleviate their uncertainties about the fate of their loved ones. After the end of the war, between 1920 and 1922, the ICRC organized the return of about 500,000 prisoners to their home countries. In 1920, the task of repatriation was handed over to the newly founded League of Nations, which appointed the Norwegian diplomat and scientist Fridtjof Nansen as its "High Commissioner for Repatriation of the War Prisoners". His legal mandate was later extended to support and care for war refugees and displaced persons when his office became that of the League of Nations "High Commissioner for Refugees". Nansen, who invented the Nansen passport for stateless refugees and was awarded the Nobel Peace Prize in 1922, appointed two delegates from the ICRC as his deputies.

A year before the end of the war, the ICRC received the 1917 Nobel Peace Prize for its outstanding wartime work. It was the only Nobel Peace Prize awarded in the period from 1914 to 1918. In 1923, the International Committee of the Red Cross adopted a change in its policy regarding the selection of new members. Until then, only citizens from the city of Geneva could serve in the Committee. This limitation was expanded to include Swiss citizens. As a direct consequence of World War I, an treaty was adopted in 1925 which outlawed the use of suffocating or poisonous gases and biological agents as weapons. Four years later, the original Convention was revised and the second Geneva Convention "relative to the Treatment of Prisoners of War" was established. The events of World War I and the respective activities of the ICRC significantly increased the reputation and authority of the Committee among the international community and led to an extension of its competencies.

As early as in 1934, a draft proposal for an additional convention for the protection of the civil population in occupied territories during an armed conflict was adopted by the International Red Cross Conference. Unfortunately, most governments had little interest in implementing this convention, and it was thus prevented from entering into force before the beginning of World War II.

The legal basis of the work of the ICRC during World War II were the Geneva Conventions in their 1929 revision. The activities of the Committee were similar to those during World War I: visiting and monitoring POW camps, organizing relief assistance for civilian populations, and administering the exchange of messages regarding prisoners and missing persons. By the end of the war, 179 delegates had conducted 12,750 visits to POW camps in 41 countries. The Central Information Agency on Prisoners-of-War ("Agence centrale des prisonniers de guerre") had a staff of 3,000, the card index tracking prisoners contained 45 million cards, and 120 million messages were exchanged by the Agency. One major obstacle was that the Nazi-controlled German Red Cross refused to cooperate with the Geneva statutes including blatant violations such as the deportation of Jews from Germany and the mass murders conducted in the Nazi concentration camps. Moreover, two other main parties to the conflict, the Soviet Union and Japan, were not party to the 1929 Geneva Conventions and were not legally required to follow the rules of the conventions.

During the war, the ICRC was unable to obtain an agreement with Nazi Germany about the treatment of detainees in concentration camps, and it eventually abandoned applying pressure in order to avoid disrupting its work with POWs. The ICRC was also unable to obtain a response to reliable information about the extermination camps and the mass killing of European Jews, Roma, et al. After November 1943, the ICRC achieved permission to send parcels to concentration camp detainees with known names and locations. Because the notices of receipt for these parcels were often signed by other inmates, the ICRC managed to register the identities of about 105,000 detainees in the concentration camps and delivered about 1.1 million parcels, primarily to the camps Dachau, Buchenwald, Ravensbrück, and Sachsenhausen.

It is known that Dr Maurice Rossel during World War II had been sent to Berlin as a delegate of the International Red Cross, as such he visited Theresienstadt in 1944. Claude Lanzmann recorded his experiences in 1979, producing a documentary entitled "Visitor from the living".

On 12 March 1945, ICRC president Jacob Burckhardt received a message from SS General Ernst Kaltenbrunner accepting the ICRC's demand to allow delegates to visit the concentration camps. This agreement was bound by the condition that these delegates would have to stay in the camps until the end of the war. Ten delegates, among them Louis Haefliger (Camp Mauthausen), Paul Dunant (Camp Theresienstadt) and Victor Maurer (Camp Dachau), accepted the assignment and visited the camps. Louis Haefliger prevented the forceful eviction or blasting of Mauthausen-Gusen by alerting American troops, thereby saving the lives of about 60,000 inmates. 

Another example of great humanitarian spirit was Friedrich Born (1903–1963), an ICRC delegate in Budapest who saved the lives of about 11,000 to 15,000 Jewish people in Hungary. Marcel Junod (1904–1961), a physician from Geneva, was another famous delegate during the Second World War. An account of his experiences, which included being one of the first foreigners to visit Hiroshima after the atomic bomb was dropped, can be found in the book "Warrior without Weapons".

In 1944, the ICRC received its second Nobel Peace Prize. As in World War I, it received the only Peace Prize awarded during the main period of war, 1939 to 1945. At the end of the war, the ICRC worked with national Red Cross societies to organize relief assistance to those countries most severely affected. In 1948, the Committee published a report reviewing its war-era activities from 1 September 1939 to 30 June 1947. Since January 1996, the ICRC archive for this period has been open to academic and public research.

On 12 August 1949, further revisions to the existing two Geneva Conventions were adopted. An additional convention "for the Amelioration of the Condition of Wounded, Sick and Shipwrecked Members of Armed Forces at Sea", now called the second Geneva Convention, was brought under the Geneva Convention umbrella as a successor to the 1907 Hague Convention X. The 1929 Geneva convention "relative to the Treatment of Prisoners of War" may have been the second Geneva Convention from a historical point of view (because it was actually formulated in Geneva), but after 1949 it came to be called the third Convention because it came later chronologically than the Hague Convention. Reacting to the experience of World War II, the Fourth Geneva Convention, a new Convention "relative to the Protection of Civilian Persons in Time of War", was established. Also, the additional protocols of 8 June 1977 were intended to make the conventions apply to internal conflicts such as civil wars. Today, the four conventions and their added protocols contain more than 600 articles, a remarkable expansion when compared to the mere 10 articles in the first 1864 convention.

In celebration of its centennial in 1963, the ICRC, together with the League of Red Cross Societies, received its third Nobel Peace Prize. Since 1993, non-Swiss individuals have been allowed to serve as Committee delegates abroad, a task which was previously restricted to Swiss citizens. Indeed, since then, the share of staff without Swiss citizenship has increased to about 35%.

On 16 October 1990, the UN General Assembly decided to grant the ICRC observer status for its assembly sessions and sub-committee meetings, the first observer status given to a private organization. The resolution was jointly proposed by 138 member states and introduced by the Italian ambassador, Vieri Traxler, in memory of the organization's origins in the Battle of Solferino. An agreement with the Swiss government signed on 19 March 1993, affirmed the already long-standing policy of full independence of the Committee from any possible interference by Switzerland. The agreement protects the full sanctity of all ICRC property in Switzerland including its headquarters and archive, grants members and staff legal immunity, exempts the ICRC from all taxes and fees, guarantees the protected and duty-free transfer of goods, services, and money, provides the ICRC with secure communication privileges at the same level as foreign embassies, and simplifies Committee travel in and out of Switzerland.

At the end of the Cold War, the ICRC's work actually became more dangerous. In the 1990s, more delegates lost their lives than at any point in its history, especially when working in local and internal armed conflicts. These incidents often demonstrated a lack of respect for the rules of the Geneva Conventions and their protection symbols. Among the slain delegates were:


ICRC is active in the Afghanistan conflict areas and has set up six physical rehabilitation centers to help land mine victims. Their support extends to the national and international armed forces, civilians and the armed opposition. They regularly visit detainees under the custody of the Afghan government and the international armed forces, but have also occasionally had access since 2009 to people detained by the Taliban. They have provided basic first aid training and aid kits to both the Afghan security forces and Taliban members because, according to an ICRC spokesperson, "ICRC's constitution stipulates that all parties harmed by warfare will be treated as fairly as possible".

In 1919, representatives from the national Red Cross societies of Britain, France, Italy, Japan, and the US came together in Paris to found the "League of Red Cross Societies". The original idea was Henry Davison's, then president of the American Red Cross. This move, led by the American Red Cross, expanded the international activities of the Red Cross movement beyond the strict mission of the ICRC to include relief assistance in response to emergency situations which were not caused by war (such as man-made or natural disasters). The ARC already had great disaster relief mission experience extending back to its foundation.

The formation of the League, as an additional international Red Cross organization alongside the ICRC, was not without controversy for a number of reasons. The ICRC had, to some extent, valid concerns about a possible rivalry between both organizations. The foundation of the League was seen as an attempt to undermine the leadership position of the ICRC within the movement and to gradually transfer most of its tasks and competencies to a multilateral institution. In addition to that, all founding members of the League were national societies from countries of the Entente or from associated partners of the Entente. The original statutes of the League from May 1919 contained further regulations which gave the five founding societies a privileged status and, due to the efforts of Henry P. Davison, the right to permanently exclude the national Red Cross societies from the countries of the Central Powers, namely Germany, Austria, Hungary, Bulgaria and Turkey, and in addition to that the national Red Cross society of Russia. These rules were contrary to the Red Cross principles of universality and equality among all national societies, a situation which furthered the concerns of the ICRC.

The first relief assistance mission organized by the League was an aid mission for the victims of a famine and subsequent typhus epidemic in Poland. Only five years after its foundation, the League had already issued 47 donation appeals for missions in 34 countries, an impressive indication of the need for this type of Red Cross work. The total sum raised by these appeals reached 685 million Swiss francs, which were used to bring emergency supplies to the victims of famines in Russia, Germany, and Albania; earthquakes in Chile, Persia, Japan, Colombia, Ecuador, Costa Rica, and Turkey; and refugee flows in Greece and Turkey. The first large-scale disaster mission of the League came after the 1923 earthquake in Japan which killed about 200,000 people and left countless more wounded and without shelter. Due to the League's coordination, the Red Cross society of Japan received goods from its sister societies reaching a total worth of about $100 million. Another important new field initiated by the League was the creation of youth Red Cross organizations within the national societies.

A joint mission of the ICRC and the League in the Russian Civil War from 1917 to 1922 marked the first time the movement was involved in an internal conflict, although still without an explicit mandate from the Geneva Conventions. The League, with support from more than 25 national societies, organized assistance missions and the distribution of food and other aid goods for civil populations affected by hunger and disease. The ICRC worked with the Russian Red Cross society and later the society of the Soviet Union, constantly emphasizing the ICRC's neutrality. In 1928, the "International Council" was founded to coordinate cooperation between the ICRC and the League, a task which was later taken over by the "Standing Commission". In the same year, a common statute for the movement was adopted for the first time, defining the respective roles of the ICRC and the League within the movement.

During the Abyssinian war between Ethiopia and Italy from 1935 to 1936, the League contributed aid supplies worth about 1.7 million Swiss francs. Because the Italian fascist regime under Benito Mussolini refused any cooperation with the Red Cross, these goods were delivered solely to Ethiopia. During the war, an estimated 29 people lost their lives while being under explicit protection of the Red Cross symbol, most of them due to attacks by the Italian Army. During the Civil War in Spain from 1936 to 1939 the League once again joined forces with the ICRC with the support of 41 national societies. In 1939 on the brink of the Second World War, the League relocated its headquarters from Paris to Geneva to take advantage of Swiss neutrality.

In 1952, the 1928 common statute of the movement was revised for the first time. Also, the period of decolonization from 1960 to 1970 was marked by a huge jump in the number of recognized national Red Cross and Red Crescent societies. By the end of the 1960s, there were more than 100 societies around the world. On December 10, 1963, the Federation and the ICRC received the Nobel Peace Prize. In 1983, the League was renamed to the "League of Red Cross and Red Crescent Societies" to reflect the growing number of national societies operating under the Red Crescent symbol. Three years later, the seven basic principles of the movement as adopted in 1965 were incorporated into its statutes. The name of the League was changed again in 1991 to its current official designation the "International Federation of Red Cross and Red Crescent Societies". In 1997, the ICRC and the IFRC signed the Seville Agreement which further defined the responsibilities of both organizations within the movement. In 2004, the IFRC began its largest mission to date after the tsunami disaster in South Asia. More than 40 national societies have worked with more than 22,000 volunteers to bring relief to the countless victims left without food and shelter and endangered by the risk of epidemics.

Altogether, there are about 97 million people worldwide who serve with the ICRC, the International Federation, and the National Societies, the majority with the latter.

The 1965 International Conference in Vienna adopted seven basic principles which should be shared by all parts of the Movement, and they were added to the official statutes of the Movement in 1986.

At the 20th International Conference in Neue Hofburg, Vienna, from 2–9 October 1965, "proclaimed" seven fundamental principles which are shared by all components of the Movement, and they were added to the official statutes of the Movement in 1986. The durability and universal acceptance is a result of the process through which they came into being in the form they have. Rather than an effort to arrive at agreement, it was an attempt to answer the question of what did they have in common, over the past 100 years, those operations and organisational units that were successful? As a result, the Fundamental Principles of the Red Cross and Red Crescent were not revealed, but "found" – through a deliberate and participative process of discovery.

That makes it even more important to note that the text that appears under each "heading" is an integral part of the Principle in question and not an interpretation that can vary with time and place.

Humanity

The International Red Cross and Red Crescent Movement, born of a desire to bring assistance without discrimination to the wounded on the battlefield, endeavours, in its international and national capacity, to prevent and alleviate human suffering wherever it may be found. Its purpose is to protect life and health and to ensure respect for the human being. It promotes mutual understanding, friendship, cooperation and lasting peace amongst all peoples.

Impartiality

It makes no discrimination as to nationality, race, religious beliefs, class or political opinions. It endeavours to relieve the suffering of individuals, being guided solely by their needs, and to give priority to the most urgent cases of distress.

Neutrality

In order to continue to enjoy the confidence of all, the Movement may not take sides in hostilities or engage at any time in controversies of a political, racial, religious or ideological nature.

Independence

The Movement is independent. The National Societies, while auxiliaries in the humanitarian services of their governments and subject to the laws of their respective countries, must always maintain their autonomy so that they may be able at all times to act in accordance with the principles of the Movement.

Voluntary Service

It is a voluntary relief movement not prompted in any manner by desire for gain. 
Unity

There can be only one Red Cross or one Red Crescent Society in any one country. It must be open to all. It must carry on its humanitarian work throughout its territory.

Universality

The International Red Cross and Red Crescent Movement, in which all Societies have equal status and share equal responsibilities and duties in helping each other, is worldwide.

The International Conference of the Red Cross and Red Crescent, which occurs once every four years, is the highest institutional body of the Movement. It gathers delegations from all of the national societies as well as from the ICRC, the IFRC and the signatory states to the Geneva Conventions. In between the conferences, the Standing Commission of the Red Cross and Red Crescent acts as the supreme body and supervises implementation of and compliance with the resolutions of the conference. In addition, the Standing Commission coordinates the cooperation between the ICRC and the IFRC. It consists of two representatives from the ICRC (including its president), two from the IFRC (including its president), and five individuals who are elected by the International Conference. The Standing Commission convenes every six months on average. Moreover, a convention of the Council of Delegates of the Movement takes place every two years in the course of the conferences of the General Assembly of the International Federation. The Council of Delegates plans and coordinates joint activities for the Movement.

The official mission of the ICRC as an impartial, neutral, and independent organization is to stand for the protection of the life and dignity of victims of international and internal armed conflicts. According to the 1997 Seville Agreement, it is the "Lead Agency" of the Movement in conflicts. The core tasks of the Committee, which are derived from the Geneva Conventions and its own statutes, are the following:


The ICRC is headquartered in the Swiss city of Geneva and has external offices in about 80 countries. It has about 12,000 staff members worldwide, about 800 of them working in its Geneva headquarters, 1,200 expatriates with about half of them serving as delegates managing its international missions and the other half being specialists like doctors, agronomists, engineers or interpreters, and about 10,000 members of individual national societies working on site.

According to Swiss law, the ICRC is defined as a private association. Contrary to popular belief, the ICRC is not a non-governmental organization in the most common sense of the term, nor is it an international organization. As it limits its members (a process called cooptation) to Swiss nationals only, it does not have a policy of open and unrestricted membership for individuals like other legally defined NGOs. The word "international" in its name does not refer to its membership but to the worldwide scope of its activities as defined by the Geneva Conventions. The ICRC has special privileges and legal immunities in many countries, based on national law in these countries or through agreements between the Committee and respective national governments.

According to its statutes it consists of 15 to 25 Swiss-citizen members, which it coopts for a period of four years. There is no limit to the number of terms an individual member can have although a three-quarters majority of all members is required for re-election after the third term.

The leading organs of the ICRC are the Directorate and the Assembly. The Directorate is the executive body of the Committee. It consists of a general director and five directors in the areas of "Operations", "Human Resources", "Resources and Operational Support", "Communication", and "International Law and Cooperation within the Movement". The members of the Directorate are appointed by the Assembly to serve for four years. The Assembly, consisting of all of the members of the Committee, convenes on a regular basis and is responsible for defining aims, guidelines, and strategies and for supervising the financial matters of the Committee. The president of the Assembly is also the president of the Committee as a whole. Furthermore, the Assembly elects a five-member Assembly Council which has the authority to decide on behalf of the full Assembly in some matters. The Council is also responsible for organizing the Assembly meetings and for facilitating communication between the Assembly and the Directorate.

Due to Geneva's location in the French-speaking part of Switzerland, the ICRC usually acts under its French name "Comité international de la Croix-Rouge" (CICR). The official symbol of the ICRC is the Red Cross on white background with the words "COMITE INTERNATIONAL GENEVE" circling the cross.

The 2009 budget of the ICRC amounts to more than 1 billion Swiss francs. Most of that money comes from the States, including Switzerland in its capacity as the depositary state of the Geneva Conventions, from national Red Cross societies, the signatory states of the Geneva Conventions, and from international organizations like the European Union. All payments to the ICRC are voluntary and are received as donations based on two types of appeals issued by the Committee: an annual "Headquarters Appeal" to cover its internal costs and "Emergency Appeals" for its individual missions.

The ICRC is asking donors for more than 1.1 billion Swiss francs to fund its work in 2010. Afghanistan is projected to become the ICRC’s biggest humanitarian operation (at 86 million Swiss francs, an 18% increase over the initial 2009 budget), followed by Iraq (85 million francs) and Sudan (76 million francs). The initial 2010 field budget for medical activities of 132 million francs represents an increase of 12 million francs over 2009.

The IFRC coordinates cooperation between national Red Cross and Red Crescent societies throughout the world and supports the foundation of new national societies in countries where no official society exists. On the international stage, the IFRC organizes and leads relief assistance missions after emergencies such as natural disasters, manmade disasters, epidemics, mass refugee flights, and other emergencies. As per the 1997 Seville Agreement, the IFRC is the Lead Agency of the Movement in any emergency situation which does not take place as part of an armed conflict. The IFRC cooperates with the national societies of those countries affected – each called the "Operating National Society" (ONS) – as well as the national societies of other countries willing to offer assistance – called "Participating National Societies" (PNS). Among the 187 national societies admitted to the General Assembly of the International Federation as full members or observers, about 25–30 regularly work as PNS in other countries. The most active of those are the American Red Cross, the British Red Cross, the German Red Cross, and the Red Cross societies of Sweden and Norway. Another major mission of the IFRC which has gained attention in recent years is its commitment to work towards a codified, worldwide ban on the use of land mines and to bring medical, psychological, and social support for people injured by land mines.

The tasks of the IFRC can therefore be summarized as follows:

The IFRC has its headquarters in Geneva. It also runs five zone offices (Africa, Americas, Asia Pacific, Europe, Middle East-North Africa), 14 permanent regional offices and has about 350 delegates in more than 60 delegations around the world. The legal basis for the work of the IFRC is its constitution. The executive body of the IFRC is a secretariat, led by a secretary general. The secretariat is supported by five divisions including "Programme Services", "Humanitarian values and humanitarian diplomacy", "National Society and Knowledge Development" and "Governance and Management Services".

The highest decision making body of the IFRC is its General Assembly, which convenes every two years with delegates from all of the national societies. Among other tasks, the General Assembly elects the secretary general. Between the convening of General Assemblies, the Governing Board is the leading body of the IFRC. It has the authority to make decisions for the IFRC in a number of areas. The Governing Board consists of the president and the vice presidents of the IFRC, the chairpersons of the Finance and Youth Commissions, and twenty elected representatives from national societies.

The symbol of the IFRC is the combination of the Red Cross (left) and Red Crescent (right) on a white background surrounded by a red rectangular frame.

The main parts of the budget of the IFRC are funded by contributions from the national societies which are members of the IFRC and through revenues from its investments. The exact amount of contributions from each member society is established by the Finance Commission and approved by the General Assembly. Any additional funding, especially for unforeseen expenses for relief assistance missions, is raised by ""appeals"" published by the IFRC and comes for voluntary donations by national societies, governments, other organizations, corporations, and individuals.

National Red Cross and Red Crescent societies exist in nearly every country in the world. Within their home country, they take on the duties and responsibilities of a national relief society as defined by International Humanitarian Law. Within the Movement, the ICRC is responsible for legally recognizing a relief society as an official national Red Cross or Red Crescent society. The exact rules for recognition are defined in the statutes of the Movement. Article 4 of these statutes contains the ""Conditions for recognition of National Societies.""


Once a National Society has been recognized by the ICRC as a component of the International Red Cross and Red Crescent Movement (the Movement), it is in principle admitted to the International Federation of Red Cross and Red Crescent Societies in accordance with the terms defined in the Constitution and Rules of Procedure of the International Federation.

There are today 190 National Societies recognized within the Movement and which are members of the International Federation.

The most recent National Societies to have been recognized within the Movement are the Maldives Red Crescent Society (9 November 2011), the Cyprus Red Cross Society, the South Sudan Red Cross Society (12 November 2013) and, the last, the Tuvalu Red Cross Society (on 1 March 2016).

Despite formal independence regarding its organizational structure and work, each national society is still bound by the laws of its home country. In many countries, national Red Cross and Red Crescent societies enjoy exceptional privileges due to agreements with their governments or specific "Red Cross Laws" granting full independence as required by the International Movement. The duties and responsibilities of a national society as defined by International Humanitarian Law and the statutes of the Movement include humanitarian aid in armed conflicts and emergency crises such as natural disasters through activities such as Restoring Family Links.

Depending on their respective human, technical, financial, and organizational resources, many national societies take on additional humanitarian tasks within their home countries such as blood donation services or acting as civilian Emergency Medical Service (EMS) providers. The ICRC and the International Federation cooperate with the national societies in their international missions, especially with human, material, and financial resources and organizing on-site logistics.

The Red Cross emblem was officially approved in Geneva in 1863.

The Red Cross flag is not to be confused with the Saint George's Cross which is on the flag of England, Barcelona, Georgia, Freiburg im Breisgau, and several other places. In order to avoid this confusion the protected symbol is sometimes referred to as the "Greek Red Cross" (now Hellenic Red Cross); that term is also used in United States law to describe the Red Cross. The red cross of the Saint George cross extends to the edge of the flag, whereas the red cross on the Red Cross flag does not.

The Red Cross flag is the colour-switched version of the Flag of Switzerland. In 1906, to put an end to the argument of the Ottoman Empire that the flag took its roots from Christianity, it was decided to promote officially the idea that the Red Cross flag had been formed by reversing the federal colours of Switzerland, although no clear evidence of this origin had ever been found.

The Red Crescent emblem was first used by ICRC volunteers during the armed conflict between the Ottoman Empire and Russia (1876–1878). The symbol was officially adopted in 1929, and so far 33 states in the Muslim world have recognized it. In common with the official promotion of the red cross symbol as a colour reversal of the Swiss flag (rather than a religious symbol), the red crescent is similarly presented as being derived from a colour reversal of the flag of the Ottoman Empire.

Nowadays, the Red Crescent symbol is used within a few Muslim countries including Turkey, Afghanistan, Pakistan, Malaysia, and Bangladesh.

On 8 December 2005, in response to growing pressure to accommodate Magen David Adom (MDA), Israel's national emergency medical, disaster, ambulance and blood bank service, as a full member of the Red Cross and Red Crescent movement, a new emblem (officially the Third Protocol Emblem, but more commonly known as the Red Crystal) was adopted by an amendment of the Geneva Conventions known as Protocol III.

The Red Lion and Sun Society of Iran was established in 1922 and admitted to the Red Cross and Red Crescent movement in 1923. However, some report the symbol was introduced at Geneva in 1864 as a counter example to the crescent and cross used by two of Iran's rivals, the Ottoman and the Russian empires. Though that claim is inconsistent with the Red Crescent's history, that history also suggests that the Red Lion and Sun, like the Red Crescent, may have been conceived during the 1877–1878 war between Russia and Turkey.

Due to the emblem's association with the Iranian monarchy, the Islamic Republic of Iran replaced the Red Lion and Sun with the Red Crescent in 1980, consistent with two existing Red Cross and Red Crescent symbols. Though the Red Lion and Sun has now fallen into disuse, Iran has in the past reserved the right to take it up again at any time; the Geneva Conventions continue to recognize it as an official emblem, and that status was confirmed by Protocol III in 2005 even as it added the Red Crystal.

For over 50 years, Israel requested the addition of a red Star of David, arguing that since Christian and Muslim emblems were recognized, the corresponding Jewish emblem should be as well. This emblem has been used by Magen David Adom (MDA), or Red Star of David, the national first-aid society of Israel of 1930, but it is not recognized by the Geneva Conventions as a protected symbol. The first use of the ″Magen David Adom″ was during the Anglo Boer War in South Africa (1899–1902) when it was used by the Ambulance Corps founded by Ben Zion Aaron in Johannesburg as a first aid corps to assist the Boer forces. Permission was given by President Paul Kruger of the South African Republic for the Star of David to be used as its insignia, rather than the conventional red cross.

The Red Cross and Red Crescent movement repeatedly rejected Israel's request over the years, stating that the Red Cross and Red Crescent emblems were not meant to represent Christianity and Islam but were colour reversals of the Swiss and the Ottoman flags, and also that if Jews (or another group) were to be given another emblem, there would be no end to the number of religious or other groups claiming an emblem for themselves. They reasoned that a proliferation of red symbols would detract from the original intention of the Red Cross emblem, which was to be a single emblem to mark vehicles and buildings protected on humanitarian grounds.

Certain Arab nations, such as Syria, also protested against the entry of MDA into the Red Cross movement, making consensus impossible for a time.
However, from 2000 to 2006 the American Red Cross withheld its dues (a total of $42 million) to the International Federation of Red Cross and Red Crescent Societies (IFRC) because of IFRC's refusal to admit MDA; this ultimately led to the creation of the Red Crystal emblem and the admission of MDA on June 22, 2006.

The Red Star of David is not recognized as a protected symbol outside Israel; instead the MDA uses the Red Crystal emblem during international operations in order to ensure protection. Depending on the circumstances, it may place the Red Star of David inside the Red Crystal, or use the Red Crystal alone.

The Australian TV network ABC and the indigenous rights group Friends of Peoples Close to Nature released a documentary called "Blood on the Cross" in 1999. It alleged the involvement of the Red Cross with the British and Indonesian military in a massacre in the Southern Highlands of West Papua during the World Wildlife Fund hostage crisis of May 1996, when Western and Indonesian activists were held hostage by separatists.

Following the broadcast of the documentary, the Red Cross announced publicly that it would appoint an individual outside the organization to investigate the allegations made in the film and any responsibility on its part. Piotr Obuchowicz was appointed to investigate the matter. The report categorically states that the Red Cross personnel accused of involvement were proven not to have been present; that a white helicopter was probably used in a military operation, but the helicopter was not a Red Cross helicopter, and must have been painted by one of several military organizations operating in the region at the time. Perhaps the Red Cross logo itself was also used, although no hard evidence was found for this; that this was part of the military operation to free the hostages, but was clearly intended to achieve surprise by deceiving the local people into thinking that a Red Cross helicopter was landing; and that the Red Cross should have responded more quickly and thoroughly to investigate the allegations than it did.






</doc>
<doc id="15489" url="https://en.wikipedia.org/wiki?curid=15489" title="Ira Gershwin">
Ira Gershwin

Ira Gershwin (6 December 1896 17 August 1983) was an American lyricist who collaborated with his younger brother, composer George Gershwin, to create some of the most memorable songs of the 20th century.

With George he wrote more than a dozen Broadway shows, featuring songs such as "I Got Rhythm", "Embraceable You", "The Man I Love" and "Someone to Watch Over Me". He was also responsible, along with DuBose Heyward, for the libretto to George's opera "Porgy and Bess".

The success the Gershwin brothers had with their collaborative works has often overshadowed the creative role that Ira played. His mastery of songwriting continued, however, after the early death of George. He wrote additional hit songs with composers Jerome Kern, Kurt Weill, Harry Warren and Harold Arlen.

His critically acclaimed 1959 book "Lyrics on Several Occasions", an amalgam of autobiography and annotated anthology, is an important source for studying the art of the lyricist in the golden age of American popular song.

Gershwin was born Israel Gershowitz in New York City, the oldest of four children of Morris (Moishe) and Rose Gershovitz (née Rosa Bruskin), who were Russian Jews, born in St Petersburg, who had emigrated to the US in 1891. Ira's siblings were George (Jacob, b. 1898), Arthur (b. 1900) and Frances (b. 1906). Morris changed the family name to "Gershwine" (or alternatively "Gershvin") well before their children rose to fame; it was not spelled "Gershwin" until later. Shy in his youth, Ira spent much of his time at home reading, but from grammar school through college he played a prominent part in several school newspapers and magazines.

He graduated in 1914 from Townsend Harris High School where he met Yip Harburg, with whom he enjoyed a lifelong friendship and a love of Gilbert and Sullivan. He attended the City College of New York but dropped out.

The childhood home of Ira and George Gershwin was in the center of the Yiddish Theater District, on the second floor at 91 Second Avenue, between East 5th Street and East 6th Street. They frequented the local Yiddish theaters.

While George began composing and "plugging" in Tin Pan Alley from the age of 18, Ira worked as a cashier in his father's Turkish baths. It was not until 1921 that Ira became involved in the music business. Alex Aarons signed Ira to write the songs for his next show, "Two Little Girls in Blue", ultimately produced by Abraham Erlanger, along with co-composers Vincent Youmans and Paul Lannin. So as not to appear to trade off George's growing reputation, Ira wrote under the pseudonym "Arthur Francis", after his youngest two siblings. His lyrics were well received, allowing him successfully to enter the show-business world with just one show. Later the same year, the Gershwins collaborated for the first time on a score; this was for "A Dangerous Maid", which played in Atlantic City and on tour.

It was not until 1924 that Ira and George teamed up to write the music for what became their first Broadway hit "Lady, Be Good". Once the brothers joined forces, their combined talents became one of the most influential forces in the history of American Musical Theatre. "When the Gershwins teamed up to write songs for "Lady, Be Good", the American musical found its native idiom." Together, they wrote the music for more than 12 shows and four films. Some of their more famous works include "The Man I Love", "Fascinating Rhythm", "Someone to Watch Over Me", "I Got Rhythm" and "They Can't Take That Away from Me". Their partnership continued until George's sudden death from a brain tumor in 1937. Following his brother's death, Ira waited nearly three years before writing again.

After this temporary retirement, Ira teamed up with accomplished composers such as Jerome Kern ("Cover Girl"); Kurt Weill ("Where Do We Go from Here?"; "Lady in the Dark"); and Harold Arlen (""; "A Star Is Born"). Over the next 14 years, Gershwin continued to write the lyrics for many film scores and a few Broadway shows. But the failure of "Park Avenue" in 1946 (a "smart" show about divorce, co-written with composer Arthur Schwartz) was his farewell to Broadway. As he wrote at the time, "Am reading a couple of stories for possible musicalization (if there is such a word) but I hope I don't like them as I think I deserve a long rest."

In 1947, he took 11 songs George had written but never used, provided them with new lyrics, and incorporated them into the Betty Grable film "The Shocking Miss Pilgrim". He later wrote comic lyrics for Billy Wilder's 1964 movie "Kiss Me, Stupid", although most critics believe his final major work was for the 1954 Judy Garland film "A Star Is Born".

American singer, pianist and musical historian Michael Feinstein worked for Gershwin in the lyricist's latter years, helping him with his archive. Several lost musical treasures were unearthed during this period, and Feinstein performed some of the material. Feinstein's book "The Gershwins and Me: A Personal History in Twelve Songs" about working for Ira, and George and Ira's music was published in 2012.

According to a 1999 story in Vanity Fair, Ira Gershwin’s love for loud music was as great as his wife’s loathing of it. When Debby Boone—daughter-in-law of his neighbor Rosemary Clooney—returned from Japan with one of the first Sony Walkmans (utilizing cassette tape), Clooney gave it to Michael Feinstein to give to Ira, "so he could crank it in his ears, you know. And he said, ‘This is absolutely wonderful!’ And he called his broker and bought Sony stock!"

Three of Ira Gershwin's songs ("They Can't Take That Away From Me" (1937), "Long Ago (And Far Away)" (1944) and "The Man That Got Away" (1954)) were nominated for an Academy Award for Best Original Song, though none won.

Along with George S Kaufman and Morrie Ryskind, he was a recipient of the 1932 Pulitzer Prize for Drama for "Of Thee I Sing".

In 1988 UCLA established The George and Ira Gershwin Lifetime Musical Achievement Award in recognition of the brothers' contribution to music, and for their gift to UCLA of the fight song "Strike Up the Band for UCLA". Recipients include Angela Lansbury (1988), Ray Charles (1991), Mel Tormé (1994), Bernadette Peters (1995), Frank Sinatra (2000), Stevie Wonder (2002), k.d. lang (2003), James Taylor (2004), Babyface (2005), Burt Bacharach (2006), Quincy Jones (2007), Lionel Richie (2008) and Julie Andrews (2009).

Ira Gershwin was a joyous listener to the sounds of the modern world. "He had a sharp eye and ear for the minutiae of living." He noted in a diary: "Heard in a day: An elevator's purr, telephone's ring, telephone's buzz, a baby's moans, a shout of delight, a screech from a 'flat wheel', hoarse honks, a hoarse voice, a tinkle, a match scratch on sandpaper, a deep resounding boom of dynamiting in the impending subway, iron hooks on the gutter."

In 1987, Ira's widow, Leonore, established the Ira Gershwin Literacy Center at University Settlement, a century-old institution at 185 Eldridge Street on the Lower East Side, New York City. The Center is designed to give English-language programs to primarily Hispanic and Chinese Americans. Ira and his younger brother George spent many after-school hours at the Settlement.

The George and Ira Gershwin Collection is at the Library of Congress Music Division. The Edward Jablonski and Lawrence D. Stewart Gershwin Collection at the Harry Ransom Humanities Research Center at the University of Texas at Austin holds a number of Ira's manuscripts and other material.

In 2007, the United States Library of Congress named its Prize for Popular Song after him and his brother George. Recognizing the profound and positive effect of American popular music on the world's culture, the prize will be given annually to a composer or performer whose lifetime contributions exemplify the standard of excellence associated with the Gershwins.

He married Leonore (née Strunsky) in 1926. He died in Beverly Hills, California, on 17 August 1983 at the age of 86. He is interred at Westchester Hills Cemetery, Hastings-on-Hudson, New York. Leonore died in 1991.





</doc>
<doc id="15490" url="https://en.wikipedia.org/wiki?curid=15490" title="Indus River">
Indus River

The Indus River (locally called Sindhū) is one of the longest rivers in Asia. Originating in the Tibetan Plateau in the vicinity of Lake Manasarovar, the river runs a course through the Ladakh region of Jammu and Kashmir, towards Gilgit-Baltistan and the Hindukush ranges, and then flows in a southerly direction along the entire length of Pakistan to merge into the Arabian Sea near the port city of Karachi in Sindh. It is the longest river and national river of Pakistan.

The river has a total drainage area exceeding . Its estimated annual flow stands at around , twice that of the Nile River and three times that of the Tigris and Euphrates rivers combined, making it the twenty-first largest river in the world in terms of annual flow. The Zanskar is its left bank tributary in Ladakh. In the plains, its left bank tributary is the Panjnad which itself has five major tributaries, namely, the Chenab, Jhelum, the Ravi, the Beas, and the Sutlej. Its principal right bank tributaries are the Shyok, the Gilgit, the Kabul, the Gomal, and the Kurram. Beginning in a mountain spring and fed with glaciers and rivers in the Himalayas, the river supports ecosystems of temperate forests, plains and arid countryside.

The northern part of the Indus Valley, with its tributaries, forms the Punjab region, while the lower course of the Indus is known as Sindh and ends in a large delta. The river has historically been important to many cultures of the region. The 3rd millennium BC saw the rise of a major urban civilization of the Bronze Age. During the 2nd millennium BC, the Punjab region was mentioned in the hymns of the Hindu Rigveda as "Sapta Sindhu" and the Zoroastrian Avesta as "Hapta Hindu" (both terms meaning "seven rivers"). Early historical kingdoms that arose in the Indus Valley include Gandhāra, and the Ror dynasty of Sauvīra. The Indus River came into the knowledge of the West early in the Classical Period, when King Darius of Persia sent his Greek subject Scylax of Caryanda to explore the river, ca. 515 BC.

This river was known to the ancient Indians in Sanskrit as "Sindhu", which is literally interpreted to mean "large body of water, sea, or ocean". The Proto-Iranian sound change "*s" > "h" occurred between 850–600 BCE, according to Asko Parpola, causing its Avestan name to become "Hendu", From Iran, the name passed to the Greeks as "Indós" ("Ἰνδός") and to the Romans as "Indus". The Persian name for the river was "Darya", which similarly has the connotations of large body of water and sea.

However, linguists state that the original meaning of "Sindhu"/"Hindu" was not a body of water, but rather a frontier or bank. The Indus river formed the frontier between the Iranian peoples and Indo-Aryan peoples.

Other variants of the name "Sindhu" include Assyrian "Sinda" (as early as the 7th century BC), Persian "Ab-e-sind", Pashto "Abasind", Arab "Al-Sind", Chinese "Sintow", and Javanese "Santri".

"India" is a Greek and Latin term for "the country of the River Indus". The region through which the river drains into sea is called Sindh and owes its name to the river (Sanskrit "Sindhu").

Megasthenes's book "Indica" derives its name from the river's Greek name, "Indós" ("Ἰνδός"), and describes Nearchus's contemporaneous account of how Alexander the Great crossed the river. The ancient Greeks referred to the Indians (people of present-day northwest India and Pakistan) as "Indói" ("Ἰνδοί"), literally meaning "the people of the Indus".

Rigveda also describes several mythical rivers, including one named "Sindhu". The Rigvedic "Sindhu" is thought to be the present-day Indus river and is attested 176 times in its text – 95 times in the plural, more often used in the generic meaning. In the Rigveda, notably in the later hymns, the meaning of the word is narrowed to refer to the Indus river in particular, as in the list of rivers mentioned in the hymn of "Nadistuti sukta". The Rigvedic hymns apply a feminine gender to all the rivers mentioned therein but "Sindhu" is the only river attributed the masculine gender which means Sindhu is the warrior and greatest among all other rivers in whole world

In other languages of the region, the river is known as सिन्धु "(Sindhu)" in Hindi and Nepali, سنڌو ("Sindhu") in Sindhi, ("Sindh") in Shahmukhi Punjabi, ਸਿੰਧ ਨਦੀ ("Sindh Nadī") in Gurmukhī Punjabi, اباسين ("Abāsin" lit. "Father of Rivers") in Pashto, نهر السند ("Nahar al-Sind") in Arabic, སེང་གེ་གཙང་པོ། ("seng ge gtsang po" lit. "Lion River" or "Lion Spring") in Tibetan, ("Yìndù") in Chinese, and "Nilab" in Turki.

The Indus River provides key water resources for Pakistan's economy – especially the "breadbasket" of Punjab province, which accounts for most of the nation's agricultural production, and Sindh. The word Punjab means "land of five rivers" and the five rivers are Jhelum, Chenab, Ravi, Beas and Sutlej, all of which finally flow into the Indus. The Indus also supports many heavy industries and provides the main supply of potable water in Pakistan.

The ultimate source of the Indus is in Tibet; the river begins at the confluence of the Sengge Zangbo and Gar Tsangpo rivers that drain the Nganglong Kangri and Gangdise Shan (Gang Rinpoche, Mt. Kailas) mountain ranges. The Indus then flows northwest through Ladakh and Baltistan into Gilgit, just south of the Karakoram range. The Shyok, Shigar and Gilgit rivers carry glacial waters into the main river. It gradually bends to the south, coming out of the hills between Peshawar and Rawalpindi. The Indus passes gigantic gorges deep near the Nanga Parbat massif. It flows swiftly across Hazara and is dammed at the Tarbela Reservoir. The Kabul River joins it near Attock. The remainder of its route to the sea is in the plains of the Punjab and Sindh, where the flow of the river becomes slow and highly braided. It is joined by the Panjnad at Mithankot. Beyond this confluence, the river, at one time, was named the "Satnad River" ("sat" = "seven", "nadī" = "river"), as the river now carried the waters of the Kabul River, the Indus River and the five Punjab rivers. Passing by Jamshoro, it ends in a large delta to the east of Thatta.

The Indus is one of the few rivers in the world to exhibit a tidal bore. The Indus system is largely fed by the snows and glaciers of the Himalayas, Karakoram and the Hindu Kush ranges of Tibet, the Indian states of Jammu and Kashmir and Himachal Pradesh and Gilgit-Baltistan region of Pakistan. The flow of the river is also determined by the seasons – it diminishes greatly in the winter, while flooding its banks in the monsoon months from July to September. There is also evidence of a steady shift in the course of the river since prehistoric times – it deviated westwards from flowing into the Rann of Kutch and adjoining Banni grasslands after the 1816 earthquake. Presently, Indus water flows in to the Rann of Kutch during its floods breaching flood banks.

The traditional source of the river is the "Senge Khabab" or "Lion's Mouth", a perennial spring, not far from the sacred Mount Kailash marked by a long low line of Tibetan chortens. There are several other tributaries nearby, which may possibly form a longer stream than Senge Khabab, but unlike the Senge Khabab, are all dependent on snowmelt. The Zanskar River, which flows into the Indus in Ladakh, has a greater volume of water than the Indus itself before that point.

The major cities of the Indus Valley Civilisation, such as Harappa and Mohenjo-daro, date back to around 3300 BC, and represent some of the largest human habitations of the ancient world. The Indus Valley Civilisation extended from across northeast Afghanistan to Pakistan and northwest India, with an upward reach from east of Jhelum River to Ropar on the upper Sutlej. The coastal settlements extended from Sutkagan Dor at the Pakistan, Iran border to Kutch in modern Gujarat, India. There is an Indus site on the Amu Darya at Shortughai in northern Afghanistan, and the Indus site Alamgirpur at the Hindon River is located only from Delhi. To date, over 1,052 cities and settlements have been found, mainly in the general region of the Ghaggar-Hakra River and its tributaries. Among the settlements were the major urban centres of Harappa and Mohenjo-daro, as well as Lothal, Dholavira, Ganeriwala, and Rakhigarhi. Only 90–96 of more than 800 known Indus Valley sites have been discovered on the Indus and its tributaries. The Sutlej, now a tributary of the Indus, in Harappan times flowed into the Ghaggar-Hakra River, in the watershed of which were more Harappan sites than along the Indus.

Most scholars believe that settlements of Gandhara grave culture of the early Indo-Aryans flourished in Gandhara from 1700 BC to 600 BC, when Mohenjo-daro and Harappa had already been abandoned.

The word "India" is derived from the Indus River. In ancient times, "India" initially referred to those regions immediately along the east bank of the Indus, but by 300 BC, Greek writers including Herodotus and Megasthenes were applying the term to the entire subcontinent that extends much farther eastward.

The lower basin of the Indus forms a natural boundary between the Iranian Plateau and the Indian subcontinent; this region embraces all or parts of the Pakistani provinces Balochistan, Khyber Pakhtunkhwa, Punjab and Sindh and the countries Afghanistan and India. It was crossed by the invading armies of Alexander, but after his Macedonians conquered the west bank—joining it to the Hellenic Empire, they elected to retreat along the southern course of the river, ending Alexander's Asian campaign. The Indus plains were later dominated by the Persian empire and then the Kushan empire. Over several centuries Muslim armies of Muhammad bin Qasim, Mahmud of Ghazni, Mohammed Ghori, Tamerlane and Babur crossed the river to invade the inner regions of the Punjab and points farther south and east

The Indus river feeds the Indus submarine fan, which is the second largest sediment body on the Earth. It consists of around 5 million cubic kilometres of material eroded from the mountains. Studies of the sediment in the modern river indicate that the Karakoram Mountains in northern Pakistan and India are the single most important source of material, with the Himalayas providing the next largest contribution, mostly via the large rivers of the Punjab (Jhelum, Ravi, Chenab, Beas and Sutlej). Analysis of sediments from the Arabian Sea has demonstrated that prior to five million years ago the Indus was not connected to these Punjab rivers which instead flowed east into the Ganges and were captured after that time. Earlier work showed that sand and silt from western Tibet was reaching the Arabian Sea by 45 million years ago, implying the existence of an ancient Indus River by that time. The delta of this proto-Indus river has subsequently been found in the Katawaz Basin, on the Afghan-Pakistan border.

In the Nanga Parbat region, the massive amounts of erosion due to the Indus river following the capture and rerouting through that area is thought to bring middle and lower crustal rocks to the surface.

In November 2011, satellite images showed that the Indus river had re-entered India, feeding Great Rann of Kutch, Little Rann of Kutch and a lake near Ahmedabad known as Nal Sarovar. Heavy rains had left the river basin along with the Lake Manchar, Lake Hemal and Kalri Lake (all in modern-day Pakistan) inundated. This happened two centuries after the Indus river shifted its course westwards following the 1819 Rann of Kutch earthquake.

The Induan Age at start of the Triassic Period of geological time is named for the Indus region.

Accounts of the Indus valley from the times of Alexander's campaign indicate a healthy forest cover in the region, which has now considerably receded. The Mughal Emperor Babur writes of encountering rhinoceroses along its bank in his memoirs (the Baburnama). Extensive deforestation and human interference in the ecology of the Shivalik Hills has led to a marked deterioration in vegetation and growing conditions. The Indus valley regions are arid with poor vegetation. Agriculture is sustained largely due to irrigation works.
The Indus river and its watershed has a rich biodiversity. It is home to around 25 amphibian species and 147 fish species, 22 of which are only found in the Indus.

The blind Indus River Dolphin ("Platanista indicus minor") is a sub-species of dolphin found only in the Indus River. It formerly also occurred in the tributaries of the Indus river. According to the World Wildlife Fund it is one of the most threatened cetaceans with only about 1,000 still existing.

Palla fish Tenualosa ilisha of the river is a delicacy for people living along the river. The population of fish in the river is moderately high, with Sukkur, Thatta and Kotri being the major fishing centres – all in the lower Sindh course. But damming and irrigation has made fish farming an important economic activity. Located southeast of Karachi, the large delta has been recognised by conservationists as one of the world's most important ecological regions. Here the river turns into many marshes, streams and creeks and meets the sea at shallow levels. Here marine fishes are found in abundance, including pomfret and prawns.

The Indus is the most important supplier of water resources to the Punjab and Sindh plains – it forms the backbone of agriculture and food production in Pakistan. The river is especially critical since rainfall is meagre in the lower Indus valley. Irrigation canals were first built by the people of the Indus Valley Civilisation, and later by the engineers of the Kushan Empire and the Mughal Empire. Modern irrigation was introduced by the British East India Company in 1850 – the construction of modern canals accompanied with the restoration of old canals. The British supervised the construction of one of the most complex irrigation networks in the world. The Guddu Barrage is long – irrigating Sukkur, Jacobabad, Larkana and Kalat. The Sukkur Barrage serves over .

After Pakistan came into existence, a water control treaty signed between India and Pakistan in 1960 guaranteed that Pakistan would receive water from the Indus River and its two tributaries the Jhelum River & the Chenab River independently of upstream control by India.

The Indus Basin Project consisted primarily of the construction of two main dams, the Mangla Dam built on the Jhelum River and the Tarbela Dam constructed on the Indus River, together with their subsidiary dams. The Pakistan Water and Power Development Authority undertook the construction of the Chashma-Jhelum link canal – linking the waters of the Indus and Jhelum rivers – extending water supplies to the regions of Bahawalpur and Multan. Pakistan constructed the Tarbela Dam near Rawalpindi – standing long and high, with an long reservoir. It supports the Chashma Barrage near Dera Ismail Khan for irrigation use and flood control and the Taunsa Barrage near Dera Ghazi Khan which also produces 100,000 kilowatts of electricity. The Kotri Barrage near Hyderabad is long and provides additional water supplies for Karachi. The extensive linking of tributaries with the Indus has helped spread water resources to the valley of Peshawar, in the Khyber Pakhtunkhwa. The extensive irrigation and dam projects provide the basis for Pakistan's large production of crops such as cotton, sugarcane and wheat. The dams also generate electricity for heavy industries and urban centers.

The inhabitants of the regions are mainly Muslim as Pakistan is an Islamic country through which the Indus river passes and forms a major natural feature and resource are diverse in ethnicity, religion, national and linguistic backgrounds. On the northern course of the river in the state of Jammu and Kashmir in India, live the Buddhist people of Ladakh, of Tibetan stock, and the Dards of Indo-Aryan or Dardic stock and practising Islam. Then it descends into Baltistan, northern Pakistan passing the main Balti city of Skardu. A river from Dubair Bala also drains into it at Dubair Bazar. People living in this area are mainly Kohistani and speak the Kohistani language. Major areas through which the Indus river passes in Kohistan are Dasu, Pattan and Dubair. As it continues through Pakistan, the Indus river forms a distinctive boundary of ethnicity and cultures – upon the western banks the population is largely Pashtun, Baloch, and of other Iranian stock. The eastern banks are largely populated by people of Indo-Aryan stock, such as the Punjabis and the Sindhis. In northern Punjab and the Khyber Pakhtunkhwa, ethnic Pashtun tribes live alongside Dardic people in the hills (Khowar, Kalash, Shina, etc.), Burushos (in Hunza), and Punjabi people.

The people living along the Indus river speak Punjabi and Sindhi on the eastern side (in Punjab and Sindh provinces respectively), Pushto plus Balochi as well as Barohi (in Khyber Pakhtoonkha and Baluchistan provinces). In the province of Sindh, the upper third of the river is inhabited by people speaking Saraiki; which is a somewhat transitional dialect of the Punjabi and Sindhi languages.

The Indus is a strategically vital resource for Pakistan's economy and society. After Pakistan and India declared Independence from the British Raj, the use of the waters of the Indus and its five eastern tributaries became a major dispute between India and Pakistan. The irrigation canals of the Sutlej valley and the Bari Doab were split – with the canals lying primarily in Pakistan and the headwork dams in India disrupting supply in some parts of Pakistan. The concern over India building large dams over various Punjab rivers that could undercut the supply flowing to Pakistan, as well as the possibility that India could divert rivers in the time of war, caused political consternation in Pakistan. Holding diplomatic talks brokered by the World Bank, India and Pakistan signed the Indus Waters Treaty in 1960. The treaty gave India control of the three easternmost rivers of the Punjab, the Sutlej, the Beas and the Ravi, while Pakistan gained control of the three western rivers, the Jhelum, the Chenab and the Indus. India retained the right to use of the western rivers for non-irrigation projects.

There are concerns that extensive deforestation, industrial pollution and global warming are affecting the vegetation and wildlife of the Indus delta, while affecting agricultural production as well. There are also concerns that the Indus river may be shifting its course westwards – although the progression spans centuries. On numerous occasions, sediment clogging owing to poor maintenance of canals has affected agricultural production and vegetation. In addition, extreme heat has caused water to evaporate, leaving salt deposits that render lands useless for cultivation.

Originally, the delta used to receive almost all of the water from the Indus river, which has an annual flow of approximately , and is accompanied by 400 million tonnes of silt. Since the 1940s, dams, barrages and irrigation works have been constructed on the river Indus. The Indus Basin Irrigation System is the "largest contiguous irrigation system developed over the past 140 years" anywhere in the world. This has reduced the flow of water and by 2018, the average annual flow of water below the Kotri barrage was , and annual amount of silt discharged was estimated to be . Substantial annual chemical/dissolved load of 46 million tons in the river basin is unable to reach the sea as it is getting trapped in the irrigated areas by increasing the ground water salinity. The result has been catastrophic for both the environment and the local population. The reduction of freshwater due to the dams also increases salinity, making the surface and ground water available in the delta area unsuitable for the freshwater species and crops. As a result, the 2010 Pakistan floods were considered "good news" for the ecosystem and population of the river delta as they brought much needed fresh water. In case of the Indus dolphin, the damming of the river has isolated the delta dolphin population from those dolphins upstream.

The Tibetan Plateau contains the world's third-largest store of ice. Qin Dahe, the former head of the China Meteorological Administration, said the recent fast pace of melting and warmer temperatures will be good for agriculture and tourism in the short term, but issued a strong warning:

"There is insufficient data to say what will happen to the Indus," says David Grey, the World Bank's senior water advisor in South Asia. "But we all have very nasty fears that the flows of the Indus could be severely, severely affected by glacier melt as a consequence of climate change," and reduced by perhaps as much as 50 percent. "Now what does that mean to a population that lives in a desert [where], without the river, there would be no life? I don't know the answer to that question," he says. "But we need to be concerned about that. Deeply, deeply concerned."

U.S. diplomat Richard Holbrooke said, shortly before his death in 2010, that he believed that falling water levels in the Indus River "could very well precipitate World War III."

Over the years factories on the banks of the Indus River have increased levels of water pollution in the river and the atmosphere around it. High levels of pollutants in the river have led to the deaths of endangered Indus River Dolphin. The Sindh Environmental Protection Agency has ordered polluting factories around the river to shut down under the Pakistan Environmental Protection Act, 1997. Death of the Indus River Dolphin has also been attributed to fishermen using poison to kill fish and scooping them up. As a result, the government banned fishing from Guddu Barrage to Sukkur.

In July 2010, following abnormally heavy monsoon rains, the Indus River rose above its banks and started flooding. The rain continued for the next two months, devastating large areas of Pakistan. In Sindh, the Indus burst its banks near Sukkur on 8 August, submerging the village of Mor Khan Jatoi. In early August, the heaviest flooding moved southward along the Indus River from severely affected northern regions toward western Punjab, where at least of cropland was destroyed, and the southern province of Sindh. , over two thousand people had died and over a million homes had been destroyed since the flooding began.

The 2011 Sindh floods began during the Pakistani monsoon season in mid-August 2011, resulting from heavy monsoon rains in Sindh, eastern Balochistan, and southern Punjab. The floods caused considerable damage; an estimated 434 civilians were killed, with 5.3 million people and 1,524,773 homes affected. Sindh is a fertile region and often called the "breadbasket" of the country; the damage and toll of the floods on the local agrarian economy was said to be extensive. At least of arable land were inundated. The flooding followed the previous year's floods, which devastated a large part of the country. Unprecedented torrential monsoon rains caused severe flooding in 16 districts of Sindh.

In Pakistan currently there are three barrages on the Indus: Guddu barrage, Sukkur Barrage, and Kotri barrage (also called Ghulam Muhammad barrage). There are some bridges on river Indus, such as, Dadu Moro Bridge, Larkana Khairpur Indus River Bridge, Thatta-Sujawal bridge, Jhirk-Mula Katiar bridge and recently planned Kandhkot-Ghotki bridge.

Kala Bagh Barrage, Chasma Barrage, and Taunsa Barrage are also built in Punjab on the Indus.

Tarbela Dam in Pakistan is constructed on the Indus River, while the controversial Kalabagh dam is also being constructed on Indus river.



</doc>
<doc id="15491" url="https://en.wikipedia.org/wiki?curid=15491" title="Integer factorization">
Integer factorization

In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these integers are further restricted to prime numbers, the process is called prime factorization.

When the numbers are sufficiently large, no efficient, non-quantum integer factorization algorithm is known. An effort by several researchers, concluded in 2009, to factor a 232-digit number (RSA-768) utilizing hundreds of machines took two years and the researchers estimated that a 1024-bit RSA modulus would take about a thousand times as long. However, it has not been proven that no efficient algorithm exists. The presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as RSA. Many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing.

Not all numbers of a given length are equally hard to factor. The hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. When they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, e.g., to avoid efficient factorization by Fermat's factorization method), even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically.

Many cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem—for example, the RSA problem. An algorithm that efficiently factors an arbitrary integer would render RSA-based public-key cryptography insecure.

By the fundamental theorem of arithmetic, every positive integer has a unique prime factorization. (By convention 1 is the empty product.) If the integer is prime then it can be recognized as such in polynomial time. If composite however, the theorem gives no insight into how to obtain the factors.

Given a general algorithm for integer factorization, any integer can be factored down to its constituent prime factors simply by repeated application of this algorithm. The situation is more complicated with special-purpose factorization algorithms, whose benefits may not be realized as well or even at all with the factors produced during decomposition. For example, if where are very large primes, trial division will quickly produce the factors 3 and 19 but will take "p" divisions to find the next factor. As a contrasting example, if "N" is the product of the primes 13729, 1372933, and 18848997161, where , Fermat's factorization method will start out with which immediately yields and hence the factors and . While these are easily recognized as respectively composite and prime, Fermat's method will take much longer to factorize the composite one because the starting value of for "a" is nowhere near 1372933.

Among the "b"-bit numbers, the most difficult to factor in practice using existing algorithms are those that are products of two primes of similar size. For this reason, these are the integers used in cryptographic applications. The largest such semiprime yet factored was RSA-768, a 768-bit number with 232 decimal digits, on December 12, 2009. This factorization was a collaboration of several research institutions, spanning two years and taking the equivalent of almost 2000 years of computing on a single-core 2.2 GHz AMD Opteron. Like all recent factorization records, this factorization was completed with a highly optimized implementation of the general number field sieve run on hundreds of machines.

No algorithm has been published that can factor all integers in polynomial time, i.e., that can factor "b"-bit numbers in time O("b") for some constant "k". The problem is clearly in class NP but has not been proved to be or not be NP-complete. 

There are published algorithms that are faster than O((1+"ε")) for all positive "ε", i.e., sub-exponential. The best published asymptotic running time is for the general number field sieve (GNFS) algorithm, which, for a "b"-bit number "n", is:

For current computers, GNFS is the best published algorithm for large "n" (more than about 400 bits). For a quantum computer, however, Peter Shor discovered an algorithm in 1994 that solves it in polynomial time. This will have significant implications for cryptography if quantum computation becomes scalable. Shor's algorithm takes only time and O("b") space on "b"-bit number inputs. In 2001, the first seven-qubit quantum computer became the first to run Shor's algorithm. It factored the number 15.

When discussing what complexity classes the integer factorization problem falls into, it is necessary to distinguish two slightly different versions of the problem:


For , the decision problem is equivalent to asking if "N" is not prime. 

An algorithm for either version provides one for the other. Repeated application of the function problem (applied to "d" and "N"/"d", and their factors, if needed) will eventually provide either a factor of "N" no larger than "M" or a factorization into primes all greater than "M". All known algorithms for the decision problem work in this way. Hence it is only of theoretical interest that, with at most log "N" queries using an algorithm for the decision problem, one would isolate a factor of "N" (or prove it prime) by binary search. 

It is not known exactly which complexity classes contain the decision version of the integer factorization problem. It is known to be in both NP and co-NP. This is because both YES and NO answers can be verified in polynomial time. An answer of YES can be certified by exhibiting a factorization with . An answer of NO can be certified by exhibiting the factorization of "N" into distinct primes, all larger than "M". We can verify their primality using the AKS primality test and that their product is "N" by multiplication. The fundamental theorem of arithmetic guarantees that there is only one possible string that will be accepted (providing the factors are required to be listed in order), which shows that the problem is in both UP and co-UP. It is known to be in BQP because of Shor's algorithm. It is suspected to be outside of all three of the complexity classes P, NP-complete, and co-NP-complete. It is therefore a candidate for the NP-intermediate complexity class. If it could be proved that it is in either NP-Complete or co-NP-Complete, that would imply NP = co-NP. That would be a very surprising result, and therefore integer factorization is widely suspected to be outside both of those classes. Many people have tried to find classical polynomial-time algorithms for it and failed, and therefore it is widely suspected to be outside P.

In contrast, the decision problem "is "N" a composite number?" (or equivalently: "is "N" a prime number?") appears to be much easier than the problem of actually finding the factors of "N". Specifically, the former can be solved in polynomial time (in the number "n" of digits of "N") with the AKS primality test. In addition, there are a number of probabilistic algorithms that can test primality very quickly in practice if one is willing to accept the vanishingly small possibility of error. The ease of primality testing is a crucial part of the RSA algorithm, as it is necessary to find large prime numbers to start with.

A special-purpose factoring algorithm's running time depends on the properties of the number to be factored or on one of its unknown factors: size, special form, etc. Exactly what the running time depends on varies between algorithms.

An important subclass of special-purpose factoring algorithms is the "Category 1" or "First Category" algorithms, whose running time depends on the size of smallest prime factor. Given an integer of unknown form, these methods are usually applied before general-purpose methods to remove small factors. For example, trial division is a Category 1 algorithm.


A general-purpose factoring algorithm, also known as a "Category 2", "Second Category", or "Kraitchik family" algorithm (after Maurice Kraitchik), has a running time which depends solely on the size of the integer to be factored. This is the type of algorithm used to factor RSA numbers. Most general-purpose factoring algorithms are based on the congruence of squares method.



In number theory, there are many integer factoring algorithms that heuristically have expected running time

in big O and L-notation.
Some examples of those algorithms are the elliptic curve method and the quadratic sieve.
Another such algorithm is the class group relations method proposed by Schnorr, Seysen, and Lenstra, that is proved under the assumption of the Generalized Riemann Hypothesis (GRH).

The Schnorr-Seysen-Lenstra probabilistic algorithm has been rigorously proven by Lenstra and Pomerance to have expected running time formula_3 by replacing the GRH assumption with the use of multipliers.
The algorithm uses the class group of positive binary quadratic forms of discriminant Δ denoted by "G".
"G" is the set of triples of integers ("a", "b", "c") in which those integers are relative prime.

Given an integer "n" that will be factored, where "n" is an odd positive integer greater than a certain constant. In this factoring algorithm the discriminant Δ is chosen as a multiple of "n", , where "d" is some positive multiplier. The algorithm expects that for one "d" there exist enough smooth forms in "G". Lenstra and Pomerance show that the choice of "d" can be restricted to a small set to guarantee the smoothness result.

Denote by "P" the set of all primes "q" with Kronecker symbol formula_4. By constructing a set of generators of "G" and prime forms "f" of "G" with "q" in "P" a sequence of relations between the set of generators and "f" are produced.
The size of "q" can be bounded by formula_5 for some constant formula_6.

The relation that will be used is a relation between the product of powers that is equal to the neutral element of "G". These relations will be used to construct a so-called ambiguous form of "G", which is an element of "G" of order dividing 2. By calculating the corresponding factorization of Δ and by taking a gcd, this ambiguous form provides the complete prime factorization of "n". This algorithm has these main steps:

Let "n" be the number to be factored.

To obtain an algorithm for factoring any positive integer, it is necessary to add a few steps to this algorithm such as trial division, and the Jacobi sum test.

The algorithm as stated is a probabilistic algorithm as it makes random choices. Its expected running time is at most formula_3.





</doc>
<doc id="15492" url="https://en.wikipedia.org/wiki?curid=15492" title="Imperial units">
Imperial units

The system of imperial units or the imperial system (also known as British Imperial or Exchequer Standards of 1825) is the system of units first defined in the British Weights and Measures Act of 1824, which was later refined and reduced. The Imperial units replaced the Winchester Standards, which were in effect from 1588 to 1825. The system came into official use across the British Empire. By the late 20th century, most nations of the former empire had officially adopted the metric system as their main system of measurement, although some imperial units are still used in the United Kingdom, Canada and other countries formerly part of the British Empire. The imperial system developed from what were first known as English units, as did the related system of United States customary units.

The Weights and Measures Act of 1824 was initially scheduled to go into effect on 1 May 1825. However, the Weights and Measures Act of 1825 pushed back the date to 1 January 1826. The 1824 Act allowed the continued use of pre-imperial units provided that they were customary, widely known, and clearly marked with imperial equivalents.

Apothecaries' units are mentioned neither in the act of 1824 nor 1825. At the time, apothecaries' weights and measures were regulated "in England, Wales, and Berwick-upon-Tweed" by the London College of Physicians, and in Ireland by the Dublin College of Physicians. In Scotland, apothecaries' units were unofficially regulated by the Edinburgh College of Physicians. The three colleges published, at infrequent intervals, pharmacopoeiae, the London and Dublin editions having the force of law.

Imperial apothecaries' measures, based on the imperial pint of 20 fluid ounces, were introduced by the publication of the London Pharmacopoeia of 1836, the Edinburgh Pharmacopoeia of 1839, and the Dublin Pharmacopoeia of 1850. The Medical Act of 1858 transferred to The Crown the right to publish the official pharmacopoeia and to regulate apothecaries' weights and measures.

Metric equivalents in this article usually assume the latest official definition. Before this date, the most precise measurement of the imperial Standard Yard was metres.

In 1824, the various different gallons in use in the British Empire were replaced by the imperial gallon, a unit close in volume to the ale gallon. It was originally defined as the volume of of distilled water weighed in air with brass weights with the barometer standing at at a temperature of . In 1963, the gallon was redefined as the volume of 10 pounds of distilled water of density weighed in air of density against weights of density , which works out to or . The Weights and Measures Act of 1985 switched to a gallon of exactly (approximately ).

These measurements were in use from 1826, when the new imperial gallon was defined, but were officially abolished in the United Kingdom on 1 January 1971. In the USA, though no longer recommended, the apothecaries' system is still used occasionally in medicine, especially in prescriptions for older medications.
In the 19th and 20th centuries, the UK used three different systems for mass and weight:

The troy pound () was made the primary unit of mass by the 1824 Act; however, its use was abolished in the UK on 1 January 1879, with only the troy ounce () and its decimal subdivisions retained. The "Weights and Measures Act 1855" (18 & 19 Victoria C72) made the avoirdupois pound the primary unit of mass. In all the systems, the fundamental unit is the pound, and all other units are defined as fractions or multiples of it.

Although the 1824 act defined the yard and pound by reference to the prototype standards, it also defined the values of certain physical constants, to make provision for re-creation of the standards if they were to be damaged. For the yard, the length of a pendulum beating seconds at the latitude of Greenwich at Mean Sea Level "in vacuo" was defined as inches. For the pound, the mass of a cubic inch of distilled water at an atmospheric pressure of 30 inches of mercury and a temperature of 62° Fahrenheit was defined as 252.458 grains, with there being 7,000 grains per pound. However, following the destruction of the original prototypes in the 1834 Houses of Parliament fire, it proved impossible to recreate the standards from these definitions, and a new Weights and Measures Act (18 & 19 Victoria. Cap. 72) was passed in 1855 which permitted the recreation of the prototypes from recognized secondary standards.

The imperial system is one of many systems of English units. Although most of the units are defined in more than one system, some subsidiary units were used to a much greater extent, or for different purposes, in one area rather than the other. The distinctions between these systems are often not drawn precisely.

One such distinction is that between these systems and older British/English units/systems or newer additions. The term "imperial" should not be applied to English units that were outlawed in the Weights and Measures Act 1824 or earlier, or which had fallen out of use by that time, nor to post-imperial inventions, such as the slug or poundal.

The US customary system is historically derived from the English units that were in use at the time of settlement. Because the United States was already independent at the time, these units were unaffected by the introduction of the imperial system.

British law now defines each imperial unit in terms of the metric equivalent. The metric system is in official use within the United Kingdom for most official applications with Imperial units remaining in widespread use amongst the public. All UK roads use the imperial system except for weight limits, and newer height or width restriction signs give metric alongside imperial.

Units of measurement regulations require all measuring devices used in trade or retail to display measurements in metric quantities. Almost all traders in the UK will accept requests from customers specified in imperial units, and scales which display in both unit systems are commonplace in the retail trade. Metric price signs may be accompanied by imperial price signs provided that the imperial signs are no larger and no more prominent than the metric ones.

The United Kingdom completed its official partial transition to the metric system in 1995, with some imperial units still legally mandated for certain applications such as draught beer and cider, road-signs, and therefore the speedometers on vehicles sold in the UK must be capable of displaying miles per hour. Even though the troy pound was outlawed in the UK in the Weights and Measures Act of 1878, the "troy ounce" "may" still be used for the weights of precious stones and metals. The original railways (many built in the Victorian era) are a big user of imperial units, with distances officially measured in miles and yards or miles and chains, and also feet and inches, and speeds are in miles per hour, although more recent systems are metric, and London Underground uses metric.

Most British people still use imperial units in everyday life for distance (miles, yards, feet and inches) and volume in some cases (especially milk and beer in pints) but rarely for canned or bottled soft drinks or petrol. Though use of kilograms is increasing, most British people also still use imperial units in everyday life for body weight (stones and pounds for adults, pounds and ounces for babies). Some government documents aimed at the public give body weight and height not only in metric units (kilograms centimetres) but also in imperial units (stones and pounds, feet and inches). A survey in 2015 found that many people did not know their body weight or height in one system or the other. People under the age of 40 preferred the metric system but people aged 40 and over preferred the imperial system. The height of horses in some English-speaking countries, including Australia, Canada, the United Kingdom and the United States is usually measured in hands, standardized to 4 inches (101.6 mm). Fuel consumption for vehicles is commonly stated in miles per gallon, though official figures always include litres per 100 km equivalents. When sold draught in licensed premises, beer and cider must be sold in pints and half-pints. Cow's milk is available in both litre- and pint-based containers in supermarkets and shops. Areas of land associated with farming, forestry and real estate are commonly advertised in acres and square feet, but for official government purposes the units are always hectares and square metres.

Office space and industrial units are usually advertised in square feet. Steel pipe sizes are sold in increments of inches, while copper pipe is sold in increments of millimetres. Road bicycles have their frames measured in centimetres, while off-road bicycles have their frames measured in inches. The size (diagonal) of television and computer monitor screens is always denominated in inches. Food sold by length or width e.g. pizzas or sandwiches, is generally sold in inches. Clothing is always sized in inches, with the metric equivalent often shown as a small supplementary indicator. Gas is usually measured by the cubic foot or cubic metre, but is billed like electricity by the kilowatt hour.

Some pre-packaged products show both metric and imperial measures and it is also common to see imperial pack sizes with metric only labels e.g. a 1 lb (i.e., 454 g) tin of Lyle's Golden Syrup is always labelled 454 g with no imperial indicator. Similarly most jars of jam and packs of sausages are labelled 454 g with no imperial indicator.

India's conversion to the metric system from the imperial system occurred in stages between 1955 and 1962. The metric system in weights and measures was adopted by the Indian Parliament in December 1956 with the "Standards of Weights and Measures Act", which took effect beginning 1 October 1958. The "Indian Coinage Act" was passed in 1955 by the Government of India to introduce decimal coinage in the country. The new system of coins became legal tender on April 1957, where the rupee consists of 100 paise. For the next five years, both the previous and new systems were legal. In April 1962, all other systems were banned. This process of metrication is called "big-bang" route, which is to simultaneously outlaw the use of pre-metric measurement, metricise, reissue all government publications and laws, and change education systems to metric.

Today all official measurements are made in the metric system. However, in common usage some older Indians may still refer to imperial units. Some measurements, such as the heights of mountains, are still recorded in feet. Additionally, the Indian numbering system of crores and lacs is used alongside otherwise metricated currency units, while tyre rim diameters are still measured in inches, as used worldwide. Road widths are popularly measured in feet but official documents use metres. Body temperature is still sometimes measured in degrees Fahrenheit. Industries like the construction and the real estate industry still use both the metric and the imperial system though it is more common for sizes of homes to be given in square feet and land in acres. Bulk cotton is sold by the "candy" (0.35 imperial tons, or 355.62 kg) or the "bale" (170 kg).

In Standard Indian English, as in Australian, Singaporean, and British English, metric units such as the litre (liter), metre (meter), and metric tonne (ton) utilise the traditional spellings brought over from French, which differ from those used in the United States and the Philippines. The imperial long ton is invariably spelt with one 'n'. (See English in the Commonwealth of Nations for more information).

Hong Kong has three main systems of units of measurement in current use:


In 1976 the Hong Kong Government started the conversion to the metric system, and as of 2012 measurements for government purposes, such as road signs, are almost always in metric units. However, all three systems are officially permitted for trade, and in the wider society a mixture of all three systems prevails.

The Chinese system's most commonly used units for length are (li), (tseung/cheung), (tsek/chek), (tsun/chun), (fen/fan) in descending scale order. These units are now rarely used in daily life, the imperial and metric systems being preferred. The imperial equivalents are written with the same basic Chinese characters as the Chinese system. In order to distinguish between the units of the two systems, the units can be prefixed with "Ying" () for the Imperial system and "Wa" () for the Chinese system. In writing, derived characters are often used, with an additional (mouth) radical to the left of the original Chinese character, for writing imperial units. The most commonly used units are the mile or "li" (), the yard or "ma" (), the foot or "chek" (), and the inch or "tsun" ().

The traditional measure of flat area is the square foot () of the imperial system, which is still in common use for real estate purposes. The measurement of agricultural plots and fields, however, is traditionally conducted in (mau) of the Chinese system.

For the measurement of volume, Hong Kong officially uses the metric system, though the gallon (, ka-lun) is also occasionally used.

During the 1970s, the metric system and SI units were introduced in Canada to replace the imperial system. Within the government, efforts to implement the metric system were extensive; almost any agency, institution, or function provided by the government uses SI units exclusively. Imperial units were eliminated from all road signs, although both systems of measurement will still be found on privately owned signs, such as the height warnings at the entrance of a parkade. In the 1980s, momentum to fully convert to the metric system stalled when the government of Brian Mulroney was elected. There was heavy opposition to metrication and as a compromise the government maintains legal definitions for and allows use of imperial units as long as metric units are shown as well.
The law requires that measured products (such as fuel and meat) be priced in metric units, although an imperial price can be shown if a metric price is present. However, there tends to be leniency in regards to fruits and vegetables being priced in imperial units only.
Environment Canada still offers an imperial unit option beside metric units, even though weather is typically measured and reported in metric units in the Canadian media. However, some radio stations near the United States border (such as CIMX and CIDR) primarily use imperial units to report the weather. Railways in Canada also continue to use Imperial units.

Imperial units are still used in ordinary conversation. Today, Canadians typically use a mix of metric and imperial measurements in their daily lives. However, the use of the metric and imperial systems varies by age. The older generation mostly uses the imperial system, while the younger generation more often uses the metric system. Newborns are measured in SI at hospitals, but the birth weight and length is also announced to family and friends in imperial units. Drivers' licences use SI units. In livestock auction markets, cattle are sold in dollars per hundredweight (short), whereas hogs are sold in dollars per hundred kilograms. Imperial units still dominate in recipes, construction, house renovation and gardening. Land is now surveyed and registered in metric units, although initial surveys used imperial units. For example, partitioning of farm land on the prairies in the late 19th and early 20th centuries was done in imperial units; this accounts for imperial units of distance and area retaining wide use in the Prairie Provinces. The size of most apartments, condominiums and houses continues to be described in square feet rather than square metres, and carpet or flooring tile is purchased by the square foot. Motor-vehicle fuel consumption is reported in both litres per 100 km and statute miles per imperial gallon, leading to the erroneous impression that Canadian vehicles are 20% more fuel-efficient than their apparently identical American counterparts for which fuel economy is reported in statute miles per US gallon (neither country specifies which gallon is used). Canadian railways maintain exclusive use of imperial measurements to describe train length (feet), train height (feet), capacity (tons), speed (mph), and trackage (miles).

Imperial units also retain common use in firearms and ammunition. Imperial measures are still used in the description of cartridge types, even when the cartridge is of relatively recent invention (e.g., .204 Ruger, .17 HMR, where the calibre is expressed in decimal fractions of an inch). However, ammunition that is already classified in metric is still kept metric (e.g., 9×19mm). In the manufacture of ammunition, bullet and powder weights are expressed in terms of grains for both metric and imperial cartridges.

As in most of the western world, air navigation is based on "nautical" units, e.g., the nautical mile, which is neither imperial nor metric, though altitude is still measured in imperial feet in keeping with the international standard.

Metrication in Australia has largely ended the use of imperial units, though for particular measurements (such as flight altitudes and nominal sizes of computer and television screens) international use of imperial units is still followed. In licensed venues, draught beer and cider is sold in glasses and jugs with sizes based on the imperial fluid ounce though rounded to the nearest 5 mL.

Although New Zealand completed metrication in the 1970s, a study of university students undertaken in 1992 found a continued use of imperial units for birth weight and human height alongside metric units.

The aviation industry is one of the last major users of the imperial system: Altitude and airport elevation are measured in feet. Navigation is done in nautical miles (a unit accepted for use with the SI); all other aspects (fuel quantity, aircraft weight, runway length, etc.) use metric.

LCD screen size found in Televisions and Computer Monitors, and rim size in automotive applications are also sold in inches, as is convention with the rest of the world.

Ireland has officially changed over to the metric system since entering the European Union, with distances on new road signs being metric since 1997 and speed limits being metric since 2005. The imperial system remains in limited use – for sales of beer in pubs (traditionally sold by the pint). All other goods are required by law to be sold in metric units, although old quantities are retained for some goods like butter and sausages, which are sold in 454-gram (1 lb) packaging. The majority of cars sold pre-2005 feature speedometers with miles per hour as the primary unit, but with a kilometres per hour display as well.

Some imperial measurements remain in limited use in Malaysia, the Philippines, Sri Lanka and South Africa. Measurements in feet and inches, especially for a person's height, are frequently encountered in conversation and non-governmental publications.

Prior to metrication, it was a common practice in Malaysia for people to refer to unnamed locations and small settlements along major roads by referring to how many miles the said locations were located from the nearest major town. In some cases, these eventually became the official names of the locations; in other cases, such names have been largely or completely superseded by new names. An example of the former is Batu 32 (literally "Mile 32" in Malay), which refers to the area surrounding the intersection between Federal Route 22 (the Tamparuli-Sandakan highway) and Federal Route 13 (the Sandakan-Tawau highway). The area is so named because it is 32 miles west of Sandakan, the nearest major town.

Petrol is still sold by the imperial gallon in Anguilla, Antigua and Barbuda, Belize, Myanmar, the Cayman Islands, Dominica, Grenada, Montserrat, St Kitts and Nevis and St. Vincent and the Grenadines. The United Arab Emirates Cabinet in 2009 issued the Decree No. (270 / 3) specifying that, from 1 January 2010, the new unit sale price for petrol will be the litre and not the gallon. This in line with the UAE Cabinet Decision No. 31 of 2006 on the national system of measurement, which mandates the use of International System of units as a basis for the legal units of measurement in the country. Sierra Leone switched to selling fuel by the litre in May 2011.

In October 2011, the Antigua and Barbuda government announced the re-launch of the Metrication Programme in accordance with the Metrology Act 2007, which established the International System of Units as the legal system of units. The Antigua and Barbuda government has committed to a full conversion from the imperial system by the first quarter of 2015.




</doc>
<doc id="15494" url="https://en.wikipedia.org/wiki?curid=15494" title="Incompatible-properties argument">
Incompatible-properties argument

The incompatible-properties argument is the idea that no description of God is consistent with reality. For example, if one takes the definition of God to be described fully from the Bible, then the claims of what properties God has described therein might be argued to lead to a contradiction.

The problem of evil is the argument that the existence of evil is incompatible with the concept of an omnipotent and perfectly good God.

A variation does not depend on the existence of evil. A truly omnipotent God could create all possible worlds. A "good" God can create only "good" worlds. A God that created all possible worlds would have no moral qualities whatsoever, and could be replaced by a random generator. The standard response is to argue a distinction between "could create" and "would create." In other words, God "could" create all possible worlds but that is simply not in God's nature. This has been argued by theologians for centuries. However, the result is that a "good" God is incompatible with some possible worlds, thus incapable of creating them without losing the property of being a totally different God. Yet, it is not necessary for God to be "good". He simply is good, but is capable of evil.

One argument based on incompatible properties rests on a definition of God that includes a will, plan or purpose and an existence outside of time. To say that a being possesses a purpose implies an inclination or tendency to steer events toward some state that does not yet exist. This, in turn, implies a privileged direction, which we may call "time". It may be one direction of causality, the direction of increasing entropy, or some other emergent property of a world. These are not identical, but one must exist in order to progress toward a goal. 

In general, God's time would not be related to our time. God might be able to operate within our time without being constrained to do so. However, God could then step outside this game for any purpose. Thus God's time must be aligned with our time if human activities are relevant to God's purpose. (In a relativistic universe, presumably this means—at any point in spacetime—time measured from t=0 at the Big Bang or end of inflation.)

A God existing outside of any sort of time could not create anything because creation substitutes one thing for another, or for nothing. Creation requires a creator that existed, by definition, prior to the thing created.

Another pair of alleged incompatible properties is omniscience and either indeterminacy or free will. Omniscience concerning the past and present (properly defined relative to Earth) is not a problem, but there is an argument that omniscience regarding the future implies it has been determined, what seems possible only in a deterministic world.

Another pair is divine simplicity and omniscience. An omniscient God must necessarily encompass all information in the universe. Information is not "ineffable" and cannot be reduced to something simpler.




</doc>
<doc id="15495" url="https://en.wikipedia.org/wiki?curid=15495" title="International Society of Olympic Historians">
International Society of Olympic Historians

The International Society of Olympic Historians (ISOH) is a non-profit organization founded in 1991 with the purpose of promoting and studying the Olympic Movement and the Olympic Games. The majority of recent books on the Olympic Games have been written by ISOH members. The ISOH publishes the Journal of Olympic History (JOH, formerly "Citius, Altius, Fortius") three times a year.

The International Society of Olympic Historians (ISOH) was formed as the result of a meeting in London, England in December 1991. The idea of forming an Olympic historical society had been the subject of correspondence – mainly between Bill Mallon (United States) and Ture Widlund (Sweden) – for many years. On Thursday, 5 December 1991, a group of potential members met at the Duke of Clarence, a small pub in the Kensington section of London. Those present were Ian Buchanan (Great Britain), Stan Greenberg (Great Britain), Ove Karlsson (Sweden), Bill Mallon (United States), Peter Matthews (Great Britain), David Wallechinsky (United States), and Ture Widlund (Sweden). The invited guests who sent regrets were: Anthony Bijkerk (Netherlands), Peter Diamond (United States), Pim Huurman (Netherlands), Erich Kamper (Austria), Volker Kluge (Germany), John Lucas (United States), and Wolf Lyberg (Sweden).

ISOH was formed with the purpose of promoting and studying the Olympic Movement and the Olympic Games. This purpose is achieved primarily through research into their history, through the gathering of historical and statistical data concerning the Olympic Movement and Olympic Games, through the publication of the research via journals and other publications, and through the cooperation of the membership.

From its inception to 2000, Ian Buchanan has been the president of the ISOH. In 2000, this function was taken over by Bill Mallon. From 2004 to 2012 Dr. Karl Lennartz (Germany) served as president and since 2012 David Wallechinsky (United States) has been president.

The ISOH publishes the "Journal of Olympic History" (formerly "Citius, Altius, Fortius").

, the ISOH has about 340 members from 48 nations. The membership includes well-known Olympic historians and researchers on Olympic topics. The majority of recent books on the Olympic Games have been written by ISOH members. Over 20 ISOH members have received the Olympic Order for their contributions to the Olympic Movement, and several members of the IOC and several Olympians are members. Other members are collectors of Olympic memorabilia, such as Raleigh DeGeer Amyx.






</doc>
<doc id="15496" url="https://en.wikipedia.org/wiki?curid=15496" title="Serie A">
Serie A

Serie A (), also called Serie A TIM due to sponsorship by TIM, is a professional league competition for football clubs located at the top of the Italian football league system and the winner is awarded the Coppa Campioni d'Italia. It has been operating for over eighty years since the 1929–30 season. It had been organized by Lega Calcio until 2010, when the Lega Serie A was created for the 2010–11 season.

Serie A is regarded as one of the best football leagues in the world and it is often depicted as the most tactical national league. Serie A was the world's second-strongest national league in 2014 according to IFFHS and has produced the highest number of European Cup finalists: Italian clubs have reached the final of the competition on a record 27 occasions, winning the title 12 times. Serie A is ranked third among European leagues according to UEFA's league coefficient, behind La Liga, the Premier League and ahead of the Bundesliga and the Ligue 1, which is based on the performance of Italian clubs in the Champions League and the Europa League during the last five years. Serie A led the UEFA ranking from 1986 to 1988 and from 1990 to 1999.

In its current format, the Italian Football Championship was revised from having regional and interregional rounds, to a single-tier league from the 1929–30 season onwards. The championship titles won prior to 1929 are officially recognised by FIGC with the same weighting as titles that were subsequently awarded. However, the 1945–46 season, when the league was played over two geographical groups due to the ravages of WWII, is not statistically considered, even if its title is fully official. All the winning teams are recognised with the title of "Campione d'Italia" ("Champion of Italy"), which is ratified by the Lega Serie A before the start of the next edition of the championship.

The league hosts three of the world's most famous clubs as Juventus, Milan and Internazionale, all founding members of the G-14, a group which represented the largest and most prestigious European football clubs since 2000 to 2008, being the first two cited also founding members of its successive organisation, European Club Association (ECA). More players have won the coveted Ballon d'Or award while playing at a Serie A club than any other league in the world – although Spain's La Liga has the highest total number of Ballon d'Or winners including the FIFA Ballon d'Or. Juventus, Italy's most successful club of the 20th century and the most successful Italian team, is tied for fourth in Europe and eighth in the world with the most official international titles. The club is also the only one in the world to have won all possible official confederation competitions. Milan is joint third club for official international titles won in the world, with 18. Internazionale, following their achievements in the 2009–10 season, became the first Italian team to have achieved a treble. Juventus, Milan and Inter, along with Roma, Fiorentina, Lazio and Napoli, are known as the Seven Sisters of Italian football.

Serie A is one of the most storied football leagues in the world. Of the 100 greatest footballers in history chosen by "FourFourTwo" magazine in 2017, 42 players have played in Serie A, more than any other league in the world.

For most of Serie A's history, there were 16 or 18 clubs competing at the top level. Since 2004–05, however, there have been 20 clubs altogether. One season (1947–48) was played with 21 teams for political reasons. Below is a complete record of how many teams played in each season throughout the league's history;
During the season, which runs from August to May, each club plays each of the other teams twice; once at home and once away, totalling 38 games for each team by the end of the season. Thus, in Italian football a true round-robin format is used. In the first half of the season, called the "andata", each team plays once against each league opponent, for a total of 19 games. In the second half of the season, called the "ritorno", the teams play in exactly the same order that they did in the first half of the season, the only difference being that home and away situations are switched. Since the 1994–95 season, teams are awarded three points for a win, one point for a draw and no points for a loss.

The top four teams in the Serie A qualify straight to the UEFA Champions League group stages (from the 2017–18 season). Teams finishing fifth and sixth qualify for the UEFA Europa League tournament. A third UEFA Europa League spot is reserved for the winner of the Coppa Italia. If the Coppa Italia champion already qualified for European football by finishing among the top seven teams in Serie A, the seventh-ranked team in Serie A is awarded the UEFA Europa League spot. The three lowest-placed teams are relegated to Serie B.

From 2005–06 season if two or more teams are tied in points (for any place), the deciding tie-breakers are as follows:


Until 2004–05 season, a playoff would be used to determine the champions, European spots or relegation, if the two teams were tied on points. Any play-off was held after the end of regular season. The last championship playoff occurred in the 1963-64 season when Bologna and Inter both finished on 54 points. Bologna won the play-off 2-0.

Prior to 1929, many clubs competed in the top level of Italian football as the earlier rounds were competed up to 1922 on a regional basis then interregional up to 1929. Below is a list of Serie A clubs who have competed in the competition when it has been a league format (66 in total).

There are 67 teams that have taken part in 87 Serie A championships in a single round that was played from the 1929–30 season until the 2018–19 season. The teams in bold compete in Serie A currently. Internazionale is the only team that has played Serie A football in every season.
Serie A, as it is structured today, began during the 1929–30 season. From 1898 to 1922, the competition was organised into regional groups. Because of ever growing teams attending regional championships, the Italian Football Federation (FIGC) split the CCI (Italian Football Confederation) in 1921. When CCI teams rejoined the FIGC created two interregional divisions renaming Categories into Divisions and splitting FIGC sections into two North-South leagues. In 1926, due to internal crises, the FIGC changed internal settings, adding southern teams to the national division, ultimately leading to the 1929–30 final settlement. No title was awarded in 1927 after Torino were stripped of the championship by the FIGC. Torino were declared champions in the 1948–49 season following a plane crash near the end of the season in which the entire team was killed.

The Serie A Championship title is often referred to as the "scudetto" ("small shield") because since the 1924–25 season, the winning team will bear a small coat of arms with the Italian tricolour on their strip in the following season. The most successful club is Juventus with 33 championships, followed by both Milan and Internazionale, with 18 championships apiece. From the 2004–05 season onwards, an actual trophy was awarded to club on the pitch after the last turn of the championship. The trophy, called the Coppa Campioni d'Italia, has officially been used since the 1960–61 season, but between 1961 and 2004 was consigned to the winning clubs at the head office of the Lega Nazionale Professionisti.

In April 2009, Serie A announced a split from Serie B. Nineteen of the twenty clubs voted in favour of the move in an argument over television rights; the relegation-threatened Lecce had voted against the decision. Maurizio Beretta, the former head of Italy's employers' association, became president of the new league.

In April 2016, it was announced that Serie A was selected by the International Football Association Board to test video replays, which were initially private for the 2016–17 season, allowing them to become a live pilot phase, with replay assistance implemented in the 2017–18 season. On the decision, FIGC President Carlo Tavecchio said, "We were among the first supporters of using technology on the pitch and we believe we have everything required to offer our contribution to this important experiment."

Serie A had logos that featured its sponsor Telecom Italia. The latest change of the logo was occurred in 2016.

In the past, individual clubs competing in the league had the rights to sell their broadcast rights to specific channels throughout Italy, unlike in most other European countries. Currently, the two broadcasters in Italy are the satellite broadcaster Sky Italia and terrestrial broadcaster Mediaset Premium for its own pay television networks; RAI is allowed to broadcast only highlights (in exclusive from 13:30 to 22:30 CET).
This is a list of television rights in Italy (until 2009–10):

For the 2010–11 and 2011–12 seasons, Serie A clubs negotiating club TV rights collectively rather than individually for the first time since 1998–99. The domestic rights for those two seasons were sold for billion to Sky Italia.

Global rights for the 2010–11 and 2011–12 seasons were sold for million to MP & Silva.

In countries and territories outside of Italy, the league is broadcast on:

In the 1990s, Serie A was at its most popular in the United Kingdom when it was shown on "Football Italia" on Channel 4, although it has actually appeared on more UK channels than any other league, rarely staying in one place for long since 2002. Serie A has appeared in the UK on BSB's The Sports Channel (1990–91), Sky Sports (1991–92), Channel 4 (1992–2002), Eurosport (2002–04), Setanta Sports and Bravo (2004–07), Channel 5 (2007–08), ESPN (2009–13), BT Sport (2013–2018) and Eleven Sports Network (from 2018).

Bold indicates clubs which will play in the 2018–19 Serie A.


Unlike La Liga, which imposed a quota on the number of non-EU players on each club, Serie A clubs could sign as many non-EU players as available on domestic transfer.

During the 1980s and 1990s, most Serie A clubs signed a large number of players from foreign nations (both EU and non-EU members). Notable foreign players to play in Serie A during this era included England internationals Paul Gascoigne and David Platt, France's Michel Platini and Laurent Blanc, Lothar Matthäus and Jürgen Klinsmann from Germany, Dutchmen Ruud Gullit and Dennis Bergkamp, and Argentina's Diego Maradona.

But since the 2003–04 season, a quota has been imposed on each of the clubs limiting the number of non-EU, non-EFTA and non-Swiss players who may be signed from abroad each season, following provisional measures introduced in the 2002–03 season, which allowed Serie A and B clubs to sign only one non-EU player in the 2002 summer transfer window.

In the middle of the 2000–01 season, the old quota system was abolished, which no longer limited each team to having more than five non-EU players and using no more than three in each match. Concurrent with the abolishment of the quota, the FIGC had investigated footballers that used fake passports. Alberto and Warley, Alejandro Da Silva and Jorginho Paulista of Udinese; Fábio Júnior and Gustavo Bartelt of Roma; Dida of Milan; Álvaro Recoba of Inter; Thomas Job, Francis Zé, Jean Ondoa of Sampdoria; and Jeda and Dede of Vicenza were all banned in July 2001 for lengths ranging from six months to one year. However, most of the bans were subsequently reduced.

The number of non-EU players was reduced from 265 in 2002–03 season to 166 in 2006–07 season. It also included players who received EU status after their respective countries joined the EU (see 2004 and 2007 enlargement), which made players such as Adrian Mutu, Valeri Bojinov, Marek Jankulovski and Marius Stankevičius EU players.

The rule underwent minor changes in August 2004, June 2005, June 2006. and June 2007.

Since the 2008–09 season, three quotas have been awarded to clubs that do not have non-EU players in their squad (previously only newly promoted clubs could have three quotas); clubs that have one non-EU player have two quotas. Those clubs that have two non-EU players, are awarded one quota and one conditional quota, which is awarded after: 1) Transferred 1 non-EU player abroad, or 2) Release 1 non-EU player as free agent, or 3) A non-EU player received EU nationality. Clubs with three or more non-EU players, have two conditional quotas, but releasing two non-EU players as free agent, will only have one quota instead of two. Serie B and Lega Pro clubs cannot sign non-EU player from abroad, except those followed the club promoted from Serie D.

Large clubs with many foreigners usually borrow quotas from other clubs that have few foreigners or no foreigners in order to sign more non-EU players. For example, Adrian Mutu joined Juventus via Livorno in 2005, as at the time Romania was not a member of the EU. Other examples include Júlio César, Victor Obinna and Maxwell, who joined Internazionale from Chievo (first two) and Empoli respectively.

On 2 July 2010, the above conditional quota reduced back to one, though if a team did not have any non-EU players, that team could still sign up to three non-EU players. In 2011 the signing quota reverted to two.

Serie A also imposed Homegrown players rule, a modification of Homegrown Player Rule (UEFA). Unlike UEFA, Serie A at first did not cap the number of players in first team squad at 25, meaning the club could employ more foreigners by increasing the size of the squad. However, a cap of 25 (under-21 players were excluded) was introduced to 2015–16 season (in 2015–16 season, squad simply require 8 homegrown players but not require 4 of them from their own youth team). In the 2016–17 season, the FIGC sanctioned Sassuolo for fielding ineligible player, Antonino Ragusa. Although the club did not exceed the capacity of 21 players that were not from their own youth team (only Domenico Berardi was eligible as youth product of their own) as well as under 21 of age (born 1995 or after, of which four players were eligible) in their 24-men call-up, It was reported that on Lega Serie A side the squad list was not updated.

In 2015–16 season, the following quota was announced.



Until 1993, Serie A matches were all played at the same time, on Sunday afternoon at 2:30 p.m. or 4:30 p.m. (depending on the number of hours of daylight). For the 1993–94 season, Lega Calcio made a notable change: a deferred match, scheduled for Sunday evening at 8:30 p.m. (8:45 p.m. from 2009–10) was made possible. This format was changed again in 1999–2000, due to the emergence of pay television in Italian football:


In 2004, due to the presence of 20 teams, it also became possible to play in midweek: on Wednesday evening, with some matches on Tuesday and others on Thursday (at 8:45). In 2010, a "lunch match" was introduced: a match played on Sunday at 12:30. Finally, for a few weeks, matches can be played on Friday or on Monday (in the evenings).




</doc>
<doc id="15501" url="https://en.wikipedia.org/wiki?curid=15501" title="Inhalant">
Inhalant

Inhalants are a broad range of household and industrial chemicals whose volatile vapors or pressurized gases are concentrated and breathed in via the nose or mouth to produce intoxication (called "getting high" in slang), in a manner not intended by the manufacturer. They are inhaled at room temperature through volatilization (in the case of gasoline or acetone) or from a pressurized container (e.g., nitrous oxide or butane), and do not include drugs that are sniffed after burning or heating. For example, amyl nitrite (poppers), nitrous oxide and toluene – a solvent widely used in contact cement and model airplane glue – are considered inhalants, but smoking tobacco, cannabis, and crack are not, even though these drugs are inhaled as smoke.

While a small number of inhalants are prescribed by medical professionals and used for medical purposes, as in the case of nitrous oxide (an anxiolytic and pain relief agent prescribed by dentists), this article focuses on inhalant use of household and industrial propellants, glues, fuels and other products in a manner not intended by the manufacturer, to produce intoxication or other psychoactive effects. These products are used as recreational drugs for their intoxicating effect. According to a 1995 report by the National Institute on Drug Abuse, the most serious inhalant abuse occurs among homeless children and teens who "...live on the streets completely without family ties." Inhalants are the only substance which is used more by younger teens than by older teens. Inhalant users inhale vapor or aerosol propellant gases using plastic bags held over the mouth or by breathing from a solvent-soaked rag or an open container. The practices are known colloquially as "sniffing", "huffing" or "bagging".

The effects of inhalants range from an alcohol-like intoxication and intense euphoria to vivid hallucinations, depending on the substance and the dose. Some inhalant users are injured due to the harmful effects of the solvents or gases or due to other chemicals used in the products that they are inhaling. As with any recreational drug, users can be injured due to dangerous behavior while they are intoxicated, such as driving under the influence. In some cases, users have died from hypoxia (lack of oxygen), pneumonia, cardiac failure or arrest, or aspiration of vomit. Brain damage is typically seen with chronic long-term use of solvents as opposed to short-term exposure.

Even though many inhalants are legal, there have been legal actions taken in some jurisdictions to limit access by minors. While solvent glue is normally a legal product, a Scottish court has ruled that supplying glue to children is illegal if the store knows the children intend to abuse the glue. In the US, thirty-eight of 50 states have enacted laws making various inhalants unavailable to those under the age of 18, or making inhalant use illegal.

Inhalants can be classified by the intended function. Most inhalant drugs that are used non-medically are ingredients in household or industrial chemical products that are not intended to be concentrated and inhaled. A small number of recreational inhalant drugs are pharmaceutical products that are used illicitly.

Another way to categorize inhalants is by their product category. There are three main product categories: solvents; gases; and medical drugs which are used illicitly.

A wide range of volatile solvents intended for household or industrial use are inhaled as recreational drugs. This includes petroleum products (gasoline and kerosene), toluene (used in paint thinner, contact cement and model glue), and acetone (used in nail polish remover). These solvents vapourize at room temperature. Ethanol (the alcohol which is normally drunk) is sometimes inhaled, but this cannot be done at room temperature. The ethanol must be converted from liquid into gaseous state (vapor) or aerosol (mist), in some cases using a nebulizer, a machine that agitates the liquid into an aerosol. The sale of nebulizers for inhaling ethanol was banned in some US states due to safety concerns.

A number of gases intended for household or industrial use are inhaled as recreational drugs. This includes chlorofluorocarbons used in aerosols and propellants (e.g., aerosol hair spray, aerosol deodorant). A gas used as a propellant in whipped cream aerosol containers, nitrous oxide, is used as a recreational drug. Pressurized canisters of propane and butane gas, both of which are intended for use as fuels, are used as inhalants.

Several medical anesthetics are used as recreational drugs, including diethyl ether (a drug that is no longer used medically, due to its high flammability and the development of safer alternatives) and nitrous oxide, which is widely used in the 2010s by dentists as an anti-anxiety drug during dental procedures. Diethyl ether has a long history of use as a recreational drug. The effects of ether intoxication are similar to those of alcohol intoxication, but more potent. Also, due to NMDA antagonism, the user may experience all the psychedelic effects present in classical dissociatives such as ketamine in forms of thought loops and feeling of mind being disconnected from one's body. Nitrous oxide is a dental anesthetic which is used as a recreational drug, either by users who have access to medical-grade gas canisters (e.g., dental hygienists or dentists) or by using the gas contained in whipped cream aerosol containers. Nitrous oxide inhalation can cause pain relief, depersonalisation, derealisation, dizziness, euphoria, and some sound distortion.

It is also possible to classify inhalants by the effect they have on the body. Solvents such as toluene and gasoline act as depressants, causing users to feel relaxed and sleepy. Many inhalants act primarily as asphyxiant gases, with their primary effect due to oxygen deprivation. Nitrous oxide can be categorized as a dissociative drug, as it can cause visual and auditory hallucinations. Other agents may have more direct effects at receptors, as inhalants exhibit a variety of mechanisms of action. The mechanisms of action of many non-medical inhalants have not been well elucidated. Anesthetic gases used for surgery, such as nitrous oxide or enflurane, are believed to induce anesthesia primarily by acting as NMDA receptor antagonists, open channel blockers that bind to the inside of the calcium channels on the outer surface of the neuron, and provide high levels of NMDA receptor blockade for a short period of time.

This makes inhaled anesthetic gases different from other NMDA antagonists, such as ketamine, which bind to a regulatory site on the NMDA-sensitive calcium transporter complex and provide slightly lower levels of NMDA blockade, but for a longer and much more predictable duration. This makes a deeper level of anesthesia achievable more easily using anesthetic gases but can also make them more dangerous than other drugs used for this purpose.

Inhalants can also be classified by chemical structure. Classes include:

Inhalant users inhale vapors or aerosol propellant gases using plastic bags held over the mouth or by breathing from an open container of solvents, such as gasoline or paint thinner. Nitrous oxide gases from whipped cream aerosol cans, aerosol hairspray or non-stick frying spray are sprayed into plastic bags. Some nitrous oxide users spray the gas into balloons. When inhaling non-stick cooking spray or other aerosol products, some users may filter the aerosolized particles out with a rag. Some gases, such as propane and butane gases, are inhaled directly from the canister. Once these solvents or gases are inhaled, the extensive capillary surface of the lungs rapidly absorb the solvent or gas, and blood levels peak rapidly. The intoxication effects occur so quickly that the effects of inhalation can resemble the intensity of effects produced by intravenous injection of other psychoactive drugs. Some harm reduction experts encourage glue sniffers to use paper bags rather than thin plastic bags, because plastic bags greatly increase the risk of suffocation, as plastic bags are more likely to stick to the users' nose and mouth while she or he is intoxicated.

Ethanol is also inhaled, either by vaporizing it by pouring it over dry ice in a narrow container and inhaling with a straw or by pouring alcohol in a corked bottle with a pipe, and then using a bicycle pump to make a spray. Alcohol can be vaporized using a simple container and open-flame heater. Medical devices such as asthma nebulizers and inhalers were also reported as means of application. The practice gained popularity in 2004, with marketing of the device dubbed AWOL (Alcohol without liquid), a play on the military term AWOL (Absent Without Leave). AWOL, created by British businessman Dominic Simler, was first introduced in Asia and Europe, and then in United States in August 2004. AWOL was used by nightclubs, at gatherings and parties, and it garnered attraction as a novelty, as people 'enjoyed passing it around in a group'. AWOL uses a nebulizer, a machine that agitates the liquid into an aerosol. AWOL's official website states that "AWOL and AWOL 1 are powered by "Electrical Air Compressors" while AWOL 2 and AWOL 3 are powered by "electrical oxygen generators"", which refer to a couple of mechanisms used by the nebulizer drug delivery device for inhalation. Although the AWOL machine is marketed as having no downsides, such as the lack of calories or hangovers, Amanda Shaffer of "Slate" describes these claims as "dubious at best". Although inhaled alcohol does reduce the caloric content, the savings are minimal. After expressed safety and health concerns, sale or use of AWOL machines was banned in a number of American states.

The effects of solvent intoxication can vary widely depending on the dose and what type of solvent or gas is inhaled. A person who has inhaled a small amount of rubber cement or paint thinner vapor may be impaired in a manner resembling alcohol inebriation. A person who has inhaled a larger quantity of solvents or gases, or a stronger chemical, may experience stronger effects such as distortion in perceptions of time and space, hallucinations, and emotional disturbances. The effects of inhalant use are also modified by the combined use of inhalants and alcohol or other drugs.

In the short term, many users experience headache, nausea and vomiting, slurred speech, loss of motor coordination, and wheezing. A characteristic "glue sniffer's rash" around the nose and mouth is sometimes seen after prolonged use. An odor of paint or solvents on clothes, skin, and breath is sometimes a sign of inhalant abuse, and paint or solvent residues can sometimes emerge in sweat.

According to NIH, even a single session of inhalant abuse "can disrupt heart rhythms and lower oxygen levels", which can lead to death. "Regular abuse can result in serious harm to the brain, heart, kidneys and liver."

Statistics on deaths caused by inhalant abuse are difficult to determine. It may be severely under-reported, because death is often attributed to a discrete event such as a stroke or a heart attack, even if the event happened because of inhalant abuse. Inhalant use or abuse was mentioned on 144 death certificates in Texas during the period 1988–1998 and was reported in 39 deaths in Virginia between 1987 and 1996 from acute voluntary exposure to abused inhalants.

Regardless of which inhalant is used, inhaling vapours or gases can lead to injury or death. One major risk is hypoxia (lack of oxygen), which can occur due to inhaling fumes from a plastic bag, or from using proper inhalation mask equipment (e.g., a medical mask for nitrous oxide) but not adding oxygen or room air. Another danger is freezing the throat. When a gas that was stored under high pressure is released, it cools abruptly and can cause frostbite if it is inhaled directly from the container. This can occur, for example, with inhaling nitrous oxide. When nitrous oxide is used as an automotive power adder, its cooling effect is used to make the fuel-air charge denser. In a person, this effect is potentially lethal. Many inhalants are volatile organic chemicals and can catch fire or explode, especially when combined with smoking. As with many other drugs, users may also injure themselves due to loss of coordination or impaired judgment, especially if they attempt to drive.

Solvents have many potential risks in common, including pneumonia, cardiac failure or arrest, and aspiration of vomit. The inhaling of some solvents can cause hearing loss, limb spasms, and damage to the central nervous system and brain. Serious but potentially reversible effects include liver and kidney damage and blood-oxygen depletion. Death from inhalants is generally caused by a very high concentration of fumes. Deliberately inhaling solvents from an attached paper or plastic bag or in a closed area greatly increases the chances of suffocation. Brain damage is typically seen with chronic long-term use as opposed to short-term exposure. Parkinsonism (see: Signs and symptoms of Parkinson's disease) has been associated with huffing.
Female inhalant users who are pregnant may have adverse effects on the fetus, and the baby may be smaller when it is born and may need additional health care (similar to those seen with alcohol – fetal alcohol syndrome). There is some evidence of birth defects and disabilities in babies born to women who sniffed solvents such as gasoline.

In the short term, death from solvent abuse occurs most commonly from aspiration of vomit while unconscious or from a combination of respiratory depression and hypoxia, the second cause being especially a risk with heavier-than-air vapors such as butane or gasoline vapor. Deaths typically occur from complications related to excessive sedation and vomiting. Actual overdose from the drug does occur, however, and inhaled solvent abuse is statistically more likely to result in life-threatening respiratory depression than intravenous use of opiates such as heroin. Most deaths from solvent abuse could be prevented if individuals were resuscitated quickly when they stopped breathing and their airway cleared if they vomited. However, most inhalant abuse takes place when people inhale solvents by themselves or in groups of people who are intoxicated. Certain solvents are more hazardous than others, such as gasoline.

In contrast, a few inhalants like amyl nitrate and diethyl ether have medical applications and are not toxic in the same sense as solvents, though they can still be dangerous when used recreationally. Nitrous oxide is thought to be particularly non-toxic, though heavy long-term use can lead to a variety of serious health problems linked to destruction of vitamin B12 and folic acid.

The hypoxic effect of inhalants can cause damage to many organ systems (particularly the brain, which has a very low tolerance for oxygen deprivation), but there can also be additional toxicity resulting from either the physical properties of the compound itself or additional ingredients present in a product. Organochlorine solvents are particularly hazardous; many of these are now restricted in developed countries due to their environmental impact.


Toxicity may also result from the pharmacological properties of the drug; excess NMDA antagonism can completely block calcium influx into neurons and provoke cell death through apoptosis, although this is more likely to be a long-term result of chronic solvent abuse than a consequence of short-term use.

Inhaling butane gas can cause drowsiness, narcosis, asphyxia, and cardiac arrhythmia. Butane is the most commonly misused volatile solvent in the UK and caused 52% of solvent-related deaths in 2000. When butane is sprayed directly into the throat, the jet of fluid can cool rapidly to −20 °C by adiabatic expansion, causing prolonged laryngospasm. Sudden sniffing death syndrome is commonly known as SSDS.
Some inhalants can also indirectly cause sudden death by cardiac arrest, in a syndrome known as "sudden sniffing death". The anaesthetic gases present in the inhalants appear to sensitize the user to adrenaline and, in this state, a sudden surge of adrenaline (e.g., from a frightening hallucination or run-in with aggressors), may cause fatal cardiac arrhythmia.

Furthermore, the inhalation of any gas that is capable of displacing oxygen in the lungs (especially gases heavier than oxygen) carries the risk of hypoxia as a result of the very mechanism by which breathing is triggered. Since reflexive breathing is prompted by elevated carbon dioxide levels (rather than diminished blood oxygen levels), breathing a concentrated, relatively inert gas (such as computer-duster tetrafluoroethane or nitrous oxide) that removes carbon dioxide from the blood without replacing it with oxygen will produce no outward signs of suffocation even when the brain is experiencing hypoxia. Once full symptoms of hypoxia appear, it may be too late to breathe without assistance, especially if the gas is heavy enough to lodge in the lungs for extended periods. Even completely inert gases, such as argon, can have this effect if oxygen is largely excluded.

Even though solvent glue is normally a legal product, there is a case where a court has ruled that supplying glue to children is illegal. Khaliq v HM Advocate was a Scottish criminal case decided by the High Court of Justiciary on appeal, in which it was decided that it was an offence at common law to supply glue sniffing materials that were otherwise legal in the knowledge that they would be used recreationally by children. Two shopkeepers in Glasgow were arrested and charged with supplying to children "glue-sniffing kits" consisting of a quantity of petroleum-based glue in a plastic bag. They argued there was nothing illegal about the items that they had supplied. On appeal, the High Court took the view that, even though glue and plastic bags might be perfectly legal, everyday items, the two shopkeepers knew perfectly well that the children were going to use the articles as inhalants and the charge on the indictment should stand. When the case came to trial at Glasgow High Court the two were sentenced to three years' imprisonment.

"Thirty-eight of 50 [US] states have enacted laws making various inhalants unavailable to those under the age of 18. Other states prohibit the sale of these items to anyone without recognition of purpose for purchase. Some states mandate laws against using these products for purposes of getting high, while some states have laws about possessing certain inhalants. Nearly every state imposes fines and jail terms for violation of their specific laws."

"Connecticut law bans the unauthorized manufacture or compounding, possession, control, sale, delivery, or administration of any "restricted substance". It defines restricted substances as... specific volatile substances if they are sold, compounded, possessed or controlled, or delivered or administered to another person for breathing, inhaling, sniffing, or drinking to induce a stimulant, depressant, or hallucinogenic effect. Violators can be fined up to $100." As well, 24 states "ban the use, possession, or sale or other distribution of inhalants... like glue and solvents."

"Louisiana prohibits the sale, transfer, or possession of model glue and inhalable toluene substances to minors. In Ohio, it is illegal to inhale certain compounds for intoxication — a common, general prohibition other states have enacted.
Some states draw their prohibitions more narrowly...In Massachusetts, retailers must ask minors for identification before selling them glue or cement that contains a solvent that can release toxic vapors."

"New Jersey... prohibits selling or offering to sell minors products containing chlorofluorocarbon that is used in refrigerant."

The sale of alkyl nitrite-based poppers was banned in Canada in 2013. Although not considered a narcotic and not illegal to possess or use, they are considered a drug. Sales that are not authorized can now be punished with fines and prison. Since 2007, reformulated poppers containing isopropyl nitrite are sold in Europe because only isobutyl nitrite is prohibited. In France, the sale of products containing butyl nitrite, pentyl nitrite, or isomers thereof, has been prohibited since 1990 on grounds of danger to consumers. In 2007, the government extended this prohibition to all alkyl nitrites that were not authorized for sale as drugs. After litigation by sex shop owners, this extension was quashed by the Council of State on the grounds that the government had failed to justify such a blanket prohibition: according to the court, the risks cited, concerning rare accidents often following abnormal usage, rather justified compulsory warnings on the packaging.

In the United Kingdom, poppers are widely available and frequently (legally) sold in gay clubs/bars, sex shops, drug paraphernalia head shops, over the Internet and on markets. It is illegal under Medicines Act 1968 to sell them advertised for human consumption, and in order to bypass this, they are usually sold as odorizers. In the U.S., originally marketed as a prescription drug in 1937, amyl nitrite remained so until 1960, when the Food and Drug Administration removed the prescription requirement due to its safety record. This requirement was reinstated in 1969, after observation of an increase in recreational use. Other alkyl nitrites were outlawed in the U.S. by Congress through the Anti-Drug Abuse Act of 1988. The law includes an exception for commercial purposes. The term "commercial purpose" is defined to mean any use other than for the production of consumer products containing volatile alkyl nitrites meant for inhaling or otherwise introducing volatile alkyl nitrites into the human body for euphoric or physical effects. The law came into effect in 1990. Visits to retail outlets selling these products reveal that some manufacturers have since reformulated their products to abide by the regulations, through the use of the legal cyclohexyl nitrite as the primary ingredient in their products, which are sold as video head cleaners, polish removers, or room odorants.

In the United States, possession of nitrous oxide is legal under federal law and is not subject to DEA purview. It is, however, regulated by the Food and Drug Administration under the Food Drug and Cosmetics Act; prosecution is possible under its "misbranding" clauses, prohibiting the sale or distribution of nitrous oxide for the purpose of human consumption as a recreational drug. Many states have laws regulating the possession, sale, and distribution of nitrous oxide. Such laws usually ban distribution to minors or limit the amount of nitrous oxide that may be sold without special license. For example, in the state of California, possession for recreational use is prohibited and qualifies as a misdemeanour. In New Zealand, the Ministry of Health has warned that nitrous oxide is a prescription medicine, and its sale or possession without a prescription is an offence under the Medicines Act. This statement would seemingly prohibit all non-medicinal uses of the chemical, though it is implied that only recreational use will be legally targeted. In India, for general anaesthesia purposes, nitrous oxide is available as Nitrous Oxide IP. India's gas cylinder rules (1985) permit the transfer of gas from one cylinder to another for breathing purposes. Because India's Food & Drug Authority (FDA-India) rules state that transferring a drug from one container to another (refilling) is equivalent to manufacturing, anyone found doing so must possess a drug manufacturing license.

Inhalant drugs are often used by children, teenagers, incarcerated or institutionalized people, and impoverished people, because these solvents and gases are ingredients in hundreds of legally available, inexpensive products, such as deodorant sprays, hair spray, contact cement and aerosol air fresheners. However, most users tend to be "...adolescents (between the ages of 12 and 17)." In some countries, chronic, heavy inhalant use is concentrated in marginalized, impoverished communities. Young people who become chronic, heavy inhalant abusers are also more likely to be those who are isolated from their families and community. The article "Epidemiology of Inhalant Abuse: An International Perspective" notes that "[t]he most serious form of obsession with inhalant use probably occurs in countries other than the United States where young children live on the streets completely without family ties. These groups almost always use inhalants at very high levels (Leal et al. 1978). This isolation can make it harder to keep in touch with the sniffer and encourage him or her to stop sniffing."

The article also states that "...high [inhalant use] rates among barrio Hispanics almost undoubtedly are related to the poverty, lack of opportunity, and social dysfunction that occur in barrios" and states that the "...same general tendency appears for Native-American youth" because "...Indian reservations are among the most disadvantaged environments in the United States; there are high rates of unemployment, little opportunity, and high rates of alcoholism and other health problems." There are a wide range of social problems associated with inhalant use, such as feelings of distress, anxiety and grief for the community; violence and damage to property; violent crime; stresses on the juvenile justice system; and stresses on youth agencies and support services.

Glue and gasoline sniffing is also a problem in parts of Africa, especially with street children. In India and South Asia, three of the most widely abused inhalants are the Dendrite brand and other forms of contact adhesives and rubber cements manufactured in Kolkata, and toluenes in paint thinners. Genkem is a brand of glue which had become the generic name for all the glues used by glue-sniffing children in Africa before the manufacturer replaced n-hexane in its ingredients in 2000.

The United Nations Office on Drugs and Crime has reported that glue sniffing is at the core of "street culture" in Nairobi, Kenya, and that the majority of street children in the city are habitual solvent users. Research conducted by Cottrell-Boyce for the African Journal of Drug and Alcohol Studies found that glue sniffing amongst Kenyan street children was primarily functional – dulling the senses against the hardship of life on the street – but it also provided a link to the support structure of the "street family" as a potent symbol of shared experience.

Similar incidents of glue sniffing among destitute youth in the Philippines have also been reported, most commonly from groups of street children and teenagers collectively known as "Rugby" boys, which were named after a brand of toluene-laden contact cement. Other toluene-containing substances have also been subject to abuse, most notably the Vulca Seal brand of roof sealants. Bostik Philippines, which currently owns the Rugby and Vulca Seal brands, has since responded to the issue by adding bitterants such as mustard oil to their Rugby line, as well as reformulating it by replacing toluene with xylene. Several other manufacturers have also followed suit.

Another very common inhalant is Erase-X, a correction fluid that contains toluene. It has become very common for school and college students to use it, because it is easily available in stationery shops in India. This fluid is also used by street and working children in Delhi.

In the UK, marginalized youth use a number of inhalants, such as solvents and propellants. In Russia and Eastern Europe, gasoline sniffing became common on Russian ships following attempts to limit the supply of alcohol to ship crews in the 1980s. The documentary "Children Underground" depicts the huffing of a solvent called Aurolac (a product used in chroming) by Romanian homeless children. During the Interbellum the inhalation of ether (etheromania) was widespread in some regions of Poland, especially in Upper Silesia—tens of thousands of people were affected by this problem.

In Canada, Native children in the isolated Northern Labrador community of Davis Inlet were the focus of national concern in 1993, when many were found to be sniffing gasoline. The Canadian and provincial Newfoundland and Labrador governments intervened on a number of occasions, sending many children away for treatment. Despite being moved to the new community of Natuashish in 2002, serious inhalant abuse problems have continued. Similar problems were reported in Sheshatshiu in 2000 and also in Pikangikum First Nation. In 2012, the issue once again made the news media in Canada. In Mexico, the inhaling of a mixture of gasoline and industrial solvents, known locally as "Activo" or "Chemo", has risen in popularity among the homeless and among the street children of Mexico City in recent years. The mixture is poured onto a handkerchief and inhaled while held in one's fist.

In the US, ether was used as a recreational drug during the 1930s Prohibition era, when alcohol was made illegal. Ether was either sniffed or drunk and, in some towns, replaced alcohol entirely. However, the risk of death from excessive sedation or overdose is greater than that with alcohol, and ether drinking is associated with damage to the stomach and gastrointestinal tract. Use of glue, paint and gasoline became more common after the 1950s. Abuse of aerosol sprays became more common in the 1980s, as older propellants such as CFCs were phased out and replaced by more environmentally friendly compounds such as propane and butane. Most inhalant solvents and gases are not regulated under drug laws such as the United States' Controlled Substances Act. However, many US states and Canadian cities have placed restrictions on the sale of some solvent-containing products to minors, particularly for products widely associated with sniffing, such as model cement. The practice of inhaling such substances is sometimes colloquially referred to as huffing, sniffing (or glue sniffing), dusting, or chroming.

Australia has long faced a petrol (gasoline) sniffing problem in isolated and impoverished aboriginal communities. Although some sources argue that sniffing was introduced by United States servicemen stationed in the nation's Top End during World War II or through experimentation by 1940s-era Cobourg Peninsula sawmill workers, other sources claim that inhalant abuse (such as glue inhalation) emerged in Australia in the late 1960s. Chronic, heavy petrol sniffing appears to occur among remote, impoverished indigenous communities, where the ready accessibility of petrol has helped to make it a common substance for abuse.

In Australia, petrol sniffing now occurs widely throughout remote Aboriginal communities in the Northern Territory, Western Australia, northern parts of South Australia and Queensland. The number of people sniffing petrol goes up and down over time as young people experiment or sniff occasionally. "Boss", or chronic, sniffers may move in and out of communities; they are often responsible for encouraging young people to take it up.

A 1983 survey of 4,165 secondary students in New South Wales showed that solvents and aerosols ranked just after analgesics (e.g., codeine pills) and alcohol for drugs that were abused. This 1983 study did not find any common usage patterns or social class factors. The causes of death for inhalant users in Australia included pneumonia, cardiac failure/arrest, aspiration of vomit, and burns. In 1985, there were 14 communities in Central Australia reporting young people sniffing. In July 1997, it was estimated that there were around 200 young people sniffing petrol across 10 communities in Central Australia. Approximately 40 were classified as chronic sniffers. There have been reports of young Aboriginal people sniffing petrol in the urban areas around Darwin and Alice Springs.

In 2005, the Government of Australia and BP Australia began the usage of opal fuel in remote areas prone to petrol sniffing. Opal is a non-sniffable fuel (which is much less likely to cause a high) and has made a difference in some indigenous communities.

One of the early musical references to inhalant use occurs in the 1974 Elton John song "The Bitch Is Back", in the line "I get high in the evening sniffing pots of glue." Inhalant use, especially glue sniffing, is widely associated with the late-1970s punk youth subculture in the UK and North America. Raymond Cochrane and Douglas Carroll claim that when glue sniffing became widespread in the late 1970s, it was "adopted by punks because public [negative] perceptions of sniffing fitted in with their self-image" as rebels against societal values. While punks at first used inhalants "experimentally and as a cheap high, adult disgust and hostility [to the practice] encouraged punks to use glue sniffing as a way of shocking society." As well, using inhalants was a way of expressing their anti-corporatist DIY (do it yourself) credo; by using inexpensive household products as inhalants, punks did not have to purchase industrially manufactured liquor or beer.

One history of the punk subculture argues that "substance abuse was often referred to in the music and did become synonymous with the genre, glue sniffing especially" because the youths' "faith in the future had died and that the youth just didn't care anymore" due to the "awareness of the threat of nuclear war and a pervasive sense of doom." In a BBC interview with a person who was a punk in the late 1970s, they said that "there was a real fear of imminent nuclear war—people were sniffing glue knowing that it could kill them, but they didn't care because they believed that very soon everybody would be dead anyway."

A number of 1970s punk rock and 1980s hardcore punk songs refer to inhalant use. The Ramones, an influential early US punk band, referred to inhalant use in several of their songs. The song "Now I Wanna Sniff Some Glue" describes adolescent boredom, and the song "Carbona not Glue" states, "My brain is stuck from shooting glue." An influential punk fanzine about the subculture and music took its name ("Sniffin' Glue") from the Ramones song. The 1980s punk band The Dead Milkmen wrote a song, "Life is Shit" from their album "Beelzebubba", about two friends hallucinating after sniffing glue. Punk-band-turned-hip-hop group the Beastie Boys penned a song "Hold it Now – Hit It", which includes the line "cause I'm beer drinkin, breath stinkin, sniffing glue." Pop punk band Sum 41 wrote a song, "Fat Lip", which refers to a character who does not "make sense from all the gas you be huffing ..." The song "Lança-perfume", written and performed by Brazilian popstar Rita Lee, became a national hit in 1980. The song is about chloroethane and its widespread recreational sale and use during the rise of Brazil's carnivals.

Inhalants are referred to by bands from other genres, including several grunge bands—an early 1990s genre that was influenced by punk rock. The 1990s grunge band Nirvana, which was influenced by punk music, penned a song, "Dumb", in which Kurt Cobain sings "my heart is broke/But I have some glue/help me inhale /And mend it with you". L7, an all-female grunge band, penned a song titled "Scrap" about a skinhead who inhales spray-paint fumes until his mind "starts to gel". Also in the 1990s, the Britpop band Suede had a UK hit with their song "Animal Nitrate" whose title is a thinly veiled reference to amyl nitrite. The Beck song "Fume" from his "Fresh Meat and Old Slabs" release is about inhaling nitrous oxide. Another Beck song, "Cold Ass Fashion", contains the line "O.G. – Original Gluesniffer!" Primus's 1998 song "Lacquer Head" is about adolescents who use inhalants to get high. Hip hop performer Eminem wrote a song, "Bad Meets Evil", which refers to breathing "...ether in three lethal amounts." The Brian Jonestown Massacre, a retro-rock band from the 1990s, has a song "Hyperventilation", which is about sniffing model-airplane cement. Frank Zappa's song "Teenage Wind" from 1981 has a reference to glue sniffing: "Nothing left to do but get out the 'ol glue; Parents, parents; Sniff it good now ..."

A number of films have depicted or referred to the use of solvent inhalants. In the 1980 comedy film "Airplane!", the character of McCroskey (Lloyd Bridges) refers to his inhalant use when he states, "I picked the wrong week to quit sniffing glue." In the 1996 film "Citizen Ruth", the character Ruth (Laura Dern), a homeless drifter, is depicted inhaling patio sealant from a paper bag in an alleyway. In the tragicomedy "Love Liza", the main character, played by Philip Seymour Hoffman, plays a man who takes up building remote-controlled airplanes as a hobby to give him an excuse to sniff the fuel in the wake of his wife's suicide.

Harmony Korine's 1997 "Gummo" depicts adolescent boys inhaling contact cement for a high. Edet Belzberg's 2001 documentary "Children Underground" chronicles the lives of Romanian street children addicted to inhaling paint. In "The Basketball Diaries", a group of boys are huffing carbona cleaning liquid at 3 minutes and 27 seconds into the movie; further on, a boy is reading a diary describing the experience of sniffing the cleaning liquid.

In the David Lynch film "Blue Velvet", the bizarre and manipulative character played by Dennis Hopper uses a mask to inhale amyl nitrite. In "Little Shop of Horrors", Steve Martin's character dies from nitrous oxide inhalation. The 1999 independent film "Boys Don't Cry" depicts two young low-income women inhaling aerosol computer cleaner (compressed gas) for a buzz. In "The Cider House Rules", Michael Caine's character is addicted to inhaling ether vapors.

In "Thirteen", the main character, a teen, uses a can of aerosol computer cleaner to get high. In the action movie "Shooter", an ex-serviceman on the run from the law (Mark Wahlberg) inhales nitrous oxide gas from a number of Whip-It! whipped cream canisters until he becomes unconscious. The South African film "The Wooden Camera" also depicts the use of inhalants by one of the main characters, a homeless teen, and their use in terms of socio-economic stratification. The title characters in "Samson and Delilah" sniff petrol; in Samson's case, possibly causing brain damage.

In the 2004 film "Taxi", Queen Latifah and Jimmy Fallon are trapped in a room with a burst tank containing nitrous oxide. Queen Latifah's character curses at Fallon while they both laugh hysterically. Fallon's character asks if it is possible to die from nitrous oxide, to which Queen Latifah's character responds with "It's laughing gas, stupid!" Neither of them suffered any side effects other than their voices becoming much deeper while in the room.

In the French horror film "Them", (2006) a French couple living in Romania are pursued by a gang of street children who break into their home at night. Olivia Bonamy's character is later tortured and forced to inhale aurolac from a silver-colored bag. During a flashback scene in the 2001 film "Hannibal", Hannibal Lecter gets Mason Verger high on amyl nitrite poppers, then convinces Verger to cut off his own face and feed it to his dogs.

The science fiction story "Waterspider" by Philip K. Dick (first published in January 1964 in "If" magazine) contains a scene in which characters from the future are discussing the culture of the early 1950s. One character says: "You mean he sniffed what they called 'airplane dope'? He was a 'glue-sniffer'?", to which another character replies: "Hardly. That was a mania among adolescents and did not become widespread in fact until a decade later. No, I am speaking about imbibing alcohol."

The book "Fear and Loathing in Las Vegas" describes how the two main characters inhale diethyl ether and amyl nitrite. 

In the comedy series "Newman and Baddiel in Pieces", Rob Newman's inhaling gas from a foghorn was a running joke in the series. One episode of the "Jeremy Kyle Show" featured a woman with a 20-year butane gas addiction. In the series "It's Always Sunny in Philadelphia", Charlie Kelly has an addiction to huffing glue. Additionally, season nine episode 8 shows Dennis, Mac and Dee getting a can of gasoline to use as a solvent, but instead end up taking turns huffing from the canister.

A 2008 episode of the reality show "Intervention" (season 5, episode 9) featured Allison, who was addicted to huffing computer duster for the short-lived, psychoactive effects. Allison has since achieved a small but significant cult following among bloggers and YouTube users. Several remixes of scenes from Allison's episode can be found online. Since 2009, Allison has worked with drug and alcohol treatment centers in Los Angeles County. In the third episode of season 5 of "American Dad!", titled "Home Adrone", Roger asks an airline stewardess to bring him industrial adhesive and a plastic bag. In the seventh episode of the fourteenth season of South Park, Towelie, an anthropomorphic towel, develops an addiction to inhaling computer duster. In the show "Squidbilles", the main character Early Cuyler is often seen inhaling gas or other substances.





</doc>
<doc id="15505" url="https://en.wikipedia.org/wiki?curid=15505" title="Iceman (Marvel Comics)">
Iceman (Marvel Comics)

Iceman (Robert Louis "Bobby" Drake) is a fictional superhero appearing in American comic books published by Marvel Comics and is a founding member of the X-Men. Created by writer Stan Lee and artist Jack Kirby, the character first appeared in "The X-Men" #1 (September 1963).

Iceman is a member of a subspecies of humans known as mutants, who are born with superhuman abilities. He has the ability to manipulate ice and cold by freezing water vapor around him. This allows him to freeze objects, as well as turn his body into ice.

The character has been frequently present in X-Men and Spider-Man-related comics, video games, animated series, and movies. Shawn Ashmore portrayed Iceman in the "X-Men" films, and voices the character in "The Super Hero Squad Show".

Created by writer Stan Lee and artist/co-writer Jack Kirby, the character first appeared in "X-Men" #1 (September 1963). Lee later admitted that Iceman was created essentially as a copy of the Human Torch, only using the opposite element for his power.

Iceman was featured in two self-titled limited comic book miniseries, one in 1984-85 written by J. M. DeMatteis and another in the 2000s by Andy Lanning and Dan Abnett, with art by Karl Kerschl. DeMatteis said of the first series, "It was my idea, so there was no one to blame but myself. I'll just say that it was a mistake and if the series made any sense whatsoever it was due to [editor] Bob Budiansky. That was a case where the editor's input was really needed - and Bob was a big help."

A mainstay in most X-Men titles, Iceman has been a main character in both "Uncanny X-Men" and the second volume of "X-Men" and was also featured in "The Champions" from 1975 to 1978 and "The New Defenders" from 1983 to 1986 as a member. He was a main character in the first volume of "X-Factor", and a star in flashback stories when he was a teenager in "X-Men: The Hidden Years" and "X-Men: First Class".

In April 2015, in issue 40 of "All-New X-Men", a time-displaced version of the teenaged Iceman was revealed as gay by his teammate, Jean Grey, who discerned this with her telepathic ability. This raised questions, because the character's adult, present-day counterpart had previously been portrayed dating women. In "Uncanny X-Men" #600, which was published in November that year, the young Iceman confronts his older self, who confirms that he is gay as well but repressed his true self, not wanting to be both gay and a mutant. In 2017, Iceman received his first ongoing solo series, which focused on the adult Bobby Drake coming to terms with life as an out gay man, his Omega-level superpowers, his legacy as a hero and fighting some of the biggest villains in the Marvel Universe. The book had been cancelled, with its last issue being in early 2018.

A new book written by Sina Grace will be a part of the Fresh Start initiative.

Robert Louis "Bobby" Drake was born in Floral Park, Long Island, New York, to William Robert Drake and Madeline Beatrice Bass-Drake. His father is Irish-American Catholic, and his mother is Jewish. Bobby's powers first manifested when he was on a date with Judy Harmon, and a local bully by the name of Rocky Beasely tried to take Judy away for himself. Knowing Judy could not put up a good fight, Bobby pointed his hand at Beasely and encased him in a block of ice. Later, the local townspeople, having heard of the incident, came looking for him in the form of an angry mob. The local sheriff had no choice but to put Bobby in jail for his own "protection". While Bobby sat in his cell at the sheriff station, the outer wall was blown open, and a young man named Scott Summers walked in and offered to take Bobby with him. After Bobby turned him down, the two mutants got into a short battle, which was soon ended by the arrival of Professor Charles Xavier.

After Xavier spoke with Bobby and his parents, Bobby's parents suggested that he go with Professor Xavier to his "school for gifted youngsters". Bobby took the suggestion and left with Professor Xavier and Cyclops to become the second member of the X-Men. He is later joined by Henry "Hank" McCoy, Jean Grey, and Warren Worthington III as the founding members of the X-Men. Drake remains self-conscious regarding the fact that he is the youngest member of the group. Appearing in his original snow covered form, he first battles Magneto along with the rest of the team, and later the Brotherhood of Evil Mutants. Bobby Drake's first girlfriend is Zelda. Not long after, he takes on a new ice-covered form. He then teams up with the Human Torch for the first time. The two would become close friends as time went on. With the X-Men, he visits the Savage Land and meets Ka-Zar for the first time. He then battles the Juggernaut, and is badly injured in his first battle against the Sentinels. He next battles Magneto by himself. Later, he visits Subterranea for the first time. Then, he and Beast battle the Maha Yogi. During his original stint with the X-Men, Drake pursues a relationship with Lorna Dane, although the relationship does not last. Iceman is among the original X-Men captured by Krakoa, leading to a new incarnation of X-Men of which he is not a member. With most of the original team, he quits the X-Men.

Iceman moves to the American west coast to attend UCLA and becomes a founding member of The Champions of Los Angeles. However, the Champions soon dissolve.

Iceman is then abducted by Master Mold, and alongside Angel, he encounters the Hulk. Iceman next aids the Thing in battling the Circus of Crime. Drake retires from life as a superhero to earn a college degree in accounting - but apparently at a college on the east coast, not UCLA. While in college, he briefly rejoins the X-Men to rescue the captives of Arcade's henchman, Miss Locke.

Iceman is reunited with Beast, encounters Cloud, and then returns as a full-time superhero in an incarnation of the Defenders alongside his former teammates, Angel and Beast. He also battles Professor Power's Secret Empire while with the Defenders. After the Defenders disband, Drake embarks on his career as an accountant.

Some time later, Iceman encounters Mirage, the "daughter" of Oblivion. Iceman journeys back in time and meets his parents before he was born, and battles Oblivion and Mirage. He then reconciles with his parents.

The original X-Men, including Iceman, reunite to form the superhero team X-Factor. With this new team, he encounters Apocalypse for the first time.

During his time with the team, Loki captures Bobby, hoping to use him to gain control over the Frost Giants. Loki enhances Bobby's powers and then extracts them to restore the size of the Frost Giants. Iceman is rescued by Thor. Loki's tampering increases Bobby's powers to such an extent that he begins to lose control of his abilities. During a later battle with the Right, he is fitted with a power-dampening belt which actually helps him control his abilities. Once able only to sheathe his own body in a protective coating of ice, Bobby finds he can encase the entirety of the Empire State Building. With time, Bobby gains sufficient control over his augmented powers and is able to stop using the inhibitor belt. Believing he has achieved his full potential, Bobby does not attempt to develop his abilities further.

With X-Factor, Bobby then defeats Apocalypse's Horsemen. Iceman helps watch over many of the younger superheroes, something he once was. Most notably, he and Beast help Boom Boom gain a more normal life. For a brief while, he also helps supervise the New Mutants and their sister team, the X-Terminators. They, in turn, save him from the deadly kiss of Infectia.

Bobby also develops a romantic relationship with Opal Tanaka. After a session of ice sledding, she discovers threatening mail in her mailbox, a precursor to harassment by her cybernetically-enhanced relatives of the Tatsu Clan of the Yakuza, which Bobby helps her out with.

After the "Muir Island Saga", Iceman rejoins the X-Men along with the rest of X-Factor.

When he rejoined the X-Men it seemed that he learned to control his powers and no longer needed the inhibitor belt, this made him believe he reached the limit of his powers. The X-Men were separated into two groups, Iceman was placed in the gold team, led by Storm, along with fellow original X-Men Jean Grey (now sans codename) and Archangel.

One day he took Opal to eat with his parents, however his dad began humiliating her because of her Japanese heritage. The four are attacked by the Cyber-Samurai, which added to William Drake's prejudices about the girl. When Bobby came across Mikhail Rasputin he used his mutant abilities on him. Bobby discovered that his potential was still far from being reached as he converted his body into ice, not just covered by it. By turning his entire body to ice, instead of just wearing an icy exterior, Bobby now was capable of using his power in new, aggressive ways, adding spikes and padding to his ice structure.

Too busy with the many threats that the X-Men faced every day, Bobby let his relationship with Opal deteriorate and, when they finally saw each other again after weeks, it was only to save her from an attack by mutant haters. Annoyed that she could only gain his attention by nearly getting killed, Opal broke off their relationship.

Later, as he was checking on Emma Frost who was in a comatose state after the mutant Trevor Fitzroy unleashed the mutant-hunting Sentinels on Emma Frost and her students known as the Hellions, the mansion was hit by an electricity breakdown. Emma woke up disoriented, possessed Bobby's mind, and used his powers in ways Bobby never had; she froze an entire river and traveled through water. She was looking for her pupils but after finding out they were dead, she left Bobby's body.

Bobby invites Rogue to tag along on a visit to his parents. Wrongly assuming a romantic relation, his father disapproves of Rogue, verbally attacking them with the same prejudices he expressed with Opal. This time, Bobby had enough and left after telling his father that he should just accept the fact that he is a mutant and he would never fit the definition he has of normal. He was upset that Emma exhibited greater control of his powers than he had. Since Rogue was having problems with Gambit, the two of them go on a road trip to ease their minds.

During the Legion Quest, Iceman was one of the X-Men who went back in time to stop Legion from killing Magneto. They succeed, but only partially. Legion does not kill Magneto, but instead accidentally murdered Xavier, his own father, years before Legion had been conceived much less born. This paradox caused the events of the Age of Apocalypse. At the last moment before the original reality ended, Iceman's fellow X-Men, Rogue and Gambit shared a kiss. When reality resumed, Rogue's mind-absorbing touch renders Gambit comatose. Having absorbed some secret haunting memories, she needs to get away from the X-Men and Iceman volunteers to join her on a road trip, though at the same time he was starting to see visions of Emma Frost. When Gambit awoke from his coma, he tracked them down and confronted Rogue about what she saw in his mind. She broke off their relationship officially and left for good. Iceman and Gambit returned to the X-Men.

When the entity known as Onslaught first appeared, Iceman was chosen as one of his test subjects, along with Storm, Cyclops and Wolverine. They were pitted against a servant of Onslaught named Post, in a specific battle area of harsh environment to test the extent of their abilities. They won and were returned to the mansion. However, Iceman's chest had been shattered in his ice form during battle, making it impossible to change back to human form. He confronted Emma Frost and demanded to know what she did to his powers in his body and how to save himself. She refused to help since she knew that Bobby would have to do it himself. When he captured her with his ice powers, she telepathically showed him his insecurities. By confronting Opal and his father in her simulation, Iceman realized that Emma was right and managed to transform back to his human body with his chest fully intact.

When Graydon Creed was running for President (with a heavy anti mutant campaign), Bobby was chosen to infiltrate in the campaign. His father stood out in a crowd and spoke in favor of the mutants, which came as a surprise to Bobby. His father's connection with Bobby was discovered, so the people that worked for Graydon captured him and almost beat him to death. Bobby decided to stay away from the X-Men for a while to be with his dad.

Zero Tolerance came across and Bobby found and helped Cecilia Reyes who was trying to keep a secret that she was a mutant. They also joined with Charlotte Jones and the morlock Marrow. After Bastion was defeated, he took Cecilia and Marrow to the mansion, and soon left the X-Men again for a while to be with his parents.

Much later, the X-Men found evidence in one of Destiny's journals of a group known as the Twelve, including Xavier, Magneto, Cyclops, Phoenix, Iceman, Polaris, Storm, Cable, Bishop, Sunfire, Mikhail Rasputin and the Living Monolith. They also learned that the Apocalypse’s Horsemen had been kidnapping these mutants from around the globe. Iceman was captured in the woods near his home by Deathbird, who had become the Horseman War. Gathering the remaining Twelve, the X-Men traveled to Egypt and confronted Apocalypse and the Skrulls. Apocalypse and his forces captured all of the Twelve during the battle, using them in a ritual to give the Chaos-Bringer a new body and incredible power. Magneto and Polaris created opposite magnetic polarities, Iceman, Storm, and Sunfire provided elemental extremes, Cyclops, Phoenix, and Cable gave the sheer power of family, Xavier represented the power of mind and Bishop and Mikhail stood for time and space, while the Monolith linked all their energies together. Nate Grey was to be Apocalypse's new host, a powerhouse to store his massive lifeforce. The Twelve managed to free themselves and Cyclops sacrificed his own body and lifeforce to keep Apocalypse from getting Nate. Though the new Apocalypse was defeated, Cyclops seemed lost forever.

After that incident, Iceman helped the Beast to secretly enter Genosha, as he wanted to study the legacy-infected mutates. When the High Evolutionary released his anti-mutation wave, they were trapped in the war-ravaged country. With the aid of Magneto, they escaped and joined forces with the rest of the scattered X-Men. They raided the Evolutionary's satellite, disabled the mutation field and defeated Sinister, who had been manipulating the Evolutionary.

Iceman was recruited by the living ship Prosh, along with other mutants, like Jean Grey, Mystique, Toad and Juggernaut, to preserve evolution and save it. In this journey Iceman developed his powers even further, which led him to no longer be afraid of the natural course of his powers and he returned to the X-Men. He joined a new team of X-Men, consisting of Angel, Nightcrawler, Wolverine and Chamber. He was using his abilities in a whole new way now, just channeling the power and not turning his body into ice.

During a heated battle with a recently evolved Black Tom Cassidy, he received a chest wound. After returning to normal his chest did not fully recover and some parts of it remained icy, and he was unable to return them to normal. At first he became afraid of it but in time it made him gain a new attitude in life, even rude at some times.

Nobody seemed to realize it at the time, but evidently this was an early sign of Iceman undergoing a secondary mutation of his own. When he repeatedly tried to evade his regular medical check-ups, school nurse Annie Ghazikhanian recognized that something odd was going on with him and pressured him to show her the wound. Bobby made her promise not to tell anyone and showed her that parts of his chest are now made of ice and he is unable to change them back to flesh and blood. Iceman wonders if he will entirely turn into ice on a permanent basis.

He developed an attitude that, to some of the newer addition to the X-Men, like Stacy X, Juggernaut and Northstar, he comes across as rather arrogant, denouncing their status as team members, as they have not been around as long as he. Iceman even went as far as offending Nightcrawler by claiming that only the original five, and no one else, has the right to call themselves an X-Man.

Bobby was further frustrated when Havok and Polaris announced their imminent wedding, him having carried a torch for Lorna for quite some time. In his frustration he turned to Annie, who had problems with the wedding too, as she was secretly in love with Havok. The nurse surprised him with the accusation of him being a racist - feeling comfortable as a mutant who could pass like a human when needed, opposed to being a fully obvious mutant or a "mere" human. With his secondary mutation manifesting, though, Iceman was in danger of losing this status. Shocked by the truth of her words, Bobby fully opened up about his fears, that as a man fully made of ice he could never feel the warmth of a physical relationship again. Touched, Annie allowed him to kiss her, but when Havok called off the wedding, wanting to be with Annie instead, she quickly dumped Bobby.

Upon encountering Azazel and his followers, Iceman's body is shattered from the neck down. Afterward, he regains his entire ice form, but cannot change back to his human appearance. As a result, Bobby becomes both bitter and despondent because of this drastic change.

Iceman joins Rogue's team after she tells him that she wants him as a member. They develop a romantic relationship after she and Gambit break up. Their first mission as a team is to fight a new threat, a powerful group known as the Children of the Vault. The team is successful and during this time, Bobby learns that he can be completely destroyed but then pull himself back together again. It was shown several times during the arc.

The next mission for the team was to locate a man called Pandemic and defeat him. The team was again successful, but Rogue was infected with a virus called Strain 88. Cable took the team, including Bobby, to his island so Rogue could get treatment.

While on Cable's island, the team and Iceman began working to defeat the Shi'ar weapon known as the Hecatomb. During the chaos, he shared a passionate kiss with Mystique. Even as he did so, he saved many lives by containing the explosion of the Conquistador, and, later, the Hecatomb itself.

As the team recovers from Hecatomb attack in Rogue's childhood home, it appears that Mystique and Iceman begin a romantic relationship. This was a front, as Mystique was using Iceman and the X-Men as a Marauder spy for Mister Sinister. Marauders soon infiltrated the house; they attempt to gain access to Destiny's Diaries on the order of Mr. Sinister (who has been gathering information about the future from anybody and anything that could foretell the future). Bobby and Cannonball escape from the Marauders in the X-Jet, with help from Emma Frost. They are pursued by Sunfire; they manage to get the better of him and take him prisoner, but not before he manages to cripple the jet. While Sunfire is unconscious, Iceman and Sam discuss the Mauraders' plan to eliminate all precognitive mutants and anyone with knowledge of the future as well as retrieving Destiny's Diaries before the Marauders can. During this time, Bobby displayed sub-atomic control of energy transfers when he prevented Sunfire from using his fire-based powers.

Cannonball and Bobby, telepathically prompted by Emma Frost, attempt to recover the diaries which are hidden in a dilapidated brewery. Mr. Sinister uses the reverse-engineered version of Xavier's Cerebro to track the pair of X-Men to the brewery. The Marauders attack Cannonball and Iceman and overtake them. Bobby, while in his ice form, suffers a gunshot wound from Mystique, which severs one of his arms above the elbow. Mister Sinister, who takes Cannonball prisoner, attempts to telepathically erase his mind so that the X-Men will find him as an empty shell. Iceman attacks Sinister, distracting him, which allows both of the X-Men to escape.

The New X-Men team decided to raid the headquarters of the Purifiers in Washington, D.C., but were forced to retreat. Pixie teleported them back to the mansion in a rush, but the entire team was scattered between D.C. and Westchester. Iceman, after recovering from his injuries, volunteered to go look for them and was given telepathic directions by Emma Frost.

Iceman was successful in finding the New X-Men, most of them injured. On the way back, they found that the O*N*E* Sentinels guarding the Xavier Institute had become infected by nano-Sentinels and attacked the school. Iceman and New X-Man X-23 helped out in the battle with the O*N*E* Sentinels. With the help of Dust and X-23, the X-Men were able to survive this battle but the nano-Sentinel infected human escaped.

Soon, Iceman participated in the final battle against the Marauders, the Acolytes, and Predator X. He was one of the X-Men who came running in to fight Predator X after it swallowed Wolverine whole. However, he also witnesses his mentor, Professor Xavier, "killed" by Bishop's bullet, which was not meant for him.

Iceman arrives in San Francisco, where he was set to meet Hepzibah, Warpath, and Angel. All four are caught in the effects of a citywide illusion created by Martinique Jason, who used her powers to transform the city into a hippie paradise. Now calling himself "Frosty", he and the others are sent by Martinique to confront Scott Summers and Emma Frost. Emma Frost is able to break up the illusion and free everyone. They eventually set up their base of operations in San Francisco as X-Men.

Iceman is one of the X-Men that assists in fighting the Skrull invasion in San Francisco.

Iceman rescues Colossus from Venom and is later made a part of a team to battle Emma Frost's Dark X-Men. During the final battle on Utopia, Iceman teams up with the X-Students to take on Mimic. Iceman had the labor of providing water to the population. He attempted to use humor to keep everyone's spirits up even though he believed that the situation was helpless and that the X-Men were living in the last days of mutantkind. Bobby then helped defeat Predator X and also helped stop Selene's resurrected army's invasion of Utopia.

During the battle with Bastion's Nimrod Sentinels, Iceman is severely injured following an attack from one of them that reversed his ice form and left him with burns on his body. He is only there for a short time because his mutant powers help him minimize the injuries he suffered and he is seen back in battle alongside Psylocke and the other X-Men.

Early solicitations show that after Wolverine and Cyclops have a major falling out, Wolverine decides to branch off and open "The Jean Grey School for Higher Learning" back in New York. Iceman is the first person Wolverine approaches and recruits to his new X-Men squad as both a professor and teammate. Iceman is chosen because Wolverine feels he has the kind of spirit the new school needs. He also has an off-and-on romantic relationship with Kitty Pryde.

When the Terrigen became toxic for mutants, Iceman was one of the recruits in Storm's new team dedicated to protecting mutants from the Terrigen by leading them to X-haven. Iceman mostly works alongside Nightcrawler, helping him search for Colossus after he is transformed into one of Apocalypse's new horsemen. Iceman and Nightcrawler manage to track Colossus to Egypt, where he ambushes them and almost kills them until another squad of X-Men comes in to help. After that Iceman joins the X-Men when they declare war against the Inhumans after discovering that in a matter of weeks the Terrigen will render earth as completely uninhabitable for mutants.

Following the war with the Phoenix Five, as Cyclops begins to lash out against government oppression of mutants, a chance comment by Bobby about how the old Cyclops wouldn't tolerate what he is currently doing inspires Beast to travel back in time and recruit to stop Cyclops. The team decides to stay in the present instead of returning into the past. They are now called the All-New X-Men and led by Kitty Pryde. The past and present Bobby are particularly shocked when they see each other. The younger Bobby is especially shocked by the older Bobby's Omega level powers, like creating ice golems, and especially his future "Ice Wizard" self in the Battle of the Atom. Eventually he and the All-New X-Men and the Guardians of the Galaxy travel to the Shiar Empire to rescue Jean Grey from a trial for the genocide that her future Dark Phoenix self committed. The team is then teleported into the Ultimate Marvel universe, where he stumbles into Mole Man's lair. He proudly creates his first ice golem in order to escape.

Returning to their universe, the team are reunited with Cyclops when they are called upon to aid the Guardians of the Galaxy again in retrieving the fabled Black Vortex. Cyclops, Iceman and Groot become superpowered by the Vortex before returning it to Captain Marvel.

Later, the younger, time-displaced Bobby is forced to confront being gay by his teammate, Jean, who privately asks him why he calls women "hot", when she knows via her psychic abilities that he is gay. This causes the younger Bobby to speculate as to the complicated identity issues faced by his older self and the decisions his older self may have made in the time between them. Together with the young Jean, the young Bobby confronts his older self, who admits to being gay, having 'concealed' that part of himself so that he could have avoid being prejudiced against for another part of himself.

After the X-Men's war against the Inhumans for the Terrigen ends, Iceman joins the rest of the young X-Men on an attempted return to their original timeline but they quickly realize that theirs isn't part of the Earth 616 timeline, leaving them stuck in that present time with no knowledge where they were originally from. Upon learning this, Iceman joins the rest of the young X-Men and leaves the rest of the X-Men to find their place in the world. Iceman then joins Magneto in Madripoor along with the rest of the time-displaced X-Men, however due to a lack of trust in their new leader the X-Men make plans and train in case Magneto returns to his former villainous ways to kill them. Instead of training with his teammates, Iceman spends most of his time trying to reach out to his new Inhuman boyfriend Romeo unsuccessfully, placing him into a stupor.

Iceman possesses the power to instantly decrease the temperature of ambient water vapor in his immediate environment to below zero degrees Celsius, thereby freezing it into ice. He is able to make ice that will not break unless he wills it to. In this manner he is able to quickly form a great variety of ice structures, including projectiles, shields, ladders, baseball bats, etc. Iceman often makes ice slides which form rapidly beneath and behind his feet, moving him along the slick surface at high speeds. He is also able to form exceedingly complicated structures within relative short time, such as miniature cities. Originally, Iceman's own body temperature would lower dramatically when his powers were active, reaching within a few tenths of a second (now his body usually converts to organic ice; see below). Iceman is immune to sub-zero temperatures; he is also able to perceive the thermal energy level of objects around him. Because cold is the absence of heat, Iceman does not actually 'emanate' cold; rather, he decreases thermal energy. As mentioned by writer Mike Carey, Iceman is "an Omega-level mutant...[and] has powers that can influence the ecosystem of the entire world." Iceman has yet to tap into his full mutant potential, but over the years he has taken more interest in developing his abilities.

In his early appearances, Iceman generally covered his body in a thick layer of what appeared to be snow; hence he looked more like a traditional snowman than an ice-man. Upon further training in the use of his powers, he was able to fashion an armor of solid ice around his body when using his powers, which afforded him some degree of protection against concussive force and projectiles. Later on, he manifested the ability to convert the tissue of his body into organic ice. He sometimes augments his organic ice form with razor sharp adornments to his shoulders, elbows, knees, and fists. Iceman has also been able to move rapidly to another distant location while in his organic ice form, being able to deposit his bodily mass into a river and reconstitute his entire mass a great distance away in a matter of minutes (by temporarily merging his molecules with those of the river). On one occasion, Iceman suffered a severe chest injury while in his ice form and was able to heal himself by converting back into his normal human form.

Iceman is also able to reconstitute his organic ice form if any part of it is damaged, or even if it is completely shattered, without permanently harming himself. He can temporarily add the mass of a body of water to his own, increasing his mass, size, and strength. He can survive not only as ice, but as liquid water and water vapor. He can also transform his body from a gaseous state back to a solid, although it is physically and mentally taxing. Iceman can also freeze sea water, as seen during the "" story arc. While he usually does not use his powers in lethal ways, his powers are so vast that they extend to the molecular level, to the point that he can freeze all of the molecules of an object/being with a thought; he once froze every single molecule of water within the body of David Haller. Iceman is also able to dissolve his own icy constructs.

Iceman's powers were pushed to their limit while possessed by Emma Frost, who used Iceman to discover the fate of her Hellions. During this time Iceman was able to control all forms of moisture, freeze fluids inside people's bodies, travel as a liquid, solid or gas. Not even the combined might of the X-Men Gold team was able to stop Emma Frost in Iceman's body. Following this, Bobby confronted Emma about how she was able to use his powers so effectively. While together they made some initial progress, she refused to train him further. Instead he turned to Storm because they share similar elemental powers and she agreed to tutor him.

When Iceman was injected with Mister Sinister's neuro-inhibiter by Mystique, he was able to save himself by drawing in all of the ambient moisture around him, rapidly replacing his poisoned cells with healthy material before the injection could kill him.

During the 2013–2014 "Battle of the Atom" storyline, Iceman's future self revealed that he has the ability to create semi-independent ice structures that can act on their own, although one of these structures—demonstrating a Hulk-like physique and intellect—has gone on to join the future version of the Brotherhood.

Aside from his superhuman powers, Iceman is also a fair hand-to-hand combatant, and received combat training at Xavier's School as well as coaching from the Black Widow and Hercules while serving with the Champions of Los Angeles. Iceman has taken as much combat training as Cyclops or Beast.

According to writer Mike Carey "one of Iceman's best personality traits is that emotionally Bobby Drake is like the ice he manipulates -- not cold but transparent. 'He's devastatingly honest. He is very up-front with his emotions and his thoughts all the time.'" "Also, he's obviously incredibly brave both in terms of facing external, physical danger as well as facing up to unpleasant situations and admitting his own mistakes."

Iceman had a brief relationship with a Japanese-American New Yorker named Opal. He subsequently exhibited strong feelings for his fellow X-Man Polaris, but she did not return those feelings, due to her feelings towards Havok. Northstar developed an unrequited crush for Iceman during their time on the same team, though Iceman never did find out about this. Iceman later had a brief relationship with the Xavier School's nurse, Annie, but she eventually left him for Havok, who had just left Polaris at the altar. When Iceman attempted to rekindle his relationship with Polaris, that too ended abruptly, and Polaris returned to Havok. Iceman then had a relationship with the X-Men's enemy Mystique, who later betrayed him, despite her continued fixation on him, in as much as she stated that she would either kill him or cure him of his personal uncertainty.

In "All-New X-Men" #40, the time-displaced Iceman is revealed to be gay, which eventually forces the youth to confront the adult version of himself about the matter. As both speak, the adult Iceman confirms the fact and that he had put all his energy into just being an X-Man as he couldn't cope with being a mutant and gay simultaneously. With the help of his younger self and Jean Grey, however, he finally comes to terms with his own sexuality, and comes out to fellow gay X-Man Anole in "Extraordinary X-Men" #6.

Iceman is Roberto Trefusis in the miniseries "Marvel 1602", a member of the group of "witchbreeds" founded by Carlos Javier and led by Scotius Summerisle. He is the nephew of naval commander Sir Francis Drake. As in the Marvel Universe, he generates ice and can assume a physical ice form.

In the "Age of Apocalypse" storyline, Bobby, along with the rest of the X-Men, is trained by Magneto. Because Magneto is harder on his students than Professor X, Bobby lacks his 616-counterpart's sense of humor. Instead, Bobby becomes very cold and inhuman, making his teammates feel uncomfortable. In addition to his normal abilities, Bobby is capable of breaking down his body and merging it with another body of water to travel great distances in a matter of seconds. He can bring others along through a process that he calls "moisture molecular inversion", though it is a painful process for the passengers. Bobby is also able to reconstitute his body from broken pieces. Just before Apocalypse's defeat, Colossus stormed right through Iceman, causing him to fall into pieces in an attempt to reach his sister. A couple of months later, Iceman, Exodus, Wild Child, and Morph were sent on a secret mission by Magneto; for a time, only Wild Child's fate was revealed.

Iceman returned in the "Uncanny X-Force" arc "The Dark Angel Saga", when the title team was forced to travel to the AoA reality. He abandoned the X-Men during a battle with the minions of Weapon X, who had by this time been transformed into that world's new Apocalypse, a defection later revealed to be the result of Iceman's having lost faith in his X-Men's ability to save their world. He was soon brought to Earth-616 by the Dark Beast, where he joined forces with Archangel on his quest to eradicate all life on Earth so he could create a new evolution process. Tired of seeing the people he cared about being killed fighting what he thought to be an unwinnable war, Drake agreed to help Archangel - despite the latter's now being the mainstream Marvel Universe's version of Drake's X-Men's arch-enemy, Apocalypse - in exchange for transport to the relative paradise that was Earth-616.

After X-Force defeated Archangel, Bobby managed to escape with McCoy and most of Archangel's other minions, but eventually broke away from them and went into hiding, living a life of hedonistic bliss in Madripoor. The "Age of Apocalypse" version of Nightcrawler - who, after being brought to Earth-616 along with the rest of his world's X-Men to help take down Archangel, decided to stay behind there in order to track down and kill the various villains who also came to the mainstream Marvel Universe - eventually tracked Bobby down to execute him for his treason. Nightcrawler is quickly overwhelmed by Drake's avatars, but eventually forces Iceman into a factory boiler room, where there's not enough moisture in the air for him to effectively use his powers. Nightcrawler quickly disables his former friend, and shoves him into an incinerator, killing the once noble hero.

During the miniseries "Earth X", Bobby had become trapped in his ice form, making him vulnerable to melting. He moves to the Arctic regions of Earth, and made an ice city for himself and the Inuit. Due to a series of events where Earth's orbital path moves, Bobby is able to return to the United States to aid in the battle against the demon Mephisto.

In the alternate timeline of the "House of M" storyline, Iceman was seen in Magneto's army during his rise to power. Bobby later appears as one of the Horsemen of Apocalypse because Apocalypse rescued Bobby from a mutant internment camp that his parents had sent him to. Magneto sends Apocalypse to dispose of his rival Black Panther; when Apocalypse is attacked en route by Black Panther's allies, Iceman aids him by freezing Namor solid and attempting to attack Storm, but he is severely injured by Sunfire.

A zombified Iceman appears in "Marvel Zombies: Dead Days" alongside zombies Wolverine and Cyclops. Ultimately, he is seen attacking Magneto. But Iceman perished at the hands of the Master Of Magnetism himself when Iceman is cut apart.

In the Mutant X universe, the Asgardian god Loki amplified Bobby's powers to a dangerous level, leaving him unable to touch any living thing without killing it. Despite this, he retains a jovial and optimistic personality. When Havok has a disagreement with Magneto and decides to leave the X-Men, Ice-Man is one of those who follow him, becoming a founding member of the Six, who eventually come to be considered the world's premiere mutant team.

After the New Exiles land on the world of warring empires, they encounter Dame Emma Frost, head of Britain's Department X and founder of Force-X. This team includes Roberta "Bobby" Drake, a female version of Bobby who is codenamed Aurion and displays ice-based abilities.

In the alternate future witnessed in the "Battle of the Atom" storyline, Iceman's future self is shown in a long brown robe with a long beard and ice staff. He has also developed the ability to create semi-independent ice structures that can act on their own, although one of these structures - demonstrating a Hulk-like physique and intellect - has gone on to join the future version of the Brotherhood, resulting in the Brotherhood using the duplicate to reinforce the illusion of themselves as the future X-Men. The duplicate Iceman was defeated by the O5 Iceman, the present Iceman, and the future Iceman during the battle in the past (Prompting the youngest Iceman to note that they only needed one more Iceman for a five-a-side basketball team).

In the alternate reality of "X-Men: Ronin" Iceman is a murderous ninja in the employ of the Hellfire Club. He works with Pyro and Avalanche as part of the 'Shadowcat Clan' and battles the X-Men.

New Excalibur battles an evil counterpart of Iceman, who is a member of the Shadow-X, the X-Men of an alternate reality in which Professor X was possessed by the Shadow King. They are brought to Earth-616 as a result of M-Day. He appeared to be mute and died during the final battle against Albion.

In "Edge of Spider-verse: Web of Fear", a Spider-Man who is a member of the Captain Britain Corps witnesses Morlun about to kill Spider-Man. Later, a larger picture is shown of Firestar and Iceman lying dead, with Ms. Lion being left out, mourning her comrades.

Iceman appears in "X-Men Noir" as one of the X Men, a crew of talented criminals. He is depicted as being very short-tempered and paranoid. He is dubbed "Iceman", and angrily insists others refer to him that way, due to his custom of using an icepick as a weapon.

In the Ultimate Marvel continuity, Bobby Drake is the youngest founding member of the X-Men. He ran away from his family at the peak of government-supported Sentinel attacks, fearing his family would be killed in such an attack.

Ultimate Iceman never had the snowman look of his counterpart, instead generating a flexible ice armor from the beginning. Bobby establishes himself as a valuable asset, singlehandedly taking out the Ultimates once with a gigantic ice wall ("see Ultimate War"), as well as singlehandedly halting an invasion by Colonel Wraith and Weapon X. He was only able to be stopped by Rogue, who was in temporary possession of Marvel Girl's telepathy. Professor X has stated that Bobby is one of the three most powerful X-Men. During the "World Tour" arc, after enlarging his armor to form a gigantic ice troll, Bobby is greatly injured by Proteus, which resulted in a lawsuit issued by his parents against Xavier. Bobby eventually rebels against his parents, and later returns to the X-Men.

While Bobby was away from the X-Men on a vacation, he had a girlfriend, but Professor Xavier erased all memories of her from Bobby's mind when he told her too much about the X-Men (he presumably also erased the girl's memories). Upon her acceptance into the X-Men, Bobby begins to date Rogue. The pair date for a considerable amount of time, but eventually break up due to Bobby's growing feelings for Shadowcat and Rogue's feelings for Gambit. Eventually Rogue leaves, and Bobby starts to date Kitty. After this, the two rekindle their relationship, but problems erupted.

In "Ultimate X-Men" #80, Bishop and Hammer, time-travellers from the future, comment on how powerful Bobby will one day become. Cyclops disbanded the X-Men in "Ultimate X-Men" #81 and Bishop and Storm created a new team. Iceman stayed at the Institute as a student only until Xavier returned and reformed his X-Men.

Professor X is later revealed to be alive and the X-Men return to the Xavier Institute, which is also when Iceman rejoins the X-Men line up. Jean Grey soon discovers that fellow X-Man, Colossus, is using a drug called Banshee to enhance his mutant abilities. The X-Men are highly against this, but Colossus manages to convince Rogue, Dazzler, Angel, Nightcrawler, and Cyclops to join him on his own X-Men team. Iceman remains on Professor X's drug-free X-Men and fights the Banshee enhanced X-Men. Xavier's X-Men win and the two teams combine again with nobody using drugs.

The Ultimatum Wave hits the X-Men next killing several of the X-Men (Beast, Dazzler, and Nightcrawler). Magneto and the Brotherhood attack the world and Iceman helps the world's heroes fight them off. Most of the X-Men die, but Iceman (alongside Rogue, Storm, Colossus, and Jean Grey) is able to survive Magneto's attack. He is last seen demolishing the X-Mansion alongside Rogue and Jean Grey and burying the deceased X-Men in its place. He finds it hard to destroy their home, but he feels it to be the right thing to do now that Professor Xavier is dead.

Later, Bobby Drake is kicked out of his home for being a mutant. With nowhere else to go, Kitty suggests to Peter Parkers' Aunt May that he move in with her, Peter, Gwen Stacy, and Johnny Storm (who also recently moved in their household). Aunt May agrees and enrolls Bobby at Midtown High under the guise of Bobby Parker, one of Peters' cousins and shaves his hair off to help keep his, Peter's and Johnny's secret identities safe.

Without knowing where to go after Peter's death, Bobby Drake asked Kitty Pride if Johhny could come with them searching for a place where to live and hide from authorities. They found the Morlock Tunnels where they live now and help mutants in danger. Firstly they rescued Rogue, who joined them, and later Jimmy Hudson (the son of Wolverine), came to them for help after escaping Stryker's imprisonment along with other mutants he freed.

In "X-Men Fairy Tales" (issue #1), Iceman appears as a white wolf with icy breath named Kori (Japanese for ice). Before he is reached by , he appears to have lost faith in friendship.

In "", Iceman appears as one of the instrumental characters in the defeat of Cassandra Nova and Khan and one of the few surviving X-Men.


In the films, "X-Men", "X2", "", and "", he is played by Shawn Ashmore. Bobby is one of the first students to reach out to Rogue and begins a romantic relationship with her. In "X2", he has an uneasy friendship/rivalry with Pyro. His relationship with his family is also strained, and his brother actually turns Bobby in to the police out of jealousy. In "X-Men: The Last Stand", his relationship with Rogue appears to be deteriorating, strained by their inability to have physical contact and by his close friendship with Kitty Pryde. During a battle with Pyro, now working for Magneto's Brotherhood, he achieves the ability to transform into his ice form from the comics. In the original dystopian timeline of "", Iceman is more powerful, transforming into ice and travelling on ice slides, but is nevertheless killed while fighting the Sentinels. After Kitty and Wolverine change the timeline, Iceman is seen once again in a relationship with Rogue at the Xavier Mansion. Bobby's scenes vary in the "Days of Future Past" known as "The Rogue Cut", where he is killed by Sentinels while helping Rogue and Magneto to escape.

In the novelization for "X-Men: The Last Stand", Iceman saves Pyro from the destruction of Dark Phoenix.





</doc>
<doc id="15506" url="https://en.wikipedia.org/wiki?curid=15506" title="Isidore of Seville">
Isidore of Seville

Saint Isidore of Seville (; ; c. 560 –  4 April 636), a scholar and, for over three decades, Archbishop of Seville, is widely regarded as the last of the Fathers of the Church, as the 19th-century historian Montalembert put it in an oft-quoted phrase, "The last scholar of the ancient world." 

At a time of disintegration of classical culture, and aristocratic violence and illiteracy, he was involved in the conversion of the Arian Visigothic kings to Catholicism, both assisting his brother Leander of Seville, and continuing after his brother's death. He was influential in the inner circle of Sisebut, Visigothic king of Hispania. Like Leander, he played a prominent role in the Councils of Toledo and Seville. The Visigothic legislation that resulted from these councils influenced the beginnings of representative government.

His fame after his death was based on his "Etymologiae", an etymological encyclopedia which assembled extracts of many books from classical antiquity that would have otherwise been lost.

Isidore was probably born in Cartagena, Spain, a former Carthaginian colony, to Severianus and Theodora. Both Severianus and Theodora belonged to notable Hispano-Roman families of high social rank. His parents were members of an influential family who were instrumental in the political-religious maneuvering that converted the Visigothic kings from Arianism to Catholicism. The Catholic Church celebrates him and all his siblings as known saints:


Isidore received his elementary education in the Cathedral school of Seville. In this institution, the first of its kind in Iberia, a body of learned men including Archbishop Saint Leander of Seville taught the trivium and quadrivium, the classic liberal arts. Saint Isidore applied himself to study diligently enough that he quickly mastered Latin, and acquired some Greek, and Hebrew.

Two centuries of Gothic control of Iberia incrementally suppressed the ancient institutions, classic learning, and manners of the Roman Empire. The associated culture entered a period of long-term decline. The ruling Visigoths nevertheless showed some respect for the outward trappings of Roman culture. Arianism meanwhile took deep root among the Visigoths as the form of Christianity that they received.

Scholars may debate whether Isidore ever personally embraced monastic life or affiliated with any religious order, but he undoubtedly esteemed the monks highly.

After the death of Saint Leander of Seville on 13 March 600 or 601, Isidore succeeded to the See of Seville. On his elevation to the episcopate, he immediately constituted himself as protector of monks.

Saint Isidore recognized that the spiritual and material welfare of the people of his See depended on assimilation of remnant Roman and ruling barbarian cultures, and consequently attempted to weld the peoples and subcultures of the Visigothic kingdom into a united nation. He used all available religious resources toward this end and succeeded. Isidore practically eradicated the heresy of Arianism and completely stifled the new heresy of Acephali at its very outset. Archbishop Isidore strengthened religious discipline throughout his See.

Archbishop Isidore also used resources of education to counteract increasingly influential Gothic barbarism throughout his episcopal jurisdiction. His quickening spirit animated the educational movement centered on Seville. Saint Isidore introduced Aristotle to his countrymen long before the Arabs studied Greek philosophy extensively.

In 619, Saint Isidore of Seville pronounced anathema against any ecclesiastic who in any way should molest the monasteries.

Saint Isidore presided over the Second Council of Seville, begun on 13 November 619, in the reign of King Sisebut, a provincial council attended by eight other bishops, all from the ecclesiastical province of Baetica in southern Spain. The Acts of the Council fully set forth the nature of Christ, countering the conceptions of Gregory, a Syrian representing the heretical Acephali.

Based on a few surviving canons found in the Pseudo-Isidorian Decretals, Saint Isidore is known to have presided over an additional provincial council around 624.

The council dealt with a conflict over the See of Écija, and wrongfully stripped bishop Martianus of his see, a situation that was rectified by the Fourth Council of Toledo. It also addressed a concern over Jews who had been forced to convert to Christianity by Sisebut failing to present their children for baptism.

The records of the council, unlike the First and Second Councils of Seville were not preserved in the Hispana, a collection of canons and decretals likely edited by Saint Isidore himself.

All bishops of Hispania attended the Fourth National Council of Toledo, begun on 5 December 633. The aged Archbishop Saint Isidore presided over its deliberations and originated most enactments of the council.

Through Isidore's influence, this Council of Toledo promulgated a decree, commanding all bishops to establish seminaries in their cathedral cities along the lines of the cathedral school at Seville, which had educated Saint Isidore decades earlier. The decree prescribed the study of Greek, Hebrew, and the liberal arts and encouraged interest in law and medicine. The authority of the Council made this education policy obligatory upon all bishops of the Kingdom of the Visigoths. The council granted remarkable position and deference to the king of the Visigoths. The independent Church bound itself in allegiance to the acknowledged king; it said nothing of allegiance to the Bishop of Rome.

Saint Isidore of Seville died on 4 April 636 after serving more than 32 years as archbishop of Seville.

Isidore's Latin style in the "Etymologiae" and elsewhere, though simple and lucid, reveals increasing local Visigothic traditions.

Isidore was the first Christian writer to try to compile a "summa" of universal knowledge, in his most important work, the "Etymologiae" (taking its title from the method he uncritically used in the transcription of his era's knowledge). It is also known by classicists as the "Origines" (the standard abbreviation being "Orig"). This encyclopedia — the first such Christian epitome—formed a huge compilation of 448 chapters in 20 volumes.

In it, as Isidore entered his own terse digest of Roman handbooks, miscellanies and compendia, he continued the trend towards abridgements and summaries that had characterised Roman learning in Late Antiquity. In the process, many fragments of classical learning are preserved which otherwise would have been hopelessly lost; "in fact, in the majority of his works, including the "Origines", he contributes little more than the mortar which connects excerpts from other authors, as if he was aware of his deficiencies and had more confidence in the "stilus maiorum" than his own" his translator Katherine Nell MacFarlane remarks.

Some of these fragments were lost in the first place because Isidore's work was so highly regarded—Braulio called it "quecunque fere sciri debentur", "practically everything that it is necessary to know"— that it superseded the use of many individual works of the classics themselves, which were not recopied and have therefore been lost: "all secular knowledge that was of use to the Christian scholar had been winnowed out and contained in one handy volume; the scholar need search no further".

The fame of this work imparted a new impetus to encyclopedic writing, which bore abundant fruit in the subsequent centuries of the Middle Ages. It was the most popular compendium in medieval libraries. It was printed in at least ten editions between 1470 and 1530, showing Isidore's continued popularity in the Renaissance. Until the 12th century brought translations from Arabic sources, Isidore transmitted what western Europeans remembered of the works of Aristotle and other Greeks, although he understood only a limited amount of Greek. The "Etymologiae" was much copied, particularly into medieval bestiaries.

Isidore's "De fide catholica contra Iudaeos" furthers Augustine of Hippo's ideas on the Jewish presence in Christian society. Like Augustine, Isidore accepted the necessity of the Jewish presence because of their expected role in the anticipated Second Coming of Christ. In "De fide catholica contra Iudaeos", Isidore exceeds the anti-rabbinic polemics of earlier theologians by criticizing Jewish practice as deliberately disingenuous.

He contributed two decisions to the Fourth Council of Toledo: Canon 60 calling for the forced removal of children from parents practicing Crypto-Judaism and their education by Christians and Canon 65 forbidding Jews and Christians of Jewish origin from holding public office.

Isidore's other works, all in Latin, include:

Isidore was one of the last of the ancient Christian philosophers; he was the last of the great Latin Church Fathers and was contemporary with Maximus the Confessor. Some consider him to be the most learned man of his age, and he exercised a far-reaching and immeasurable influence on the educational life of the Middle Ages. His contemporary and friend, Braulio of Zaragoza, regarded him as a man raised up by God to save the Iberian peoples from the tidal wave of barbarism that threatened to inundate the ancient civilization of Hispania.

The Eighth Council of Toledo (653) recorded its admiration of his character in these glowing terms: "The extraordinary doctor, the latest ornament of the Catholic Church, the most learned man of the latter ages, always to be named with reverence, Isidore". This tribute was endorsed by the Fifteenth Council of Toledo, held in 688 and later in 1598 by Pope Clement VIII. Isidore was declared a Doctor of the Church in 1722 by Pope Innocent XIII. 

Isidore was interred in Seville. His tomb represented an important place of veneration for the Mozarabs during the centuries after the Arab conquest of Visigothic Hispania. In the middle of the 11th century, with the division of Al Andalus into taifas and the strengthening of the Christian holdings in the Iberian peninsula, Ferdinand I of León and Castile found himself in a position to extract tribute from the fractured Arab states. In addition to money, Abbad II al-Mu'tadid, the Abbasid ruler of Seville (1042–1069), agreed to turn over St. Isidore's remains to Ferdinand I. A Catholic poet described al-Mutatid placing a brocaded cover over Isidore's sarcophagus, and remarked, "Now you are leaving here, revered Isidore. You know well how much your fame was mine!" Ferdinand had Isidore's remains reinterred in the then-recently constructed Basilica of San Isidoro in León. Today, many of his bones are buried in the cathedral of Murcia, Spain.

In Dante's "Paradiso" (X.130), Isidore is mentioned among theologians and Doctors of the Church alongside the Scot Richard of St. Victor and the Englishman Bede the Venerable.

The University of Dayton has named their implementation of the Sakai Project in honour of Saint Isidore.

His likeness, along with that of Leander of Sevile and Ferdinand III of Castile, are depicted on the crest badge of Sevilla FC.

The Order of St. Isidore of Seville is a chivalric order formed on January 1, 2000. An international organisation, the order aims to honour Saint Isidore as patron saint of the Internet, alongside promoting Christian chivalry online. Members, who may be men or women, receive a modern-day knighthood.






</doc>
<doc id="15507" url="https://en.wikipedia.org/wiki?curid=15507" title="Compounds of carbon">
Compounds of carbon

Compounds of carbon are defined as chemical substances containing carbon. More compounds of carbon exist than any other chemical element except for hydrogen. Organic carbon compounds are far more numerous than inorganic carbon compounds. In general bonds of carbon with other elements are covalent bonds. Carbon is tetravalent but carbon free radicals and carbenes occur as short-lived intermediates. Ions of carbon are carbocations and carbanions are also short-lived. An important carbon property is catenation as the ability to form long carbon chains and rings.

The known inorganic chemistry of the allotropes of carbon (diamond, graphite, and the fullerenes) blossomed with the discovery of buckminsterfullerene in 1985, as additional fullerenes and their various derivatives were discovered. O
derivatives is inclusion compounds, in which an ion is enclosed by the all-carbon shell of the fullerene. This incflusion is denoted by the "@" symbol in endohedral fullerenes. For example, an ion consisting of a lithium ion trapped within buckminsterfullerene would be denoted Li@C. As with any other ionic compound, this complex ion could in principle pair with a counterion to form a salt. Other elements are also incorporated in so-called graphite intercalation compounds.

Carbides are binary compounds of carbon with an element that is less electronegative than it. The most important are
AlC,
BC,
CaC,
FeC,
HfC,
SiC,
TaC,
TiC, and
WC.

It was once thought that organic compounds could only be created by living organisms. Over time, however, scientists learned how to synthesize organic compounds in the lab. The number of organic compounds is immense and the known number of defined compounds is close to 10 million. However, an indefinitely large number of such compounds are theoretically possible. 

By definition, an organic compound must contain at least one atom of carbon, but this criterion is not generally regarded as sufficient. Indeed, the distinction between organic and inorganic compounds is ultimately a matter of convention, and there are several compounds that have been classified either way, such as:
COCl,
CSCl,
CS(NH),
CO(NH).
With carbon bonded to metals the field of organic chemistry crosses over into organometallic chemistry.

There is a rich variety of carbon chemistry that does not fall within the realm of organic chemistry and is thus called inorganic carbon chemistry.

There are many oxides of carbon (oxocarbons), of which the most common are carbon dioxide (CO) and carbon monoxide (CO). Other less known oxides include carbon suboxide (CO) and mellitic anhydride (CO). There are also numerous unstable or elusive oxides, such as dicarbon monoxide (CO), oxalic anhydride (CO), and carbon trioxide (CO). 

There are several oxocarbon anions, negative ions that consist solely of oxygen and carbon. The most common are the carbonate (CO) and oxalate (CO). The corresponding acids are the highly unstable carbonic acid (HCO) and the quite stable oxalic acid (HCO), respectively. These anions can be partially deprotonated to give the bicarbonate (HCO) and hydrogenoxalate (HCO). Other more exotic carbon–oxygen anions exist, such as acetylenedicarboxylate (OC–C≡C–CO), mellitate (CO), squarate (CO), and rhodizonate (CO). The anhydrides of some of these acids are oxides of carbon; carbon dioxide, for instance, can be seen as the anhydride of carbonic acid. 

Some important carbonates are
AgCO,
BaCO,
CaCO,
CdCO,
Ce(CO),
CoCO,
CsCO,
CuCO,
FeCO,
KCO,
La(CO),
LiCO,
MgCO,
MnCO,
(NH)CO,
NaCO,
NiCO,
PbCO,
SrCO, and
ZnCO.

The most important bicarbonates include 
NHHCO,
Ca(HCO),
KHCO, and
NaHCO.

The most important oxalates include
AgCO,
BaCO,
CaCO, 
Ce(CO),
KCO, and
NaCO.

Carbonyls are coordination complexes between transition metals and carbonyl ligands. Metal carbonyls are complexes that are formed with the neutral ligand CO. These complexes are covalent. Here is a list of some carbonyls:
Cr(CO),
Co(CO),
Fe(CO),
Mn(CO),
Mo(CO),
Ni(CO),
W(CO).

Important inorganic carbon-sulfur compounds are the carbon sulfides carbon disulfide (CS) and carbonyl sulfide (OCS). Carbon monosulfide (CS) unlike carbon monoxide is very unstable. Important compound classes are thiocarbonates, thiocarbamates, dithiocarbamates and trithiocarbonates. 
Small inorganic carbon – nitrogen compounds are cyanogen, hydrogen cyanide, cyanamide, isocyanic acid and cyanogen chloride.
Paracyanogen is the polymerization product of cyanogen. Cyanuric chloride is the trimer of cyanogen chloride and 2-cyanoguanidine is the dimer of cyanamide. 

Other types of inorganic compounds include the inorganic salts and complexes of the carbon-containing cyanide, cyanate, fulminate, thiocyanate and cyanamide ions. Examples of cyanides are copper cyanide (CuCN) and potassium cyanide (KCN), examples of cyanates are potassium cyanate (KNCO) and silver cyanate (AgNCO), examples of fulminates are silver fulminate (AgOCN) and mercury fulminate (HgOCN) and an example of a thiocyanate is potassium thiocyanate (KSCN).

The common carbon halides are carbon tetrafluoride (CF), carbon tetrachloride (CCl), carbon tetrabromide (CBr), carbon tetraiodide (CI), and a large number of other carbon-halogen compounds.

A carborane is a cluster composed of boron and carbon atoms such as HCBH.

There are hundreds of alloys that contain carbon. The most common of these alloys is steel, sometimes called "carbon steel" (see ). All kinds of steel contain some amount of carbon, by definition, and all ferrous alloys contain some carbon. 

Some other common alloys that are based on iron and carbon include anthracite iron, cast iron, pig iron, and wrought iron. 

In more technical uses, there are also spiegeleisen, an alloy of iron, manganese, and carbon; and stellite, an alloy of cobalt, chromium, tungsten, and carbon. 

Whether it was placed there deliberately or not, some traces of carbon is also found in these common metals and their alloys: aluminum, chromium, magnesium, molybdenum, niobium, thorium, titanium, tungsten, uranium, vanadium, zinc, and zirconium. For example, many of these metals are smelted with coke, a form of carbon; and aluminum and magnesium are made in electrolytic cells with carbon electrodes. Some distribution of carbon into all of these metals is inevitable.


</doc>
<doc id="15508" url="https://en.wikipedia.org/wiki?curid=15508" title="Industrial espionage">
Industrial espionage

Industrial espionage, economic espionage, corporate spying or corporate espionage is a form of espionage conducted for commercial purposes instead of purely national security. Economic espionage is conducted or orchestrated by governments and is international in scope, while industrial or corporate espionage is more often national and occurs between companies or corporations.

"Competitive intelligence" describes the legal and ethical activity of systematically gathering, analyzing and managing information on industrial competitors. It may include activities such as examining newspaper articles, corporate publications, websites, patent filings, specialised databases, information at trade shows and the like to determine information on a corporation. The compilation of these crucial elements is sometimes termed CIS or CRS, a "Competitive Intelligence Solution" or "Competitive Response Solution". With its roots in market research, "competitive intelligence" has been described as the "application of principles and practices from military and national intelligence to the domain of global business"; it is the business equivalent of open-source intelligence.

The difference between competitive intelligence and economic or industrial espionage is not clear; one needs to understand the legal basics to recognize how to draw the line between the two. Others maintain it is sometimes quite difficult to tell the difference between legal and illegal methods, especially if considering the ethical side of information gathering, making the definition even more elusive.

Economic or industrial espionage takes place in two main forms. In short, the purpose of espionage is to gather knowledge about (an) organization(s). It may include the acquisition of intellectual property, such as information on industrial manufacture, ideas, techniques and processes, recipes and formulas. Or it could include sequestration of proprietary or operational information, such as that on customer datasets, pricing, sales, marketing, research and development, policies, prospective bids, planning or marketing strategies or the changing compositions and locations of production. It may describe activities such as theft of trade secrets, bribery, blackmail and technological surveillance. As well as orchestrating espionage on commercial organizations, governments can also be targets — for example, to determine the terms of a tender for a government contract so that another tenderer

Economic and industrial espionage is most commonly associated with technology-heavy industries, including computer software and hardware, biotechnology, aerospace, telecommunications, transportation and engine technology, automobiles, machine tools, energy, materials and coatings and so on. Silicon Valley is known to be one of the world's most targeted areas for espionage, though any industry with information of use to competitors may be a target.

Information can make the difference between success and failure; if a trade secret is stolen, the competitive playing field is leveled or even tipped in favor of a competitor.
Although a lot of information-gathering is accomplished legally through competitive intelligence, at times corporations feel the best way to get information is to take it. Economic or industrial espionage is a threat to any business whose livelihood depends on information.

In recent years, economic or industrial espionage has taken on an expanded definition. For instance, attempts to sabotage a corporation may be considered industrial espionage; in this sense, the term takes on the wider connotations of its parent word. That espionage and sabotage (corporate or otherwise) have become more clearly associated with each other is also demonstrated by a number of profiling studies, some government, some corporate. The United States government currently has a polygraph examination entitled the "Test of Espionage and Sabotage" (TES), contributing to the increasingly popular, though not consensus, notion, by those studying espionage and sabotage countermeasures, of the interrelationship between the two. In practice, particularly by "trusted insiders," they are generally considered functionally identical for the purpose of informing countermeasures.

Economic or industrial espionage commonly occurs in one of two ways. Firstly, a dissatisfied employee appropriates information to advance interests or to damage the company. Secondly, a competitor or foreign government seeks information to advance its own technological or financial interest. "Moles", or trusted insiders, are generally considered the best sources for economic or industrial espionage. Historically known as a "patsy", an insider can be induced, willingly or under duress, to provide information. A patsy may be initially asked to hand over inconsequential information and, once compromised by committing a crime, bribed into handing over more sensitive material. Individuals may leave one company to take up employment with another and take sensitive information with them. Such apparent behavior has been the focus of numerous industrial espionage cases that have resulted in legal battles. Some countries hire individuals to do spying rather than use of their own intelligence agencies. Academics, business delegates, and students are often thought to be used by governments in gathering information. Some countries, such as Japan, have been reported to expect students be debriefed on returning home. A spy may follow a guided tour of a factory and then get "lost". A spy could be an engineer, a maintenance man, a cleaner, an insurance salesman, or an inspector: anyone who has legitimate access to the premises.

A spy may break into the premises to steal data and may search through waste paper and refuse, known as "dumpster diving". Information may be compromised via unsolicited requests for information, marketing surveys or use of technical support or research or software facilities. Outsourced industrial producers may ask for information outside the agreed-upon contract.

Computers have facilitated the process of collecting information because of the ease of access to large amounts of information through physical contact or the Internet.

Computers have become key in exercising industrial espionage due to the enormous amount of information they contain and its ease of being copied and transmitted. The use of computers for espionage increased rapidly in the 1990s. Information has been commonly stolen by being copied from unattended computers in offices, those gaining unsupervised access doing so through subsidiary jobs, such as cleaners or repairmen. Laptops were, and still are, a prime target, with those traveling abroad on business being warned not to leave them for any period of time. Perpetrators of espionage have been known to find many ways of conning unsuspecting individuals into parting, often only temporarily, from their possessions, enabling others to access and steal information. A "bag-op" refers to the use of hotel staff to access data, such as through laptops, in hotel rooms. Information may be stolen in transit, in taxis, at airport baggage counters, baggage carousels, on trains and so on.

The rise of the internet and computer networks has expanded the range and detail of information available and the ease of access for the purpose of industrial espionage. Worldwide, around 50,000 companies a day are thought to come under cyberattack with the rate estimated as doubling each year. This type of operation is generally identified as state backed or sponsored, because the "access to personal, financial or analytic resources" identified exceed that which could be accessed by cybercriminals or individual hackers. Sensitive military or defense engineering or other industrial information may not have immediate monetary value to criminals, compared with, say, bank details. Analysis of cyberattacks suggests deep knowledge of networks, with targeted attacks, obtained by numerous individuals operating in a sustained organized way.

The rising use of the internet has also extended opportunities for industrial espionage with the aim of sabotage. In the early 2000s, it was noticed that energy companies were increasingly coming under attack from hackers. Energy power systems, doing jobs like monitoring power grids or water flow, once isolated from the other computer networks, were now being connected to the internet, leaving them more vulnerable, having historically few built-in security features. The use of these methods of industrial espionage have increasingly become a concern for governments, due to potential attacks by terrorist groups or hostile foreign governments.

One of the means of perpetrators conducting industrial espionage is by exploiting vulnerabilities in computer software. Malware and spyware as "a tool for industrial espionage", in "transmitting digital copies of trade secrets, customer plans, future plans and contacts". Newer forms of malware include devices which surreptitiously switch on mobile phones camera and recording devices. In attempts to tackle such attacks on their intellectual property, companies are increasingly keeping important information off network, leaving an "air gap", with some companies building "Faraday cages" to shield from electromagnetic or cellphone transmissions.

The distributed denial of service (DDoS) attack uses compromised computer systems to orchestrate a flood of requests on the target system, causing it to shut down and deny service to other users. It could potentially be used for economic or industrial espionage with the purpose of sabotage. This method was allegedly utilized by Russian secret services, over a period of two weeks on a cyberattack on Estonia in May 2007, in response to the removal of a Soviet era war memorial.

Economic and industrial espionage has a long history. The work of Father Francois Xavier d'Entrecolles in Jingdezhen, China to reveal to Europe the manufacturing methods of Chinese porcelain in 1712 is sometimes considered an early case of industrial espionage.

Historical accounts have been written of industrial espionage between Britain and France. Attributed to Britain's emergence as an "industrial creditor", the second decade of the 18th century saw the emergence of a large-scale state-sponsored effort to surreptitiously take British industrial technology to France. Witnesses confirmed both the inveigling of tradespersons abroad and the placing of apprentices in England. Protests by those such as iron workers in Sheffield and steel workers in Newcastle, about skilled industrial workers being enticed abroad, led to the first English legislation aimed at preventing this method of economic and industrial espionage.

East-West commercial development opportunities after World War I saw a rise in Soviet interest in American and European manufacturing know-how, exploited by Amtorg Corporation. Later, with Western restrictions on the export of items thought likely to increase military capabilities to the USSR, Soviet industrial espionage was a well known adjunct to other spying activities up until the 1980s. "BYTE" reported in April 1984, for example, that although the Soviets sought to develop their own microelectronics, their technology appeared to be several years behind the West's. Soviet CPUs required multiple chips and appeared to be close or exact copies of American products such as the Intel 3000 and DEC LSI-11/2.

Some of these activities were directed via the East German Stasi (Ministry for State Security). One such operation, known as "Operation Brunnhilde" operated from the mid-1950s until early 1966 and made use of spies from many Communist Bloc countries. Through at least 20 forays, many western European industrial secrets were compromised. One member of the "Brunnhilde" ring was a Swiss chemical engineer, Dr. Jean Paul Soupert (also known as "Air Bubble"), living in Brussels. He was described by Peter Wright in Spycatcher as having been "doubled" by the Belgian Sûreté de l'État. He revealed information about industrial espionage conducted by the ring, including the fact that Russian agents had obtained details of Concorde's advanced electronics system. He testified against two Kodak employees, living and working in Britain, during a trial in which they were accused of passing information on industrial processes to him, though they were eventually acquitted.

A secret report from the Military-Industrial Commission of the USSR (VPK), from 1979–80, detailed how "spetsinformatsiya" ( i.e. "special records") could be utilised in twelve different military industrial areas. Writing in the Bulletin of the Atomic Scientists, Philip Hanson detailed a "spetsinformatsiya" system in which 12 industrial branch ministries formulated requests for information to aid technological development in their military programs. Acquisition plans were described as operating on 2 year and 5 year cycles with about 3000 tasks under way each year. Efforts were aimed at civilian as well as military industrial targets, such as in the petrochemical industries. Some information was garnered so as to compare levels of competitor to Soviet technological advancement. Much unclassified information was also gathered, blurring the boundary with "competitive intelligence".

The Soviet military was recognised as making much better use of acquired information, compared to civilian industry, where their record in replicating and developing industrial technology was poor.

Following the demise of the Soviet Union and the end of the Cold War, commentators, including the US Congressional Intelligence Committee, noted a redirection amongst the espionage community from military to industrial targets, with Western and former communist countries making use of "underemployed" spies and expanding programs directed at stealing such information.

The legacy of Cold War spying included not just the redirection of personnel but the use of spying apparatus such as computer databases, scanners for eavesdropping, spy satellites, bugs and wires.

Between 1987 and 1989, IBM and Texas Instruments were thought to have been targeted by French spies with the intention of helping France's Groupe Bull. In 1993, U.S. aerospace companies were also thought to have been targeted by French interests. During the early 1990s, France was described as one of the most aggressive pursuers of espionage to garner foreign industrial and technological secrets. France accused the U.S. of attempting to sabotage its high tech industrial base. The government of France has been alleged to have conducted ongoing industrial espionage against American aerodynamics and satellite companies.

In 1993, car manufacturer Opel, the German division of General Motors, accused Volkswagen of industrial espionage after Opel's chief of production, Jose Ignacio Lopez, and seven other executives moved to Volkswagen. Volkswagen subsequently threatened to sue for defamation, resulting in a four-year legal battle. The case, which was finally settled in 1997, resulted in one of the largest settlements in the history of industrial espionage, with Volkswagen agreeing to pay General Motors $100 million and to buy at least $1 billion of car parts from the company over 7 years, although it did not explicitly apologize for Lopez's behavior.

In April 2009 the US based hospitality company Starwood accused its rival Hilton of a "massive" case of industrial espionage. After being purchased by private equity group Blackstone, Hilton employed 10 managers and executives from Starwood. Under intense pressure to improve profits, Starwood accused Hilton of stealing corporate information relating to its luxury brand concepts, used in setting up its own Denizen hotels. Specifically, former head of its luxury brands group, Ron Klein, was accused of downloading "truckloads of documents" from a laptop to his personal email account.

GhostNet was a "vast surveillance system" reported by Canadian researchers based at the University of Toronto in March 2009. Using targeted emails it compromised thousands of computers in governmental organisations, enabling attackers to scan for information and transfer this back to a "digital storage facility in China".

On 13 January 2010, Google announced that operators, from within China, had hacked into their Google China operation, stealing intellectual property and, in particular, accessing the email accounts of human rights activists. The attack was thought to have been part of a more widespread cyber attack on companies within China which has become known as Operation Aurora. Intruders were thought to have launched a zero-day attack, exploiting a weakness in the Microsoft Internet Explorer browser, the malware used being a modification of the trojan "Hydraq". Concerned about the possibility of hackers taking advantage of this previously unknown weakness in Internet Explorer, the governments of Germany and, subsequently France, issued warnings not to use the browser.

There was speculation that "insiders" had been involved in the attack, with some Google China employees being denied access to the company's internal networks after the company's announcement. In February 2010, computer experts from the U.S. National Security Agency claimed that the attacks on Google probably originated from two Chinese universities associated with expertise in computer science, Shanghai Jiao Tong University and the Shandong Lanxiang Vocational School, the latter having close links to the Chinese military.

Google claimed at least 20 other companies had also been targeted in the cyber attack, said by the "London Times", to have been part of an "ambitious and sophisticated attempt to steal secrets from unwitting corporate victims" including "defence contractors, finance and technology companies". Rather than being the work of individuals or organised criminals, the level of sophistication of the attack was thought to have been "more typical of a nation state". Some commentators speculated as to whether the attack was part of what is thought to be a concerted Chinese industrial espionage operation aimed at getting "high-tech information to jump-start China's economy". Critics pointed to what was alleged to be a lax attitude to the intellectual property of foreign businesses in China, letting them operate but then seeking to copy or reverse engineer their technology for the benefit of Chinese "national champions". In Google's case, they may have (also) been concerned about the possible misappropriation of source code or other technology for the benefit of Chinese rival Baidu. In March 2010 Google subsequently decided to cease offering censored results in China, leading to the closing of its Chinese operation.

The US based firm CyberSitter announced in January 2010 that it was suing the Chinese government, and other US companies, for stealing its anti pornography software, with the accusation that it had been incorporated into China's Green Dam program, used by the state to censor children's internet access. CyberSitter accused Green Dam creators as having copied around 3000 lines of code. They were described as having done 'a sloppy job of copying,' with some lines of the copied code continuing to direct people to the CyberSitter website. The attorney acting for CyberSitter maintained "I don't think I have ever seen such clear-cut stealing".

The United States charged two former NetLogic Inc. engineers, Lan Lee and Yuefei Ge, of committing economic espionage against TSMC and NetLogic, Inc. A jury acquitted the defendants of the charges with regard to TSMC and deadlocked on the charges with regard to NetLogic. In May 2010, a federal judge dismissed all the espionage charges against the two defendants. The judge ruled that the U.S. Government presented no evidence of espionage.

In May 2010, the federal jury convicted Chordiant Software, Inc., a U.S. corporation, of stealing Dongxiao Yue's JRPC technologies and used them in a product called Chordiant Marketing Director. Yue previously filed lawsuits against Symantec Corporation for a similar theft.

Revelations from the Snowden documents have provided information to the effect that the United States, notably vis-à-vis the NSA, has been conducting aggressive economic espionage against Brazil. Canadian intelligence has apparently supported U.S. economic espionage efforts.

A recent report to the US government, by aerospace and defense company Northrop Grumman, describes Chinese economic espionage as comprising "the single greatest threat to U.S. technology". Joe Stewart, of SecureWorks, blogging on the 2009 cyber attack on Google, referred to a "persistent campaign of 'espionage-by-malware' emanating from the People’s Republic of China (PRC)" with both corporate and state secrets being "Shanghaied" over the past 5 or 6 years. The Northrop Grumann report states that the collection of US defense engineering data through cyberattack is regarded as having "saved the recipient of the information years of R&D and significant amounts of funding". Concerns about the extent of cyberattacks on the US emanating from China has led to the situation being described as the dawn of a "new cold cyberwar". In response to these and other reports, Amitai Etzioni of the Institute for Communitarian Policy Studies has suggested that China and the United States should agree to a policy of mutually assured restraint with respect to cyberspace. This would involve allowing both states to take the measures they deem necessary for their self-defense while simultaneously agreeing to refrain from taking offensive steps; it would also entail vetting these commitments.

According to Edward Snowden, the National Security Agency spies on foreign companies. In June 2015 Wikileaks published documents over National Security Agency spied French companies.

In December 2007, it was revealed that Jonathan Evans, head of the United Kingdom's MI5, had sent out confidential letters to 300 chief executives and security chiefs at the country's banks, accountants and legal firms warning of attacks from Chinese 'state organisations'. A summary was also posted on the secure website of the Centre for the Protection of the National Infrastructure, accessed by some of the nation's 'critical infrastructure' companies, including 'telecoms firms, banks and water and electricity companies'. One security expert warned about the use of 'custom trojans,' software specifically designed to hack into a particular firm and feed back data. Whilst China was identified as the country most active in the use of internet spying, up to 120 other countries were said to be using similar techniques. The Chinese government responded to UK accusations of economic espionage by saying that the report of such activities was 'slanderous' and that the government opposed hacking which is prohibited by law.

German counter-intelligence experts have maintained the German economy is losing around €53 billion or the equivalent of 30,000 jobs to economic espionage yearly.

In Operation Eikonal German BND agents received "selector lists" from the NSA – search terms for their dragnet surveillance. They contain IP addresses, mobile phone numbers and email accounts with the BND surveillance system containing hundreds of thousands and possibly more than a million such targets. These lists have been subject of controversy as in 2008 it was revealed that they contained some terms targeting the European Aeronautic Defence and Space Company (EADS), the Eurocopter project as well as French administration, which were first noticed by BND employees in 2005. After the revelations made by whistleblower Edward Snowden the BND decided to investigate the issue whose October 2013 conclusion was that at least 2,000 of these selectors were aimed at Western European or even German interests which has been a violation of the Memorandum of Agreement that the US and Germany signed in 2002 in the wake of the 9/11 terror attacks. After reports emerged in 2014 that EADS and Eurocopter had been surveillance targets the Left Party and the Greens filed an official request to obtain evidence of the violations.

The BND's project group charged with supporting the NSA investigative committee in German parliament set up in spring 2014, reviewed the selectors and discovered 40,000 suspicious search parameters, including espionage targets in Western European governments and numerous companies. The group also confirmed suspicions that the NSA had systematically violated German interests and concluded that the Americans could have perpetrated economic espionage directly under the Germans' noses. The investigative parliamentary committee was not granted access to the NSA's selectors list as an appeal led by opposition politicians failed at Germany's top court - instead the ruling coalition appointed an administrative judge, Kurt Graulich, as a "person of trust" who was granted access to the list and briefed the investigative commission on its contents after analyzing the 40,000 parameters. In his almost 300-paged report Graulich concluded that European government agencies were targeted massively and that Americans hence broke contractual agreements. He also found that German targets which received special protection from surveillance of domestic intelligence agencies by Germany's Basic Law (Grundgesetz) − including numerous enterprises based in Germany – were featured in the NSA's wishlist in a surprising plenitude.




</doc>
<doc id="15511" url="https://en.wikipedia.org/wiki?curid=15511" title="Isaac Bashevis Singer">
Isaac Bashevis Singer

Isaac Bashevis Singer (; November 21, 1902 – July 24, 1991) was a Polish-born Jewish writer in Yiddish, awarded the Nobel Prize in Literature in 1978. The Polish form of his birth name was Icek Hersz Zynger. He used his mother's first name in an initial literary pseudonym, "Izaak Baszewis", which he later expanded. He was a leading figure in the Yiddish literary movement, writing and publishing only in Yiddish. He was also awarded two U.S. National Book Awards, one in Children's Literature for his memoir "A Day Of Pleasure: Stories of a Boy Growing Up in Warsaw" (1970) and one in Fiction for his collection "A Crown of Feathers and Other Stories" (1974).

Isaac Bashevis Singer was born in 1902 in Leoncin village near Warsaw, Poland, under military partitions by the Russian Empire. A few years later, the family moved to a nearby Polish town of Radzymin. The exact date of his birth is uncertain, but most probably it was November 21, 1902, a date that Singer gave both to his official biographer Paul Kresh, and his secretary Dvorah Telushkin. It is also consistent with the historical events he and his brother refer to in their childhood memoirs. The often-quoted birth date, July 14, 1904 was made up by the author in his youth, most probably to make himself younger to avoid the draft.

His father was a Hasidic rabbi and his mother, Bathsheba, was the daughter of the rabbi of Biłgoraj. Singer later used her name in his pen name "Bashevis" (Bathsheba's). Both his older siblings, sister Esther Kreitman (1891–1954) and brother Israel Joshua Singer (1893–1944), became writers as well. Esther was the first of the family to write stories.

The family moved to the court of the Rabbi of Radzymin in 1907, where his father became head of the Yeshiva. After the Yeshiva building burned down in 1908, the family moved to a flat at ul. Krochmalna 10. In the spring of 1914, the Singers moved to No. 12.

The street where Singer grew up was located in the impoverished, Yiddish-speaking Jewish quarter of Warsaw. There his father served as a rabbi, and was called on to be a judge, arbitrator, religious authority and spiritual leader in the Jewish community. The unique atmosphere of pre-war Krochmalna Street can be found both in the collection of "Varshavsky-stories", which tell stories from Singer's childhood, as well as in those novels and stories which take place in pre-war Warsaw.

In 1917, because of the hardships of World War I, the family split up. Singer moved with his mother and younger brother Moshe to his mother's hometown of Biłgoraj, a traditional "shtetl," where his mother's brothers had followed his grandfather as rabbis. When his father became a village rabbi again in 1921, Singer returned to Warsaw. He entered the Tachkemoni Rabbinical Seminary and soon decided that neither the school nor the profession suited him. He returned to Biłgoraj, where he tried to support himself by giving Hebrew lessons, but soon gave up and joined his parents, considering himself a failure. In 1923 his older brother Israel Joshua arranged for him to move to Warsaw to work as a proofreader for the Jewish "Literarische Bleter," of which the brother was an editor.

In 1935, four years before the German invasion, Singer emigrated from Poland to the United States. He was fearful of the growing Nazi threat in neighboring Germany. The move separated the author from his common-law first wife Runia Pontsch and son Israel Zamir (1929–2014); they emigrated to Moscow and then Palestine. The three met again twenty years later in 1955.

Singer settled in New York City, where he took up work as a journalist and columnist for "The Jewish Daily Forward" (), a Yiddish-language newspaper. After a promising start, he became despondent and for some years felt ""Lost in America"" (title of his 1974 novel published in Yiddish; published in English in 1981).

In 1938, he met Alma Wassermann née Haimann (1907–1996), a German-Jewish refugee from Munich. They married in 1940, and their union seemed to release energy in him; he returned to prolific writing and to contributing to the "Forward". In addition to his pen name of "Bashevis," he published under the pen names of "Warszawski" (pron. Varshavsky) during World War II, and "D. Segal." They lived for many years in the Belnord apartment building on Manhattan's Upper West Side.

In 1981, Singer delivered a commencement address at the University at Albany, and was presented with an honorary doctorate.

Singer died on July 24, 1991 in Surfside, Florida, after suffering a series of strokes. He was buried in Cedar Park Cemetery, Emerson, New Jersey. A street in Surfside, Florida is named Isaac Singer Boulevard in his honor; and so is a city square in Lublin, Poland. The full academic scholarship for undergraduate students at the University of Miami is also named in his honor.

Singer's first published story won the literary competition of the "literarishe bletter" and garnered him a reputation as a promising talent. A reflection of his formative years in "the kitchen of literature" can be found in many of his later works. IB Singer published his first novel, "Satan in Goray", in installments in the literary magazine "Globus", which he had co-founded with his life-long friend, the Yiddish poet Aaron Zeitlin in 1935. The book recounts events of 1648 in the village of Goraj (close to Biłgoraj), where the Jews of Poland lost a third of their population in a wholesale attack by Cossacks. It explores the effects of the seventeenth-century faraway false messiah, Shabbatai Zvi, on the local population. Its last chapter imitates the style of a medieval Yiddish chronicle. With a stark depiction of innocence crushed by circumstance, the novel appears to foreshadow coming danger. In his later work, "The Slave" (1962), Singer returns to the aftermath of 1648, in a love story between a Jewish man and a Gentile woman. He portrays the traumatized and desperate survivors of the historic catastrophe with even deeper understanding.

Singer became a literary contributor to the "Forward" only after his older brother Israel died in 1945. That year, Singer published "The Family Moskat" in his brother's honor. His own style showed in the daring turns of his action and characters, with double adultery during the holiest of nights of Judaism, the evening of Yom Kippur (despite being printed in a Jewish family newspaper in 1945). He was almost forced to stop writing the novel by his legendary editor-in-chief, Abraham Cahan, but was saved by readers who wanted the story to go on. After this, his stories—which he had published in Yiddish literary newspapers before—were printed in the "Forward" as well. Throughout the 1940s, Singer's reputation grew.

Singer believed in the power of his native language and thought that there was still a large audience, including in New York, who longed to read in Yiddish. In an interview in "Encounter" (February 1979), he claimed that although the Jews of Poland had died, "something—call it spirit or whatever—is still somewhere in the universe. This is a mystical kind of feeling, but I feel there is truth in it."

Some of his colleagues and readers were shocked by his all-encompassing view of human nature. He wrote about female homosexuality ("Zeitl and Rickel", "Tseytl un Rikl"), published in "The Seance and Other Stories"), transvestism ("Yentl the Yeshiva Boy" in "Short Friday"), and of rabbis corrupted by demons ("Zeidlus the Pope" in "Short Friday"). In those novels and stories which refer to events in his own life, he portrays himself unflatteringly (with some degree of accuracy) as an artist who is self-centered yet has a keen eye for the sufferings and tribulations of others.

Singer had many literary influences; besides the religious texts he studied, he grew up with a rich array of Jewish folktales and worldly Yiddish detective-stories about "Max Spitzkopf" and his assistant "Fuchs". He read Russian, including Dostoyevsky's "Crime and Punishment" at the age of fourteen. He wrote in memoirs about the importance of the Yiddish translations donated in book-crates from America, which he studied as a teenager in Bilgoraj: "I read everything: Stories, novels, plays, essays... I read Rajsen, Strindberg, Don Kaplanowitsch, Turgenev, Tolstoy, Maupassant and Chekhov." He studied many philosophers, among them Spinoza, Arthur Schopenhauer, and Otto Weininger. Among his Yiddish contemporaries, Singer considered his older brother to be his greatest artistic example; he was also a life-long friend and admirer of the author and poet Aaron Zeitlin.

Of his non-Yiddish-contemporaries, he was strongly influenced by the writings of Knut Hamsun, many of whose works he later translated, while he had a more critical attitude towards Thomas Mann, whose approach to writing he considered opposed to his own. Contrary to Hamsun's approach, Singer shaped his world not only with the egos of his characters, but also using the moral commitments of the Jewish tradition known from his youth and embodied by his father in the stories about Singer's youth. There was a dichotomy between the life his heroes lead and the life they feel they should lead — which gives his art a modernity his predecessors did not express. His themes of witchcraft, mystery and legend draw on traditional sources, but they are contrasted with a modern and ironic consciousness. They are also concerned with the bizarre and the grotesque.

Another important strand of his art is intra-familial strife, which he experienced firsthand when taking refuge with his mother and younger brother at his uncle's home in Biłgoraj. This is the central theme in Singer's big family chronicles, such as "The Family Moskat" (1950), "The Manor" (1967), and "The Estate" (1969). Some critics believe these show the influence of Thomas Mann's novel "Buddenbrooks"; Singer had translated Mann's "Der Zauberberg" ("The Magic Mountain") into Yiddish as a young writer.

Singer always wrote and published in Yiddish. His novels were serialized in newspapers, which also published his short stories. He edited his novels and stories for their publication in English in the United States; these versions were used as the basis for translation into other languages. He referred to his English version as his "second original." This has led to an ongoing controversy whether the "real Singer" can be found in the Yiddish original, with its finely tuned language and sometimes rambling construction, or in the more tightly edited American versions, where the language is usually simpler and more direct. Many of Singer's stories and novels have not yet been translated.

In the short story form, in which many critics feel he made his most lasting contributions, his greatest influences were writers Anton Chekhov and Guy de Maupassant, Russian and French, respectively. From Maupassant, Singer developed a finely grained sense of drama. Like those of the French master, Singer's stories can pack enormous visceral excitement in the space of a few pages. From Chekhov, Singer developed his ability to draw characters of enormous complexity and dignity in the briefest of spaces. In the foreword to his personally selected volume of his finest short stories he describes the two aforementioned writers as the greatest masters of the short story form.

Several respected artists have illustrated Singer's novels, short stories, and children's books, including Raphael Soyer, Maurice Sendak, Larry Rivers, and Irene Lieblich. Singer personally selected Lieblich to illustrate two of his books for children, "A Tale of Three Wishes" and "The Power of Light: Eight Stories for Hanukkah," after seeing her paintings at an Artists Equity exhibition in New York City. A Holocaust survivor, Lieblich was from Zamosc, Poland, a town adjacent to the area where Singer was raised. As their memories of shtetl life were so similar, Singer found Lieblich's images ideally suited to illustrate his texts. Of her style, Singer wrote that "her works are rooted in Jewish folklore and are faithful to Jewish life and the Jewish spirit."

Singer published at least 18 novels, 14 children's books, a number of memoirs, essays and articles. He is best known as a writer of short stories, which have been published in more than a dozen collections. The first collection of Singer's short stories in English, "Gimpel the Fool", was published in 1957. The title story was translated by Saul Bellow and published in May 1953 in the "Partisan Review". Selections from Singer's "Varshavsky-stories" in the "Daily Forward" were later published in anthologies such as "My Father's Court" (1966). Later collections include "A Crown of Feathers" (1973), with notable masterpieces in between, such as "The Spinoza of Market Street" (1961) and "A Friend of Kafka" (1970). His stories and novels reflect the world of the East European Jewry in which he grew up. After his many years in America, his stories also portrayed the world of the immigrants and their pursuit of an elusive American dream, which seems always beyond reach.

Prior to Singer's winning the Nobel Prize, English translations of dozens of his stories were frequently published in popular magazines such as "Playboy" and "Esquire". They were publishing literary works and included his stories among their best; in turn, he found them to be appropriate outlets for his work.

Throughout the 1960s, Singer continued to write on questions of personal morality. Because of the controversial aspects of his plots, he was a target of scathing criticism from many quarters, some of it for not being "moral" enough, some for writing stories that no one wanted to hear. To his critics he replied, "Literature must spring from the past, from the love of the uniform force that wrote it, and not from the uncertainty of the future." 

Singer was awarded the Nobel Prize in 1978.

His novel "Enemies, a Love Story" was adapted as a film by the same name (1989) and was quite popular, bringing new readers to his work. He featured a Holocaust survivor who deals with varying desires, complex family relationships, and a loss of faith.

Singer's story, "Yentl, the Yeshiva Boy," was adapted into a film by that name (1983) starring singer Barbra Streisand.

Alan Arkin starred as Yasha, the principal character in the 1979 film version of The Magician of Lublin, which also featured Shelley Winters, Louise Fletcher, Valerie Perrine and Lou Jacobi. In the final scene, Yasha achieves his lifelong ambition of being able to fly, though not quite as the magic trick he had originally planned.

Perhaps the most fascinating Singer-inspired film is 1974's "Mr. Singer's Nightmare or Mrs. Pupkos Beard," directed by Bruce Davidson, a renowned photographer who became Singer's neighbor. This unique film is a half-hour mixture of documentary and fantasy for which Singer wrote the script and played the leading role.

The 2007 film "Love Comes Lately", starring Otto Tausig, was adapted from several of Singer's stories.

Singer's relationship to Judaism was complex and unconventional. He identified as a skeptic and a loner, though he felt a connection to his Orthodox roots. Ultimately, he developed a view of religion and philosophy, which he called "private mysticism: Since God was completely unknown and eternally silent, He could be endowed with whatever traits one elected to hang upon Him."

Singer was raised Orthodox and learned all the Jewish prayers, studied Hebrew, and learned Torah and Talmud. As he recounted in the autobiographical short story "In My Father's Court," he broke away from his parents in his early twenties. Influenced by his older brother, who had done the same, he began spending time with non-religious Bohemian artists in Warsaw. Although Singer believed in a God, as in traditional Judaism, he stopped attending Jewish religious services of any kind, even on the High Holy Days. He struggled throughout his life with the feeling that a kind and compassionate God would never support the great suffering he saw around him, especially the Holocaust deaths of so many of the Polish Jews from his childhood. In one interview with the photographer Richard Kaplan, he said, "I am angry at God because of what happened to my brothers." Singer's older brother died suddenly in February 1944, in New York, of a thrombosis; his younger brother perished in Soviet Russia around 1945, after being deported with his mother and wife to Southern Kazakhstan in Stalin's purges.

Despite the complexities of his religious outlook, Singer lived in the midst of the Jewish community throughout his life. He did not seem to be comfortable unless he was surrounded by Jews; particularly Jews born in Europe. Although he spoke English, Hebrew, and Polish fluently, he always considered Yiddish his natural tongue. He always wrote in Yiddish and he was the last notable American author to be writing in this language. After he had achieved success as a writer in New York, Singer and his wife began spending time during the winters in Miami with its Jewish community, many of them New Yorkers.

Eventually, as senior citizens, they moved to Miami. They identified closely with the European Jewish community. After his death, Singer was buried in a traditional Jewish ceremony in a Jewish cemetery.

Especially in his short fiction, Singer often wrote about various Jews having religious struggles; sometimes these struggles became violent, bringing death or mental illness. In one story his narrator meets a young woman in New York whom he knew from an Orthodox family in Poland. She has become a kind of hippie, sings American folk music with a guitar, and rejects Judaism, although the narrator comments that in many ways she seems typically Jewish. The narrator says that he often meets Jews who think they are anything but Jewish, and yet still are.

In the end, Singer remains an unquestionably Jewish writer, yet his precise views about Jews, Judaism, and the Jewish God are open to interpretation. Whatever they were, they lay at the center of his literary art.

Singer was a prominent Jewish vegetarian for the last 35 years of his life and often included vegetarian themes in his works. In his short story, "The Slaughterer", he described the anguish of an appointed slaughterer trying to reconcile his compassion for animals with his job of killing them. He felt that the ingestion of meat was a denial of all ideals and all religions: "How can we speak of right and justice if we take an innocent creature and shed its blood?" When asked if he had become a vegetarian for health reasons, he replied: "I did it for the health of the chickens."

In "The Letter Writer", he wrote "In relation to [animals], all people are Nazis; for the animals, it is an eternal Treblinka." which became a classical reference in the discussions about the legitimacy of the comparison of animal exploitation with the Holocaust.

In the preface to Steven Rosen's "Food for Spirit: Vegetarianism and the World Religions" (1986), Singer wrote, "When a human kills an animal for food, he is neglecting his own hunger for justice. Man prays for mercy, but is unwilling to extend it to others. Why should man then expect mercy from God? It's unfair to expect something that you are not willing to give. It is inconsistent. I can never accept inconsistency or injustice. Even if it comes from God. If there would come a voice from God saying, "I'm against vegetarianism!" I would say, "Well, I am for it!" This is how strongly I feel in this regard."

Singer described himself as "conservative," adding that "I don't believe by flattering the masses all the time we really achieve much." His conservative side was most apparent in his Yiddish writing and journalism, where he was openly hostile to Marxist sociopolitical agendas. In "Forverts" he once wrote, "It may seem like terrible "apikorses" [heresy], but conservative governments in America, England, France, have handled Jews no worse than liberal governments... The Jew's worst enemies were always those elements that the modern Jew convinced himself (really hypnotized himself) were his friends."


Note: Publication dates refer to English editions, not the Yiddish originals, which often predate the versions in translation by 10 to 20 years.












 


</doc>
<doc id="15513" url="https://en.wikipedia.org/wiki?curid=15513" title="Islamic eschatology">
Islamic eschatology

Islamic eschatology is the branch of Islamic theology concerning the end of the world, and the "Day of resurrection" after that, known as "Yawm al-Qiyāmah" (, , "the Day of Resurrection") or "Yawm ad-Dīn" (, , "the Day of Judgment"). when the annihilation of all life will be followed by its resurrection and judgment by God. When al-Qiyamah will happen is not specified, but according to prophecy, primarily elaborated by hadith-literature, there are major and minor signs that will foretell its coming. Many verses in the Quran mention the Last Judgment.

The main subject of Surat al-Qiyama is the resurrection. The Great Tribulation is described in the hadith and commentaries of the ulama, including al-Ghazali, Ibn Kathir, Ibn Majah, Muhammad al-Bukhari, and Ibn Khuzaymah. The Day of Judgment is also known as the Day of Reckoning, the Last Day, and the Hour ("al-sā'ah").

Unlike the Quran, the hadith contain several events, happening before the Day of Judgment, which are describe as several "minor signs" and twelve "major signs". During this period, terrible corruption and chaos would rule the earth, caused by the Masih ad-Dajjal (the Antichrist in Islam), then a messianic figure will appear, defeating the Dajjal and establish a period of peace, liberating Islam from cruelty. These events will be followed by a time of serenity when people live according to religious values.

Like other Abrahamic religions, Islam teaches that there will be a resurrection of the dead followed by a final tribulation and eternal division of the righteous and wicked. Islamic apocalyptic literature describing Armageddon is often known as "fitna", "Al-Malhama Al-Kubra" (The Great Massacre) or "ghaybah" in Shī'a Islam. The righteous are rewarded with the pleasures of Jannah (Paradise), while the unrighteous are punished in Jahannam (Hell).
The resurrection and final judgement are fundamental beliefs in Islam. According to the Quran, without them, the creation of humanity would be in vain.Thus the Day of Judgment, al-Qiyāmah, (also known as the Day of Reckoning or Resurrection, the Last Day, or the Hour) is one of the six articles of faith in Sunni Islam, and one of seven in Shia Islam.

Two main sources in Islamic scripture discuss the Last Judgment and the tribulation associated with it: the Quran, which is viewed in Islam as infallible, and the hadith, or sayings of the prophet. Hadith are viewed with more flexibility due to the late compilation of the sayings in written form, two hundred years after the death of Muhammad. The Last Judgment and the tribulation have also been discussed in the commentaries of ulama such as al-Ghazali, Ibn Kathir, and Muhammad al-Bukhari.

Interpretations of the Quran yield the following specifics:

Although Islamic philosophers and scholars were in general agreement on a bodily resurrection after death, however their opinions differ in regard what bodily resurrection will be like. Some of the theories are the following:

In Islam, a number of major and minor signs foretell the end of days. There is debate over whether they could occur concurrently or must be at different points in time, although Islamic scholars typically divide them into three major periods.




Following the second period, the third will be marked by the ten major signs known as "alamatu's-sa'ah al-kubra" (the major signs of the end). They are as follows:

Mahdi () means "guided one", is a messianic figure in Islamic tradition. He makes his first appearance in the hadiths and is thought as the "first sign of the third period". Hadith state that he will be a descendant of Muhammad through Muhammad's daughter Fatimah and cousin Ali. The Mahdi will be looked upon to kill al-Dajjal, to end the disintegration of the Muslim community, and to prepare for the reign of Jesus, who will rule for a time thereafter. The Mahdi will fulfill his prophetic mission, a vision of justice and peace, before submitting to Jesus' rule. The physical features of Mahdi are described in the hadith—he will be of Arab complexion, of average height, with a big forehead, large eyes, and a sharp nose. He will have a mole on his cheek, the sign of the prophet on his shoulder, and be recognised by the caliphate while he sits in his own home. As written by Abu Dawud, "Our Mahdi will have a broad forehead and a pointed (prominent) nose. He will fill the earth with justice as it is filled with injustice and tyranny. He will rule for seven years." In some accounts, after the seven years of peace, God will send a "cold wind" causing everyone with the smallest measure of human-kindness or faith, to die and carry them straight to heaven. Therefore, only the wicked will remain and be victims of terrible animals and Satan, until the day of resurrection. Otherwise, the Mahdi will kill Satan before the last day, in most Shia accounts.

Though the predictions of the duration of his rule differ, hadith are consistent in describing that God will perfect him in a single night, imbuing him with inspiration and wisdom, and his name will be announced from the sky. The Mahdi will bring back worship of true Islamic values, and bring the Ark of the Covenant to light. He will conquer Istanbul and Mount Daylam and will regard Jerusalem and the Dome as his home. His banner will be that of the prophet Muhammad: black and unstitched, with a halo. Furled since the death of Muhammad, the banner will unfurl when the Mahdi appears. He will be helped by angels and others that will prepare the way for him. He will understand the secrets of abjad.

Amr bin Shuaib learned from his grandfather that the Messenger of God said, "In Dhu al-Qi'dah (Islamic month), there will be fight among the tribes, Muslim pilgrims will be looted and there will be a battle in Mina in which many people will be slain and blood will flow until it runs over the Jamaratul Aqba (one of the three stone pillars at Mina). The man they seek will flee and will be found between the Rukn (a corner of the Kaaba containing the Black Stone) and the Maqam of Prophet Abraham (near Ka'ba). He will be forced to accept people's Bay'ah (being chosen as a Leader/Caliph). The number of those offering Bay'ah will be the same as the number of the people of Badr (Muslim fighters who participated in the Battle of Badr at time of Prophet Mohammad). Then, the dweller of Heaven and the dweller of the Earth will be pleased with him."

Sunni and Shi'a Islam have different beliefs regarding the identity of Mahdi. Historically, Sunni Islam considers religious authority as being derived from the caliph, who was appointed by the companions of Muhammad at his death. The Sunnis view the Mahdi as the successor of Mohammad; the Mahdi is expected to arrive to rule the world and reestablish righteousness. Some Sunnis share a belief that there may be no actual Mahdi, but that a series of mujaddid will instead lead to an Islamic revolution of a renewal of faith and avoidance of deviation from God's path. Sunni tradition has attributed such intellectual and spiritual attributes to numerous Muslims at the end of each Muslim century from the origin of Islam to the present day. This classical interpretation is favored by Sunni scholars like Ghazali and Ibn Taymiyyah.

Contrarily, Shi'a Islam vested religious authority in those of the bloodline of Muhammad, favoring his cousin and son by marriage, Ali. Ali was appointed the first Imam; and according to Twelver interpretation, he was followed by eleven more. Muhammad al-Mahdi, otherwise known as the Twelfth Imam, went into hiding in 873 at the age of four. His father was al-`Askari, who had been murdered; and so he was hidden from the authorities of the Abbasid Caliphate. He maintained contact with his followers until 940, when he entered the Occultation. Twelverism believes that al-Mahdi is the current Imam, and will emerge at the end of the current age. Some scholars say that, although unnoticed by others present, the Mahdi of Twelver Islam continues to make an annual pilgrimage while he resides outside of Mecca. In contradistinction, Sunni Islam foresees him as a separate and new person. The present Ayatollahs of Iran see themselves as joint caretakers of the office of the Imam until he returns.

The Mahdi is not described in the Qurʾān, only in hadith, with scholars suggesting he arose when Arabian tribes were settling in Syria under Muawiya. "They anticipated 'the Mahdi who will lead the rising people of the Yemen back to their country' in order to restore the glory of their lost Himyarite kingdom. It was believed that he would eventually conquer Constantinople."

Throughout history, there have been multiple claimants to the role of Mahdi, who by their pious deeds acquired followings. One of these men, Muhammad ibn al-Hanafiyyah, was said to have judgment and character over rival caliphs; and mysteries of his death arose in the 8th century. It was believed he had in fact not died and would one day return as the Mahdi. The sect of Mahdavis arose as followers of another claimant, Muhammad Mahdi of Janpur, in the 15th century. Furthermore, a potential Mahdi, Muhammad Ahmad of Sudan, was believed to hold the title following his self-proclamation, in 1881, and stand against the Turco-Egyptian government, as well as the British. Additionally, Mirza Ghulam Ahmad of Punjab claimed to be the Mahdi during the same period as Muhammad Ahmad, but was considered a heretic by Orthodox Muslims, though he amassed a substantial following of 10 to 20 million and is credited with founding the Ahmadiyya sect, which, today, is established in over 200 countries and territories of the world.

Two 19th-century religious movements—linked to Shi'ism, Bábism, and the Bahá'í Faith—believe that their prophets, the Báb (d. 1850) and Bahá'u'lláh, fulfilled the prophecy. The Báb is thought to be the return of the Twelfth Imam, who is also the Mahdi according to the twelver tradition, and Bahá'u'lláh that of Jesus. Since the Baha'is now are a successful international religion with possibly as many as six million followers, their concept of the fulfillment of Islamic prophecy now extends well beyond the Islamic world.

Isa is the Arabic name for Jesus, and his return is considered the third major sign of the last days (the second being the appearance of Jesus's nemesis Masih ad-Dajjal). Although Muhammad is the preeminent Prophet in Islam, Jesus is the only Prophet mentioned in the Quran, other than Idris (Enoch), who is said not to have died but who was rather raised up by God. Thus, in accordance with post-Quranic hadith, Jesus conceivably will return to Earth as a just judge before the Day of Judgment. As written in hadith:

Hadith reference both the Mahdi and Isa simultaneously and the return of the Mahdi will coincide with the return of Isa, who will descend from the heavens in "al-Quds" at dawn. The two will meet, and the Mahdi will lead the people in fajr prayer. After the prayer, they will open a gate to the west and encounter Masih ad-Dajjal. After the defeat of ad-Dajjal, Isa will lead a peaceful forty-year reign until his death. He will be buried in a tomb beside Muhammad in Medina. Though the two certainly differ regarding their role and persona in Islamic eschatology, the figures of the Mahdi and Isa are ultimately inseparable, according to the Prophet. Though Isa is said to descend upon the world once again, the Mahdi will already be present.

Masih ad-Dajjal does not appear in the Quran but is a prominent figure in hadith and Islamic eschatology as a whole. He appears gruesome and is blind in his right eye. His one eye is thought to be a symbol that correlates with how single-minded he is in achieving his goal of converting Muslims to his side. Al-Dajjal has the intention of gaining followers through his miracle working and apparent wealth and generosity. To resist such munificence is a test for true believers of Islam, who have been warned about Al-Dajjal's power, and who must resist his material temptations. It is thought that Al-Dajjal will appear prior to the Day of Judgment, where he will engage in an epic battle with, and be killed by, Jesus. Al-Dajjal functions symbolically as a key cog in the overall Islamic eschatological picture, which emphasises the world coming to an end, of good finally triumphing over evil, and of the remarkable events that will prefigure the replacement of the mortal world with a more authentic form of existence in the afterlife. Various Muslim political movements use the concept of Al-Dajjal to comment on contemporary events, and often identify him with opposing regimes or other worldly forces that they consider as harmful to Islam.

The fourth major sign of the end time will be that the wall which imprisons the nations of Ya'juj and Ma'juj will break, and they will surge forth. Some Islamic scholars, such as Imran Nazar Hosein, believe the wall began to crack during the life of Muhammad. This is supported in the hadith, when the prophet mentions that "a hole has been made in the wall containing the Ya'juj and Ma'juj", indicating the size of the hole with his thumb and index finger. Their release will occur forty years prior to the Last Judgment:

They will ravage the earth. Ultimately, God will send worms and insects to destroy them.

Before doomsday, a thin ruler from Ethiopia, with short legs, will attack Mecca and destroy the Kaaba. Another sign is the appearance of the "da'ba-tul-ard", or the Beast of the Earth.

The entire world will be engulfed by "dukhan" or smoke, for forty days, and there will be three huge earthquakes. The Qur'an will be taken to heaven and even the huffaz will not recall its verses. Finally, a pleasant breeze will blow that shall cause all believers to die, but infidels and sinners will remain alive. A fire will start, from Hadramawt in Yemen, that will gather all the people of the world in the land of Mahshar, and al-Qiyamah will commence.

In the Qur'an, "barzakh" () is the intermediate state in which "nafs" of the deceased are held between realities to rest with loved ones until "Qiyamah".

The eighth sign is a breeze bearing a pleasant scent, which will emanate from Yemen, causing the "awliya", "sulaha" and the pious to die peacefully once they inhale it. After the believers die, there will be a period of 120 years during which the world will contain only "kafirs", sinners, oppressors, liars, and adulterers; and there will be a reversion to idolatry.

The ninth sign is the rising of the sun from the west after a long night. After midday, the sun will set again. According to hadith:
The final signs will be "nafkhatu'l-ula", when a trumpet will be sounded for the first time, and which will result in the death of the remaining sinners. Then there will be a period of forty years, after which the eleventh sign is the sounding of a second trumpet to signal the resurrection as "ba'as ba'da'l-mawt". As written in the Qur'an:

Finally, there will be no more injustice: 

At divine judgment, each person's "Book of Deeds" will be read, in which "every small and great thing is recorded", but with actions before adolescence omitted. Records shall be given with the right hand if they are good, and the left if they are evil. Even the smallest acts will not be ignored: This will be followed by perfect, divine, and merciful justice. The age of the hereafter, or the rest of eternity, is the final stage after the Day of Judgment, when all will receive their judgment from God.
The dead will stand in a grand assembly, awaiting a scroll detailing their righteous deeds, sinful acts, and ultimate judgment. Muhammad will be the first to be resurrected.

If one did good deeds, one would go to "Jannah", and if unrighteous, would go to "Jahannam". Punishments will include "adhab", or severe pain, and "khizy" or shame. There will also be a punishment of the grave (for those who disbelieved) between death and the resurrection.

Ibn al-Nafis wrote of Islamic eschatology in "Theologus Autodidactus" (circa AD 1270), where he used reason, science, and early Islamic philosophy to explain how he believed al-Qiyamah would unfold, told in the form of a theological fiction novel.

Imran Nazar Hosein wrote numerous books that deal with Islamic eschatology ("Ilmu Ākhir al-Zamān" – Knowledge of the later days), among which the most famous is "Jerusalem in the Qur'an".




</doc>
<doc id="15514" url="https://en.wikipedia.org/wiki?curid=15514" title="Iblis">
Iblis

The term "Iblis" () may have been derived from the Arabic verbal root (with the broad meaning of "remain in grief") or "(", "he despaired"). Furthermore, the name is related to "talbis" meaning confusion. Another possibility is that it is derived from Ancient Greek "()", which is also the source of the English word 'devil'. However, there is no general agreement on the root of the term. The name itself could not be found before the Quran.

Although Iblis resembles the Christian notion of Satan, there is no direct correlation. In respect of their disobedience to God the motivation of Iblis and that of the Christian Satan differ, and unlike the Christian notion of Satan there is no mention of Iblis trying to take God's throne Also, given the strict monotheism of Islam, Iblis is not considered the adversary of God with a duel between God and another entity regarded as impossible. The enmity of Iblis applies just to God's creation. Iblis only has the power God granted him after his request for permission to tempt humans away from God's path. He is not the cause of evil, but takes advantage of the inclination of humans to be self-centered. Furthermore, the transformation of Iblis from angelic into demonic is a reminder of God's capacity to reverse injustice even on an ontological level. It is both a warning and a reminder because the special gifts given by God can also be taken away by him. 

Iblis is mentioned 11 times in the Quran by name, 9 times related to his rebellion against God's command to prostrate himself before Adam. More often occurs the term Shaitan, that is, sometimes related to Iblis. Then God created Adam, He ordered all the angels to bow before the new creation. All the angels bowed down, but Iblis refused to do so. He argued that since he himself was created from fire, he is superior to humans, made from mud, and that he should not prostrate himself before Adam. For his haughtiness, he was banished from heaven and condemned to hell. Therefore, Iblis made a request for the ability to try to mislead Adam and his descendants. God grants his request but also warned that he will have no power over God's servants.

Sufism rejects any concept of dualism and instead believes in the unity of existence. Therefore, some mystics hold, Iblis refused to bow to Adam because he was fully devoted to God alone and refused to bow to anyone else. Accordingly, he is regarded together with Muhammad as the two most perfect monotheists. But while Muhammad is the instrument of God's mercy, Iblis is the instrument of God's anger. Furthermore, Iblis is depicted as an example of a true lover of God, teacher of oneness despite physical separation than a failed creature, since he preferred to go to hell than to prostrate himself before someone else other than the "Beloved" (here referring to God). Thus Iblis became an example for unrequited love.

However, not all Sufis are in agreement with a positive depiction of Iblis. Rumi's viewpoint on Iblis is much more in tune with Islamic orthodoxy. Rumi views Iblis as the manifestation of the great sins haughtiness and envy. He states: "(Cunning) intelligence is from Iblis, and love from Adam." Iblis represents the principle of "one-eyed" intellect; he only saw the outward earthly form of Adam, but was blind to the Divine spark hidden in him, using an illicit method of comparison. Hasan of Basra holds that Iblis was the first who used "analogy", comparing himself to someone else, this causing his sin. Iblis therefore also represents humans' psyche moving towards sin or shows how love can cause envy and anxiety losing a beloved one.

Illustrations of Iblis in Islamic paintings often depict him black-faced, a feature which would later symbolize any Satanic figure or heretic, and with a black body, symbolizing his corrupted nature. Another common depiction shows Iblis wearing special headcovering, clearly different from the traditional Islamic turban. In one painting however, Iblis wears a traditional Islamic headcovering. The turban probably refers to a narration of Iblis' fall: there he wore a turban, then he was sent down from heaven.

There are different opinions regarding the affiliation of Iblis and there are certain major arguments, found in Quran and hadith literature, to determine his essence:






At-Tabari, a schoolar whose tafsir is highly rewarded in Sunni Islam, and whose opinion became the major opinion for the majority Sunnism and classical Islam, also shared by scholars such as Ibn Abbas, Tabari, Ash'ari, and Al-Baydawi, wrote regarding the essence of Iblis:
"The reason people held this opinion [that Iblis was not an angel] is that God stated in His Book that He created Iblis from the fire of the Samum (15:27) and from smokeless fire (55:15), but did not state that He created the angels from any like of that. And God states he was of the jinn, so they said that it is not possible that he should be related to that which God does not relate him to; they said that Iblis had progeny and offspring, but the angels do not procreate or have children.

But these reasons only bespeak the weakness of these people's knowledge, for there is nothing objectionable in that God should have created the categories of His angels from all kinds of things that He had created: He created some of them from light, some of them from fire, and some of them from what He willed apart from that. There sis thus nothing in God's omitting to state what He created His angels from what He created His angels from, and in His stating what He created Iblis from, which necessarily implies that Iblis is outside of the meaing of [angel], for it is possible that He created a category of His angels, among whom was Iblis, from fire, and even that Iblis was unique in that He created him, and no other angels of His, from the fire of the Samum.
Likewise, he cannot be excluded from being an angel by fact that he had progeny or offspring , beause passion and lust, from which the other angels were free, was compounded in him when God willed disobedience in him. As for God's statement that he was <one of the jinn>, it is not to be rejected that everything which hides itself (ijtanna) from the sight is a 'jinn', . . . and Iblis and the angels should then be among them, because they hide themselves from the eyes of mankind."

On the other hand, the famous exegete Ibn Kathir, whose opinion became the major opinion in modern Salafism, regarding the essence of Iblis, stated in his tafsir, that Iblis was not an angel, an opinnion also shared by scholars such as Hasan of Basra, Ibn Taymiyyah and Al-Munajjid but also suggested by Ja'far al-Sadiq,:

"When Allah commanded the angels to prostrate before Adam, Iblis was included in this command. Although Iblis was not an angel, he was trying- and pretending - to imitate the angels' behavior and deeds, and this is why he was also included in the command to the angels to prostrate before Adam. Satan was critized for defying that command, (. . .)

When matters crucial every vessel leaks that which to contains and is betrayed by its true nature. Iblis used to do, what the angels did and resembled them in their devotion and worship, so he was included when they were addressed, but he disobeyed and went what he was told to do. So Allah points out here that he was one of the Jinn, he was created from fire, as He says elsewhere."
Finally, the following four interpretations are commonly accepted among Islamic traditions:





Iblis was once an Archangel and God's dearest creature. He was the teacher of the other angels, and God gave him authority over the lower heavens. When the jinn increasingly caused corruption on earth, God sent him with an army of angels to battle the jinn and they drove them into the mountains. Iblis felt superior to any other creature, especially since he was, unlike the other angels, created from fire. Then God created Adam and commanded the angels to prostrate before him, Iblis felt humiliated, and refused to accept the superiority of man, who was, according to Iblis, just another fallible entity, who would corrupt God's earth, like the jinn he just defeated before, while he would "praise God's glory day and night". Therefore he was cast out of heaven, but God allows him to proof Iblis objection concerning humanity by tempting them into sin, and gave him a chance to redeem himself. Thus although Iblis's pride lost him his position in heaven, he remained part of God's plan.

Once the angels battled the earthen jinn and took prisoners. One of these was Iblis, who grew up among the angels in heaven, and because of his piety was elevated to their rank. When God ordered the angels to prostrate themselves before Adam, Iblis abused his free-will and refused haughtily, considering himself superior because of his physical nature constituted of fire and not of clay.Therefore he was damned to hell but was given respite until the Day of Judgment.

Although the Serpent is not mentioned in the Qur'an, Qur'anic commentaries as well as the Stories of the Prophets added the serpent borrowed from Gnostic and Jewish oral tradition circulating in the Arabian Peninsula. Iblis tries to enter the abode of Adam, but the angelic guardian keeps him away. Then Iblis invents a plan to trick the guardian. He approaches a peacock and tells him that all creatures will die and the peacock's beauty will perish. But if he gets the fruit of eternity, every creature will last forever. Therefore, the peacock convinces the serpent to slip Iblis into the Garden, by carrying him in his mouth. In the Garden, Iblis speaks through the serpent to Adam and Eve, and tricks them into eating from the forbidden tree. Modern Muslims accuse the Yazidis of devil-worship for venerating the peacock. 

Ghazali told a tale about Iblis meeting Moses on the slopes of Sinai. Accordingly, Moses asks Iblis why he refused God's order. Iblis replied that the command was actually a test. Then Moses replied, obviously Iblis was punished by being turned from an angel to a devil. Iblis responds, his form is just temporary.

Just as there are different views on the origin of Iblis, there are different opinions regarding his lot during the Endtime:



</doc>
<doc id="15516" url="https://en.wikipedia.org/wiki?curid=15516" title="Intelsat">
Intelsat

Intelsat, S.A. is a communications satellite services provider. Originally formed as International Telecommunications Satellite Organization ("ITSO", or INTELSAT), it was—from 1964 to 2001—an intergovernmental consortium owning and managing a constellation of communications satellites providing international broadcast services.

, Intelsat operates a fleet of 52 communications satellites, which is one of the world's largest fleet of commercial satellites. They claim to serve around 1,500 customers and employ a staff of approximately 1,100 people.

The intergovernmental organization (IGO) began on 20 August 1964, with 11 participating countries. On 6 April 1965, Intelsat’s first satellite, the Intelsat I (nicknamed "Early Bird"), was placed in geostationary orbit above the Atlantic Ocean by a Delta D rocket.

In 1973, the name was changed and there were 80 signatories. Intelsat provides service to over 600 Earth stations in more than 149 countries, territories and dependencies. By 2001, INTELSAT had over 100 members. It was also this year that INTELSAT privatised and changed its name to Intelsat.

Since its inception, Intelsat has used several versions (blocks) of its dedicated Intelsat satellites. Intelsat completes each block of spacecraft independently, leading to a variety of contractors over the years. Intelsat’s largest spacecraft supplier is Space Systems/Loral, having built 31 spacecraft (as of 2003), or nearly half of the fleet.

The network in its early years was not as robust as it is now. A failure of the Atlantic satellite in the spring of 1969 threatened to stop the "Apollo 11" mission; a replacement satellite went into a bad orbit and could not be recovered in time; NASA had to resort to using undersea cable telephone circuits to bring Apollo's communications to NASA during the mission. Fortunately, during the Apollo 11 moonwalk, the moon was over the Pacific Ocean, and so other antennas were used, as well as INTELSAT III, which was in geostationary orbit over the Pacific.

By the 1990s, building and launching satellites was no longer exclusively a government domain and as country-specific telecommunications systems were privatized, several private satellite operators arose to meet the growing demand. In the U.S., satellite operators such as PanAmSat, Orion Communications, Columbia Communications, Iridium, Globalstar, TRW and others formed under the umbrella of the Alliance for Competitive International Satellite Services (ACISS) to press for an end to the IGOs and the monopoly position of COMSAT the US signatory to Intelsat and Inmarsat. In March 2001, the US Congress passed the Open Market Reorganisation for the Betterment of International Telecommunications (ORBIT) Act to privatise COMSAT and reform the role of the international organizations. In April 1998, to address US government concerns about market power, Intelsat's senior management spun off five of its older satellites to a private Dutch entity, New Skies Satellites, which became a direct competitor to Intelsat. To avert the US government's interference with Intelsat, Intelsat's senior management unsuccessfully considered relocating the IGO to another country.

On 18 July 2001, Intelsat became a private company, 37 years after formation. Prior to Intelsat's privatisation in 2001, ownership and investment in INTELSAT (measured in shares) was distributed among INTELSAT members according to their use of services. Investment shares determined each member’s percentage of the total contribution needed to finance capital expenditures. The organisation’s primary source of revenue was satellite usage fees which, after deduction of operating costs, was redistributed to INTELSAT members in proportion to their shares as repayment of capital and compensation for use of capital. Satellite services were available to any organisation (both INTELSAT members and non-members), and all users paid the same rates.

Today, the number of Intelsat satellites, as well as ocean-spanning fibre-optic lines, allows rapid rerouting of traffic when one satellite fails. Modern satellites are more robust, lasting longer with much larger capacity.

Intelsat Americas-7 (known formerly as Telstar 7 and now known as Galaxy 27) experienced a several-day power failure on 29 November 2004. The satellite returned to service with reduced capacity.
Intelsat was sold for U.S. $3.1bn in January 2005 to four private equity firms: Madison Dearborn Partners, Apax Partners, Permira and Apollo Global Management. The company acquired PanAmSat on 3 July 2006, and is now the world's largest provider of fixed satellite services, operating a fleet of 52 satellites in prime orbital locations. In June 2007 BC Partners announced they had acquired 76 percent of Intelsat for about 3.75 billion euros.

In April 2013, the renamed Intelsat S.A. undertook an initial public offering on the New York Stock Exchange, raising a net $550 million USD, of which $492 million was paid immediately to reduce outstanding company debts of $15.9 billion USD. In May, the company announced it would be purchasing four new high-performance Boeing EpicNG 702 MP satellites.

There were talks that Intelsat was to merge with Softbank-backed OneWeb. However, on 1 June 2017 it was announced that the bondholders would not accept the offer and the merger would be terminated as of 2 June 2017.

Intelsat maintains its corporate headquarters in Luxembourg, with a majority of staff and satellite functions. Administrative headquarters is located at the Intelsat Corporation offices in Tysons Corner, Virginia. A highly international business, Intelsat sources the majority of its revenue from non-U.S. located customers. Intelsat's biggest teleport is the Teleport Fuchsstadt in Germany.

, Intelsat has agreed to purchase one-half of the propellant payload that an MDA Corporation spacecraft satellite-servicing demonstration project would take to geostationary orbit. Catching up in orbit with four or five Intelsat communication satellites, a fuel load of of fuel delivered to each satellite would add somewhere between two and four years of additional service life.
A near-end-of-life Intelsat satellite will be moved to a graveyard orbit above the geostationary belt where the refueling will be done, "without consequence" to the Intelsat business.

, the business model was still evolving. MDA "could ask customers to pay per kilogram of fuel successfully added to [each] satellite, with the per-kilogram price being a function of the additional revenue the operator can expect to generate from the spacecraft’s extended operational life."

The plan is that the fuel-depot vehicle would maneuver to several satellites, dock at the target satellite’s apogee-kick motor, remove a small part of the target spacecraft’s thermal protection blanket, connect to a fuel-pressure line and deliver the propellant. "MDA officials estimate the docking maneuver would take the communications satellite out of service for about 20 minutes."

On February 1, 2007, Intelsat changed the names of 16 of its satellites formerly known under the Intelsat Americas and PanAmSat brands to Galaxy and Intelsat, respectively.


Over time, Intelsat has worked with most of the commercial launch services providers worldwide. Their satellites are often among the most massive of their generation, requiring the most powerful and reliable rockets on the market at a given time. In the 21st century, most Intelsat missions were conducted by Arianespace with the European Ariane 4 and Ariane 5 launchers, and by ILS with Proton-K and Proton-M rockets manufactured by Khrunichev in Russia. Intelsat also took advantage of the equatorial Sea Launch offering with Zenit-3SL rockets launched from the Ocean Odyssey floating platform, until they suspended operations in 2014. On May 30, 2012, Intelsat signed a contract with SpaceX for one of the first Falcon Heavy launch vehicles, marking the return of Intelsat to American launchers after many flights on Atlas II in the 1990s and a single Atlas V launch in 2009.





</doc>
<doc id="15517" url="https://en.wikipedia.org/wiki?curid=15517" title="ITSO">
ITSO

ITSO may stand for:


</doc>
<doc id="15521" url="https://en.wikipedia.org/wiki?curid=15521" title="Indian numerals">
Indian numerals

Indian numerals are the symbols representing numbers in India. These numerals are generally used in the context of the decimal Hindu–Arabic numeral system, and are distinct from, though related by descent to Arabic numerals.

Below is a list of the Indian numerals in their modern Devanagari form, the corresponding Hindu-Arabic (European) equivalents, their Hindi and Sanskrit pronunciation, and translations in some languages.

Since Sanskrit is an Indo-European language, the words for numerals closely resemble those of Greek and Latin. The word "Shunya" for zero was translated into Arabic as "صفر" "sifr", meaning 'nothing' which became the term "zero" in many European languages from Medieval Latin, "zephirum".

Devanagari digits shapes may vary depending on geographical area.
The five Indian languages (Hindi, Marathi, Konkani, Nepali and Sanskrit itself) that have adapted the Devanagari script to their use also naturally employ the numeral symbols above; of course, the names for the numbers vary by language. The table below presents a listing of the symbols used in various modern Indian scripts in comparison to Hindu-Arabic and Eastern Arabic-Indic numerals for the numbers from zero to nine:

"For numerals in Bengali language and Assamese languages see Bengali-Assamese numerals."

Tamil and Malayalam also have distinct forms for numerals 10, 100, 1000 as ௰, ௱, ௲ and ൰, ൱, ൲, respectively.

A decimal place system has been traced back to c. 500 in India. Before that epoch, the Brahmi numeral system was in use; that system did not encompass the concept of the place-value of numbers. Instead, Brahmi numerals included additional symbols for the tens, as well as separate symbols for "hundred" and "thousand".

The Indian place-system numerals spread to neighboring Persia, where they were picked up by the conquering Arabs. In 662, Severus Sebokht - a Nestorian bishop living in Syria wrote:

I will omit all discussion of the science of the Indians ... of their subtle discoveries in astronomy — discoveries that are more ingenious than those of the Greeks and the Babylonians - and of their valuable methods of calculation which surpass description. I wish only to say that this computation is done by means of nine signs. If those who believe that because they speak Greek they have arrived at the limits of science would read the Indian texts they would be convinced even if a little late in the day that there are others who know something of value.

The addition of zero as a tenth positional digit is documented from the 7th century by Brahmagupta, though the earlier Bakhshali Manuscript, written sometime before the 5th century, also included zero. But it is in Khmer numerals of modern Cambodia where the first extant material evidence of zero as a numerical figure, dating its use back to the seventh century, is found.

As it was from the Arabs that the Europeans learned this system, the Europeans called them "Arabic numerals;" the Arabs refer to their numerals as "Indian numerals". In academic circles they are called the "Hindu–Arabic" or "Indo–Arabic" numerals.

The significance of the development of the positional number system is probably best described by the French mathematician Pierre Simon Laplace (1749–1827) who wrote:

It is India that gave us the ingenious method of expressing all numbers by the means of ten symbols, each symbol receiving a value of position, as well as an absolute value; a profound and important idea which appears so simple to us now that we ignore its true merit, but its very simplicity, the great ease which it has lent to all computations, puts our arithmetic in the first rank of useful inventions, and we shall appreciate the grandeur of this achievement when we remember that it escaped the genius of Archimedes and Apollonius, two of the greatest minds produced by antiquity.

Tobias Dantzig had this to say in "Number":

This long period of nearly five thousand years saw the rise and fall of many civilizations, each leaving behind a heritage of literature, art, philosophy, and religion. But what was the net achievement in the field of reckoning, the earliest art practiced by man? An inflexible numeration so crude as to make progress well nigh impossible, and a calculating device so limited in scope that even elementary calculations called for the services of an expert. [...] man used these devices for thousands of years [...] without contributing a single important idea to the system!





</doc>
<doc id="15524" url="https://en.wikipedia.org/wiki?curid=15524" title="Ian Botham">
Ian Botham

Sir Ian Terence Botham, OBE (born 24 November 1955) is an English former cricketer and current cricket commentator. Widely regarded as one of the greatest all-rounders in cricket history, Botham represented England in both Test and One-Day International cricket. He played most of his first-class cricket for Somerset, and also for Worcestershire, Durham and Queensland. He was an aggressive right-handed batsman and, as a right arm fast-medium bowler, was noted for his swing bowling. He generally fielded close to the wicket, predominantly in the slips. In Test cricket, Botham scored 14 centuries with a highest score of 208, and from 1986 to 1988, he held the world record for the most Test wickets until overtaken by fellow all-rounder Sir Richard Hadlee. He took five wickets in an innings 27 times and 10 wickets in a match four times. In 1980, he became the second player in Test history to complete the "match double" of scoring 100 runs and taking 10 wickets in the same match.

Botham has at times been involved in controversy including a highly publicised court case involving rival all-rounder Imran Khan and an ongoing dispute with the Royal Society for the Protection of Birds (RSPB). These incidents, allied to his on-field success, have attracted media attention, especially from the tabloid press. Botham has made effective use of the fame given to him by the publicity because he is actively concerned about leukaemia in children and has undertaken several long distance walks to raise money for research into the disease. These efforts have been highly successful and have realised millions of pounds for Bloodwise, of which he became president. In recognition of his services to charity, he was awarded a knighthood in the 2007 New Years Honours List. On 8 August 2009, he was inducted into the ICC Cricket Hall of Fame.

Botham has a wide range of sporting interests outside cricket. He was a talented footballer at school and had to choose between cricket and football as a career. He chose cricket but, even so, he did play professional football for a few seasons and made eleven appearances in the Football League for Scunthorpe United. He is a keen golfer and his other pastimes include angling and shooting.

Ian Botham was born in Heswall, which at the time was part of Cheshire county, to Herbert Leslie ("Les") Botham and Violet Marie, Collett. His father had been in the Fleet Air Arm for twenty years spanning the Second World War; his mother was a nurse. The family moved to Yeovil before Botham's third birthday after his father got a job as a test engineer at Westland Helicopters. Both his parents played cricket: his father for Westland Sports Club while his mother captained a nursing services team at Sherborne. Botham developed an eagerness for the game before he had started school: he would climb through the fence of the Yeovil Boys' Grammar School to watch the pupils play cricket. At the age of around four, he came home with a cricket ball and asked his mother "Do you know how to hold a ball when you're going to bowl a daisy-cutter?" He subsequently demonstrated the grip and went away to practise bowling it. 

Botham attended Milford Junior School in the town and it was there that his "love affair" with sport began. He played both cricket and football for the school's teams at the age of nine; two years earlier than most of his contemporaries. Playing against the older boys forced Botham to learn to hit the ball hard, and improve to their standard. At the same age he went to matches with his father, who played for Westland Sports Club, and if one of the teams was short, he would try to get a match. His father recalled that though he never got to bowl, and rarely got to bat, he received praise for the standard of his fielding. He joined the Boys' Brigade where more sporting opportunities were available. By the time he was nine, he had begun to "haunt" local recreation grounds with his kit always ready, looking to play for any team that was short of players. By the age of twelve he was playing occasional matches for Yeovil Cricket Club's second team.

Botham went on to Bucklers Mead Comprehensive School where he continued to do well in sport and played for the school's cricket and football teams. He became captain of their under-16 cricket team when he was thirteen. His performances for the school drew the attention of Somerset County Cricket Club's youth coach Bill Andrews. Still thirteen, he scored 80 runs on debut for Somerset's under-15s side against Wiltshire, but the team captain Phil Slocombe did not call on him to bowl as he considered him to be a specialist batsman. Two years later, Botham had the opportunity to choose between football and cricket: Bert Head, manager of Crystal Palace offered him apprentice forms with the First Division club. He already had a contract with Somerset and, after discussing the offer with his father, decided to continue to pursue a cricket career, as he believed he was a better cricketer. When informed that he wanted to be a sportsman, Botham's careers teacher said to him: "Fine, everyone wants to play sport, but what are you really going to do?"
In 1972, at the age of 16, Botham left school intent on playing cricket for Somerset, who retained his contract but felt he was too young to justify a full professional deal. So, Botham joined the ground staff at Lord's. As a ground boy, he had numerous tasks such as "cleaning the pavilion windows, pushing the roller on matchdays, selling scorecards, pressing electronic buttons on the scoreboards and rushing bowling analyses to the dressing-room". He also received coaching and plenty of time in the practice nets, and was often the first to arrive and the last to leave practice. Despite his time in the nets, Botham was only considered by Marylebone Cricket Club (MCC) coach Harry Sharp to have the potential to become a "good, average county cricketer." Botham travelled to play for Somerset under-25s a number of times during the season, but failed to excel in any of the matches. His appearances for the MCC were of a similar vein: he rarely scored more than 50 runs, and was used sparingly as a bowler. In one such match against "Scotland A", the MCC Young Cricketers used eight bowlers in their second innings, but Botham was not among them.

The following year, still a ground boy at Lord's, Botham was asked to return to play for Somerset's under-25s more often. Against Glamorgan U-25, he scored 91 runs and took three tail-end wickets, while just under a month later he claimed a further three wickets against Hampshire. He advanced to play for the county's second team in the Minor Counties Championship, and although he was still used sparingly as a bowler, he made some good scores with the bat, most significantly against Cornwall, against whom he aggregated 194 runs in four innings. During winter nets prior to the season, Botham had caught the eye of the former England Test cricketer Tom Cartwright, who coached at Millfield School in addition to playing for Somerset. Cartwright was impressed with Botham's foot-work and physical co-ordination, and helped him learn the basics of swing bowling, something Botham picked up "astonishly quickly" according to Cartwright.

Botham had done well for the Second XI and he later acknowledged the help and advice he received from Somerset players Peter Robinson, Graham Burgess and Ken Palmer. Botham made his senior debut, aged 17, for Somerset on Sunday, 2 September 1973 when he played in a List A John Player League (JPL) match (38 overs each) against Sussex at the County Ground, Hove. The match was in the same week that his time on the Lord's ground staff was completed. Somerset batted first and Botham, number seven in the batting order, scored two runs before he was dismissed leg before wicket (lbw) by Mike Buss. Somerset totalled 139 for 9. Sussex won comfortably by six wickets, reaching 141 for four with fifteen deliveries remaining. Botham bowled three overs without success, conceding 22 runs. He did impress, however, by taking a diving catch to dismiss his future England colleague Tony Greig off the bowling of his captain Brian Close.

A week later, Botham made a second appearance in the JPL against Surrey at The Oval in the final match of the season. Somerset were well beaten by 68 runs. Botham had his first bowling success when he dismissed Geoff Howarth lbw. He bowled four overs and took one for 14. As in his first match, he scored two batting at number seven, this time being caught and bowled by Intikhab Alam. These were his only two senior appearances in 1973, Somerset finishing 11th in the JPL. In summary, Botham scored four runs, took one wicket for 14 and held one catch.

Aged 18, Botham was a regular in the Somerset team from the beginning of the 1974 season and made his first-class début 8–10 May in a County Championship match against Lancashire at the County Ground, Taunton. Viv Richards, from Antigua and Barbuda, made his County Championship début for Somerset in the same match and Lancashire's team included Clive Lloyd, two players who would loom large in Botham's future Test career. Brian Close won the toss and decided to bat first. On day one, Somerset were all out for 285 and Lancashire reached 41 for none. Botham batted at number seven and scored 13 before being caught. Day two was rain-affected and Lancashire advanced to 200 for none. Their innings closed on the final day at 381 for eight. Botham bowled only three overs and his figures were none for 15; he held one catch to dismiss Jack Simmons. Somerset played for the draw and were 104 for two at the end. Botham did not bat again.

On 12 June 1974, he played against Hampshire at Taunton in a Benson & Hedges Cup (B&H Cup) quarter-final. Hampshire won the toss and decided to bat. They scored 182 all out with Botham taking two for 33 including the prize wicket of Barry Richards, bowled for 13. Botham was number nine in Somerset's batting order and came in with his team struggling at 113 for 7. Almost immediately, that became 113 for 8 and he had only the tailenders Hallam Moseley and Bob Clapp to support him. He was facing the West Indian fast bowler Andy Roberts who delivered a bouncer which hit him in the mouth. Despite heavy bleeding and the eventual loss of four teeth, Botham refused to leave the field and carried on batting. He hit two sixes and made 45*, enabling Somerset to win by one wicket. He won the Gold Award. Later, he said he should have left the field but was full of praise for Moseley and Clapp.

In a County Championship match on 13 July 1974, Botham scored his first half-century in first-class cricket. He made 59 in Somerset's first innings against Middlesex at Taunton, the highest individual score in a low-scoring match which Somerset won by 73 runs. Middlesex's captain was Mike Brearley, who would become a very influential figure in Botham's career. A month later, in a match against Leicestershire at Clarence Park, Weston-super-Mare, Botham achieved his first-ever five wickets in an innings (5wI) with five for 59. He took seven in the match which Somerset won by 179 runs, largely thanks to Close who scored 59 and 114*.

Botham showed great promise in 1974, his first full season in which Somerset finished fifth in the County Championship and a close second to Leicestershire in the JPL. They also reached the semi-finals in both the Gillette Cup and the B&H Cup. In 18 first-class appearances, Botham scored 441 runs with a highest of 59, took 30 first-class wickets with a best of five for 59 and held 15 catches. He played in 18 List A matches too, scoring 222 runs with a highest of 45* (his Gold Award innings against Hampshire), took 12 wickets with a best of two for 16 and held four catches.

Botham continued to make progress in 1975. Somerset struggled in the County Championship, winning only four of their twenty matches and finished joint 12th. In the JPL, they slumped badly from second to 14th. They reached the quarter-final of the B&H Cup but only the second round of the Gillette Cup. Botham played in 22 first-class and 23 List A matches so it was a busy season for him. In first-class, he scored 584 runs with a highest of 65, one of two half-centuries, and held 18 catches. He took 62 wickets, doubling his 1974 tally, with a best of five for 69, his only 5wI that season. In List A, he scored 232 runs with a highest of 38* and held seven catches. He took 32 wickets with a best of three for 34.

1976 was a significant season for Botham as he scored over 1,000 runs for the first time, completed his first century and earned international selection by England in two Limited Overs Internationals. Somerset improved in the County Championship to finish seventh, winning seven matches. They were one of five teams tied for first place in the JPL but their run rate was less than that of Kent, who were declared the champions. Somerset lost their opening match in the Gillette Cup and were eliminated at the group stage of the B&H Cup. Botham, though, came on in leaps and bounds. He totalled 1,022 first-class runs in 20 matches with a highest of 167*, his first-ever century and he also scored six half-centuries. With the ball, he took 66 wickets with a best of six for 16. He had four 5wI and, for the first time, ten wickets in a match (10wM). He played in a total of 22 List A matches, including the two for England, scoring 395 runs with a highest of 46. He took 33 wickets with a best of four for 41.

In the County Championship match against Sussex at Hove in May, Botham came very close to his maiden century but was dismissed for 97, his highest score to date. The match was drawn. At the end of the month, Somerset played Gloucestershire in a remarkable match at Taunton. Batting first, Somerset scored 333 for seven (innings closed) and then, thanks to six for 25 by Botham, bowled out Gloucestershire for only 79. The follow-on was enforced but Gloucestershire proved a much tougher nut to crack second time around. With Zaheer Abbas scoring 141, they made 372 and left Somerset needing 118 to win. Botham took five for 125 in the second innings for a match analysis of 11 for 150, his maiden 10wM. This match ended the same way as the famous Test at Headingley in 1981 but the boot was on the other foot for Botham here because he was on the team that enforced the follow-on – and lost. Mike Procter and Tony Brown did the damage and bowled Somerset out for 110 in 42 overs, Gloucestershire winning by just eight runs. 

Botham scored his maiden first-class century at Trent Bridge on Tuesday 3 August 1976 in the County Championship game against Nottinghamshire (Notts) who won the toss and decided to bat first. Derek Randall scored 204* and the Notts innings closed at 364 for 4 (Botham one for 59). Somerset were 52 for one at close of play. On day two, Somerset scored 304 for 8 (innings closed) and Botham, batting at number six, scored 80. At close of play, Notts in their second innings were 107 for four, thus extending their lead to 167 with six wickets standing. On day three, Notts advanced to 240 for nine declared (Botham one for 16), leaving Somerset with a difficult target of 301. At 40 for two and with both their openers gone, Brian Close changed his batting order and summoned Botham to come in at number four. Close himself had gone in at three but he was out soon afterwards for 35. With support from Graham Burgess (78), Botham laid into the Notts bowling and scored an impressive 167 not out. Somerset reached 302 for four in only 65 overs and won by six wickets.

Botham's international début for England was on 26 August 1976 in a Limited Overs International (LOI) against the West Indies at the North Marine Road Ground, Scarborough. The series was called the Prudential Trophy and the teams had 55 overs each per innings. Botham, still only 20, was the youngest player. At Scarborough, England captain Alan Knott lost the toss and Clive Lloyd, captaining the West Indies, elected to field first. Botham was number seven in the batting order and came in at 136 for five to join Graham Barlow. He scored only one before he was caught by Roy Fredericks off the bowling of his future "Sky Sports" colleague Michael Holding. England's innings closed at 202 for eight with Barlow 80 not out. West Indies lost Fredericks almost immediately but that brought Viv Richards to the crease and he hit 119 not out, winning the man of the match award, and leading West Indies to victory in only 41 overs by six wickets. Botham had the consolation of taking his first international wicket when he had Lawrence Rowe caught by Mike Hendrick for 10. He bowled only three overs and took some punishment from Richards, his return being one for 26. 

In the second match at Lord's, Botham was replaced by returning England captain Tony Greig. England lost by 36 runs as Richards, this time with 97, was again the difference between the teams. Having lost the series, England recalled Botham for the final match at Edgbaston on 30–31 August. The match was extended to two days and overs reduced to 32 per side. Tony Greig won the toss and decided to field. England began well and dismissed Fredericks and Richards, for a duck, in only the second over. West Indies were then seven for one but a powerful innings by Clive Lloyd pulled them out of trouble and they reached 223 for nine, innings closed. Botham bowled three very expensive overs, conceding 31 runs, but he did manage to bowl out Michael Holding for his second international wicket. England were never in the hunt and were bowled out for 173, West Indies winning by 50 runs and claiming the series 3–0. Botham again batted at number seven and made a good start, scoring 20 at a run a ball, but he was then caught by Bernard Julien off Fredericks and England were 151 for seven with only Knott and the tailenders left.

In the winter of 1976–77, after he had made his first two international appearances, Botham played Grade cricket in Australia for the University of Melbourne Cricket Club. He was joined by Yorkshire's Graham Stevenson. They were signed for the second half of the season on a sponsorship arranged through the Test and County Cricket Board (TCCB) by Whitbread's Brewery. Five of the competition's 15 rounds were abandoned because of adverse weather. It was apparently on this trip that Botham originally fell out with the former Australian captain Ian Chappell. The cause seems to have been a cricket-related argument in a bar, which may have resulted in Chappell being pushed off his stool (the story is widely sourced but accounts differ). This became a long-running feud and, as late as the 2010–11 Ashes series, there was an altercation between Botham and Chappell in a car park at the Adelaide Oval.

Botham produced a number of good batting and bowling performances for Somerset in 1977 and these impressed the Test selectors who included him in the team for the third Test against Australia at Trent Bridge, starting on 28 July. Having captured 36 first-class wickets through May and June, Botham had something of a purple patch in July which earned him his Test call-up. In the match against Sussex at Hove, which Somerset won by an innings and 37 runs, he took four for 111 and six for 50 for his second 10wM. In Somerset's innings of 448 for eight, he shared a 4th wicket partnership of 174 with Viv Richards. Botham scored 62, Richards 204. He took 22 more wickets, including two 5wI, in the next three County Championship games before his Test debut. In the whole season, playing 17 first-class matches, he took 88 wickets with six 5wI and one 10wM, his second innings return at Hove being his best. His batting was not quite as good as in 1976 as his average was down but he scored 738 runs with a highest of 114, which was his sole century, and five half-centuries. He scored the century in July against Hampshire at Taunton, 114 in Somerset's first innings of 284, and followed it with bowling returns of four for 69 and four for 43, another impressive all-round effort which earned Somerset a win by 152 runs. Somerset had a good season in the County Championship, finishing fourth. They reached the semi-final of the Gillette Cup but, without the injured Botham, were well beaten by eventual winners Middlesex. They were a poor tenth in the JPL and were eliminated from the B&H Cup at the group stage.

Botham made his Test début at Trent Bridge on 28 July 1977 in the third Test against Australia. His début was somewhat overshadowed by the return from self-imposed Test exile of Geoffrey Boycott. England went into the match with a 1–0 series lead having won the second Test after the first had been drawn. The series was played against the background of the so-called "Packer Affair" which resulted in the establishment of World Series Cricket in the next Australian season. Because of Tony Greig's involvement, he had been stripped of the England captaincy but remained in the team under new captain Mike Brearley. England had three all-rounders at Trent Bridge with Greig, Geoff Miller and Botham all playing. Australian captain Greg Chappell won the toss and decided to bat first. Australia scored 243 and were all out shortly before the close on day one. Botham, aged 21, made an immediate impact and took five for 74, the highlight being the wicket of Chappell, bowled for just 19. England batted all through day two and into day three as Boycott, in his first Test innings since 1974, and Knott both made centuries. Botham came in at number eight on day three and scored 25 before he was bowled by Max Walker. England were all out not long afterwards for 364, a first innings lead of 121. Botham had no joy in Australia's second innings with none for 60. A century by Rick McCosker enabled Australia to score 309 before they were all out in the evening session on day four. Bob Willis took five for 88. England needed 189 to win and completed the job, by seven wickets, well into the final day with Brearley scoring 81 and Boycott, who batted on all five days, 80 not out. Botham didn't get a second innings.

Botham's impressive bowling at Trent Bridge meant he was an automatic choice for the fourth Test at Headingley two weeks later. England won the toss, decided to bat first and went on to win by an innings and 85 runs to secure a winning 3–0 lead in the series and regain The Ashes, which they had lost in 1974–75. The match is famous for Boycott's one hundredth career century, scored on his home county ground and in his second Test since his return to the England fold. Botham was bowled third ball by Ray Bright without scoring. He made amends with the ball by taking five for 21 in only eleven overs, Australia being bowled out for only 103. The follow-on was enforced and Australia this time made 248, but Botham (none for 47) did not take a wicket. He was injured during the second innings when he accidentally trod on the ball and broke a bone in his foot. He was unable to play again in the 1977 season.

His promising start as Test player resulted in two awards. He was named Young Cricketer of the Year for 1977 by the Cricket Writers' Club; and was selected as one of the "Wisden Cricketers of the Year" (i.e., for 1977 but announced in the 1978 edition). "Wisden" commented that his 1977 season "was marred only by a week's cricket idleness carrying the drinks at the Prudential matches, and a foot injury which ruined for him the end of the season and probably robbed him of a rare double. He finished with 88 wickets and 738 runs". Importantly, the foot injury was a broken toe sustained when he trod on the ball at Headingley and Botham subsequently needed treatment for it at his local hospital in Taunton. It was while going to one of his appointments that he took a wrong turn and ended up on a children's ward where he learned that some of the children were dying of leukaemia. This incident sparked his charitable crusade on behalf of leukaemia research.

England were in Pakistan from November 1977 to January 1978, playing three Tests and three LOIs. Botham was almost fully recovered from his foot injury but did not play in any of the Tests. He took part in all three LOIs and in some of the first-class matches against club teams. From January to March, England were in New Zealand for a three-match Test series under the captaincy of Geoff Boycott. Botham impressed in a first-class match against Canterbury at Lancaster Park, scoring 126 not out in the second innings against an attack including Richard Hadlee and was selected for the first Test at Basin Reserve. Botham had an indifferent game there and England, twice bowled out by Hadlee, lost by 72 runs. In the next match at Carisbrook against Otago, Botham achieved a 10wM with seven for 58 (his career best return to date) in the second innings, enabling the England XI to win by six wickets. England won the second Test at Lancaster Park by 174 runs after an outstanding all-round performance by Botham who scored 103 and 30 not out and took five for 73 and three for 38. He also held three catches. The final Test was played at Eden Park and was drawn, the series ending 1–1. New Zealand batted first and totalled 315 with Geoff Howarth scoring 122. Botham took five for 109 in 34 overs. England replied with 429 all out (Clive Radley 158, Botham 53). New Zealand then chose to bat out time and Howarth scored his second century of the match (Botham none for 51). Botham's form in New Zealand cemented his place in the England team.

In the 1978 English season, Pakistan and New Zealand both visited to play three Tests each and Botham featured in all six matches. Having scored exactly 100 in the first Test against Pakistan at Edgbaston, England winning by an innings and 57 runs, Botham in the second at Lord's scored 108 and then, after none for 17 in the first innings, achieved his Test and first-class career best return of eight for 34 in the second, England winning by an innings and 120 runs. The third Test was ruined by the weather and England won the series 2–0. Against New Zealand, Botham did little with the bat but his bowling was outstanding. In the second Test he took nine wickets in the match as England won by an innings and then a 10wM in the final match at Lord's with six for 101 and five for 39. England won the series 3–0.

Due to his England commitments, Botham appeared infrequently for Somerset in 1978. His best performances for them were a return of seven for 61 against Glamorgan and an innings of 80 against Sussex in the Gillette Cup final at Lord's. This was Somerset's first limited overs final and they lost by five wickets despite Botham's effort. They were involved in a tight contest for the JPL title and were placed second on run rate after tying with Hampshire and Leicestershire on 48 points each. Somerset did quite well in the County Championship, finishing fifth after winning nine matches, and reached the semi-final of the B&H Cup.

Botham's first tour of Australia was in 1978–79. England, defending the Ashes they had regained in 1977, played six Tests under Mike Brearley's leadership. Australia had what was effectively "a reserve team" because their leading players were contracted to World Series Cricket for the season. The difference in standard was evident on the first day of the first Test at the Gabba as Botham, Chris Old and Bob Willis bowled them out for only 116 in just 38 overs, England going on to win easily enough by seven wickets. Apart from a surprise defeat in the third Test, England were never troubled and won the series 5–1. Botham's performance in the series was satisfactory but there were no headlines and only modest averages. He took 23 wickets at 24.65 with a best return of four for 42. He scored 291 runs with a highest of 74 at 29.10. He held 11 catches.

Botham played for England in the 1979 Cricket World Cup and was a member of their losing team in the final. He was again an infrequent member of the Somerset team because of the World Cup and the Test series against India. It became a memorable season for Somerset as they built on their form in 1978 to win both the Gillette Cup and the JPL, their first-ever senior trophies. Botham played in the Gillette Cup final at Lord's, in which they defeated Northamptonshire by 45 runs, thanks to a century by Viv Richards. They slipped to eighth in the County Championship. In the B&H Cup, however, they were expelled from the competition for bringing the game into disrepute after an unsporting declaration, designed to protect the team's run rate, by team captain Brian Rose.

The England v India series in 1979 took place after the World Cup ended and four Tests were played. England won the first at Edgbaston by an innings and 83 runs after opening with a massive total of 633 for five declared. Botham scored 33 and then took two for 86 and five for 70. On the first day of the second Test at Lord's, Botham swept through the Indian batting with five for 35 and a catch off Mike Hendrick to dismiss them for only 96 in 56 overs. Surprisingly, however, India recovered to salvage a draw. In the third Test at Headingley, it was Botham the batsman who did the business, scoring 137 from 152 balls in England's first innings total of 270 (the next highest innings was 31 by Geoff Boycott). The match was ruined by the weather and was drawn. In the final Test at The Oval, England opened with 305 (Botham 38); India replied with 202 (Botham four for 65); and England with 334 for eight declared (Botham run out for a duck) extended their lead to 437 with four sessions remaining. Thanks to a brilliant 221 by Sunil Gavaskar, India came agonisingly close to pulling off a remarkable last day victory but ran out of time on 429 for eight (Botham three for 97), just nine runs short, and so England won the series 1–0 with three draws.

The shambolic state of international cricket at the end of the 1970s was illustrated by the panic resulting from a hastily convened settlement between World Series Cricket and the Australian Board of Control. Although they had visited Australia only twelve months earlier to play for the Ashes, England were persuaded to go there again and play another three Tests, but with the Ashes not at stake. As "Wisden" put it, the programme did not have the best interests of cricket at heart, particularly Australian cricket below Test level, which had been "swamped by the accent on Test and one-day internationals, neatly parcelled to present a cricketing package suitable for maximum exploitation on television". The matches were widely perceived to be semi-official only and received "a definite thumbs down". Botham was a member of the England team and played in all three matches which, rightly or wrongly, count towards his Test statistics. England were largely faithful to the players who had toured Australia the previous winter and Derek Underwood was the only World Series player they recalled; they did not recall Alan Knott, for example, while Tony Greig was beyond the pale. Australia recalled Greg Chappell, Dennis Lillee, Rod Marsh and Jeff Thomson, fielding a team that was a mixture of old and new. In the first match, played at the WACA Ground, Botham had match figures of eleven for 176 but to no avail as Australia won by 138 runs. Having excelled with the ball in that match, Botham did so with his bat in the third one, scoring an unbeaten 119 in the second innings of the third. Australia won all three matches of a series best forgotten for all its attendant politics, but Botham had enhanced his reputation as a world-class all-rounder.

Botham's third overseas tour was to India in February 1980. It was the fiftieth anniversary of India's entry into Test cricket and so England played a single commemorative Test at the Wankhede Stadium in Bombay. It turned into a personal triumph for Botham who became the first player in Test history to score a century and take ten wickets in the same match. England's wicketkeeper Bob Taylor held ten catches in the match, eight of them off Botham's bowling.

India won the toss and decided to bat first but, with Botham taking six for 58, they were all out on day one for 242. England replied with 296, the highlight being Botham's 114 from just 144 balls; he began his innings with England in trouble at 57 for four. This quickly became 58 for five and Botham was joined by England's other match hero Taylor. England's first five batsmen had contributed just 51 to the total. Botham was often unfairly labelled a "big hitter" but in fact his style was very orthodox (i.e., he "played straight") and in this innings he scored 17 fours but, significantly, no sixes. Taylor provided dogged support and their sixth wicket partnership realised 171 runs. When Botham was out near the end of day two, the score was 229 for six and England reached 232 for six at close of play, still ten runs behind. On the third morning, Taylor led England past India's total and, with useful batting performances by the specialist bowlers, England totalled 296 to gain a first innings lead of 54.

India's second innings was a disaster and they lost eight wickets by the close of play on the third day with only Kapil Dev offering any resistance. They were all out early on the fourth day for 149. Botham was the outstanding performer again, taking seven for 48 which gave him match figures of thirteen for 106. Geoffrey Boycott and Graham Gooch scored the necessary runs for England to win by ten wickets with a day to spare.

Mike Brearley announced his retirement from Test cricket after the Jubilee Test in Bombay and, somewhat surprisingly given his lack of captaincy experience, Botham was appointed to replace him as England's captain for the forthcoming home series against West Indies, who were at the time the world's outstanding team. Botham led England in twelve Tests in 1980 and 1981 but he was unsuccessful, the team achieving no wins, eight draws and four defeats under his leadership. In addition, his form suffered and was eventually dismissed from the post, although he did actually resign just before the selectors were about to fire him. In Botham's defence, nine of his matches as captain were against West Indies, who afterwards won twelve of their next thirteen Tests against England. The other three were all against Australia.

In 1980, which was a wet summer, West Indies had the better of all five Test matches, although, with the rain constantly intervening, they were able to win only one of them. Botham had a poor season as a bowler and, in all first-class cricket, took just 40 wickets at the high average of 34.67 with a best return of only four for 38. He did better as a batsman, scoring 1,149 runs (the second time, after 1976, that he topped a thousand in a season) at 42.55. He completed two centuries and six other half-centuries. His highest score in the season was ultimately the highest of his career: 228 for Somerset against Gloucestershire at Taunton in May. He batted for just over three hours, hitting 27 fours and ten sixes. With Gloucestershire batting out time for a draw on the final day, Somerset used all eleven players as bowlers. Apart from an innings of 57 in the first Test, Botham contributed little to England in the series and that innings was the only time he reached 50 in all his twelve Tests as England captain.

Somerset came close to retaining their JPL title in 1980 but had to be content with second place, only two points behind Warwickshire. They finished a credible fifth in the County Championship but were eliminated from both the Gillette and B&H Cups in the opening phase.

Botham led England on the controversial tour of the West Indies from January to April 1981. The second Test, scheduled to be played at Bourda, was cancelled after the Guyanese government revoking the visa of Robin Jackman because of his playing and coaching links with South Africa. The other four Tests were played and West Indies won the series 2–0 but England were helped by rain in the two drawn matches. Botham took the most wickets for England, but "Wisden" said "his bowling never recovered the full rhythm of a year before". His batting, however, apart from one good LOI performance, "was found wanting in technique, concentration and eventually in confidence". In "Wisden's" view, Botham's loss of form "could be cited as eloquent evidence of the undesirability of saddling a fast bowler and vital all-rounder with the extra burden of captaincy".

The England captaincy had affected Botham's form as a player and in his last Test as captain, against Australia at Lord's in 1981, he was dismissed for a pair. According to "Wisden" editor Matthew Engel, writing in "ESPNcricinfo", Botham "resigned (a minute before being sacked), his form shot to pieces" after that match. Australia were then leading the series 1–0 after two Tests with four more to be played. Botham was replaced by the returning Mike Brearley, who had been his predecessor until retiring from Test cricket in 1980.

Botham continued to play for England under Brearley and achieved the highpoint of his career in the next three Tests as England recovered to win The Ashes. In the third Test at Headingley, Australia opened with 401 for 9 declared, despite good bowling by Botham who took 6 for 95. England responded poorly and were dismissed for 174. Botham was the only batsman to perform at all well and scored 50, which was his first Test half-century since he had been awarded the captaincy thirteen Tests earlier. Having been forced to follow-on, England collapsed again and at 135 for 7 on the afternoon of the fourth day, an innings defeat looked certain. Bookmakers had reportedly been offering odds of 500/1 against an England win after the follow-on was enforced. Botham, himself not long at the wicket, was the remaining recognised batsman as he was joined by the fast bowler Graham Dilley, number nine in the batting order, with only Chris Old and Bob Willis to come. With able support from Dilley (56) and Old (29), Botham hit out and by the close of play was 145 not out with Willis hanging on at the other end on 1 not out. England's lead was just 124 but there remained a glimmer of hope. On the final day's play, there was time for four more runs from Botham before Willis was out and Botham was left on 149 not out. Australia, with plenty of time remaining, needed 130 to win and were generally expected to get them. After Botham took the first wicket, Willis took 8 for 43 to dismiss Australia for only 111. England had won by 18 runs and it was only the second time in history that a team following on had won a Test match.

Botham's outstanding form continued through the next two Tests. In the fourth at Edgbaston, a low-scoring match left Australia batting last and needing 151 to win. They reached 105 for 5 and were still favourites at that point but, in an inspired spell of bowling, Botham then took five wickets for only one run in 28 balls to give England victory by 29 runs. In the fifth Test at Old Trafford, Botham scored 118 in a partnership of 149 with Chris Tavaré before he was dismissed. He hit six sixes in that innings. England won that match to take a winning 3–1 series lead. The last Test at The Oval was drawn, Botham achieving a 10wM by taking six for 125 and four for 128. He was named Man of the Series after scoring 399 runs, taking 34 wickets and holding 12 catches.

Somerset won the Benson & Hedges Cup for the first time in 1981 and did well in the County Championship too, finishing third. They were again runners-up in the JPL but a long way behind the winners Essex. In the renamed NatWest Trophy (formerly Gillette Cup), Somerset were knocked out in the second round. Botham played in the B&H final at Lord's, in which Somerset defeated Surrey by seven wickets. He took no wickets but provided Viv Richards (132 not out) with good support in the run chase. Botham ended the season with 67 wickets at 25.55, a best return of six for 90 (for Somerset v Sussex) and one 10wM (sixth Test). He scored 925 runs with a highest of 149* (third Test) at 42.04; and held 19 catches.

During this period, Botham played in 25 Tests. There were home series against both India and Pakistan in 1982; and New Zealand in 1983. His overseas tours were to India and Sri Lanka in 1981–82 (he took part in the inaugural Test played by Sri Lanka); to Australia in 1982–83; and to New Zealand and Pakistan in 1983–84. He played for England in the 1983 Cricket World Cup and was a member of their losing team in the semi-final.

Botham's return to India was less than triumphant and "Wisden" took him to task for his "ineffectiveness with the ball". Having achieved a match analysis of nine for 133 at Bombay, where England were beaten on a poor pitch, Botham took only eight more wickets, at 65 each, in the last five Tests and "Wisden" said this "was a telling blow to England's chance of levelling the series".

1982 was a good all-round season for Botham, especially as Somerset retained the Benson & Hedges Cup. In 17 first-class matches, he scored 1,241 runs with a highest of 208 against India (this was ultimately his career highest in Test cricket) at a good average of 44.32. He took 66 wickets at the low average of 22.98 with a best return of five for 46. England won their Test series against Pakistan by 2–1 and the one against India 1–0. Botham scored two centuries against India: 128 at Old Trafford and his career high 208 at The Oval. Somerset finished sixth and ninth in the County Championship and the JPL respectively. They reached the quarter-final of the NatWest Trophy and their season highlight was retaining the B&H Cup they won in 1981. In the final at Lord's, Somerset dismissed Nottinghamshire for only 130 (Botham two for 19)and won easily by nine wickets.

Botham toured Australia again in 1982–83 with England seeking to retain the Ashes, but Australia won the series 2–1 despite England winning, at the Melbourne Cricket Ground (MCG), a Test described by "Wisden" as "one of the most exciting Test matches ever played". Botham had a poor series and tour. He played in nine first-class matches and scored only 434 runs at the low average of 24.11 with a highest of 65. He was no better with the ball, taking just 29 wickets for a too-high 35.62 with a best return of four for 43. He did, however, field well and held 17 catches, nearly two a match.
In the 1983 English season, Somerset won the NatWest Trophy for the first time, defeating Kent in the Lord's final by 24 runs with Botham as their captain. They were very close to taking the JPL title too but, having tied with Yorkshire on 46 points, they were placed second on run rate. In the County Championship, they won only three matches and finished tenth. They were knocked out of the B&H Cup early. Botham had a good season with the bat, scoring 852 runs in his 14 first-class matches at 40.57 with a highest score of 152 among three centuries. He did less well with the ball: only 22 wickets at the high average of 33.09. New Zealand played a four-match Test series against England after the World Cup and, at the 29th attempt, finally defeated England for the first time in a Test match in England. England won the other three matches convincingly, however, to take the series 3–1. Botham did little with the ball, the same story as in his whole season, but he did score a century (103) in the final Test at Trent Bridge (see photo).

In the winter of 1983–84, England toured New Zealand from January to February and Pakistan in March. Apart from one innings at Basin Reserve in the first Test against New Zealand, Botham was a disappointment on this tour, especially as a bowler. He scored 138 in the first Test, sharing in a sixth wicket partnership of 232 with Derek Randall (164), but the match was drawn. It was a poor tour for England, all told, and described by "Wisden" as "ranking among the unhappiest they have ever undertaken". England lost both series 1–0. Botham left Pakistan after the first Test there, the one England lost, to have a knee problem investigated at home.

After ten seasons as a first-team regular, Botham was appointed Somerset club captain in 1984 and 1985. In the County Championship, they finished seventh in 1984 and then dropped to 17th (bottom of the table) in 1985. In the JPL, they were 15th in 1984 and eleventh in 1985. They made little impression in either of the B&H Cup or the NatWest Trophy so, all in all, Botham's captaincy period was a lean time for the club who had enjoyed its most successful period ever in the preceding seasons.

Botham played in 18 Tests from 1984 to 1986, ten of them (five home, five away) against West Indies. Throughout Botham's Test career, the highest international standards were set by West Indies and Botham was generally unsuccessful against them. In both of these series, 1984 and 1985–86, West Indies beat England 5–0 in whitewashes that were dubbed "blackwash". Like all his England team-mates, Botham was generally hapless against one of the all-time greatest cricket teams.

Ironically, his highest score and both his best and worst bowling performances against West Indies occurred in the same match at Lord's in 1984. Clive Lloyd won the toss and, perhaps mistakenly, elected to field. The first day was rain-affected and England, 167 for two overnight, scored 286 thanks to a century by Graeme Fowler; Botham scored a useful 30. West Indies lost three quick wickets, all of them to Botham who was a "reminder of his old self" in the words of "Wisden", but recovered to reach 119 for three at the close of play on day two. In the third morning, Viv Richards was dismissed by Botham under dubious circumstances but Botham was inspired by the capture of his great friend's wicket and went on to take eight for 103, dismissing West Indies for 245 and for once giving England a chance of victory against the world's best team, with a first innings lead of 41. This was Botham's best-ever bowling performance against West Indies by some distance. England began their second innings and had been reduced to 88 for four when Botham joined Allan Lamb. They reached 114 for four at day three close. There was no Sunday play and England resumed on the Monday 155 runs ahead with six wickets standing. Botham and Lamb added 128 for the fifth wicket before Botham was out for 81, including nine fours and one six, easily his highest score and best innings against West Indies. Lamb made a century and England were all out on the Tuesday morning (final day) for exactly 300. West Indies needed 342 to win in five and a half hours. They lost Desmond Haynes to a run out at 57 for 0, whereupon Larry Gomes (92 not out) joined Gordon Greenidge (214 not out) and West Indies went on to win by nine wickets with 11.5 of the last twenty overs to spare. Although "Wisden" does not name Botham except as an "inattentive" fielder who dropped a catch, it describes the England bowlers "looking second-rate and nobody but Willis bowling the right line or setting the right field to the powerful and phlegmatic Greenidge". Botham bowled the most overs, 20, and with nought for 117 he conceded almost a run a ball (Willis had nought for 48 from 15 overs). In mitigation, "Wisden" conceded that Greenidge played "the innings of his life, and his ruthless batting probably made the bowling look worse that it was".

In 1985, Botham played in all six Tests against a poor Australian team as England, themselves a second-rate team based on their recent performances, comfortably regained the Ashes and he was the leading wicket-taker, but the series was dominated by England's specialist batsmen, especially Mike Gatting and David Gower. Botham, who by this time had adopted a dyed blonde mullet haircut as a trademark, contributed relatively little with the bat, compared with the massive totals amassed by Gower, Gatting, Graham Gooch and Tim Robinson. He scored 250 runs at 31.25 with a highest of 85. He did take the most wickets (31 at 27.58 with a best of five for 109) but he was rarely impressive and he was bowling to a weak batting side, Allan Border apart. England's best bowler was Richard Ellison who played only twice and took 17 wickets at only 10.88 with a best of six for 77 and one 10wM.

Botham was suspended for 63 days by the England and Wales Cricket Board in 1986 after he admitted in an interview that he had smoked cannabis. Due to the ban, Botham played in only one Test which was the final one of the series against New Zealand.

Botham was succeeded by Peter Roebuck as Somerset captain for 1986 but, during the season, tensions arose in the Somerset dressing room which eventually exploded into a full-scale row and resulted in the sacking by the club of Botham's friends Viv Richards and Joel Garner. Botham, who supported Richards and Garner, decided to resign at the end of the season. 1986 was not a season for Botham to remember except for one brilliant List A innings when he made his career highest score in the limited overs form of 175 not out for Somerset against Northamptonshire in a 39-over JPL match at the Wellingborough School ground. It was to no avail, however, as the weather intervened and the game ended in no result. His innings remains a ground record.

Botham's final tour of Australia was in 1986–87 under Mike Gatting's captaincy. He played in four Tests and England won the Ashes for the last time until 2005. In many ways, the series was also Botham's last hurrah because he scored his final Test century (138 in the first Test at Brisbane which England won by seven wickets) and took his final Test 5wI (five for 41 in the fourth Test at the MCG which England won by an innings and 14 runs). "Wisden" pointed out that although Botham had a modest series statistically, "he was an asset to the side" because of his enthusiasm and "going out of his way to encourage younger players, especially Phil DeFreitas".

After his resignation from Somerset, Botham joined Worcestershire for the 1987 season and spent five seasons with them. In 1987, he scored 126* against his old county but otherwise he was more successful as a limited overs batsman, scoring two centuries and averaging 40.94. His bowling too was much better in the shorter form, wherein he averaged 21.29 against 42.04 in first-class. His limited overs efforts helped Worcestershire to win the Sunday League, now under a different sponsor. They finished ninth in the County Championship and were unsuccessful in the two knockout trophies.

Botham played in the five 1987 Tests against Pakistan, the last time he represented England in a full series. He scored 232 runs in the series with one half-century (51*) at 33.14; and took only seven wickets which were enormously expensive. Pakistan won by an innings at Headingley with the other four Tests drawn. When Pakistan totalled 708 at The Oval, the 217 runs conceded by Botham, from 52 overs, were the most by an England bowler, passing the 204 by Ian Peebles, from 71 overs, against Australia at The Oval in 1930.

Botham spent the 1987–88 Australian season with Queensland, playing for them in the Sheffield Shield. Queensland were one of the better state teams in the 1980s and were always in the Shield's top three from 1983–84 to 1990–91 but didn't win it. In Botham's season there, his team-mates including Allan Border (captain), wicketkeeper Ian Healy and pace bowler Craig McDermott, they finished second to Western Australia. Botham scored several half-centuries and took a reasonable number of wickets and helped Queensland make the Sheffield Shield final. When the Queensland team flew to Perth for the final, Botham was involved in an altercation where he allegedly assaulted a fellow airline passenger who had intervened in an argument between the Queensland players. Queensland lost the final. Botham was fined $800 by a magistrate and $5,000 by the Australian Cricket Board. He was consequently sacked by Queensland.

Botham was unfit for most of the 1988 season and played in only four first-class and seven limited overs matches during April and May. He did not play for England. Nevertheless, Worcestershire won both the County Championship and the Sunday League. Botham was out of action for eleven months.

He returned in May 1989 and, bowling well in the County Championship, helped Worcestershire to a second successive title. With England struggling against Allan Border's rebuilt Australian team which featured the likes of Healy, McDermott, Steve Waugh, Merv Hughes and Mark Taylor, Botham was recalled for the third, fourth and fifth Tests of the pivotal Ashes 1989 series. He could do little to stem a tide which had now turned completely in Australia's favour and looked completely out of his depth. He scored only 62 runs at the very low average of 15.50 and took just three wickets at an enormously expensive 80.33.

Another two-year absence from Test cricket ensued until he was recalled again to play against West Indies at The Oval in August 1991. Two weeks later, he played against Sri Lanka at Lord's. He helped Worcestershire to win the B&H Cup for the only time in 1991.

Botham's final tour was to Australia and New Zealand in 1991–92. He played in the World Cup in Australia and in the third Test in New Zealand. Botham did not win any man of the match awards in the World Cup until 1992 when he won two. Against India at the WACA Ground, he bowled tightly and restricted India, needing 237, to only 27 runs from his ten overs, an economy rate of 2.70 which was significantly lower than anyone else's. He captured two wickets and one of them was Sachin Tendulkar. England won by nine runs. Against Australia at Sydney Cricket Ground later in the competition, Botham won the award for the sort of all-round performance which had made his reputation. Australia won the toss and decided to bat first. They scored 171 all out in 49 overs and Botham took four for 31 in his ten. He then opened the England innings with Graham Gooch and scored 53 from only 77 balls in a partnership with Gooch of 107. England went on to win by eight wickets with nine overs to spare.

In 1992, Botham joined County Championship newcomers Durham and he played in the first two Tests against Pakistan, the second one at Lord's being his final Test appearance. Botham scored 2 and 6, cheaply dismissed each time by the pace of Waqar Younis. As a bowler, he was used for only five overs, his final Test return being none for nine. England lost the match by two wickets and Pakistan went on to win the series 2–1.

It was in 1992 that Botham was appointed an Officer of the Order of the British Empire (OBE) for services to cricket and for his charity work in the Queen's Birthday Honours.

Botham retired from cricket midway through the 1993 season, his last match being for Durham against the visiting Australians at The Racecourse 17–19 July 1993. Durham batted first and scored 385 for eight declared (Wayne Larkins 151). In his final first-class innings, Botham scored 32. In reply, Australia could only make 221, thanks to Simon Brown who took seven for 70 (Botham none for 21). Being 164 behind, Australia had to follow on and a victory for Durham was possible but centuries by Matthew Hayden and David Boon saved Australia and the match was drawn. Botham's final bowling return was none for 45 from eleven overs. In the final over of the game, Botham also kept wicket, without wearing gloves or pads.

Botham's Test career spanned 16 seasons and he played in 102 matches. He scored 5,200 runs at an average of 33.54 with a highest score of 208 in his 14 centuries. He took 383 wickets at an average of 28.40 with a best return of eight for 34 and achieved ten wickets in a match four times. He held 120 catches.

In 116 LOIs from 1976 to 1992, he scored 2,113 runs with a highest score of 79; took 145 wickets with a best return of four for 31; and held 36 catches. A straight comparison of these totals with those of his Test career reveal that he was less effective in the limited overs form of the game. He did have some outstanding LOI matches, however, winning six man of the match awards. Botham took part in three editions of the Cricket World Cup: 1979, 1983 and 1992. He played in 22 World Cup matches including the finals in 1979 and 1992, both of which England lost, and he was in England's losing team in the 1983 semi-final.

Botham was the 21st player to achieve the "double" of 1,000 runs and 100 wickets in Test cricket and he went on to score 5,200 runs and take 383 wickets, as well as holding 120 catches.

He held the world record for the greatest number of Test wickets from 21 August 1986 to 12 November 1988. His predecessor was Dennis Lillee who had retired with 355 wickets in 70 matches. Botham extended the record to 373 in 94 matches before he was overtaken by Richard Hadlee. Botham ended with 383 wickets in 102 matches while Hadlee extended the record to 431 in 86 matches. See List of Test cricket records#Career.

As described above, Botham in 1980 became the second player to achieve the "match double" of 100 runs and ten wickets in Test cricket, following Alan Davidson in 1960–61. Botham was, however, the first to score a century and take ten wickets in a Test match (Davidson scored 44 and 80). The century and ten double has since been achieved by Imran Khan who scored 117 and took six for 98 and five for 82 against India at the Iqbal Stadium in Faisalabad in January 1983.

Compared with many of cricket's greatest players, most of whom were specialists, Botham's averages seem fairly ordinary but this overlooks the fact that he was a genuine all-rounder and it is rare for this type of player to achieve world-class status. Since the Second World War, Botham is one of perhaps a dozen or so world-class all-rounders whereas there have been numerous world-class specialists. Some of the great all-rounders, such as Garfield Sobers and Jacques Kallis as batsmen or Alan Davidson and Richard Hadlee as bowlers, could justifiably be described as world-class specialists in their main discipline who were effective practitioners of the other. The genuine all-rounders to achieve world-class status during the era, besides Botham himself, have included Keith Miller, Richie Benaud, Mike Procter, Clive Rice, Imran Khan, Kapil Dev and Andrew Flintoff.

Of note, Botham's first 202 wickets came at 21.20 per wicket, while his final 181 cost on average 36.43 apiece; the first average is one that would make Botham one of the greatest bowlers of the modern era, ranking alongside the West Indian greats Curtly Ambrose (career average 20.99), Malcolm Marshall (career average 20.94), and Joel Garner (career average 20.97), but the second average depicts a player who, as a specialist bowler, would be unable to sustain a place in many Test teams. This difference can be largely attributed to the longer term effects of a back injury he sustained in 1980; this limited his bowling pace and his ability to swing the ball.

Botham's batting – although never the equal of his bowling abilities – declined as well, with a batting average of 38.80 for his first 51 Tests substantially higher than the 28.87 he managed in his last 51 Tests, again a number that would be considered unsatisfactory for a specialist batsman in most Test sides. In the first 5 years of Botham's Test career, when not playing as captain, he scored 2,557 runs at an average of 49.17 including 11 centuries and a highest score of 208, took 196 wickets at an average of 21.28 including nineteen 5 wicket hauls and held 50 catches. Such figures denote a player who would easily maintain a place in any Test side as a specialist batsman or bowler alone. During this period his reputation as one of the leading Test all-rounders was firmly established.

Botham had an affinity with Brian Close, his first county captain who became a mentor to him, as they shared a determination to do well and win matches. "Wisden" has commented on another shared characteristic: "outstanding courage", mainly because Botham would readily field anywhere, generally in the slips but also in dangerous positions near the batsman and he was a brilliant fielder. As a batsman, Botham was often wrongly labelled by the tabloid press as a "big hitter" (effectively inferring that he was a "slogger") but, while it is true that his strength enabled him to drive a ball for six and his courage to hook one for six, Botham actually had a very correct batting style as he stood side-on and played straight: "Wisden" praised his "straight hitting and square cutting". Botham might not have been good enough to retain a regular England place as a specialist batsman (his Test career batting average was a fairly modest 33.54) but as a bowler who was capable of taking 383 Test wickets, he certainly would. "Wisden" praised Tom Cartwright for helping to develop Botham's technique as a swing bowler and, by the time he made his Test debut in 1977, Botham had mastered change of pace, the outswinger and the fast inswinging yorker, all formidable parts of his repertoire which eventually enabled him to break the world Test wicket record.

Writing in "Barclays World of Cricket" (1986), former England captain Tony Lewis commented upon Botham's strength, enthusiasm and aggression "which he took into every game". Lewis, however, pointed out that Botham's exuberance often reduced the efficiency of his play, in that he would take too many risks or refuse to give up on a bowling tactic despite ongoing heavy cost. He summarised Botham as an exciting cricketer who lacked self-discipline. Botham was in the middle of his career when the book was published but Lewis emphasised the speed at which Botham had achieved certain milestones such as 1,000 runs and 100 wickets in Test cricket. At that time, there seemed no reason why Botham should not go on reaching milestones but he had already peaked and, in retrospect, his career had a meteoric aspect. His rival Imran Khan asserted this when he said: "Botham was someone who I don't think ever did justice to his talent. When he started he could have done anything, but he declined very quickly. In a way our careers were the opposite of each other. I started quite slowly but got better, maximised my talent. He went the other way, I think".

Because of Botham's natural exuberance, allied to the more outstanding all-round performances of his early Test career and his tendency to court controversy, he became a so-called "celebrity" in the eyes of the tabloid press and, as Derek Birley put it: "for the young he was the great hero (at Headingley in 1981)". As a result, media hacks ignorantly proclaimed him to be the greatest cricketer England ever had, the world's greatest-ever all-rounder, etc. and this syndrome is still evident in 2017 when a contributor to "ESPNcricinfo" writes about Botham "revelling in his status as cricket's first superstar". So-called "superstardom" is a concept that in fact dates right back to the 18th century in cricket because, in his classic book, John Nyren described Hambledon's master batsman John Small as "a star of the first magnitude" (i.e., a "superstar"). A century after Small came W. G. Grace, something of a "superstar" himself, whom C. L. R. James in another classic book placed among the three most eminent Victorians. In the 20th century, the likes of Jack Hobbs, Wilfred Rhodes, Wally Hammond, Denis Compton, Fred Trueman and Gary Sobers were all "superstars" in their day; as was an Australian called Don Bradman.

The tabloid/juvenile hagiography of Botham has caused many knowledgeable commentators to assert that Botham was overrated, a problem for him that dates right back to his first Test series in 1977. When naming him as a Cricketer of the Year in its 1978 edition, "Wisden" described Botham as "a determined character who knows where he is aiming, and who will, quite naturally and fiercely, address himself to the interesting view that he is overrated". Denis Compton, another great English cricketer who was idolised by the media, dismissed Botham as "overrated" and said he "only did well because all the best players had joined Packer": i.e., for World Series Cricket (WSC). The point made by Compton about WSC concerns its impact on the Australian team. In 1981 and 1985, their squads to England in the aftermath of WSC were two of their weakest ever, as the Test series results confirm because, apart from Allan Border, they lacked top-class batsmen. Birley countered Compton's point by writing that "a player can hardly be blamed for the quality of the opposition he faces". As described above, Botham excelled in 1981 after he shed the millstone of captaincy and he was England's main strike bowler in 1985 but, in 1989, after the post-Packer tide had turned and Australia could put a world-class team into the field again, Botham was made to look ordinary.

Given all the arguments about whether Botham was the greatest or whether he was overrated, it must be stressed that there is no evidence anywhere of Botham himself claiming, Muhammad Ali-style (or even Fred Trueman-style), that he ever was "the greatest" or of him "revelling in superstar status". Rather, he would readily give praise to his colleagues and effectively assert that cricket is a team game in which he always did his best and played to win (just like Brian Close, his mentor): for example, the credit which Botham gave to his batting partners Hallam Moseley and Bob Clapp after the 1974 Benson and Hedges quarter-final against Hampshire; and to Bob Willis who, after all, was the actual match-winner at Headingley in 1981. Botham's innings there was an outstanding performance but, in match terms, it gave England a ray of hope which, without Willis's brilliant bowling on the final day, would have been extinguished for, as "Wisden" said: "This was Willis' hour". Botham has turned all the publicity to good use in one important respect as it made him famous outside cricket and that has generated increased public interest, and investment, in his charity fundraising efforts.

Botham cannot be dismissed as "overrated" or lauded as "the greatest", but he was unquestionably an outstanding player, especially in the first half of his career, as the table above demonstrates. As a Test batsman, he scored 14 centuries with a highest score of 208 against India in 1982. Thirteen of his centuries were scored before his thirtieth birthday in November 1985. It is a similar picture in terms of his Test bowling. He took ten wickets in a match four times and five wickets in an innings 27 times. Apart from a five for 71 in 1985–86 and a five for 41 in 1986–87, all those successes were achieved before his thirtieth birthday.

In 1994, the year after he retired, Botham became embroiled in a legal dispute with Imran Khan who, in an article for "India Today", had accused Botham and Allan Lamb of bringing cricket into disrepute. Botham and Lamb instigated a libel action in response. The case was heard at the High Court in 1996 with the court choosing to hear on the second day a separate action brought solely by Botham against Imran who had suggested in a newspaper article that Botham had been involved in ball-tampering. This would become the subject of a court case later on, one that Imran would go on to win. Botham was liable for all expenses in the court case in the ruling, including those incurred by Imran.

Botham was a talented footballer but, believing he was better at cricket, he chose the latter for his full-time career. Even so, he played football as a centre half from 1978 to 1985 for Yeovil Town and Scunthorpe United. He made eleven appearances in the Football League for Scunthorpe. Whilst with Yeovil, Botham made an appearance for the Football Association XI (a representative side for non-League footballers) against the Northern Football League at Croft Park during the 1984–85 season.

Botham has been a prodigious fundraiser for charitable causes, undertaking a total of 12 long-distance charity walks. His first, in 1985, was a 900-mile trek from John o' Groats to Land's End. His efforts were inspired after a visit to Taunton's Musgrove Park Hospital in 1977 whilst receiving treatment for a broken toe. When he took a wrong turn into a children's ward, he was devastated to learn that some of the children had only weeks to live, and why. At the time, he was an expectant father himself. Since then, his efforts have raised more than £12 million for charity, with leukaemia research the main cause to benefit. In recognition of this work, Botham in 2003 became the first-ever President of Bloodwise, the UK's leading blood cancer charity.

On 10 October 2007, he was invested a Knight Bachelor by Queen Elizabeth II at Buckingham Palace, having been appointed in the Queen's Birthday Honours "for services to Charity and to Cricket".

After retiring from cricket, Botham became involved in the media and has worked as an analyst and commentator for Sky Sports for many years. He has earned much respect as a broadcaster because of his deep knowledge and understanding of cricket; he imparts information and opinion objectively, giving praise where it is due and constructive criticism where that is due. Unlike Fred Trueman and others, he does not hark back to "in my day". "Wisden" editor Matthew Engel remarked on Botham's calmness, wit and sagacity as a TV commentator, though admitting he was surprised by it.

On 9 August 2009, while commentating on the fourth Ashes Test at Headingley that season, Botham was invited to take part in an on-field ceremony to induct him into the ICC Cricket Hall of Fame along with the Yorkshire greats Wilfred Rhodes, Fred Trueman and Geoffrey Boycott. Geoff Boycott was also in attendance, along with Fred Trueman's widow Veronica and Colin Graves who, as Yorkshire County Cricket Club chairman, accepted the honour on behalf of Wilfred Rhodes. Botham said: "To be named amongst 55 of the most prolific players in cricketing history is a great honour for me. To have my cricketing career recognised in the ICC Cricket Hall of Fame is not something I would have thought when I began playing cricket but to be receiving this award today is something I'm extremely grateful for". Colin Graves included Botham in his tribute to Rhodes when he said: "It is a great honour to accept the cap on behalf of a Yorkshire legend. Wilfred Rhodes was an exceedingly gifted player and is rightly regarded as one of England's greatest all-rounders. I am also delighted to see two other great Yorkshiremen and another great all-rounder inducted into the ICC Cricket Hall of Fame today".

He was the subject of "This Is Your Life" in 1981 when he was surprised by Eamonn Andrews.

In 1976, in Doncaster, Botham married Kathryn ("Kathy") Waller (now Lady Botham) whom he first met in June 1974. After their marriage, they lived until the late 1980s in Epworth, near Scunthorpe. They have one son, Liam (born August 1977), and two daughters. Liam is a former professional cricketer and rugby player. The family currently live in Almeria, owning two houses, and Botham frequently plays golf there. His daughter Sarah owns a restaurant and wine bar in the town.

Botham is an avid trout and salmon angler. As a result, he was invited to present a TV series called "Botham on the Fly". He has also been a team captain on the BBC series "A Question of Sport".

Besides angling and golf, Botham enjoys game shooting and owns a grouse moor. This has resulted in a high-profile dispute with the Royal Society for the Protection of Birds (RSPB). In August 2016, he called for Chris Packham to be sacked by the BBC as part of a campaign funded by the grouse shooting industry, after Packham had highlighted the industry's involvement in the illegal killing of endangered species of birds of prey.

Botham is a staunch supporter of Britain’s withdrawal from the European Union. He appeared at a number of pro-leave campaign events in the run-up the United Kingdom European Union membership referendum, 2016.





</doc>
<doc id="15526" url="https://en.wikipedia.org/wiki?curid=15526" title="Id Software">
Id Software

id Software LLC (; see Company name) is an American video game developer headquartered in Dallas, Texas. The company was founded on February 1, 1991, by four members of the computer company Softdisk, programmers John Carmack and John Romero, game designer Tom Hall, and artist Adrian Carmack (no relation to John Carmack). Business manager Jay Wilbur was also involved.

id Software made important technological developments in video game technologies for the PC (running MS-DOS and Windows), including work done for the "Wolfenstein", "Doom", and "Quake" franchises. id's work was particularly important in 3D computer graphics technology and in game engines that are heavily used throughout the video game industry.

The company was also heavily involved in the creation of the first-person shooter genre. "Wolfenstein 3D" is often considered as the first true FPS, "Doom" was a game that popularized the genre and PC gaming in general, and "Quake" was id's first true 3D first-person shooter.

On June 24, 2009, ZeniMax Media acquired the company. In 2015, they opened a second studio in Frankfurt, Germany.

The founders of id Software met in the offices of Softdisk developing multiple games for Softdisk's monthly publishing, including "Dangerous Dave". In September 1990, John Carmack developed an efficient way to rapidly side-scroll graphics on the PC. Upon making this breakthrough, Carmack and Tom Hall stayed up late into the night making a replica of the first level of the popular 1988 NES game "Super Mario Bros. 3", inserting stock graphics of John Romero's Dangerous Dave character in lieu of Mario. When Romero saw the demo, entitled "Dangerous Dave in Copyright Infringement", he realized that Carmack's breakthrough could have potential. The team that would later form id Software immediately began moonlighting, going so far as to "borrow" company computers that were not being used over the weekends and at nights while they designed their own remake of "Super Mario Bros. 3".

Despite their work, Nintendo turned them down, saying they had no interest in expanding to the PC market, and that Mario games were to remain exclusive to Nintendo consoles. Around this time, Scott Miller of Apogee Software learned of the group and their exceptional talent, having played one of Romero's Softdisk games, "Dangerous Dave", and contacted Romero under the guise of multiple fan letters that Romero came to realize all originated from the same address. When he confronted Miller, Miller explained that the deception was necessary since Softdisk screened letters it received. Although disappointed by not actually having received mail from multiple fans, Romero and other Softdisk developers began proposing ideas to Miller, including "Commander Keen" in December 1990, which became a very successful shareware game. After their first royalty check Romero, Carmack, and Adrian Carmack (no relation) decided to start their own company. After hiring Hall, the group finished the "Commander Keen" series, then hired Jay Wilbur and Kevin Cloud and began working on "Wolfenstein 3D".

The shareware distribution method was initially employed by id Software through Apogee Software to sell their products, such as the "Commander Keen", "Wolfenstein" and "Doom" games. They would release the first part of their trilogy as shareware, then sell the other two installments by mail order. Only later (about the time of the release of "Doom II") did id Software release their games via more traditional shrink-wrapped boxes in stores (through other game publishers).

After "Wolfenstein 3D"s great success, id began working on "Doom". After Hall left the company it hired Sandy Petersen and Dave Taylor before the release of "Doom" in December 1994.

On June 24, 2009, it was announced that id Software had been acquired by ZeniMax Media (owner of Bethesda Softworks). The deal would eventually affect publishing deals id Software had before the acquisition, namely "Rage", which was being published through Electronic Arts. id Software moved from the "cube-shaped" Mesquite office to a newly built location in Richardson, Texas in January 2011.

On June 26, 2013, id Software president Todd Hollenshead quit after 17 years of service.

On November 22, 2013, it was announced id Software co-founder and Technical Director John Carmack had fully resigned from the company to work full-time at Oculus VR which he joined as CTO in August 2013. He was the last of the original founders to leave the company.

The company writes its name with a lowercase "id", which is pronounced as in "did" or "kid", and, according to the book "Masters of Doom", the group identified itself as "Ideas from the Deep" in the early days of Softdisk but that, in the end, the name 'id' came from the phrase "in demand". Disliking "in demand" as "lame", someone suggested a connection with Sigmund Freud's psychological concept of id, which the others accepted. Evidence of the reference can be found as early as "Wolfenstein 3D" with the statement "that's id, as in the id, ego, and superego in the psyche" appearing in the game's documentation. Prior to an update to the website, id's History page made a direct reference to Freud.


Arranged in chronological order:

Starting with their first shareware game series, "Commander Keen", id Software has licensed the core source code for the game, or what is more commonly known as the engine. Brainstormed by John Romero, id Software held a weekend session titled "The id Summer Seminar" in the summer of 1991 with prospective buyers including Scott Miller, George Broussard, Ken Rogoway, Jim Norwood and Todd Replogle. One of the nights, id Software put together an impromptu game known as "Wac-Man" to demonstrate not only the technical prowess of the "Keen" engine, but also how it worked internally.

id Software has developed their own game engine for each of their titles when moving to the next technological milestone, including "Commander Keen", "Wolfenstein 3D", "ShadowCaster", "Doom", "Quake", "Quake II", and "Quake III", as well as the technology used in making "Doom 3". After being used first for id Software's in-house game, the engines are licensed out to other developers. According to "Eurogamer.net", "id Software has been synonymous with PC game engines since the concept of a detached game engine was first popularized". During the mid to late 1990s, "the launch of each successive round of technology it's been expected to occupy a headlining position", with the "Quake III" engine being most widely adopted of their engines. However id Tech 4 had far fewer licensees than the Unreal Engine from Epic Games, due to the long development time that went into "Doom 3" which id Software had to release before licensing out that engine to others.

In conjunction with his self-professed affinity for sharing source code, John Carmack has open-sourced most of the major id Software engines under the GNU General Public License. Historically, the source code for each engine has been released once the code base is 5 years old. Consequently, many home grown projects have sprung up porting the code to different platforms, cleaning up the source code, or providing major modifications to the core engine. "Wolfenstein 3D", "DOOM" and "Quake" engine ports are ubiquitous to nearly all platforms capable of running games, such as hand-held PCs, iPods, the PSP, the Nintendo DS and more. Impressive core modifications include DarkPlaces which adds stencil shadow volumes into the original "Quake" engine along with a more efficient network protocol. Another such project is ioquake3, which maintains a goal of cleaning up the source code, adding features and fixing bugs. Even earlier id Software code, namely for "Hovertank 3D" and "Catacomb 3D", was released in June 2014 by Flat Rock Software.

The GPL release of the "Quake III" engine's source code was moved from the end of 2004 to August 2005 as the engine was still being licensed to commercial customers who would otherwise be concerned over the sudden loss in value of their recent investment.

On August 4, 2011, John Carmack revealed during his QuakeCon 2011 keynote that they will be releasing the source code of the "Doom 3" engine (id Tech 4) during the year.

id Software publicly stated they would not support the Wii console (possibly due to technical limitations), although they have since indicated that they may release titles on that platform (although it would be limited to their games released during the 1990s).

Since id Software revealed their engine id Tech 5, they call their engines "id Tech", followed by a version number. Older engines have retroactively been renamed to fit this scheme, with the "Doom" engine as id Tech 1.

id Software was an early pioneer in the Linux gaming market, and id Software's Linux games have been some of the most popular of the platform. Many id Software games won the Readers' and Editors' Choice awards of Linux Journal. Some id Software titles ported to Linux are "Doom" (the first id Software game to be ported), "Quake", "Quake II", "Quake III Arena", "Return to Castle Wolfenstein", "", "Doom 3", "Quake 4", and "". Since id Software and some of its licensees released the source code for some of their previous games, several games which were not ported (such as "Wolfenstein 3D", "Spear of Destiny", "Heretic", "", "Hexen II", and "Strife") can run on Linux and other operating systems natively through the use of source ports. "Quake Live" also launched with Linux support, although this, alongside OS X support, was later removed when changed to a standalone title.

The tradition of porting to Linux was first started by Dave D. Taylor, with David Kirsch doing some later porting. Since "Quake III Arena", Linux porting had been handled by Timothee Besset. The majority of all id Tech 4 games, including those made by other developers, have a Linux client available, the only current exceptions being "Wolfenstein" and "Brink". Similarly, almost all of the games utilizing the Quake II engine have Linux ports, the only exceptions being those created by Ion Storm. Despite fears by the Linux gaming community that id Tech 5 would not be ported to that platform, Timothee Besset in his blog has stated "I'll be damned if we don't find the time to get Linux builds done". Besset has stated that id Software's primary justification for releasing Linux builds is better code quality, along with a technical interest for the platform. However, on January 26, 2012, Besset announced that he had left id.

John Carmack has expressed his stance with regard to Linux builds in the past. In December 2000 Todd Hollenshead expressed support for Linux: "All said, we will continue to be a leading supporter of the Linux platform because we believe it is a technically sound OS and is the OS of choice for many server ops." However, on April 25, 2012, Carmack revealed that "there are no plans for a native Linux client" of id's most recent game, "Rage". In February 2013, Carmack argued for improving emulation as the "proper technical direction for gaming on Linux", though this was also due to ZeniMax's refusal to support "unofficial binaries", given all prior ports (except for "Quake III Arena", via Loki Software, and earlier versions of "Quake Live") having only ever been unofficial. Carmack didn't mention official games "Quake: The Offering" and "Quake II: Colossus" ported by id Software to Linux and published by Macmillan Computer Publishing USA.
The "Commander Keen" series, a platform game introducing one of the first smooth side-scrolling game engines for MS-DOS, brought id Software into the gaming mainstream. The game was very successful and spawned a whole series of titles. It was also the series of id Software that designer Tom Hall was most affiliated with. The first "Commander Keen" trilogy was released on December 14, 1990.

The company's breakout product was released on May 5, 1992: "Wolfenstein 3D", a first-person shooter (FPS) with smooth 3D graphics that were unprecedented in computer games, and with violent gameplay that many gamers found engaging. After essentially founding an entire genre with this game, id Software created "Doom", "", "Quake", "Quake II", "Quake III Arena", "Quake 4", and "Doom 3". Each of these first-person shooters featured progressively higher levels of graphical technology. "Wolfenstein 3D" spawned a prequel and a sequel: the prequel called "Spear of Destiny", and the second, "Return to Castle Wolfenstein", using the id Tech 3 engine. A third "Wolfenstein" sequel, simply titled "Wolfenstein", was released by Raven Software, using the id Tech 4 engine. Another sequel, named ""; was developed using the id Tech 5 engine and released by MachineGames in 2014, with it getting a prequel by the name of "" a year later; followed by a direct sequel titled "" in 2017.

Eighteen months after their release of "Wolfenstein 3D", on December 10, 1993, id Software released "Doom" which would again set new standards for graphic quality and graphic violence in computer gaming. "Doom" featured a sci-fi/horror setting with graphic quality that had never been seen on personal computers or even video game consoles. "Doom" became a cultural phenomenon and its violent theme would eventually launch a new wave of criticism decrying the dangers of violence in video games. "Doom" was ported to numerous platforms, inspired many knock-offs, and was eventually followed by the technically similar "". id Software made its mark in video game history with the shareware release of "Doom", and eventually revisited the theme of this game in 2004 with their release of "Doom 3". John Carmack said in an interview at QuakeCon 2007 that there will be a "Doom 4". It began development on May 7, 2008. "Doom", the fourth installation and a reboot of the Doom series, was released on Microsoft Windows, PlayStation 4, and Xbox One on May 13, 2016, and was later released on Nintendo Switch on November 10, 2017.

On June 22, 1996, the release of "Quake" marked the second milestone in id Software history. "Quake" combined a cutting edge fully 3D engine, the "Quake" engine, with a distinctive art style to create critically acclaimed graphics for its time. Audio was not neglected either, having recruited Nine Inch Nails frontman Trent Reznor to facilitate unique sound effects and ambient music for the game. (A small homage was paid to Nine Inch Nails in the form of the band's logo appearing on the ammunition boxes for the nailgun weapon.) It also included the work of Michael Abrash. Furthermore, "Quake"'s main innovation, the capability to play a deathmatch (competitive gameplay between living opponents instead of against computer-controlled characters) over the Internet (especially through the add-on "QuakeWorld"), seared the title into the minds of gamers as another smash hit.

In 2008, id Software was honored at the 59th Annual Technology & Engineering Emmy Awards for the pioneering work "Quake" represented in user modifiable games. id Software is the only game development company ever honored twice by the National Academy of Television Arts & Sciences, having been given an Emmy Award in 2007 for creation of the 3D technology that underlies modern shooter video games.

The "Quake" series continued with "Quake II" in 1997. Activision purchased a 49% stake in id Software, making it a second party which took publishing duties until 2009. However, the game is not a storyline sequel, and instead focuses on an assault on an alien planet, Stroggos, in retaliation for Strogg attacks on Earth. Most of the subsequent entries in the "Quake" franchise follow this storyline. "Quake III Arena" (1999), the next title in the series, has minimal plot, but centers around the "Arena Eternal", a gladiatorial setting created by an alien race known as the Vadrigar and populated by combatants plucked from various points in time and space. Among these combatants are some characters either drawn from or based on those in "Doom" ("Doomguy"), "Quake" (Ranger, Wrack), and "Quake II" (Bitterman, Tank Jr., Grunt, Stripe). "Quake IV" (2005) picks up where "Quake II" left off – finishing the war between the humans and Strogg. The spin-off "" acts as a prequel to "Quake II", when the Strogg first invade Earth. It should be noted that "Quake IV" and "Enemy Territory: Quake Wars" were made by outside developers and not id.

There have also been a few other spin off games such as Quake Mobile in 2005 and "Quake Live", an internet browser based modification of "Quake III". A game called "Quake Arena DS" was planned and canceled for the Nintendo DS. John Carmack stated, at QuakeCon 2007, that the "id Tech 5" engine would be used for a new "Quake" game.

Todd Hollenshead announced in May 2007 that id Software had begun working on an all new series that would be using a new engine. Hollenshead also mentioned that the title would be completely developed in-house, marking the first game since 2004's "Doom 3" to be done so. At 2007's WWDC, John Carmack showed the new engine called id Tech 5. Later that year, at QuakeCon 2007, the title of the new game was revealed as "Rage".

On July 14, 2008, id Software announced at the 2008 E3 event that they would be publishing "Rage" through Electronic Arts, and not id's longtime publisher Activision. However, since then ZeniMax has also announced that they are publishing "Rage" through Bethesda Softworks.

On August 12, 2010, during Quakecon 2010, id Software announced "Rage" US ship date of September 13, 2011, and a European ship date of September 15, 2011. During the keynote, id Software also demonstrated a "Rage" spin-off title running on the iPhone. This technology demo later became "Rage HD".

On May 14, 2018, Bethesda Softworks announced "Rage 2", a co-development between id Software and Avalanche Studios.

During its early days, id Software produced much more varied games; these include the early 3D first-person shooter experiments that led to "Wolfenstein 3D" and "Doom" – "Hovertank 3D" and "Catacomb 3D". There was also the "Rescue Rover" series, which had two games – "Rescue Rover" and "Rescue Rover 2". Also there was John Romero's "Dangerous Dave" series, which included such notables as the tech demo ("In Copyright Infringement") which led to the "Commander Keen" engine, and the decently popular "Dangerous Dave in the Haunted Mansion". "In the Haunted Mansion" was powered by the same engine as the earlier id Software game "Shadow Knights", which was one of the several games written by id Software to fulfill their contractual obligation to produce games for Softdisk, where the id Software founders had been employed. id Software has also overseen several games using its technology that were not made in one of their IPs such as "ShadowCaster", (early-id Tech 1), "Heretic", "" (id Tech 1), "Hexen II" ("Quake" engine), and "Orcs and Elves" ("Doom RPG" engine).

id Software has also been associated with novels since the publication of the original "Doom" novels. This has been restarted from 2008 onward with Matthew J. Costello's (a story consultant for "Doom 3" and now "Rage") new "Doom 3" novels: "" and "".

id Software became involved in film development when they were in the production team of the film adaption of their "Doom" franchise in 2005. In August 2007, Todd Hollenshead stated at QuakeCon 2007 that a "Return to Castle Wolfenstein" movie is in development which re-teams the "Silent Hill" writer/producer team, Roger Avary as writer and director and Samuel Hadida as producer.

id Software was the target of controversy over two of their most popular games, "Doom" and the earlier "Wolfenstein 3D":

"Doom" was notorious for its high levels of gore and occultism along with satanic imagery, which generated controversy from a broad range of groups. Yahoo! Games listed it as one of the top ten most controversial games of all time.

The game again sparked controversy throughout a period of school shootings in the United States when it was found that Eric Harris and Dylan Klebold, who committed the Columbine High School massacre in 1999, were avid players of the game. While planning for the massacre, Harris said that the killing would be "like playing "Doom"", and "it'll be like the LA riots, the Oklahoma bombing, World War II, Vietnam, "Duke Nukem" and "Doom" all mixed together", and that his shotgun was "straight out of the game". A rumor spread afterwards that Harris had designed a "Doom" level that looked like the high school, populated with representations of Harris's classmates and teachers, and that Harris practiced for his role in the shootings by playing the level over and over. Although Harris did design "Doom" levels, none of them were based on Columbine High School.

While "Doom" and other violent video games have been blamed for nationally covered school shootings, 2008 research featured by Greater Good Science Center shows that the two are not closely related. Harvard medical school researchers Cheryl Olson and Lawrence Kutner found that violent video games did not correlate to school shootings. The U.S. Secret Service and Department of Education analyzed 37 incidents of school violence and sought to develop a profile of school shooters; they discovered that the most common traits among shooters were that they were male and had histories of depression and attempted suicide. While many of the killers—like the vast majority of young teenage boys—did play video games, this study did not find a relationship between game play and school shootings. In fact, only one eighth of the shooters showed any special interest in violent video games, far less than the number of shooters who seemed attracted to books and movies with violent content.

As for "Wolfenstein 3D", due to its use of Nazi symbols such as the swastika and the anthem of the Nazi Party, "Horst-Wessel-Lied", as theme music, the PC version of the game was withdrawn from circulation in Germany in 1994, following a verdict by the Amtsgericht München on January 25, 1994. Despite the fact that Nazis are portrayed as the enemy in "Wolfenstein", the use of those symbols is a federal offense in Germany unless certain circumstances apply. Similarly, the Atari Jaguar version was confiscated following a verdict by the Amtsgericht Berlin Tiergarten on December 7, 1994.

Due to concerns from Nintendo of America, the Super NES version was modified to not include any swastikas or Nazi references; furthermore, blood was replaced with sweat to make the game seem less violent, and the attack dogs in the game were replaced by giant mutant rats. Employees of id Software are quoted in "The Official DOOM Player Guide" about the reaction to "Wolfenstein", claiming it to be ironic that it was morally acceptable to shoot people and rats, but not dogs. Two new weapons were added as well. The Super NES version was not as successful as the PC version.

In 2003, the book "Masters of Doom" chronicled the development of id Software, concentrating on the personalities and interaction of John Carmack and John Romero. Below are the key people involved with id's success.

Carmack's skill at 3D programming is widely recognized in the software industry and from its inception, he was id's lead programmer. On August 7, 2013, he joined Oculus VR, a company developing virtual reality headsets, and left id Software on November 22, 2013.

John Romero, who was forced to resign after the release of "Quake", later formed the ill-fated company Ion Storm. There, he became infamous through the development of "Daikatana", which was received negatively from reviewers and gamers alike upon release.

Both Tom Hall and John Romero have reputations as designers and idea men who have helped shape some of the key PC gaming titles of the 1990s.

Tom Hall was forced to resign by id Software during the early days of "Doom" development, but not before he had some impact; for example, he was responsible for the inclusion of teleporters in the game. He was let go before the shareware release of "Doom" and then went to work for Apogee, developing "Rise of the Triad" with the "Developers of Incredible Power". When he finished work on that game, he found he was not compatible with the "Prey" development team at Apogee, and therefore left to join his ex-id Software compatriot John Romero at Ion Storm. Hall has frequently commented that if he could obtain the rights to "Commander Keen", he would immediately develop another Keen title.

Sandy Petersen was a level designer for 19 of the 27 levels in the original "Doom" title as well as 17 of the 32 levels of "Doom II". As a fan of H.P. Lovecraft, his influence is apparent in the Lovecraftian feel of the monsters for "Quake", and he created "Inferno", the third "episode" of the first DOOM. He was forced to resign from id Software during the production of "Quake II" and most of his work was scrapped before the title was released.

American McGee was a level designer for "Doom II", "The Ultimate Doom", "Quake", and "Quake II". He was asked to resign after the release of "Quake II", then moved to Electronic Arts where he gained industry notoriety with the development of his own game "American McGee's Alice". After leaving Electronic Arts, he became an independent entrepreneur and game developer. McGee now heads independent game development house Spicy Horse in Shanghai, where he works on various projects.



</doc>
<doc id="15531" url="https://en.wikipedia.org/wiki?curid=15531" title="Isaac Stern">
Isaac Stern

Isaac Stern (; "Isaak Solomonovich Shtern"; 21 July 1920 – 22 September 2001) was an American violinist.

The son of Solomon and Clara Stern, Isaac Stern was born in Kremenets, Ukraine (then part of the Russian Empire) into a Volhynian-Jewish family. He was 14 months old when his family moved to San Francisco in 1921. He received his first music lessons from his mother. In 1928, he enrolled at the San Francisco Conservatory of Music, where he studied until 1931 before going on to study privately with Louis Persinger. He returned to the San Francisco Conservatory to study for five years with Naoum Blinder, to whom he said he owed the most. At his public début on 18 February 1936, aged 15, he played Saint-Saëns' Violin Concerto No. 3 in B minor with the San Francisco Symphony under the direction of Pierre Monteux. Reflecting on his background, Stern once memorably quipped that cultural exchanges between the U.S. and Soviet Russia were simple affairs:

Stern toured the Soviet Union in 1951, the first American violinist to do so. In 1967, Stern stated his refusal to return to the USSR until the Soviet regime allowed artists to enter and leave the country freely. His only visit to Germany was in 1999, for a series of master classes, but he never performed publicly in Germany.

Stern was married three times. His first marriage, in 1948 to ballerina Nora Kaye, ended in divorce after 18 months, but the two of them subsequently remained friends. On 17 August 1951, he married Vera Lindenblit. They had three children together, including conductors Michael and David Stern. Their marriage ended in divorce in 1994 after 43 years. In 1996, Stern married his third wife, Linda Reynolds. His third wife, his three children, and his five grandchildren survived him.

Stern died 22 September 2001 of heart failure in a Manhattan, New York hospital after an extended stay.

In 1940, Stern began performing with Russian-born pianist Alexander Zakin, collaborating until 1977. Within musical circles, Stern became renowned both for his recordings and for championing certain younger players. Among his discoveries were cellists Yo-Yo Ma and Jian Wang, and violinists Itzhak Perlman and Pinchas Zukerman.

In the 1960s, he played a major role in saving New York City's Carnegie Hall from demolition, by organising the Citizens' Committee to Save Carnegie Hall. Following the purchase of Carnegie Hall by New York City, the Carnegie Hall Corporation was formed, and Stern was chosen as its first president, a title he held until his death. Carnegie Hall later named its main auditorium in his honor.

Among Stern's many recordings are concertos by Brahms, Bach, Beethoven, Mendelssohn, Sibelius, Tchaikovsky, and Vivaldi and modern works by Barber, Bartók, Stravinsky, Bernstein, Rochberg, and Dutilleux. The Dutilleux concerto, entitled "L'arbre des songes" ["The Tree of Dreams"] was a 1985 commission by Stern himself. He also dubbed actors' violin-playing in several films, such as "Fiddler on the Roof".

Stern served as musical advisor for the 1946 film, "Humoresque", about a rising violin star and his patron, played respectively by John Garfield and Joan Crawford. He was also the featured violin soloist on the soundtrack for the 1971 film of Fiddler on the Roof. In 1999, he appeared in the film "Music of the Heart", along with Itzhak Perlman and several other famed violinists, with a youth orchestra led by Meryl Streep (the film was based on the true story of a gifted violin teacher in Harlem who eventually took her musicians to play a concert in Carnegie Hall).
In his autobiography, co-authored with Chaim Potok, "My First 79 Years", Stern cited Nathan Milstein and Arthur Grumiaux as major influences on his style of playing.

He won Grammys for his work with Eugene Istomin and Leonard Rose in their famous chamber music trio in the 1960s and '70s, while also continuing his duo work with Alexander Zakin during this time. Stern recorded a series of piano quartets in the 1980s and 1990s with Emanuel Ax, Jaime Laredo and Yo-Yo Ma, including those of Mozart, Beethoven, Schumann and Fauré, winning another Grammy in 1992 for the Brahms quartets Opp. 25 and 26.

In 1979, seven years after Richard Nixon made the first official visit by a US President to the country, the People's Republic of China offered Stern and pianist David Golub an unprecedented invitation to tour the country. While there, he collaborated with the China Central Symphony Society (now China National Symphony) under the direction of conductor Li Delun. Their visit was filmed and resulted in the Oscar-winning documentary, "".

Stern maintained close ties with Israel. Stern began performing in the country in 1949. In 1973, he performed for wounded Israeli soldiers during the Yom Kippur War. During the 1991 Gulf War and Iraq's Scud missile attacks on Israel, he played in the Jerusalem Theater. During his performance, an air raid siren sounded, causing the audience to panic. Stern then stepped onto the stage and began playing a movement of Bach. The audience then calmed down, donned gas masks, and sat throughout the rest of his performance. Stern was a supporter of several educational projects in Israel, among them the America-Israel Foundation and the Jerusalem Music Center.

Stern's favorite instrument was the Ysaÿe Guarnerius, one of the violins produced by the Cremonese luthier Giuseppe Guarneri del Gesù. It had previously been played by the violin virtuoso and composer Eugène Ysaÿe.

Among other instruments, Stern played the "Kruse-Vormbaum" Stradivarius (1728), the "ex-Stern" Bergonzi (1733), the "Panette" Guarneri del Gesù (1737), a Michele Angelo Bergonzi (1739–1757), the "Arma Senkrah" Guadagnini (1750), a Giovanni Guadagnini (1754), a J. B. Vuillaume copy of the "Panette" Guarneri del Gesu of 1737 (c.1850), and the "ex-Nicolas I" J.B. Vuillaume (1840). He also owned two contemporary instruments by Samuel Zygmuntowicz.

In 2001, Stern's collection of instruments, bows and musical ephemera was sold through Tarisio Auctions. The May 2003 auction set a number of world records and was at the time the second highest grossing violin auction of all time, with total sales of over $3.3M.


In 2012, a street in Tel Aviv was named for Stern.











</doc>
<doc id="15532" url="https://en.wikipedia.org/wiki?curid=15532" title="Integral">
Integral

In mathematics, an integral assigns numbers to functions in a way that can describe displacement, area, volume, and other concepts that arise by combining infinitesimal data. Integration is one of the two main operations of calculus, with its inverse operation, differentiation, being the other. Given a function of a real variable and an interval of the real line, the definite integral

is defined informally as the signed area of the region in the -plane that is bounded by the graph of , the -axis and the vertical lines and . The area above the -axis adds to the total and that below the -axis subtracts from the total.

The operation of integration, up to an additive constant, is the inverse of the operation of differentiation. For this reason, the term "integral" may also refer to the related notion of the antiderivative, a function whose derivative is the given function . In this case, it is called an "indefinite integral" and is written:

The integrals discussed in this article are those termed "definite integrals". It is the fundamental theorem of calculus that connects differentiation with the definite integral: if is a continuous real-valued function defined on a closed interval , then, once an antiderivative of is known, the definite integral of over that interval is given by

The principles of integration were formulated independently by Isaac Newton and Gottfried Wilhelm Leibniz in the late 17th century, who thought of the integral as an infinite sum of rectangles of infinitesimal width. Bernhard Riemann gave a rigorous mathematical definition of integrals. It is based on a limiting procedure that approximates the area of a curvilinear region by breaking the region into thin vertical slabs. Beginning in the nineteenth century, more sophisticated notions of integrals began to appear, where the type of the function as well as the domain over which the integration is performed has been generalised. A line integral is defined for functions of two or more variables, and the interval of integration is replaced by a curve connecting the two endpoints. In a surface integral, the curve is replaced by a piece of a surface in three-dimensional space.

The first documented systematic technique capable of determining integrals is the method of exhaustion of the ancient Greek astronomer Eudoxus ("ca." 370 BC), which sought to find areas and volumes by breaking them up into an infinite number of divisions for which the area or volume was known. This method was further developed and employed by Archimedes in the 3rd century BC and used to calculate areas for parabolas and an approximation to the area of a circle.

A similar method was independently developed in China around the 3rd century AD by Liu Hui, who used it to find the area of the circle. This method was later used in the 5th century by Chinese father-and-son mathematicians Zu Chongzhi and Zu Geng to find the volume of a sphere (; ).

The next significant advances in integral calculus did not begin to appear until the 17th century. At this time, the work of Cavalieri with his method of Indivisibles, and work by Fermat, began to lay the foundations of modern calculus, with Cavalieri computing the integrals of up to degree in Cavalieri's quadrature formula. Further steps were made in the early 17th century by Barrow and Torricelli, who provided the first hints of a connection between integration and differentiation. Barrow provided the first proof of the fundamental theorem of calculus. Wallis generalized Cavalieri's method, computing integrals of to a general power, including negative powers and fractional powers.

The major advance in integration came in the 17th century with the independent discovery of the fundamental theorem of calculus by Newton and Leibniz. The theorem demonstrates a connection between integration and differentiation. This connection, combined with the comparative ease of differentiation, can be exploited to calculate integrals. In particular, the fundamental theorem of calculus allows one to solve a much broader class of problems. Equal in importance is the comprehensive mathematical framework that both Newton and Leibniz developed. Given the name infinitesimal calculus, it allowed for precise analysis of functions within continuous domains. This framework eventually became modern calculus, whose notation for integrals is drawn directly from the work of Leibniz.
While Newton and Leibniz provided a systematic approach to integration, their work lacked a degree of rigour. Bishop Berkeley memorably attacked the vanishing increments used by Newton, calling them "ghosts of departed quantities". Calculus acquired a firmer footing with the development of limits. Integration was first rigorously formalized, using limits, by Riemann. Although all bounded piecewise continuous functions are Riemann-integrable on a bounded interval, subsequently more general functions were considered—particularly in the context of Fourier analysis—to which Riemann's definition does not apply, and Lebesgue formulated a different definition of integral, founded in measure theory (a subfield of real analysis). Other definitions of integral, extending Riemann's and Lebesgue's approaches, were proposed. These approaches based on the real number system are the ones most common today, but alternative approaches exist, such as a definition of integral as the standard part of an infinite Riemann sum, based on the hyperreal number system.

Isaac Newton used a small vertical bar above a variable to indicate integration, or placed the variable inside a box. The vertical bar was easily confused with or , which are used to indicate differentiation, and the box notation was difficult for printers to reproduce, so these notations were not widely adopted.

The modern notation for the indefinite integral was introduced by Gottfried Wilhelm Leibniz in 1675 (; ). He adapted the integral symbol, ∫, from the letter "ſ" (long s), standing for "summa" (written as "ſumma"; Latin for "sum" or "total"). The modern notation for the definite integral, with limits above and below the integral sign, was first used by Joseph Fourier in "Mémoires" of the French Academy around 1819–20, reprinted in his book of 1822 (; ).

Integrals are used extensively in many areas of mathematics as well as in many other areas that rely on mathematics.

For example, in probability theory, integrals are used to determine the probability of some random variable falling within a certain range. Moreover, the integral under an entire probability density function must equal 1, which provides a test of whether a function with no negative values could be a density function or not.

Integrals can be used for computing the area of a two-dimensional region that has a curved boundary, as well as computing the volume of a three-dimensional object that has a curved boundary.

Integrals are also used in physics, in areas like kinematics to find quantities like displacement, time, and velocity. For example, in rectilinear motion, the displacement of an object over the time interval formula_4 is given by: 

where formula_6 is the velocity expressed as a function of time. The work done by a force formula_7 (given as a function of position) from an initial position formula_8 to a final position formula_9 is:

Integrals are also used in thermodynamics, where thermodynamic integration is used to calculate the difference in free energy between two given states.

The integral with respect to of a real-valued function of a real variable on the interval is written as

The integral sign represents integration. The symbol , called the differential of the variable , indicates that the variable of integration is . The function to be integrated is called the integrand. The symbol is separated from the integrand by a space (as shown). If a function has an integral, it is said to be integrable. The points and are called the limits of the integral. An integral where the limits are specified is called a definite integral. The integral is said to be over the interval .

If the integral goes from a finite value "a" to the upper limit infinity, the integral expresses the limit of the integral from "a" to a value "b" as "b" goes to infinity. If the value of the integral gets closer and closer to a finite value, the integral is said to converge to that value. If not, the integral is said to diverge.

When the limits are omitted, as in
the integral is called an indefinite integral, which represents a class of functions (the antiderivative) whose derivative is the integrand. The fundamental theorem of calculus relates the evaluation of definite integrals to indefinite integrals. Occasionally, limits of integration are omitted for definite integrals when the same limits occur repeatedly in a particular context. Usually, the author will make this convention clear at the beginning of the relevant text.

There are several extensions of the notation for integrals to encompass integration on unbounded domains and/or in multiple dimensions (see later sections of this article).

Historically, the symbol "dx" was taken to represent an infinitesimally "small piece" of the independent variable "x" to be multiplied by the integrand and summed up in an infinite sense. While this notion is still heuristically useful, later mathematicians have deemed infinitesimal quantities to be untenable from the standpoint of the real number system. In introductory calculus, the expression "dx" is therefore not assigned an independent meaning; instead, it is viewed as part of the symbol for integration and serves as its delimiter on the right side of the expression being integrated. 

In more sophisticated contexts, "dx" can have its own significance, the meaning of which depending on the particular area of mathematics being discussed. When used in one of these ways, the original Leibnitz notation is co-opted to apply to a generalization of the original definition of the integral. Some common interpretations of "dx" include: an integrator function in Riemann-Stieltjes integration (indicated by "dα"("x") in general), a measure in Lebesgue theory (indicated by "dμ" in general), or a differential form in exterior calculus (indicated by formula_13 in general). In the last case, even the letter "d" has an independent meaning — as the exterior derivative operator on differential forms.

Conversely, in advanced settings, it is not uncommon to leave out "dx" when only the simple Riemann integral is being used, or the exact type of integral is immaterial. For instance, one might write formula_14 to express the linearity of the integral, a property shared by the Riemann integral and all generalizations thereof.

In modern Arabic mathematical notation, a reflected integral symbol is used instead of the symbol , since the Arabic script and mathematical expressions go right to left. Some authors, particularly of European origin, use an upright "d" to indicate the variable of integration (i.e., instead of ), since properly speaking, "d" is not a variable. Also, the symbol is not always placed after , as for instance in
In the first expression, the differential is treated as an infinitesimal "multiplicative" factor, formally following a "commutative property" when "multiplied" by the expression 3/("x"+1). In the second expression, showing the differentials first highlights and clarifies the variables that are being integrated with respect to, a practice particularly popular with physicists.

Integrals appear in many practical situations. If a swimming pool is rectangular with a flat bottom, then from its length, width, and depth we can easily determine the volume of water it can contain (to fill it), the area of its surface (to cover it), and the length of its edge (to rope it). But if it is oval with a rounded bottom, all of these quantities call for integrals. Practical approximations may suffice for such trivial examples, but precision engineering (of any discipline) requires exact and rigorous values for these elements.
To start off, consider the curve between and with (see figure). We ask:
and call this (yet unknown) area the (definite) integral of . The notation for this integral will be

As a first approximation, look at the unit square given by the sides to and and . Its area is exactly 1. Actually, the true value of the integral must be somewhat less than 1. Decreasing the width of the approximation rectangles and increasing the number of rectangles gives a better result; so cross the interval in five steps, using the approximation points 0, 1/5, 2/5, and so on to 1. Fit a box for each step using the right end height of each curve piece, thus , , and so on to . Summing the areas of these rectangles, we get a better approximation for the sought integral, namely

We are taking a sum of finitely many function values of , multiplied with the differences of two subsequent approximation points. We can easily see that the approximation is still too large. Using more steps produces a closer approximation, but will always be too high and will never be exact. Alternatively, replacing these subintervals by ones with the left end height of each piece, we will get an approximation that is too low: for example, with twelve such subintervals we will get an approximate value for the area of 0.6203. 

The key idea is the transition from adding "finitely many" differences of approximation points multiplied by their respective function values to using infinitely many fine, or "infinitesimal" steps. When this transition is completed in the above example, it turns out that the area under the curve within the stated bounds is 2/3.

The notation
conceives the integral as a weighted sum, denoted by the elongated , of function values, , multiplied by infinitesimal step widths, the so-called "differentials", denoted by .

Historically, after the failure of early efforts to rigorously interpret infinitesimals, Riemann formally defined integrals as a limit of weighted sums, so that the suggested the limit of a difference (namely, the interval width). Shortcomings of Riemann's dependence on intervals and continuity motivated newer definitions, especially the Lebesgue integral, which is founded on an ability to extend the idea of "measure" in much more flexible ways. Thus the notation
refers to a weighted sum in which the function values are partitioned, with measuring the weight to be assigned to each value. Here denotes the region of integration.

There are many ways of formally defining an integral, not all of which are equivalent. The differences exist mostly to deal with differing special cases which may not be integrable under other definitions, but also occasionally for pedagogical reasons. The most commonly used definitions of integral are Riemann integrals and Lebesgue integrals.

The Riemann integral is defined in terms of Riemann sums of functions with respect to "tagged partitions" of an interval. Let be a closed interval of the real line; then a "tagged partition" of is a finite sequence

This partitions the interval into sub-intervals indexed by , each of which is "tagged" with a distinguished point . A "Riemann sum" of a function with respect to such a tagged partition is defined as
thus each term of the sum is the area of a rectangle with height equal to the function value at the distinguished point of the given sub-interval, and width the same as the sub-interval width. Let be the width of sub-interval ; then the "mesh" of such a tagged partition is the width of the largest sub-interval formed by the partition, . The "Riemann integral" of a function over the interval is equal to if:
When the chosen tags give the maximum (respectively, minimum) value of each interval, the Riemann sum becomes an upper (respectively, lower) Darboux sum, suggesting the close connection between the Riemann integral and the Darboux integral.

It is often of interest, both in theory and applications, to be able to pass to the limit under the integral. For instance, a sequence of functions can frequently be constructed that approximate, in a suitable sense, the solution to a problem. Then the integral of the solution function should be the limit of the integrals of the approximations. However, many functions that can be obtained as limits are not Riemann-integrable, and so such limit theorems do not hold with the Riemann integral. Therefore, it is of great importance to have a definition of the integral that allows a wider class of functions to be integrated .

Such an integral is the Lebesgue integral, that exploits the following fact to enlarge the class of integrable functions: if the values of a function are rearranged over the domain, the integral of a function should remain the same. Thus Henri Lebesgue introduced the integral bearing his name, explaining this integral thus in a letter to Paul Montel:
As puts it, "To compute the Riemann integral of , one partitions the domain into subintervals", while in the Lebesgue integral, "one is in effect partitioning the range of ". The definition of the Lebesgue integral thus begins with a measure, μ. In the simplest case, the Lebesgue measure of an interval is its width, , so that the Lebesgue integral agrees with the (proper) Riemann integral when both exist. In more complicated cases, the sets being measured can be highly fragmented, with no continuity and no resemblance to intervals.

Using the "partitioning the range of " philosophy, the integral of a non-negative function should be the sum over of the areas between a thin horizontal strip between and . This area is just . Let }. The Lebesgue integral of is then defined by 
where the integral on the right is an ordinary improper Riemann integral ( is a strictly decreasing positive function, and therefore has a well-defined improper Riemann integral). For a suitable class of functions (the measurable functions) this defines the Lebesgue integral.

A general measurable function is Lebesgue-integrable if the sum of the absolute values of the areas of the regions between the graph of and the -axis is finite:
In that case, the integral is, as in the Riemannian case, the difference between the area above the -axis and the area below the -axis:
where

Although the Riemann and Lebesgue integrals are the most widely used definitions of the integral, a number of others exist, including:

The collection of Riemann-integrable functions on a closed interval forms a vector space under the operations of pointwise addition and multiplication by a scalar, and the operation of integration

is a linear functional on this vector space. Thus, firstly, the collection of integrable functions is closed under taking linear combinations; and, secondly, the integral of a linear combination is the linear combination of the integrals,

Similarly, the set of real-valued Lebesgue-integrable functions on a given measure space with measure is closed under taking linear combinations and hence form a vector space, and the Lebesgue integral

is a linear functional on this vector space, so that

More generally, consider the vector space of all measurable functions on a measure space , taking values in a locally compact complete topological vector space over a locally compact topological field . Then one may define an abstract integration map assigning to each function an element of or the symbol ,
that is compatible with linear combinations. In this situation, the linearity holds for the subspace of functions whose integral is an element of (i.e. "finite"). The most important special cases arise when is , , or a finite extension of the field of p-adic numbers, and is a finite-dimensional vector space over , and when and is a complex Hilbert space.

Linearity, together with some natural continuity properties and normalisation for a certain class of "simple" functions, may be used to give an alternative definition of the integral. This is the approach of Daniell for the case of real-valued functions on a set , generalized by Nicolas Bourbaki to functions with values in a locally compact topological vector space. See for an axiomatic characterisation of the integral.

A number of general inequalities hold for Riemann-integrable functions defined on a closed and bounded interval and can be generalized to other notions of integral (Lebesgue and Daniell).







In this section, is a real-valued Riemann-integrable function. The integral
over an interval is defined if . This means that the upper and lower sums of the function are evaluated on a partition whose values are increasing. Geometrically, this signifies that integration takes place "left to right", evaluating within intervals where an interval with a higher index lies to the right of one with a lower index. The values and , the end-points of the interval, are called the limits of integration of . Integrals can also be defined if :

This, with , implies:

The first convention is necessary in consideration of taking integrals over subintervals of ; the second says that an integral taken over a degenerate interval, or a point, should be zero. One reason for the first convention is that the integrability of on an interval implies that is integrable on any subinterval , but in particular integrals have the property that:

With the first convention, the resulting relation
is then well-defined for any cyclic permutation of , , and .

The "fundamental theorem of calculus" is the statement that differentiation and integration are inverse operations: if a continuous function is first integrated and then differentiated, the original function is retrieved. An important consequence, sometimes called the "second fundamental theorem of calculus", allows one to compute integrals by using an antiderivative of the function to be integrated.

Let be a continuous real-valued function defined on a closed interval . Let be the function defined, for all in , by
Then, is continuous on , differentiable on the open interval , and

for all in .

Let be a real-valued function defined on a closed interval [] that admits an antiderivative on . That is, and are functions such that for all in ,

If is integrable on then

The second fundamental theorem allows many integrals to be calculated explicitly. For example, to calculate the integral
of the square root function between 0 and 1, it is sufficient to find an antiderivative, that is, a function whose derivative equals :
One such function is . Then the value of the integral in question is

This is a case of a general rule, that for , with , the related function, the so-called antiderivative is Tables of this and similar antiderivatives can be used to calculate integrals explicitly, in much the same way that tables of derivatives can be used.

A "proper" Riemann integral assumes the integrand is defined and finite on a closed and bounded interval, bracketed by the limits of integration. An improper integral occurs when one or more of these conditions is not satisfied. In some cases such integrals may be defined by considering the limit of a sequence of proper Riemann integrals on progressively larger intervals.

If the interval is unbounded, for instance at its upper end, then the improper integral is the limit as that endpoint goes to infinity.
If the integrand is only defined or finite on a half-open interval, for instance , then again a limit may provide a finite result.

That is, the improper integral is the limit of proper integrals as one endpoint of the interval of integration approaches either a specified real number, or , or . In more complicated cases, limits are required at both endpoints, or at interior points.

Just as the definite integral of a positive function of one variable represents the area of the region between the graph of the function and the "x"-axis, the "double integral" of a positive function of two variables represents the volume of the region between the surface defined by the function and the plane that contains its domain. For example, a function in two dimensions depends on two real variables, "x" and "y", and the integral of a function "f" over the rectangle "R" given as the Cartesian product of two intervals formula_56 can be written

where the differential indicates that integration is taken with respect to area. This double integral can be defined using Riemann sums, and represents the (signed) volume under the graph of over the domain "R". Under suitable conditions (e.g., if "f" is continuous), then Fubini's theorem guarantees that this integral can be expressed as an equivalent iterated integral

This reduces the problem of computing a double integral to computing one-dimensional integrals. Because of this, another notation for the integral over "R" uses a double integral sign:

Integration over more general domains is possible. The integral of a function "f", with respect to volume, over a subset "D" of ℝ is denoted by notation such as

or similar. See volume integral.

The concept of an integral can be extended to more general domains of integration, such as curved lines and surfaces. Such integrals are known as line integrals and surface integrals respectively. These have important applications in physics, as when dealing with vector fields.

A "line integral" (sometimes called a "path integral") is an integral where the function to be integrated is evaluated along a curve. Various different line integrals are in use. In the case of a closed curve it is also called a "contour integral".

The function to be integrated may be a scalar field or a vector field. The value of the line integral is the sum of values of the field at all points on the curve, weighted by some scalar function on the curve (commonly arc length or, for a vector field, the scalar product of the vector field with a differential vector in the curve). This weighting distinguishes the line integral from simpler integrals defined on intervals. Many simple formulas in physics have natural continuous analogs in terms of line integrals; for example, the fact that work is equal to force, , multiplied by displacement, , may be expressed (in terms of vector quantities) as:
For an object moving along a path in a vector field such as an electric field or gravitational field, the total work done by the field on the object is obtained by summing up the differential work done in moving from to . This gives the line integral

A "surface integral" is a definite integral taken over a surface (which may be a curved set in space); it can be thought of as the double integral analog of the line integral. The function to be integrated may be a scalar field or a vector field. The value of the surface integral is the sum of the field at all points on the surface. This can be achieved by splitting the surface into surface elements, which provide the partitioning for Riemann sums.

For an example of applications of surface integrals, consider a vector field on a surface ; that is, for each point in , is a vector. Imagine that we have a fluid flowing through , such that determines the velocity of the fluid at . The flux is defined as the quantity of fluid flowing through in unit amount of time. To find the flux, we need to take the dot product of with the unit surface normal to at each point, which will give us a scalar field, which we integrate over the surface:
The fluid flux in this example may be from a physical fluid such as water or air, or from electrical or magnetic flux. Thus surface integrals have applications in physics, particularly with the classical theory of electromagnetism.

In complex analysis, the integrand is a complex-valued function of a complex variable instead of a real function of a real variable . When a complex function is integrated along a curve formula_64 in the complex plane, the integral is denoted as follows

This is known as a contour integral.

A differential form is a mathematical concept in the fields of multivariable calculus, differential topology, and tensors. Differential forms are organized by degree. For example, a one-form is a weighted sum of the differentials of the coordinates, such as:
where "E", "F", "G" are functions in three dimensions. A differential one-form can be integrated over an oriented path, and the resulting integral is just another way of writing a line integral. Here the basic differentials "dx", "dy", "dz" measure infinitesimal oriented lengths parallel to the three coordinate axes.

A differential two-form is a sum of the form
Here the basic two-forms formula_68 measure oriented areas parallel to the coordinate two-planes. The symbol formula_69 denotes the wedge product, which is similar to the cross product in the sense that the wedge product of two forms representing oriented lengths represents an oriented area. A two-form can be integrated over an oriented surface, and the resulting integral is equivalent to the surface integral giving the flux of formula_70.

Unlike the cross product, and the three-dimensional vector calculus, the wedge product and the calculus of differential forms makes sense in arbitrary dimension and on more general manifolds (curves, surfaces, and their higher-dimensional analogs). The exterior derivative plays the role of the gradient and curl of vector calculus, and Stokes' theorem simultaneously generalizes the three theorems of vector calculus: the divergence theorem, Green's theorem, and the Kelvin-Stokes theorem.

The discrete equivalent of integration is summation. Summations and integrals can be put on the same foundations using the theory of Lebesgue integrals or time scale calculus.

The most basic technique for computing definite integrals of one real variable is based on the fundamental theorem of calculus. Let be the function of to be integrated over a given interval . Then, find an antiderivative of ; that is, a function such that on the interval. Provided the integrand and integral have no singularities on the path of integration, by the fundamental theorem of calculus,

The integral is not actually the antiderivative, but the fundamental theorem provides a way to use antiderivatives to evaluate definite integrals.

The most difficult step is usually to find the antiderivative of . It is rarely possible to glance at a function and write down its antiderivative. More often, it is necessary to use one of the many techniques that have been developed to evaluate integrals. Most of these techniques rewrite one integral as a different one which is hopefully more tractable. Techniques include:
Alternative methods exist to compute more complex integrals. Many nonelementary integrals can be expanded in a Taylor series and integrated term by term. Occasionally, the resulting infinite series can be summed analytically. The method of convolution using Meijer G-functions can also be used, assuming that the integrand can be written as a product of Meijer G-functions. There are also many less common ways of calculating definite integrals; for instance, Parseval's identity can be used to transform an integral over a rectangular region into an infinite sum. Occasionally, an integral can be evaluated by a trick; for an example of this, see Gaussian integral.

Computations of volumes of solids of revolution can usually be done with disk integration or shell integration.

Specific results which have been worked out by various techniques are collected in the list of integrals.

Many problems in mathematics, physics, and engineering involve integration where an explicit formula for the integral is desired. Extensive tables of integrals have been compiled and published over the years for this purpose. With the spread of computers, many professionals, educators, and students have turned to computer algebra systems that are specifically designed to perform difficult or tedious tasks, including integration. Symbolic integration has been one of the motivations for the development of the first such systems, like Macsyma.

A major mathematical difficulty in symbolic integration is that in many cases, a closed formula for the antiderivative of a rather simple-looking function does not exist. For instance, it is known that the antiderivatives of the functions and cannot be expressed in the closed form involving only rational and exponential functions, logarithm, trigonometric functions and inverse trigonometric functions, and the operations of multiplication and composition; in other words, none of the three given functions is integrable in elementary functions, which are the functions which may be built from rational functions, roots of a polynomial, logarithm, and exponential functions. The Risch algorithm provides a general criterion to determine whether the antiderivative of an elementary function is elementary, and, if it is, to compute it. Unfortunately, it turns out that functions with closed expressions of antiderivatives are the exception rather than the rule. Consequently, computerized algebra systems have no hope of being able to find an antiderivative for a randomly constructed elementary function. On the positive side, if the 'building blocks' for antiderivatives are fixed in advance, it may be still be possible to decide whether the antiderivative of a given function can be expressed using these blocks and operations of multiplication and composition, and to find the symbolic answer whenever it exists. The Risch algorithm, implemented in Mathematica and other computer algebra systems, does just that for functions and antiderivatives built from rational functions, radicals, logarithm, and exponential functions.

Some special integrands occur often enough to warrant special study. In particular, it may be useful to have, in the set of antiderivatives, the special functions (like the Legendre functions, the hypergeometric function, the gamma function, the incomplete gamma function and so on — see Symbolic integration for more details). Extending the Risch's algorithm to include such functions is possible but challenging and has been an active research subject.

More recently a new approach has emerged, using "D"-finite functions, which are the solutions of linear differential equations with polynomial coefficients. Most of the elementary and special functions are "D"-finite, and the integral of a "D"-finite function is also a "D"-finite function. This provides an algorithm to express the antiderivative of a "D"-finite function as the solution of a differential equation.

This theory also allows one to compute the definite integral of a "D"-function as the sum of a series given by the first coefficients, and provides an algorithm to compute any coefficient.

Some integrals found in real applications can be computed by closed-form antiderivatives. Others are not so accommodating. Some antiderivatives do not have closed forms, some closed forms require special functions that themselves are a challenge to compute, and others are so complex that finding the exact answer is too slow. This motivates the study and application of numerical approximations of integrals. This subject, called "numerical integration" or "numerical quadrature", arose early in the study of integration for the purpose of making hand calculations. The development of general-purpose computers made numerical integration more practical and drove a desire for improvements. The goals of numerical integration are accuracy, reliability, efficiency, and generality, and sophisticated modern methods can vastly outperform a naive method by all four measures (; ; ).

Consider, for example, the integral
which has the exact answer . (In ordinary practice, the answer is not known in advance, so an important task — not explored here — is to decide when an approximation is good enough.) A “calculus book” approach divides the integration range into, say, 16 equal pieces, and computes function values.

Using the left end of each piece, the rectangle method sums 16 function values and multiplies by the step width, , here 0.25, to get an approximate value of 3.94325 for the integral. The accuracy is not impressive, but calculus formally uses pieces of infinitesimal width, so initially this may seem little cause for concern. Indeed, repeatedly doubling the number of steps eventually produces an approximation of 3.76001. However, 2 pieces are required, a great computational expense for such little accuracy; and a reach for greater accuracy can force steps so small that arithmetic precision becomes an obstacle.

A better approach replaces the rectangles used in a Riemann sum with trapezoids. The trapezoid rule is almost as easy to calculate; it sums all 17 function values, but weights the first and last by one half, and again multiplies by the step width. This immediately improves the approximation to 3.76925, which is noticeably more accurate. Furthermore, only 2 pieces are needed to achieve 3.76000, substantially less computation than the rectangle method for comparable accuracy. The idea behind the trapezoid rule, that more accurate approximations to the function yield better approximations to the integral, can be carried further. Simpson's rule approximates the integrand by a piecewise quadratic function. Riemann sums, the trapezoid rule, and Simpson's rule are examples of a family of quadrature rules called Newton–Cotes formulas. The degree Newton–Cotes quadrature rule approximates the polynomial on each subinterval by a degree polynomial. This polynomial is chosen to interpolate the values of the function on the interval. Higher degree Newton-Cotes approximations can be more accurate, but they require more function evaluations (already Simpson's rule requires twice the function evaluations of the trapezoid rule), and they can suffer from numerical inaccuracy due to Runge's phenomenon. One solution to this problem is Clenshaw–Curtis quadrature, in which the integrand is approximated by expanding it in terms of Chebyshev polynomials. This produces an approximation whose values never deviate far from those of the original function.

Romberg's method builds on the trapezoid method to great effect. First, the step lengths are halved incrementally, giving trapezoid approximations denoted by , and so on, where is half of . For each new step size, only half the new function values need to be computed; the others carry over from the previous size (as shown in the table above). But the really powerful idea is to interpolate a polynomial through the approximations, and extrapolate to . With this method a numerically "exact" answer here requires only four pieces (five function values). The Lagrange polynomial interpolating {(4.00,6.128), (2.00,4.352), (1.00,3.908)} is 3.76 + 0.148, producing the extrapolated value 3.76 at .

Gaussian quadrature often requires noticeably less work for superior accuracy. In this example, it can compute the function values at just two positions, , then double each value and sum to get the numerically exact answer. The explanation for this dramatic success lies in the choice of points. Unlike Newton–Cotes rules, which interpolate the integrand at evenly spaced points, Gaussian quadrature evaluates the function at the roots of a set of orthogonal polynomials. An -point Gaussian method is exact for polynomials of degree up to . The function in this example is a degree 3 polynomial, plus a term that cancels because the chosen endpoints are symmetric around zero. (Cancellation also benefits the Romberg method.)

In practice, each method must use extra evaluations to ensure an error bound on an unknown function; this tends to offset some of the advantage of the pure Gaussian method, and motivates the popular Gauss–Kronrod quadrature formulae. More broadly, adaptive quadrature partitions a range into pieces based on function properties, so that data points are concentrated where they are needed most.

The computation of higher-dimensional integrals (for example, volume calculations) makes important use of such alternatives as Monte Carlo integration.

A calculus text is no substitute for numerical analysis, but the reverse is also true. Even the best adaptive numerical code sometimes requires a user to help with the more demanding integrals. For example, improper integrals may require a change of variable or methods that can avoid infinite function values, and known properties like symmetry and periodicity may provide critical leverage. For example, the integral formula_73 is difficult to evaluate numerically because it is infinite at . However, the substitution transforms the integral into formula_74, which has no singularities at all.

The area of an arbitrary two-dimensional shape can be determined using a measuring instrument called planimeter. The volume of irregular objects can be measured with precision by the fluid displaced as the object is submerged.

Area can sometimes be found via geometrical compass-and-straightedge constructions of an equivalent square.







</doc>
<doc id="15533" url="https://en.wikipedia.org/wiki?curid=15533" title="Zionist political violence">
Zionist political violence

Zionist political violence or refers to acts of violence or terror committed by Zionists.

Actions have been carried out by individuals and Jewish paramilitary groups such as the Irgun, the Lehi, the Haganah and the Palmach as part of a conflict between Jews, British authorities, and Palestinian Arabs, regarding land, immigration, and control over Palestine.

British soldiers and officials, United Nations personnel, Palestinian Arab fighters and civilians, and Jewish fighters and civilians have been targets or victims of these actions. Domestic, commercial, and government property, infrastructure, and material have also been attacked.

During World War I, Zionist volunteers fought in the Jewish Legion of the British Army against the Ottoman Turks

During the 1920 Nebi Musa riots, the 1921 Jaffa riots and the 1929 Palestine riots, Palestinian Arabs manifested hostility against zionist immigration, which provoked the reaction of Jewish militias. In 1935, the Irgun, a Zionist underground military organization, split off from the Haganah. The Irgun were the armed expression of the nascent ideology of Revisionist Zionism founded by Ze'ev Jabotinsky. He expressed this ideology as ""every Jew had the right to enter Palestine; only active retaliation would deter the Arab and the British; only Jewish armed force would ensure the Jewish state"".

During the 1936–39 Arab revolt in Palestine, Palestinian Arabs fought for the end of the Mandate and the creation of an Arab state based on the whole of Palestine. They attacked both British and Jews as well as some Palestinian Arabs who supported a Pan-Arabism. Mainstream Zionists, represented by the Vaad Leumi and the Haganah, practiced the policy of Havlagah (restraint), while Irgun militants did not follow this policy and called themselves "Havlagah breakers." The Irgun began bombing Palestinian Arab civilian targets in 1938. While the Palestinian Arabs were "carefully disarmed" by the British Mandatory authorities by 1939, the Zionists were not.

After the beginning of World War II, the Haganah and Irgun suspended their activity against the British in support of their war against Nazi Germany. The smaller Lehi continued anti-British attacks and direct action throughout the war. At that time, the British also supported the creation and the training of Palmach, as a unit that could withstand a German offensive in the area, with the consent of Yishuv which saw an opportunity to get trained units and soldiers for the planned Jewish state and during 1944–1945, the most mainstream Jewish paramilitary organization, Haganah, cooperated with the British authorities against the Lehi and Etzel.

After World War II, between 1945 and the 29 November 1947 Partition vote, British soldiers and policemen were targeted by Irgun and Lehi. Haganah and Palmah first collaborated with the British against them, particularly during the Hunting Season, before actively joining them in the Jewish Resistance Movement, then finally choosing an official neutral position after 1946 while the Irgun and the Lehi went on their attacks against the British.

The Haganah carried out violent attacks in Palestine, such as the liberation of interned immigrants from the Atlit camp, the bombing of the country's railroad network, sabotage raids on radar installations and bases of the British Palestine police. It also continued to organize illegal immigration.

In February 1947, the British announced that they would end the mandate and withdraw from Palestine and they asked the arbitration of the United Nations. After the vote of the Partition Plan for Palestine on 30 November 1947, civil war broke out in Palestine. Jewish and Arab communities fought each other violently in campaigns of attacks, retaliations and counter-retaliations which provoked around 800 deaths after two months. Arab volunteers entered Palestine to fight alongside the Palestinian Arabs. In April, 6 weeks before the termination of the Mandate, the Jewish militias launched wide operations to control the territory dedicated to them by the Partition Plan. Many atrocities occurred during this time. The Arab population in the mixed cities of Tiberias, Safed, Haifa, Jaffa, Beisan and Acre and in the neighbouring villages fled or were expelled during this period. During the Battle for Jerusalem (1948) where the Jewish community of 100,000 people was besieged, most Arab villages of the Tel Aviv – Jerusalem corridor were captured by Jewish militias and leveled.

At the beginning of the civil war, the Jewish militias organized several bombing attacks against civilians and military Arab targets. On 12 December, Irgun placed a car bomb opposite the Damascus Gate, killing 20 people. On 4 January 1948, the Lehi detonated a lorry bomb against the headquarters of the paramilitary Najjada located in Jaffa's Town Hall, killing 15 Arabs and injuring 80. During the night between 5 and 6 January, the Haganah bombed the Semiramis Hotel in Jerusalem that had been reported to hide Arab militiamen, killing 24 people. The next day, Irgun members in a stolen police van rolled a barrel bomb into a large group of civilians who were waiting for a bus by the Jaffa Gate, killing around 16. Another Irgun bomb went off in the Ramla market on February 18, killing 7 residents and injuring 45. On 28 February, the Palmah organised a bombing attack against a garage at Haifa, killing 30 people.

Irgun was described as a terrorist organization by the United Nations, British, and United States governments, and in media such as "The New York Times" newspaper, and by the Anglo-American Committee of Inquiry. In 1946, The World Zionist Congress strongly condemned terrorist activities in Palestine and "the shedding of innocent blood as a means of political warfare". Irgun was specifically condemned.

Menachem Begin was called a terrorist and a fascist by Albert Einstein and 27 other prominent Jewish intellectuals in a letter to the New York Times which was published on December 4, 1948. Specifically condemned was the participation of the Irgun in the Deir Yassin massacre:


The letter warns American Jews against supporting Begin's request for funding of his political party Herut, and ends with the warning:

Lehi was described as a terrorist organization by the British authorities and United Nations mediator Ralph Bunche.

During the conflict between Arabs and Jews in Palestine before the war, the criterion of "Purity of arms" was used to distinguish between the respective attitudes of the Irgun and Haganah towards Arabs, with the latter priding itself on its adherence to principle. The Jewish society in the British Mandate Palestine generally disapproved and denounced violent attacks both on grounds moral rejection and political disagreement, stressing that terrorism is counter-productive in the Zionist quest for Jewish self-determination. Generally speaking, this precept requires that "weapons remain pure [and that] they are employed only in self-defence and [never] against innocent civilians and defenceless people". But if it "remained a central value in education" it was "rather vague and intentionally blurred" at the practical level.

In 1946, at a meeting held between the heads of the Haganah, David Ben-Gurion predicted a confrontation between the Arabs of Palestine and the Arab states. Concerning the "principle of purity of arms", he stressed that: "The end does not justify all means. Our war is based on moral grounds" and during the 1948 War, the Mapam, the political party affiliated to Palmach, asked "a strict observance of the Jewish Purity of arms to secure the moral character of [the] war". When he was later criticized by Mapam members for his attitude concerning the Arab refugee problem, Ben-Gurion reminded them of the Palestinian exodus from Lydda and Ramle and the fact Palmah officers had been responsible for the "outrage that had encouraged the Arabs' flight made the party uncomfortable."

According to Avi Shlaim, this condemnation of the use of violence is one of the key features of 'the conventional Zionist account or old history' whose 'popular-heroic-moralistic version' is 'taught in Israeli schools and used extensively in the quest for legitimacy abroad'. Benny Morris adds that '[t]he Israelis' collective memory of fighters characterized by "purity of arms" is also undermined by the evidence of [the dozen case] of rapes committed in conquered towns and villages.' According to him, 'after the 1948 war, the Israelis tended to hail the "purity of arms" of its militiamen and soldiers to contrast this with Arab barbarism, which on occasion expressed itself in the mutilation of captured Jewish corpses.' According to him, 'this reinforced the Israelis' positive self-image and helped them "sell" the new state abroad and (...) demonized the enemy'.

Some Israelis justify acts of political violence. Sixty years after participating in the assassination of Count Bernadotte, Geula Cohen had no regrets. As a broadcaster on Lehi's radio, she recalled the threats against Bernadotte in advance of the assassination. "I told him if you are not going to leave Jerusalem and go to your Stockholm, you won't be any more." Asked if it was right to assassinate Bernadotte, she replied, "There is no question about it. We would not have Jerusalem any more." In July 2006, the Menachem Begin Heritage Center organized a conference to mark the 60th anniversary of the King David Hotel bombing. The conference was attended by past and future Prime Minister Benjamin Netanyahu and former members of Irgun. The British Ambassador in Tel Aviv and the Consul-General in Jerusalem protested that a plaque commemorating the bombing stated "For reasons known only to the British, the hotel was not evacuated." Netanyahu, then chairman of Likud and Leader of the Opposition in the Knesset, opined that the bombing was a legitimate act with a military target, distinguishing it from an act of terror intended to harm civilians, since Irgun sent warnings to evacuate the building. He said "Imagine that Hamas or Hizbullah would call the military headquarters in Tel Aviv and say, 'We have placed a bomb and we are asking you to evacuate the area.' They don't do that. That is the difference." The British Ambassador in Tel Aviv and the Consul-General in Jerusalem protested, saying "We do not think that it is right for an act of terrorism, which led to the loss of many lives, to be commemorated", and wrote to the Mayor of Jerusalem that such an "act of terror" could not be honored. The British government also demanded the removal of the plaque, pointing out that the statement on it accusing the British of failing to evacuate the hotel was untrue and "did not absolve those who planted the bomb." To prevent a diplomatic incident, changes were made in the plaque's text. The final English version says "Warning phone calls have been made to the hotel, The Palestine Post and the French Consulate, urging the hotel's occupants to leave immediately. The hotel was not evacuated and after 25 minutes the bombs exploded. To the Irgun's regret, 92 persons were killed."





</doc>
<doc id="15536" url="https://en.wikipedia.org/wiki?curid=15536" title="List of airports">
List of airports

An airport is an aerodrome with facilities for flights to take off and land. Airports often have facilities to store and maintain aircraft, and a control tower. An airport consists of a landing area, which comprises an aerially accessible open space including at least one operationally active surface such as a runway for a plane to take off or a helipad, and often includes adjacent utility buildings such as control towers, hangars and terminals.

An airport with a helipad for rotorcraft but no runway is called a heliport. An airport for use by seaplanes and amphibious aircraft is called a seaplane base. Such a base typically includes a stretch of open water for takeoffs and landings, and seaplane docks for tying-up.

An international airport has additional facilities for customs and immigration.





</doc>
