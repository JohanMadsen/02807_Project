<doc id="23041" url="https://en.wikipedia.org/wiki?curid=23041" title="Puerto Rico">
Puerto Rico

Puerto Rico (Spanish for "Rich Port"), officially the Commonwealth of Puerto Rico (, "Free Associated State of Puerto Rico") and briefly called Porto Rico, is an unincorporated territory of the United States located in the northeast Caribbean Sea, approximately southeast of Miami, Florida.

An archipelago among the Greater Antilles, Puerto Rico includes the main island of Puerto Rico and a number of smaller ones, such as Mona, Culebra, and Vieques. The capital and most populous city is San Juan. Its official languages are Spanish and English, though Spanish predominates. The island's population is approximately 3.4 million. Puerto Rico's history, tropical climate, natural scenery, traditional cuisine, and tax incentives make it a destination for travelers from around the world.

Originally populated by the indigenous Taíno people, the island was claimed in 1493 by Christopher Columbus for Spain during his second voyage. Later it endured invasion attempts from the French, Dutch, and British. Four centuries of Spanish colonial government influenced the island's cultural landscapes with waves of African slaves, Canarian, and Andalusian settlers. In the Spanish Empire, Puerto Rico played a secondary, but strategic role when compared to wealthier colonies like Peru and the mainland parts of New Spain. Spain's distant administrative control continued up to the end of the 19th century, helping to produce a distinctive creole Hispanic culture and language that combined elements from the Native Americans, Africans, and Iberians. In 1898, following the Spanish–American War, the United States acquired Puerto Rico under the terms of the Treaty of Paris. The treaty took effect on April 11, 1899.

Puerto Ricans are by law citizens of the United States and may move freely between the island and the mainland. As it is not a state, Puerto Rico does not have a vote in the United States Congress, which governs the territory with full jurisdiction under the Puerto Rico Federal Relations Act of 1950. However, Puerto Rico does have one non-voting member of the House called a Resident Commissioner. As residents of a U.S. territory, American citizens in Puerto Rico are disenfranchised at the national level and do not vote for president and vice president of the United States, and do not pay federal income tax on Puerto Rican income. Like other territories and the District of Columbia, Puerto Rico does not have U.S. senators. Congress approved a local constitution in 1952, allowing U.S. citizens on the territory to elect a governor. A 2012 referendum showed a majority (54% of those who voted) disagreed with "the present form of territorial status". A second question asking about a new model, had full statehood the preferred option among those who voted for a change of status, although a significant number of people did not answer the second question of the referendum. A fifth referendum was held on June 11, 2017, with "Statehood" and "Independence/Free Association" initially as the only available choices. At the recommendation of the Department of Justice, an option for the "current territorial status" was added. The referendum showed an overwhelming support for statehood, with 97.18% voting for it, although the voter turnout had a historically low figure of only 22.99% of the registered voters casting their ballots.

In early 2017, the Puerto Rican government-debt crisis posed serious problems for the government. The outstanding bond debt had climbed to $70 billion at a time with 12.4% unemployment. The debt had been increasing during a decade long recession. This was the second major financial crisis to affect the island after the Great Depression when the U.S. government, in 1935, provided relief efforts through the Puerto Rico Reconstruction Administration. On May 3, 2017, Puerto Rico's financial oversight board in the U.S. District Court for Puerto Rico filed the debt restructuring petition which was made under Title III of PROMESA. By early August 2017, the debt was $72 billion with a 45% poverty rate.

In late September 2017, Hurricane Maria made landfall in Puerto Rico, causing devastating damage. The island's electrical grid was largely destroyed, with repairs expected to take months to complete, provoking the largest power outage in American history. Recovery efforts were somewhat slow in the first few months, and over 200,000 residents had moved to Florida alone by late November 2017.

Puerto Rico means "rich port" in Spanish. Puerto Ricans often call the island ' – a derivation of ', its indigenous Taíno name, which means "Land of the Valiant Lord". The terms ' and ' derive from ' and ' respectively, and are commonly used to identify someone of Puerto Rican heritage. The island is also popularly known in Spanish as ", meaning "the island of enchantment".

Columbus named the island ', in honor of Saint John the Baptist, while the capital city was named ' ("Rich Port City"). Eventually traders and other maritime visitors came to refer to the entire island as Puerto Rico, while San Juan became the name used for the main trading/shipping port and the capital city.

The island's name was changed to "Porto Rico" by the United States after the Treaty of Paris of 1898. The anglicized name was used by the U.S. government and private enterprises. The name was changed back to Puerto Rico by a joint resolution in Congress introduced by Félix Córdova Dávila in 1931.

The official name of the entity in Spanish is " ("free associated state of Puerto Rico"), while its official English name is Commonwealth of Puerto Rico.

The ancient history of the archipelago which is now Puerto Rico is not well known. Unlike other indigenous cultures in the New World (Aztec, Maya and Inca) which left behind abundant archeological and physical evidence of their societies, scant artifacts and evidence remain of the Puerto Rico's indigenous population. Scarce archaeological findings and early Spanish accounts from the colonial era constitute all that is known about them. The first comprehensive book on the history of Puerto Rico was written by Fray Íñigo Abbad y Lasierra in 1786, nearly three centuries after the first Spaniards landed on the island.

The first known settlers were the Ortoiroid people, an Archaic Period culture of Amerindian hunters and fishermen who migrated from the South American mainland. Some scholars suggest their settlement dates back about 4,000 years. An archeological dig in 1990 on the island of Vieques found the remains of a man, designated as the "Puerto Ferro Man", which was dated to around 2000 BC. The Ortoiroid were displaced by the Saladoid, a culture from the same region that arrived on the island between 430 and 250 BC.

The Igneri tribe migrated to Puerto Rico between 120 and 400 AD from the region of the Orinoco river in northern South America. The Arcaico and Igneri co-existed on the island between the 4th and 10th centuries.

Between the 7th and 11th centuries, the Taíno culture developed on the island. By approximately 1000 AD, it had become dominant. At the time of Columbus' arrival, an estimated 30,000 to 60,000 Taíno Amerindians, led by the "cacique" (chief) Agüeybaná, inhabited the island. They called it "Boriken", meaning "the great land of the valiant and noble Lord". The natives lived in small villages, each led by a cacique. They subsisted by hunting and fishing, done generally by men, as well as by the women's gathering and processing of indigenous cassava root and fruit. This lasted until Columbus arrived in 1493.

When Columbus arrived in Puerto Rico during his second voyage on November 19, 1493, the island was inhabited by the Taíno. They called it "Borikén" ("Borinquen" in Spanish transliteration). Columbus named the island San Juan Bautista, in honor of St John the Baptist. Having reported the findings of his first travel, Columbus brought with him this time a letter from King Ferdinand empowered by a papal bull that authorized any course of action necessary for the expansion of the Spanish Empire and the Christian faith. Juan Ponce de León, a lieutenant under Columbus, founded the first Spanish settlement, Caparra, on August 8, 1508. He later served as the first governor of the island. Eventually, traders and other maritime visitors came to refer to the entire island as Puerto Rico, and San Juan became the name of the main trading/shipping port.

At the beginning of the 16th century, the Spanish people began to colonize the island. Despite the Laws of Burgos of 1512 and other decrees for the protection of the indigenous population, some Taíno Indians were forced into an encomienda system of forced labor in the early years of colonization. The population suffered extremely high fatalities from epidemics of European infectious diseases.

In 1520, King Charles I of Spain issued a royal decree collectively emancipating the remaining Taíno population. By that time, the Taíno people were few in number. Enslaved Africans had already begun to be imported to compensate for the native labor loss, but their numbers were proportionate to the diminished commercial interest Spain soon began to demonstrate for the island colony. Other nearby islands, like Cuba, Saint-Domingue, and Guadeloupe, attracted more of the slave trade than Puerto Rico, probably because of greater agricultural interests in those islands, on which colonists had developed large sugar plantations and had the capital to invest in the Atlantic slave trade.

From the beginning of the country, the colonial administration relied heavily on the industry of enslaved Africans and creole blacks for public works and defenses, primarily in coastal ports and cities, where the tiny colonial population had hunkered down. With no significant industries or large-scale agricultural production as yet, enslaved and free communities lodged around the few littoral settlements, particularly around San Juan, also forming lasting Afro-creole communities. Meanwhile, in the island's interior, there developed a mixed and independent peasantry that relied on a subsistence economy. This mostly unsupervised population supplied villages and settlements with foodstuffs and, in relative isolation, set the pattern for what later would be known as the Puerto Rican Jíbaro culture. By the end of the 16th century, the Spanish Empire was diminishing and, in the face of increasing raids from European competitors, the colonial administration throughout the Americas fell into a "bunker mentality". Imperial strategists and urban planners redesigned port settlements into military posts with the objective of protecting Spanish territorial claims and ensuring the safe passing of the king's silver-laden Atlantic Fleet to the Iberian Peninsula. San Juan served as an important port-of-call for ships driven across the Atlantic by its powerful trade winds. West Indies convoys linked Spain to the island, sailing between Cádiz and the Spanish West Indies. The colony's seat of government was on the forested Islet of San Juan and for a time became one of the most heavily fortified settlements in the Spanish Caribbean earning the name of the "Walled City". The islet is still dotted with the various forts and walls, such as La Fortaleza, Castillo San Felipe del Morro, and Castillo San Cristóbal, designed to protect the population and the strategic Port of San Juan from the raids of the Spanish European competitors.

In 1625, in the Battle of San Juan, the Dutch commander Boudewijn Hendricksz tested the defenses' limits like no one else before. Learning from Francis Drake's previous failures here, he circumvented the cannons of the castle of San Felipe del Morro and quickly brought his 17 ships into the San Juan Bay. He then occupied the port and attacked the city while the population hurried for shelter behind the Morro's moat and high battlements. Historians consider this event the worst attack on San Juan. Though the Dutch set the village on fire, they failed to conquer the Morro, and its batteries pounded their troops and ships until Hendricksz deemed the cause lost. Hendricksz's expedition eventually helped propel a fortification frenzy. Constructions of defenses for the San Cristóbal Hill were soon ordered so as to prevent the landing of invaders out of reach of the Morro's artillery. Urban planning responded to the needs of keeping the colony in Spanish hands.

During the late 16th and early 17th centuries, Spain concentrated its colonial efforts on the more prosperous mainland North, Central, and South American colonies. With the advent of the lively Bourbon Dynasty in Spain in the 1700s, the island of Puerto Rico began a gradual shift to more imperial attention. More roads began connecting previously isolated inland settlements to coastal cities, and coastal settlements like Arecibo, Mayaguez, and Ponce began acquiring importance of their own, separate from San Juan. By the end of the 18th century, merchant ships from an array of nationalities threatened the tight regulations of the Mercantilist system, which turned each colony solely toward the European metropole and limited contact with other nations. U.S. ships came to surpass Spanish trade and with this also came the exploitation of the island's natural resources. Slavers, which had made but few stops on the island before, began selling more enslaved Africans to growing sugar and coffee plantations. The increasing number of Atlantic wars in which the Caribbean islands played major roles, like the War of Jenkins' Ear, the Seven Years' War and the Atlantic Revolutions, ensured Puerto Rico's growing esteem in Madrid's eyes. On April 17, 1797, Sir Ralph Abercromby’s fleet invaded the island with a force of 6,000–13,000 men, which included German soldiers and Royal Marines and 60 to 64 ships. Fierce fighting continued for the next days with Spanish troops. Both sides suffered heavy losses. On Sunday April 30 the British ceased their attack and began their retreat from San Juan. By the time independence movements in the larger Spanish colonies gained success, new waves of loyal creole immigrants began to arrive in Puerto Rico, helping to tilt the island's political balance toward the Crown.
In 1809, to secure its political bond with the island and in the midst of the European Peninsular War, the Supreme Central Junta based in Cádiz recognized Puerto Rico as an overseas province of Spain. This gave the island residents the right to elect representatives to the recently convened Spanish parliament (Cádiz Cortes), with equal representation to mainland Iberian, Mediterranean (Balearic Islands) and Atlantic maritime Spanish provinces (Canary Islands).

Ramón Power y Giralt, the first Spanish parliamentary representative from the island of Puerto Rico, died after serving a three-year term in the Cortes. These parliamentary and constitutional reforms were in force from 1810 to 1814, and again from 1820 to 1823. They were twice reversed during the restoration of the traditional monarchy by Ferdinand VII. Immigration and commercial trade reforms in the 19th century increased the island's ethnic European population and economy and expanded the Spanish cultural and social imprint on the local character of the island.

Minor slave revolts had occurred on the island throughout the years, with the revolt planned and organized by Marcos Xiorro in 1821 being the most important. Even though the conspiracy was unsuccessful, Xiorro achieved legendary status and is part of Puerto Rico's folklore.

In the early 19th century, Puerto Rico spawned an independence movement that, due to harsh persecution by the Spanish authorities, convened in the island of St. Thomas. The movement was largely inspired by the ideals of Simón Bolívar in establishing a United Provinces of New Granada and Venezuela, that included Puerto Rico and Cuba. Among the influential members of this movement were Brigadier General Antonio Valero de Bernabé and María de las Mercedes Barbudo. The movement was discovered, and Governor Miguel de la Torre had its members imprisoned or exiled.

With the increasingly rapid growth of independent former Spanish colonies in the South and Central American states in the first part of the 19th century, the Spanish Crown considered Puerto Rico and Cuba of strategic importance. To increase its hold on its last two New World colonies, the Spanish Crown revived the Royal Decree of Graces of 1815 as a result of which 450,000 immigrants, mainly Spaniards, settled on the island in the period up until the American conquest. Printed in three languages—Spanish, English, and French—it was intended to also attract non-Spanish Europeans, with the hope that the independence movements would lose their popularity if new settlers had stronger ties to the Crown. Hundreds of non Spanish families, mainly from Corsica, France, Germany, Ireland, Italy and Scotland, also immigrated to the island.

Free land was offered as an incentive to those who wanted to populate the two islands, on the condition that they swear their loyalty to the Spanish Crown and allegiance to the Roman Catholic Church. The offer was very successful, and European immigration continued even after 1898. Puerto Rico still receives Spanish and European immigration.
Poverty and political estrangement with Spain led to a small but significant uprising in 1868 known as "Grito de Lares." It began in the rural town of Lares, but was subdued when rebels moved to the neighboring town of San Sebastián.

Leaders of this independence movement included Ramón Emeterio Betances, considered the "father" of the Puerto Rican independence movement, and other political figures such as Segundo Ruiz Belvis. Slavery was abolished in Puerto Rico in 1873, "with provisions for periods of apprenticeship".

Leaders of "El Grito de Lares" went into exile in New York City. Many joined the Puerto Rican Revolutionary Committee, founded on December 8, 1895, and continued their quest for Puerto Rican independence. In 1897, Antonio Mattei Lluberas and the local leaders of the independence movement in Yauco organized another uprising, which became known as the "Intentona de Yauco". They raised what they called the Puerto Rican flag, which was adopted as the national flag. The local conservative political factions opposed independence. Rumors of the planned event spread to the local Spanish authorities who acted swiftly and put an end to what would be the last major uprising in the island to Spanish colonial rule.

In 1897, Luis Muñoz Rivera and others persuaded the liberal Spanish government to agree to grant limited self-government to the island by royal decree in the Autonomic Charter, including a bicameral legislature. In 1898, Puerto Rico's first, but short-lived, quasi-autonomous government was organized as an "overseas province" of Spain. This bilaterally agreed-upon charter maintained a governor appointed by the King of Spain – who held the power to annul any legislative decision – and a partially elected parliamentary structure. In February, Governor-General Manuel Macías inaugurated the new government under the Autonomic Charter. General elections were held in March and the new government began to function on July 17, 1898.

In 1890, Captain Alfred Thayer Mahan, a member of the Navy War Board and leading U.S. strategic thinker, published a book titled "The Influence of Sea Power upon History" in which he argued for the establishment of a large and powerful navy modeled after the British Royal Navy. Part of his strategy called for the acquisition of colonies in the Caribbean, which would serve as coaling and naval stations. They would serve as strategic points of defense with the construction of a canal through the Isthmus of Panama, to allow easier passage of ships between the Atlantic and Pacific oceans.

William H. Seward, the former Secretary of State under presidents Abraham Lincoln and Andrew Johnson, had also stressed the importance of building a canal in Honduras, Nicaragua or Panama. He suggested that the United States annex the Dominican Republic and purchase Puerto Rico and Cuba. The U.S. Senate did not approve his annexation proposal, and Spain rejected the U.S. offer of dollars for Puerto Rico and Cuba.

Since 1894, the United States Naval War College had been developing contingency plans for a war with Spain. By 1896, the U.S. Office of Naval Intelligence had prepared a plan that included military operations in Puerto Rican waters. Except for one 1895 plan, which recommended annexation of the island then named "Isle of Pines" (later renamed as Isla de la Juventud), a recommendation dropped in later planning, plans developed for attacks on Spanish territories were intended as support operations against Spain's forces in and around Cuba. Recent research suggests that the U.S. did consider Puerto Rico valuable as a naval station, and recognized that it and Cuba generated lucrative crops of sugar – a valuable commercial commodity which the United States lacked, before the development of the sugar beet industry in the United States.
On July 25, 1898, during the Spanish–American War, the U.S. invaded Puerto Rico with a landing at Guánica. As an outcome of the war, Spain ceded Puerto Rico, along with the Philippines and Guam, then under Spanish sovereignty, to the U.S. under the Treaty of Paris, which went into effect on April 11, 1899. Spain relinquished sovereignty over Cuba, but did not cede it to the U.S.

The United States and Puerto Rico began a long-standing metropolis-colony relationship. In the early 20th century, Puerto Rico was ruled by the military, with officials including the governor appointed by the President of the United States. The Foraker Act of 1900 gave Puerto Rico a certain amount of civilian popular government, including a popularly elected House of Representatives. The upper house and governor were appointed by the United States.

Its judicial system was constructed to follow the American legal system; a Puerto Rico Supreme Court and a United State District Court for the territory were established. It was authorized a non-voting member of Congress, by the title of "Resident Commissioner", who was appointed. In addition, this Act extended all U.S. laws "not locally inapplicable" to Puerto Rico, specifying, in particular, exemption from U.S. Internal Revenue laws.

The Act empowered the civil government to legislate on "all matters of legislative character not locally inapplicable", including the power to modify and repeal any laws then in existence in Puerto Rico, though the U.S. Congress retained the power to annul acts of the Puerto Rico legislature. During an address to the Puerto Rican legislature in 1906, President Theodore Roosevelt recommended that Puerto Ricans become U.S. citizens.

In 1914, the Puerto Rican House of Delegates voted unanimously in favor of independence from the United States, but this was rejected by the U.S. Congress as "unconstitutional", and in violation of the 1900 Foraker Act.

In 1917, the U.S. Congress passed the Jones–Shafroth Act, popularly called the Jones Act, which granted Puerto Ricans, born on or after, April 25, 1898, U.S. citizenship. Opponents, which included all of the Puerto Rican House of Delegates, who voted unanimously against it, said that the U.S. imposed citizenship in order to draft Puerto Rican men into the army as American entry into World War I became likely.

The same Act provided for a popularly elected Senate to complete a bicameral Legislative Assembly, as well as a bill of rights. It authorized the popular election of the Resident Commissioner to a four-year term.
Natural disasters, including a major earthquake and tsunami in 1918, and several hurricanes, and the Great Depression impoverished the island during the first few decades under U.S. rule. Some political leaders, such as Pedro Albizu Campos, who led the Puerto Rican Nationalist Party, demanded change in relations with the United States. He organized a protest at the University of Puerto Rico in 1935, in which four were killed by police.

In 1936, U.S. Senator Millard Tydings introduced a bill supporting independence for Puerto Rico, but it was opposed by Luis Muñoz Marín of the Liberal Party of Puerto Rico. (Tydings had co-sponsored the Tydings–McDuffie Act, which provided independence to the Philippines after a 10-year transition under a limited autonomy.) All the Puerto Rican parties supported the bill, but Muñoz Marín opposed it. Tydings did not gain passage of the bill.

In 1937, Albizu Campos' party organized a protest in which numerous people were killed by police in Ponce. The Insular Police, resembling the National Guard, opened fire upon unarmed cadets and bystanders alike. The attack on unarmed protesters was reported by the U.S. Congressman Vito Marcantonio and confirmed by the report of the Hays Commission, which investigated the events. The commission was led by Arthur Garfield Hays, counsel to the American Civil Liberties Union.

Nineteen people were killed and over 200 were badly wounded, many in their backs while running away. The Hays Commission declared it a massacre and police mob action, and it has since been known as the Ponce massacre. In the aftermath, on April 2, 1943, Tydings introduced a bill in Congress calling for independence for Puerto Rico. This bill ultimately was defeated.

During the latter years of the Roosevelt–Truman administrations, the internal governance was changed in a compromise reached with Luis Muñoz Marín and other Puerto Rican leaders. In 1946, President Truman appointed the first Puerto Rican-born governor, Jesús T. Piñero.

Since 2007, the Puerto Rico State Department has developed a protocol to issue certificates of Puerto Rican citizenship to Puerto Ricans. In order to be eligible, applicants must have been born in Puerto Rico; born outside of Puerto Rico to a Puerto Rican–born parent; or be an American citizen with at least one year of residence in Puerto Rico.

In 1947, the U.S. granted Puerto Ricans the right to democratically elect their own governor. In 1948, Luis Muñoz Marín became the first popularly elected governor of Puerto Rico.
A bill was introduced before the Puerto Rican Senate which would restrain the rights of the independence and nationalist movements in the island. The Senate at the time was controlled by the Popular Democratic Party (PPD), and was presided over by Luis Muñoz Marín. The bill, also known as the Gag Law (), was approved by the legislature on May 21, 1948. It made it illegal to display a Puerto Rican flag, to sing a pro-independence tune, to talk of independence, or to campaign for independence.

The bill, which resembled the Smith Act passed in the United States, was signed and made into law on June 10, 1948, by the U.S. appointed governor of Puerto Rico, Jesús T. Piñero, and became known as "Law 53" ().

In accordance with this law, it would be a crime to print, publish, sell, exhibit, organize or help anyone organize any society, group or assembly of people whose intentions are to paralyze or destroy the insular government. Anyone accused and found guilty of disobeying the law could be sentenced to ten years of prison, be fined $10,000 (U.S.), or both. According to Dr. Leopoldo Figueroa, a member of the Puerto Rico House of Representatives, the law was repressive and in violation of the First Amendment of the U.S. Constitution, which guarantees Freedom of Speech. He asserted that the law as such was a violation of the civil rights of the people of Puerto Rico. The law was repealed in 1957.

In 1950, the U.S. Congress granted Puerto Ricans the right to organize a constitutional convention via a referendum that gave them the option of voting their preference, "yes" or "no", on a proposed U.S. law that would organize Puerto Rico as a "commonwealth" that would continue United States sovereignty over Puerto Rico and its people. Puerto Rico's electorate expressed its support for this measure in 1951 with a second referendum to ratify the constitution. The Constitution of Puerto Rico was formally adopted on July 3, 1952. The Constitutional Convention specified the name by which the body politic would be known.

On February 4, 1952, the convention approved Resolution 22 which chose in English the word "Commonwealth", meaning a "politically organized community" or "state", which is simultaneously connected by a compact or treaty to another political system. Puerto Rico officially designates itself with the term "Commonwealth of Puerto Rico" in its constitution, as a translation into English of the term to "Estado Libre Asociado" (ELA).

In 1967 Puerto Rico's Legislative Assembly polled the political preferences of the Puerto Rican electorate by passing a plebiscite act that provided for a vote on the status of Puerto Rico. This constituted the first plebiscite by the Legislature for a choice among three status options (commonwealth, statehood, and independence). In subsequent plebiscites organized by Puerto Rico held in 1993 and 1998 (without any formal commitment on the part of the U.S. Government to honor the results), the current political status failed to receive majority support. In 1993, Commonwealth status won by a plurality of votes (48.6% versus 46.3% for statehood), while the "none of the above" option, which was the Popular Democratic Party-sponsored choice, won in 1998 with 50.3% of the votes (versus 46.5% for statehood). Disputes arose as to the definition of each of the ballot alternatives, and Commonwealth advocates, among others, reportedly urged a vote for "none of the above".
In 1950, the U.S. Congress approved Public Law 600 (P.L. 81-600), which allowed for a democratic referendum in Puerto Rico to determine whether Puerto Ricans desired to draft their own local constitution. This Act was meant to be adopted in the "nature of a compact". It required congressional approval of the Puerto Rico Constitution before it could go into effect, and repealed certain sections of the Organic Act of 1917. The sections of this statute left in force were entitled the "Puerto Rican Federal Relations Act". U.S. Secretary of the Interior Oscar L. Chapman, under whose Department resided responsibility of Puerto Rican affairs, clarified the new commonwealth status in this manner:

On October 30, 1950, Pedro Albizu Campos and other nationalists led a three-day revolt against the United States in various cities and towns of Puerto Rico, in what is known as the Puerto Rican Nationalist Party Revolts of the 1950s. The most notable occurred in Jayuya and Utuado. In the Jayuya revolt, known as the Jayuya Uprising, the Puerto Rican governor declared martial law, and attacked the insurgents in Jayuya with infantry, artillery and bombers under control of the Puerto Rican commander. The Utuado uprising culminated in what is known as the Utuado massacre.

On November 1, 1950, Puerto Rican nationalists from New York City, Griselio Torresola and Oscar Collazo, attempted to assassinate President Harry S. Truman at his temporary residence of Blair House. Torresola was killed during the attack, but Collazo was wounded and captured. He was convicted of murder and sentenced to death, but President Truman commuted his sentence to life. After Collazo served 29 years in a federal prison, President Jimmy Carter commuted his sentence to times served and he was released in 1979.

Pedro Albizu Campos served many years in a federal prison in Atlanta, for seditious conspiracy to overthrow the U.S. government in Puerto Rico.

The Constitution of Puerto Rico was approved by a Constitutional Convention on February 6, 1952, and 82% of the voters in a March referendum. It was modified and ratified by the U.S. Congress, approved by President Truman on July 3 of that year, and proclaimed by Gov. Muñoz Marín on July 25, 1952. This was the anniversary of July 25, 1898, landing of U.S. troops in the Puerto Rican Campaign of the Spanish–American War, until then celebrated as an annual Puerto Rico holiday.

Puerto Rico adopted the name of "Estado Libre Asociado de Puerto Rico" (literally "Associated Free State of Puerto Rico"), officially translated into English as Commonwealth, for its body politic. "The United States Congress legislates over many fundamental aspects of Puerto Rican life, including citizenship, the currency, the postal service, foreign policy, military defense, communications, labor relations, the environment, commerce, finance, health and welfare, and many others."

During the 1950s and 1960s, Puerto Rico experienced rapid industrialization, due in large part to "Operación Manos a la Obra" ("Operation Bootstrap"), an offshoot of FDR's New Deal. It was intended to transform Puerto Rico's economy from agriculture-based to manufacturing-based to provide more jobs. Puerto Rico has become a major tourist destination, as well as a global center for pharmaceutical manufacturing.

Four referendums have been held since the late 20th century to resolve the political status. The 2012 referendum showed a majority (54% of the voters) in favor of a change in status, with full statehood the preferred option of those who wanted a change.
Because there were almost 500,000 blank ballots in the 2012 referendum, creating confusion as to the voters' true desire, Congress decided to ignore the vote.

The first three plebiscites provided voters with three options: statehood, free association, and independence. The Puerto Rican status referendum, 2017 in June 2017 was going to offer only two options: Statehood and Independence/Free Association. However, a letter from the Donald Trump administration recommended adding the Commonwealth, the current status, in the plebiscite. The option had been removed from this plebiscite in response to the results of the plebiscite in 2012 which asked whether to remain in the current status and No had won. The Trump administration cited changes in demographics during the past 5 years to add the option once again. Amendments to the plebiscite bill were adopted making ballot wording changes requested by the Department of Justice, as well as adding a "current territorial status" option. While 97 percent voted in favor of statehood, the turnout was low; only some 23 percent voted. After the ballots were counted the Justice Department was non-committal. The Justice Department had asked for the 2017 plebiscite to be postponed but the Rosselló government chose not to do so. After the outcome was announced, the department told the Associated Press that it had "not reviewed or approved the ballot’s language".

Former Governor Aníbal Acevedo Vilá (2005–2009) is convinced that statehood is not the solution for either the U.S. or for Puerto Rico "for economic, identity and cultural reasons". He pointed out that voter turnout for the 2017 referendum was extremely low, and suggests that a different type of mutually-beneficial relationship should be found.

If the federal government agrees to discuss an association agreement, the conditions would be negotiated between the two entities. The agreement might cover topics such as the role of the U.S. military in Puerto Rico, the use of the U.S. currency, free trade between the two entities, and whether Puerto Ricans would be U.S. citizens.

The three current Free Associated States (Marshall Islands, Micronesia and Palau) use the American dollar, receive some financial support and the promise of military defense if they refuse military access to any other country. Their citizens are allowed to work in the U.S. and serve in its military.

Governor Ricardo Rosselló is strongly in favor of statehood to help develop the economy and help to "solve our 500-year-old colonial dilemma ... Colonialism is not an option ... It’s a civil rights issue ... 3.5 million citizens seeking an absolute democracy," he told the news media. Benefits of statehood include an additional $10 billion per year in federal funds, the right to vote in presidential elections, higher Social Security and Medicare benefits, and a right for its government agencies and municipalities to file for bankruptcy. The latter is currently prohibited.

Statehood might be useful as a means of dealing with the financial crisis, since it would allow for bankruptcy and the relevant protection. According to the Government Development Bank, this might be the only solution to the debt crisis. Congress has the power to vote to allow Chapter 9 protection without the need for statehood, but in late 2015 there was very little support in the House for this concept. Other benefits to statehood include increased disability benefits and Medicaid funding, the right to vote in Presidential elections and the higher (federal) minimum wage.

Subsequent to the 2017 referendum, Puerto Rico's legislators are also expected to vote on a bill that would allow the Governor to draft a state constitution and hold elections to choose senators and representatives to the federal Congress. In spite of the outcome of the referendum, and the so-called Tennessee Plan (above), action by the United States Congress would be necessary to implement changes to the status of Puerto Rico under the Territorial Clause of the United States Constitution.

Since 1953, the UN has been considering the political status of Puerto Rico and how to assist it in achieving "independence" or "decolonization". In 1978, the Special Committee determined that a "colonial relationship" existed between the U.S. and Puerto Rico.

The UN's Special Committee on Decolonization has often referred to Puerto Rico as a "nation" in its reports, because, internationally, the people of Puerto Rico are often considered to be a Caribbean nation with their own national identity. Most recently, in a June 2016 report, the Special Committee called for the United States to expedite the process to allow self-determination in Puerto Rico. More specifically, the group called on the United States to expedite a process that would allow the people of Puerto Rico to exercise fully their right to self-determination and independence. ... allow the Puerto Rican people to take decisions in a sovereign manner, and to address their urgent economic and social needs, including unemployment, marginalization, insolvency and poverty".

On November 27, 1953, shortly after the establishment of the Commonwealth, the General Assembly of the United Nations approved Resolution 748, removing Puerto Rico's classification as a non-self-governing territory. The General Assembly did not apply the full list of criteria which was enunciated in 1960 when it took favorable note of the cessation of transmission of information regarding the non-self-governing status of Puerto Rico.

According to the White House Task Force on Puerto Rico's Political Status in its December 21, 2007 report, the U.S., in its written submission to the UN in 1953, never represented that Congress could not change its relationship with Puerto Rico without the territory's consent. It stated that the U.S. Justice Department in 1959 reiterated that Congress held power over Puerto Rico pursuant to the Territorial Clause of the U.S. Constitution.

In 1993 the United States Court of Appeals for the Eleventh Circuit stated that Congress may unilaterally repeal the Puerto Rican Constitution or the Puerto Rican Federal Relations Act and replace them with any rules or regulations of its choice. In a 1996 report on a Puerto Rico status political bill, the U.S. House Committee on Resources stated, "Puerto Rico's current status does not meet the criteria for any of the options for full self-government under Resolution 1541" (the three established forms of full self-government being stated in the report as (1) national independence, (2) free association based on separate sovereignty, or (3) full integration with another nation on the basis of equality). The report concluded that Puerto Rico "... remains an unincorporated colony and does not have the status of 'free association' with the United States as that status is defined under United States law or international practice", that the establishment of local self-government with the consent of the people can be unilaterally revoked by the U.S. Congress, and that U.S. Congress can also withdraw the U.S. citizenship of Puerto Rican residents of Puerto Rico at any time, for a legitimate Federal purpose. The application of the U.S. Constitution to Puerto Rico is limited by the Insular Cases.

In 2006, 2007, 2009, 2010, and 2011 the United Nations Special Committee on Decolonization passed resolutions calling on the United States to expedite a process "that would allow Puerto Ricans to fully exercise their inalienable right to self-determination and independence", and to release all Puerto Rican political prisoners in U.S. prisons, to clean up, decontaminate and return the lands in the islands of Vieques and Culebra to the people of Puerto Rico, to perform a probe into U.S. human rights violations on the island and a probe into the killing by the FBI of pro-independence leader Filiberto Ojeda Rios.

On July 15, 2009, the United Nations Special Committee on Decolonization approved a draft resolution calling on the Government of the United States to expedite a process that would allow the Puerto Rican people to exercise fully their inalienable right to self-determination and independence.

On April 29, 2010, the U.S. House voted 223–169 to approve a measure for a federally sanctioned process for Puerto Rico's self-determination, allowing Puerto Rico to set a new referendum on whether to continue its present form of commonwealth, or to have a different political status. If Puerto Ricans voted to continue as a commonwealth, the Government of Puerto Rico was authorized to conduct additional plebiscites at intervals of every eight years from the date on which the results of the prior plebiscite were certified; if Puerto Ricans voted to have a different political status, a second referendum would determine whether Puerto Rico would become a U.S. state, an independent country, or a sovereign nation associated with the U.S. that would not be subject to the Territorial Clause of the United States Constitution. During the House debate, a fourth option, to retain its present form of commonwealth (sometimes referred to as "the status quo") political status, was added as an option in the second plebiscite.

Immediately following U.S. House passage, H.R. 2499 was sent to the U.S. Senate, where it was given two formal readings and referred to the Senate Committee on Energy and Natural Resources. On December 22, 2010, the 111th United States Congress adjourned without any Senate vote on H.R.2499, killing the bill.

The latest Task Force report was released on March 11, 2011. The report suggested a two-plebiscite process, including a "first plebiscite that requires the people of Puerto Rico to choose whether they wish to be part of the United States (either via Statehood or Commonwealth) or wish to be independent (via Independence or Free Association). If continuing to be part of the United States were chosen in the first plebiscite, a second vote would be taken between Statehood and Commonwealth."

On June 14, 2011, President Barack Obama "promised to support 'a clear decision' by the people of Puerto Rico on statehood". That same month, the United Nations Special Committee on Decolonization passed a resolution and adopted a consensus text introduced by Cuba's delegate on June 20, 2011, calling on the United States to expedite a process "that would allow Puerto Ricans to fully exercise their inalienable right to self-determination and independence".

On November 6, 2012, a two-question referendum took place, simultaneous with the general elections. The first question asked voters whether they wanted to maintain the current status under the territorial clause of the U.S. Constitution. The second question posed three alternate status options if the first question was approved: statehood, independence or free association. For the first question, 54 percent voted against the current Commonwealth status. For the second question, 61.16% voted for statehood, 33.34% for a sovereign free associated state, and 5.49% for independence.

There were also 515,348 blank and invalidated ballots, which are not reflected in the final tally, as they are not considered cast votes under Puerto Rico law. On December 11, 2012, Puerto Rico's Legislature passed a concurrent resolution to request to the President and the U.S. Congress action on November 6, 2012 plebiscite results. But on April 10, 2013, with the issue still being widely debated, the White House announced that it will seek $2.5 million to hold another referendum, this next one being the first Puerto Rican status referendum to be financed by the U.S. Federal government.

In December 2015, the U.S. Government submitted a brief as Amicus Curiae to the U.S. Supreme Court related to the case Commonwealth of Puerto Rico v. Sanchez Valle. The U.S. Government official position is that the U.S. Constitution does not contemplate "sovereign territories". That the Court has consistently recognized that "there is no sovereignty in a Territory of the United States but that of the United States itself". and a U.S. territory has "no independent sovereignty comparable to that of a state. That is because "the Government of a territory owes its existence wholly to the United States". Congress's plenary authority over federal territories includes the authority to permit self-government, whereby local officials administer a territory's internal affairs.

On June 9, 2016, in Commonwealth of Puerto Rico vs Sanchez Valle, a 6–2 majority of the Supreme Court of the United States determined that Puerto Rico is a territory and lacks Sovereignty.

On June 30, 2016, the President of the United States of America signed a new law approved by U.S. Congress, H.R. 5278: PROMESA, establishing a Control Board over the Puerto Rico Government. This board will have a significant degree of federal control involved in its establishment and operations. In particular, the authority to establish the control board derives from the federal government's constitutional power to "make all needful rules and regulations" regarding U.S. territories; The President would appoint all seven voting members of the board; and the board would have broad sovereign powers to effectively overrule decisions by Puerto Rico's legislature, governor, and other public authorities.
The latest referendum on statehood, independence, or an associated republic was held on November 6, 2012. The people of Puerto Rico made history by requesting, for the first time ever, the conclusion of the island's current territorial status. Almost 78% of registered voters participated in a plebiscite held to resolve Puerto Rico's status, and a slim but clear majority (54%) disagreed with Puerto Rico maintaining its present territorial status. Furthermore, among the possible alternatives, sixty-one percent (61%) of voters chose the statehood option, while one third of the ballots were submitted blank.

On December 11, 2012, the Legislative Assembly of Puerto Rico enacted a concurrent resolution requesting the President and the Congress of the United States to respond to the referendum of the people of Puerto Rico, held on November 6, 2012, to end its current form of territorial status and to begin the process to admit Puerto Rico as a State. The initiative has not made Puerto Rico into a state.

In late September 2017, Hurricane Maria hit the island as a Category 4 storm causing severe damage to homes, other buildings and infrastructure. The recovery as of late November was slow but progress had been made. Electricity was restored to two-thirds of the island, although there was some doubt as to the number of residents getting reliable power. In January 2018, it was reported that close to 40 percent of the island's customers still did not have electricity.
The vast majority had access to water but were still required to boil it. The number still living in shelters had dropped to 982 with thousands of others living with relatives. The official death toll at the time was 58 but some sources indicated that the actual number is much higher. A dam on the island was close to failure and officials were concerned about additional flooding from this source.

Thousands had left Puerto Rico, with close to 200,000 having arrived in Florida alone. Those who were then living on the mainland experienced difficulty in getting health care benefits. A "The New York Times" report on November 27 said it was understandable that Puerto Ricans wanted to leave the island. "Basic essentials are hard to find and electricity and other utilities are unreliable or entirely inaccessible. Much of the population has been unable to return to jobs or to school and access to health care has been severely limited." The Center for Puerto Rican Studies at New York's Hunter College estimated that some half million people, about 14% of the population, may permanently leave by 2019.

The total damage on the island was estimated as up to $95 billion. By the end of November, FEMA had received over a million applications for aid and had approved about a quarter of those. The US government had agreed in October to provide funding to rebuild and up to $4.9 billion in loans to help the island's government. FEMA had $464 million earmarked to help local governments rebuild public buildings and infrastructure. Bills for other funding were being considered in Washington but little progress had been made on those.

A November 28, 2017 report by the Sierra Club included this comment: "It will take years to rebuild Puerto Rico, not just from the worst hurricane to make landfall since 1932, but to sustainably overcome environmental injustices which made Maria's devastation even more catastrophic".

In May, 2017, the Natural Resources Defense Council reported that Puerto Rico's water system was the worst as measured by the Clean Water Act. 70% of the population drank water that violated U.S. law.

A tourism web site report in March 2018 indicated that all airports were operating, although Luis Munoz Marin Airport would not be back to handling the full number of flights until July 2018. Some 90% of the island was receiving electricity, although the power structure would require another $17.6 billion for full rebuilding, according to the United States Department of Energy.
Nearly all residents had access to telecommunications service and running water. All hospitals were operating. Some 83% of hotel rooms were available for use and the cruise ship port was receiving ships; 58 arrived in San Juan in February. The island was encouraging operators to increase the number of tourists.

Reports in April 2018 stated that Puerto Rico will receive $18.5 billion from the United States Department of Housing and Urban Development to help rebuild homes and infrastructure. This was substantially less than the $46 billion requested by the governor. However, the island was expecting to receive approximately $50 billion for disaster relief over the subsequent six years mostly via the Federal Emergency Management Agency. At that time, nearly 2800 families were living in FEMA-sponsored short-term housing across 34 states and Puerto Rico. Nearly half of the schools are operating at only 60% classroom capacity. Over 280 public schools would not reopen in the fall; 827 were expected to be operational. Almost 40,000 students left the island's schools since May 2017; some of these were in schools in the mainland U.S. (Before the hurricanes, Puerto Rico had planned to close 179 schools due to inadequate numbers of students.)

The official number of Hurricane Maria-related deaths as reported by the government of Puerto Rico is 64. The Commonwealth has commissioned George Washington University to assess the death toll and is awaiting that report. An academic study based on household surveys and reported in the New England Journal of Medicine estimated that the number of hurricane-related deaths during the period September 20, 2017 to December 31, 2017 was around 4,600 (range 793-8,498)

Puerto Rico consists of the main island of Puerto Rico and various smaller islands, including Vieques, Culebra, Mona, Desecheo, and Caja de Muertos. Of these five, only Culebra and Vieques are inhabited year-round. Mona, which has played a key role in maritime history, is uninhabited most of the year except for employees of the Puerto Rico Department of Natural Resources. There are many other even smaller islets, like Monito, which is near to Mona, Isla de Cabras and La Isleta de San Juan, both located on the San Juan Bay. The latter is the only inhabited islet with communities like Old San Juan and Puerta de Tierra, and connected to the main island by bridges.

The Commonwealth of Puerto Rico has an area of , of which is land and is water. Puerto Rico is larger than two U.S. states, Delaware and Rhode Island. The maximum length of the main island from east to west is , and the maximum width from north to south is . Puerto Rico is the smallest of the Greater Antilles. It is 80% of the size of Jamaica, just over 18% of the size of Hispaniola and 8% of the size of Cuba, the largest of the Greater Antilles.

The island is mostly mountainous with large coastal areas in the north and south. The main mountain range is called "La Cordillera Central" (The Central Range). The highest elevation in Puerto Rico, Cerro de Punta , is located in this range.

Another important peak is El Yunque, one of the highest in the "Sierra de Luquillo" at the El Yunque National Forest, with an elevation of .
Puerto Rico has 17 lakes, all man-made, and more than 50 rivers, most originating in the Cordillera Central. Rivers in the northern region of the island are typically longer and of higher water flow rates than those of the south, since the south receives less rain than the central and northern regions.

Puerto Rico is composed of Cretaceous to Eocene volcanic and plutonic rocks, overlain by younger Oligocene and more recent carbonates and other sedimentary rocks. Most of the caverns and karst topography on the island occurs in the northern region in the carbonates. The oldest rocks are approximately years old (Jurassic) and are located at Sierra Bermeja in the southwest part of the island. They may represent part of the oceanic crust and are believed to come from the Pacific Ocean realm.

Puerto Rico lies at the boundary between the Caribbean and North American plates and is being deformed by the tectonic stresses caused by their interaction. These stresses may cause earthquakes and tsunamis. These seismic events, along with landslides, represent some of the most dangerous geologic hazards in the island and in the northeastern Caribbean.

The most recent major earthquake occurred on , 1918, and had an estimated magnitude of 7.5 on the Richter scale. It originated off the coast of Aguadilla, several kilometers off the northern coast, and was accompanied by a tsunami. It caused extensive property damage and widespread losses, damaging infrastructure, especially bridges. It resulted in an estimated 116 deaths and $4 million in property damage. The failure of the government to move rapidly to provide for the general welfare contributed to political activism by opponents and eventually to the rise of the Puerto Rican Nationalist Party.

The Puerto Rico Trench, the largest and deepest trench in the Atlantic, is located about north of Puerto Rico at the boundary between the Caribbean and North American plates. It is long. At its deepest point, named the Milwaukee Deep, it is almost deep.

The climate of Puerto Rico in the Köppen climate classification is tropical rainforest. Temperatures are warm to hot year round, averaging near 85 °F (29 °C) in lower elevations and 70 °F (21 °C) in the mountains. Easterly trade winds pass across the island year round. Puerto Rico has a rainy season which stretches from April into November. The mountains of the Cordillera Central are the main cause of the variations in the temperature and rainfall that occur over very short distances. The mountains can also cause wide variation in local wind speed and direction due to their sheltering and channeling effects adding to the climatic variation.

The island has an average temperature of throughout the year, with an average minimum temperature of and maximum of . Daily temperature changes seasonally are quite small in the lowlands and coastal areas. The temperature in the south is usually a few degrees higher than the north and temperatures in the central interior mountains are always cooler than those on the rest of the island.

Between the dry and wet season, there is a temperature change of around . This is mainly due to the warm waters of the tropical Atlantic Ocean, which significantly modify cooler air moving in from the north and northwest. Coastal waters temperatures around the years are about in February to in August. The highest temperature ever recorded was at Arecibo, while the lowest temperature ever recorded was in the mountains at Adjuntas, Aibonito, and Corozal. The average yearly precipitation is .

Puerto Rico experiences the Atlantic hurricane season, similar to the remainder of the Caribbean Sea and North Atlantic oceans. On average, a quarter of its annual rainfall is contributed from tropical cyclones, which are more prevalent during periods of La Niña than El Niño. A cyclone of tropical storm strength passes near Puerto Rico, on average, every five years. A hurricane passes in the vicinity of the island, on average, every seven years. Since 1851, the Lake Okeechobee Hurricane of September 1928 is the only hurricane to make landfall as a Category 5 hurricane.

In the busy 2017 Atlantic hurricane season, Puerto Rico avoided a direct hit by the Category 5 Hurricane Irma on September 8, 2017, but high winds caused a loss of electrical power to some one million residents. Almost 50% of hospitals were operating with power provided by generators. The Category 4 Hurricane Jose, as expected, veered away from Puerto Rico. A short time later, the devastating Hurricane Maria made landfall in Puerto Rico as a Category 4 hurricane, with sustained winds, powerful rains and widespread flooding causing tremendous destruction, including the electrical grid, which could remain out for 4–6 months. With such widespread destruction and a great need for supplies – everything from drinking water, food, medicine and personal care items to fuel for generators and construction materials for rebuilding the island – Gov. Rossello and several Congressmen called on the federal government to waive the WWI-era Jones Act (protectionist provisions: ships made and owned in U.S., and with U.S. crews), which essentially double Puerto Rico's cost for shipped goods relative to neighboring islands. On September 28, U.S. President Donald Trump waived the Act for ten days.

Species endemic to the archipelago number 239 plants, 16 birds and 39 amphibians/reptiles, recognized as of 1998. Most of these (234, 12 and 33 respectively) are found on the main island. The most recognizable endemic species and a symbol of Puerto Rican pride is the "coquí", a small frog easily identified by the sound of its call, from which it gets its name. Most "coquí" species (13 of 17) live in the El Yunque National Forest, a tropical rainforest in the northeast of the island previously known as the Caribbean National Forest. El Yunque is home to more than 240 plants, 26 of which are endemic to the island. It is also home to 50 bird species, including the critically endangered Puerto Rican amazon.

Across the island in the southwest, the of dry land at the Guánica Commonwealth Forest Reserve contain over 600 uncommon species of plants and animals, including 48 endangered species and 16 endemic to Puerto Rico.

The population of Puerto Rico has been shaped by Amerindian settlement, European colonization, slavery, economic migration, and Puerto Rico's status as unincorporated territory of the United States.

The estimated population of Puerto Rico as of July 1, 2015, was 3,474,182, a 6.75% decrease since the 2010 United States Census. From 2000 to 2010, the population decreased, the first such decrease in census history for Puerto Rico. It went from the 3,808,610 residents registered in the 2000 Census to 3,725,789 in the 2010 Census.

A declining and aging population presents additional problems for the society. The U.S. Census Bureau's estimate for July 1, 2016 was 3,411,307 people, down substantially from the 2010 data which had indicated 3,725,789 people.

Continuous European immigration and high natural increase helped the population of Puerto Rico grow from 155,426 in 1800, to almost a million by the close of the 19th century.

A census conducted by royal decree on September 30, 1858, gave the following totals of the Puerto Rican population at that time: 341,015 were Free colored; 300,430 identified as Whites; and 41,736 were slaves.

During the 19th century hundreds of families arrived in Puerto Rico, primarily from the Canary Islands and Andalusia, but also from other parts of Spain such as Catalonia, Asturias, Galicia and the Balearic Islands and numerous Spanish loyalists from Spain's former colonies in South America. Settlers from outside Spain also arrived in the islands, including from Corsica, France, Lebanon, China, Portugal, Ireland, Scotland, Germany and Italy. This immigration from non-Hispanic countries was the result of the "Real Cedula de Gracias de 1815" ("Royal Decree of Graces of 1815"), which allowed European Catholics to settle in the island with land allotments in the interior of the island, provided they paid taxes and continued to support the Catholic Church.
Between 1960 and 1990 the census questionnaire in Puerto Rico did not ask about race or ethnicity. The 2000 United States Census included a racial self-identification question in Puerto Rico. According to the census, most Puerto Ricans identified as White and Hispanic; few identified as Black or some other race.

A recent population genetics study conducted in Puerto Rico suggests that between 52.6% and 84% of the population possess some degree of Amerindian mitochondrial DNA (mtDNA) in their maternal ancestry, usually in a combination with other ancestries such as aboriginal Guanche North-West African ancestry brought by Spanish settlers from the Canary Islands.

In addition, these DNA studies show Amerindian ancestry in addition to the Taíno.

One genetic study on the racial makeup of Puerto Ricans (including all races) found them to be roughly around 61% West Eurasian/North African (overwhelmingly of Spanish provenance), 27% Sub-Saharan African and 11% Native American. Another genetic study from 2007, claimed that "the average genomewide individual (ie. Puerto Rican) ancestry proportions have been estimated as 66%, 18%, and 16%, for European, West African, and Native American, respectively." Other study estimates 63.7% European, 21.2% (Sub-Saharan) African, and 15.2% Native American; European ancestry is more prevalent in the West and in Central Puerto Rico, African in Eastern Puerto Rico, and Native American in Northern Puerto Rico.

A Pew Research survey indicated a literacy rate of 90.4% (adult population) in 2012 based on data from the United Nations and a life expectancy of 79.3 years.

Puerto Rico has recently become the permanent home of over 100,000 legal residents. The vast majority of recent immigrants, both legal and illegal, come from the Dominican Republic and Haiti. Other sources sending in significant numbers of recent immigrants include Cuba, Mexico, Colombia, Panama, Jamaica, Venezuela, Spain, and Nigeria. Also, there are many non-Puerto Rican U.S. citizens settling in Puerto Rico, from the mainland United States and the U.S. Virgin Islands, as well as Nuyoricans (stateside Puerto Ricans) coming back to Puerto Rico. Most recent immigrants settle in and around San Juan.

Emigration is a major part of contemporary Puerto Rican history. Starting soon after World War II, poverty, cheap airfares, and promotion by the island government caused waves of Puerto Ricans to move to the United States, particularly to the Northeastern states, and Florida. This trend continued even as Puerto Rico's economy improved and its birth rate declined. Puerto Ricans continue to follow a pattern of "circular migration", with some migrants returning to the island. In recent years, the population has declined markedly, falling nearly 1% in 2012 and an additional 1% (36,000 people) in 2013 due to a falling birthrate and emigration.

According to the 2010 Census, the number of Puerto Ricans living in the United States outside of Puerto Rico far exceeds those living in Puerto Rico. Emigration exceeds immigration. As those who leave tend to be better educated than those that remain, this accentuates the drain on Puerto Rico’s economy.

Based on the July 1, 2016 estimate by the U.S. Census Bureau, the population of the Commonwealth had declined by 314,482 people since the 2010 Census data had been tabulated.

The most populous city is the capital, San Juan, with approximately 371,400 people based on a 2015 estimate by the Census Bureau. Other major cities include Bayamón, Carolina, Ponce, and Caguas. Of the ten most populous cities on the island, eight are located within what is considered San Juan's metropolitan area, while the other two are located in the south (Ponce) and west (Mayagüez) of the island.

The official languages of the executive branch of government of Puerto Rico are Spanish and English, with Spanish being the primary language. Spanish is, and has been, the only official language of the entire Commonwealth judiciary system, despite a 1902 English-only language law. However, all official business of the U.S. District Court for the District of Puerto Rico is conducted in English. English is the primary language of less than 10% of the population. Spanish is the dominant language of business, education and daily life on the island, spoken by nearly 95% of the population.

The U.S. Census Bureau's 2015 update provides the following facts: 94.1% of adults speak Spanish, 5.8% speak only English,
78.3% do not speak English "very well".

In Puerto Rico, public school instruction is conducted almost entirely in Spanish. There have been pilot programs in about a dozen of the over 1,400 public schools aimed at conducting instruction in English only. Objections from teaching staff are common, perhaps because many of them are not fully fluent in English. English is taught as a second language and is a compulsory subject from elementary levels to high school. The languages of the deaf community are American Sign Language and its local variant, Puerto Rican Sign Language.

The Spanish of Puerto Rico has evolved into having many idiosyncrasies in vocabulary and syntax that differentiate it from the Spanish spoken elsewhere. As a product of Puerto Rican history, the island possesses a unique Spanish dialect. Puerto Rican Spanish utilizes many Taíno words, as well as English words. The largest influence on the Spanish spoken in Puerto Rico is that of the Canary Islands. Taíno loanwords are most often used in the context of vegetation, natural phenomena, and native musical instruments. Similarly, words attributed to primarily West African languages were adopted in the contexts of foods, music, and dances, particularly in coastal towns with concentrations of descendants of Sub-Saharan Africans.

The Roman Catholic Church was brought by Spanish colonists and gradually became the dominant religion in Puerto Rico. The first dioceses in the Americas, including that of Puerto Rico, were authorized by Pope Julius II in 1511. One Pope, John Paul II, visited Puerto Rico in October 1984. All municipalities in Puerto Rico have at least one Catholic church, most of which are located at the town center, or plaza. African slaves brought and maintained various ethnic African religious practices associated with different peoples; in particular, the Yoruba beliefs of Santería and/or Ifá, and the Kongo-derived Palo Mayombe. Some aspects were absorbed into syncretic Christianity.

Protestantism, which was suppressed under the Spanish Catholic regime, has slightly reemerged under United States rule, making contemporary Puerto Rico more interconfessional than in previous centuries, although Catholicism continues to be the dominant religion. The first Protestant church, Iglesia de la Santísima Trinidad, was established in Ponce by the Anglican Diocese of Antigua in 1872. It was the first non-Roman Catholic Church in the entire Spanish Empire in the Americas.

Pollster Pablo Ramos stated in 1998 that the population was 38% Roman Catholic, 28% Pentecostal, and 18% were members of independent churches, which would give a Protestant percentage of 46% if the last two populations are combined. Protestants collectively added up to almost two million people. Another researcher gave a more conservative assessment of the proportion of Protestants:
Puerto Rico, by virtue of its long political association with the United States, is the most Protestant of Latin American countries, with a Protestant population of approximately 33 to 38 percent, the majority of whom are Pentecostal. David Stoll calculates that if we extrapolate the growth rates of evangelical churches from 1960–1985 for another twenty-five years Puerto Rico will become 75 percent evangelical. (Ana Adams: "Brincando el Charco..." in "Power, Politics and Pentecostals in Latin America", Edward Cleary, ed., 1997. p. 164).

The data provided for 2014 by Pew Research Center, is summarized in the chart to the right. An Associated Press article in March 2014 stated that "more than 70 percent of whom identify themselves as Catholic" but provided no source for this information.

The CIA World Factbook reports that 85% of the population of Puerto Rico identifies as Roman Catholic, while 15% identify as Protestant and Other. Neither a date or a source for that information is provided and may not be recent. A 2013 Pew Research survey found that only about 45% of Puerto Rican adults identified themselves as Catholic, 29% as Protestant and 20% as unaffiliated with a religion. The people surveyed by Pew consisted of Puerto Ricans living in the 50 states and DC and may not be indicative of those living in the Commonwealth.

By 2014, a Pew Research report, with the sub-title "Widespread Change in a Historically Catholic Region", indicated that only 56% of Puerto Ricans were Catholic and that 33% were Protestant; this survey was completed between October 2013 and February 2014.

An Eastern Orthodox community, the Dormition of the Most Holy Theotokos / St. Spyridon's Church is located in Trujillo Alto, and serves the small Orthodox community. This affiliation accounted for under 1% of the population in 2010 according to the Pew Research report. In 1940, Juanita García Peraza founded the Mita Congregation, the first religion of Puerto Rican origin. Taíno religious practices have been rediscovered/reinvented to a degree by a handful of advocates. Similarly, some aspects of African religious traditions have been kept by some adherents. In 1952, a handful of American Jews established the island's first synagogue; this religion accounts for under 1% of the population in 2010 according to the Pew Research report. The synagogue, called "Sha'are Zedeck", hired its first rabbi in 1954. Puerto Rico has the largest Jewish community in the Caribbean, numbering 3000 people (date not stated), and is the only Caribbean island in which the Conservative, Reform and Orthodox Jewish movements all are represented. In 2007, there were about 5,000 Muslims in Puerto Rico, representing about 0.13% of the population. Eight mosques are located throughout the island, with most Muslims living in Río Piedras and Caguas, most of these Muslims are of Palestinian and Jordanian descent. There is also a Bahá'í community on the island. In 2015, the 25,832 Jehovah's Witnesses represented about 0.70% of the population, with 324 congregations. The Padmasambhava Buddhist Center, whose followers practice Tibetan Buddhism, as well as Nichiren Buddhism have branches in Puerto Rico. There are several atheist activist and educational organizations, and an atheistic parody religion called the Pastafarian Church of Puerto Rico.

Puerto Rico has 8 senatorial districts, 40 representative districts and 78 municipalities. It has a republican form of government with separation of powers subject to the jurisdiction and sovereignty of the United States. Its current powers are all delegated by the United States Congress and lack full protection under the United States Constitution. Puerto Rico's head of state is the President of the United States.

The government of Puerto Rico, based on the formal republican system, is composed of three branches: the executive, legislative, and judicial branch. The executive branch is headed by the governor, currently Ricky Rosselló. The legislative branch consists of a bicameral legislature called the Legislative Assembly, made up of a Senate as its upper chamber and a House of Representatives as its lower chamber. The Senate is headed by the President of the Senate, currently Thomas Rivera Schatz, while the House of Representatives is headed by the Speaker of the House, currently Johnny Méndez. The governor and legislators are elected by popular vote every four years with the last election held in November 2016.

The judicial branch is headed by the Chief Justice of the Supreme Court of Puerto Rico, currently Maite Oronoz Rodríguez. Members of the judicial branch are appointed by the governor with the advice and consent of the Senate.

Puerto Rico is represented in the United States Congress by a nonvoting delegate, the Resident Commissioner, currently Jenniffer González. Current congressional rules have removed the Commissioner's power to vote in the Committee of the Whole, but the Commissioner can vote in committee.

Puerto Rican elections are governed by the Federal Election Commission and the State Elections Commission of Puerto Rico. While residing in Puerto Rico, Puerto Ricans cannot vote in U.S. presidential elections, but they can vote in primaries. Puerto Ricans who become residents of a U.S. state can vote in presidential elections.

Puerto Rico hosts consulates from 41 countries, mainly from the Americas and Europe, with most located in San Juan. As an unincorporated territory of the United States, Puerto Rico does not have any first-order administrative divisions as defined by the U.S. government, but has 78 municipalities at the second level. Mona Island is not a municipality, but part of the municipality of Mayagüez.

Municipalities are subdivided into wards or barrios, and those into sectors. Each municipality has a mayor and a municipal legislature elected for a four-year term. The municipality of San Juan (previously called "town"), was founded first, in 1521, San Germán in 1570, Coamo in 1579, Arecibo in 1614, Aguada in 1692 and Ponce in 1692. An increase of settlement saw the founding of 30 municipalities in the 18th century and 34 in the 19th. Six were founded in the 20th century; the last was Florida in 1971.

Since 1952, Puerto Rico has had three main political parties: the Popular Democratic Party (PPD in Spanish), the New Progressive Party (PNP in Spanish) and the Puerto Rican Independence Party (PIP). The three parties stand for different political status. The PPD, for example, seeks to maintain the island's status with the U.S. as a commonwealth, while the PNP, on the other hand, seeks to make Puerto Rico a state of the United States. The PIP, in contrast, seeks a complete separation from the United States by seeking to make Puerto Rico a sovereign nation. In terms of party strength, the PPD and PNP usually hold about 47% of the vote each while the PIP holds only about 5%.

After 2007, other parties emerged on the island. The first, the Puerto Ricans for Puerto Rico Party (PPR in Spanish) was registered that same year. The party claims that it seeks to address the islands' problems from a status-neutral platform. But it ceased to remain as a registered party when it failed to obtain the required number of votes in the 2008 general election. Four years later, the 2012 election saw the emergence of the Movimiento Unión Soberanista (MUS; English: "Sovereign Union Movement") and the Partido del Pueblo Trabajador (PPT; English: "Working People's Party") but none obtained more than 1% of the vote.

Other non-registered parties include the Puerto Rican Nationalist Party, the Socialist Workers Movement, and the Hostosian National Independence Movement.

The insular legal system is a blend of civil law and the common law systems.

Puerto Rico is the only current U.S. possession whose legal system operates primarily in a language other than American English: namely, Spanish. Because the U.S. federal government operates primarily in English, all Puerto Rican attorneys must be bilingual in order to litigate in English in U.S. federal courts, and litigate federal preemption issues in Puerto Rican courts.

Title 48 of the United States Code outlines the role of the United States Code to United States territories and insular areas such as Puerto Rico. After the U.S. government assumed control of Puerto Rico in 1901, it initiated legal reforms resulting in the adoption of codes of criminal law, criminal procedure, and civil procedure modeled after those then in effect in California. Although Puerto Rico has since followed the federal example of transferring criminal and civil procedure from statutory law to rules promulgated by the judiciary, several portions of its criminal law still reflect the influence of the California Penal Code.

The judicial branch is headed by the Chief Justice of the Puerto Rico Supreme Court, which is the only appellate court required by the Constitution. All other courts are created by the Legislative Assembly of Puerto Rico. There is also a Federal District Court for Puerto Rico. Someone accused of a criminal act at the federal level may not be accused for the same act in a Commonwealth court, unlike a state court, since Puerto Rico as a territory lacks sovereignty separate from Congress as a state does. Such a parallel accusation would constitute double jeopardy.

The nature of Puerto Rico's political relationship with the U.S. is the subject of ongoing debate in Puerto Rico, the United States Congress, and the United Nations. Specifically, the basic question is whether Puerto Rico should remain a U.S. territory, become a U.S. state, or become an independent country.

Constitutionally, Puerto Rico is subject to the plenary powers of the United States Congress under the territorial clause of Article IV of the U.S. Constitution. Laws enacted at the federal level in the United States apply to Puerto Rico as well, regardless of its political status. Their residents do not have voting representation in the U.S. Congress. Like the different states of the United States, Puerto Rico lacks "the full sovereignty of an independent nation", for example, the power to manage its "external relations with other nations", which is held by the U.S. federal government. The Supreme Court of the United States has indicated that once the U.S. Constitution has been extended to an area (by Congress or the courts), its coverage is irrevocable. To hold that the political branches may switch the Constitution on or off at will would lead to a regime in which they, not this Court, say "what the law is".

Puerto Ricans "were collectively made U.S. citizens" in 1917 as a result of the Jones-Shafroth Act. U.S. citizens residing in Puerto Rico cannot vote for the U.S. president, though both major parties, Republican and Democratic, run primary elections in Puerto Rico to send delegates to vote on a presidential candidate. Since Puerto Rico is an unincorporated territory (see above) and not a U.S. state, the United States Constitution does not fully enfranchise U.S. citizens residing in Puerto Rico.

Only fundamental rights under the American federal constitution and adjudications are applied to Puerto Ricans. Various other U.S. Supreme Court decisions have held which rights apply in Puerto Rico and which ones do not. Puerto Ricans have a long history of service in the U.S. Armed Forces and, since 1917, they have been included in the U.S. compulsory draft whensoever it has been in effect.

Though the Commonwealth government has its own tax laws, Puerto Ricans are also required to pay many kinds of U.S. federal taxes, not including the federal personal income tax for Puerto Rico-sourced income, but only under certain circumstances. In 2009, Puerto Rico paid into the U.S. Treasury. Residents of Puerto Rico pay into Social Security, and are thus eligible for Social Security benefits upon retirement. They are excluded from the Supplemental Security Income (SSI), and the island actually receives a smaller fraction of the Medicaid funding it would receive if it were a U.S. state. Also, Medicare providers receive less-than-full state-like reimbursements for services rendered to beneficiaries in Puerto Rico, even though the latter paid fully into the system.

While a state may try an individual for the same crime he/she was tried in federal court, this is not the case in Puerto Rico. Being a U.S. territory, Puerto Rico's authority to enact a criminal code derives from Congress and not from local sovereignty as with the states. Thus, such a parallel accusation would constitute double jeopardy and is constitutionally impermissible.

In 1992, President George H. W. Bush issued a memorandum to heads of executive departments and agencies establishing the current administrative relationship between the federal government and the Commonwealth of Puerto Rico. This memorandum directs all federal departments, agencies, and officials to treat Puerto Rico administratively as if it were a state, insofar as doing so would not disrupt federal programs or operations.

Many federal executive branch agencies have significant presence in Puerto Rico, just as in any state, including the Federal Bureau of Investigation, Federal Emergency Management Agency, Transportation Security Administration, Social Security Administration, and others. While Puerto Rico has its own Commonwealth judicial system similar to that of a U.S. state, there is also a U.S. federal district court in Puerto Rico, and Puerto Ricans have served as judges in that Court and in other federal courts on the U.S. mainland regardless of their residency status at the time of their appointment. Sonia Sotomayor, a New Yorker of Puerto Rican descent, serves as an Associate Justice of the Supreme Court of the United States. Puerto Ricans have also been frequently appointed to high-level federal positions, including serving as United States Ambassadors to other nations.

Puerto Rico is subject to the Commerce and Territorial Clause of the Constitution of the United States and, therefore, is restricted on how it can engage with other nations, sharing the opportunities and limitations that state governments have albeit not being one. As is the case with state governments, regardless, it has established several trade agreements with other nations, particularly with Hispanic American countries such as Colombia and Panamá.

It has also established trade promotion offices in many foreign countries, all Spanish-speaking, and within the United States itself, which now include Spain, the Dominican Republic, Panama, Colombia, Washington, D.C., New York City and Florida, and has included in the past offices in Chile, Costa Rica, and Mexico. Such agreements require permission from the U.S. Department of State; most, are simply allowed by existing laws or trade treaties between the United States and other nations which supersede trade agreements pursued by Puerto Rico and different U.S. states.

At the local level, Puerto Rico established by law that the international relations which states and territories are allowed to engage must be handled by the Department of State of Puerto Rico, an executive department, headed by the Secretary of State of Puerto Rico, who also serves as the territory's lieutenant governor. It is also charged to liaise with general consuls and honorary consuls based in Puerto Rico. The Puerto Rico Federal Affairs Administration, along with the Office of the Resident Commissioner, manage all its intergovernmental affairs before entities of or in the United States (including the federal government of the United States, local and state governments of the United States, and public or private entities in the United States).

Both entities frequently assist the Department of State of Puerto Rico in engaging with Washington, D.C.-based ambassadors and federal agencies that handle Puerto Rico's foreign affairs, such as the U.S. Department of State, the Agency for International Development, and others. The current Secretary of State is Víctor Suárez Meléndez from the Popular Democratic Party and member of the Democratic Party of the United States, while the current Director of the Puerto Rico Federal Affairs Administration is Juan Eugenio Hernández Mayoral also from the Popular Democratic and member of the Democratic Party.

The Resident Commissioner of Puerto Rico, the delegate elected by Puerto Ricans to represent them before the federal government, including the U.S. Congress, sits in the United States House of Representatives, serves and votes on congressional committees, and functions in every respect as a legislator except being denied a vote on the final disposition of legislation on the House floor, also engages in foreign affairs to the same extent as other members of Congress. The current Resident Commissioner is Pedro Pierluisi from the New Progressive Party and member of the Democratic Party of the United States.

Many Puerto Ricans have served as United States ambassadors to different nations and international organizations, such as the Organization of American States, mostly but not exclusively in Latin America. For example, Maricarmen Aponte, a Puerto Rican and now an Acting Assistant Secretary of State, previously served as U.S. ambassador to El Salvador.

As it is a territory of the United States of America, the defense of Puerto Rico is provided by the United States as part of the Treaty of Paris with the President of the United States as its commander-in-chief. Puerto Rico has its own Puerto Rico National Guard, and its own state defense force, the Puerto Rico State Guard, which by local law is under the authority of the Puerto Rico National Guard.

The commander-in-chief of both local forces is the governor of Puerto Rico who delegates his authority to the Puerto Rico Adjutant General, currently Brigadier General Isabelo Rivera. The Adjutant General, in turn, delegates the authority over the State Guard to another officer but retains the authority over the Puerto Rico National Guard as a whole.

U.S. military installations in Puerto Rico were part of the U.S. Atlantic Command (LANTCOM after 1993 USACOM), which had authority over all U.S. military operations that took place throughout the Atlantic. Puerto Rico had been seen as crucial in supporting LANTCOM's mission until 1999, when U.S. Atlantic Command was renamed and given a new mission as United States Joint Forces Command. Puerto Rico is currently under the responsibility of United States Northern Command.

Both the Naval Forces Caribbean (NFC) and the Fleet Air Caribbean (FAIR) were formerly based at the Roosevelt Roads Naval Station. The NFC had authority over all U.S. Naval activity in the waters of the Caribbean while FAIR had authority over all U.S. military flights and air operations over the Caribbean. With the closing of the Roosevelt Roads and Vieques Island training facilities, the U.S. Navy has basically exited from Puerto Rico, except for the ships that steam by, and the only significant military presence in the island is the U.S. Army at Ft Buchanan, the Puerto Rican Army and Air National Guards, and the U.S. Coast Guard. Protests over the noise of bombing practice forced the closure of the naval base. This resulted in a loss of 6,000 jobs and an annual decrease in local income of $300 million.

A branch of the U.S. Army National Guard is stationed in Puerto Rico — known as the Puerto Rico Army National Guard — which performs missions equivalent to those of the Army National Guards of the different states of the United States, including ground defense, disaster relief, and control of civil unrest. The local National Guard also incorporates a branch of the U.S. Air National Guard — known as the Puerto Rico Air National Guard — which performs missions equivalent to those of the Air National Guards of each one of the U.S. states.
At different times in the 20th century, the U.S. had about 25 military or naval installations in Puerto Rico, some very small ones, as well as large installations. The largest of these installations were the former Roosevelt Roads Naval Station in Ceiba, the Atlantic Fleet Weapons Training Facility (AFWTF) on Vieques, the National Guard training facility at Camp Santiago in Salinas, Fort Allen in Juana Diaz, the Army's Fort Buchanan in San Juan, the former U.S. Air Force Ramey Air Force Base in Aguadilla, and the Puerto Rico Air National Guard at Muñiz Air Force base in San Juan.

The former U.S. Navy facilities at Roosevelt Roads, Vieques, and Sabana Seca have been deactivated and partially turned over to the local government. Other than U.S. Coast Guard and Puerto Rico National Guard facilities, there are only two remaining military installations in Puerto Rico: the U.S. Army's small Ft. Buchanan (supporting local veterans and reserve units) and the PRANG (Puerto Rico Air National Guard) Muñiz Air Base (the C-130 Fleet). In recent years, the U.S. Congress has considered their deactivations, but these have been opposed by diverse public and private entities in Puerto Rico – such as retired military who rely on Ft. Buchanan for the services available there.

Puerto Ricans have participated in many of the military conflicts in which the United States has been involved. For example, they participated in the American Revolution, when volunteers from Puerto Rico, Cuba, and Mexico fought the British in 1779 under the command of General Bernardo de Gálvez (1746–1786), and have continued to participate up to the present-day conflicts in Iraq and Afghanistan.

A significant number of Puerto Ricans participate as members and work for the U.S. Armed Services, largely as National Guard members and civilian employees. The size of the overall military-related community in Puerto Rico is estimated to be 100,000 individuals. This includes retired personnel. Fort Buchanan has about 4,000 military and civilian personnel. In addition, approximately 17,000 people are members of the Puerto Rico Army and Air National Guards, or the U.S. Reserve forces. Puerto Rican soldiers have served in every U.S. military conflict from World War I to the current military engagement known by the United States and its allies as the War against Terrorism.

The 65th Infantry Regiment, nicknamed ""The Borinqueneers"" from the original Taíno name of the island (Borinquen), is a Puerto Rican regiment of the United States Army. The regiment's motto is "Honor et Fidelitas", Latin for "Honor and Fidelity". The 65th Infantry Regiment participated in World War I, World War II, the Korean War, and the War on Terror and in 2014 was awarded the Congressional Gold Medal, presented by President Barack Obama, for its heroism during the Korean War.

There are no counties, as there are in 48 of the 50 United States. There are 78 municipalities. Municipalities are subdivided into "barrios", and those into sectors. Each municipality has a mayor and a municipal legislature elected to four-year terms.

The economy of Puerto Rico is classified as a high income economy by the World Bank and as the most competitive economy in Latin America by the World Economic Forum but Puerto Rico currently has a public debt of $72.204 billion (equivalent to 103% of GNP), and a government deficit of $2.5 billion. According to World Bank, gross national income per capita of Puerto Rico in 2013 is $23,830 (PPP, International Dollars), ranked as 63rd among all sovereign and dependent territories entities in the world. Its economy is mainly driven by manufacturing (primarily pharmaceuticals, textiles, petrochemicals and electronics) followed by the service industry (primarily finance, insurance, real estate and tourism). In recent years, the territory has also become a popular destination for MICE (meetings, incentives, conferencing, exhibitions), with a modern convention centre district overlooking the Port of San Juan.

The geography of Puerto Rico and its political status are both determining factors on its economic prosperity, primarily due to its relatively small size as an island; its lack of natural resources used to produce raw materials, and, consequently, its dependence on imports; as well as its territorial status with the United States, which controls its foreign policy while exerting trading restrictions, particularly in its shipping industry.

Puerto Rico experienced a recession from 2006 to 2011, interrupted by 4 quarters of economic growth, and entered into recession again in 2013, following growing fiscal imbalance and the expiration of the IRS Section 936 corporate incentives that the U.S. Internal Revenue Code had applied to Puerto Rico. This IRS section was critical to the economy, as it established tax exemptions for U.S. corporations that settled in Puerto Rico, and allowed their insular subsidiaries to send their earnings to the parent corporation at any time, without paying federal tax on corporate income. Puerto Rico has surprisingly been able to maintain a relatively low inflation in the past decade while maintaining a purchasing power parity per capita higher than 80% of the rest of the world.

Academically, most of Puerto Rico's economic woes stem from federal regulations that expired, have been repealed, or no longer apply to Puerto Rico; its inability to become self-sufficient and self-sustainable throughout history; its highly politicized public policy which tends to change whenever a political party gains power; as well as its highly inefficient local government which has accrued a public debt equal to 68% of its gross domestic product throughout time.

In comparison to the different states of the United States, Puerto Rico is poorer than Mississippi (the poorest state of the U.S.) with 41% of its population below the poverty line. When compared to Latin America, Puerto Rico has the highest GDP per capita in the region. Its main trading partners are the United States itself, Ireland, and Japan, with most products coming from East Asia, mainly from China, Hong Kong, and Taiwan. At a global scale, Puerto Rico's dependency on oil for transportation and electricity generation, as well as its dependency on food imports and raw materials, makes Puerto Rico volatile and highly reactive to changes in the world economy and climate. Puerto Rico's agricultural sector represents less than 1% of GNP.

In early 2017, the Puerto Rican government-debt crisis posed serious problems for the government which was saddled with outstanding bond debt that had climbed to $70 billion at a time with a 45 percent poverty rate and 12.4% unemployment that is more than twice the mainland U.S. average. The debt had been increasing during a decade long recession.

The Commonwealth had been defaulting on many debts, including bonds, since 2015. With debt payments due, the Governor was facing the risk of a government shutdown and failure to fund the managed health care system. "Without action before April, Puerto Rico’s ability to execute contracts for Fiscal Year 2018 with its managed care organizations will be threatened, thereby putting at risk beginning July 1, 2017 the health care of up to 900,000 poor U.S. citizens living in Puerto Rico", according to a letter sent to Congress by the Secretary of the Treasury and the Secretary of Health and Human Services. They also said that "Congress must enact measures recommended by both Republicans and Democrats that fix Puerto Rico’s inequitable health care financing structure and promote sustained economic growth."

Initially, the oversight board created under PROMESA called for Puerto Rico's governor Ricardo Rosselló to deliver a fiscal turnaround plan by January 28. Just before that deadline, the control board gave the Commonwealth government until February 28 to present a fiscal plan (including negotiations with creditors for restructuring debt) to solve the problems. A moratorium on lawsuits by debtors was extended to May 31. It is essential for Puerto Rico to reach restructuring deals to avoid a bankruptcy-like process under PROMESA. An internal survey conducted by the Puerto Rican Economists Association revealed that the majority of Puerto Rican economists reject the policy recommendations of the Board and the Rosselló government, with more than 80% of economists arguing in favor of auditing the debt.

In early August 2017, the island's financial oversight board (created by PROMESA) planned to institute two days off without pay per month for government employees, down from the original plan of four days per month; the latter had been expected to achieve $218 million in savings. Governor Rossello rejected this plan as unjustified and unnecessary. Pension reforms were also discussed including a proposal for a 10% reduction in benefits to begin addressing the $50 billion in unfunded pension liabilities.

Puerto Rico has an operating budget of about U.S.$9.8 billion with expenses at about $10.4 billion, creating a structural deficit of $775 million (about 7.9% of the budget). The practice of approving budgets with a structural deficit has been done for consecutive years starting in 2000. Throughout those years, including present time, all budgets contemplated issuing bonds to cover these projected deficits rather than making structural adjustments. This practice increased Puerto Rico’s cumulative debt, as the government had already been issuing bonds to balance its actual budget for four decades since 1973.
Projected deficits added substantial burdens to an already indebted nation which accrued a public debt of $71B or about 70% of Puerto Rico's gross domestic product. This sparked an ongoing government-debt crisis after Puerto Rico's general obligation bonds were downgraded to speculative non-investment grade ("junk status") by three credit rating agencies. In terms of financial control, almost 9.6%—or about $1.5 billion—of Puerto Rico's central government budget expenses for FY2014 is expected to be spent on debt service. Harsher budget cuts are expected as Puerto Rico must now repay larger chunks of debts in the following years.

For practical reasons the budget is divided into two aspects: a "general budget" which comprises the assignments funded exclusively by the Department of Treasury of Puerto Rico, and the "consolidated budget" which comprises the assignments funded by the general budget, by Puerto Rico's government-owned corporations, by revenue expected from loans, by the sale of government bonds, by subsidies extended by the federal government of the United States, and by other funds.

Both budgets contrast each other drastically, with the consolidated budget being usually thrice the size of the general budget; currently $29B and $9.0B respectively. Almost one out of every four dollars in the consolidated budget comes from U.S. federal subsidies while government-owned corporations compose more than 31% of the consolidated budget.

The critical aspects come from the sale of bonds, which comprise 7% of the consolidated budget – a ratio that increased annually due to the government's inability to prepare a balanced budget in addition to being incapable of generating enough income to cover all its expenses. In particular, the government-owned corporations add a heavy burden to the overall budget and public debt, as none is self-sufficient. For example, in FY2011 the government-owned corporations reported aggregated losses of more than $1.3B with the Puerto Rico Highways and Transportation Authority (PRHTA) reporting losses of $409M, the Puerto Rico Electric Power Authority (PREPA; the government monopoly that controls all electricity on the island) reporting losses of $272M, while the Puerto Rico Aqueducts and Sewers Authority (PRASA; the government monopoly that controls all water utilities on the island) reported losses of $112M.

Losses by government-owned corporations have been defrayed through the issuance of bonds compounding more than 40% of Puerto Rico's entire public debt today. Holistically, from FY2000–FY2010 Puerto Rico's debt grew at a compound annual growth rate (CAGR) of 9% while GDP remained stagnant. This has not always provided a long-term solution. In early July 2017 for example, the PREPA power authority was effectively bankrupt after defaulting in a plan to restructure $9 billion in bond debt; the agency planned to seek Court protection.

In terms of protocol, the governor, together with the Puerto Rico Office of Management and Budget (OGP in Spanish), formulates the budget he believes is required to operate all government branches for the ensuing fiscal year. He then submits this formulation as a budget request to the Puerto Rican legislature before July 1, the date established by law as the beginning of Puerto Rico's fiscal year. While the constitution establishes that the request must be submitted "at the beginning of each regular session", the request is typically submitted during the first week of May as the regular sessions of the legislature begin in January and it would be impractical to submit a request so far in advance. Once submitted, the budget is then approved by the legislature, typically with amendments, through a joint resolution and is referred back to the governor for his approval. The governor then either approves it or vetoes it. If vetoed the legislature can then either refer it back with amendments for the governor's approval, or approve it without the governor's consent by two-thirds of the bodies of each chamber.

Once the budget is approved, the Department of Treasury disburses funds to the Office of Management and Budget which in turn disburses the funds to the respective agencies, while the Puerto Rico Government Development Bank (the government's intergovernmental bank) manages all related banking affairs including those related to the government-owned corporations.

The cost of living in Puerto Rico is high and has increased over the past decade. San Juan's in particular is higher than Atlanta, Dallas, and Seattle but lower than Boston, Chicago, and New York City. One factor is housing prices which are comparable to Miami and Los Angeles, although property taxes are considerably lower than most places in the United States.

Statistics used for cost of living sometimes do not take into account certain costs, such as the high cost of electricity, which has hovered in the 24¢ to 30¢ range per kilowatt/hour, two to three times the national average, increased travel costs for longer flights, additional shipping fees, and the loss of promotional participation opportunities for customers "outside the continental United States". While some online stores do offer free shipping on orders to Puerto Rico, many merchants exclude Hawaii, Alaska, Puerto Rico and other United States territories.

The household median income is stated as $19,350 and the mean income as $30,463 in the U.S. Census Bureau's 2015 update. The report also indicates that 45.5% of individuals are below the poverty level. The median home value in Puerto Rico ranges from U.S.$100,000 to U.S.$214,000, while the national median home value sits at $119,600.
One of the most cited contributors to the high cost of living in Puerto Rico is the Merchant Marine Act of 1920, also known as the Jones Act, which prevents foreign-flagged ships from carrying cargo between two American ports, a practice known as cabotage. Because of the Jones Act, foreign ships inbound with goods from Central and South America, Western Europe, and Africa cannot stop in Puerto Rico, offload Puerto Rico-bound goods, load mainland-bound Puerto Rico-manufactured goods, and continue to U.S. ports. Instead, they must proceed directly to U.S. ports, where distributors break bulk and send Puerto Rico-bound manufactured goods to Puerto Rico across the ocean by U.S.-flagged ships.

The local government of Puerto Rico has requested several times to the U.S. Congress to exclude Puerto Rico from the Jones Act restrictions without success. The most recent measure has been taken by the 17th Legislative Assembly of Puerto Rico through R. Conc. del S. 21. These measures have always received support from all the major local political parties.

In 2013 the Government Accountability Office published a report which concluded that "repealing or amending the Jones Act cabotage law might cut Puerto Rico shipping costs" and that "shippers believed that opening the trade to non-U.S.-flag competition could lower costs". However, the same GAO report also found that "[shippers] doing business in Puerto Rico that GAO contacted reported that the freight rates are often—although not always—lower for foreign carriers going to and from Puerto Rico and foreign locations than the rates shippers pay to ship similar cargo to and from the United States, despite longer distances. Data were not available to allow us to validate the examples given or verify the extent to which this difference occurred." Ultimately, the report concluded that "[the] effects of modifying the application of the Jones Act for Puerto Rico are highly uncertain" for both Puerto Rico and the United States, particularly for the U.S. shipping industry and the military preparedness of the United States.

The first school in Puerto Rico was the "Escuela de Gramática" (Grammar School). It was established by Bishop Alonso Manso in 1513, in the area where the Cathedral of San Juan was to be constructed. The school was free of charge and the courses taught were Latin language, literature, history, science, art, philosophy and theology.

Education in Puerto Rico is divided in three levels—Primary (elementary school grades 1–6), Secondary (intermediate and high school grades 7–12), and Higher Level (undergraduate and graduate studies). As of 2002, the literacy rate of the Puerto Rican population was 94.1%; by gender, it was 93.9% for males and 94.4% for females. According to the 2000 Census, 60.0% of the population attained a high school degree or higher level of education, and 18.3% has a bachelor's degree or higher.

Instruction at the primary school level is compulsory between the ages of 5 and 18. , there are 1539 public schools and 806 private schools.

The largest and oldest university system is the public University of Puerto Rico (UPR) with 11 campuses. The largest private university systems on the island are the Sistema Universitario Ana G. Mendez which operates the Universidad del Turabo, Metropolitan University and Universidad del Este. Other private universities include the multi-campus Inter American University, the Pontifical Catholic University, Universidad Politécnica de Puerto Rico, and the Universidad del Sagrado Corazón. Puerto Rico has four schools of Medicine and three ABA-approved Law Schools.

, medical care in Puerto Rico had been heavily impacted by emigration of doctors to the mainland and underfunding of the Medicare and Medicaid programs which serve 60% of the island's population. Affordable medical insurance under the Affordable Care Act is not available in Puerto Rico as, since Puerto Ricans pay no income tax, no subsidies are available.

The city of San Juan has a system of triage, hospital, and preventive care health services. The municipal government sponsors regular health fairs in different areas of the city focusing on health care for the elderly and the disabled.

In 2017, there were 69 hospitals in Puerto Rico.

There are twenty hospitals in San Juan, half of which are operated by the government. The largest hospital is the "Centro Médico de Río Piedras" (the Río Piedras Medical Center). Founded in 1956, it is operated by the Medical Services Administration of the Department of Health of Puerto Rico, and is actually a network of eight hospitals:

The city of San Juan operates nine other hospitals. Of these, eight are Diagnostic and Treatment Centers located in communities throughout San Juan. These nine hospitals are:

There are also ten private hospitals in San Juan. These are:

The city of Ponce is served by several clinics and hospitals. There are four comprehensive care hospitals: Hospital Dr. Pila, Hospital San Cristobal, Hospital San Lucas, and Hospital de Damas. In addition, Hospital Oncológico Andrés Grillasca specializes in the treatment of cancer, and Hospital Siquiátrico specializes in mental disorders. There is also a U.S. Department of Veterans Affairs Outpatient Clinic that provides health services to U.S. veterans. The U.S. Veterans Administration will build a new hospital in the city to satisfy regional needs. Hospital de Damas is listed in the U.S. News & World Report as one of the best hospitals under the U.S. flag. Ponce has the highest concentration of medical infrastructure per inhabitant of any municipality in Puerto Rico.

On the island of Culebra, there is a small hospital in the island called "Hospital de Culebra". It also offers pharmacy services to residents and visitors. For emergencies, patients are transported by plane to Fajardo on the main island.

The town of Caguas has three hospitals: Hospital Hima San Pablo, Menonita Caguas Regional Hospital, and the San Juan Bautista Medical Center.

The town of Cayey is served by the "Hospital Menonita de Cayey", and the "Hospital Municipal de Cayey."

"Reforma de Salud de Puerto Rico" (Puerto Rico Health Reform) – locally referred to as "La Reforma" (The Reform) – is a government-run program which provides medical and health care services to the indigent and impoverished, by means of contracting private health insurance companies, rather than employing government-owned hospitals and emergency centers. The Reform is administered by the Puerto Rico Health Insurance Administration.

The overall rate of crime is low in Puerto Rico. The territory has a high firearm homicide rate. The homicide rate of 19.2 per 100,000 inhabitants was significantly higher than any U.S. state in 2014. Most homicide victims are gang members and drug traffickers with about 80% of homicides in Puerto Rico being drug related.

Modern Puerto Rican culture is a unique mix of cultural antecedents: including European (predominantly Spanish, Italian, French, German and Irish), African, and, more recently, some North American and lots of South Americans. A large number of Cubans and Dominicans have relocated to the island in the past few decades.

From the Spanish, Puerto Rico received the Spanish language, the Catholic religion and the vast majority of their cultural and moral values and traditions. The United States added English-language influence, the university system and the adoption of some holidays and practices. On March 12, 1903, the University of Puerto Rico was officially founded, branching out from the "Escuela Normal Industrial", a smaller organization that was founded in Fajardo three years before.

Much of Puerto Rican culture centers on the influence of music and has been shaped by other cultures combining with local and traditional rhythms. Early in the history of Puerto Rican music, the influences of Spanish and African traditions were most noticeable. The cultural movements across the Caribbean and North America have played a vital role in the more recent musical influences which have reached Puerto Rico.

The official symbols of Puerto Rico are the "reinita mora" or Puerto Rican spindalis (a type of bird), the "flor de maga" (a type of flower), and the "ceiba" or kapok (a type of tree). The unofficial animal and a symbol of Puerto Rican pride is the coquí, a small frog. Other popular symbols of Puerto Rico are the "jíbaro" (the "countryman"), and the carite.

The architecture of Puerto Rico demonstrates a broad variety of traditions, styles and national influences accumulated over four centuries of Spanish rule, and a century of American rule. Spanish colonial architecture, Ibero-Islamic, art deco, post-modern, and many other architectural forms are visible throughout the island. From town to town, there are also many regional distinctions.
Old San Juan is one of the two "barrios", in addition to Santurce, that made up the municipality of San Juan from 1864 to 1951, at which time the former independent municipality of Río Piedras was annexed. With its abundance of shops, historic places, museums, open air cafés, restaurants, gracious homes, tree-shaded plazas, and its old beauty and architectonical peculiarity, Old San Juan is a main spot for local and internal tourism. The district is also characterized by numerous public plazas and churches including San José Church and the Cathedral of San Juan Bautista, which contains the tomb of the Spanish explorer Juan Ponce de León. It also houses the oldest Catholic school for elementary education in Puerto Rico, the Colegio de Párvulos, built in 1865.

The oldest parts of the district of Old San Juan remain partly enclosed by massive walls. Several defensive structures and notable forts, such as the emblematic Fort San Felipe del Morro, Fort San Cristóbal, and El Palacio de Santa Catalina, also known as La Fortaleza, acted as the primary defenses of the settlement which was subjected to numerous attacks. La Fortaleza continues to serve also as the executive mansion for the Governor of Puerto Rico. Many of the historic fortifications are part of San Juan National Historic Site.

During the 1940s, sections of Old San Juan fell into disrepair, and many renovation plans were suggested. There was even a strong push to develop Old San Juan as a "small Manhattan". Strict remodeling codes were implemented to prevent new constructions from affecting the common colonial Spanish architectural themes of the old city. When a project proposal suggested that the old Carmelite Convent in San Juan be demolished to erect a new hotel, the Institute had the building declared as a historic building, and then asked that it be converted to a hotel in a renewed facility. This was what became the "Hotel El Convento" in Old San Juan. The paradigm to reconstruct and renovate the old city and revitalize it has been followed by other cities in the Americas, particularly Havana, Lima and Cartagena de Indias.

Ponce Creole is an architectural style created in Ponce, Puerto Rico, in the late 19th and early 20th centuries. This style of Puerto Rican buildings is found predominantly in residential homes in Ponce that developed between 1895 and 1920. Ponce Creole architecture borrows heavily from the traditions of the French, the Spaniards, and the Caribbean to create houses that were especially built to withstand the hot and dry climate of the region, and to take advantage of the sun and sea breezes characteristic of the southern Puerto Rico's Caribbean Sea coast. It is a blend of wood and masonry, incorporating architectural elements of other styles, from Classical revival and Spanish Revival to Victorian.

Puerto Rican art reflects many influences, much from its ethnically diverse background. A form of folk art, called "santos" evolved from the Catholic Church's use of sculptures to convert indigenous Puerto Ricans to Christianity. "Santos" depict figures of saints and other religious icons and are made from native wood, clay, and stone. After shaping simple, they are often finished by painting them in vivid colors. "Santos" vary in size, with the smallest examples around eight inches tall and the largest about twenty inches tall. Traditionally, santos were seen as messengers between the earth and Heaven. As such, they occupied a special place on household altars, where people prayed to them, asked for help, or tried to summon their protection.

Also popular, "caretas" or "vejigantes" are masks worn during carnivals. Similar masks signifying evil spirits were used in both Spain and Africa, though for different purposes. The Spanish used their masks to frighten lapsed Christians into returning to the church, while tribal Africans used them as protection from the evil spirits they represented. True to their historic origins Puerto Rican "caretas" always bear at least several horns and fangs. While usually constructed of papier-mâché, coconut shells and fine metal screening are sometimes used as well. Red and black were the typical colors for "caretas" but their palette has expanded to include a wide variety of bright hues and patterns.

Puerto Rican literature evolved from the art of oral story telling to its present-day status. Written works by the native islanders of Puerto Rico were prohibited and repressed by the Spanish colonial government. Only those who were commissioned by the Spanish Crown to document the chronological history of the island were allowed to write.

Diego de Torres Vargas was allowed to circumvent this strict prohibition for three reasons: he was a priest, he came from a prosperous Spanish family, and his father was a Sergeant Major in the Spanish Army, who died while defending Puerto Rico from an invasion by the Dutch armada. In 1647, Torres Vargas wrote "Descripción de la Ciudad e Isla de Puerto Rico" ("Description of the Island and City of Puerto Rico"). This historical book was the first to make a detailed geographic description of the island.

The book described all the fruits and commercial establishments of the time, mostly centered in the towns of San Juan and Ponce. The book also listed and described every mine, church, and hospital in the island at the time. The book contained notices on the State and Capital, plus an extensive and erudite bibliography. "Descripción de la Ciudad e Isla de Puerto Rico" was the first successful attempt at writing a comprehensive history of Puerto Rico.

Some of Puerto Rico's earliest writers were influenced by the teachings of Rafael Cordero. Among these was Dr. Manuel A. Alonso, the first Puerto Rican writer of notable importance. In 1849 he published "El Gíbaro", a collection of verses whose main themes were the poor Puerto Rican country farmer. Eugenio María de Hostos wrote "La peregrinación de Bayoán" in 1863, which used Bartolomé de las Casas as a spring board to reflect on Caribbean identity. After this first novel, Hostos abandoned fiction in favor of the essay which he saw as offering greater possibilities for inspiring social change.

In the late 19th century, with the arrival of the first printing press and the founding of the Royal Academy of Belles Letters, Puerto Rican literature began to flourish. The first writers to express their political views in regard to Spanish colonial rule of the island were journalists. After the United States invaded Puerto Rico during the Spanish–American War and the island was ceded to the Americans as a condition of the Treaty of Paris of 1898, writers and poets began to express their opposition to the new colonial rule by writing about patriotic themes.

Alejandro Tapia y Rivera, also known as the Father of Puerto Rican Literature, ushered in a new age of historiography with the publication of "The Historical Library of Puerto Rico". Cayetano Coll y Toste was another Puerto Rican historian and writer. His work "The Indo-Antillano Vocabulary" is valuable in understanding the way the Taínos lived. Dr. Manuel Zeno Gandía in 1894 wrote "La Charca" and told about the harsh life in the remote and mountainous coffee regions in Puerto Rico. Dr. Antonio S. Pedreira, described in his work "Insularismo" the cultural survival of the Puerto Rican identity after the American invasion.

With the Puerto Rican diaspora of the 1940s, Puerto Rican literature was greatly influenced by a phenomenon known as the Nuyorican Movement. Puerto Rican literature continued to flourish and many Puerto Ricans have since distinguished themselves as authors, journalists, poets, novelists, playwrights, screenwriters, essayists and have also stood out in other literary fields. The influence of Puerto Rican literature has transcended the boundaries of the island to the United States and the rest of the world. Over the past fifty years, significant writers include Ed Vega, Luis Rafael Sánchez, Piri Thomas, Giannina Braschi, and Miguel Piñero. Esmeralda Santiago has written an autobiographical trilogy about growing up in modern Puerto Rico as well as an historical novel, "Conquistadora", about life on a sugar plantation during the mid-19th century.

The media in Puerto Rico includes local radio stations, television stations and newspapers, the majority of which are conducted in Spanish. There are also three stations of the U.S. Armed Forces Radio and Television Service. Newspapers with daily distribution are El Nuevo Dia, El Vocero and Indice, Metro, and Primera Hora. El Vocero is distributed free of charge as well as Indice and Metro.

Newspapers distributed on a weekly or regional basis include Claridad, La Perla del Sur, La Opinion, Vision, and La Estrella del Norte, among others. Several television channels provide local content in the island. These include WIPR-TV, Telemundo, Univision Puerto Rico, WAPA-TV, and WKAQ-TV.

The music of Puerto Rico has evolved as a heterogeneous and dynamic product of diverse cultural resources. The most conspicuous musical sources have been Spain and West Africa, although many aspects of Puerto Rican music reflect origins elsewhere in Europe and the Caribbean and, over the last century, from the U.S. Puerto Rican music culture today comprises a wide and rich variety of genres, ranging from indigenous genres like bomba, plena, aguinaldo, danza and salsa to recent hybrids like reggaeton.

Puerto Rico has some national instruments, like the cuatro (Spanish for "four"). The cuatro is a local instrument that was made by the "Jibaro" or people from the mountains. Originally, the Cuatro consisted of four steel strings, hence its name, but currently the Cuatro consists of five double steel strings. It is easily confused with a guitar, even by locals. When held upright, from right to left, the strings are G, D, A, E, B.

In the realm of classical music, the island hosts two main orchestras, the Orquesta Sinfónica de Puerto Rico and the Orquesta Filarmónica de Puerto Rico. The Casals Festival takes place annually in San Juan, drawing in classical musicians from around the world.

With respect to opera, the legendary Puerto Rican tenor Antonio Paoli was so celebrated, that he performed private recitals for Pope Pius X and the Czar Nicholas II of Russia. In 1907, Paoli was the first operatic artist in world history to record an entire opera – when he participated in a performance of "Pagliacci" by Ruggiero Leoncavallo in Milan, Italy.

Over the past fifty years, Puerto Rican artists such as Jorge Emmanuelli, Yomo Toro, Ramito, Jose Feliciano, Bobby Capo, Rafael Cortijo, Ismael Rivera, Chayanne, Tito Puente, Eddie Palmieri, Ray Barreto, Dave Valentin, Omar Rodríguez-López, Hector Lavoe, Ricky Martin, Marc Anthony and Luis Fonsi have gained fame internationally.
Puerto Rican cuisine has its roots in the cooking traditions and practices of Europe (Spain), Africa and the native Taínos. In the latter part of the 19th century, the cuisine of Puerto Rico was greatly influenced by the United States in the ingredients used in its preparation. Puerto Rican cuisine has transcended the boundaries of the island, and can be found in several countries outside the archipelago. Basic ingredients include grains and legumes, herbs and spices, starchy tropical tubers, vegetables, meat and poultry, seafood and shellfish, and fruits. Main dishes include "mofongo", "arroz con gandules", "pasteles", "alcapurrias" and pig roast (or lechón). Beverages include "maví" and "piña colada". Desserts include flan, "arroz con dulce" (sweet rice pudding), "piraguas", "brazo gitanos", "tembleque", "polvorones", and "dulce de leche".

Locals call their cuisine cocina criolla. The traditional Puerto Rican cuisine was well established by the end of the 19th century. By 1848 the first restaurant, La Mallorquina, opened in Old San Juan. "El Cocinero Puertorriqueño", the island's first cookbook was published in 1849.
From the diet of the Taíno people come many tropical roots and tubers like "yautía" (taro) and especially "Yuca" (cassava), from which thin cracker-like "casabe" bread is made. Ajicito or cachucha pepper, a slightly hot habanero pepper, "recao/culantro" (spiny leaf), "achiote" (annatto), "peppers", "ají caballero" (the hottest pepper native to Puerto Rico), peanuts, guavas, pineapples, "jicacos" (cocoplum), "quenepas" (mamoncillo), "lerenes" (Guinea arrowroot), "calabazas" (tropical pumpkins), and "guanabanas" (soursops) are all Taíno foods. The Taínos also grew varieties of beans and some maize/corn, but maize was not as dominant in their cooking as it was for the peoples living on the mainland of Mesoamerica. This is due to the frequent hurricanes that Puerto Rico experiences, which destroy crops of maize, leaving more safeguarded plants like "conucos" (hills of "yuca" grown together).

Spanish / European influence is also seen in Puerto Rican cuisine. Wheat, chickpeas (garbanzos), capers, olives, olive oil, black pepper, onions, garlic, "cilantrillo" (cilantro), oregano, basil, sugarcane, citrus fruit, eggplant, ham, lard, chicken, beef, pork, and cheese all came to Borikén (Puerto Rico's native Taino name) from Spain. The tradition of cooking complex stews and rice dishes in pots such as rice and beans are also thought to be originally European (much like Italians, Spaniards, and the British). Early Dutch, French, Italian, and Chinese immigrants influenced not only the culture but Puerto Rican cooking as well. This great variety of traditions came together to form La Cocina Criolla.

Coconuts, coffee (brought by the Arabs and Corsos to Yauco from Kafa, Ethiopia), okra, yams, sesame seeds, "gandules" (pigeon peas in English) sweet bananas, plantains, other root vegetables and Guinea hen, all come to Puerto Rico from Africa.

Puerto Rico has been commemorated on four U.S. postal stamps and four personalities have been featured. Insular Territories were commemorated in 1937, the third stamp honored Puerto Rico featuring 'La Fortaleza', the Spanish Governor's Palace. The first free election for governor of the U.S. colony of Puerto Rico was honored on April 27, 1949, at San Juan, Puerto Rico. 'Inauguration' on the 3-cent stamp refers to the election of Luis Munoz Marin, the first democratically elected governor of Puerto Rico. San Juan, Puerto Rico was commemorated with an 8-cent stamp on its 450th anniversary issued September 12, 1971, featuring a sentry box from Castillo San Felipe del Morro. In the "Flags of our nation series" 2008–2012, of the fifty-five, five territorial flags were featured. Forever stamps included the Puerto Rico Flag illustrated by a bird issued 2011.

Four Puerto Rican personalities have been featured on U.S. postage stamps. These include Roberto Clemente in 1984 as an individual and in the Legends of Baseball series issued in 2000. Luis Muñoz Marín in the Great Americans series, on February 18, 1990, Julia de Burgos in the Literary Arts series, issued 2010, and José Ferrer in the Distinguished American series, issued 2012.

Baseball was one of the first sports to gain widespread popularity in Puerto Rico. The Puerto Rico Baseball League serves as the only active professional league, operating as a winter league. No Major League Baseball franchise or affiliate plays in Puerto Rico, however, San Juan hosted the Montreal Expos for several series in 2003 and 2004 before they moved to Washington, D.C. and became the Washington Nationals.

The Puerto Rico national baseball team has participated in the World Cup of Baseball winning one gold (1951), four silver and four bronze medals, the Caribbean Series (winning fourteen times) and the World Baseball Classic. On , San Juan's Hiram Bithorn Stadium hosted the opening round as well as the second round of the newly formed World Baseball Classic. Puerto Rican baseball players include Hall of Famers Roberto Clemente, Orlando Cepeda and Roberto Alomar, enshrined in 1973, 1999, and 2011 respectively.

Boxing, basketball, and volleyball are considered popular sports as well. Wilfredo Gómez and McWilliams Arroyo have won their respective divisions at the World Amateur Boxing Championships. Other medalists include José Pedraza, who holds a silver medal, and three boxers who finished in third place, José Luis Vellón, Nelson Dieppa and McJoe Arroyo. In the professional circuit, Puerto Rico has the third-most boxing world champions and it is the global leader in champions per capita. These include Miguel Cotto, Félix Trinidad, Wilfred Benítez and Gómez among others.

The Puerto Rico national basketball team joined the International Basketball Federation in 1957. Since then, it has won more than 30 medals in international competitions, including gold in three FIBA Americas Championships and the 1994 Goodwill Games August 8, 2004, became a landmark date for the team when it became the first team to defeat the United States in an Olympic tournament since the integration of National Basketball Association players. Winning the inaugural game with scores of 92–73 as part of the 2004 Summer Olympics organized in Athens, Greece. Baloncesto Superior Nacional acts as the top-level professional basketball league in Puerto Rico, and has experienced success since its beginning in 1930.
Puerto Rico is also a member of FIFA and CONCACAF. In 2008, the archipelago's first unified league, the Puerto Rico Soccer League, was established.

Other sports include professional wrestling and road running. The World Wrestling Council and International Wrestling Association are the largest wrestling promotions in the main island. The World's Best 10K, held annually in San Juan, has been ranked among the 20 most competitive races globally. The "Puerto Rico All Stars" team, which has won twelve world championships in unicycle basketball.

Organized Streetball has gathered some exposition, with teams like "Puerto Rico Street Ball" competing against established organizations including the Capitanes de Arecibo and AND1's Mixtape Tour Team. Six years after the first visit, AND1 returned as part of their renamed Live Tour, losing to the Puerto Rico Streetballers. Consequently, practitioners of this style have earned participation in international teams, including Orlando "El Gato" Meléndez, who became the first Puerto Rican born athlete to play for the Harlem Globetrotters. Orlando Antigua, whose mother is Puerto Rican, in 1995 became the first Hispanic and the first non-black in 52 years to play for the Harlem Globetrotters.

Puerto Rico has representation in all international competitions including the Summer and Winter Olympics, the Pan American Games, the Caribbean World Series, and the Central American and Caribbean Games. Puerto Rico hosted the Pan Am Games in 1979 (officially in San Juan), and The Central American and Caribbean Games were hosted in 1993 in Ponce and in 2010 in Mayagüez.

Puerto Rican athletes have won nine medals in Olympic competition (one gold, two silver, six bronze), the first one in 1948 by boxer Juan Evangelista Venegas. Monica Puig won the first gold medal for Puerto Rico in the Olympic Games by winning the Women's Tennis singles title in Rio 2016.

Cities and towns in Puerto Rico are interconnected by a system of roads, freeways, expressways, and highways maintained by the Highways and Transportation Authority under the jurisdiction of the U.S. Department of Transportation, and patrolled by the Puerto Rico Police Department. The island's metropolitan area is served by a public bus transit system and a metro system called "Tren Urbano" (in English: Urban Train). Other forms of public transportation include seaborne ferries (that serve Puerto Rico's archipelago) as well as "Carros Públicos" (private mini buses).

Puerto Rico has three international airports, the Luis Muñoz Marín International Airport in Carolina, Mercedita Airport in Ponce, and the Rafael Hernández Airport in Aguadilla, and 27 local airports. The Luis Muñoz Marín International Airport is the largest aerial transportation hub in the Caribbean.
Puerto Rico has nine ports in different cities across the main island. The San Juan Port is the largest in Puerto Rico, and the busiest port in the Caribbean and the 10th busiest in the United States in terms of commercial activity and cargo movement, respectively. The second largest port is the Port of the Americas in Ponce, currently under expansion to increase cargo capacity to twenty-foot containers (TEUs) per year.

The Puerto Rico Electric Power Authority (PREPA)—Spanish: "Autoridad de Energía Eléctrica " (AEE)—is an electric power company and the government-owned corporation of Puerto Rico responsible for electricity generation, power transmission, and power distribution in Puerto Rico. PREPA is the only entity authorized to conduct such business in Puerto Rico, effectively making it a government monopoly. The Authority is ruled by a Governing Board appointed by the Governor with the advice and consent of the Senate of Puerto Rico, and is run by an Executive Director.

Telecommunications in Puerto Rico includes radio, television, fixed and mobile telephones, and the Internet. Broadcasting in Puerto Rico is regulated by the U.S. Federal Communications Commission (FCC). , there were 30 TV stations, 125 radio stations and roughly 1 million TV sets on the island. Cable TV subscription services are available and the U.S. Armed Forces Radio and Television Service also broadcast on the island.




Geography

United States government

United Nations (U.N.) Declaration on Puerto Rico


</doc>
<doc id="23042" url="https://en.wikipedia.org/wiki?curid=23042" title="Republic (disambiguation)">
Republic (disambiguation)

A republic is a form of government.

Republic(s) or The Republic may also refer to:
















</doc>
<doc id="23046" url="https://en.wikipedia.org/wiki?curid=23046" title="Placebo effect (disambiguation)">
Placebo effect (disambiguation)

Placebo effect may refer to:


</doc>
<doc id="23047" url="https://en.wikipedia.org/wiki?curid=23047" title="Pseudoscience">
Pseudoscience

Pseudoscience consists of statements, beliefs, or practices that are claimed to be both scientific and factual, but are incompatible with the scientific method. Pseudoscience is often characterized by contradictory, exaggerated or unfalsifiable claims; reliance on confirmation bias rather than rigorous attempts at refutation; lack of openness to evaluation by other experts; and absence of systematic practices when developing theories, and continued adherence long after they have been experimentally discredited. The term "pseudoscience" is considered pejorative because it suggests something is being presented as science inaccurately or even deceptively. Those described as practicing or advocating pseudoscience often dispute the characterization.

The demarcation between science and pseudoscience has philosophical and scientific implications. Differentiating science from pseudoscience has practical implications in the case of health care, expert testimony, environmental policies, and science education. Distinguishing scientific facts and theories from pseudoscientific beliefs, such as those found in astrology, alchemy, medical quackery, occult beliefs, and creation science, is part of science education and scientific literacy.

Pseudoscience can cause negative consequences in the real world. Antivaccine activists present pseudoscientific studies that falsely call into question the safety of vaccines. Homeopathic remedies with no active ingredients have been promoted for deadly diseases.

The word "pseudoscience" is derived from the Greek root "pseudo" meaning false and the English word "science", from the Latin word "scientia", meaning "knowledge". Although the term has been in use since at least the late 18th century (e.g. in 1796 by James Pettit Andrews in reference to alchemy) the concept of pseudoscience as distinct from real or proper science seems to have become more widespread during the mid-19th century. Among the earliest uses of "pseudo-science" was in an 1844 article in the "Northern Journal of Medicine", issue 387:
An earlier use of the term was in 1843 by the French physiologist François Magendie. During the 20th century, the word was used pejoratively to describe explanations of phenomena which were claimed to be scientific, but which were not in fact supported by reliable experimental evidence. From time-to-time, though, the usage of the word occurred in a more formal, technical manner in response to a perceived threat to individual and institutional security in a social and cultural setting.

Philosophers classify types of knowledge. In English, the word "science" is used to indicate specifically the natural sciences and related fields, which are called the social sciences.

Different philosophers of science may disagree on the exact limits – for example, is mathematics a formal science that is closer to the empirical ones, or is pure mathematics closer to the philosophical study of logic and therefore not a science? – but all agree that all of the ideas that are not scientific are non-scientific. The large category of non-science includes all matters outside the natural and social sciences, such as the study of history, metaphysics, religion, art, and the humanities.

Dividing the category again, unscientific claims are a subset of the large category of non-scientific claims. This category specifically includes all matters that are directly opposed to good science. Un-science includes both bad science (such as an error made in a good-faith attempt at learning something about the natural world) and pseudoscience. Thus pseudoscience is a subset of un-science, and un-science, in turn, is subset of non-science.

Pseudoscience is differentiated from science because – although it claims to be science – pseudoscience does not adhere to accepted scientific standards, such as the scientific method, falsifiability of claims, and Mertonian norms.

A number of basic principles are accepted by scientists as standards for determining whether a body of knowledge, method, or practice is scientific. Experimental results should be reproducible and verified by other researchers. These principles are intended to ensure experiments can be reproduced measurably given the same conditions, allowing further investigation to determine whether a hypothesis or theory related to given phenomena is valid and reliable. Standards require the scientific method to be applied throughout, and bias to be controlled for or eliminated through randomization, fair sampling procedures, blinding of studies, and other methods. All gathered data, including the experimental or environmental conditions, are expected to be documented for scrutiny and made available for peer review, allowing further experiments or studies to be conducted to confirm or falsify results. Statistical quantification of significance, confidence, and error are also important tools for the scientific method.

During the mid-20th century, the philosopher Karl Popper emphasized the criterion of falsifiability to distinguish science from nonscience. Statements, hypotheses, or theories have falsifiability or refutability if there is the inherent possibility that they can be proven false. That is, if it is possible to conceive of an observation or an argument which negates them. Popper used astrology and psychoanalysis as examples of pseudoscience and Einstein's theory of relativity as an example of science. He subdivided nonscience into philosophical, mathematical, mythological, religious and metaphysical formulations on one hand, and pseudoscientific formulations on the other, though he did not provide clear criteria for the differences.

Another example which shows the distinct need for a claim to be falsifiable was stated in Carl Sagan's publication "The Demon-Haunted World" when he discusses an invisible dragon that he has in his garage. The point is made that there is no physical test to refute the claim of the presence of this dragon. No matter what test you think you can devise, there is then a reason why this does not apply to the invisible dragon, so one can never prove that the initial claim is wrong. Sagan concludes; "Now, what's the difference between an invisible, incorporeal, floating dragon who spits heatless fire and no dragon at all?". He states that "your inability to invalidate my hypothesis is not at all the same thing as proving it true", once again explaining that even if such a claim were true, it would be outside the realm of scientific inquiry.

During 1942, Robert K. Merton identified a set of five "norms" which he characterized as what makes a real science. If any of the norms were violated, Merton considered the enterprise to be nonscience. These are not broadly accepted by the scientific community. His norms were:

During 1978, Paul Thagard proposed that pseudoscience is primarily distinguishable from science when it is less progressive than alternative theories over a long period of time, and its proponents fail to acknowledge or address problems with the theory. During 1983, Mario Bunge has suggested the categories of "belief fields" and "research fields" to help distinguish between pseudoscience and science, where the former is primarily personal and subjective and the latter involves a certain systematic method.

Philosophers of science such as Paul Feyerabend argued that a distinction between science and nonscience is neither possible nor desirable. Among the issues which can make the distinction difficult is variable rates of evolution among the theories and methods of science in response to new data.

Larry Laudan has suggested pseudoscience has no scientific meaning and is mostly used to describe our emotions: "If we would stand up and be counted on the side of reason, we ought to drop terms like 'pseudo-science' and 'unscientific' from our vocabulary; they are just hollow phrases which do only emotive work for us". Likewise, Richard McNally states, "The term 'pseudoscience' has become little more than an inflammatory buzzword for quickly dismissing one's opponents in media sound-bites" and "When therapeutic entrepreneurs make claims on behalf of their interventions, we should not waste our time trying to determine whether their interventions qualify as pseudoscientific. Rather, we should ask them: How do you know that your intervention works? What is your evidence?"

For philosophers Silvio Funtowicz and Jerome R. Ravetz "pseudo-science may be defined as one where the uncertainty of its inputs must be suppressed, lest they render its outputs totally indeterminate". The definition, in the book "Uncertainty and quality in science for policy" (p. 54), alludes to the loss of craft skills in handling quantitative information, and to the bad practice of achieving precision in prediction (inference) only at the expenses of ignoring uncertainty in the input which was used to formulate the prediction. This use of the term is common among practitioners of post-normal science. Understood in this way, pseudoscience can be fought using good practices to assesses uncertainty in quantitative information, such as NUSAP and – in the case of mathematical modelling – sensitivity auditing.

The history of pseudoscience is the study of pseudoscientific theories over time. A pseudoscience is a set of ideas that presents itself as science, while it does not meet the criteria to be properly called such.

Distinguishing between proper science and pseudoscience is sometimes difficult. One proposal for demarcation between the two is the falsification criterion, attributed most notably to the philosopher Karl Popper. In the history of science and the history of pseudoscience it can be especially difficult to separate the two, because some sciences developed from pseudosciences. An example of this transformation is the science chemistry, which traces its origins to pseudoscientific or pre-scientific study of alchemy.

The vast diversity in pseudosciences further complicates the history of science. Some modern pseudosciences, such as astrology and acupuncture, originated before the scientific era. Others developed as part of an ideology, such as Lysenkoism, or as a response to perceived threats to an ideology. Examples of this ideological process are creation science and intelligent design, which were developed in response to the scientific theory of evolution.

A topic, practice, or body of knowledge might reasonably be termed pseudoscientific when it is presented as consistent with the norms of scientific research, but it demonstrably fails to meet these norms.

Karl Popper stated it is insufficient to distinguish science from pseudoscience, or from metaphysics (such as the philosophical question of what existence means), by the criterion of rigorous adherence to the empirical method, which is essentially inductive, based on observation or experimentation. He proposed a method to distinguish between genuine empirical, nonempirical or even pseudoempirical methods. The latter case was exemplified by astrology, which appeals to observation and experimentation. While it had astonishing empirical evidence based on observation, on horoscopes and biographies, it crucially failed to use acceptable scientific standards. Popper proposed falsifiability as an important criterion in distinguishing science from pseudoscience.

To demonstrate this point, Popper gave two cases of human behavior and typical explanations from Sigmund Freud and Alfred Adler's theories: "that of a man who pushes a child into the water with the intention of drowning it; and that of a man who sacrifices his life in an attempt to save the child." From Freud's perspective, the first man would have suffered from psychological repression, probably originating from an Oedipus complex, whereas the second man had attained sublimation. From Adler's perspective, the first and second man suffered from feelings of inferiority and had to prove himself, which drove him to commit the crime or, in the second case, drove him to rescue the child. Popper was not able to find any counterexamples of human behavior in which the behavior could not be explained in the terms of Adler's or Freud's theory. Popper argued it was that the observation always fitted or confirmed the theory which, rather than being its strength, was actually its weakness.

In contrast, Popper gave the example of Einstein's gravitational theory, which predicted "light must be attracted by heavy bodies (such as the Sun), precisely as material bodies were attracted." Following from this, stars closer to the Sun would appear to have moved a small distance away from the Sun, and away from each other. This prediction was particularly striking to Popper because it involved considerable risk. The brightness of the Sun prevented this effect from being observed under normal circumstances, so photographs had to be taken during an eclipse and compared to photographs taken at night. Popper states, "If observation shows that the predicted effect is definitely absent, then the theory is simply refuted." Popper summed up his criterion for the scientific status of a theory as depending on its falsifiability, refutability, or testability.

Paul R. Thagard used astrology as a case study to distinguish science from pseudoscience and proposed principles and criteria to delineate them. First, astrology has not progressed in that it has not been updated nor added any explanatory power since Ptolemy. Second, it has ignored outstanding problems such as the precession of equinoxes in astronomy. Third, alternative theories of personality and behavior have grown progressively to encompass explanations of phenomena which astrology statically attributes to heavenly forces. Fourth, astrologers have remained uninterested in furthering the theory to deal with outstanding problems or in critically evaluating the theory in relation to other theories. Thagard intended this criterion to be extended to areas other than astrology. He believed it would delineate as pseudoscientific such practices as witchcraft and pyramidology, while leaving physics, chemistry and biology in the realm of science. Biorhythms, which like astrology relied uncritically on birth dates, did not meet the criterion of pseudoscience at the time because there were no alternative explanations for the same observations. The use of this criterion has the consequence that a theory can be scientific at one time and pseudoscientific at a later time.

Science is also distinguishable from revelation, theology, or spirituality in that it offers insight into the physical world obtained by empirical research and testing. The most notable disputes concern the evolution of living organisms, the idea of common descent, the geologic history of the Earth, the formation of the solar system, and the origin of the universe. Systems of belief that derive from divine or inspired knowledge are not considered pseudoscience if they do not claim either to be scientific or to overturn well-established science. Moreover, some specific religious claims, such as the power of intercessory prayer to heal the sick, although they may be based on untestable beliefs, can be tested by the scientific method.

Some statements and common beliefs of popular science may not meet the criteria of science. "Pop" science may blur the divide between science and pseudoscience among the general public, and may also involve science fiction. Indeed, pop science is disseminated to, and can also easily emanate from, persons not accountable to scientific methodology and expert peer review.

If the claims of a given field can be tested experimentally and standards are upheld, it is not pseudoscience, however odd, astonishing, or counterintuitive the claims are. If claims made are inconsistent with existing experimental results or established theory, but the method is sound, caution should be used, since science consists of testing hypotheses which may turn out to be false. In such a case, the work may be better described as ideas that are "not yet generally accepted". "Protoscience" is a term sometimes used to describe a hypothesis that has not yet been tested adequately by the scientific method, but which is otherwise consistent with existing science or which, where inconsistent, offers reasonable account of the inconsistency. It may also describe the transition from a body of practical knowledge into a scientific field.







A large percentage of the United States population lacks scientific literacy, not adequately understanding scientific principles and method. In the "Journal of College Science Teaching", Art Hobson writes, "Pseudoscientific beliefs are surprisingly widespread in our culture even among public school science teachers and newspaper editors, and are closely related to scientific illiteracy." However, a 10,000-student study in the same journal concluded there was no strong correlation between science knowledge and belief in pseudoscience.

In his book "The Demon-Haunted World" Carl Sagan discusses the government of China and the Chinese Communist Party's concern about Western pseudoscience developments and certain ancient Chinese practices in China. He sees pseudoscience occurring in the United States as part of a worldwide trend and suggests its causes, dangers, diagnosis and treatment may be universal.

During 2006, the U.S. National Science Foundation (NSF) issued an executive summary of a paper on science and engineering which briefly discussed the prevalence of pseudoscience in modern times. It said, "belief in pseudoscience is widespread" and, referencing a Gallup Poll, stated that belief in the 10 commonly believed examples of paranormal phenomena listed in the poll were "pseudoscientific beliefs".
The items were "extrasensory perception (ESP), that houses can be haunted, ghosts, telepathy, clairvoyance, astrology, that people can communicate mentally with someone who has died, witches, reincarnation, and channelling". Such beliefs in pseudoscience represent a lack of knowledge of how science works. The scientific community may attempt to communicate information about science out of concern for the public's susceptibility to unproven claims.

The National Science Foundation stated that pseudoscientific beliefs in the U.S. became more widespread during the 1990s, peaked about 2001, and then decreased slightly since with pseudoscientific beliefs remaining common. According to the NSF report, there is a lack of knowledge of pseudoscientific issues in society and pseudoscientific practices are commonly followed. Surveys indicate about a third of all adult Americans consider astrology to be scientific.

In a report Singer and Benassi (1981) wrote that pseudoscientific beliefs have their origin from at least four sources.

Another American study (Eve and Dunn, 1990) supported the findings of Singer and Benassi and found pseudoscientific belief being promoted by high school life science and biology teachers.

The psychology of pseudoscience attempts to explore and analyze pseudoscientific thinking by means of thorough clarification on making the distinction of what is considered scientific vs. pseudoscientific. The human proclivity for seeking confirmation rather than refutation (confirmation bias), the tendency to hold comforting beliefs, and the tendency to overgeneralize have been proposed as reasons for pseudoscientific thinking. According to Beyerstein (1991), humans are prone to associations based on resemblances only, and often prone to misattribution in cause-effect thinking.

Michael Shermer's theory of belief-dependent realism is driven by the belief that the brain is essentially a "belief engine," which scans data perceived by the senses and looks for patterns and meaning. There is also the tendency for the brain to create cognitive biases, as a result of inferences and assumptions made without logic and based on instinct — usually resulting in patterns in cognition. These tendencies of patternicity and agenticity are also driven "by a meta-bias called the bias blind spot, or the tendency to recognize the power of cognitive biases in other people but to be blind to their influence on our own beliefs."
Lindeman states that social motives (i.e., "to comprehend self and the world, to have a sense of control over outcomes, to belong, to find the world benevolent and to maintain one's self-esteem") are often "more easily" fulfilled by pseudoscience than by scientific information. Furthermore, pseudoscientific explanations are generally not analyzed rationally, but instead experientially. Operating within a different set of rules compared to rational thinking, experiential thinking regards an explanation as valid if the explanation is "personally functional, satisfying and sufficient", offering a description of the world that may be more personal than can be provided by science and reducing the amount of potential work involved in understanding complex events and outcomes.

There is a trend to believe in pseudoscience more than scientific evidence. Some people believe the prevalence of pseudoscientific beliefs is due to widespread "scientific illiteracy". Individuals lacking scientific literacy are more susceptible to wishful thinking, since they are likely to turn to immediate gratification powered by System 1, our default operating system which requires little to no effort. This system encourages one to accept the conclusions they believe, and reject the ones they do not. Further analysis of complex pseudoscientific phenomena require System 2, which follows rules, compares objects along multiple dimensions and weighs options. These two systems have several other differences which are further discussed in the dual-process theory. The scientific and secular systems of morality and meaning are generally unsatisfying to most people. Humans are, by nature, a forward-minded species pursuing greater avenues of happiness and satisfaction, but we are all too frequently willing to grasp at unrealistic promises of a better life.

Psychology has much to discuss about pseudoscience thinking, as it is the illusory perceptions of causality and effectiveness of numerous individuals that needs to be illuminated. Research suggests that illusionary thinking happens in most people when exposed to certain circumstances such as reading a book, an advertisement or the testimony of others are the basis of pseudoscience beliefs. It is assumed that illusions are not unusual, and given the right conditions, illusions are able to occur systematically even in normal emotional situations. One of the things pseudoscience believers quibble most about is that academic science usually treats them as fools. Minimizing these illusions in the real world is not simple. To this aim, designing evidence-based educational programs can be effective to help people identify and reduce their own illusions.

In the philosophy and history of science, Imre Lakatos stresses the social and political importance of the demarcation problem, the normative methodological problem of distinguishing between science and pseudoscience. His distinctive historical analysis of scientific methodology based on research programmes suggests: "scientists regard the successful theoretical prediction of stunning novel facts – such as the return of Halley's comet or the gravitational bending of light rays – as what demarcates good scientific theories from pseudo-scientific and degenerate theories, and in spite of all scientific theories being forever confronted by 'an ocean of counterexamples'". Lakatos offers a "novel fallibilist analysis of the development of Newton's celestial dynamics, [his] favourite historical example of his methodology" and argues in light of this historical turn, that his account answers for certain inadequacies in those of Karl Popper and Thomas Kuhn. "Nonetheless, Lakatos did recognize the force of Kuhn's historical criticism of Popper – all important theories have been surrounded by an 'ocean of anomalies', which on a falsificationist view would require the rejection of the theory outright... Lakatos sought to reconcile the rationalism of Popperian falsificationism with what seemed to be its own refutation by history".

The boundary between science and pseudoscience is disputed and difficult to determine analytically, even after more than a century of study by philosophers of science and scientists, and despite some basic agreements on the fundamentals of the scientific method. The concept of pseudoscience rests on an understanding that the scientific method has been misrepresented or misapplied with respect to a given theory, but many philosophers of science maintain that different kinds of methods are held as appropriate across different fields and different eras of human history. According to Lakatos, the typical descriptive unit of great scientific achievements is not an isolated hypothesis but "a powerful problem-solving machinery, which, with the help of sophisticated mathematical techniques, digests anomalies and even turns them into positive evidence."

The demarcation problem between science and pseudoscience brings up debate in the realms of science, philosophy and politics. Imre Lakatos, for instance, points out that the Communist Party of the Soviet Union at one point declared that Mendelian genetics was pseudoscientific and had its advocates, including well-established scientists such as Nikolai Vavilov, sent to a Gulag and that the "liberal Establishment of the West" denies freedom of speech to topics it regards as pseudoscience, particularly where they run up against social mores.

It becomes pseudoscientific when science cannot be separated from ideology, scientists misrepresent scientific findings to promote or draw attention for publicity, when politicians, journalists and a nation's intellectual elite distort the facts of science for short-term political gain, or when powerful individuals of the public conflate causation and cofactors by clever wordplay. These ideas reduce the authority, value, integrity and independence of science in society.

Distinguishing science from pseudoscience has practical implications in the case of health care, expert testimony, environmental policies, and science education. Treatments with a patina of scientific authority which have not actually been subjected to actual scientific testing may be ineffective, expensive and dangerous to patients and confuse health providers, insurers, government decision makers and the public as to what treatments are appropriate. Claims advanced by pseudoscience may result in government officials and educators making bad decisions in selecting curricula.

The extent to which students acquire a range of social and cognitive thinking skills related to the proper usage of science and technology determines whether they are scientifically literate. Education in the sciences encounters new dimensions with the changing landscape of science and technology, a fast-changing culture and a knowledge-driven era. A reinvention of the school science curriculum is one that shapes students to contend with its changing influence on human welfare. Scientific literacy, which allows a person to distinguish science from pseudosciences such as astrology, is among the attributes that enable students to adapt to the changing world. Its characteristics are embedded in a curriculum where students are engaged in resolving problems, conducting investigations, or developing projects.

Friedman mentions why most scientists avoid educating about pseudoscience, including that paying undue attention to pseudoscience could dignify it. On the other hand, Park emphasizes how pseudoscience can be a threat to society and considers that scientists have a responsibility to teach how to distinguish science from pseudoscience.

Pseudosciences such as homeopathy, even if generally benign, are used by charlatans. This poses a serious issue because it enables incompetent practitioners to administer health care. True-believing zealots may pose a more serious threat than typical con men because of their affection to homeopathy's ideology. Irrational health care is not harmless and it is careless to create patient confidence in pseudomedicine.

On December 8, 2016, Michael V. LeVine, writing in "Business Insider", pointed out the dangers posed by the "Natural News" website: "Snake-oil salesmen have pushed false cures since the dawn of medicine, and now websites like "Natural News" flood social media with dangerous anti-pharmaceutical, anti-vaccination and anti-GMO pseudoscience that puts millions at risk of contracting preventable illnesses."






</doc>
<doc id="23048" url="https://en.wikipedia.org/wiki?curid=23048" title="Prion">
Prion

Prions are misfolded proteins that are associated with several fatal neurodegenerative diseases in animals and humans. It is not known what causes the normal prion protein to misfold; the abnormal 3-D structure is suspected to confer infectious properties. The word prion derives from ‘"proteinaceous infectious particle"’. Prions composed of the prion protein (PrP) are hypothesized as the cause of transmissible spongiform encephalopathies (TSEs), including scrapie in sheep and bovine spongiform encephalopathy (BSE) in cattle—known popularly as "mad cow disease". In humans, prions have been hypothesized as the cause of Creutzfeldt–Jakob disease (CJD) and its variant (vCJD), Gerstmann–Sträussler–Scheinker syndrome, “fatal familial insomnia”, and kuru. All known prion diseases in mammals affect the structure of the brain or other neural tissue; all are progressive, have no known effective treatment, and are always fatal. Multiple system atrophy (MSA), a rare human neurodegenerative disease, features a misfolded version of a protein called alpha-synuclein, and is therefore also classifiable as a prion disease. Several yeast proteins have also been identified as having prionogenic properties.

The hypothesized role of a protein as an infectious agent stands in contrast to all other known infectious agents such as viruses, bacteria, fungi, and parasites, all of which contain nucleic acids (DNA, RNA, or both). Synthetic prions, created in the laboratory independent of any biological source, have little or no ability to cause infection with TSEs. However, when synthetic prions are administered in combination with cofactors, such as phosphatidylethanolamine and RNA molecules, then this can transmit TSEs. Several scientific observations remain unexplained by the prion hypothesis: It is known that mice with severe combined immunodeficiency do not develop scrapie following inoculation with brain tissue from animals infected with scrapie, suggesting that either the role of immunity in prion pathogenesis is incompletely understood or that there is some other flaw in current understanding of prion pathophysiology. More recently, it has been shown that scrapie and Creutzfeld-Jacob Disease 
may require agent specific nucleic acids for transmission of infection. For these reasons, the prion / TSE hypothesis incompletely accounts for the observed data.

In yeast, protein refolding to the prion configuration is assisted by chaperone proteins such as Hsp104. All known prions induce the formation of an amyloid fold, in which the protein polymerises into an aggregate consisting of tightly packed beta sheets. Amyloid aggregates are fibrils, growing at their ends, and replicate when breakage causes two growing ends to become four growing ends. The incubation period of prion diseases is determined by the exponential growth rate associated with prion replication, which is a balance between the linear growth and the breakage of aggregates.

Prion aggregates are stable, accumulate in infected tissue, and are associated with tissue damage and cell death. This structural stability means that prions are resistant to denaturation by chemical and physical agents, making disposal and containment of these particles difficult. Prion structure varies slightly between species, but nonetheless prion replication is subject to epimutation and natural selection just like other forms of replication.

During the 1960s, two London-based researchers, radiation biologist Tikvah Alper and biophysicist John Stanley Griffith, developed the hypothesis that the transmissible spongiform encephalopathies are caused by an infectious agent consisting solely of proteins. Earlier investigations by E. J. Field into scrapie and kuru had found evidence that the transfer of pathologically inert polysaccharides that only become infectious in the host. Alper and Griffith wanted to account for the discovery that the mysterious infectious agent causing the diseases scrapie and Creutzfeldt–Jakob disease resisted ionizing radiation. (A single ionizing "hit" normally destroys an entire infectious particle, and the dose needed to hit half the particles depends on the size of the particles. Empirical results of ionizing doses applied to the unknown infectious substance evidenced an infectious particle size too small to be a viral mechanism.) In his paper, entitled "Does the agent of Scrapie replicate without nucleic acid", Griffith proposed 3 ways in which a protein could be a pathogen. In the first hypothesis, he suggested that if the protein is the product of a normally suppressed gene, and introducing the protein could induce the gene's expression, that is, wake the dormant gene up, then the result would be a process indistinguishable from replication, as the gene's expression would produce the protein, which would then go wake the gene up in other cells. His second hypothesis forms the basis of the modern Prion theory, and proposed that an abnormal form of a cellular protein can convert normal proteins of the same type into its abnormal form, thus leading to replication. His third hypothesis proposed that the agent could be an antibody if the antibody was its own target antigen, as such an antibody would result in more and more antibody being produced against itself. However, Griffith acknowlaged that this third hypothesis was unlikely to be true due to the lack of a detectable immune response.

Francis Crick recognized the potential significance of the Griffith protein-only hypothesis for scrapie propagation in the second edition of his "Central dogma of molecular biology" (1970): While asserting that the flow of sequence information from protein to protein, or from protein to RNA and DNA was "precluded", he noted that Griffith's hypothesis was a potential contradiction (although it was not so promoted by Griffith). The revised hypothesis was later formulated, in part, to accommodate reverse transcription (which both Howard Temin and David Baltimore discovered in 1970).

In 1982, Stanley B. Prusiner of the University of California, San Francisco announced that his team had purified the hypothetical infectious protein, which did not appear to be present in healthy hosts, though they did not manage to isolate the protein until two years after Prusiner's announcement. The protein was named a prion, for "Proteinacious infectious particle", taken from the words protein and infection. When the Prion was first discovered, Griffith's first hypothesis that the protein was the product of a normally silent gene was favored by many, however, it was subsiquently discovered that the same protein exists in normal hosts but in different form. 
Following the discovery of the same protein in different form in uninfected induviduals, the specific protein that the prion was composed of was named the Prion Protein (PrP), and Griffith's second hypothesis: that an ablormal form of a host protein can convert other proteins of the same type into it's abnormal form, became the dominant theory. Prusiner won the Nobel Prize in Physiology or Medicine in 1997 for his research into prions.

The protein that prions are made of (PrP) is found throughout the body, even in healthy people and animals. However, PrP found in infectious material has a different structure and is resistant to proteases, the enzymes in the body that can normally break down proteins. The normal form of the protein is called PrP, while the infectious form is called PrP — the "C" refers to 'cellular' PrP, while the "Sc" refers to 'scrapie', the prototypic prion disease, occurring in sheep. While PrP is structurally well-defined, PrP is certainly polydisperse and defined at a relatively poor level. PrP can be induced to fold into other more-or-less well-defined isoforms in vitro, and their relationship to the form(s) that are pathogenic in vivo is not yet clear.

PrP is a normal protein found on the membranes of cells. It has 209 amino acids (in humans), one disulfide bond, a molecular mass of 35–36 kDa and a mainly alpha-helical structure. Several topological forms exist; one cell surface form anchored via glycolipid and two transmembrane forms. The normal protein is not sedimentable; meaning that it cannot be separated by centrifuging techniques. Its function is a complex issue that continues to be investigated. PrP binds copper (II) ions with high affinity. The significance of this finding is not clear, but it is presumed to relate to PrP structure or function. PrP is readily digested by proteinase K and can be liberated from the cell surface in vitro by the enzyme phosphoinositide phospholipase C (PI-PLC), which cleaves the glycophosphatidylinositol (GPI) glycolipid anchor. PrP has been reported to play important roles in cell-cell adhesion and intracellular signaling "in vivo", and may therefore be involved in cell-cell communication in the brain.

Protease-resistant PrP-like protein (PrP) is an isoform of PrP from which is structurally altered and converted into a misfolded proteinase K-resistant form "in vitro". To model conversion of PrP to PrP in vitro, Saborio "et al". rapidly converted PrP into a PrP by a procedure involving cyclic amplification of protein misfolding. The term "PrP" has been made to distinguish between PrP, which is isolated from infectious tissue and associated with the transmissible spongiform encephalopathy agent. For example, unlike PrP, PrP may not necessarily be infectious.

The infectious isoform of PrP, known as PrP, is able to convert normal PrP proteins into the infectious isoform by changing their conformation, or shape; this, in turn, alters the way the proteins interconnect. PrP always causes prion disease. Although the exact 3D structure of PrP is not known, it has a higher proportion of β-sheet structure in place of the normal α-helix structure. Aggregations of these abnormal isoforms form highly structured amyloid fibers, which accumulate to form plaques. It is unclear as to whether these aggregates are the cause of cell damage or are simply a side-effect of the underlying disease process. The end of each fiber acts as a template onto which free protein molecules may attach, allowing the fiber to grow. Under most circumstances, only PrP molecules with an identical amino acid sequence to the infectious PrP are incorporated into the growing fiber. However, rare cross-species transmission is also possible.

The physiological function of the prion protein remains poorly understood. While data from in vitro experiments suggest many dissimilar roles, studies on PrP knockout mice have provided only limited information because these animals exhibit only minor abnormalities. In research done in mice, it was found that the cleavage of PrP proteins in peripheral nerves causes the activation of myelin repair in Schwann cells and that the lack of PrP proteins caused demyelination in those cells.

A review of evidence in 2005 suggested that PrP may have a normal function in maintenance of long-term memory. As well, a 2004 study found that mice lacking genes for normal cellular PrP protein show altered hippocampal long-term potentiation.

A 2006 article from the Whitehead Institute for Biomedical Research indicates that PrP expression on stem cells is necessary for an organism's self-renewal of bone marrow. The study showed that all long-term hematopoietic stem cells express PrP on their cell membrane and that hematopoietic tissues with PrP-null stem cells exhibit increased sensitivity to cell depletion.

The first hypothesis that tried to explain how prions replicate in a protein-only manner was the heterodimer model. This model assumed that a single PrP molecule binds to a single PrP molecule and catalyzes its conversion into PrP. The two PrP molecules then come apart and can go on to convert more PrP. However, a model of prion replication must explain both how prions propagate, and why their spontaneous appearance is so rare. Manfred Eigen showed that the heterodimer model requires PrP to be an extraordinarily effective catalyst, increasing the rate of the conversion reaction by a factor of around 10. This problem does not arise if PrP exists only in aggregated forms such as amyloid, where cooperativity may act as a barrier to spontaneous conversion. What is more, despite considerable effort, infectious monomeric PrP has never been isolated.

An alternative model assumes that PrP exists only as fibrils, and that fibril ends bind PrP and convert it into PrP. If this were all, then the quantity of prions would increase linearly, forming ever longer fibrils. But exponential growth of both PrP and of the quantity of infectious particles is observed during prion disease. This can be explained by taking into account fibril breakage. A mathematical solution for the exponential growth rate resulting from the combination of fibril growth and fibril breakage has been found. The exponential growth rate depends largely on the square root of the PrP concentration. The incubation period is determined by the exponential growth rate, and in vivo data on prion diseases in transgenic mice match this prediction. The same square root dependence is also seen in vitro in experiments with a variety of different amyloid proteins.

The mechanism of prion replication has implications for designing drugs. Since the incubation period of prion diseases is so long, an effective drug does not need to eliminate all prions, but simply needs to slow down the rate of exponential growth. Models predict that the most effective way to achieve this, using a drug with the lowest possible dose, is to find a drug that binds to fibril ends and blocks them from growing any further.

Until 2015 all known mammalian prion diseases were considered to be caused by the prion protein, PrP; in 2015 Multiple System Atrophy was found to be transmissible and was hypothesized to be caused by a new prion, the misfolded form of a protein called alpha-synuclein. The endogenous, properly folded form of the prion protein is denoted PrP (for Common" or Cellular"), whereas the disease-linked, misfolded form is denoted PrP (for "Scrapie"), after one of the diseases first linked to prions and neurodegeneration.) The precise structure of the prion is not known, though they can be formed by combining PrP, polyadenylic acid, and lipids in a protein misfolding cyclic amplification (PMCA) reaction. Proteins showing prion-type behavior are also found in some fungi, which has been useful in helping to understand mammalian prions. Fungal prions do not appear to cause disease in their hosts.

Prions cause neurodegenerative disease by aggregating extracellularly within the central nervous system to form plaques known as amyloid, which disrupt the normal tissue structure. This disruption is characterized by "holes" in the tissue with resultant spongy architecture due to the vacuole formation in the neurons. Other histological changes include astrogliosis and the absence of an inflammatory reaction. While the incubation period for prion diseases is relatively long (5 to 20 years), once symptoms appear the disease progresses rapidly, leading to brain damage and death. Neurodegenerative symptoms can include convulsions, dementia, ataxia (balance and coordination dysfunction), and behavioural or personality changes.

All known prion diseases are untreatable and fatal. However, a vaccine developed in mice may provide insight into providing a vaccine to resist prion infections in humans. Additionally, in 2006 scientists announced that they had genetically engineered cattle lacking a necessary gene for prion production – thus theoretically making them immune to BSE, building on research indicating that mice lacking normally occurring prion protein are resistant to infection by scrapie prion protein. In 2013, a study revealed that 1 in 2,000 people in the United Kingdom might harbour the infectious prion protein that causes vCJD.

Many different mammalian species can be affected by prion diseases, as the prion protein (PrP) is very similar in all mammals. Due to small differences in PrP between different species it is unusual for a prion disease to transmit from one species to another. The human prion disease variant Creutzfeldt–Jakob disease, however, is thought to be caused by a prion that typically infects cattle, causing Bovine spongiform encephalopathy and is transmitted through infected meat.

It has been recognized that prion diseases can arise in three different ways: acquired, familial, or sporadic. It is often assumed that the diseased form directly interacts with the normal form to make it rearrange its structure. One idea, the "Protein X" hypothesis, is that an as-yet unidentified cellular protein (Protein X) enables the conversion of PrP to PrP by bringing a molecule of each of the two together into a complex.

Current research suggests that the primary method of infection in animals is through ingestion. It is thought that prions may be deposited in the environment through the remains of dead animals and via urine, saliva, and other body fluids. They may then linger in the soil by binding to clay and other minerals.

A University of California research team, led by Nobel Prize winner Stanley Prusiner, has provided evidence for the theory that infection can occur from prions in manure. And, since manure is present in many areas surrounding water reservoirs, as well as used on many crop fields, it raises the possibility of widespread transmission. It was reported in January 2011 that researchers had discovered prions spreading through airborne transmission on aerosol particles, in an animal testing experiment focusing on scrapie infection in laboratory mice. Preliminary evidence supporting the notion that prions can be transmitted through use of urine-derived human menopausal gonadotropin, administered for the treatment of infertility, was published in 2011.

In 2015, researchers at The University of Texas Health Science Center at Houston found that plants can be a vector for prions. When researchers fed hamsters grass that grew on ground where a deer that died with chronic wasting disease (CWD) was buried, the hamsters became ill with CWD, suggesting that prions can bind to plants, which then take them up into the leaf and stem structure, where they can be eaten by herbivores, thus completing the cycle. It is thus possible that there is a progressively accumulating number of prions in the environment.

Infectious particles possessing nucleic acid are dependent upon it to direct their continued replication. Prions, however, are infectious by their effect on normal versions of the protein. Sterilizing prions, therefore, requires the denaturation of the protein to a state in which the molecule is no longer able to induce the abnormal folding of normal proteins. In general, prions are quite resistant to proteases, heat, ionizing radiation, and formaldehyde treatments, although their infectivity can be reduced by such treatments. Effective prion decontamination relies upon protein hydrolysis or reduction or destruction of protein tertiary structure. Examples include sodium hypochlorite, sodium hydroxide, and strongly acidic detergents such as LpH. 134 °C (274 °F) for 18 minutes in a pressurized steam autoclave has been found to be somewhat effective in deactivating the agent of disease. Ozone sterilization is currently being studied as a potential method for prion denaturation and deactivation. Renaturation of a completely denatured prion to infectious status has not yet been achieved; however, partially denatured prions can be renatured to an infective status under certain artificial conditions.

The World Health Organization recommends any of the following three procedures for the sterilization of all heat-resistant surgical instruments to ensure that they are not contaminated with prions:


Fungal proteins exhibiting templated conformational change were discovered in the yeast "Saccharomyces cerevisiae" by Reed Wickner in the early 1990s. For their mechanistic similarity to mammalian prions, they were termed yeast prions. Subsequent to this, a prion has also been found in the fungus "Podospora anserina". These prions behave similarly to PrP, but, in general, are nontoxic to their hosts. Susan Lindquist's group at the Whitehead Institute has argued some of the fungal prions are not associated with any disease state, but may have a useful role; however, researchers at the NIH have also provided arguments suggesting that fungal prions could be considered a diseased state. There is mounting evidence that fungal proteins have evolved specific functions that are beneficial to the microorganism that enhance their ability to adapt to their diverse environments.

, there are eight known prion proteins in fungi, seven in "Saccharomyces cerevisiae" (Sup35, Rnq1, Ure2, Swi1, Mot3, Cyc8, and Mod5) and one in "Podospora anserina" (HET-s). The article that reported the discovery of a prion form, the Mca1 protein, was retracted due to the fact that the data could not be reproduced. Notably, most of the fungal prions are based on glutamine/asparagine-rich sequences, with the exception of HET-s and Mod5.

Research into fungal prions has given strong support to the protein-only concept, since purified protein extracted from cells with a prion state has been demonstrated to convert the normal form of the protein into a misfolded form "in vitro", and in the process, preserve the information corresponding to different strains of the prion state. It has also shed some light on prion domains, which are regions in a protein that promote the conversion into a prion. Fungal prions have helped to suggest mechanisms of conversion that may apply to all prions, though fungal prions appear distinct from infectious mammalian prions in the lack of cofactor required for propagation. The characteristic prion domains may vary between species—e.g., characteristic fungal prion domains are not found in mammalian prions.

Advancements in computer modeling have allowed scientists to identify compounds that can treat prion-caused diseases, such as one compound found to bind a cavity in the PrP and stabilize the conformation, reducing the amount of harmful PrP.

Antiprion antibodies capable of crossing the blood-brain-barrier and targeting cytosolic prion protein (an otherwise major obstacle in prion therapeutics) have been described.

In the last decade, some progress dealing with ultra-high-pressure inactivation of prion infectivity in processed meat has been reported.

In 2011, it was discovered that prions could be degraded by lichens.

Astemizole has been found to have anti-prion activity.

Another type of chemical that may be effective against prion infection is the luminescent conjugated polythiophenes, fluorescent compounds that are often used to stain tissue samples. In a 2015 study, led by Adriano Aguzzi, professor of neurobiology at the University of Zurich, found that when they injected mice with a prion disease and then with polythiophenes, the mice survived eighty percent longer than the control mice that were only injected with the prion disease.

There continues to be a very practical problem with diagnosis of prion diseases, including BSE and CJD. They have an incubation period of months to decades, during which there are no symptoms, even though the pathway of converting the normal brain PrP protein into the toxic, disease-related PrP form has started. At present, there is virtually no way to detect PrP reliably except by examining the brain using neuropathological and immunohistochemical methods after death. Accumulation of the abnormally folded PrP form of the PrP protein is a characteristic of the disease, but it is present at very low levels in easily accessible body fluids like blood or urine. Researchers have tried to develop methods to measure PrP, but there are still no fully accepted methods for use in materials such as blood.

In 2010, a team from New York described detection of PrP even when initially present at only one part in a hundred billion (10) in brain tissue. The method combines amplification with a novel technology called Surround Optical Fiber Immunoassay (SOFIA) and some specific antibodies against PrP. After amplifying and then concentrating any PrP, the samples are labelled with a fluorescent dye using an antibody for specificity and then finally loaded into a micro-capillary tube. This tube is placed in a specially constructed apparatus so that it is totally surrounded by optical fibres to capture all light emitted once the dye is excited using a laser.

The RT-QuIC assay, a microplate reader-based prion detection method which uses as reagents normally folded prions, fluorescently labelled so that they "light up" when they are misfolded; samples suspected of containing misfolded prions are added and misfolded reagents can be detected by standard fluorescence detection methods.

The cause of the Transmissible spongiform encephalopathies (TSE) is currently unknown, but the diseases are known to be associated with prions. Whether prions cause TSEs or are the result of infection with another agent such as a virus is a matter of debate by a minority of scientists. The following are some hypotheses.

The prion hypothesis states that the main component of the TSE agent is composed of a misfolded protein. The Prion hypothesis can be devided into to subhypothesis: the protein only hypothesis, and the multi-component hypothesis.

Prior to the discovery of prions, it was thought that all pathogens used nucleic acids to direct their replication. The "protein-only hypothesis" states that a protein structure can replicate without the use of nucleic acids. This was initially controversial as it contradicts the central dogma of molecular biology, which describes nucleic acid as the central form of replicative information.

Evidence in favor of a protein-only hypothesis includes:

A gene for the normal protein has been identified: the "PRNP" gene. In all inherited cases of prion disease, there is a mutation in the "PRNP" gene. Many different "PRNP" mutations have been identified and these proteins are more likely to fold into abnormal prion. Although this discovery puts a hole in the general prion hypothesis, that prions can aggregate only proteins of identical amino acid make-up. These mutations can occur throughout the gene. Some mutations involve expansion of the octapeptide repeat region at the N-terminal of PrP. Other mutations that have been identified as a cause of inherited prion disease occur at positions 102, 117 & 198 (GSS), 178, 200, 210 & 232 (CJD) and 178 (Fatal Familial Insomnia, FFI). The cause of prion disease can be sporadic, genetic, or infectious, or a combination of these factors. For example, to have scrapie, both an infectious agent and a susceptible genotype must be present.

Despite much effort, significant titers of prion infectivity have never been produced by refolding pure PrP molecules, raising doubt about the validity of the "protein only" hypothesis. In addition, the "protein only" hypothesis fails to provide a molecular explanation for the ability of prion strains to target specific areas of the brain in distinct patterns. These shortcomings, along with additional experimental data, have given rise to the "multi-component" or "cofactor variation" hypothesis.

In 2007, biochemist Surachai Supattapone and his colleagues at Dartmouth College produced purified infectious prions "de novo" from defined components (PrP, co-purified lipids, and a synthetic polyanionic molecule). These researchers also showed that the polyanionic molecule required for prion formation was selectively incorporated into high-affinity complexes with PrP molecules, leading them to hypothesize that infectious prions may be composed of multiple host components, including PrP, lipid, and polyanionic molecules, rather than PrP alone.

In 2010, Jiyan Ma and colleagues at the Ohio State University produced infectious prions from a recipe of bacterially expressed recombinant PrP, POPG phospholipid, and RNA, further supporting the multi-component hypothesis. This finding is in contrast to studies that found minimally infectious prions produced from recombinant PrP alone.

In 2012, Supattapone and colleagues purified the membrane lipid phosphatidylethanolamine as a solitary endogenous cofactor capable of facilitating the formation of high-titer recombinant prions derived from multiple prion strains. They also reported that the cofactor is essential for maintaining the infectious conformation of PrP, and that cofactor molecules dictate the strain properties of infectious prions.

The following are some of the current difficulties and challenges:





Whether prions cause disease or are merely a symptom caused by a different agent is still a matter of debate and research. The following sections describe several hypotheses: some pertain to the composition of the infectious agent (protein-only, protein with other components, virus, or other), while others pertain to its mechanism of reproduction.

Reports suggest that imbalance of brain metal homeostasis may be a cause of PrP-associated neurotoxicity, though the underlying mechanisms are difficult to explain based on existing information. Proposed hypotheses include a functional role for PrP in metal metabolism, and loss of this function due to aggregation to the disease-associated PrP form as the cause of brain metal imbalance. Other views suggest gain of toxic function by PrP due to sequestration of PrP-associated metals within the aggregates, resulting in the generation of redox-active PrP complexes. The physiological implications of some PrP-metal interactions are known, while others are still unclear. The pathological implications of PrP-metal interaction include metal-induced oxidative damage, and in some instances conversion of PrP to a PrP-like form.

The protein-only hypothesis has been criticised by those maintaining that the simplest explanation of the evidence to date is viral. For more than a decade, Yale University neuropathologist Laura Manuelidis has been proposing that prion diseases are caused instead by an unidentified slow virus. In January 2007, she and her colleagues published an article reporting to have found a virus in 10%, or less, of their scrapie-infected cells in culture. In 2016, Sotirios Botsios and Laura Manuelidis showed evidence that TSE specific nucleic acids may be required for infectious transmission of CJD and Scrapie.

Evidence in favor of a viral hypothesis includes:

Studies propagating TSE infectivity in cell-free reactions and in purified component chemical reactions is thought to strongly suggest against TSE viral nature. However, some viruses, such as Poliovirus, have the ability to replicate in cell-free reactions.

The 'virino hypothesis' postulates that the TSE agent is a foreign, self replicating nucleic acid or nucleic acid fragment bound to PrP.

"Spiroplasma" is a cell wall–deficient bacterium related to "Mycoplasma", which some think may be the cause of the TSEs. The lack of a cell wall means it is not susceptible to conventional antibiotics such as penicillin, which target cell wall synthesis. Frank O. Bastian of Louisiana State University first discovered "Spiroplasma"-like inclusions in the brain of a CJD patient during an autopsy in 1979 and has hypothesized that this bacterium could possibly be the cause of the TSEs.

However, , with the exception of "Spiroplasma mirum" strain SMCA causing spongiform microcystic encephalitis in suckling rats, other researchers have been unable to duplicate these findings, casting doubt on the "Spiroplasma" hypothesis. In defense of the "Spiroplasma" hypothesis, Bastian pointed out that "Spiroplasma" is hard to culture and that strain variation makes it hard to detect certain strains using PCR and other techniques, thus giving a false negative.

"Acinetobacter" is a bacterium which some think is the cause of the TSEs. Mainly because some CJD patients produce antibodies against "Acinetobacter calcoaceticus".

Prion-like domains have been found in a variety of other mammalian proteins. Some of these proteins have been implicated in the ontogeny of age-related neurodegenerative disorders such as amyotrophic lateral sclerosis (ALS, known as Motor Neurone Disease outside the US), frontotemporal lobar degeneration with ubiquitin-positive inclusions (FTLD-U), Alzheimer's disease, and Huntington's disease, as well as some forms of Systemic Amyloidosis including AA (Secondary) Amyloidosis that develops in humans and animals with inflammatory and infectious diseases such as Tuberculosis, Crohn's disease, Rheumatoid arthritis, and HIV AIDS. AA amyloidosis, like prion disease, may be transmissible. This has given rise to the 'prion paradigm', where otherwise harmless proteins can be converted to a pathogenic form by a small number of misfolded, nucleating proteins.

The definition of a prion-like domain arises from the study of fungal prions. In yeast, prionogenic proteins have a portable prion domain that is both necessary and sufficient for self-templating and protein aggregation. This has been shown by attaching the prion domain to a reporter protein, which then aggregates like a known prion. Similarly, removing the prion domain from a fungal prion protein inhibits prionogenesis. This modular view of prion behaviour has led to the hypothesis that similar prion domains are present in animal proteins, in addition to PrP. These fungal prion domains have several characteristic sequence features. They are typically enriched in asparagine, glutamine, tyrosine and glycine residues, with an asparagine bias being particularly conducive to the aggregative property of prions. Historically, prionogenesis has been seen as independent of sequence and only dependent on relative residue content. However, this has been shown to be false, with the spacing of prolines and charged residues having been shown to be critical in amyloid formation.

Bioinformatic screens have predicted that over 250 human proteins contain prion-like domains (PrLD). These domains are hypothesized to have the same transmissible, amyloidogenic properties of PrP and known fungal proteins. As in yeast, proteins involved in gene expression and RNA binding seem to be particularly enriched in PrLD's, compared to other classes of protein. In particular, 29 of the known 210 proteins with an RNA recognition motif also have a putative prion domain. Meanwhile, several of these RNA-binding proteins have been independently identified as pathogenic in cases of ALS, FTLD-U, Alzheimer's disease, and Huntington's disease.

The pathogenicity of prions and proteins with prion-like domains is hypothesized to arise from their self-templating ability and the resulting exponential growth of amyloid fibrils. The presence of amyloid fibrils in patients with degenerative diseases has been well documented. These amyloid fibrils are seen as the result of pathogenic proteins that self-propagate and form highly stable, non-functional aggregates. While this does not necessarily imply a causal relationship between amyloid and degenerative diseases, the toxicity of certain amyloid forms and the overproduction of amyloid in familial cases of degenerative disorders supports the idea that amyloid formation is generally toxic.

Specifically, aggregation of TDP-43, an RNA-binding protein, has been found in ALS/MND patients, and mutations in the genes coding for these proteins have been identified in familial cases of ALS/MND. These mutations promote the misfolding of the proteins into a prion-like conformation. The misfolded form of TDP-43 forms cytoplasmic inclusions in afflicted neurons, and is found depleted in the nucleus. In addition to ALS/MND and FTLD-U, TDP-43 pathology is a feature of many cases of Alzheimer's disease, Parkinson's disease and Huntington's disease. The misfolding of TDP-43 is largely directed by its prion-like domain. This domain is inherently prone to misfolding, while pathological mutations in TDP-43 have been found to increase this propensity to misfold, explaining the presence of these mutations in familial cases of ALS/MND. As in yeast, the prion-like domain of TDP-43 has been shown to be both necessary and sufficient for protein misfolding and aggregation.

Similarly, pathogenic mutations have been identified in the prion-like domains of heterogeneous nuclear riboproteins hnRNPA2B1 and hnRNPA1 in familial cases of muscle, brain, bone and motor neuron degeneration. The wild-type form of all of these proteins show a tendency to self-assemble into amyloid fibrils, while the pathogenic mutations exacerbate this behaviour and lead to excess accumulation.

The word "prion", coined in 1982 by Stanley B. Prusiner, is a portmanteau derived from protein and infection, hence prion, and is short for "proteinaceous infectious particle", in reference to its ability to self-propagate and transmit its conformation to other proteins. Its main pronunciation is , although , as the homographic name of the bird is pronounced, is also heard. In his 1982 paper introducing the term, Prusiner specified that it be "pronounced "pree"-on."








</doc>
<doc id="23051" url="https://en.wikipedia.org/wiki?curid=23051" title="Pascal">
Pascal

Pascal or PASCAL may refer to:






</doc>
<doc id="23053" url="https://en.wikipedia.org/wiki?curid=23053" title="Periodic table">
Periodic table

The periodic table is a tabular arrangement of the chemical elements, ordered by their atomic number, electron configuration, and recurring chemical properties, whose structure shows "periodic trends". Generally, within one row (period) the elements are metals to the left, and non-metals to the right, with the elements having similar chemical behaviours placed in the same column. Table rows are commonly called periods and columns are called groups. Six groups have accepted names as well as assigned numbers: for example, group 17 elements are the halogens; and group 18 are the noble gases. Also displayed are four simple rectangular areas or blocks associated with the filling of different atomic orbitals.

The organization of the periodic table can be used to derive relationships between the various element properties, but also the predicted chemical properties and behaviours of undiscovered or newly synthesized elements. Russian chemist Dmitri Mendeleev was the first to publish a recognizable periodic table in 1869, developed mainly to illustrate periodic trends of the then-known elements. He also predicted some properties of unidentified elements that were expected to fill gaps within the table. Most of his forecasts proved to be correct. Mendeleev's idea has been slowly expanded and refined with the discovery or synthesis of further new elements and the development of new theoretical models to explain chemical behaviour. The modern periodic table now provides a useful framework for analyzing chemical reactions, and continues to be widely used in chemistry, nuclear physics and other sciences.

All the elements from atomic numbers 1 (hydrogen) through 118 (oganesson) have been either discovered or synthesized, completing the first seven rows of the periodic table. The first 98 elements exist in nature, although some are found only in trace amounts and others were synthesized in laboratories before being found in nature. Elements 99 to 118 have only been synthesized in laboratories or nuclear reactors. The synthesis of elements having higher atomic numbers is currently being pursued: these elements would begin an eighth row, and theoretical work has been done to suggest possible candidates for this extension. Numerous synthetic radionuclides of naturally occurring elements have also been produced in laboratories.

Each chemical element has a unique atomic number ("Z") representing the number of protons in its nucleus. Most elements have differing numbers of neutrons among different atoms, with these variants being referred to as isotopes. For example, carbon has three naturally occurring isotopes: all of its atoms have six protons and most have six neutrons as well, but about one per cent have seven neutrons, and a very small fraction have eight neutrons. Isotopes are never separated in the periodic table; they are always grouped together under a single element. Elements with no stable isotopes have the atomic masses of their most stable isotopes, where such masses are shown, listed in parentheses.

In the standard periodic table, the elements are listed in order of increasing atomic number "Z" (the number of protons in the nucleus of an atom). A new row ("period") is started when a new electron shell has its first electron. Columns ("groups") are determined by the electron configuration of the atom; elements with the same number of electrons in a particular subshell fall into the same columns (e.g. oxygen and selenium are in the same column because they both have four electrons in the outermost p-subshell). Elements with similar chemical properties generally fall into the same group in the periodic table, although in the f-block, and to some respect in the d-block, the elements in the same period tend to have similar properties, as well. Thus, it is relatively easy to predict the chemical properties of an element if one knows the properties of the elements around it.

, the periodic table has 118 confirmed elements, from element 1 (hydrogen) to 118 (oganesson). Elements 113, 115, 117 and 118, the most recent discoveries, were officially confirmed by the International Union of Pure and Applied Chemistry (IUPAC) in December 2015. Their proposed names, nihonium (Nh), moscovium (Mc), tennessine (Ts) and oganesson (Og) respectively, were announced by the IUPAC in June 2016 and made official in November 2016.

The first 94 elements occur naturally; the remaining 24, americium to oganesson (95–118), occur only when synthesized in laboratories. Of the 94 naturally occurring elements, 83 are primordial and 11 occur only in decay chains of primordial elements. No element heavier than einsteinium (element 99) has ever been observed in macroscopic quantities in its pure form, nor has astatine (element 85); francium (element 87) has been only photographed in the form of light emitted from microscopic quantities (300,000 atoms).

A "group" or "family" is a vertical column in the periodic table. Groups usually have more significant periodic trends than periods and blocks, explained below. Modern quantum mechanical theories of atomic structure explain group trends by proposing that elements within the same group generally have the same electron configurations in their valence shell. Consequently, elements in the same group tend to have a shared chemistry and exhibit a clear trend in properties with increasing atomic number. In some parts of the periodic table, such as the d-block and the f-block, horizontal similarities can be as important as, or more pronounced than, vertical similarities.

Under an international naming convention, the groups are numbered numerically from 1 to 18 from the leftmost column (the alkali metals) to the rightmost column (the noble gases). Previously, they were known by roman numerals. In America, the roman numerals were followed by either an "A" if the group was in the s- or p-block, or a "B" if the group was in the d-block. The roman numerals used correspond to the last digit of today's naming convention (e.g. the group 4 elements were group IVB, and the group 14 elements were group IVA). In Europe, the lettering was similar, except that "A" was used if the group was before group 10, and "B" was used for groups including and after group 10. In addition, groups 8, 9 and 10 used to be treated as one triple-sized group, known collectively in both notations as group VIII. In 1988, the new IUPAC naming system was put into use, and the old group names were deprecated.

Some of these groups have been given trivial (unsystematic) names, as seen in the table below, although some are rarely used. Groups 3–10 have no trivial names and are referred to simply by their group numbers or by the name of the first member of their group (such as "the scandium group" for group 3), since they display fewer similarities and/or vertical trends.

Elements in the same group tend to show patterns in atomic radius, ionization energy, and electronegativity. From top to bottom in a group, the atomic radii of the elements increase. Since there are more filled energy levels, valence electrons are found farther from the nucleus. From the top, each successive element has a lower ionization energy because it is easier to remove an electron since the atoms are less tightly bound. Similarly, a group has a top-to-bottom decrease in electronegativity due to an increasing distance between valence electrons and the nucleus. There are exceptions to these trends: for example, in group 11, electronegativity increases farther down the group.

A "period" is a horizontal row in the periodic table. Although groups generally have more significant periodic trends, there are regions where horizontal trends are more significant than vertical group trends, such as the f-block, where the lanthanides and actinides form two substantial horizontal series of elements.

Elements in the same period show trends in atomic radius, ionization energy, electron affinity, and electronegativity. Moving left to right across a period, atomic radius usually decreases. This occurs because each successive element has an added proton and electron, which causes the electron to be drawn closer to the nucleus. This decrease in atomic radius also causes the ionization energy to increase when moving from left to right across a period. The more tightly bound an element is, the more energy is required to remove an electron. Electronegativity increases in the same manner as ionization energy because of the pull exerted on the electrons by the nucleus. Electron affinity also shows a slight trend across a period. Metals (left side of a period) generally have a lower electron affinity than nonmetals (right side of a period), with the exception of the noble gases.

Specific regions of the periodic table can be referred to as "blocks" in recognition of the sequence in which the electron shells of the elements are filled. Each block is named according to the subshell in which the "last" electron notionally resides. The s-block comprises the first two groups (alkali metals and alkaline earth metals) as well as hydrogen and helium. The p-block comprises the last six groups, which are groups 13 to 18 in IUPAC group numbering (3A to 8A in American group numbering) and contains, among other elements, all of the metalloids. The d-block comprises groups 3 to 12 (or 3B to 2B in American group numbering) and contains all of the transition metals. The f-block, often offset below the rest of the periodic table, has no group numbers and comprises lanthanides and actinides.

According to their shared physical and chemical properties, the elements can be classified into the major categories of metals, metalloids and nonmetals. Metals are generally shiny, highly conducting solids that form alloys with one another and salt-like ionic compounds with nonmetals (other than noble gases). A majority of nonmetals are coloured or colourless insulating gases; nonmetals that form compounds with other nonmetals, feature covalent bonding. In between metals and nonmetals are metalloids, which have intermediate or mixed properties.

Metal and nonmetals can be further classified into subcategories that show a gradation from metallic to non-metallic properties, when going left to right in the rows. The metals may be subdivided into the highly reactive alkali metals, through the less reactive alkaline earth metals, lanthanides and actinides, via the archetypal transition metals, and ending in the physically and chemically weak post-transition metals. Nonmetals may be simply subdivided into the polyatomic nonmetals, being nearer to the metalloids and show some incipient metallic character; the essentially nonmetallic diatomic nonmetals, nonmetallic and the almost completely inert, monatomic noble gases. Specialized groupings such as refractory metals and noble metals, are examples of subsets of transition metals, also known and occasionally denoted.

Placing elements into categories and subcategories based just on shared properties is imperfect. There is a large disparity of properties within each category with notable overlaps at the boundaries, as is the case with most classification schemes. Beryllium, for example, is classified as an alkaline earth metal although its amphoteric chemistry and tendency to mostly form covalent compounds are both attributes of a chemically weak or post-transition metal. Radon is classified as a nonmetallic noble gas yet has some cationic chemistry that is characteristic of metals. Other classification schemes are possible such as the division of the elements into mineralogical occurrence categories, or crystalline structures. Categorizing the elements in this fashion dates back to at least 1869 when Hinrichs wrote that simple boundary lines could be placed on the periodic table to show elements having shared properties, such as metals, nonmetals, or gaseous elements.

The electron configuration or organisation of electrons orbiting neutral atoms shows a recurring pattern or periodicity. The electrons occupy a series of electron shells (numbered 1, 2, and so on). Each shell consists of one or more subshells (named s, p, d, f and g). As atomic number increases, electrons progressively fill these shells and subshells more or less according to the Madelung rule or energy ordering rule, as shown in the diagram. The electron configuration for neon, for example, is 1s 2s 2p. With an atomic number of ten, neon has two electrons in the first shell, and eight electrons in the second shell; there are two electrons in the s subshell and six in the p subshell. In periodic table terms, the first time an electron occupies a new shell corresponds to the start of each new period, these positions being occupied by hydrogen and the alkali metals.

Since the properties of an element are mostly determined by its electron configuration, the properties of the elements likewise show recurring patterns or periodic behaviour, some examples of which are shown in the diagrams below for atomic radii, ionization energy and electron affinity. It is this periodicity of properties, manifestations of which were noticed well before the underlying theory was developed, that led to the establishment of the periodic law (the properties of the elements recur at varying intervals) and the formulation of the first periodic tables.

Atomic radii vary in a predictable and explainable manner across the periodic table. For instance, the radii generally decrease along each period of the table, from the alkali metals to the noble gases; and increase down each group. The radius increases sharply between the noble gas at the end of each period and the alkali metal at the beginning of the next period. These trends of the atomic radii (and of various other chemical and physical properties of the elements) can be explained by the electron shell theory of the atom; they provided important evidence for the development and confirmation of quantum theory.

The electrons in the 4f-subshell, which is progressively filled across the lanthanide series, are not particularly effective at shielding the increasing nuclear charge from the sub-shells further out. The elements immediately following the lanthanides have atomic radii that are smaller than would be expected and that are almost identical to the atomic radii of the elements immediately above them. Hence hafnium has virtually the same atomic radius (and chemistry) as zirconium, and tantalum has an atomic radius similar to niobium, and so forth. This is known as the lanthanide contraction. The effect of the lanthanide contraction is noticeable up to platinum (element 78), after which it is masked by a relativistic effect known as the inert pair effect. The d-block contraction, which is a similar effect between the d-block and p-block, is less pronounced than the lanthanide contraction but arises from a similar cause.

The first ionization energy is the energy it takes to remove one electron from an atom, the second ionization energy is the energy it takes to remove a second electron from the atom, and so on. For a given atom, successive ionization energies increase with the degree of ionization. For magnesium as an example, the first ionization energy is 738 kJ/mol and the second is 1450 kJ/mol. Electrons in the closer orbitals experience greater forces of electrostatic attraction; thus, their removal requires increasingly more energy. Ionization energy becomes greater up and to the right of the periodic table.

Large jumps in the successive molar ionization energies occur when removing an electron from a noble gas (complete electron shell) configuration. For magnesium again, the first two molar ionization energies of magnesium given above correspond to removing the two 3s electrons, and the third ionization energy is a much larger 7730 kJ/mol, for the removal of a 2p electron from the very stable neon-like configuration of Mg. Similar jumps occur in the ionization energies of other third-row atoms.

Electronegativity is the tendency of an atom to attract a shared pair of electrons. An atom's electronegativity is affected by both its atomic number and the distance between the valence electrons and the nucleus. The higher its electronegativity, the more an element attracts electrons. It was first proposed by Linus Pauling in 1932. In general, electronegativity increases on passing from left to right along a period, and decreases on descending a group. Hence, fluorine is the most electronegative of the elements, while caesium is the least, at least of those elements for which substantial data is available.

There are some exceptions to this general rule. Gallium and germanium have higher electronegativities than aluminium and silicon respectively because of the d-block contraction. Elements of the fourth period immediately after the first row of the transition metals have unusually small atomic radii because the 3d-electrons are not effective at shielding the increased nuclear charge, and smaller atomic size correlates with higher electronegativity. The anomalously high electronegativity of lead, particularly when compared to thallium and bismuth, appears to be an artifact of data selection and data availability. Methods of calculation other than the Pauling method show the normal periodic trends for these elements.

The electron affinity of an atom is the amount of energy released when an electron is added to a neutral atom to form a negative ion. Although electron affinity varies greatly, some patterns emerge. Generally, nonmetals have more positive electron affinity values than metals. Chlorine most strongly attracts an extra electron. The electron affinities of the noble gases have not been measured conclusively, so they may or may not have slightly negative values.

Electron affinity generally increases across a period. This is caused by the filling of the valence shell of the atom; a group 17 atom releases more energy than a group 1 atom on gaining an electron because it obtains a filled valence shell and is therefore more stable.

A trend of decreasing electron affinity going down groups would be expected. The additional electron will be entering an orbital farther away from the nucleus. As such this electron would be less attracted to the nucleus and would release less energy when added. In going down a group, around one-third of elements are anomalous, with heavier elements having higher electron affinities than their next lighter congenors. Largely, this is due to the poor shielding by d and f electrons. A uniform decrease in electron affinity only applies to group 1 atoms.

The lower the values of ionization energy, electronegativity and electron affinity, the more metallic character the element has. Conversely, nonmetallic character increases with higher values of these properties. Given the periodic trends of these three properties, metallic character tends to decrease going across a period (or row) and, with some irregularities (mostly) due to poor screening of the nucleus by d and f electrons, and relativistic effects, tends to increase going down a group (or column or family). Thus, the most metallic elements (such as caesium and francium) are found at the bottom left of traditional periodic tables and the most nonmetallic elements (oxygen, fluorine, chlorine) at the top right. The combination of horizontal and vertical trends in metallic character explains the stair-shaped dividing line between metals and nonmetals found on some periodic tables, and the practice of sometimes categorizing several elements adjacent to that line, or elements adjacent to those elements, as metalloids.

From left to right across the four blocks of the long- or 32-column form of the periodic table are a series of linking or bridging groups of elements, located approximately between each block. These groups, like the metalloids, show properties in between, or that are a mixture of, groups to either side. Chemically, the group 3 elements, scandium, yttrium, lanthanum and actinium behave largely like the alkaline earth metals or, more generally, "s" block metals but have some of the physical properties of "d" block transition metals. Lutetium and lawrencium, at the end of the end of the "f" block, may constitute another linking or bridging group. Lutetium behaves chemically as a lanthanide but shows a mix of lanthanide and transition metal physical properties. Lawrencium, as an analogue of lutetium, would presumable display like characteristics. The coinage metals in group 11 (copper, silver, and gold) are chemically capable of acting as either transition metals or main group metals. The volatile group 12 metals, zinc, cadmium and mercury are sometimes regarded as linking the "d" block to the "p" block. Notionally they are "d" block elements but they have few transition metal properties and are more like their "p" block neighbors in group 13. The relatively inert noble gases, in group 18, bridge the most reactive groups of elements in the periodic table—the halogens in group 17 and the alkali metals in group 1.

In 1789, Antoine Lavoisier published a list of 33 chemical elements, grouping them into gases, metals, nonmetals, and earths. Chemists spent the following century searching for a more precise classification scheme. In 1829, Johann Wolfgang Döbereiner observed that many of the elements could be grouped into triads based on their chemical properties. Lithium, sodium, and potassium, for example, were grouped together in a triad as soft, reactive metals. Döbereiner also observed that, when arranged by atomic weight, the second member of each triad was roughly the average of the first and the third; this became known as the Law of Triads. German chemist Leopold Gmelin worked with this system, and by 1843 he had identified ten triads, three groups of four, and one group of five. Jean-Baptiste Dumas published work in 1857 describing relationships between various groups of metals. Although various chemists were able to identify relationships between small groups of elements, they had yet to build one scheme that encompassed them all.

In 1857, German chemist August Kekulé observed that carbon often has four other atoms bonded to it. Methane, for example, has one carbon atom and four hydrogen atoms. This concept eventually became known as valency; different elements bond with different numbers of atoms.

In 1862, Alexandre-Emile Béguyer de Chancourtois, a French geologist, published an early form of periodic table, which he called the telluric helix or screw. He was the first person to notice the periodicity of the elements. With the elements arranged in a spiral on a cylinder by order of increasing atomic weight, de Chancourtois showed that elements with similar properties seemed to occur at regular intervals. His chart included some ions and compounds in addition to elements. His paper also used geological rather than chemical terms and did not include a diagram; as a result, it received little attention until the work of Dmitri Mendeleev.

In 1864, Julius Lothar Meyer, a German chemist, published a table with 44 elements arranged by valency. The table showed that elements with similar properties often shared the same valency. Concurrently, English chemist William Odling published an arrangement of 57 elements, ordered on the basis of their atomic weights. With some irregularities and gaps, he noticed what appeared to be a periodicity of atomic weights among the elements and that this accorded with "their usually received groupings". Odling alluded to the idea of a periodic law but did not pursue it. He subsequently proposed (in 1870) a valence-based classification of the elements.

English chemist John Newlands produced a series of papers from 1863 to 1866 noting that when the elements were listed in order of increasing atomic weight, similar physical and chemical properties recurred at intervals of eight; he likened such periodicity to the octaves of music. This so termed Law of Octaves was ridiculed by Newlands' contemporaries, and the Chemical Society refused to publish his work. Newlands was nonetheless able to draft a table of the elements and used it to predict the existence of missing elements, such as germanium. The Chemical Society only acknowledged the significance of his discoveries five years after they credited Mendeleev.

In 1867, Gustavus Hinrichs, a Danish born academic chemist based in America, published a spiral periodic system based on atomic spectra and weights, and chemical similarities. His work was regarded as idiosyncratic, ostentatious and labyrinthine and this may have militated against its recognition and acceptance.

Russian chemistry professor Dmitri Mendeleev and German chemist Julius Lothar Meyer independently published their periodic tables in 1869 and 1870, respectively. Mendeleev's table was his first published version; that of Meyer was an expanded version of his (Meyer's) table of 1864. They both constructed their tables by listing the elements in rows or columns in order of atomic weight and starting a new row or column when the characteristics of the elements began to repeat.

The recognition and acceptance afforded to Mendeleev's table came from two decisions he made. The first was to leave gaps in the table when it seemed that the corresponding element had not yet been discovered. Mendeleev was not the first chemist to do so, but he was the first to be recognized as using the trends in his periodic table to predict the properties of those missing elements, such as gallium and germanium. The second decision was to occasionally ignore the order suggested by the atomic weights and switch adjacent elements, such as tellurium and iodine, to better classify them into chemical families. 

Mendeleev published in 1869, using atomic weight to organize the elements, information determinable to fair precision in his time. Atomic weight worked well enough to allow Mendeleev to accurately predict the properties of missing elements. 

Following the discovery, in 1911, by Ernest Rutherford of the atomic nucleus, it was proposed that the integer count of the nuclear charge is identical to the sequential place of each element in the periodic table. In 1913, Henry Moseley using X-ray spectroscopy confirmed this proposal experimentally. Moseley determined the value of the nuclear charge of each element, and showed that Mendeleev's ordering actually places the elements in sequential order by nuclear charge. Nuclear charge is identical to proton count, and determines the value of the atomic number (Z) of each element. Using atomic number gives a definitive, integer-based sequence for the elements. Moseley predicted, in 1913, that the only elements still missing between aluminium (Z=13) and gold (Z=79) were Z = 43, 61, 72, and 75, all of which were later discovered. The atomic number is the absolute definition of an element, and gives a factual basis for the ordering of the periodic table. The periodic table is used to predict the properties of new synthetic elements before they are produced and studied.

In 1871, Mendeleev published his periodic table in a new form, with groups of similar elements arranged in columns rather than in rows, and those columns numbered I to VIII corresponding with the element's oxidation state. He also gave detailed predictions for the properties of elements he had earlier noted were missing, but should exist. These gaps were subsequently filled as chemists discovered additional naturally occurring elements. It is often stated that the last naturally occurring element to be discovered was francium (referred to by Mendeleev as "eka-caesium") in 1939. Plutonium, produced synthetically in 1940, was identified in trace quantities as a naturally occurring element in 1971.

The popular periodic table layout, also known as the common or standard form (as shown at various other points in this article), is attributable to Horace Groves Deming. In 1923, Deming, an American chemist, published short (Mendeleev style) and medium (18-column) form periodic tables. Merck and Company prepared a handout form of Deming's 18-column medium table, in 1928, which was widely circulated in American schools. By the 1930s Deming's table was appearing in handbooks and encyclopaedias of chemistry. It was also distributed for many years by the Sargent-Welch Scientific Company.

With the development of modern quantum mechanical theories of electron configurations within atoms, it became apparent that each period (row) in the table corresponded to the filling of a quantum shell of electrons. Larger atoms have more electron sub-shells, so later tables have required progressively longer periods.

In 1945, Glenn Seaborg, an American scientist, made the suggestion that the actinide elements, like the lanthanides, were filling an f sub-level. Before this time the actinides were thought to be forming a fourth d-block row. Seaborg's colleagues advised him not to publish such a radical suggestion as it would most likely ruin his career. As Seaborg considered he did not then have a career to bring into disrepute, he published anyway. Seaborg's suggestion was found to be correct and he subsequently went on to win the 1951 Nobel Prize in chemistry for his work in synthesizing actinide elements.

Although minute quantities of some transuranic elements occur naturally, they were all first discovered in laboratories. Their production has expanded the periodic table significantly, the first of these being neptunium, synthesized in 1939. Because many of the transuranic elements are highly unstable and decay quickly, they are challenging to detect and characterize when produced. There have been controversies concerning the acceptance of competing discovery claims for some elements, requiring independent review to determine which party has priority, and hence naming rights. In 2010, a joint Russia–US collaboration at Dubna, Moscow Oblast, Russia, claimed to have synthesized six atoms of tennessine (element 117), making it the most recently claimed discovery. It, along with nihonium (element 113), moscovium (element 115), and oganesson (element 118), are the four most recently named elements, whose names all became official on 28 November 2016.

The modern periodic table is sometimes expanded into its long or 32-column form by reinstating the footnoted f-block elements into their natural position between the s- and d-blocks. Unlike the 18-column form this arrangement results in "no interruptions in the sequence of increasing atomic numbers". The relationship of the f-block to the other blocks of the periodic table also becomes easier to see. Jensen advocates a form of table with 32 columns on the grounds that the lanthanides and actinides are otherwise relegated in the minds of students as dull, unimportant elements that can be quarantined and ignored. Despite these advantages the 32-column form is generally avoided by editors on account of its undue rectangular ratio (compared to a book page ratio), and the familiarity of chemists with the modern form (as introduced by Seaborg).

Within 100 years of the appearance of Mendeleev's table in 1869 it has been estimated that around 700 different periodic table versions were published. As well as numerous rectangular variations, other periodic table formats have been shaped, for example, like a circle, cube, cylinder, building, spiral, lemniscate, octagonal prism, pyramid, sphere, or triangle. Such alternatives are often developed to highlight or emphasize chemical or physical properties of the elements that are not as apparent in traditional periodic tables.
A popular alternative structure is that of Theodor Benfey (1960). The elements are arranged in a continuous spiral, with hydrogen at the centre and the transition metals, lanthanides, and actinides occupying peninsulas.

Most periodic tables are two-dimensional; three-dimensional tables are known to as far back as at least 1862 (pre-dating Mendeleev's two-dimensional table of 1869). More recent examples include Courtines' Periodic Classification (1925), Wringley's Lamina System (1949),
Giguère's Periodic helix (1965) and Dufour's Periodic Tree (1996). Going one further, Stowe's Physicist's Periodic Table (1989) has been described as being four-dimensional (having three spatial dimensions and one colour dimension).

The various forms of periodic tables can be thought of as lying on a chemistry–physics continuum. Towards the chemistry end of the continuum can be found, as an example, Rayner-Canham's "unruly" Inorganic Chemist's Periodic Table (2002), which emphasizes trends and patterns, and unusual chemical relationships and properties. Near the physics end of the continuum is Janet's Left-Step Periodic Table (1928). This has a structure that shows a closer connection to the order of electron-shell filling and, by association, quantum mechanics. A somewhat similar approach has been taken by Alper, albeit criticized by Eric Scerri as disregarding the need to display chemical and physical periodicity. Somewhere in the middle of the continuum is the ubiquitous common or standard form of periodic table. This is regarded as better expressing empirical trends in physical state, electrical and thermal conductivity, and oxidation numbers, and other properties easily inferred from traditional techniques of the chemical laboratory. Its popularity is thought to be a result of this layout having a good balance of features in terms of ease of construction and size, and its depiction of atomic order and periodic trends.

Simply following electron configurations, hydrogen (electronic configuration 1s) and helium (1s) should be placed in groups 1 and 2, above lithium (1s2s) and beryllium (1s2s). While such a placement is common for hydrogen, it is rarely used for helium outside of the context of electron configurations: When the noble gases (then called "inert gases") were first discovered around 1900, they were known as "group 0", reflecting no chemical reactivity of these elements known at that point, and helium was placed on the top of that group, as it did share the extreme chemical inertness seen throughout the group. As the group changed its formal number, many authors continued to assign helium directly above neon, in group 18; one of the examples of such placing is the current IUPAC table.

Hydrogen's chemical properties are not very close to those of the alkali metals, which occupy group 1. On this basis it is sometimes placed elsewhere. A common alternative is at the top of group 17 given hydrogen's strictly univalent and largely non-metallic chemistry, and the strictly univalent and non-metallic chemistry of fluorine (the element otherwise at the top of group 17). Sometimes, to show hydrogen has properties corresponding to both those of the alkali metals and the halogens, it is shown at the top of the two columns simultaneously. Another suggestion is above carbon in group 14: placed that way, it fits well into the trends of increasing ionization potential values and electron affinity values, and is not too far from the electronegativity trend, even though hydrogen cannot show the tetravalence characteristic of the heavier group 14 elements. Finally, hydrogen is sometimes placed separately from any group; this is based on how general properties of hydrogen differ from that of any group. The other period 1 element, helium, is sometimes placed separately from any group as well. The property that distinguishes helium from the rest of the noble gases (even though the extraordinary inertness of helium is extremely close to that of neon and argon) is that in its closed electron shell, helium has only two electrons in the outermost electron orbital, while the rest of the noble gases have eight.

Although scandium and yttrium are always the first two elements in group 3, the identity of the next two elements is not completely settled. They are commonly lanthanum and actinium, and less often lutetium and lawrencium. The two variants originate from historical difficulties in placing the lanthanides in the periodic table, and arguments as to where the "f" block elements start and end. It has been claimed that such arguments are proof that, "it is a mistake to break the [periodic] system into sharply delimited blocks". A third variant shows the two positions below yttrium as being occupied by the lanthanides and the actinides.

Chemical and physical arguments have been made in support of lutetium and lawrencium but the majority of authors seem unconvinced. Most working chemists are not aware there is any controversy. In December 2015 an IUPAC project was established to make a recommendation on the matter.

Lanthanum and actinium are commonly depicted as the remaining group 3 members. It has been suggested that this layout originated in the 1940s, with the appearance of periodic tables relying on the electron configurations of the elements and the notion of the differentiating electron. The configurations of caesium, barium and lanthanum are [Xe]6s, [Xe]6s and [Xe]5d6s. Lanthanum thus has a 5d differentiating electron and this establishes it "in group 3 as the first member of the d-block for period 6". A consistent set of electron configurations is then seen in group 3: scandium [Ar]3d4s, yttrium [Kr]4d5s and lanthanum [Xe]5d6s. Still in period 6, ytterbium was assigned an electron configuration of [Xe]4f5d6s and lutetium [Xe]4f5d6s, "resulting in a 4f differentiating electron for lutetium and firmly establishing it as the last member of the f-block for period 6". Later spectroscopic work found that the electron configuration of ytterbium was in fact [Xe]4f6s. This meant that ytterbium and lutetium—the latter with [Xe]4f5d6s—both had 14 f-electrons, "resulting in a d- rather than an f- differentiating electron" for lutetium and making it an "equally valid candidate" with [Xe]5d6s lanthanum, for the group 3 periodic table position below yttrium. Lanthanum has the advantage of incumbency since the 5d electron appears for the first time in its structure whereas it appears for the third time in lutetium, having also made a brief second appearance in gadolinium.

In terms of chemical behaviour, and trends going down group 3 for properties such as melting point, electronegativity and ionic radius, scandium, yttrium, lanthanum and actinium are similar to their group 1–2 counterparts. In this variant, the number of "f" electrons in the most common (trivalent) ions of the f-block elements consistently matches their position in the f-block. For example, the f-electron counts for the trivalent ions of the first three f-block elements are Ce 1, Pr 2 and Nd 3.

In other tables, lutetium and lawrencium are the remaining group 3 members. Early techniques for chemically separating scandium, yttrium and lutetium relied on the fact that these elements occurred together in the so-called "yttrium group" whereas La and Ac occurred together in the "cerium group". Accordingly, lutetium rather than lanthanum was assigned to group 3 by some chemists in the 1920s and 30s. Several physicists in the 1950s and '60s favoured lutetium, in light of a comparison of several of its physical properties with those of lanthanum. This arrangement, in which lanthanum is the first member of the f-block, is disputed by some authors since lanthanum lacks any f-electrons. It has been argued that this is not valid concern given other periodic table anomalies—thorium, for example, has no f-electrons yet is part of the f-block. As for lawrencium, its gas phase atomic electron configuration was confirmed in 2015 as [Rn]5f7s7p. Such a configuration represents another periodic table anomaly, regardless of whether lawrencium is located in the f-block or the d-block, as the only potentially applicable p-block position has been reserved for nihonium with its predicted configuration of [Rn]5f6d7s7p.

Chemically, scandium, yttrium and lutetium (and presumably lawrencium) behave like trivalent versions of the group 1–2 metals. On the other hand, trends going down the group for properties such as melting point, electronegativity and ionic radius, are similar to those found among their group 4–8 counterparts. In this variant, the number of "f" electrons in the gaseous forms of the f-block atoms usually matches their position in the f-block. For example, the f-electron counts for the first five f-block elements are La 0, Ce 1, Pr 3, Nd 4 and Pm 5.

A few authors position all thirty lanthanides and actinides in the two positions below yttrium (usually via footnote markers).
This variant emphasizes similarities in the chemistry of the 15 lanthanide elements (La–Lu), possibly at the expense of ambiguity as to which elements occupy the two group 3 positions below yttrium, and a 15-column wide "f" block (there can only be 14 elements in any row of the "f" block).

The definition of a transition metal, as given by IUPAC, is an element whose atom has an incomplete d sub-shell, or which can give rise to cations with an incomplete d sub-shell. By this definition all of the elements in groups 3–11 are transition metals. The IUPAC definition therefore excludes group 12, comprising zinc, cadmium and mercury, from the transition metals category.

Some chemists treat the categories "d-block elements" and "transition metals" interchangeably, thereby including groups 3–12 among the transition metals. In this instance the group 12 elements are treated as a special case of transition metal in which the d electrons are not ordinarily involved in chemical bonding. The 2007 report of mercury(IV) fluoride (HgF), a compound in which mercury would use its d electrons for bonding, has prompted some commentators to suggest that mercury can be regarded as a transition metal. Other commentators, such as Jensen, have argued that the formation of a compound like HgF can occur only under highly abnormal conditions; indeed, its existence is currently disputed. As such, mercury could not be regarded as a transition metal by any reasonable interpretation of the ordinary meaning of the term.

Still other chemists further exclude the group 3 elements from the definition of a transition metal. They do so on the basis that the group 3 elements do not form any ions having a partially occupied d shell and do not therefore exhibit any properties characteristic of transition metal chemistry. In this case, only groups 4–11 are regarded as transition metals. Though the group 3 elements show few of the characteristic chemical properties of the transition metals, they do show some of their characteristic physical properties (on account of the presence in each atom of a single d electron).

Although all elements up to oganesson have been discovered, of the elements above hassium (element 108), only copernicium (element 112), nihonium (element 113), and flerovium (element 114) have known chemical properties, and only for copernicium is there enough evidence for a conclusive categorisation at present. The other elements may behave differently from what would be predicted by extrapolation, due to relativistic effects; for example, flerovium has been predicted to possibly exhibit some noble-gas-like properties, even though it is currently placed in the carbon group. The current experimental evidence still leaves open the question of whether flerovium behaves more like a metal or a noble gas.

It is unclear whether new elements will continue the pattern of the current periodic table as period 8, or require further adaptations or adjustments. Seaborg expected the eighth period to follow the previously established pattern exactly, so that it would include a two-element s-block for elements 119 and 120, a new g-block for the next 18 elements, and 30 additional elements continuing the current f-, d-, and p-blocks, culminating in element 168, the next noble gas. More recently, physicists such as Pekka Pyykkö have theorized that these additional elements do not follow the Madelung rule, which predicts how electron shells are filled and thus affects the appearance of the present periodic table. There are currently several competing theoretical models for the placement of the elements of atomic number less than or equal to 172. In all of these it is element 172, rather than element 168, that emerges as the next noble gas after oganesson, although these must be regarded as speculative as no complete calculations have been done beyond element 122.

The number of possible elements is not known. A very early suggestion made by Elliot Adams in 1911, and based on the arrangement of elements in each horizontal periodic table row, was that elements of atomic weight greater than circa 256 (which would equate to between elements 99 and 100 in modern-day terms) did not exist. A higher—more recent—estimate is that the periodic table may end soon after the island of stability, which is expected to centre around element 126, as the extension of the periodic and nuclides tables is restricted by proton and neutron drip lines. Other predictions of an end to the periodic table include at element 128 by John Emsley, at element 137 by Richard Feynman, and at element 155 by Albert Khazan.

The Bohr model exhibits difficulty for atoms with atomic number greater than 137, as any element with an atomic number greater than 137 would require 1s electrons to be travelling faster than "c", the speed of light. Hence the non-relativistic Bohr model is inaccurate when applied to such an element.

The relativistic Dirac equation has problems for elements with more than 137 protons. For such elements, the wave function of the Dirac ground state is oscillatory rather than bound, and there is no gap between the positive and negative energy spectra, as in the Klein paradox. More accurate calculations taking into account the effects of the finite size of the nucleus indicate that the binding energy first exceeds the limit for elements with more than 173 protons. For heavier elements, if the innermost orbital (1s) is not filled, the electric field of the nucleus will pull an electron out of the vacuum, resulting in the spontaneous emission of a positron. This does not happen if the innermost orbital is filled, so that element 173 is not necessarily the end of the periodic table.

The many different forms of periodic table have prompted the question of whether there is an optimal or definitive form of periodic table. The answer to this question is thought to depend on whether the chemical periodicity seen to occur among the elements has an underlying truth, effectively hard-wired into the universe, or if any such periodicity is instead the product of subjective human interpretation, contingent upon the circumstances, beliefs and predilections of human observers. An objective basis for chemical periodicity would settle the questions about the location of hydrogen and helium, and the composition of group 3. Such an underlying truth, if it exists, is thought to have not yet been discovered. In its absence, the many different forms of periodic table can be regarded as variations on the theme of chemical periodicity, each of which explores and emphasizes different aspects, properties, perspectives and relationships of and among the elements.




</doc>
<doc id="23055" url="https://en.wikipedia.org/wiki?curid=23055" title="Potassium">
Potassium

Potassium is a chemical element with symbol K (from Neo-Latin "kalium") and atomic number 19. It was first isolated from potash, the ashes of plants, from which its name derives. In the periodic table, potassium is one of the alkali metals. All of the alkali metals have a single valence electron in the outer electron shell, which is easily removed to create an ion with a positive charge – a cation, which combines with anions to form salts. Potassium in nature occurs only in ionic salts. Elemental potassium is a soft silvery-white alkali metal that oxidizes rapidly in air and reacts vigorously with water, generating sufficient heat to ignite hydrogen emitted in the reaction, and burning with a lilac-colored flame. It is found dissolved in sea water (which is 0.04% potassium by weight), and is part of many minerals.

Potassium is chemically very similar to sodium, the previous element in group 1 of the periodic table. They have a similar first ionization energy, which allows for each atom to give up its sole outer electron. That they are different elements that combine with the same anions to make similar salts was suspected in 1702, and was proven in 1807 using electrolysis. Naturally occurring potassium is composed of three isotopes, of which is radioactive. Traces of are found in all potassium, and it is the most common radioisotope in the human body.

Potassium ions are vital for the functioning of all living cells. The transfer of potassium ions through nerve cell membranes is necessary for normal nerve transmission; potassium deficiency and excess can each result in numerous signs and symptoms, including an abnormal heart rhythm and various electrocardiographic abnormalities. Fresh fruits and vegetables are good dietary sources of potassium. The body responds to the influx of dietary potassium, which raises serum potassium levels, with a shift of potassium from outside to inside cells and an increase in potassium excretion by the kidneys.

Most industrial applications of potassium exploit the high solubility in water of potassium compounds, such as potassium soaps. Heavy crop production rapidly depletes the soil of potassium, and this can be remedied with agricultural fertilizers containing potassium, accounting for 95% of global potassium chemical production.

The English name for the element "potassium" comes from the word "potash", which refers to an early method of extracting various potassium salts: placing in a "pot" the "ash" of burnt wood or tree leaves, adding water, heating, and evaporating the solution. When Humphry Davy first isolated the pure element using electrolysis in 1807, he named it "potassium", which he derived from the word potash.

The symbol "K" stems from "kali", itself from the root word "alkali", which in turn comes from "" "al-qalyah" "plant ashes." In 1797, the German chemist Martin Klaproth discovered "potash" in the minerals leucite and lepidolite, and realized that "potash" was not a product of plant growth but actually contained a new element, which he proposed to call "kali". In 1807, Humphry Davy produced the element via electrolysis: in 1809, Ludwig Wilhelm Gilbert proposed the name "Kalium" for Davy's "potassium". In 1814, the Swedish chemist Berzelius advocated the name "kalium" for potassium, with the chemical symbol "K".

The English and French speaking countries adopted Davy and Gay-Lussac/Thénard's name Potassium, while the Germanic countries adopted Gilbert/Klaproth's name Kalium. The "Gold Book" of the International Union of Physical and Applied Chemistry has designated the official chemical symbol as K.

Potassium is the second least dense metal after lithium. It is a soft solid with a low melting point, and can be easily cut with a knife. Freshly cut potassium is silvery in appearance, but it begins to tarnish toward gray immediately on exposure to air. In a flame test, potassium and its compounds emit a lilac color with a peak emission wavelength of 766.5 nanometers.

Neutral potassium atoms have 19 electrons, one more than the extremely stable configuration of the noble gas argon. Because of this and its low first ionization energy of 418.8 kJ/mol, the potassium atom is much more likely to lose the last electron and acquire a positive charge than to gain one and acquire a negative charge (though negatively charged alkalide ions are not impossible). This process requires so little energy that potassium is readily oxidized by atmospheric oxygen. In contrast, the second ionization energy is very high (3052 kJ/mol), because removal of two electrons breaks the stable noble gas electronic configuration (the configuration of the inert argon). Potassium therefore does not form compounds with the oxidation state of +2 or higher.

Potassium is an extremely active metal that reacts violently with oxygen in water and air. With oxygen it forms potassium peroxide, and with water potassium forms potassium hydroxide. The reaction of potassium with water is dangerous because of its violent exothermic character and the production of hydrogen gas. Hydrogen reacts again with atmospheric oxygen, producing water, which reacts with the remaining potassium. This reaction requires only traces of water; because of this, potassium and the liquid sodium-potassium (NaK) alloy are potent desiccants that can be used to dry solvents prior to distillation.

Because of the sensitivity of potassium to water and air, reactions with other elements are possible only in an inert atmosphere such as argon gas using air-free techniques. Potassium does not react with most hydrocarbons such as mineral oil or kerosene. It readily dissolves in liquid ammonia, up to 480 g per 1000 g of ammonia at 0 °C. Depending on the concentration, the ammonia solutions are blue to yellow, and their electrical conductivity is similar to that of liquid metals. In a pure solution, potassium slowly reacts with ammonia to form , but this reaction is accelerated by minute amounts of transition metal salts. Because it can reduce the salts to the metal, potassium is often used as the reductant in the preparation of finely divided metals from their salts by the Rieke method. For example, the preparation of magnesium by this method employs potassium as the reductant:

The only common oxidation state for potassium is +1. Potassium metal is a powerful reducing agent that is easily oxidized to the monopositive cation, . Once oxidized, it is very stable and difficult to reduce back to the metal.

Potassium oxidizes faster than most metals and often forms oxides containing oxygen-oxygen bonds, as do all alkali metals except lithium. There are three possible oxides of potassium: potassium oxide (KO), potassium peroxide (KO), and potassium superoxide (KO); they contain three different oxygen-based ions: oxide (), peroxide (), and superoxide (). The latter two species, especially the superoxide, are rare and are formed only in reaction of very electropositive metals (Na, K, Rb, Cs, etc.) with oxygen; these species contain oxygen-oxygen bonds. All potassium-oxygen binary compounds are known to react with water violently, forming potassium hydroxide.

Potassium hydroxide (KOH) is a very strong alkali, and up to 1.21 kg of it can dissolve in merely one liter of water. KOH reacts readily with carbon dioxide to produce potassium carbonate, and is used to remove traces of the gas from air.

In general, potassium compounds are highly ionic and, owing to the high hydration energy of the ion, have excellent water solubility. The main species in water solution are the aquated complexes where n = 6 and 7. The potassium ion is colorless in water and is very difficult to precipitate; possible precipitation methods include reactions with sodium tetraphenylborate, hexachloroplatinic acid, and sodium cobaltinitrite into potassium tetraphenylborate, potassium hexachloroplatinate, and potassium cobaltinitrite.

There are 24 known isotopes of potassium, three of which occur naturally: (93.3%), (0.0117%), and (6.7%). Naturally occurring has a half-life of 1.250×10 years. It decays to stable by electron capture or positron emission (11.2%) or to stable by beta decay (88.8%). The decay of to is the basis of a common method for dating rocks. The conventional K-Ar dating method depends on the assumption that the rocks contained no argon at the time of formation and that all the subsequent radiogenic argon () was quantitatively retained. Minerals are dated by measurement of the concentration of potassium and the amount of radiogenic that has accumulated. The minerals best suited for dating include biotite, muscovite, metamorphic hornblende, and volcanic feldspar; whole rock samples from volcanic flows and shallow instrusives can also be dated if they are unaltered. Apart from dating, potassium isotopes have been used as tracers in studies of weathering and for nutrient cycling studies because potassium is a macronutrient required for life.

Potassium is formed in Supernovae by nucleosynthesis from lighter atoms. Potassium is principally created in Type II supernovae via an explosive oxygen-burning process. is also formed in s-process nucleosynthesis and the neon burning process.

Potassium is the 20th most abundant element in the solar system and the 17th most abundant element by weight in the earth. It makes up about 2.6% of the weight of the earth's crust and is the seventh most abundant element in the crust. The potassium concentration in seawater is 0.39 g/L (0.039 wt/v%), about one twenty-seventh the concentration of sodium.

Potash is primarily a mixture of potassium salts because plants have little or no sodium content, and the rest of a plant's major mineral content consists of calcium salts of relatively low solubility in water. While potash has been used since ancient times, it was not understood for most of its history to be a fundamentally different substance from sodium mineral salts. Georg Ernst Stahl obtained experimental evidence that led him to suggest the fundamental difference of sodium and potassium salts in 1702, and Henri Louis Duhamel du Monceau was able to prove this difference in 1736. The exact chemical composition of potassium and sodium compounds, and the status as chemical element of potassium and sodium, was not known then, and thus Antoine Lavoisier did not include the alkali in his list of chemical elements in 1789. For a long time the only significant applications for potash were the production of glass, bleach, soap and gunpowder as potassium nitrate. Potassium soaps from animal fats and vegetable oils were especially prized because they tend to be more water-soluble and of softer texture, and are therefore known as soft soaps. The discovery by Justus Liebig in 1840 that potassium is a necessary element for plants and that most types of soil lack potassium caused a steep rise in demand for potassium salts. Wood-ash from fir trees was initially used as a potassium salt source for fertilizer, but, with the discovery in 1868 of mineral deposits containing potassium chloride near Staßfurt, Germany, the production of potassium-containing fertilizers began at an industrial scale. Other potash deposits were discovered, and by the 1960s Canada became the dominant producer.

Potassium "metal" was first isolated in 1807 by Sir Humphry Davy, who derived it from caustic potash (KOH, potassium hydroxide) by electrolysis of molten KOH with the newly discovered voltaic pile. Potassium was the first metal that was isolated by electrolysis. Later in the same year, Davy reported extraction of the metal sodium from a mineral derivative (caustic soda, NaOH, or lye) rather than a plant salt, by a similar technique, demonstrating that the elements, and thus the salts, are different. Although the production of potassium and sodium metal should have shown that both are elements, it took some time before this view was universally accepted.

Elemental potassium does not occur in nature because of its high reactivity. It reacts violently with water (see section Precautions below) and also reacts with oxygen. Orthoclase (potassium feldspar) is a common rock-forming mineral. Granite for example contains 5% potassium, which is well above the average in the Earth's crust. Sylvite (KCl), carnallite , kainite and langbeinite are the minerals found in large evaporite deposits worldwide. The deposits often show layers starting with the least soluble at the bottom and the most soluble on top. Deposits of niter (potassium nitrate) are formed by decomposition of organic material in contact with atmosphere, mostly in caves; because of the good water solubility of niter the formation of larger deposits requires special environmental conditions.

Potassium is the eighth or ninth most common element by mass (0.2%) in the human body, so that a 60 kg adult contains a total of about 120 g of potassium. The body has about as much potassium as sulfur and chlorine, and only calcium and phosphorus are more abundant (with the exception of the ubiquitous CHON elements). Potassium ions are present in a wide variety of proteins and enzymes.

Potassium levels influence multiple physiological processes, including

Potassium homeostasis denotes the maintenance of the total body potassium content, plasma potassium level, and the ratio of the intracellular to extracellular potassium concentrations within narrow limits, in the face of pulsatile intake (meals), obligatory renal excretion, and shifts between intracellular and extracellular compartments.

Plasma potassium is normally kept at 3.5 to 5.0 millimoles (mmol) [or milliequivalents (mEq)] per liter by multiple mechanisms. Levels outside this range are associated with an increasing rate of death from multiple causes, and some cardiac, kidney, and lung diseases progress more rapidly if serum potassium levels are not maintained within the normal range.

An average meal of 40-50 mmol presents the body with more potassium than is present in all plasma (20-25 mmol). However, this surge causes the plasma potassium to rise only 10% at most as a result of prompt and efficient clearance by both renal and extra-renal mechanisms.

Hypokalemia, a deficiency of potassium in the plasma, can be fatal if severe. Common causes are increased gastrintestinal loss (vomiting, diarrhea), and increased renal loss (diuresis). Deficiency symptoms include muscle weakness, paralytic ileus, ECG abnormalities, decreased reflex response; and in severe cases, respiratory paralysis, alkalosis, and cardiac arrhythmia.

Potassium content in the plasma is tightly controlled by four basic mechanisms, which have various names and classifications. The four are 1) a reactive negative-feedback system, 2) a reactive feed-forward system, 3) a predictive or circadian system, and 4) an internal or cell membrane transport system. Collectively, the first three are sometimes termed the "external potassium homeostasis system"; and the first two, the "reactive potassium homeostasis system".

Renal handling of potassium is closely connected to sodium handling. Potassium is the major cation (positive ion) inside animal cells [150 mmol/L, (4.8 g)], while sodium is the major cation of extracellular fluid [150 mmol/L, (3.345 g)]. In the kidneys, about 180 liters of plasma is filtered through the glomeruli and into the renal tubules per day. This filtering involves about 600 g of sodium and 33 g of potassium. Since only 1–10 g of sodium and 1–4 g of potassium are likely to be replaced by diet, renal filtering must efficiently reabsorb the remainder from the plasma.

Sodium is reabsorbed to maintain extracellular volume, osmotic pressure, and serum sodium concentration within narrow limits; potassium is reabsorbed to maintain serum potassium concentration within narrow limits. Sodium pumps in the renal tubules operate to reabsorb sodium. Potassium must be conserved also, but, because the amount of potassium in the blood plasma is very small and the pool of potassium in the cells is about thirty times as large, the situation is not so critical for potassium. Since potassium is moved passively in counter flow to sodium in response to an apparent (but not actual) Donnan equilibrium, the urine can never sink below the concentration of potassium in serum except sometimes by actively excreting water at the end of the processing. Potassium is excreted twice and reabsorbed three times before the urine reaches the collecting tubules. At that point, urine usually has about the same potassium concentration as plasma. At the end of the processing, potassium is secreted one more time if the serum levels are too high.

With no potassium intake, it is excreted at about 200 mg per day until, in about a week, potassium in the serum declines to a mildly deficient level of 3.0–3.5 mmol/L. If potassium is still withheld, the concentration continues to fall until a severe deficiency causes eventual death.

The potassium moves passively through pores in the cell membrane. When ions move through pumps there is a gate in the pumps on either side of the cell membrane and only one gate can be open at once. As a result, approximately 100 ions are forced through per second. Pores have only one gate, and there only one kind of ion can stream through, at 10 million to 100 million ions per second. The pores require calcium to open although it is thought that the calcium works in reverse by blocking at least one of the pores. Carbonyl groups inside the pore on the amino acids mimic the water hydration that takes place in water solution by the nature of the electrostatic charges on four carbonyl groups inside the pore.

The U.S. Institute of Medicine (IOM) sets Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs), or Adequate Intakes (AIs) for when there is not sufficient information to set EARs and RDAs. Collectively the EARs, RDAs, AIs and ULs are referred to as Dietary Reference Intakes. The AIs for potassium are: 400mg of potassium for 0-6-month-old males, 700mg of potassium for 7-12-month-old males, 3,000mg of potassium for 1-3-year-old males, 3,800mg of potassium for 4-8-year-old males, 4,500mg of potassium for 9-13-year-old males, and 4,700mg of potassium for males that are 14 years old and older.
The AIs for potassium are: 400mg of potassium for 0-6-month-old females, 700mg of potassium for 7-12-month-old females, 3,000mg of potassium for 1-3-year-old females, 3,800mg of potassium for 4-8-year-old females, 4,500mg of potassium for 9-13-year-old females, and 4,700mg of potassium for females that are 14 years old and older.
The AIs for potassium are: 4,700mg of potassium for 14-50-year-old pregnant females; furthermore, 5,100mg of potassium for 14-50-year-old lactating females. As for safety, the IOM also sets Tolerable upper intake levels (ULs) for vitamins and minerals, but for potassium the evidence was insufficient, so no UL established.

Most Americans consume only half that amount per day.

Likewise, in the European Union, in particular in Germany and Italy, insufficient potassium intake is somewhat common. However, the British National Health Service recommends a lower intake, saying that adults need 3,500 mg per day and that excess amounts may cause health problems such as stomach pain and diarrhoea.

Potassium is present in all fruits, vegetables, meat and fish. Foods with high potassium concentrations include yam, parsley, dried apricots, milk, chocolate, all nuts (especially almonds and pistachios), potatoes, bamboo shoots, bananas, avocados, coconut water, soybeans, and bran.

The USDA lists tomato paste, orange juice, beet greens, white beans, potatoes, plantains, bananas, apricots, and many other dietary sources of potassium, ranked in descending order according to potassium content. A day's worth of potassium is in 5 plantains or 11 bananas.

Diets low in potassium can lead to hypertension and hypokalemia.

Supplements of potassium are most widely used in conjunction with diuretics that block reabsorption of sodium and water upstream from the distal tubule (thiazides and loop diuretics), because this promotes increased distal tubular potassium secretion, with resultant increased potassium excretion. A variety of prescription and over-the counter supplements are available. Potassium chloride may be dissolved in water, but the salty/bitter taste make liquid supplements unpalatable. Typical doses range from 10 mmol (400 mg), to 20 mmol (800 mg). Potassium is also available in tablets or capsules, which are formulated to allow potassium to leach slowly out of a matrix, since very high concentrations of potassium ion that occur adjacent to a solid tablet can injure the gastric or intestinal mucosa. For this reason, non-prescription potassium pills are limited by law in the US to a maximum of 99 mg of potassium.

Since the kidneys are the site of potassium excretion, individuals with impaired kidney function are at risk for hyperkalemia if dietary potassium and supplements are not restricted. The more severe the impairment, the more severe is the restriction necessary to avoid hyperkalemia.

A meta-analysis concluded that a 1640 mg increase in the daily intake of potassium was associated with a 21% lower risk of stroke. Potassium chloride and potassium bicarbonate may be useful to control mild hypertension.

Potassium can be detected by taste because it triggers three of the five types of taste sensations, according to concentration. Dilute solutions of potassium ions taste sweet, allowing moderate concentrations in milk and juices, while higher concentrations become increasingly bitter/alkaline, and finally also salty to the taste. The combined bitterness and saltiness of high-potassium solutions makes high-dose potassium supplementation by liquid drinks a palatability challenge.

Potassium salts such as carnallite, langbeinite, polyhalite, and sylvite form extensive evaporite deposits in ancient lake bottoms and seabeds, making extraction of potassium salts in these environments commercially viable. The principal source of potassium – potash – is mined in Canada, Russia, Belarus, Kazakhstan, Germany, Israel, United States, Jordan, and other places around the world. The first mined deposits were located near Staßfurt, Germany, but the deposits span from Great Britain over Germany into Poland. They are located in the Zechstein and were deposited in the Middle to Late Permian. The largest deposits ever found lie below the surface of the Canadian province of Saskatchewan. The deposits are located in the Elk Point Group produced in the Middle Devonian. Saskatchewan, where several large mines have operated since the 1960s pioneered the technique of freezing of wet sands (the Blairmore formation) to drive mine shafts through them. The main potash mining company in Saskatchewan is the Potash Corporation of Saskatchewan. The water of the Dead Sea is used by Israel and Jordan as a source of potash, while the concentration in normal oceans is too low for commercial production at current prices.

Several methods are used to separate potassium salts from sodium and magnesium compounds. The most-used method is fractional precipitation using the solubility differences of the salts at different temperatures. Electrostatic separation of the ground salt mixture is also used in some mines. The resulting sodium and magnesium waste is either stored underground or piled up in slag heaps. Most of the mined potassium mineral ends up as potassium chloride after processing. The mineral industry refers to potassium chloride either as potash, muriate of potash, or simply MOP.

Pure potassium metal can be isolated by electrolysis of its hydroxide in a process that has changed little since it was first used by Humphry Davy in 1807. Although the electrolysis process was developed and used in industrial scale in the 1920s, the thermal method by reacting sodium with potassium chloride in a chemical equilibrium reaction became the dominant method in the 1950s.

The production of sodium potassium alloys is accomplished by changing the reaction time and the amount of sodium used in the reaction. The Griesheimer process employing the reaction of potassium fluoride with calcium carbide was also used to produce potassium.

Reagent-grade potassium metal costs about $10.00/pound ($22/kg) in 2010 when purchased by the tonne. Lower purity metal is considerably cheaper. The market is volatile because long-term storage of the metal is difficult. It must be stored in a dry inert gas atmosphere or anhydrous mineral oil to prevent the formation of a surface layer of potassium superoxide, a pressure-sensitive explosive that detonates when scratched. The resulting explosion often starts a fire difficult to extinguish.

Potassium ions are an essential component of plant nutrition and are found in most soil types. They are used as a fertilizer in agriculture, horticulture, and hydroponic culture in the form of chloride (KCl), sulfate (), or nitrate (). Agricultural fertilizers consume 95% of global potassium chemical production, and about 90% of this potassium is supplied as KCl. The potassium content of most plants range from 0.5% to 2% of the harvested weight of crops, conventionally expressed as amount of . Modern high-yield agriculture depends upon fertilizers to replace the potassium lost at harvest. Most agricultural fertilizers contain potassium chloride, while potassium sulfate is used for chloride-sensitive crops or crops needing higher sulfur content. The sulfate is produced mostly by decomposition of the complex minerals kainite () and langbeinite (). Only a very few fertilizers contain potassium nitrate. In 2005, about 93% of world potassium production was consumed by the fertilizer industry.

Potassium sodium tartrate (, Rochelle salt) is the main constituent of baking powder; it is also used in the silvering of mirrors. Potassium bromate () is a strong oxidizer (E924), used to improve dough strength and rise height. Potassium bisulfite () is used as a food preservative, for example in wine and beer-making (but not in meats). It is also used to bleach textiles and straw, and in the tanning of leathers.

Major potassium chemicals are potassium hydroxide, potassium carbonate, potassium sulfate, and potassium chloride. Megatons of these compounds are produced annually.

Potassium hydroxide is a strong base, which is used in industry to neutralize strong and weak acids, to control pH and to manufacture potassium salts. It is also used to saponify fats and oils, in industrial cleaners, and in hydrolysis reactions, for example of esters.

Potassium nitrate () or saltpeter is obtained from natural sources such as guano and evaporites or manufactured via the Haber process; it is the oxidant in gunpowder (black powder) and an important agricultural fertilizer. Potassium cyanide (KCN) is used industrially to dissolve copper and precious metals, in particular silver and gold, by forming complexes. Its applications include gold mining, electroplating, and electroforming of these metals; it is also used in organic synthesis to make nitriles. Potassium carbonate ( or potash) is used in the manufacture of glass, soap, color TV tubes, fluorescent lamps, textile dyes and pigments. Potassium permanganate () is an oxidizing, bleaching and purification substance and is used for production of saccharin. Potassium chlorate () is added to matches and explosives. Potassium bromide (KBr) was formerly used as a sedative and in photography.

Potassium chromate () is used in inks, dyes, stains (bright yellowish-red color); in explosives and fireworks; in the tanning of leather, in fly paper and safety matches, but all these uses are due to the chemistry of the chromate ion, rather than the potassium ion.

There are thousands of uses of various potassium compounds. One example is potassium superoxide, , an orange solid that acts as a portable source of oxygen and a carbon dioxide absorber. It is widely used in respiration systems in mines, submarines and spacecraft as it takes less volume than the gaseous oxygen.

Another example is potassium cobaltinitrite, , which is used as artist's pigment under the name of Aureolin or Cobalt Yellow.

The stable isotopes of potassium can be laser cooled and used to probe fundamental and technological problems in quantum physics. The two bosonic isotopes possess convenient Feshbach resonances to enable studies requiring tunable interactions, while K is one of only two stable fermions amongst the alkali metals.

An alloy of sodium and potassium, NaK is a liquid used as a heat-transfer medium and a desiccant for producing dry and air-free solvents. It can also be used in reactive distillation. The ternary alloy of 12% Na, 47% K and 41% Cs has the lowest melting point of −78 °C of any metallic compound.

Metallic potassium is used in several types of magnetometers.

Potassium metal reacts violently with water producing potassium hydroxide (KOH) and hydrogen gas.

This reaction is exothermic and releases enough heat to ignite the resulting hydrogen in the presence of oxygen, possibly explosively splashing onlookers with potassium hydroxide, which is a strong alkali that destroys living tissue and causes skin burns. Finely grated potassium ignites in air at room temperature. The bulk metal ignites in air if heated. Because its density is 0.89 g/cm, burning potassium floats in water that exposes it to atmospheric oxygen. Many common fire extinguishing agents, including water, either are ineffective or make a potassium fire worse. Nitrogen, argon, sodium chloride (table salt), sodium carbonate (soda ash), and silicon dioxide (sand) are effective if they are dry. Some Class D dry powder extinguishers designed for metal fires are also effective. These agents deprive the fire of oxygen and cool the potassium metal.

Potassium reacts violently with halogens and detonates in the presence of bromine. It also reacts explosively with sulfuric acid. During combustion, potassium forms peroxides and superoxides. These peroxides may react violently with organic compounds such as oils. Both peroxides and superoxides may react explosively with metallic potassium.

Because potassium reacts with water vapor in the air, it is usually stored under anhydrous mineral oil or kerosene. Unlike lithium and sodium, however, potassium should not be stored under oil for longer than six months, unless in an inert (oxygen free) atmosphere, or under vacuum. After prolonged storage in air dangerous shock-sensitive peroxides can form on the metal and under the lid of the container, and can detonate upon opening.

Because of the highly reactive nature of potassium metal, it must be handled with great care, with full skin and eye protection and preferably an explosion-resistant barrier between the user and the metal. Ingestion of large amounts of potassium compounds can lead to hyperkalemia, strongly influencing the cardiovascular system. Potassium chloride is used in the United States for lethal injection executions.




</doc>
<doc id="23056" url="https://en.wikipedia.org/wiki?curid=23056" title="Pope">
Pope

The pope ( from "pappas", a child's word for "father"), also known as the supreme pontiff (from Latin "pontifex maximus" "greatest priest"), is the Bishop of Rome and therefore "ex officio" the leader of the worldwide Catholic Church. The primacy of the Roman bishop is largely derived from his role as the supposed apostolic successor to Saint Peter, to whom Jesus is said to have given the Keys of Heaven and the powers of "binding and loosing", naming him as the "rock" upon which the church would be built. Since the 1860s, the pope has also been head of state of Vatican City, a sovereign city-state entirely enclaved within Rome, Italy. The current pope is Francis, who was elected on 13 March 2013, succeeding Benedict XVI.

The office of the pope is the papacy. His ecclesiastical jurisdiction, the Diocese of Rome, is often called the "Holy See" or the "Apostolic See", the latter name being based on the belief that the Bishop of Rome is the apostolic successor to Saint Peter. The pope is considered one of the world's most powerful people because of his extensive diplomatic and cultural influence.

The papacy is one of the most enduring institutions in the world and has had a prominent part in world history. In ancient times the popes helped spread Christianity, and intervened to find resolutions in various doctrinal disputes. In the Middle Ages, they played a role of secular importance in Western Europe, often acting as arbitrators between Christian monarchs. Currently, in addition to the expansion of the Christian faith and doctrine, the popes are involved in ecumenism and interfaith dialogue, charitable work, and the defense of human rights.

In some periods of history, popes, who originally had no temporal powers, accrued wide secular powers rivalling those of temporal rulers. However, in recent centuries the temporal authority of the papacy has declined and the office is now almost exclusively focused on religious matters. By contrast, papal claims of spiritual authority have been increasingly firmly expressed over time, culminating in 1870 with the proclamation of the dogma of papal infallibility for rare occasions when the pope speaks "ex cathedra"—literally "from the chair (of Saint Peter)"—to issue a formal definition of faith or morals.

The word "pope" derives from Greek πάππας meaning "father". In the early centuries of Christianity, this title was applied, especially in the east, to all bishops and other senior clergy, and later became reserved in the west to the Bishop of Rome, a reservation made official only in the 11th century. The earliest record of the use of this title was in regard to the by then deceased Patriarch of Alexandria, Pope Heraclas of Alexandria (232–248). The earliest recorded use of the title "pope" in English dates to the mid-10th century, when it was used in reference to Pope Vitalian in an Old English translation of Bede's "Historia ecclesiastica gentis Anglorum".

The Catholic Church teaches that the pastoral office, the office of shepherding the Church, that was held by the apostles, as a group or "college" with Saint Peter as their head, is now held by their successors, the bishops, with the bishop of Rome (the pope) as their head. Thus, is derived another title by which the pope is known, that of "Supreme Pontiff".

The Catholic Church teaches that Jesus personally appointed Peter as leader of the Church, and the Catholic Church's dogmatic constitution "Lumen gentium" makes a clear distinction between apostles and bishops, presenting the latter as the successors of the former, with the pope as successor of Peter, in that he is head of the bishops as Peter was head of the apostles. Some historians argue against the notion that Peter was the first bishop of Rome, noting that the episcopal see in Rome can be traced back no earlier than the third century. The writings of the Church Father Irenaeus who wrote around AD 180 reflect a belief that Peter "founded and organized" the Church at Rome. Moreover, Irenaeus was not the first to write of Peter's presence in the early Roman Church. Clement of Rome wrote in a letter to the Corinthians, "c." 96, about the persecution of Christians in Rome as the "struggles in our time" and presented to the Corinthians its heroes, "first, the greatest and most just columns", the "good apostles" Peter and Paul. St. Ignatius of Antioch wrote shortly after Clement and in his letter from the city of Smyrna to the Romans he said he would not command them as Peter and Paul did. Given this and other evidence, such as Emperor Constantine's erection of the "Old St. Peter's Basilica" on the location of St. Peter's tomb, as held and given to him by Rome's Christian community, many scholars agree that Peter was martyred in Rome under Nero, although some scholars argue that he may have been martyred in Palestine.

The New Testament offers no proof that Jesus established the papacy, nor even that he established Peter as the first bishop of Rome. Some theologians argue, using Peter's own words, that Christ intended himself—and not Peter—as the foundation of the church. Others have argued that the church is indeed built upon Jesus and faith, but also on the disciples as the roots and foundations of the church on the basis of Paul's teaching in Romans and Ephesians, though not primarily Peter.

First-century Christian communities would have had a group of presbyter-bishops functioning as leaders of their local churches. Gradually, episcopacies were established in metropolitan areas. Antioch may have developed such a structure before Rome. In Rome, there were many who claimed to be the rightful bishop, though again Irenaeus stressed the validity of one line of bishops from the time of St. Peter up to his contemporary Pope Victor I and listed them. Some writers claim that the emergence of a single bishop in Rome probably did not occur until the middle of the 2nd century. In their view, Linus, Cletus and Clement were possibly prominent presbyter-bishops, but not necessarily monarchical bishops.

Documents of the 1st century and early 2nd century indicate that the bishop of Rome had some kind of pre-eminence and prominence in the Church as a whole, as even a letter from the bishop, or patriarch, of Antioch acknowledged the Bishop of Rome as "a first among equals", though the detail of what this meant is unclear.

It seems that at first the terms "episcopos" and "presbyter" were used interchangeably. The consensus among scholars has been that, at the turn of the 1st and 2nd centuries, local congregations were led by bishops and presbyters whose offices were overlapping or indistinguishable. Some say that there was probably "no single 'monarchical' bishop in Rome before the middle of the 2nd century...and likely later." Other scholars and historians disagree, citing the historical records of St. Ignatius of Antioch (d 107) and St. Irenaeus who recorded the linear succession of Bishops of Rome (the popes) up until their own times. They also cite the importance accorded to the Bishops of Rome in the ecumenical councils, including the early ones.

In the early Christian era, Rome and a few other cities had claims on the leadership of worldwide Church. James the Just, known as "the brother of the Lord", served as head of the Jerusalem church, which is still honored as the "Mother Church" in Orthodox tradition. Alexandria had been a center of Jewish learning and became a center of Christian learning. Rome had a large congregation early in the apostolic period whom Paul the Apostle addressed in his Epistle to the Romans, and according to tradition Paul was martyred there. 

During the 1st century of the Church (c. 30–130), the Roman capital became recognized as a Christian center of exceptional importance. Clement I, at the end of the 1st century, wrote an epistle to the Church in Corinth intervening in a major dispute, and apologizing for not having taken action earlier. However, there are only a few other references of that time to recognition of the authoritative primacy of the Roman See outside of Rome. In the Ravenna Document of 13 October 2007, theologians chosen by the Catholic and the Eastern Orthodox Churches stated: "41. Both sides agree ... that Rome, as the Church that 'presides in love' according to the phrase of St Ignatius of Antioch, occupied the first place in the "taxis", and that the bishop of Rome was therefore the "protos" among the patriarchs. Translated into English, the statement means "first among equals". What form that should take is still a matter of disagreement, just as it was when the Catholic and Orthodox Churches split in the Great East-West Schism. They also disagree on the interpretation of the historical evidence from this era regarding the prerogatives of the Bishop of Rome as "protos", a matter that was already understood in different ways in the first millennium." 

In the late 2nd century AD, there were more manifestations of Roman authority over other churches. In 189, assertion of the primacy of the Church of Rome may be indicated in Irenaeus's "Against Heresies" (3:3:2): "With [the Church of Rome], because of its superior origin, all the churches must agree ... and it is in her that the faithful everywhere have maintained the apostolic tradition." In AD 195, Pope Victor I, in what is seen as an exercise of Roman authority over other churches, excommunicated the Quartodecimans for observing Easter on the 14th of Nisan, the date of the Jewish Passover, a tradition handed down by John the Evangelist (see Easter controversy). Celebration of Easter on a Sunday, as insisted on by the pope, is the system that has prevailed (see computus).

The Edict of Milan in 313 granted freedom to all religions in the Roman Empire, beginning the Peace of the Church. In 325, the First Council of Nicaea condemned Arianism, declaring trinitarianism dogmatic, and in its sixth canon recognized the special role of the sees of Rome, Alexandria, and Antioch. Great defenders of Trinitarian faith included the popes, especially Pope Liberius, who was exiled to Berea by Constantius II for his Trinitarian faith, Damasus I, and several other bishops.

In 380, the Edict of Thessalonica declared Nicene Christianity to be the state religion of the empire, with the name "Catholic Christians" reserved for those who accepted that faith. While the civil power in the Eastern Roman Empire controlled the church, and the Ecumenical Patriarch of Constantinople, the capital, wielded much power, in the Western Roman Empire, the Bishops of Rome were able to consolidate the influence and power they already possessed. After the Fall of the Western Roman Empire, barbarian tribes were converted to Arian Christianity or Catholicism; Clovis I, king of the Franks, was the first important barbarian ruler to convert to Catholicism rather than Arianism, allying himself with the papacy. Other tribes, such as the Visigoths, later abandoned Arianism in favour of Catholicism.

After the fall of the Western Roman Empire, the pope served as a source of authority and continuity. Pope Gregory I ("c" 540–604) administered the church with strict reform. From an ancient senatorial family, Gregory worked with the stern judgement and discipline typical of ancient Roman rule. Theologically, he represents the shift from the classical to the medieval outlook; his popular writings are full of dramatic miracles, potent relics, demons, angels, ghosts, and the approaching end of the world.

Gregory's successors were largely dominated by the Exarch of Ravenna, the Byzantine emperor's representative in the Italian Peninsula. These humiliations, the weakening of the Byzantine Empire in the face of the Muslim conquests, and the inability of the emperor to protect the papal estates against the Lombards, made Pope Stephen II turn from Emperor Constantine V. He appealed to the Franks to protect his lands. Pepin the Short subdued the Lombards and donated Italian land to the papacy. When Pope Leo III crowned Charlemagne (800) as Roman Emperor, he established the precedent that, in Western Europe, no man would be emperor without being crowned by a pope.

The low point of the papacy was 867–1049. This period includes the Saeculum obscurum, the Crescentii era, and the Tusculan Papacy. The papacy came under the control of vying political factions. Popes were variously imprisoned, starved, killed, and deposed by force. The family of a certain papal official made and unmade popes for fifty years. The official's great-grandson, Pope John XII, held orgies of debauchery in the Lateran Palace. Otto I, Holy Roman Emperor had John accused in an ecclesiastical court, which deposed him and elected a layman as Pope Leo VIII. John mutilated the Imperial representatives in Rome and had himself reinstated as pope. Conflict between the Emperor and the papacy continued, and eventually dukes in league with the emperor were buying bishops and popes almost openly.

In 1049, Leo IX became pope, at last a pope with the character to face the papacy's problems. He traveled to the major cities of Europe to deal with the church's moral problems firsthand, notably simony and clerical marriage and concubinage. With his long journey, he restored the prestige of the papacy in Northern Europe.

From the 7th century it became common for European monarchies and nobility to found churches and perform investiture or deposition of clergy in their states and fiefdoms, their personal interests causing corruption among the clergy. This practice had become common because often the prelates and secular rulers were also participants in public life. To combat this and other practices that had corrupted the Church between the years 900 and 1050, centres emerged promoting ecclesiastical reform, the most important being the Abbey of Cluny, which spread its ideals throughout Europe. This reform movement gained strength with the election of Pope Gregory VII in 1073, who adopted a series of measures in the movement known as the Gregorian Reform, in order to fight strongly against simony and the abuse of civil power and try to restore ecclesiastical discipline, including clerical celibacy. The conflict between popes and secular autocratic rulers such as the Holy Roman Emperor Henry IV and Henry I of England, known as the Investiture Controversy, was only resolved in 1122, by the Concordat of Worms, in which Pope Callixtus II decreed that clerics were to be invested by clerical leaders, and temporal rulers by lay investiture. Soon after, Pope Alexander III began reforms that would lead to the establishment of canon law.

Since the beginning of the 7th century, the Caliphate had conquered much of the southern Mediterranean, and represented a threat to Christianity. In 1095, the Byzantine emperor, Alexios I Komnenos, asked for military aid from Pope Urban II in the ongoing Byzantine–Seljuq wars. Urban, at the council of Clermont, called the First Crusade to assist the Byzantine Empire to regain the old Christian territories, especially Jerusalem.

With the East–West Schism, the Eastern Orthodox Church and the Catholic Church split definitively in 1054. This fracture was caused more by political events than by slight divergences of creed. Popes had galled the Byzantine emperors by siding with the king of the Franks, crowning a rival Roman emperor, appropriating the Exarchate of Ravenna, and driving into Greek Italy.

In the Middle Ages, popes struggled with monarchs over power.

From 1309 to 1377, the pope resided not in Rome but in Avignon. The Avignon Papacy was notorious for greed and corruption. During this period, the pope was effectively an ally of the Kingdom of France, alienating France's enemies, such as the Kingdom of England.

The pope was understood to have the power to draw on the Treasury of Merit built up by the saints and by Christ, so that he could grant indulgences, reducing one's time in purgatory. The concept that a monetary fine or donation accompanied contrition, confession, and prayer eventually gave way to the common assumption that indulgences depended on a simple monetary contribution. The popes condemned misunderstandings and abuses, but were too pressed for income to exercise effective control over indulgences.

Popes also contended with the cardinals, who sometimes attempted to assert the authority of Catholic Ecumenical Councils over the pope's. Conciliarism holds that the supreme authority of the church lies with a General Council, not with the pope. Its foundations were laid early in the 13th century, and it culminated in the 15th century. The failure of Conciliarism to gain broad acceptance after the 15th century is taken as a factor in the Protestant Reformation.

Various Antipopes challenged papal authority, especially during the Western Schism (1378–1417). In this schism, the papacy had returned to Rome from Avignon, but an antipope was installed in Avignon, as if to extend the papacy there.

The Eastern Church continued to decline with the Eastern Roman (Byzantine) Empire, undercutting Constantinople's claim to equality with Rome. Twice an Eastern Emperor tried to force the Eastern Church to reunify with the West. First in the Second Council of Lyon (1272–1274) and secondly in the Council of Florence (1431–1449). Papal claims of superiority were a sticking point in reunification, which failed in any event. In the 15th century, the Ottoman Empire captured Constantinople.

Protestant Reformers criticized the papacy as corrupt and characterized the pope as the antichrist.

Popes instituted a Catholic Reformation (1560–1648), which addressed the challenges of the Protestant Reformation and instituted internal reforms. Pope Paul III initiated the Council of Trent (1545–1563), whose definitions of doctrine and whose reforms sealed the triumph of the papacy over elements in the church that sought conciliation with Protestants and opposed papal claims.

Gradually forced to give up secular power, the popes focused on spiritual issues.

In 1870, the First Vatican Council proclaimed the dogma of papal infallibility for those rare occasions the pope speaks "ex cathedra" when issuing a solemn definition of faith or morals.

Later the same year, Victor Emmanuel II of Italy seized Rome from the pope's control and substantially completed the Italian unification.

In 1929, the Lateran Treaty between the Kingdom of Italy and the Holy See established Vatican City as an independent city-state, guaranteeing papal independence from secular rule.

In 1950, Pope Pius XII defined the Assumption of Mary as dogma, the only time that a pope has spoken "ex cathedra" since papal infallibility was explicitly declared.

The Petrine Doctrine is still controversial as an issue of doctrine that continues to divide the eastern and western churches and separate Protestants from Rome.

The Catholic Church teaches that, within the Christian community, the bishops as a body have succeeded to the body of the apostles ("apostolic succession") and the Bishop of Rome has succeeded to Saint Peter.

Scriptural texts proposed in support of Peter's special position in relation to the church include:

The symbolic keys in the Papal coats of arms are a reference to the phrase "the keys of the kingdom of heaven" in the first of these texts. Some Protestant writers have maintained that the "rock" that Jesus speaks of in this text is Jesus himself or the faith expressed by Peter. This idea is undermined by the Biblical usage of "Cephas," which is the masculine form of "rock" in Aramaic, to describe Peter. The "Encyclopædia Britannica" comments that "the consensus of the great majority of scholars today is that the most obvious and traditional understanding should be construed, namely, that rock refers to the person of Peter".

The pope was originally chosen by those senior clergymen resident in and near Rome. In 1059 the electorate was restricted to the Cardinals of the Holy Roman Church, and the individual votes of all Cardinal Electors were made equal in 1179. The electors are now limited to those who have not reached 80 on the day before the death or resignation of a pope. The pope does not need to be a Cardinal Elector or indeed a Cardinal; however, since the pope is the Bishop of Rome, only those who can be ordained a bishop can be elected, which means that any male baptized Catholic is eligible. The last to be elected when not yet a bishop was Pope Gregory XVI in 1831, and the last to be elected when not even a priest was Pope Leo X in 1513, and the last to be elected when not a cardinal was Pope Urban VI in 1378. If someone who is not a bishop is elected, he must be given episcopal ordination before the election is announced to the people.

The Second Council of Lyon was convened on 7 May 1274, to regulate the election of the pope. This Council decreed that the cardinal electors must meet within ten days of the pope's death, and that they must remain in seclusion until a pope has been elected; this was prompted by the three-year "sede vacante" following the death of Pope Clement IV in 1268. By the mid-16th century, the electoral process had evolved into its present form, allowing for variation in the time between the death of the pope and the meeting of the cardinal electors. 

Traditionally, the vote was conducted by Acclamation, by selection (by committee), or by plenary vote. Acclamation was the simplest procedure, consisting entirely of a voice vote.
The election of the pope almost always takes place in the Sistine Chapel, in a sequestered meeting called a "conclave" (so called because the cardinal electors are theoretically locked in, "cum clave", i.e., with key, until they elect a new pope). Three cardinals are chosen by lot to collect the votes of absent cardinal electors (by reason of illness), three are chosen by lot to count the votes, and three are chosen by lot to review the count of the votes. The ballots are distributed and each cardinal elector writes the name of his choice on it and pledges aloud that he is voting for "one whom under God I think ought to be elected" before folding and depositing his vote on a plate atop a large chalice placed on the altar. For the Papal conclave, 2005, a special urn was used for this purpose instead of a chalice and plate. The plate is then used to drop the ballot into the chalice, making it difficult for electors to insert multiple ballots. Before being read, the ballots are counted while still folded; if the number of ballots does not match the number of electors, the ballots are burned unopened and a new vote is held. Otherwise, each ballot is read aloud by the presiding Cardinal, who pierces the ballot with a needle and thread, stringing all the ballots together and tying the ends of the thread to ensure accuracy and honesty. Balloting continues until someone is elected by a two-thirds majority. 
One of the most prominent aspects of the papal election process is the means by which the results of a ballot are announced to the world. Once the ballots are counted and bound together, they are burned in a special stove erected in the Sistine Chapel, with the smoke escaping through a small chimney visible from Saint Peter's Square. The ballots from an unsuccessful vote are burned along with a chemical compound to create black smoke, or "fumata nera". (Traditionally, wet straw was used to produce the black smoke, but this was not completely reliable. The chemical compound is more reliable than the straw.) When a vote is successful, the ballots are burned alone, sending white smoke ("fumata bianca") through the chimney and announcing to the world the election of a new pope. Starting with the Papal conclave, 2005, church bells are also rung as a signal that a new pope has been chosen. 

The Dean of the College of Cardinals then asks two solemn questions of the cardinal who has been elected. First he asks, "Do you freely accept your election as Supreme Pontiff?" If he replies with the word ""Accepto"", his reign begins at that instant. If he replies "not", his reign begins at the inauguration ceremony several days afterward. The Dean asks next, "By what name shall you be called?" The new pope announces the regnal name he has chosen. If the Dean himself is elected pope, the Vice Dean performs this task. 

The new pope is led through the "Door of Tears" to a dressing room where three sets of white papal vestments ("immantatio") await: small, medium, and large. Donning the appropriate vestments and reemerging into the Sistine Chapel, the new pope is given the "Fisherman's Ring" by the Camerlengo of the Holy Roman Church, whom he first either reconfirms or reappoints. The pope assumes a place of honor as the rest of the cardinals wait in turn to offer their first "obedience" ("adoratio") and to receive his blessing. 

The Senior Cardinal Deacon announces from a balcony over St. Peter's Square the following proclamation: "Annuntio vobis gaudium magnum! Habemus Papam!" ("I announce to you a great joy! We have a pope!"). He announces the new pope's Christian name along with his newly chosen regnal name. 

Until 1978 the pope's election was followed in a few days by the Papal coronation, which started with a procession with great pomp and circumstance from the Sistine Chapel to St. Peter's Basilica, with the newly elected pope borne in the "sedia gestatoria". After a solemn Papal Mass, the new pope was crowned with the "triregnum" (papal tiara) and he gave for the first time as pope the famous blessing "Urbi et Orbi" ("to the City [Rome] and to the World"). Another renowned part of the coronation was the lighting of a bundle of flax at the top of a gilded pole, which would flare brightly for a moment and then promptly extinguish, as he said, "Sic transit gloria mundi" ("Thus passes worldly glory"). A similar warning against papal hubris made on this occasion was the traditional exclamation, ""Annos Petri non videbis"", reminding the newly crowned pope that he would not live to see his rule lasting as long as that of St. Peter. According to tradition, he headed the church for 35 years and has thus far been the longest-reigning pope in the history of the Catholic Church. 

A traditionalist Catholic belief that lacks reliable authority claims that a Papal Oath was sworn, at their coronation, by all popes from Pope Agatho to Pope Paul VI and that it was omitted with the abolition of the coronation ceremony.

The Latin term, "sede vacante" ("while the see is vacant"), refers to a papal interregnum, the period between the death or resignation of a pope and the election of his successor. From this term is derived the term sedevacantism, which designates a category of dissident Catholics who maintain that there is no canonically and legitimately elected pope, and that there is therefore a "sede vacante". One of the most common reasons for holding this belief is the idea that the reforms of the Second Vatican Council, and especially the reform of the Tridentine Mass with the Mass of Paul VI, are heretical and that those responsible for initiating and maintaining these changes are heretics and not true popes.

For centuries, from 1378 on, those elected to the papacy were predominantly Italians. Prior to the election of the Polish cardinal Karol Wojtyla as Pope John Paul II in 1978, the last non-Italian was Pope Adrian VI of the Netherlands, elected in 1522. John Paul II was followed by election of the German-born Pope Benedict XVI, who was in turn followed by Argentine-born Pope Francis, who is the first non-European after 1272 years and the first Latin American, despite having an Italian ancestry. 

The current regulations regarding a papal interregnum—that is, a "sede vacante" ("vacant seat")—were promulgated by Pope John Paul II in his 1996 document "Universi Dominici Gregis". During the "sede vacante" period, the College of Cardinals is collectively responsible for the government of the Church and of the Vatican itself, under the direction of the Camerlengo of the Holy Roman Church; however, canon law specifically forbids the cardinals from introducing any innovation in the government of the Church during the vacancy of the Holy See. Any decision that requires the assent of the pope has to wait until the new pope has been elected and accepts office. 

In recent centuries, when a pope was judged to have died, it was reportedly traditional for the Cardinal Camerlengo to confirm the death ceremonially by gently tapping the pope's head thrice with a silver hammer, calling his birth name each time. This was not done on the deaths of popes John Paul I and John Paul II. The Cardinal Camerlengo retrieves the Ring of the Fisherman and cuts it in two in the presence of the Cardinals. The pope's seals are defaced, to keep them from ever being used again, and his personal apartment is sealed. 

The body lies in state for several days before being interred in the crypt of a leading church or cathedral; all popes who have died in the 20th and 21st centuries have been interred in St. Peter's Basilica. A nine-day period of mourning ("novendialis") follows the interment.

It is highly unusual for a pope to resign. The 1983 Code of Canon Law states, "If it happens that the Roman Pontiff resigns his office, it is required for validity that the resignation is made freely and properly manifested but not that it is accepted by anyone." Benedict XVI, who vacated the Holy See on 28 February 2013, was the most recent to do so since Gregory XII's resignation in 1415.

The official list of titles of the pope, in the order in which they are given in the "Annuario Pontificio", is:

The best-known title, that of "Pope", does not appear in the official list, but is commonly used in the titles of documents, and appears, in abbreviated form, in their signatures. Thus Pope Paul VI signed as "Paulus PP. VI", the "PP." standing for ""papa"" ("pope").

The title "Pope" was from the early 3rd century an honorific designation used for "any" bishop in the West. In the East, it was used only for the Bishop of Alexandria. Pope Marcellinus (d. 304) is the first Bishop of Rome shown in sources to have had the title "Pope" used of him. From the 6th century, the imperial chancery of Constantinople normally reserved this designation for the Bishop of Rome. From the early 6th century, it began to be confined in the West to the Bishop of Rome, a practice that was firmly in place by the 11th century, when Pope Gregory VII declared it reserved for the Bishop of Rome. 

In Eastern Christianity, where the title "Pope" is used also of the Bishop of Alexandria, the Bishop of Rome is often referred to as the "Pope of Rome", regardless of whether the speaker or writer is in communion with Rome or not. 

"Vicar of Jesus Christ" ("Vicarius Iesu Christi") is one of the official titles of the Pope given in the "Annuario Pontificio". It is commonly used in the slightly abbreviated form "Vicar of Christ" ("Vicarius Christi"). While it is only one of the terms with which the pope is referred to as "Vicar", it is "more expressive of his supreme headship of the Church on Earth, which he bears in virtue of the commission of Christ and with vicarial power derived from him", a vicarial power believed to have been conferred on Saint Peter when Christ said to him: "Feed my lambs...Feed my sheep" ().

The first record of the application of this title to a Bishop of Rome appears in a synod of 495 with reference to Pope Gelasius I. But at that time, and down to the 9th century, other bishops too referred to themselves as vicars of Christ, and for another four centuries this description was sometimes used of kings and even judges, as it had been used in the 5th and 6th centuries to refer to the Byzantine emperor. Earlier still, in the 3rd century, Tertullian used "vicar of Christ" to refer to the Holy Spirit sent by Jesus. Its use specifically for the pope appears in the 13th century in connection with the reforms of Pope Innocent III, as can be observed already in his 1199 letter to Leo I, King of Armenia. Other historians suggest that this title was already used in this way in association with the pontificate of Pope Eugene III (1145–1153).

This title "Vicar of Christ" is thus not used of the pope alone and has been used of all bishops since the early centuries. The Second Vatican Council referred to all bishops as "vicars and ambassadors of Christ", and this description of the bishops was repeated by Pope John Paul II in his encyclical "Ut unum sint," 95. The difference is that the other bishops are vicars of Christ for their own local churches, the pope is vicar of Christ for the whole Church.

On at least one occasion the title "Vicar of God" (a reference to Christ as God) was used of the pope.

The title "Vicar of Peter" ("Vicarius Petri") is used only of the pope, not of other bishops. Variations of it include: "Vicar of the Prince of the Apostles" ("Vicarius Principis Apostolorum") and "Vicar of the Apostolic See" ("Vicarius Sedis Apostolicae"). Saint Boniface described Pope Gregory II as vicar of Peter in the oath of fealty that he took in 722. In today's Roman Missal, the description "vicar of Peter" is found also in the collect of the Mass for a saint who was a pope.

The term "pontiff" is derived from the , which literally means "bridge builder" ("pons" + "facere") and which designated a member of the principal college of priests in ancient Rome. The Latin word was translated into ancient Greek variously: as , , , (hierophant), or (archiereus, high priest) The head of the college was known as the Pontifex Maximus (the greatest pontiff).

In Christian use, "pontifex" appears in the Vulgate translation of the New Testament to indicate the High Priest of Israel (in the original Koine Greek, ). The term came to be applied to any Christian bishop, but since the 11th century commonly refers specifically to the Bishop of Rome, who is more strictly called the "Roman Pontiff". The use of the term to refer to bishops in general is reflected in the terms "Roman Pontifical" (a book containing rites reserved for bishops, such as confirmation and ordination), and "pontificals" (the insignia of bishops).

The "Annuario Pontificio" lists as one of the official titles of the pope that of "Supreme Pontiff of the Universal Church" (). He is also commonly called the Supreme Pontiff or the Sovereign Pontiff (). 

"Pontifex Maximus", similar in meaning to "Summus Pontifex", is a title commonly found in inscriptions on papal buildings, paintings, statues and coins, usually abbreviated as "Pont. Max" or "P.M." The office of Pontifex Maximus, or head of the College of Pontiffs, was held by Julius Caesar and thereafter, by the Roman emperors, until Gratian (375–383) relinquished it. Tertullian, when he had become a Montanist, used the title derisively of either the pope or the Bishop of Carthage. The popes began to use this title regularly only in the 15th century.

Although the description "servant of the servants of God" () was also used by other Church leaders, including Augustine of Hippo and Benedict of Nursia, it was first used extensively as a papal title by Pope Gregory I, reportedly as a lesson in humility for the Patriarch of Constantinople, John the Faster, who had assumed the title "Ecumenical Patriarch". It became reserved for the pope in the 12th century and is used in papal bulls and similar important papal documents.

From 1863 until 2005, the "Annuario Pontificio" also included the title "Patriarch of the West". This title was first used by Pope Theodore I in 642, and was only used occasionally. Indeed, it did not begin to appear in the pontifical yearbook until 1863. On 22 March 2006, the Vatican released a statement explaining this omission on the grounds of expressing a "historical and theological reality" and of "being useful to ecumenical dialogue". The title Patriarch of the West symbolized the pope's special relationship with, and jurisdiction over, the Latin Church—and the omission of the title neither symbolizes in any way a change in this relationship, nor distorts the relationship between the Holy See and the Eastern Churches, as solemnly proclaimed by the Second Vatican Council.

Other titles commonly used are "His Holiness" (either used alone or as an honorific prefix "His Holiness Pope Francis"; and as "Your Holiness" as a form of address), "Holy Father". In Spanish and Italian, ""Beatísimo/Beatissimo Padre"" (Most Blessed Father) is often used in preference to ""Santísimo/Santissimo Padre"" (Most Holy Father). In the medieval period, ""Dominus Apostolicus"" ("the Apostolic Lord") was also used.

Pope Francis signs some documents with his name alone, either in Latin ("Franciscus", as in an encyclical dated 29 June 2013) or in another language. Other documents he signs in accordance with the tradition of using Latin only and including, in the abbreviated form "PP.", the description "Papa". Popes who have an ordinal numeral in their name traditionally place the abbreviation "PP." before the ordinal numeral, as in "Benedictus PP. XVI" (Pope Benedict XVI), except in bulls of canonization and decrees of ecumenical councils, which a pope signs with the formula, "Ego N. Episcopus Ecclesiae catholicae", without the numeral, as in "Ego Benedictus Episcopus Ecclesiae catholicae" (I, Benedict, Bishop of the Catholic Church). The pope's signature is followed, in bulls of canonization, by those of all the cardinals resident in Rome, and in decrees of ecumenical councils, by the signatures of the other bishops participating in the council, each signing as Bishop of a particular see.

Papal bulls are headed "N. Episcopus Servus Servorum Dei" ("Name, Bishop, Servant of the Servants of God"). In general, they are not signed by the pope, but Pope John Paul II introduced in the mid-1980s the custom by which the pope signs not only bulls of canonization but also, using his normal signature, such as "Benedictus PP. XVI", bulls of nomination of bishops. 

In heraldry, each pope has his own personal coat of arms. Though unique for each pope, the arms have for several centuries been traditionally accompanied by two keys in saltire (i.e., crossed over one another so as to form an "X") behind the escutcheon (shield) (one silver key and one gold key, tied with a red cord), and above them a silver "triregnum" with three gold crowns and red "infulae" (lappets—two strips of fabric hanging from the back of the triregnum which fall over the neck and shoulders when worn). This is blazoned: "two keys in saltire or and argent, interlacing in the rings or, beneath a tiara argent, crowned or". The 21st century has seen departures from this tradition. In 2005, Pope Benedict XVI, while maintaining the crossed keys behind the shield, omitted the papal tiara from his personal coat of arms, replacing it with a mitre with three horizontal lines. Beneath the shield he added the pallium, a papal symbol of authority more ancient than the tiara, the use of which is also granted to metropolitan archbishops as a sign of communion with the See of Rome. Although the tiara was omitted in the pope's personal coat of arms, the coat of arms of the Holy See, which includes the tiara, remained unaltered. In 2013, Pope Francis maintained the mitre that replaced the tiara, but omitted the pallium. He also departed from papal tradition by adding beneath the shield his personal pastoral motto: "Miserando atque eligendo". 

The flag most frequently associated with the pope is the yellow and white flag of Vatican City, with the arms of the Holy See (blazoned: "Gules, two keys in saltire or and argent, interlacing in the rings or, beneath a tiara argent, crowned or") on the right-hand side (the "fly") in the white half of the flag (the left-hand side—the "hoist"—is yellow). The pope's escucheon does not appear on the flag. This flag was first adopted in 1808, whereas the previous flag had been red and gold. Although Pope Benedict XVI replaced the triregnum with a mitre on his personal coat of arms, it has been retained on the flag. 

Pope Pius V (reigned 1566–1572), is often credited with having originated the custom whereby the pope wears white, by continuing after his election to wear the white habit of the Dominican order. In reality, the basic papal attire was white long before. The earliest document that describes it as such is the "Ordo XIII", a book of ceremonies compiled in about 1274. Later books of ceremonies describe the pope as wearing a red mantle, mozzetta, camauro and shoes, and a white cassock and stockings. Many contemporary portraits of 15th and 16th-century predecessors of Pius V show them wearing a white cassock similar to his.

The status and authority of the Pope in the Catholic Church was dogmatically defined by the First Vatican Council on 18 July 1870. In its Dogmatic Constitution of the Church of Christ, the Council established the following canons:

If anyone says that the blessed Apostle Peter was not established by the Lord Christ as the chief of all the apostles, and the visible head of the whole militant Church, or, that the same received great honour but did not receive from the same our Lord Jesus Christ directly and immediately the primacy in true and proper jurisdiction: let him be anathema.

If anyone says that it is not from the institution of Christ the Lord Himself, or by divine right that the blessed Peter has perpetual successors in the primacy over the universal Church, or that the Roman Pontiff is not the successor of blessed Peter in the same primacy, let him be anathema.

If anyone thus speaks, that the Roman Pontiff has only the office of inspection or direction, but not the full and supreme power of jurisdiction over the universal Church, not only in things which pertain to faith and morals, but also in those which pertain to the discipline and government of the Church spread over the whole world; or, that he possesses only the more important parts, but not the whole plenitude of this supreme power; or that this power of his is not ordinary and immediate, or over the churches altogether and individually, and over the pastors and the faithful altogether and individually: let him be anathema.

We, adhering faithfully to the tradition received from the beginning of the Christian faith, to the glory of God, our Saviour, the elevation of the Catholic religion and the salvation of Christian peoples, with the approbation of the sacred Council, teach and explain that the dogma has been divinely revealed: that the Roman Pontiff, when he speaks ex cathedra, that is, when carrying out the duty of the pastor and teacher of all Christians by his supreme apostolic authority he defines a doctrine of faith or morals to be held by the universal Church, through the divine assistance promised him in blessed Peter, operates with that infallibility with which the divine Redeemer wished that His church be instructed in defining doctrine on faith and morals; and so such definitions of the Roman Pontiff from himself, but not from the consensus of the Church, are unalterable. But if anyone presumes to contradict this definition of Ours, which may God forbid: let him be anathema.

In its Dogmatic Constitution on the Church (1964), the Second Vatican Council declared:

On 11 October 2012, on the occasion of the 50th anniversary of the opening of the Second Vatican Council 60 prominent theologians, (including Hans Küng), put out a Declaration, stating that the intention of Vatican II to balance authority in the Church has not been realised. "Many of the key insights of Vatican II have not at all, or only partially, been implemented . . . A principal source of present-day stagnation lies in misunderstanding and abuse affecting the exercise of authority in our Church."

The pope's official seat or cathedral is the Archbasilica of St. John Lateran, and his official residence is the Apostolic Palace. He also possesses a summer residence at Castel Gandolfo, situated on the site of the ancient city of Alba Longa. Until the time of the Avignon Papacy, the residence of the pope was the Lateran Palace, donated by the Roman emperor Constantine the Great. 

The pope's ecclesiastical jurisdiction (the Holy See) is distinct from his secular jurisdiction (Vatican City). It is the Holy See that conducts international relations; for hundreds of years, the papal court (the Roman Curia) has functioned as the government of the Catholic Church. 

The names "Holy See" and "Apostolic see" are ecclesiastical terminology for the ordinary jurisdiction of the Bishop of Rome (including the Roman Curia); the pope's various honors, powers, and privileges within the Catholic Church and the international community derive from his Episcopate of Rome in lineal succession from the Apostle Saint Peter (see Apostolic succession). Consequently, Rome has traditionally occupied a central position in the Catholic Church, although this is not necessarily so. The pope derives his pontificate from being Bishop of Rome but is not required to live there; according to the Latin formula "ubi Papa, ibi Curia", wherever the pope resides is the central government of the Church, provided that the pope is Bishop of Rome. As such, between 1309 and 1378, the popes lived in Avignon, France (see Avignon Papacy), a period often called the Babylonian captivity in allusion to the Biblical narrative of Jews of the ancient Kingdom of Judah living as captives in Babylonia.

Though the pope is the diocesan Bishop of the Diocese of Rome, he delegates most of the day-to-day work of leading the diocese to the Cardinal Vicar, who assures direct episcopal oversight of the diocese's pastoral needs, not in his own name but in that of the pope. The current Cardinal Vicar is Agostino Vallini, who was appointed to the office in June 2008. 

Though the progressive Christianisation of the Roman Empire in the 4th century did not confer upon bishops civil authority within the state, the gradual withdrawal of imperial authority during the 5th century left the pope the senior imperial civilian official in Rome, as bishops were increasingly directing civil affairs in other cities of the Western Empire. This status as a secular and civil ruler was vividly displayed by Pope Leo I's confrontation with Attila in 452. The first expansion of papal rule outside of Rome came in 728 with the Donation of Sutri, which in turn was substantially increased in 754, when the Frankish ruler Pippin the Younger gave to the pope the land from his conquest of the Lombards. The pope may have utilized the forged Donation of Constantine to gain this land, which formed the core of the Papal States. This document, accepted as genuine until the 15th century, states that Constantine the Great placed the entire Western Empire of Rome under papal rule. In 800, Pope Leo III crowned the Frankish ruler Charlemagne as Roman Emperor, a major step toward establishing what later became known as the Holy Roman Empire; from that date onward the popes claimed the prerogative to crown the Emperor, though the right fell into disuse after the coronation of Charles V in 1530. Pope Pius VII was present at the coronation of Napoleon I in 1804 but did not actually perform the crowning. As mentioned above, the pope's sovereignty over the Papal States ended in 1870 with their annexation by Italy.

Popes like Alexander VI, an ambitious if spectacularly corrupt politician, and Pope Julius II, a formidable general and statesman, were not afraid to use power to achieve their own ends, which included increasing the power of the papacy. This political and temporal authority was demonstrated through the papal role in the Holy Roman Empire (especially prominent during periods of contention with the Emperors, such as during the Pontificates of Pope Gregory VII and Pope Alexander III). Papal bulls, interdict, and excommunication (or the threat thereof) have been used many times to increase papal power. The Bull "Laudabiliter" in 1155 authorized Henry II of England to invade Ireland. In 1207, Innocent III placed England under interdict until King John made his kingdom a fiefdom to the Pope, complete with yearly tribute, saying, "we offer and freely yield...to our lord Pope Innocent III and his catholic successors, the whole kingdom of England and the whole kingdom of Ireland with all their rights and appurtenences for the remission of our sins". The Bull "Inter caetera" in 1493 led to the Treaty of Tordesillas in 1494, which divided the world into areas of Spanish and Portuguese rule. The Bull "Regnans in Excelsis" in 1570 excommunicated Elizabeth I of England and declared that all her subjects were released from all allegiance to her. The Bull, "Inter gravissimas", in 1582 established the Gregorian calendar.

Under international law, a serving head of state has sovereign immunity from the jurisdiction of the courts of other countries, though not from that of international tribunals. This immunity is sometimes loosely referred to as "diplomatic immunity", which is, strictly speaking, the immunity enjoyed by the "diplomatic representatives" of a head of state.

International law treats the Holy See, essentially the central government of the Catholic Church, as the juridical equal of a state. It is distinct from the state of Vatican City, existing for many centuries before the foundation of the latter. (It is common for publications and news media to use "the Vatican", "Vatican City", and even "Rome" as metonyms for the Holy See.) Most countries of the world maintain the same form of diplomatic relations with the Holy See that they entertain with other states. Even countries without those diplomatic relations participate in international organizations of which the Holy See is a full member.

It is as head of the state-equivalent worldwide religious jurisdiction of the Holy See (not of the territory of Vatican City) that the U.S. Justice Department ruled that the pope enjoys head-of-state immunity. This head-of-state immunity, recognized by the United States, must be distinguished from that envisaged under the United States' Foreign Sovereign Immunities Act of 1976, which, while recognizing the basic immunity of foreign governments from being sued in American courts, lays down nine exceptions, including commercial activity and actions in the United States by agents or employees of the foreign governments. It was in relation to the latter that, in November 2008, the United States Court of Appeals in Cincinnati decided that a case over sexual abuse by Catholic priests could proceed, provided the plaintiffs could prove that the bishops accused of negligent supervision were acting as employees or agents of the Holy See and were following official Holy See policy.

In April 2010, there was press coverage in Britain concerning a proposed plan by atheist campaigners and a prominent barrister to have Pope Benedict XVI arrested and prosecuted in the UK for alleged offences, dating from several decades before, in failing to take appropriate action regarding Catholic sex abuse cases and concerning their disputing his immunity from prosecution in that country. This was generally dismissed as "unrealistic and spurious". Another barrister said that it was a "matter of embarrassment that a senior British lawyer would want to allow himself to be associated with such a silly idea".

The pope's claim to authority is either disputed or not recognised at all by other churches. The reasons for these objections differ from denomination to denomination.

Other traditional Christian churches (Assyrian Church of the East, the Oriental Orthodox Church, the Eastern Orthodox Church, the Old Catholic Church, the Anglican Communion, the Independent Catholic churches, etc.) accept the doctrine of Apostolic succession and, to varying extents, papal claims to a primacy of honour, while generally rejecting the pope as the successor to Peter in any other sense than that of other bishops. Primacy is regarded as a consequence of the pope's position as bishop of the original capital city of the Roman Empire, a definition explicitly spelled out in the 28th canon of the Council of Chalcedon. These churches see no foundation to papal claims of "universal immediate jurisdiction", or to claims of papal infallibility. Several of these churches refer to such claims as "ultramontanism".

Protestant denominations of Christianity reject the claims of Petrine primacy of honor, Petrine primacy of jurisdiction, and papal infallibility. These denominations vary from simply not accepting the pope's claim to authority as legitimate and valid, to believing that the pope is the Antichrist from 1 John 2:18, the Man of Sin from 2 Thessalonians 2:3–12, and the Beast out of the Earth from Revelation 13:11–18.

This sweeping rejection is held by, among others, some denominations of Lutherans: Confessional Lutherans hold that the pope is the Antichrist, stating that this article of faith is part of a "quia" ("because") rather than "quatenus" ("insofar as") subscription to the Book of Concord. In 1932, one of these Confessional churches, the Lutheran Church–Missouri Synod (LCMS), adopted "A Brief Statement of the Doctrinal Position of the Missouri Synod", which a small number of Lutheran church bodies now hold. The Lutheran Churches of the Reformation, the Concordia Lutheran Conference, the Church of the Lutheran Confession, and the Illinois Lutheran Conference all hold to the "Brief Statement", which the LCMS places on its website. The Wisconsin Evangelical Lutheran Synod (WELS), another Confessional Lutheran church that declares the Papacy to be the Antichrist, released its own statement, the "Statement on the Antichrist", in 1959. The WELS still holds to this statement.

Historically, Protestants objected to the papacy's claim of temporal power over all secular governments, including territorial claims in Italy, the papacy's complex relationship with secular states such as the Roman and Byzantine Empires, and the autocratic character of the papal office. In Western Christianity these objections both contributed to and are products of the Protestant Reformation.

Groups sometimes form around antipopes, who claim the Pontificate without being canonically and properly elected to it.

Traditionally, this term was reserved for claimants with a significant following of cardinals or other clergy. The existence of an antipope is usually due either to doctrinal controversy within the Church (heresy) or to confusion as to who is the legitimate pope at the time (schism). Briefly in the 15th century, three separate lines of popes claimed authenticity (see Papal Schism). Even Catholics do not all agree whether certain historical figures were popes or antipopes. Though antipope movements were significant at one time, they are now overwhelmingly minor fringe causes.

In the earlier centuries of Christianity, the title "Pope", meaning "father", had been used by all bishops. Some popes used the term and others did not. Eventually, the title became associated especially with the Bishop of Rome. In a few cases, the term is used for other Christian clerical authorities.

In English, Catholic priests are still addressed as "father", but the term "pope" is reserved for the head of the church hierarchy.

"Black Pope" is a name that was popularly, but unofficially, given to the Superior General of the Society of Jesus due to the Jesuits' importance within the Church. This name, based on the black colour of his cassock, was used to suggest a parallel between him and the "White Pope" (since the time of Pope Pius V the popes dress in white) and the Cardinal Prefect of the Congregation for the Evangelization of Peoples (formerly called the Sacred Congregation for the Propagation of the Faith), whose red cardinal's cassock gave him the name of the "Red Pope" in view of the authority over all territories that were not considered in some way Catholic. In the present time this cardinal has power over mission territories for Catholicism, essentially the Churches of Africa and Asia, but in the past his competence extended also to all lands where Protestants or Eastern Christianity was dominant. Some remnants of this situation remain, with the result that, for instance, New Zealand is still in the care of this Congregation.

Since the papacy of Heraclas in the 3rd century, the Bishop of the Alexandria in both the Coptic Orthodox Church of Alexandria and the Greek Orthodox Church of Alexandria continue to be called "Pope", the former being called "Coptic Pope" or, more properly, "Pope and Patriarch of All Africa on the Holy Orthodox and Apostolic Throne of Saint Mark the Evangelist and Holy Apostle" and the latter called "Pope and Patriarch of Alexandria and All Africa".

In the Bulgarian Orthodox Church, Russian Orthodox Church and Serbian Orthodox Church, it is not unusual for a village priest to be called a "pope" ("поп" "pop"). However, this should be differentiated from the words used for the head of the Catholic Church (Bulgarian "папа" "papa", Russian "папа римский" "papa rimskiy").

Some new religious movements including ones within Christianity, especially those that have disassociated themselves from the Catholic Church yet retain a Catholic hierarchical framework, have used the designation "pope" for a movement's founder or current leader. Examples like in Africa is the Legio Maria Church of Africa and in European Spain is Palmarian Catholic Church. Another example is Cao Dai, a Vietnamese faith that duplicates the Catholic hierarchy, which is declared legitimate by religious authorities in Cao Dai due to the fact that, according to them, God created both Catholicism and Cao Dai.

Although the average reign of the pope from the Middle Ages was a decade, a number of those whose reign lengths can be determined from contemporary historical data are the following:

During the Western Schism, Avignon Pope Benedict XIII (1394–1423) ruled for 28 years, seven months and 12 days, which would place him third in the above list. However, since he is regarded as an anti-pope, he is not mentioned in the list above.

There have been a number of popes whose reign lasted about a month or less. In the following list the number of calendar days includes partial days. Thus, for example, if a pope's reign commenced on 1 August and he died on 2 August, this would count as having reigned for two calendar days.

Stephen (23–26 March 752) died of stroke three days after his election, and before his consecration as a bishop. He is not recognized as a valid pope, but was added to the lists of popes in the 15th century as "Stephen II", causing difficulties in enumerating later popes named Stephen. The Holy See's "Annuario Pontificio", in its list of popes and antipopes, attaches a footnote to its mention of Stephen II (III):

Published every year by the Roman Curia, the "Annuario Pontificio" attaches no consecutive numbers to the popes, stating that it is impossible to decide which side represented at various times the legitimate succession, in particular regarding Pope Leo VIII, Pope Benedict V and some mid-11th-century popes.




</doc>
<doc id="23059" url="https://en.wikipedia.org/wiki?curid=23059" title="Passover">
Passover

Passover or Pesach (; from Hebrew "Pesah, Pesakh") is a major, biblically derived Jewish holiday. Jews celebrate Passover as a commemoration of their liberation by God from slavery in ancient Egypt and their freedom as a nation under the leadership of Moses. It commemorates the story of the Exodus as described in the Hebrew Bible, especially in the Book of Exodus, in which the Israelites were freed from slavery in Egypt. According to standard biblical chronology, this event would have taken place at about 1300 BCE (AM 2450).

Passover is a spring festival which during the existence of the Temple in Jerusalem was connected to the offering of the "first-fruits of the barley", barley being the first grain to ripen and to be harvested in the Land of Israel.

Passover commences on the 15th of the Hebrew month of Nisan and lasts for either seven days (in Israel and for Reform Jews and other progressive Jews around the world who adhere to the Biblical commandment) or eight days for Orthodox, Hasidic, and most Conservative Jews (in the diaspora). In Judaism, a day commences at dusk and lasts until the following dusk, thus the first day of Passover begins after dusk of the 14th of Nisan and ends at dusk of the 15th day of the month of Nisan. The rituals unique to the Passover celebrations commence with the Passover Seder when the 15th of Nisan has begun. In the Northern Hemisphere Passover takes place in spring as the Torah prescribes it: "in the month of [the] spring" ( ). It is one of the most widely celebrated Jewish holidays.

In the narrative of the Exodus, the Bible tells that God helped the Children of Israel escape from their slavery in Egypt by inflicting ten plagues upon the ancient Egyptians before the Pharaoh would release his Israelite slaves; the tenth and worst of the plagues was the .

The Israelites were instructed to mark the doorposts of their homes with the blood of a slaughtered spring lamb and, upon seeing this, the spirit of the Lord knew to "pass over" the first-born in these homes, hence the English name of the holiday.

When the Pharaoh freed the Israelites, it is said that they left in such a hurry that they could not wait for bread dough to rise (leaven). In commemoration, for the duration of Passover no leavened bread is eaten, for which reason Passover was called the feast of unleavened bread in the Torah or Old Testament. Thus "matzo" (flat unleavened bread) is eaten during Passover and it is a tradition of the holiday.

Historically, together with Shavuot ("Pentecost") and Sukkot ("Tabernacles"), Passover is one of the Three Pilgrimage Festivals ("Shalosh Regalim") during which the entire population of the kingdom of Judah made a pilgrimage to the Temple in Jerusalem. Samaritans still make this pilgrimage to Mount Gerizim, but only men participate in public worship.

The Passover begins on the 15th day of the month of Nisan, which typically falls in March or April of the Gregorian calendar. Passover is a spring festival, so the 15th day of Nisan typically begins on the night of a full moon after the northern vernal equinox. However, due to intercalary months or leap months falling after the vernal equinox, Passover sometimes starts on the second full moon after vernal equinox, as in 2016.

To ensure that Passover did not start before spring, the tradition in ancient Israel held that the first day of Nisan would not start until the barley was ripe, being the test for the onset of spring. If the barley was not ripe, or various other phenomena indicated that spring was not yet imminent, an intercalary month (Adar II) would be added. However, since at least the 4th century, the date has been fixed mathematically.

In Israel, Passover is the seven-day holiday of the Feast of Unleavened Bread, with the first and last days celebrated as legal holidays and as holy days involving holiday meals, special prayer services, and abstention from work; the intervening days are known as Chol HaMoed ("Weekdays [of] the Festival"). Diaspora Jews historically celebrated the festival for eight days. Reform and Reconstructionist Jews and Israeli Jews, wherever they are, usually celebrate the holiday over seven days. The reason for this extra day is due to enactment of the ancient Jewish sages. It is thought by many scholars that Jews outside of Israel could not be certain if their local calendars fully conformed to practice of the Temple at Jerusalem, so they added an extra day. But as this practice attaches only to certain (major) sacred days, others posit the extra day may have been added to accommodate people who had to travel long distances to participate in communal worship and ritual practices; or the practice may have evolved as a compromise between conflicting interpretations of Jewish Law regarding the calendar; or it may have evolved as a safety measure in areas where Jews were commonly in danger, so that their enemies would not be certain on which day to attack.

Karaites and Samaritans use different versions of the Jewish calendar, which are often out of sync with the modern Jewish calendar by one or two days. In "2009", for example, Nisan 15 on the Jewish calendar used by Rabbinic Judaism corresponds to April 9. On the calendars used by Karaites and Samaritans, "Abib" or "Aviv" 15 (as opposed to 'Nisan') corresponds to April 11 in "2009". The Karaite and Samaritan Passovers are each one day long, followed by the six-day Festival of Unleavened Bread – for a total of seven days.

The origins of the Passover festival antedate the Exodus. The Passover ritual, prior to Deuteronomy, is widely thought to have its origins in an apotropaic rite, unrelated to the Exodus, to ensure the protection of a family home, a rite conducted wholly within a clan. Hyssop was employed to daub the blood of a slaughtered sheep on the lintels and door posts to ensure that demonic forces could not enter the home. A further hypothesis maintains that, once the Priestly Code was promulgated, the exodus narrative took on a central function, as the apotropaic rite was, arguably, amalgamated with the Canaanite agricultural festival of spring which was a ceremony of Unleavened Bread, connected with the barley harvest. As the Exodus motif grew, the original function and symbolism of these double origins was lost. Several motifs replicate the features associated with the Mesopotamian Akitu festival. Other scholars, John Van Seters, J.B.Segal and Tamara Prosic disagree with the merged two-festivals hypothesis.

Called the "festival [of] the matzot" (Hebrew: חג המצות "hag hamatzot") in the Hebrew Bible, the commandment to keep Passover is recorded in the Book of Leviticus:

In the first month, on the fourteenth day of the month at dusk is the LORD's Passover. And on the fifteenth day of the same month is the feast of unleavened bread unto the LORD; seven days ye shall eat unleavened bread. In the first day ye shall have a holy convocation; ye shall do no manner of servile work. And ye shall bring an offering made by fire unto the LORD seven days; in the seventh day is a holy convocation; ye shall do no manner of servile work. ()

The biblical regulations for the observance of the festival require that all leavening be disposed of before the beginning of the 15th of Nisan An unblemished lamb or goat, known as the Korban Pesach or "Paschal Lamb", is to be set apart on Nisan 10, and slaughtered at dusk as Nisan 14 ends in preparation for the 15th of Nisan when it will be eaten after being roasted. The literal meaning of the Hebrew is "between the two evenings". It is then to be eaten "that night", Nisan 15, roasted, without the removal of its internal organs with unleavened bread, known as matzo, and bitter herbs known as maror. Nothing of the sacrifice on which the sun rises by the morning of the 15th of Nisan may be eaten, but must be burned. The sacrifices may be performed only in a specific place prescribed by God (for Judaism, Jerusalem, and for Samaritans, Mount Gerizim).

The biblical regulations pertaining to the original Passover, at the time of the Exodus only, also include how the meal was to be eaten: "with your loins girded, your shoes on your feet, and your staff in your hand; and ye shall eat it in haste: it is the LORD's passover" .

The biblical requirements of slaying the Paschal lamb in the individual homes of the Hebrews and smearing the blood of the lamb on their doorways were celebrated in Egypt. However, once Israel was in the wilderness and the tabernacle was in operation, a change was made in those two original requirements (). Passover lambs were to be sacrificed at the door of the tabernacle and no longer in the homes of the Jews. No longer, therefore, could blood be smeared on doorways.

The biblical commandments concerning the Passover (and the Feast of Unleavened Bread) stress the importance of remembering:


Some of these details can be corroborated, and to some extent amplified, in extrabiblical sources. The removal (or "sealing up") of the leaven is referred to in the Elephantine papyri, an Aramaic papyrus from 5th century BCE Elephantine in Egypt. The slaughter of the lambs on the 14th is mentioned in "The Book of Jubilees", a Jewish work of the Ptolemaic period, and by the Herodian-era writers Josephus and Philo. These sources also indicate that "between the two evenings" was taken to mean the afternoon. "Jubilees" states the sacrifice was eaten that night, and together with Josephus states that nothing of the sacrifice was allowed to remain until morning. Philo states that the banquet included hymns and prayers.

The English term "Passover" is first known to be recorded in the English language in William Tyndale's translation of the Bible, later appearing in the King James Version as well. It is a literal translation of the Hebrew term.

The Hebrew is rendered as Tiberian , and Modern Hebrew: "Pesah, Pesakh;" The Yiddish word is Latinized variously as "Peysekh, Paysakh, Paysokh". The etymology is disputed, and hypotheses are divided whether to connect it to "psh" (to protect, save) or to a word meaning 'limp, dance with limping motions.' Cognate languages yield similar terms with distinct meanings, such as 'make soft, soothe, placate' (Akkadian "passahu"), 'harvest, commemoration, blow' (Egyptian), or 'separate' (Arabic "fsh"")".

The verb ""pasàch"" () is first mentioned in the Torah's account of the Exodus from Egypt (), and there is some debate about its exact meaning: the commonly held assumption that it means "He passed over" (פסח), in reference to God "passing over" (or "skipping") the houses of the Hebrews during the final of the Ten Plagues of Egypt, stems from the translation provided in the Septuagint (παρελευσεται in , and εσκεπασεν in ).
Targum Onkelos translates "pesach" as וְיֵחוֹס "he had pity" coming from the Hebrew root ‘חסה’ meaning to have pity.

Judging from other instances of the verb, and instances of parallelism, a more faithful translation may be "he hovered over, guarding." Indeed, this is the image invoked by the verb in Isaiah 31:5: "As birds hovering, so will the Lord of hosts protect Jerusalem; He will deliver it as He protecteth it, He will rescue it as He "passeth over"" (כְּצִפֳּרִים עָפוֹת – כֵּן יָגֵן יְהוָה צְבָאוֹת, עַל-יְרוּשָׁלִָם; גָּנוֹן וְהִצִּיל, פָּסֹחַ וְהִמְלִיט.) () Both meanings become apparent in Exodus 12:23 when parsed as: the Lord will pass (hover, guard) over the door, and will not suffer the destroyer (destroying angel is commanded to pass by the children of Israel) to come in unto your houses to smite.

The term "Pesach" (Hebrew: ) may also refer to the lamb or goat which was designated as the Passover sacrifice (called the "Korban Pesach" in Hebrew). Four days before the Exodus, the Hebrews were commanded to set aside a lamb (), and inspect it daily for blemishes. During the day on the 14th of Nisan, they were to slaughter the animal and use its blood to mark their lintels and door posts. Up until midnight on the 15th of Nisan, they were to consume the lamb. On the night of the first Passover at the start of the original Exodus, each family (or group of families) gathered together to eat a meal that included the meat of the "Korban Pesach" while the Tenth Plague ravaged Egypt.

The main entity in Passover according to Judaism is the sacrificial lamb.

During the existence of the Tabernacle and later the Temple in Jerusalem, the focus of the Passover festival was the Passover sacrifice (Hebrew "korban Pesach"), also known as the Paschal lamb, eaten during the Passover Seder on the 15th of Nisan. Every family large enough to completely consume a young lamb or wild goat was required to offer one for sacrifice at the Jewish Temple on the afternoon of the 14th day of Nisan (), and eat it that night, which was the 15th of Nisan (). If the family was too small to finish eating the entire offering in one sitting, an offering was made for a group of families. The sacrifice could not be offered with anything leavened (), and had to be roasted, without its head, feet, or inner organs being removed () and eaten together with unleavened bread ("matzo") and bitter herbs ("maror"). One had to be careful not to break any bones from the offering (), and none of the meat could be left over by morning ( ).

Because of the Passover sacrifice's status as a sacred offering, the only people allowed to eat it were those who had the obligation to bring the offering. Among those who could not offer or eat the Passover lamb were an apostate (), a servant (), an uncircumcised man (), a person in a state of ritual impurity, except when a majority of Jews are in such a state ("Pesahim" 66b), and a non-Jew. The offering had to be made before a quorum of 30 ("Pesahim" 64b). In the Temple, the Levites sang Hallel while the priests performed the sacrificial service. Men and women were equally obligated regarding the offering ("Pesahim" 91b).

Women were obligated, as men, to perform the Korban Pesach and to participate in a Seder.

Today, in the absence of the Temple, when no sacrifices are offered or eaten, the mitzvah of the "Korban Pesach" is memorialized in the "Seder Korban Pesach", a set of scriptural and Rabbinic passages dealing with the Passover sacrifice, customarily recited after the "Mincha" (afternoon prayer) service on the 14th on Nisan, and in the form of the "zeroa", a symbolic food placed on the Passover Seder Plate (but not eaten), which is usually a roasted shankbone (or a chicken wing or neck). The eating of the afikoman substitutes for the eating of the "Korban Pesach" at the end of the Seder meal (Mishnah Pesachim 119a). Many Sephardi Jews have the custom of eating lamb or goat meat during the Seder in memory of the "Korban Pesach".

Chametz (חמץ, "leavening") is made from one of five types of grains combined with water and left to stand for more than eighteen minutes. The consumption, keeping, and owning of "chametz" is forbidden during Passover. Yeast and fermentation are not themselves forbidden as seen for example by wine, which is required, rather than merely permitted. According to Halakha, the ownership of such "chametz" is also proscribed.

"Chametz" does not include baking soda, baking powder or like products. Although these are defined in English as leavening agents, they leaven by chemical reaction, not by biological fermentation. Thus, bagels, waffles and pancakes made with baking soda and matzo meal are considered permissible, while bagels made with sourdough and pancakes and waffles made with yeast are prohibited.

The Torah commandments regarding "chametz" are:

Observant Jews spend the weeks before Passover in a flurry of thorough housecleaning, to remove every morsel of "chametz" from every part of the home. Jewish law requires the elimination of olive-sized or larger quantities of leavening from one's possession, but most housekeeping goes beyond this. Even the cracks of kitchen counters are thoroughly scrubbed, for example, to remove any traces of flour and yeast, however small. Any item or implement that has handled "chametz" is generally put away and not used during Passover.

Some hotels, resorts, and even cruise ships across America, Europe and Israel also undergo a thorough housecleaning to make their premises "kosher for Pesach" to cater to observant Jews.

Some scholars suggest that the command to abstain from leavened food or yeast suggests that sacrifices offered to God involve the offering of objects in "their least altered state", that would be nearest to the way in which they were initially made by God. According to other scholars the absence of leaven or yeast means that leaven or yeast symbolizes corruption and spoiling.

Additionally, there is a tradition of not eating matzoh (flat unleavened bread) in the 30 days before Passover begins so that there will be an increased appetite for it during Passover itself.

"Chametz" may be sold rather than discarded, especially in the case of relatively valuable forms such as liquor distilled from wheat, with the products being repurchased afterward. In some cases, they may never leave the house, instead being formally sold while remaining in the original owner's possession in a locked cabinet until they can be repurchased after the holiday. Modern observance may also include sealing cabinets and drawers which contain "Chametz" shut by using adhesive tape, which serves a similar purpose to a lock but also shows evidence of tampering. Although the practice of selling "Chametz" dates back many years, some Reform rabbinical authorities have come to regard it with disdain – since the supposed "new owner" never takes actual possession of the goods.

The sale of "chametz" may also be conducted communally via a rabbi, who becomes the "agent" for all the community's Jews through a halakhic procedure called a "kinyan" (acquisition). Each householder must put aside all the "chametz" he is selling into a box or cupboard, and the rabbi enters into a contract to sell all the "chametz" to a non-Jew (who is not obligated to celebrate the commandments) in exchange for a small down payment ("e.g." $1.00), with the remainder due after Passover. This sale is considered completely binding according to Halakha, and at any time during the holiday, the buyer may come to take or partake of his property. The rabbi then re-purchases the goods for less than they were sold at the end of the holiday.

On the night of the fourteenth of Nisan, the night before the Passover Seder (after nightfall on the evening before Passover eve), Jews do a formal search in their homes known as "bedikat chametz" for any possible remaining leaven ("chametz"). The Talmudic sages instructed that a search for "chametz" be made in every home, place of work, or any place where "chametz" may have been brought during the year. When the first Seder is on a Saturday night, the search is conducted on the preceding Thursday night (thirteenth of Nisan) as "chametz" cannot be burned during Shabbat.

The Talmud in Pesahim (p. 2a) derives from the Torah that the search for "chametz" be conducted by the light of a candle and therefore is done at night, and although the final destruction of the "chametz" (usually by burning it in a small bonfire) is done on the next morning, the blessing is made at night because the search is both in preparation for and part of the commandments to remove and destroy all "chametz" from one's possession.

Before the search is begun there is a special blessing. If several people or family members assist in the search then only one person, usually the head of that family recites the blessing having in mind to include everyone present:

In Hebrew:

ברוך אתה יהוה אלהינו מלך העולם אשר קדשנו במצותיו וצונו על בעור חמץ

The search is then usually conducted by the head of the household joined by his family including children under the supervision of their parents.

It is customary to turn off the lights and conduct the search by candlelight, using a feather and a wooden spoon: candlelight effectively illuminates corners without casting shadows; the feather can dust crumbs out of their hiding places; and the wooden spoon which collects the crumbs can be burned the next day with the "chametz". However, most contemporary Jewish-Orthodox authorities permit using a flashlight, while some strongly encourage it due to the danger coupled with using a candle.

Because the house is assumed to have been thoroughly cleaned by the night before Passover, there is some concern that making a blessing over the search for "chametz" will be in vain ("bracha l'vatala") if nothing is found. Thus, 10 morsels of bread or cereal smaller than the size of an olive are traditionally hidden throughout the house in order to ensure that some "chametz" will be found.

Upon conclusion of the search, with all the small pieces safely wrapped up and put in one bag or place, to be burned the next morning, the following is said:

Original declaration as recited in Aramaic:

כל חמירא וחמיעא דאכא ברשותי דלא חמתה ודלא בערתה ודלא ידענא לה לבטל ולהוי הפקר כעפרא דארעא

Note that if the 14th of Nissan is Shabbat, many of the below will be celebrated on the 13th instead due to restrictions in place during Shabbat.

On the day preceding the first Passover seder (or on Thursday morning preceding the seder, when the first seder falls on Motza'ei Shabbat), firstborn sons are commanded to celebrate the Fast of the Firstborn which commemorates the salvation of the Hebrew firstborns. According to , God struck down all Egyptian firstborns while the Israelites were not affected. However, it is customary for synagogues to conduct a "siyum" (ceremony marking the completion of a section of Torah learning) right after morning prayers, and the celebratory meal that follows cancels the firstborn's obligation to fast.

On the morning of the 14th of Nisan, any leavened products that remain in the householder's possession, along with the 10 morsels of bread from the previous night's search, are burned ("s'rayfat chametz"). The head of the household repeats the declaration of "biyur chametz", declaring any "chametz" that may not have been found to be null and void "as the dust of the earth":

Original declaration as recited in Aramaic:

כל חמירא וחמיעא דאכא ברשותי דלא חמתה ודלא בערתה ודלא ידענא לה לבטל ולהוי הפקר כעפרא דארעא

Should more "chametz" actually be found in the house during the Passover holiday, it must be burnt as soon as possible.

Unlike "chametz", which can be eaten any day of the year except during Passover, kosher for Passover foods can be eaten year-round. They need not be burnt or otherwise discarded after the holiday ends.

The historic "Paschal lamb" Passover sacrifice ("Korban Pesach") has not been brought following the Romans' destruction of the Second Jewish temple approximately two thousand years ago, and it is therefore still not part of the modern Jewish holiday.

However, the Paschal lamb is still a principal feature of Falashah, Karaite and Samaritan observance.

In the times when the Jewish Temples stood, the lamb was slaughtered and cooked on the evening of Passover and was completely consumed before the morning as described in .

Due to the Torah injunction not to eat "chametz" during Passover (), observant families typically own complete sets of serving dishes, glassware and silverware (and in some cases, even separate dishwashers and sinks) which have never come into contact with "chametz", for use only during Passover. Under certain circumstances, some "chametz" utensils can be immersed in boiling water ("hagalat keilim") to purge them of any traces of "chametz" that may have accumulated during the year. Many Sephardic families thoroughly wash their year-round glassware and then use it for Passover, as the Sephardic position is that glass does not absorb enough traces of food to present a problem. Similarly, ovens may be used for Passover either by setting the self-cleaning function to the highest degree for a certain period of time, or by applying a blow torch to the interior until the oven glows red hot (a process called "libun gamur").

A symbol of the Passover holiday is matzo, an unleavened flatbread made solely from flour and water which is continually worked from mixing through baking, so that it is not allowed to rise. Matzo may be made by machine or by hand. The Torah contains an instruction to eat matzo, specifically, on the first night of Passover and to eat only unleavened bread (in practice, matzo) during the entire week of Passover. Consequently, the eating of matzo figures prominently in the Passover Seder. There are several explanations for this.

The Torah says that it is because the Hebrews left Egypt with such haste that there was no time to allow baked bread to rise; thus flat, unleavened bread, matzo, is a reminder of the rapid departure of the Exodus. Other scholars teach that in the time of the Exodus, matzo was commonly baked for the purpose of traveling because it preserved well and was light to carry (making it similar to hardtack), suggesting that matzo was baked intentionally for the long journey ahead.

Matzo has also been called "Lechem Oni" (Hebrew: "bread of poverty"). There is an attendant explanation that matzo serves as a symbol to remind Jews what it is like to be a poor slave and to promote humility, appreciate freedom, and avoid the inflated ego symbolized by more luxurious leavened bread.

"Shmura matzo" ("watched" or "guarded" matzo), is the bread of preference for the Passover Seder in Orthodox Jewish communities. Shmura matzo is made from wheat that is guarded from contamination by chametz from the time of summer harvest to its baking into matzos five to ten months later.

In the weeks before Passover, matzos are prepared for holiday consumption. In many Orthodox Jewish communities, men traditionally gather in groups (""chaburas"") to bake handmade matzo for use at the Seder, the dough being rolled by hand, resulting in a large and round matzo. "Chaburas" also work together in machine-made matzo factories, which produce the typically square-shaped matzo sold in stores.

The baking of matzo is labor-intensive, as only 18–22 minutes is permitted between the mixing of flour and water to the conclusion of baking and removal from the oven. Consequently, only a small number of matzos can be baked at one time, and the "chabura" members are enjoined to work the dough constantly so that it is not allowed to ferment and rise. A special cutting tool is run over the dough just before baking to prick any bubbles which might make the matza puff up; this creates the familiar dotted holes in the matzo.

After the matzos come out of the oven, the entire work area is scrubbed down and swept to make sure that no pieces of old, potentially leavened dough remain, as any stray pieces are now "chametz", and can contaminate the next batch of matzo.

Some machine-made matzos are completed within 5 minutes of being kneaded.

It is traditional for Jewish families to gather on the first night of Passover (first two nights in Orthodox and Conservative communities outside Israel) for a special dinner called a seder (סדר – derived from the Hebrew word for "order" or "arrangement", referring to the very specific order of the ritual). The table is set with the finest china and silverware to reflect the importance of the meal. During this meal, the story of the Exodus from Egypt is retold using a special text called the Haggadah. Four cups of wine are consumed at various stages in the narrative. The Haggadah divides the night's procedure into 15 parts:


These 15 parts parallel the 15 steps in the Temple in Jerusalem on which the Levites stood during Temple services, and which were memorialized in the 15 Psalms (#120–134) known as "Shir HaMa'alot" (Hebrew: , "Songs of Ascent").

The seder is replete with questions, answers, and unusual practices (e.g. the recital of Kiddush which is not immediately followed by the blessing over bread, which is the traditional procedure for all other holiday meals) to arouse the interest and curiosity of the children at the table. The children are also rewarded with nuts and candies when they ask questions and participate in the discussion of the Exodus and its aftermath. Likewise, they are encouraged to search for the "afikoman", the piece of matzo which is the last thing eaten at the seder. Audience participation and interaction is the rule, and many families' seders last long into the night with animated discussions and much singing. The seder concludes with additional songs of praise and faith printed in the Haggadah, including "Chad Gadya" ("One Little Kid" or "One Little Goat").

Maror (bitter herbs) symbolizes the bitterness of slavery in Egypt. The following verse from the Torah underscores that symbolism: "And they embittered ("ve-yimareru" וימררו) their lives with hard labor, with mortar and with bricks and with all manner of labor in the field; any labor that they made them do was with hard labor" (Exodus 1:14).

There is a Rabbinic requirement that four cups of wine are to be drunk during the seder meal. This applies to both men and women. The Mishnah says (Pes. 10:1) that even the poorest man in Israel has an obligation to drink. Each cup is connected to a different part of the seder: the first cup is for Kiddush, the second cup is connected with the recounting of the Exodus, the drinking of the third cup concludes Birkat Hamazon and the fourth cup is associated with Hallel.

Children have a very important role in the Passover seder. Traditionally the youngest child is prompted to ask questions about the Passover seder, beginning with the words, "Mah Nishtana HaLeila HaZeh" (Why is this night different from all other nights?). The questions encourage the gathering to discuss the significance of the symbols in the meal. The questions asked by the child are:

Often the leader of the seder and the other adults at the meal will use prompted responses from the Haggadah, which states, "The more one talks about the Exodus from Egypt, the more praiseworthy he is." Many readings, prayers, and stories are used to recount the story of the Exodus. Many households add their own commentary and interpretation and often the story of the Jews is related to the theme of liberation and its implications worldwide.

The "afikoman" – an integral part of the Seder itself – is used to engage the interest and excitement of the children at the table. During the fourth part of the Seder, called "Yachatz", the leader breaks the middle piece of matzo into two. He sets aside the larger portion as the "afikoman". Many families use the "afikoman" as a device for keeping the children awake and alert throughout the Seder proceedings by hiding the "afikoman" and offering a prize for its return. Alternatively, the children are allowed to "steal" the "afikoman" and demand a reward for its return. In either case, the "afikoman" must be consumed during the twelfth part of the Seder, "Tzafun".

After the Hallel, the fourth glass of wine is drunk, and participants recite a prayer that ends in "Next year in Jerusalem!". This is followed by several lyric prayers that expound upon God's mercy and kindness, and give thanks for the survival of the Jewish people through a history of exile and hardship. "Echad Mi Yodea" ("Who Knows One?") is a playful song, testing the general knowledge of the children (and the adults). Some of these songs, such as "Chad Gadya" are allegorical.

Beginning on the second night of Passover, the 16th day of Nisan, Jews begin the practice of the Counting of the Omer, a nightly reminder of the approach of the holiday of Shavuot 50 days hence. Each night after the evening prayer service, men and women recite a special blessing and then enumerate the day of the Omer. On the first night, for example, they say, "Today is the first day in (or, to) the Omer"; on the second night, "Today is the second day in the Omer." The counting also involves weeks; thus, the seventh day is commemorated, "Today is the seventh day, which is one week in the Omer." The eighth day is marked, "Today is the eighth day, which is one week and one day in the Omer," etc.

When the Temple stood in Jerusalem, a sheaf of new-cut barley was presented before the altar on the second day of Unleavened Bread. Josephus writes:
On the second day of unleavened bread, that is to say the sixteenth, our people partake of the crops which they have reaped and which have not been touched till then, and esteeming it right first to do homage to God, to whom they owe the abundance of these gifts, they offer to him the first-fruits of the barley in the following way. After parching and crushing the little sheaf of ears and purifying the barley for grinding, they bring to the altar an "assaron" for God, and, having flung a handful thereof on the altar, they leave the rest for the use of the priests. Thereafter all are permitted, publicly or individually, to begin harvest. Since the destruction of the Temple, this offering is brought in word rather than deed.

One explanation for the Counting of the Omer is that it shows the connection between Passover and Shavuot. The physical freedom that the Hebrews achieved at the Exodus from Egypt was only the beginning of a process that climaxed with the spiritual freedom they gained at the giving of the Torah at Mount Sinai. Another explanation is that the newborn nation which emerged after the Exodus needed time to learn their new responsibilities vis-a-vis Torah and mitzvot before accepting God's law. The distinction between the Omer offering – a measure of barley, typically animal fodder – and the Shavuot offering – two loaves of wheat bread, human food – symbolizes the transition process.

In Israel, Passover lasts for seven days with the first and last days being major Jewish holidays. In Orthodox and Conservative communities, no work is performed on those days, with most of the rules relating to the observances of Shabbat being applied.

Outside Israel, in Orthodox and Conservative communities, the holiday lasts for eight days with the first two days and last two days being major holidays. In the intermediate days necessary work can be performed. Reform Judaism observes Passover over seven days, with the first and last days being major holidays.

Like the holiday of Sukkot, the intermediary days of Passover are known as Chol HaMoed (festival weekdays) and are imbued with a semi-festive status. It is a time for family outings and picnic lunches of matzo, hardboiled eggs, fruits and vegetables, and Passover treats such as macaroons and homemade candies.

Passover cake recipes call for potato starch or Passover cake flour made from finely granulated matzo instead of regular flour, and a large amount of eggs to achieve fluffiness. Cookie recipes use matzo farfel (broken bits of matzo) or ground nuts as the base. For families with Eastern European backgrounds, borsht, a soup made with beets, is a Passover tradition.

While kosher for Passover packaged goods are available in stores, some families opt to cook everything from scratch during Passover week. In Israel, families that do not kasher their ovens can bake cakes, casseroles, and even meat on the stovetop in a Wonder Pot, an Israeli invention consisting of three parts: an aluminium pot shaped like a Bundt pan, a hooded cover perforated with venting holes, and a thick, round, metal disc with a center hole which is placed between the Wonder Pot and the flame to disperse heat.

"Shvi'i shel Pesach" (שביעי של פסח) ("seventh [day] of Passover") is another full Jewish holiday, with special prayer services and festive meals. Outside the Land of Israel, in the Jewish diaspora, "Shvi'i shel Pesach" is celebrated on both the seventh and eighth days of Passover. This holiday commemorates the day the Children of Israel reached the Red Sea and witnessed both the miraculous "Splitting of the Sea" (Passage of the Red Sea), the drowning of all the Egyptian chariots, horses and soldiers that pursued them. According to the Midrash, only the Pharaoh was spared to give testimony to the miracle that occurred.

Hasidic Rebbes traditionally hold a "tish" on the night of "Shvi'i shel Pesach" and place a cup or bowl of water on the table before them. They use this opportunity to speak about the Splitting of the Sea to their disciples, and sing songs of praise to God.

The "Second Passover" (Pesach Sheni) on the 14th of Iyar in the Hebrew Calendar is mentioned in the Hebrew Bible (Numbers 9:6–13) as a make-up day for people who were unable to offer the pesach sacrifice at the appropriate time due to ritual impurity or distance from Jerusalem. Just as on the first Pesach night, breaking bones from the second Paschal offering (Numbers 9:12) or leaving meat over until morning (Numbers 9:12) is prohibited.

Today, Pesach Sheni on the 14th of Iyar has the status of a very minor holiday (so much so that many of the Jewish people have never even heard of it, and it essentially does not exist outside of Orthodox and traditional Conservative Judaism). There are not really any special prayers or observances that are considered Jewish law. The only change in the liturgy is that in some communities "Tachanun", a penitential prayer omitted on holidays, is not said. There is a custom, though not Jewish law, to eat just one piece of matzo on that night.

Because the house is free of chametz for eight days, the Jewish household typically eats different foods during the week of Passover. Some include:

Ashkenazi foods

Sephardi foods

The story of Passover, with its message that slaves can go free, and that the future can be better than the present, has inspired a number of religious sermons, prayers, and songs – including spirituals (what used to be called "Negro Spirituals"), within the African-American community.

Rabbi Philip R. Alstat, an early leader of Conservative Judaism, known for his fiery rhetoric and powerful oratory skills, wrote and spoke in 1939 about the power of the Passover story during the rise of Nazi persecution and terror:

Perhaps in our generation the counsel of our Talmudic sages may seem superfluous, for today the story of our enslavement in Egypt is kept alive not only by ritualistic symbolism, but even more so by tragic realism. We are the contemporaries and witnesses of its daily re-enactment. Are not our hapless brethren in the German Reich eating "the bread of affliction"? Are not their lives embittered by complete disenfranchisement and forced labor? Are they not lashed mercilessly by brutal taskmasters behind the walls of concentration camps? Are not many of their men-folk being murdered in cold blood? Is not the ruthlessness of the Egyptian Pharaoh surpassed by the sadism of the Nazi dictators?And yet, even in this hour of disaster and degradation, it is still helpful to "visualize oneself among those who had gone forth out of Egypt." It gives stability and equilibrium to the spirit. Only our estranged kinsmen, the assimilated, and the de-Judaized, go to pieces under the impact of the blow...But those who visualize themselves among the groups who have gone forth from the successive Egypts in our history never lose their sense of perspective, nor are they overwhelmed by confusion and despair... It is this faith, born of racial experience and wisdom, which gives the oppressed the strength to outlive the oppressors and to endure until the day of ultimate triumph when we shall "be brought forth from bondage unto freedom, from sorrow unto joy, from mourning unto festivity, from darkness unto great light, and from servitude unto redemption.

The Christian celebration of Good Friday finds its roots in the Jewish feast of Passover, the evening on which Jesus was crucified as the Passover Lamb.

In the Sunni sect of Islam, it is recommended to fast on the day of Ashura (10th of Muharram) based on narrations attributed to Muhammad. The fast is celebrated in order to commemorate the day when Moses and his followers were saved from Pharaoh by God by creating a path in the Red Sea (i.e. The Exodus). According to Muslim tradition, the Jews of Madinah used to fast on the tenth of Muharram in observance of Passover. In narrations recorded in the al-Hadith (sayings of the Islamic Prophet Muhammad) of Sahih al-Bukhari, it is recommended that Muslims fast on this day. It is also stipulated that its observance should differ from the feast of Passover which is celebrated by the Jews, and he stated that Muslims should fast for two days instead of one, either on the 9th and 10th day or on the 10th and 11th day of Muharram.




</doc>
<doc id="23062" url="https://en.wikipedia.org/wiki?curid=23062" title="Post Office Protocol">
Post Office Protocol

In computing, the Post Office Protocol (POP) is an application-layer Internet standard protocol used by e-mail clients to retrieve e-mail from a server in an Internet Protocol (IP) network. POP version 3 (POP3) is the most recent level of development in common use. POP has largely been superseded by the Internet Message Access Protocol (IMAP).

POP supports download-and-delete requirements for access to remote mailboxes (termed maildrop in the POP RFC's). Although most POP clients have an option to leave mail on server after download, e-mail clients using POP generally connect, retrieve all messages, store them on the client system, and delete them from the server. Other protocols, notably the Internet Message Access Protocol (IMAP) provide more features of message management to typical mailbox operations. A POP3 server listens on well-known port number 110 for service requests. Encrypted communication for POP3 is either requested after protocol initiation, using the STLS command, if supported, or by POP3S, which connects to the server using Transport Layer Security (TLS) or Secure Sockets Layer (SSL) on well-known TCP port number 995.

Available messages to the client are fixed when a POP session opens the maildrop, and are identified by message-number local to that session or, optionally, by a unique identifier assigned to the message by the POP server. This unique identifier is permanent and unique to the maildrop and allows a client to access the same message in different POP sessions. Mail is retrieved and marked for deletion by message-number. When the client exits the session, the mail marked for deletion is removed from the maildrop.

POP1 was specified in RFC 918 (1984), POP2 by RFC 937 (1985).

POP3 originated with RFC 1081 (1988). The most recent specification is RFC 1939, updated with an extension mechanism (RFC 2449) and an authentication mechanism in RFC 1734.

The original POP3 specification supported only an unencrypted USER/PASS login mechanism or Berkeley .rhosts access control. POP3 currently supports several authentication methods to provide varying levels of protection against illegitimate access to a user's e-mail. Most are provided by the POP3 extension mechanisms. POP3 clients support SASL authentication methods via the AUTH extension. MIT Project Athena also produced a Kerberized version. RFC 1460 introduced APOP into the core protocol. APOP is a challenge/response protocol which uses the MD5 hash function in an attempt to avoid replay attacks and disclosure of the shared secret. Clients implementing APOP include Mozilla Thunderbird, Opera Mail, Eudora, KMail, Novell Evolution, RimArts' Becky!, Windows Live Mail, PowerMail, Apple Mail, and Mutt. RFC 1460 was obsoleted by RFC 1725, which was in turn obsoleted by RFC 1939.

POP4 exists only as an informal proposal adding basic folder management, multipart message support, as well as message flag management to compete with IMAP; but its development has not progressed since 2003.

An extension mechanism was proposed in RFC 2449 to accommodate general extensions as well as announce in an organized manner support for optional commands, such as TOP and UIDL. The RFC did not intend to encourage extensions, and reaffirmed that the role of POP3 is to provide simple support for mainly download-and-delete requirements of mailbox handling.

The extensions are termed capabilities and are listed by the CAPA command. Except for APOP, the optional commands were included in the initial set of capabilities. Following the lead of ESMTP (RFC 5321), capabilities beginning with an X signify local capabilities.

The STARTTLS extension allows the use of Transport Layer Security (TLS) or Secure Sockets Layer (SSL) to be negotiated using the "STLS" command, on the standard POP3 port, rather than an alternate. Some clients and servers instead use the alternate-port method, which uses TCP port 995 (POP3S).

Demon Internet introduced extensions to POP3 that allow multiple accounts per domain, and has become known as "Standard Dial-up POP3 Service" (SDPS). To access each account, the username includes the hostname, as "john@hostname" or "john+hostname".

Google Apps uses the same method.

In computing, local e-mail clients can use the Kerberized Post Office Protocol (KPOP), an application-layer Internet standard protocol, to retrieve e-mail from a remote server over a TCP/IP connection. The KPOP protocol is based on the POP3 protocol with the differences that it adds Kerberos security and that it runs by default over TCP port number 1109 instead of 110. One mail server software implementation is found in the Cyrus IMAP server.


The APOP usage is a direct example from RFC 1939 page 18.

RFC 1939 APOP support indicated by <1896.697170952@dbc.mtview.ca.us> here:

POP3 servers without the optional APOP command expect the client to log in with the USER and PASS commands:





</doc>
<doc id="23069" url="https://en.wikipedia.org/wiki?curid=23069" title="Punch (magazine)">
Punch (magazine)

Punch; or, The London Charivari was a British weekly magazine of humour and satire established in 1841 by Henry Mayhew and engraver Ebenezer Landells. Historically, it was most influential in the 1840s and 1850s, when it helped to coin the term "cartoon" in its modern sense as a humorous illustration. 
After the 1940s, when its circulation peaked, it went into a long decline, closing in 1992. It was revived in 1996, but closed again in 2002.

"Punch" was founded on 17 July 1841 by Henry Mayhew and engraver Ebenezer Landells, on an initial investment of £25. It was jointly edited by Mayhew and Mark Lemon. It was subtitled "The London Charivari" in homage to Charles Philipon's French satirical humour magazine "Le Charivari". Reflecting their satiric and humorous intent, the two editors took for their name and masthead the anarchic glove puppet, Mr. Punch, of Punch and Judy; the name also referred to a joke made early on about one of the magazine's first editors, Lemon, that "punch is nothing without lemon". Mayhew ceased to be joint editor in 1842 and became "suggestor in chief" until he severed his connection in 1845. The magazine initially struggled for readers, except for an 1842 "Almanack" issue which shocked its creators by selling 90,000 copies. In December 1842 due to financial difficulties the magazine was sold to Bradbury and Evans, both printers and publishers. Bradbury and Evans capitalised on newly evolving mass printing technologies and also were the publishers for Charles Dickens and William Makepeace Thackeray.

The term "cartoon" to refer to comic drawings was first used in "Punch" in 1843, when the Houses of Parliament were to be decorated with murals, and "cartoons" for the mural were displayed for the public; the term "cartoon" then meant a finished preliminary sketch on a large piece of cardboard, or in Italian. "Punch" humorously appropriated the term to refer to its political cartoons, and the popularity of the "Punch" cartoons led to the term's widespread use.

The illustrator Archibald Henning designed the cover of the magazine's first issues. The cover design varied in the early years, though Richard Doyle designed what became the magazine's masthead in 1849. Artists who published in "Punch" during the 1840s and 50s included John Leech, Richard Doyle, John Tenniel and Charles Keene. This group became known as "The "Punch" Brotherhood", which also included Charles Dickens who joined Bradbury and Evans after leaving Chapman and Hall in 1843. "Punch" authors and artists also contributed to another Bradbury and Evans literary magazine called "Once A Week" (est.1859), created in response to Dickens' departure from "Household Words".

In the 1860s and 1870s, conservative "Punch" faced competition from upstart liberal journal "Fun", but after about 1874, "Fun"&apos;s fortunes faded. At Evans's café in London, the two journals had "Round tables" in competition with each other.
After months of financial difficulty and lack of market success, "Punch" became a staple for British drawing rooms because of its sophisticated humour and absence of offensive material, especially when viewed against the satirical press of the time. "The Times" and the Sunday paper "News of the World" used small pieces from "Punch" as column fillers, giving the magazine free publicity and indirectly granting a degree of respectability, a privilege not enjoyed by any other comic publication. "Punch" would share a friendly relationship with not only "The Times" but journals aimed at intellectual audiences such as the "Westminster Review", which published a fifty-three page illustrated article on "Punch's" first two volumes. Historian Richard Altick writes that "To judge from the number of references to it in the private letters and memoirs of the 1840s..."Punch" had become a household word within a year or two of its founding, beginning in the middle class and soon reaching the pinnacle of society, royalty itself".

Increasing in readership and popularity throughout the remainder of the 1840s and 1850s, "Punch" was the success story of a threepenny weekly paper that had become one of the most talked-about and enjoyed periodicals. "Punch" enjoyed an audience including Elizabeth Barrett, Robert Browning, Thomas Carlyle, Edward FitzGerald, Charlotte Brontë, Queen Victoria, Prince Albert, Ralph Waldo Emerson, Emily Dickinson, Herman Melville, Henry Wadsworth Longfellow, and James Russell Lowell. "Punch" gave several phrases to the English language, including The Crystal Palace, and the "Curate's egg" (first seen in an 1895 cartoon). Several British humour classics were first serialised in "Punch", such as the "Diary of a Nobody" and "1066 and All That". Towards the end of the nineteenth century, the artistic roster included Harry Furniss, Linley Sambourne, Francis Carruthers Gould, and Phil May. Among the outstanding cartoonists of the following century were Bernard Partridge, H. M. Bateman, Bernard Hollowood who also edited the magazine from 1957 to 1968, Kenneth Mahood and Norman Thelwell.

Circulation broke the 100,000 mark around 1910, and peaked in 1947–1948 at 175,000 to 184,000. Sales declined steadily thereafter; ultimately, the magazine was forced to close in 2002 after 161 years of publication.

"Punch" was widely emulated worldwide and was popular in the colonies. The colonial experience, especially in India, influenced Punch and its iconography. Tenniel's "Punch" cartoons of the 1857 Sepoy Mutiny led to a surge in the magazine's popularity. Colonial India was frequently caricatured in "Punch" and was an important source of knowledge of India for British readers.

"Punch" material was collected in book formats from the late nineteenth century, which included "Pick of the Punch" annuals with cartoons and text features, "Punch and the War" (a 1941 collection of WWII-related cartoons), and "A Big Bowl of Punch" – which was republished a number of times. Many Punch cartoonists of the late 20th century published collections of their own, partly based on "Punch" contributions.

"Punch" magazine ceased publishing in 1992.

In early 1996, the Egyptian businessman Mohamed Al-Fayed bought the rights to the name, and "Punch" was re-launched later that year. It was reported that the magazine was intended to be a spoiler aimed at "Private Eye", which had published many items critical of Fayed. The magazine never became profitable in its new incarnation, and at the end of May 2002 it was announced that "Punch" would once more cease publication. Press reports quoted a loss of £16 million over the six years of publication, with only 6,000 subscribers at the end.

Whereas the earlier version of "Punch" prominently featured the clownish character Punchinello (Punch of Punch and Judy) performing antics on front covers, the resurrected "Punch" magazine did not use this character, but featured on its weekly covers a photograph of a boxing glove, thus informing its readers that the new magazine intended its name to mean "punch" in the sense of a punch in the eye.

In 2004, much of the archive was acquired by the British Library, including the famous "Punch" table. The long oval Victorian table was used for staff meetings and other occasions, and was brought into the offices sometime around 1855. The wooden surface is scarred with the carved initials of the magazine's longtime writers, artists and editors, as well as six invited "strangers" including James Thurber and Prince Charles. Mark Twain declined the invitation, saying that the already-carved initials of William Makepeace Thackeray included his own.



"Punch" was influential in British colonies around the world, and in countries including Turkey, India, Japan, and China, with "Punch" imitators appearing in Cairo, Yokohama, Tokyo, Hong Kong, and Shanghai.





</doc>
<doc id="23070" url="https://en.wikipedia.org/wiki?curid=23070" title="Pacific Ocean">
Pacific Ocean

The Pacific Ocean is the largest and deepest of Earth's oceanic divisions. It extends from the Arctic Ocean in the north to the Southern Ocean (or, depending on definition, to Antarctica) in the south and is bounded by Asia and Australia in the west and the Americas in the east.

At in area (as defined with an Antarctic southern border), this largest division of the World Ocean—and, in turn, the hydrosphere—covers about 46% of Earth's water surface and about one-third of its total surface area, making it larger than all of Earth's land area combined. Both the center of the Water Hemisphere and the Western Hemisphere are in the Pacific Ocean. The equator subdivides it into the North Pacific Ocean and South Pacific Ocean, with two exceptions: the Galápagos and Gilbert Islands, while straddling the equator, are deemed wholly within the South Pacific. Its mean depth is . The Mariana Trench in the western North Pacific is the deepest point in the world, reaching a depth of . The western Pacific has many peripheral seas.

Though the peoples of Asia and Oceania have traveled the Pacific Ocean since prehistoric times, the eastern Pacific was first sighted by Europeans in the early 16th century when Spanish explorer Vasco Núñez de Balboa crossed the Isthmus of Panama in 1513 and discovered the great "southern sea" which he named "Mar del Sur" (in Spanish). The ocean's current name was coined by Portuguese explorer Ferdinand Magellan during the Spanish circumnavigation of the world in 1521, as he encountered favorable winds on reaching the ocean. He called it "Mar Pacífico", which in both Portuguese and Spanish means "peaceful sea".

Important human migrations occurred in the Pacific in prehistoric times. About 3000 BC, the Austronesian peoples on the island of Taiwan mastered the art of long-distance canoe travel and spread themselves and their languages south to the Philippines, Indonesia, and maritime Southeast Asia; west towards Madagascar; southeast towards New Guinea and Melanesia (intermarrying with native Papuans); and east to the islands of Micronesia, Oceania and Polynesia.

Long-distance trade developed all along the coast from Mozambique to Japan. Trade, and therefore knowledge, extended to the Indonesian islands but apparently not Australia. By at least 878 when there was a significant Islamic settlement in Canton much of this trade was controlled by Arabs or Muslims. In 219 BC Xu Fu sailed out into the Pacific searching for the elixir of immortality. From 1404 to 1433 Zheng He led expeditions into the Indian Ocean.

The first contact of European navigators with the western edge of the Pacific Ocean was made by the Portuguese expeditions of António de Abreu and Francisco Serrão, via the Lesser Sunda Islands, to the Maluku Islands, in 1512, and with Jorge Álvares's expedition to southern China in 1513, both ordered by Afonso de Albuquerque from Malacca.

The east side of the ocean was discovered by Spanish explorer Vasco Núñez de Balboa in 1513 after his expedition crossed the Isthmus of Panama and reached a new ocean. He named it "Mar del Sur" (literally, "Sea of the South" or "South Sea") because the ocean was to the south of the coast of the isthmus where he first observed the Pacific.

Later, Portuguese explorer Ferdinand Magellan sailed the Pacific East to West on a Castilian ("Spanish") expedition of world circumnavigation starting in 1519. Magellan called the ocean "Pacífico" (or "Pacific" meaning, "peaceful") because, after sailing through the stormy seas off Cape Horn, the expedition found calm waters. The ocean was often called the "Sea of Magellan" in his honor until the eighteenth century. Although Magellan himself died in the Philippines in 1521, Spanish Basque navigator Juan Sebastián Elcano led the expedition back to Spain across the Indian Ocean and round the Cape of Good Hope, completing the first world circumnavigation in a single expedition in 1522. Sailing around and east of the Moluccas, between 1525 and 1527, Portuguese expeditions discovered the Caroline Islands, the Aru Islands, and Papua New Guinea. In 1542–43 the Portuguese also reached Japan.

In 1564, five Spanish ships consisting of 379 explorers crossed the ocean from Mexico led by Miguel López de Legazpi and sailed to the Philippines and Mariana Islands. For the remainder of the 16th century, Spanish influence was paramount, with ships sailing from Mexico and Peru across the Pacific Ocean to the Philippines via Guam, and establishing the Spanish East Indies. The Manila galleons operated for two and a half centuries linking Manila and Acapulco, in one of the longest trade routes in history. Spanish expeditions also discovered Tuvalu, the Marquesas, the Cook Islands, the Solomon Islands, and the Admiralty Islands in the South Pacific.

Later, in the quest for Terra Australis (i.e., "the [great] Southern Land"), Spanish explorations in the 17th century, such as the expedition led by the Portuguese navigator Pedro Fernandes de Queirós, discovered the Pitcairn and Vanuatu archipelagos, and sailed the Torres Strait between Australia and New Guinea, named after navigator Luís Vaz de Torres. Dutch explorers, sailing around southern Africa, also engaged in discovery and trade; Willem Janszoon, made the first completely documented European landing in Australia (1606), in Cape York Peninsula, and Abel Janszoon Tasman circumnavigated and landed on parts of the Australian continental coast and discovered Tasmania and New Zealand in 1642.

In the 16th and 17th century Spain considered the Pacific Ocean a "Mare clausum"—a sea closed to other naval powers. As the only known entrance from the Atlantic the Strait of Magellan was at times patrolled by fleets sent to prevent entrance of non-Spanish ships. On the western end of the Pacific Ocean the Dutch threatened the Spanish Philippines.

The 18th century marked the beginning of major exploration by the Russians in Alaska and the Aleutian Islands, such as the First Kamchatka expedition and the Great Northern Expedition, led by the Danish Russian navy officer Vitus Bering. Spain also sent expeditions to the Pacific Northwest reaching Vancouver Island in southern Canada, and Alaska. The French explored and settled Polynesia, and the British made three voyages with James Cook to the South Pacific and Australia, Hawaii, and the North American Pacific Northwest. In 1768, Pierre-Antoine Véron, a young astronomer accompanying Louis Antoine de Bougainville on his voyage of exploration, established the width of the Pacific with precision for the first time in history. One of the earliest voyages of scientific exploration was organized by Spain in the Malaspina Expedition of 1789–1794. It sailed vast areas of the Pacific, from Cape Horn to Alaska, Guam and the Philippines, New Zealand, Australia, and the South Pacific.

Growing imperialism during the 19th century resulted in the occupation of much of Oceania by other European powers, and later, Japan and the United States. Significant contributions to oceanographic knowledge were made by the voyages of HMS "Beagle" in the 1830s, with Charles Darwin aboard; HMS "Challenger" during the 1870s; the USS "Tuscarora" (1873–76); and the German "Gazelle" (1874–76).

In Oceania, France got a leading position as imperial power after making Tahiti and New Caledonia protectorates in 1842 and 1853 respectively. After navy visits to Easter Island in 1875 and 1887, Chilean navy officer Policarpo Toro managed to negotiate an incorporation of the island into Chile with native Rapanui in 1888. By occupying Easter Island, Chile joined the imperial nations. By 1900 nearly all Pacific islands were in control of Britain, France, United States, Germany, Japan, and Chile.

Although the United States gained control of Guam and the Philippines from Spain in 1898, Japan controlled most of the western Pacific by 1914 and occupied many other islands during World War II. However, by the end of that war, Japan was defeated and the U.S. Pacific Fleet was the virtual master of the ocean. Since the end of World War II, many former colonies in the Pacific have become independent states.

The Pacific separates Asia and Australia from the Americas. It may be further subdivided by the equator into northern (North Pacific) and southern (South Pacific) portions. It extends from the Antarctic region in the South to the Arctic in the north. The Pacific Ocean encompasses approximately one-third of the Earth's surface, having an area of —significantly larger than Earth's entire landmass of some .

Extending approximately from the Bering Sea in the Arctic to the northern extent of the circumpolar Southern Ocean at 60°S (older definitions extend it to Antarctica's Ross Sea), the Pacific reaches its greatest east-west width at about 5°N latitude, where it stretches approximately from Indonesia to the coast of Colombia—halfway around the world, and more than five times the diameter of the Moon. The lowest known point on Earth—the Mariana Trench—lies below sea level. Its average depth is , putting the total water volume at roughly .

Due to the effects of plate tectonics, the Pacific Ocean is currently shrinking by roughly per year on three sides, roughly averaging a year. By contrast, the Atlantic Ocean is increasing in size.

Along the Pacific Ocean's irregular western margins lie many seas, the largest of which are the Celebes Sea, Coral Sea, East China Sea (East Sea), Philippine Sea, Sea of Japan (East Sea), South China Sea (South Sea), Sulu Sea, Tasman Sea, and Yellow Sea (West Sea of Korea). The Indonesian Seaway (including the Strait of Malacca and Torres Strait) joins the Pacific and the Indian Ocean to the west, and Drake Passage and the Strait of Magellan link the Pacific with the Atlantic Ocean on the east. To the north, the Bering Strait connects the Pacific with the Arctic Ocean.

As the Pacific straddles the 180th meridian, the "West Pacific" (or "western Pacific", near Asia) is in the Eastern Hemisphere, while the "East Pacific" (or "eastern Pacific", near the Americas) is in the Western Hemisphere.

The Southern Pacific Ocean harbors the Southeast Indian Ridge crossing from south of Australia turning into the Pacific-Antarctic Ridge (north of the South Pole) and merges with another ridge (south of South America) to form the East Pacific Rise which also connects with another ridge (south of North America) which overlooks the Juan de Fuca Ridge.

For most of Magellan's voyage from the Strait of Magellan to the Philippines, the explorer indeed found the ocean peaceful. However, the Pacific is not always peaceful. Many tropical storms batter the islands of the Pacific. The lands around the Pacific Rim are full of volcanoes and often affected by earthquakes. Tsunamis, caused by underwater earthquakes, have devastated many islands and in some cases destroyed entire towns.

The Martin Waldseemüller map of 1507 was the first to show the Americas separating two distinct oceans. Later, the Diogo Ribeiro map of 1529 was the first to show the Pacific at about its proper size.


" The status of Taiwan and China is disputed. For more information, see political status of Taiwan."

This ocean has most of the islands in the world. There are about 25,000 islands in the Pacific Ocean. The islands entirely within the Pacific Ocean can be divided into three main groups known as Micronesia, Melanesia and Polynesia. Micronesia, which lies north of the equator and west of the International Date Line, includes the Mariana Islands in the northwest, the Caroline Islands in the center, the Marshall Islands to the west and the islands of Kiribati in the southeast.

Melanesia, to the southwest, includes New Guinea, the world's second largest island after Greenland and by far the largest of the Pacific islands. The other main Melanesian groups from north to south are the Bismarck Archipelago, the Solomon Islands, Santa Cruz, Vanuatu, Fiji and New Caledonia.

The largest area, Polynesia, stretching from Hawaii in the north to New Zealand in the south, also encompasses Tuvalu, Tokelau, Samoa, Tonga and the Kermadec Islands to the west, the Cook Islands, Society Islands and Austral Islands in the center, and the Marquesas Islands, Tuamotu, Mangareva Islands, and Easter Island to the east.

Islands in the Pacific Ocean are of four basic types: continental islands, high islands, coral reefs and uplifted coral platforms. Continental islands lie outside the andesite line and include New Guinea, the islands of New Zealand, and the Philippines. Some of these islands are structurally associated with nearby continents. High islands are of volcanic origin, and many contain active volcanoes. Among these are Bougainville, Hawaii, and the Solomon Islands.

The coral reefs of the South Pacific are low-lying structures that have built up on basaltic lava flows under the ocean's surface. One of the most dramatic is the Great Barrier Reef off northeastern Australia with chains of reef patches. A second island type formed of coral is the uplifted coral platform, which is usually slightly larger than the low coral islands. Examples include Banaba (formerly Ocean Island) and Makatea in the Tuamotu group of French Polynesia.

The volume of the Pacific Ocean, representing about 50.1 percent of the world's oceanic water, has been estimated at some . Surface water temperatures in the Pacific can vary from , the freezing point of sea water, in the poleward areas to about near the equator. Salinity also varies latitudinally, reaching a maximum of 37 parts per thousand in the southeastern area. The water near the equator, which can have a salinity as low as 34 parts per thousand, is less salty than that found in the mid-latitudes because of abundant equatorial precipitation throughout the year. The lowest counts of less than 32 parts per thousand are found in the far north as less evaporation of seawater takes place in these frigid areas. The motion of Pacific waters is generally clockwise in the Northern Hemisphere (the North Pacific gyre) and counter-clockwise in the Southern Hemisphere. The North Equatorial Current, driven westward along latitude 15°N by the trade winds, turns north near the Philippines to become the warm Japan or Kuroshio Current.

Turning eastward at about 45°N, the Kuroshio forks and some water moves northward as the Aleutian Current, while the rest turns southward to rejoin the North Equatorial Current. The Aleutian Current branches as it approaches North America and forms the base of a counter-clockwise circulation in the Bering Sea. Its southern arm becomes the chilled slow, south-flowing California Current. The South Equatorial Current, flowing west along the equator, swings southward east of New Guinea, turns east at about 50°S, and joins the main westerly circulation of the South Pacific, which includes the Earth-circling Antarctic Circumpolar Current. As it approaches the Chilean coast, the South Equatorial Current divides; one branch flows around Cape Horn and the other turns north to form the Peru or Humboldt Current.

The climate patterns of the Northern and Southern Hemispheres generally mirror each other. The trade winds in the southern and eastern Pacific are remarkably steady while conditions in the North Pacific are far more varied with, for example, cold winter temperatures on the east coast of Russia contrasting with the milder weather off British Columbia during the winter months due to the preferred flow of ocean currents.

In the tropical and subtropical Pacific, the El Niño Southern Oscillation (ENSO) affects weather conditions. To determine the phase of ENSO, the most recent three-month sea surface temperature average for the area approximately to the southeast of Hawaii is computed, and if the region is more than above or below normal for that period, then an El Niño or La Niña is considered in progress. 
In the tropical western Pacific, the monsoon and the related wet season during the summer months contrast with dry winds in the winter which blow over the ocean from the Asian landmass. Worldwide, tropical cyclone activity peaks in late summer, when the difference between temperatures aloft and sea surface temperatures is the greatest. However, each particular basin has its own seasonal patterns. On a worldwide scale, May is the least active month, while September is the most active month. November is the only month in which all the tropical cyclone basins are active. The Pacific hosts the two most active tropical cyclone basins, which are the northwestern Pacific and the eastern Pacific. Pacific hurricanes form south of Mexico, sometimes striking the western Mexican coast and occasionally the southwestern United States between June and October, while typhoons forming in the northwestern Pacific moving into southeast and east Asia from May to December. Tropical cyclones also form in the South Pacific basin, where they occasionally impact island nations.

In the arctic, icing from October to May can present a hazard for shipping while persistent fog occurs from June to December. A climatological low in the Gulf of Alaska keeps the southern coast wet and mild during the winter months. The Westerlies and associated jet stream within the Mid-Latitudes can be particularly strong, especially in the Southern Hemisphere, due to the temperature difference between the tropics and Antarctica, which records the coldest temperature readings on the planet. In the Southern hemisphere, because of the stormy and cloudy conditions associated with extratropical cyclones riding the jet stream, it is usual to refer to the Westerlies as the Roaring Forties, Furious Fifties and Shrieking Sixties according to the varying degrees of latitude.

The ocean was first mapped by Abraham Ortelius; he called it Maris Pacifici following Ferdinand Magellan's description of it as "a pacific sea" during his circumnavigation from 1519 to 1522. To Magellan, it seemed much more calm (pacific) than the Atlantic.

The andesite line is the most significant regional distinction in the Pacific. A petrologic boundary, it separates the deeper, mafic igneous rock of the Central Pacific Basin from the partially submerged continental areas of felsic igneous rock on its margins. The andesite line follows the western edge of the islands off California and passes south of the Aleutian arc, along the eastern edge of the Kamchatka Peninsula, the Kuril Islands, Japan, the Mariana Islands, the Solomon Islands, and New Zealand's North Island.

The dissimilarity continues northeastward along the western edge of the Andes Cordillera along South America to Mexico, returning then to the islands off California. Indonesia, the Philippines, Japan, New Guinea, and New Zealand lie outside the andesite line.

Within the closed loop of the andesite line are most of the deep troughs, submerged volcanic mountains, and oceanic volcanic islands that characterize the Pacific basin. Here basaltic lavas gently flow out of rifts to build huge dome-shaped volcanic mountains whose eroded summits form island arcs, chains, and clusters. Outside the andesite line, volcanism is of the explosive type, and the Pacific Ring of Fire is the world's foremost belt of explosive volcanism. The Ring of Fire is named after the several hundred active volcanoes that sit above the various subduction zones.

The Pacific Ocean is the only ocean which is almost totally bounded by subduction zones. Only the Antarctic and Australian coasts have no nearby subduction zones.

The Pacific Ocean was born 750 million years ago at the breakup of Rodinia, although it is generally called the Panthalassic Ocean until the breakup of Pangea, about 200 million years ago. The oldest Pacific Ocean floor is only around 180 Ma old, with older crust subducted by now.

The Pacific Ocean contains several long seamount chains, formed by hotspot volcanism. These include the Hawaiian–Emperor seamount chain and the Louisville Ridge.

The exploitation of the Pacific's mineral wealth is hampered by the ocean's great depths. In shallow waters of the continental shelves off the coasts of Australia and New Zealand, petroleum and natural gas are extracted, and pearls are harvested along the coasts of Australia, Japan, Papua New Guinea, Nicaragua, Panama, and the Philippines, although in sharply declining volume in some cases.

Fish are an important economic asset in the Pacific. The shallower shoreline waters of the continents and the more temperate islands yield herring, salmon, sardines, snapper, swordfish, and tuna, as well as shellfish. Overfishing has become a serious problem in some areas. For example, catches in the rich fishing grounds of the Okhotsk Sea off the Russian coast have been reduced by at least half since the 1990s as a result of overfishing.

The quantity of small plastic fragments floating in the north-east Pacific Ocean increased a hundredfold between 1972 and 2012.

Marine pollution is a generic term for the harmful entry into the ocean of chemicals or particles. The main culprits are those using the rivers for disposing of their waste. The rivers then empty into the ocean, often also bringing chemicals used as fertilizers in agriculture. The excess of oxygen-depleting chemicals in the water leads to hypoxia and the creation of a dead zone.

Marine debris, also known as marine litter, is human-created waste that has ended up floating in a lake, sea, ocean, or waterway. Oceanic debris tends to accumulate at the center of gyres and coastlines, frequently washing aground where it is known as beach litter.

In addition, the Pacific Ocean has served as the crash site of satellites, including Mars 96, Fobos-Grunt, and Upper Atmosphere Research Satellite.






</doc>
<doc id="23071" url="https://en.wikipedia.org/wiki?curid=23071" title="Prince Edward Island">
Prince Edward Island

Prince Edward Island (PEI or P.E.I.; ) is a province of Canada consisting of the island of the same name, and several much smaller islands. Prince Edward Island is one of the three Maritime Provinces and is the smallest province in both land area and population. It is part of the traditional lands of the Mi'kmaq, and became a British colony in the 1700s and was federated into Canada as a province in 1873. Its capital is Charlottetown. According to the 2016 census, the province of Prince Edward Island has 142,907 residents.

The backbone of the economy is farming; it produces 25% of Canada's potatoes. The island has several informal names: "Garden of the Gulf", referring to the pastoral scenery and lush agricultural lands throughout the province; and "Birthplace of Confederation" or "Cradle of Confederation", referring to the Charlottetown Conference in 1864, although PEI did not join Confederation until 1873, when it became the seventh Canadian province. Historically, PEI is one of Canada's older settlements and demographically still reflects older immigration to the country, with Celtic, Anglo-Saxon and French surnames being dominant to this day.

PEI is located about north of Halifax, Nova Scotia, and east of Quebec City. It consists of the main island and 231 minor islands. Altogether, the entire province has a land area of . The main island is in size, slightly larger than the U.S. state of Delaware. It is the 104th-largest island in the world and Canada's 23rd-largest island. PEI is the only subnational jurisdiction of North America outside the Caribbean to have no mainland territory, and the only such jurisdiction to have no land boundary, as the U.S. state of Hawaii is a part of Oceania, not North America.

In 1798, the British named the island colony for Prince Edward, Duke of Kent and Strathearn (1767–1820), the fourth son of King George III and the father of Queen Victoria. Prince Edward has been called "Father of the Canadian Crown". The following island landmarks are also named after the Duke of Kent:


In French, the island is called today "Île-du-Prince-Édouard", but its former French name, as part of Acadia, was "Île Saint-Jean" (St. John's Island). The island is known in Scottish Gaelic as "Eilean a' Phrionnsa" (lit. "the Island of the Prince", the local form of the longer 'Eilean a' Phrionnsa Iomhair/Eideard') or "Eilean Eòin" for some Gaelic speakers in Nova Scotia though not on PEI (lit. "John's Island" in reference to the island's former name). The island is known in the Mi'kmaq language as "Abegweit" or "Epekwitk" roughly translated as "land cradled in the waves".

Prince Edward Island is located in the Gulf of St. Lawrence, west of Cape Breton Island, north of the Nova Scotia peninsula, and east of New Brunswick. Its southern shore bounds the Northumberland Strait. The island has two urban areas. The largest surrounds Charlottetown Harbour, situated centrally on the island's southern shore, and consists of the capital city Charlottetown, and suburban towns Cornwall and Stratford and a developing urban fringe. A much smaller urban area surrounds Summerside Harbour, situated on the southern shore west of Charlottetown Harbour, and consists primarily of the city of Summerside. As with all natural harbours on the island, Charlottetown and Summerside harbours are created by rias.

The island's landscape is pastoral. Rolling hills, woods, reddish white sand beaches, ocean coves and the famous red soil have given Prince Edward Island a reputation as a province of outstanding natural beauty. The provincial government has enacted laws to preserve the landscape through regulation, although there is a lack of consistent enforcement, and an absence of province-wide zoning and land-use planning. Under the Planning Act of the province, municipalities have the option to assume responsibility for land-use planning through the development and adoption of official plans and land use bylaws. Thirty-one municipalities have taken responsibility for planning. In areas where municipalities have not assumed responsibility for planning, the Province remains responsible for development control.

The island's lush landscape has a strong bearing on its economy and culture. The author Lucy Maud Montgomery drew inspiration from the land during the late Victorian Era for the setting of her classic novel "Anne of Green Gables" (1908). Today, many of the same qualities that Montgomery and others found in the island are enjoyed by tourists who visit year-round. They enjoy a variety of leisure activities, including beaches, various golf courses, eco-tourism adventures, touring the countryside, and enjoying cultural events in local communities around the island.

The smaller, rural communities as well as the towns and villages throughout the province, retain a slower-paced, old-world flavour. Prince Edward Island has become popular as a tourist destination for relaxation. The economy of most rural communities on the island is based on small-scale agriculture. Industrial farming has increased as businesses buy and consolidate older farm properties.

The coastline has a combination of long beaches, dunes, red sandstone cliffs, salt water marshes, and numerous bays and harbours. The beaches, dunes and sandstone cliffs consist of sedimentary rock and other material with a high iron concentration, which oxidises upon exposure to the air. The geological properties of a white silica sand found at Basin Head are unique in the province; the sand grains cause a scrubbing noise as they rub against each other when walked on, and have been called the "singing sands".

Large dune fields on the north shore can be found on barrier islands at the entrances to various bays and harbours. The magnificent sand dunes at Greenwich are of particular significance. The shifting, parabolic dune system is home to a variety of birds and rare plants; it is also a site of significant archeological interest.

Despite Prince Edward Island's small size and reputation as a largely rural province, it is the most densely populated province in Canada.

The climate of the island is considered to be moderate and strongly influenced by the surrounding seas. As such, it is milder than inland locations owing to the warm waters from the Gulf of St. Lawrence. The climate is characterized by changeable weather throughout the year; it has some of the most variable day-to-day weather in Canada, in which specific weather conditions seldom last for long.

During July and August, the average daytime high in PEI is ; however, the temperature can sometimes exceed during these months. In the winter months of January and February, the average daytime high is . The Island receives an average yearly rainfall of and an average yearly snowfall of .

Winters are moderately cold and long but are milder than inland locations, with clashes of cold Arctic air and milder Atlantic air causing frequent temperature swings. The climate is considered to be more continental than oceanic since the Gulf of St. Lawrence freezes over, thus eliminating any moderation. The mean temperature is in January. During the winter months, the island usually has many storms (which may produce rain as well as snow) and blizzards since during this time, storms originating from the North Atlantic or the Gulf of Mexico frequently pass through. Springtime temperatures typically remain cool until the sea ice has melted, usually in late April or early May. 

Summers are moderately warm, but rarely uncomfortable, with the daily maximum temperature only occasionally reaching as high as . Autumn is a pleasant season, as the moderating Gulf waters delay the onset of frost, although storm activity increases compared to the summer. There is ample precipitation throughout the year, although it is heaviest in the late autumn, early winter and mid spring.

Between 250 and 300 million years ago, freshwater streams flowing from ancient mountains brought silt, sand and gravel into what is now the Gulf of St. Lawrence. These sediments accumulated to form a sedimentary basin, and make up the island's bedrock. When the Pleistocene glaciers receded about 15,000 years ago, glacial debris such as till were left behind to cover most of the area that would become the island. This area was connected to the mainland by a strip of land, but when ocean levels rose as the glaciers melted this land strip was flooded, forming the island. As the land rebounded from the weight of the ice, the island rose up to elevate it further from the surrounding water.

Most of the bedrock in Prince Edward Island is composed of red sandstone, part of the Permian aged Pictou Group.

Although commercial deposits of minerals have not been found, exploration in the 1940s for natural gas beneath the northeastern end of the province resulted in the discovery of an undisclosed quantity of gas. The Island was reported by government to have only 0.08 tcf of "technically recoverable" natural gas. Twenty exploration wells for hydrocarbon resources have been drilled on Prince Edward Island and offshore. The first reported well was Hillsborough No.#1, drilled in Charlottetown Harbour in 1944 (the world’s first offshore well), and the most recent was New Harmony No.#1 in 2007. Since the resurgence of exploration in the mid-1990s, all wells that have shown promising gas deposits have been stimulated through hydraulic fracture or “fracking”. All oil and natural gas exploration and exploitation activities on the Island are governed by the "Oil and Natural Gas Act" R.S.P.E.I. 1988, Cap. 0-5 and its associated regulations and orders.

The Province of Prince Edward Island is completely dependent on groundwater for its source of drinking water. As groundwater flows through an aquifer it is naturally filtered. The water for City of Charlottetown is extracted from thirteen wells in three wellfields and distributed to customers. The water removed is replenished by precipitation.

Infrastructure in Charlottetown that was installed in 1888 is still in existence. With the age of the system in the older part of Charlottetown, concern has been raised regarding lead pipes. The Utility has been working with its residents on a lead replacement program. A plebiscite in 1967 was held in Charlottetown over fluoridation, and residents voted in favour. Under provincial legislation, the Utility is required to report to its residents on an annual basis. It is also required to do regular sampling of the water and an overview is included in each annual report. The Winter River watershed provides about 92 per cent of the 18 million litre water supply for the city of Charlottetown, which had difficulty in each of 2011, 2012 and 2013 with its supply, until water meters were installed.

Minister of Communities, Land and Environment Robert Mitchell tabled a discussion paper on the proposed "Water Act" for the province on July 8, 2015. The use of groundwater came under scrutiny as the potato industry, which accounts for $1 billion every year and 50% of farm receipts, has pressed the government to lift a moratorium on high-capacity water wells for irrigation. The release of the discussion paper was to set off a consultation process in the autumn of 2015.

Detailed information about the quality of drinking water in PEI communities and watersheds can be found at the Department of Environment, Labour and Justice. It provides a summary of the ongoing testing of drinking water done by the Prince Edward Island Analytical Laboratories. Average drinking water quality results are available, and information on the following parameters are provided: alkalinity, cadmium, calcium, chloride, chromium, iron, magnesium, manganese, nickel, nitrate, pH, phosphorus, potassium, sodium, and sulfate, as well as the presence of pesticides. Water testing services are provided for a variety of clients through the PEI Analytical Laboratories, which assesses according to the recommendations of the Guidelines for Canadian Drinking Water Quality published by Health Canada.

Prince Edward Island used to have native moose, bear, caribou, wolf, and other larger species. Due to hunting and habitat disruption these species are no longer found on the island. Some species common to P.E.I. are red foxes, coyote, blue jays, and robins. Skunks and raccoons are common non-native species. Species at risk in P.E.I. include piping plovers, american eel, bobolinks, little brown bat, and beach pinweed.

Some species are unique to the province. In 2008, a new ascomycete species, "Jahnula apiospora" (Jahnulales, Dothideomycetes), was collected from submerged wood in a freshwater creek on Prince Edward Island, Canada. North Atlantic right whales, one of the rarest whale species, were once thought to be rare visitors into St. Lawrence regions until 1994, have been showing dramatic increases (annual concentrations were discovered off Percé in 1995 and gradual increases across the regions since in 1998), and since in 2014, notable numbers of whales have been recorded around Cape Breton to Prince Edward Island as 35 to 40 whales were seen in these areas in 2015.

Since before the influx of Europeans, the Mi'kmaq First Nations have inhabited Prince Edward Island as part of the region of Mi'kma'ki. They named the Island "Epekwitk", meaning "cradled on the waves"; Europeans represented the pronunciation as "Abegweit". Another name is "Minegoo". The Mi'kmaq's legend is that the island was formed by the Great Spirit placing on the Blue Waters some dark red crescent-shaped clay. There are two Mi'kmaq First Nation communities on Epekwitk today.

In 1534, Jacques Cartier was the first European to see the island. In 1604, France claimed the lands of the Maritimes, including Prince Edward Island, establishing the French colony of Acadia. The island was named "Île Saint-Jean" by the French. The Mi'kmaq never recognized the claim but welcomed the French as trading partners and allies.

After the Siege of Louisbourg (1745) during the War of the Austrian Succession, the New Englanders also captured Île Saint-Jean (Prince Edward Island). An English detachment landed at Port-la-Joye. Under the command of Joseph du Pont Duvivier, the French had a garrison of 20 French troops at Port-la-Joye. The troops fled and New Englanders burned the capital to the ground. Duvivier and the twenty men retreated up the Northeast River (Hillsborough River), pursued by the New Englanders until the French troops received reinforcements from the Acadian militia and the Mi'kmaq. 

The French troops and their allies were able to drive the New Englanders to their boats, nine New Englanders killed, wounded or made prisoner. The New Englanders took six Acadian hostages, who would be executed if the Acadians or Mi'kmaq rebelled against New England control. The New England troops left for Louisbourg. Duvivier and his 20 troops left for Quebec. After the fall of Louisbourg, the resident French population of Île Royal were deported to France. The Acadians of Île Saint-Jean lived under the threat of deportation for the remainder of the war. 

The New Englanders had a force of two war ships and 200 soldiers stationed at Port-La-Joye. To regain Acadia, Ramezay was sent from Quebec to the region to join forces with the Duc d'Anville expedition. Upon arriving at Chignecto, he sent Boishebert to Île Saint-Jean on a reconnaissance to assess the size of the New England force. After Boishebert returned, Ramezay sent Joseph-Michel Legardeur de Croisille et de Montesson along with over 500 men, 200 of whom were Mi'kmaq, to Port-La-Joye. In July 1746, the battle happened near York River. Montesson and his troops killed forty New Englanders and captured the rest. Montesson was commended for having distinguished himself in his first independent command.

Roughly one thousand Acadians lived on the island, many of whom had fled to the island from mainland Nova Scotia during the first wave of the British-ordered expulsion in 1755, reaching a population of 5,000. However, many more were forcibly deported during the second wave of the expulsion after the Siege of Louisbourg (1758). In the Île Saint-Jean Campaign (1758) General Jeffery Amherst ordered Colonel Andrew Rollo to capture the island. Many Acadians died in the expulsion en route to France; on December 13, 1758, the transport ship "Duke William" sank and 364 died. A day earlier the "Violet" sank and 280 died; several days later sank with 213 on board.

Great Britain claimed the island as part of Nova Scotia in 1763, when France gave up its claim to the island. This was under the terms of the "Treaty of Paris" which settled the Seven Years' War. The island was split into a separate colony in 1769, which the British called St. John's Island (also the Island of St. John's). P.E.I. was part of New Scotland at this time. The high influx of Scottish Highlanders in the late 1700s resulted in P.E.I. having the highest provincial percentage of Scottish immigrants in Canada. (Not to underestimate the highest percentage of Scottish on nearby Cape Breton Island, however only being a part thereof the province of Nova Scotia.) This, in turn, led to a higher proportion of Scottish Gaelic speakers and thriving culture surviving on P.E.I. than Scotland itself, as the settlers avoided English influence overseas. Hence the formally existing Prince Edward Island Highland Regiment founded in 1875.

The first British governor of St. John's Island, Walter Patterson, was appointed in 1769. Assuming office in 1770, he had a controversial career during which land title disputes and factional conflict slowed the initial attempts to populate and develop the island under a feudal system. In an attempt to attract settlers from Ireland, in one of his first acts (1770) Patterson led the island's colonial assembly to rename the island "New Ireland", but the British Government promptly vetoed this as exceeding the authority vested in the colonial government; only the Privy Council in London could change the name of a colony.

In the mid-1760s, a survey team divided the Island into 67 lots. On July 1, 1767, these properties were allocated to supporters of King George III by means of a lottery. Ownership of the land remained in the hands of landlords in England, angering Island settlers who were unable to gain title to land on which they worked and lived. Significant rent charges (to absentee landlords) created further anger. The land had been given to the absentee landlords with a number of conditions attached regarding upkeep and settlement terms; many of these conditions were not satisfied. Islanders spent decades trying to convince the Crown to confiscate the lots, however the descendants of the original owners were generally well connected to the British government and refused to give up the land.

In 1853, the Island government passed the Land Purchase Act which empowered them to purchase lands from those owners who were willing to sell, and then resell the land to settlers for low prices. This scheme collapsed when the Island ran short of money to continue with the purchases. Many of these lands also were fertile, and were some of the key factors to sustaining Prince Edward Island's economy.

During the American Revolutionary War Charlottetown was raided in 1775 by a pair of American-employed privateers. Two armed schooners, "Franklin" and "Hancock", from Beverly, Massachusetts, made prisoner of the attorney-general at Charlottetown, on advice given them by some Pictou residents after they had taken eight fishing vessels in the Gut of Canso.

During and after the American Revolutionary War, from 1776 to 1783, the colony's efforts to attract exiled Loyalist refugees from the rebellious American colonies met with some success. Walter Patterson's brother, John Patterson, one of the original grantees of land on the island, was a temporarily exiled Loyalist and led efforts to persuade others to come.

The 1787 dismissal of Governor Patterson and his recall to London in 1789 dampened his brother's efforts, leading John to focus on his interests in the United States (one of John's sons, Commodore Daniel Patterson, became a noted United States Navy hero, and John's grandsons, Rear Admiral Thomas H. Patterson and Lt. Carlile Pioou). Edmund Fanning, also a Loyalist exiled by the Revolution, took over as the second governor, serving until 1804. His tenure was more successful than Patterson's.

On November 29, 1798, during Fanning's administration, Great Britain granted approval to change the colony's name from St. John's Island to Prince Edward Island to distinguish it from similar names in what is now Atlantic Canada, such as the cities of Saint John, New Brunswick and St. John's in Newfoundland. The colony's new name honoured the fourth son of King George III, Prince Edward Augustus, the Duke of Kent (1767–1820), who subsequently led the British military forces on the continent as Commander-in-Chief, North America (1799–1800), with his headquarters in Halifax. (Prince Edward later became the father of the future Queen Victoria.)

In September 1864, Prince Edward Island hosted the Charlottetown Conference, which was the first meeting in the process leading to the Quebec Resolutions and the creation of Canada in 1867. Prince Edward Island did not find the terms of union favourable and balked at joining in 1867, choosing to remain a colony of the United Kingdom. In the late 1860s, the colony examined various options, including the possibility of becoming a discrete dominion unto itself, as well as entertaining delegations from the United States, who were interested in Prince Edward Island joining the United States.

In 1871, the colony began construction of a railway and, frustrated by Great Britain's Colonial Office, began negotiations with the United States. In 1873, Canadian Prime Minister Sir John A. Macdonald, anxious to thwart American expansionism and facing the distraction of the Pacific Scandal, negotiated for Prince Edward Island to join Canada. The Dominion Government of Canada assumed the colony's extensive railway debts and agreed to finance a buy-out of the last of the colony's absentee landlords to free the island of leasehold tenure and from any new immigrants entering the island (accomplished through the passage of the "Land Purchase Act, 1875"). Prince Edward Island entered Confederation on July 1, 1873.

As a result of having hosted the inaugural meeting of Confederation, the Charlottetown Conference, Prince Edward Island presents itself as the "Birthplace of Confederation" and this is commemorated through several buildings, a ferry vessel, and the Confederation Bridge (constructed 1993 to 1997). The most prominent building in the province honouring this event is the Confederation Centre of the Arts, presented as a gift to Prince Edward Islanders by the 10 provincial governments and the Federal Government upon the centenary of the Charlottetown Conference, where it stands in Charlottetown as a national monument to the "Fathers of Confederation". The Centre is one of the 22 National Historic Sites of Canada located in Prince Edward Island.

According to the 2011 National Household Survey, the largest ethnic group consists of people of Scottish descent (39.2%), followed by English (31.1%), Irish (30.4%), French (21.1%), German (5.2%), and Dutch (3.1%) descent. Prince Edward Island's population is largely white; there are few visible minorities. Chinese Canadians are the largest visible minority group of Prince Edward Island, comprising 1.3% of the province's population. Almost half of respondents identified their ethnicity as "Canadian".
"Source: Statistics Canada"

The Canada 2016 Census showed a population of 142,910. Of the 140,020 singular responses to the census question concerning mother tongue, the most commonly reported languages were as follows:

In addition, there were 460 responses of both English and a "non-official language"; 30 of both French and a "non-official language"; 485 of both English and French; and 20 of English, French, and a "non-official language". (Figures shown are for the number of single language responses and the percentage of total single-language responses.)

Traditionally the population has been evenly divided between Catholic and Protestant affiliations. The 2001 census indicated number of adherents for the Roman Catholic Church with 63,240 (47%) and various Protestant churches with 57,805 (43%). This included the United Church of Canada with 26,570 (20%); the Presbyterian Church with 7,885 (6%) and the Anglican Church of Canada with 6,525 (5%); those with no religion were among the lowest of the provinces with 8,705 (6.5%). If one considers that the founders of the United Church of Canada were largely Presbyterians in Prince Edward Island, the Island has one of the highest percentages of Presbyterians in the country. The Island also has one of the largest number of Free Church of Scotland buildings in Canada, though attendance at many of these churches is very low today.

The provincial economy is dominated by the seasonal industries of agriculture, tourism, and the fishery. The province is limited in terms of heavy industry and manufacturing, though Cavendish Farms runs extensive food manufacturing operations on PEI.

Agriculture remains the dominant industry in the provincial economy, as it has since colonial times. The Island has a total land area of with approximately cleared for agricultural use. In 2006, the Census of Agriculture counted 1700 farms on the Island. During the 20th century, potatoes replaced mixed farming as the leading cash crop, accounting for one-third of provincial farm income. The number of acres under potato production in 2010 was 88,000, while soy accounted for 55,000. There are approximately 330 potato growers on PEI, with the grand majority of these being family farms, often with multiple generations working together. The province currently accounts for a third of Canada's total potato production, producing approximately annually. Comparatively, the state of Idaho produces approximately annually, with a population approximately 9.5 times greater. The province is a major producer of seed potatoes, exporting to more than twenty countries around the world. An estimated total of 70% of the land is cultivated and 25% of all potatoes grown in Canada originate from P.E.I. The processing of frozen fried potatoes, green vegetables, and berries is a leading business activity.

As a legacy of the island's colonial history, the provincial government enforces extremely strict rules for non-resident land ownership, especially since the PEI "Lands Protection Act" of 1982. Residents and corporations are limited to maximum holdings of 400 and 1,200 hectares respectively. There are also restrictions on non-resident ownership of shorelines.

The island's economy has grown significantly over the last decade in key areas of innovation. Aerospace, Bioscience, ICT and Renewable energy have been a focus for growth and diversification. Aerospace alone now accounts for over 25% of the province's international exports and is the island's fourth largest industry at $355 million in annual sales. The Bioscience industry employs over 1300 people and generates over $150 million in sales.

Many of the province's coastal communities rely upon shellfish harvesting, particularly lobster fishing as well as oyster fishing and mussel farming.

The sale of carbonated beverages such as beer and soft drinks in non-refillable containers, such as aluminum cans or plastic bottles, was banned in 1976 as an environmental measure in response to public concerns over litter. Beer and soft drink companies opted to use refillable glass bottles for their products which were redeemable at stores and bottle depots.

Though often environmental and economic agendas may be at odds, the ‘ban the can’ legislation along with being environmentally driven, was also economically motivated as it protected jobs. Seaman's Beverages, a bottling company and carbonated beverage manufacturer, was established in 1939 and a major employer in Charlottetown, Prince Edward Island. Making it illegal to retail cans led to a bigger share of the carbonated beverage market for Seamans. Seamans Beverages was eventually acquired by Pepsi Bottling Group Inc in 2002 prior to the lifting of the legislation.

The introduction of recycling programs for cans and plastic bottles in neighbouring provinces in recent years (also using a redemption system) has seen the provincial government introduce legislation to reverse this ban with the restriction lifted on May 3, 2008.

Prince Edward Island has one of Canada's highest provincial retail sales tax rate at 9%. The tax is applied to almost all goods and services except some clothing, food and home heating fuel. The tax, along with the Federal Goods and Services Tax, is incorporated into the Harmonized Sales Tax in the province.

The provincial government provides consumer protection in the form of regulation for certain items, ranging from apartment rent increases to petroleum products including gas, diesel, propane and heating oil. These are regulated through the Prince Edward Island Regulatory and Appeals Commission (IRAC). IRAC is authorized to limit the number of companies who are permitted to sell petroleum products.

The average family income on Prince Edward Island is $62,110/year, and the minimum wage of $10.50/hour as of July 1, 2015.

At present, approximately fifteen percent of electricity consumed on the island is generated from renewable energy (largely wind turbines); the provincial government has set renewable energy targets as high as 30-50% for electricity consumed by 2015. Until wind generation, the province relied entirely on electricity imports on a submarine cable from New Brunswick. A thermal oil-fired generating station in Charlottetown is also available. Electricity rates in the province were in 2011 the highest in Canada, at a domestic rate of 0.161 $/kWh. The province imports about 85 per cent of its power through New Brunswick. 

The maintenance shutdown of Point Lepreau nuclear plant forced the province to acquire most of its electrons on the expensive open market. The result was a steep price hikes of about 25 per cent in the three years to 2011 but the province later subsidised rates. Residents were to pay 11.2 per cent more for electricity when the harmonized sales tax was adopted in April 2013, according to the P.E.I. Energy Accord that was tabled in the legislature on December 7, 2012. and passed as the "Electric Power (Energy Accord Continuation) Amendment Act", which establishes electric pricing from April 1, 2013, to March 1, 2016. Regulatory powers are derived for IRAC from the "Electric Power Act". Since 1918 Maritime Electric has delivered electricity to customers on the Island. The utility is currently owned and operated by Fortis Inc.

The provincial government is responsible for such areas as health and social services, education, economic development, labour legislation and civil law. These matters of government are carried out in the provincial capital, Charlottetown.

Prince Edward Island is governed by a parliamentary government within the construct of constitutional monarchy; the monarchy in Prince Edward Island is the foundation of the executive, legislative, and judicial branches. The sovereign is Queen Elizabeth II, who also serves as head of state of 15 other Commonwealth countries, each of Canada's nine other provinces, and the Canadian federal realm, and resides predominantly in the United Kingdom. As such, the Queen's representative, the Lieutenant Governor of Prince Edward Island (presently Antoinette Perry), carries out most of the royal duties in Prince Edward Island.

The direct participation of the royal and viceroyal figures in any of these areas of governance is limited; in practice, their use of the executive powers is directed by the Executive Council, a committee of ministers of the Crown responsible to the unicameral, elected Legislative Assembly and chosen and headed by the Premier of Prince Edward Island (presently Wade MacLauchlan), the head of government. To ensure the stability of government, the lieutenant governor will usually appoint as premier the person who is the current leader of the political party that can obtain the confidence of a plurality in the Legislative Assembly. The leader of the party with the second-most seats usually becomes the Leader of Her Majesty's Loyal Opposition (presently Jamie Fox) and is part of an adversarial parliamentary system intended to keep the government in check.

Each of the 27 Members of the Legislative Assembly (MLA) is elected by simple plurality in an electoral district. General elections are called by the lieutenant governor on the first Monday in October four years after the previous election, or may be called, on the advice of the premier, should the government lose a confidence vote in the legislature. Traditionally, politics in the province have been dominated by both the Liberal Party and the Progressive Conservative Party. But however, since 2015 election, Green Party of Prince Edward Island has to begin rising in couple districts as of late 2017 with then-recent was Charlottetown-Parkdale district has elected one of Party's member Hannah Bell from formerly seated Liberal MLA Doug Currie.

The Mi'kmaq Confederacy of PEI is the tribal council and provincial-territorial organization in the province that represents both the Lennox Island and Abegweit First Nations.

Prince Edward Island's transportation network has traditionally revolved around its seaports of Charlottetown, Summerside, Borden, Georgetown, and Souris —linked to its railway system, and the two main airports in Charlottetown and Summerside, for communication with mainland North America. The railway system was abandoned by CN in 1989 in favour of an agreement with the federal government to improve major highways.

Until 1997, the province was linked by two passenger-vehicle ferry services to the mainland: one, provided by Marine Atlantic, operated year-round between Borden and Cape Tormentine, New Brunswick; the other, provided by Northumberland Ferries Limited, operates seasonally between Wood Islands and Caribou, Nova Scotia. A third ferry service provided by CTMA operates all year round with seasonal times between Souris and Cap-aux-Meules, Quebec, in the Magdalen Islands.
On June 1, 1997, the Confederation Bridge opened, connecting Borden-Carleton to Cape Jourimain, New Brunswick. The world's longest bridge over ice-covered waters, it replaced the Marine Atlantic ferry service. Since then, the Confederation Bridge's assured transportation link to the mainland has altered the province's tourism and agricultural and fisheries export economies.

Several airlines service the Charlottetown Airport (CYYG); the Summerside Airport (CYSU) is an additional option for general aviation.

The Island has the highest concentration of roadways in Canada. The provincially managed portion of the network consists of of paved roadways and of non-paved or clay roads.

The province has very strict laws regarding use of road-side signs. Billboards and the use of portable signs are banned. There are standard direction information signs on roads in the province for various businesses and attractions in the immediate area. Some municipalities' by-laws also restrict the types of permanent signs that may be installed on private property.

There is an extensive bicycling / hiking trail that spans the island. The Confederation Trail is a recreational trail system. The land was once owned and used by Canadian National Railway (CN) as a rail line on the island.

Prince Edward Island is home to one university, the University of Prince Edward Island (UPEI), located in the city of Charlottetown.

The university was created by the Island legislature to replace Prince of Wales College and St. Dunstan's University. UPEI is also home to the Atlantic Veterinary College, which offers the region's only veterinary medicine program.

Holland College is the provincial community college, with campuses across the province, including specialised facilities such as the Atlantic Police Academy, Marine Training Centre, and the Culinary Institute of Canada.

Prince Edward Island is also home to Maritime Christian College. It is also home to Immanuel Christian School, a private Christian School in Charlottetown.

Prince Edward Island's public school system has an English school district named the English Language School Board, as well as a Francophone district, the Commission scolaire de langue française. The English language districts have a total of 10 secondary schools and 54 intermediate and elementary schools while the Francophone district has 6 schools covering all grades. 22 per cent of the student population is enrolled in French immersion. This is one of the highest levels in the country.

Today 23.5 per cent of residents aged 15 to 19 have bilingual skills, an increase of 100 per cent in a decade.
Prince Edward Island, along with most rural regions in North America, is experiencing an accelerated rate of youth emigration. The provincial government has projected that public school enrollment will decline by 40% during the 2010s.

The province has a single health administrative region (or district health authority) called Health PEI. Health PEI receives funding for its operations and is regulated by the Department of Health and Wellness.

Many PEI homes and businesses are serviced by central sewage collection and/or treatment systems. These are operated either by a municipality or a private utility. Many industrial operations have their own wastewater treatment facilities. Staff members with the Department of Environment, Labour and Justice provide advice to operators, as needed, on proper system maintenance. The IRAC regulates municipal water and sewer in the province, now under the "Environmental Protection Act". Since around 1900, the residents of the City of Charlottetown have benefited from a central sanitary sewer service. Early disposal practices, while advanced for their time, eventually were found to compromise the ecological integrity of the nearby Hillsborough River and the Charlottetown Harbour. By 1974, the Commission had spearheaded the development of a primary wastewater treatment plant, known as the Charlottetown Pollution Control Plant, together with the construction of several pumping stations along the City’s waterfront, and outfall piping deep into the Hillsborough River.

There are eight hospitals in the province.

Prince Edward Island offers programs and services in areas such as acute care, primary care, home care, palliative care, public health, chronic disease prevention, and mental health and addictions, to name a few. The provincial government has opened several family health centres in recent years in various rural and urban communities. A provincial cancer treatment centre at the Queen Elizabeth Hospital provides support to those dealing with various types of cancer-related illnesses. A family medicine residency program was established in 2009 with the Dalhousie University Faculty of Medicine as a means to encourage new physicians to work in Prince Edward Island.

Long-term-care services are also available with several programs in place to support seniors wishing to remain independent in their communities. Many medications for seniors are subsidized through a provincial pharmaceutical plan, however, Prince Edward Island remains one of the only provinces lacking a catastrophic drug coverage program for its residents.

The provincial government has several programs for early illness detection, including mammography and pap screening clinics. There are also asthma education and diabetes education programs, as well as prenatal programs, immunization programs and dental health risk prevention programs for children. The government is also attempting to implement a comprehensive integrated Electronic Health Record system.

The provincial government has recently committed to enhancing primary care and home care services and has invested in health care facilities in recent capital budgets; mostly replacements and upgrades to provincial government operated nursing homes and hospitals.

Some specialist services require patients to be referred to clinics and specialists in neighbouring provinces. Specialist operations and treatments are also provided at larger tertiary referral hospitals in neighbouring provinces such as the IWK Health Centre and Queen Elizabeth II Health Sciences Centre in Nova Scotia or the Saint John Regional Hospital, Moncton Hospital, and Dr. Georges-L.-Dumont University Hospital Centre in New Brunswick.

Ground ambulance service in Prince Edward Island is provided under contract by Island EMS. Air ambulance service is provided under contract by LifeFlight.

In recent decades, Prince Edward Island's population has shown statistically significant and abnormally high rates of diagnosed rare cancers, particularly in rural areas. Health officials, ecologists and environmental activists point to the use of pesticides for industrial potato farming as a primary contaminant.

Prince Edward Island is the only province in Canada that does not provide abortion services through its hospitals. The last abortion was performed in the province in 1982 prior to the opening of the Queen Elizabeth Hospital which saw the closure of the Roman Catholic-affiliated Charlottetown Hospital and the non-denominational Prince Edward Island Hospital; a condition of the "merger" being that abortions not be performed in the province. In 1988, following the court decision "R. v. Morgentaler", the then-opposition Progressive Conservative Party of Prince Edward Island tabled a motion demanding that the ban on abortions be upheld at the province's hospitals; the then-governing Prince Edward Island Liberal Party under Premier Joe Ghiz acquiesced and the ban was upheld. The Government of Prince Edward Island will fund abortions for women who travel to another province. Women from Prince Edward Island may also travel to the nearest private user-pay clinic, where they must pay for the procedure using their own funds. Formerly this was the Morgentaler Clinic in Fredericton, New Brunswick until this clinic closed due to lack of funds in July 2014. The clinic was reopened under new ownership in 2016 as Clinic 554 with expanded services. During that gap, women had to travel to Halifax or further. In 2016, the Liberal government led by Premier Wade MacLauchlan announced they would open a women's reproductive health clinic to provide abortions within the province.

The island's cultural traditions of art, music and creative writing are supported through the public education system. There is an annual arts festival, the Charlottetown Festival, hosted at the Confederation Centre of the Arts.

Lucy Maud Montgomery, who was born in Clifton (now New London) in 1874, wrote some 20 novels and numerous short stories that have been collected into anthologies. Her first "Anne" book "Anne of Green Gables" was published in 1908. The musical play "Anne of Green Gables" has run every year at the Charlottetown festival for more than four decades. The sequel, "Anne & Gilbert", premiered in the Playhouse in Victoria in 2005. The actual location of Green Gables, the house featured in Montgomery's "Anne" books, is in Cavendish, on the north shore of PEI.

Elmer Blaney Harris founded an artists colony at Fortune Bridge and set his famous play "Johnny Belinda" on the island. Robert Harris was a well-known artist.

Prince Edward Island's documented music history begins in the 19th century with religious music, some written by the local pump and block maker and organ-importer, Watson Duchemin. Several big bands including the Sons of Temperance Band and the Charlottetown Brass Band were active. Today, Acadian, Celtic, folk, and rock music prevail, with exponents including Gene MacLellan, his daughter Catherine MacLellan, Al Tuck, Lennie Gallant, Two Hours Traffic and Paper Lions. The celebrated singer-songwriter Stompin' Tom Connors spent his formative years in Skinners Pond. Celtic music is certainly the most common traditional music on the island, with fiddling and step dancing being very common. This tradition, largely Scottish, Irish and Acadian in origin is very similar to the music of Cape Breton and to a lesser extent, Newfoundland and is unique to the region. Due to the Islands influence as a former Highlander Clans Scottish colony, a March 4/4 for bagpipes was composed in honour of Prince Edward Island. There is also an annual jazz festival, the P.E.I. Jazz and Blues Festival.

There is an annual arts festival, the Charlottetown Festival, hosted at the Confederation Centre of the Arts as well as the Island Fringe Festival that takes place around Charlottetown. An annual jazz festival, the P.E.I. Jazz and Blues Festival. is a one-week-long series of concerts taking place at several venues including Murphy's Community Center, outdoor stages, and churches in Charlottetown. The moving of its date to mid-August caused in 2011 a serious loss in funding from Ottawa's regional development agency ACOA. The musician's line up in 2011 included Oliver Jones, Sophie Milman, Matt Dusk, Jack de Keyzer, Jack Semple, Meaghan Smith, Meaghan Blanchard, Hupman Brothers, Alex Dean, Charlie A'Court, Sean Ferris, Jimmy Bowskill, West End Blues Band, Bad Habits, Brian McConnell and Mellotones. There is also Canada Rocks, and the Cavendish Beach Music Festival. With agriculture and fishery playing a large role in the economy, P.E.I. has been marketed as a food tourism destination. Several food festivals have become popular such as the Fall Flavours festival and the Shellfish Festival.



Hainan Province, China, has been the sister province of Prince Edward Island since 2001. This came about after Vice-Governor Lin Fanglue stayed for two days to hold discussions about partnership opportunities and trade.




</doc>
<doc id="23080" url="https://en.wikipedia.org/wiki?curid=23080" title="Pretty Good Privacy">
Pretty Good Privacy

Pretty Good Privacy (PGP) is an encryption program that provides cryptographic privacy and authentication for data communication. PGP is used for signing, encrypting, and decrypting texts, e-mails, files, directories, and whole disk partitions and to increase the security of e-mail communications. Phil Zimmermann developed PGP in 1991.

PGP and similar software follow the OpenPGP standard (RFC 4880) for encrypting and decrypting data.

PGP encryption uses a serial combination of hashing, data compression, symmetric-key cryptography, and finally public-key cryptography; each step uses one of several supported algorithms. Each public key is bound to a user name or an e-mail address. The first version of this system was generally known as a web of trust to contrast with the X.509 system, which uses a hierarchical approach based on certificate authority and which was added to PGP implementations later. Current versions of PGP encryption include both options through an automated key management server.

As PGP evolves, versions that support newer features and algorithms are able to create encrypted messages that older PGP systems cannot decrypt, even with a valid private key. Therefore, it is essential that partners in PGP communication understand each other's capabilities or at least agree on PGP settings.

PGP can be used to send messages confidentially. For this, PGP combines symmetric-key encryption and public-key encryption. The message is encrypted using a symmetric encryption algorithm, which requires a symmetric key. Each symmetric key is used only once and is also called a session key. The message and its session key are sent to the receiver. The session key must be sent to the receiver so they know how to decrypt the message, but to protect it during transmission it is encrypted with the receiver's public key. Only the private key belonging to the receiver can decrypt the session key.

PGP supports message authentication and integrity checking. The latter is used to detect whether a message has been altered since it was completed (the "message integrity" property) and the former to determine whether it was actually sent by the person or entity claimed to be the sender (a "digital signature"). Because the content is encrypted, any changes in the message will result in failure of the decryption with the appropriate key. The sender uses PGP to create a digital signature for the message with either the RSA or DSA algorithms. To do so, PGP computes a hash (also called a message digest) from the plaintext and then creates the digital signature from that hash using the sender's private key.

Both when encrypting messages and when verifying signatures, it is critical that the public key used to send messages to someone or some entity actually does 'belong' to the intended recipient. Simply downloading a public key from somewhere is not an overwhelming assurance of that association; deliberate (or accidental) impersonation is possible. From its first version, PGP has always included provisions for distributing user's public keys in an 'identity certification ', which is also constructed cryptographically so that any tampering (or accidental garble) is readily detectable. However, merely making a certificate which is impossible to modify without being detected is insufficient; this can prevent corruption only after the certificate has been created, not before. Users must also ensure by some means that the public key in a certificate actually does belong to the person or entity claiming it. From its first release, PGP products have included an internal certificate 'vetting scheme' to assist with this, a trust model which has been called a web of trust. A given public key (or more specifically, information binding a user name to a key) may be digitally signed by a third party user to attest to the association between someone (actually a user name) and the key. There are several levels of confidence which can be included in such signatures. Although many programs read and write this information, few (if any) include this level of certification when calculating whether to trust a key.

The web of trust protocol was first described by Phil Zimmermann in 1992, in the manual for PGP version 2.0:

The web of trust mechanism has advantages over a centrally managed public key infrastructure scheme such as that used by S/MIME but has not been universally used. Users have been willing to accept certificates and check their validity manually or to simply accept them. No satisfactory solution has been found for the underlying problem.

In the (more recent) OpenPGP specification, "trust signatures" can be used to support creation of certificate authorities. A trust signature indicates both that the key belongs to its claimed owner and that the owner of the key is trustworthy to sign other keys at one level below their own. A level 0 signature is comparable to a web of trust signature since only the validity of the key is certified. A level 1 signature is similar to the trust one has in a certificate authority because a key signed to level 1 is able to issue an unlimited number of level 0 signatures. A level 2 signature is highly analogous to the trust assumption users must rely on whenever they use the default certificate authority list (like those included in web browsers); it allows the owner of the key to make other keys certificate authorities.

PGP versions have always included a way to cancel ('revoke') identity certificates. A lost or compromised private key will require this if communication security is to be retained by that user. This is, more or less, equivalent to the certificate revocation lists of centralised PKI schemes. Recent PGP versions have also supported certificate expiration dates.

The problem of correctly identifying a public key as belonging to a particular user is not unique to PGP. All public key/private key cryptosystems have the same problem, even if in slightly different guises, and no fully satisfactory solution is known. PGP's original scheme at least leaves the decision as to whether or not to use its endorsement/vetting system to the user, while most other PKI schemes do not, requiring instead that every certificate attested to by a central certificate authority be accepted as correct.

To the best of publicly available information, there is no known method which will allow a person or group to break PGP encryption by cryptographic or computational means. Indeed, in 1995, cryptographer Bruce Schneier characterized an early version as being "the closest you're likely to get to military-grade encryption." Early versions of PGP have been found to have theoretical vulnerabilities and so current versions are recommended. In addition to protecting data in transit over a network, PGP encryption can also be used to protect data in long-term data storage such as disk files. These long-term storage options are also known as data at rest, i.e. data stored, not in transit.

The cryptographic security of PGP encryption depends on the assumption that the algorithms used are unbreakable by direct cryptanalysis with current equipment and techniques.

In the original version, the RSA algorithm was used to encrypt session keys. RSA's security depends upon the one-way function nature of mathematical integer factoring. Similarly, the symmetric key algorithm used in PGP version 2 was IDEA, which might at some point in the future be found to have previously undetected cryptanalytic flaws. Specific instances of current PGP or IDEA insecurities (if they exist) are not publicly known. As current versions of PGP have added additional encryption algorithms, their cryptographic vulnerability varies with the algorithm used. However, none of the algorithms in current use are publicly known to have cryptanalytic weaknesses.

New versions of PGP are released periodically and vulnerabilities are fixed by developers as they come to light. Any agency wanting to read PGP messages would probably use easier means than standard cryptanalysis, e.g. rubber-hose cryptanalysis or black-bag cryptanalysis (e.g. installing some form of trojan horse or keystroke logging software/hardware on the target computer to capture encrypted keyrings and their passwords). The FBI has already used this attack against PGP in its investigations. However, any such vulnerabilities apply not just to PGP but to any conventional encryption software.

In 2003, an incident involving seized Psion PDAs belonging to members of the Red Brigade indicated that neither the Italian police nor the FBI were able to decrypt PGP-encrypted files stored on them.

A second incident in December 2006, (see "In re Boucher"), involving US customs agents who seized a laptop PC that allegedly contained child pornography, indicates that US government agencies find it "nearly impossible" to access PGP-encrypted files. Additionally, a magistrate judge ruling on the case in November 2007 has stated that forcing the suspect to reveal his PGP passphrase would violate his Fifth Amendment rights i.e. a suspect's constitutional right not to incriminate himself. The Fifth Amendment issue was opened again as the government appealed the case and a federal district judge ordered the defendant to provide the key.

Evidence suggests that as of 2007, British police investigators are unable to break PGP, so instead have resorted to using RIPA legislation to demand the passwords/keys. In November 2009 a British citizen was convicted under RIPA legislation and jailed for nine months for refusing to provide police investigators with encryption keys to PGP-encrypted files.

Phil Zimmermann created the first version of PGP encryption in 1991. The name, "Pretty Good Privacy" was inspired by the name of a grocery store, "Ralph's Pretty Good Grocery", featured in radio host Garrison Keillor's fictional town, Lake Wobegon. This first version included a symmetric-key algorithm that Zimmermann had designed himself, named BassOmatic after a "Saturday Night Live" sketch. Zimmermann had been a long-time anti-nuclear activist, and created PGP encryption so that similarly inclined people might securely use BBSs and securely store messages and files. No license was required for its non-commercial use. There was not even a nominal charge, and the complete source code was included with all copies.

In a posting of June 5, 2001, entitled "PGP Marks 10th Anniversary", Zimmermann describes the circumstances surrounding his release of PGP:

PGP found its way onto the Internet and rapidly acquired a considerable following around the world. Users and supporters included dissidents in totalitarian countries (some affecting letters to Zimmermann have been published, some of which have been included in testimony before the US Congress), civil libertarians in other parts of the world (see Zimmermann's published testimony in various hearings), and the 'free communications' activists who called themselves cypherpunks (who provided both publicity and distribution); decades later, CryptoParty activists did much the same via Twitter.

Shortly after its release, PGP encryption found its way outside the United States, and in February 1993 Zimmermann became the formal target of a criminal investigation by the US Government for "munitions export without a license". Cryptosystems using keys larger than 40 bits were then considered munitions within the definition of the US export regulations; PGP has never used keys smaller than 128 bits, so it qualified at that time. Penalties for violation, if found guilty, were substantial. After several years, the investigation of Zimmermann was closed without filing criminal charges against him or anyone else.

Zimmermann challenged these regulations in an imaginative way. He published the entire source code of PGP in a hardback book, via MIT Press, which was distributed and sold widely. Anybody wishing to build their own copy of PGP could cut off the covers, separate the pages, and scan them using an OCR program (or conceivably enter it as a type-in program if OCR software was not available), creating a set of source code text files. One could then build the application using the freely available GNU Compiler Collection. PGP would thus be available anywhere in the world. The claimed principle was simple: export of "munitions"—guns, bombs, planes, and software—was (and remains) restricted; but the export of "books" is protected by the First Amendment. The question was never tested in court with respect to PGP. In cases addressing other encryption software, however, two federal appeals courts have established the rule that cryptographic software source code is speech protected by the First Amendment (the Ninth Circuit Court of Appeals in the Bernstein case and the Sixth Circuit Court of Appeals in the Junger case).

US export regulations regarding cryptography remain in force, but were liberalized substantially throughout the late 1990s. Since 2000, compliance with the regulations is also much easier. PGP encryption no longer meets the definition of a non-exportable weapon, and can be exported internationally except to seven specific countries and a list of named groups and individuals (with whom substantially all US trade is prohibited under various US export controls).

During this turmoil, Zimmermann's team worked on a new version of PGP encryption called PGP 3. This new version was to have considerable security improvements, including a new certificate structure which fixed small security flaws in the PGP 2.x certificates as well as permitting a certificate to include separate keys for signing and encryption. Furthermore, the experience with patent and export problems led them to eschew patents entirely. PGP 3 introduced use of the CAST-128 (a.k.a. CAST5) symmetric key algorithm, and the DSA and ElGamal asymmetric key algorithms, all of which were unencumbered by patents.

After the Federal criminal investigation ended in 1996, Zimmermann and his team started a company to produce new versions of PGP encryption. They merged with Viacrypt (to whom Zimmermann had sold commercial rights and who had licensed RSA directly from RSADSI), which then changed its name to PGP Incorporated. The newly combined Viacrypt/PGP team started work on new versions of PGP encryption based on the PGP 3 system. Unlike PGP 2, which was an exclusively command line program, PGP 3 was designed from the start as a software library allowing users to work from a command line or inside a GUI environment. The original agreement between Viacrypt and the Zimmermann team had been that Viacrypt would have even-numbered versions and Zimmermann odd-numbered versions. Viacrypt, thus, created a new version (based on PGP 2) that they called PGP 4. To remove confusion about how it could be that PGP 3 was the successor to PGP 4, PGP 3 was renamed and released as PGP 5 in May 1997.

In December 1997, PGP Inc. was acquired by Network Associates, Inc. ("NAI"). Zimmermann and the PGP team became NAI employees. NAI was the first company to have a legal export strategy by publishing source code. Under NAI, the PGP team added disk encryption, desktop firewalls, intrusion detection, and IPsec VPNs to the PGP family. After the export regulation liberalizations of 2000 which no longer required publishing of source, NAI stopped releasing source code.

In early 2001, Zimmermann left NAI. He served as Chief Cryptographer for Hush Communications, who provide an OpenPGP-based e-mail service, Hushmail. He has also worked with Veridis and other companies. In October 2001, NAI announced that its PGP assets were for sale and that it was suspending further development of PGP encryption. The only remaining asset kept was the PGP E-Business Server (the original PGP Commandline version). In February 2002, NAI canceled all support for PGP products, with the exception of the renamed commandline product. NAI (formerly McAfee, then Intel Security, and now McAfee again) continued to sell and support the product under the name McAfee E-Business Server until 2013.

In August 2002, several ex-PGP team members formed a new company, PGP Corporation, and bought the PGP assets (except for the command line version) from NAI. The new company was funded by Rob Theis of Doll Capital Management (DCM) and Terry Garnett of Venrock Associates. PGP Corporation supports existing PGP users and honors NAI's support contracts. Zimmermann now serves as a special advisor and consultant to PGP Corporation, as well as continuing to run his own consulting company. In 2003, PGP Corporation created a new server-based product called PGP Universal. In mid-2004, PGP Corporation shipped its own command line version called PGP Command Line, which integrates with the other PGP Encryption Platform applications. In 2005, PGP Corporation made its first acquisition—the German software company Glück & Kanja Technology AG, which is now PGP Deutschland AG. In 2010, PGP Corporation acquired Hamburg-based certificate authority TC TrustCenter and its parent company, ChosenSecurity, to form its PGP TrustCenter division.

Since the 2002 purchase of NAI's PGP assets, PGP Corporation has offered worldwide PGP technical support from its offices in Draper, Utah; Offenbach, Germany; and Tokyo, Japan.

On April 29, 2010, Symantec Corp. announced that it would acquire PGP for $300 million with the intent of integrating it into its Enterprise Security Group. This acquisition was finalized and announced to the public on June 7, 2010. The source code of PGP Desktop 10 is available for peer review.

Also in 2010, Intel Corporation acquired McAfee. In 2013, the McAfee E-Business Server was transferred to Software Diversified Services, which now sells, supports, and develops it under the name SDS E-Business Server.

For the enterprise, Townsend Security currently offers the only commercial versions of PGP for the IBM i and IBM z mainframe platforms. Townsend Security partnered with Network Associates in 2000 to create a compatible version of PGP for the IBM i platform. Townsend Security again ported PGP in 2008, this time to the IBM z mainframe. This version of PGP relies on free z/OS encryption facility, which utilizes hardware acceleration.

In May 2018, a bug in certain implementations of PGP named EFAIL was discovered which could reveal the plaintext contents of emails encrypted with it.

While originally used primarily for encrypting the contents of e-mail messages and attachments from a desktop client, PGP products have been diversified since 2002 into a set of encryption applications which can be managed by an optional central policy server. PGP encryption applications include e-mail and attachments, digital signatures, laptop full disk encryption, file and folder security, protection for IM sessions, batch file transfer encryption, and protection for files and folders stored on network servers and, more recently, encrypted or signed HTTP request/responses by means of a client-side (Enigform) and a server-side (mod openpgp) module. There is also a Wordpress plugin available, called wp-enigform-authentication, that takes advantage of the session management features of Enigform with mod_openpgp.

The PGP Desktop 9.x family includes PGP Desktop Email, PGP Whole Disk Encryption, and PGP NetShare. Additionally, a number of Desktop bundles are also available. Depending on application, the products feature desktop e-mail, digital signatures, IM security, whole disk encryption, file and folder security, encrypted self-extracting archives, and secure shredding of deleted files. Capabilities are licensed in different ways depending on features required.

The PGP Universal Server 2.x management console handles centralized deployment, security policy, policy enforcement, key management, and reporting. It is used for automated e-mail encryption in the gateway and manages PGP Desktop 9.x clients. In addition to its local keyserver, PGP Universal Server works with the PGP public keyserver—called the PGP Global Directory—to find recipient keys. It has the capability of delivering e-mail securely when no recipient key is found via a secure HTTPS browser session.

With PGP Desktop 9.x managed by PGP Universal Server 2.x, first released in 2005, all PGP encryption applications are based on a new proxy-based architecture. These newer versions of PGP software eliminate the use of e-mail plug-ins and insulate the user from changes to other desktop applications. All desktop and server operations are now based on security policies and operate in an automated fashion. The PGP Universal server automates the creation, management, and expiration of keys, sharing these keys among all PGP encryption applications.

The Symantec PGP platform has now undergone a rename. PGP Desktop is now known as Symantec Encryption Desktop, and the PGP Universal Server is now known as Symantec Encryption Management Server. The current shipping versions are Symantec Encryption Desktop 10.3.0 (Windows and Mac OS platforms) and Symantec Encryption Server 3.3.2.

Also available are PGP Command Line, which enables command line-based encryption and signing of information for storage, transfer, and backup, as well as the PGP Support Package for BlackBerry which enables RIM BlackBerry devices to enjoy sender-to-recipient messaging encryption.

New versions of PGP applications use both OpenPGP and the S/MIME, allowing communications with any user of a NIST specified standard.

Inside PGP Inc., there was still concern about patent issues. RSADSI was challenging the continuation of the Viacrypt RSA license to the newly merged firm. The company adopted an informal internal standard they called "Unencumbered PGP" which would "use no algorithm with licensing difficulties". Because of PGP encryption's importance worldwide, many wanted to write their own software that would interoperate with PGP 5. Zimmermann became convinced that an open standard for PGP encryption was critical for them and for the cryptographic community as a whole. In July 1997, PGP Inc. proposed to the IETF that there be a standard called OpenPGP. They gave the IETF permission to use the name OpenPGP to describe this new standard as well as any program that supported the standard. The IETF accepted the proposal and started the OpenPGP Working Group.

OpenPGP is on the Internet Standards Track and is under active development. Many e-mail clients provide OpenPGP-compliant email security as described in RFC 3156. The current specification is RFC 4880 (November 2007), the successor to RFC 2440. RFC 4880 specifies a suite of required algorithms consisting of ElGamal encryption, DSA, Triple DES and SHA-1. In addition to these algorithms, the standard recommends RSA as described in PKCS #1 v1.5 for encryption and signing, as well as AES-128, CAST-128 and IDEA. Beyond these, many other algorithms are supported. The standard was extended to support Camellia cipher by RFC 5581 in 2009, and signing and key exchange based on Elliptic Curve Cryptography (ECC) (i.e. ECDSA and ECDH) by RFC 6637 in 2012. Support for ECC encryption was added by the proposed RFC 4880bis in 2014.

The Free Software Foundation has developed its own OpenPGP-compliant program called GNU Privacy Guard (abbreviated GnuPG or GPG). GnuPG is freely available together with all source code under the GNU General Public License (GPL) and is maintained separately from several Graphical User Interfaces (GUIs) that interact with the GnuPG library for encryption, decryption and signing functions (see KGPG, Seahorse, MacGPG). Several other vendors have also developed OpenPGP-compliant software.

The development of an opensource OpenPGP-compliant library, OpenPGPjs, written in JavaScript has allowed web based applications to use PGP encryption in the web browser.

There are several iOS and Android OpenPGP-compliant applications such as iPGMail for iOS and OpenKeychain for Android, which enable key generation and encryption/decryption of email and files on Apple's iOS and Google's Android.


OpenPGP's encryption can ensure secure delivery of files and messages, as well as provide verification of who created or sent the message using a process called digital signing. The open source office suite LibreOffice implemented document signing with OpenPGP as of version 5.4.0 on Linux. Using OpenPGP for communication requires participation by both the sender and recipient. OpenPGP can also be used to secure sensitive files when they're stored in vulnerable places like mobile devices or in the cloud.

With the advancement of cryptography, parts of PGP have been criticized for being dated:


In October 2017, the ROCA vulnerability was announced that affects RSA keys generated by buggy Infineon firmware used on Yubikey 4 tokens, often used with PGP. Many published PGP keys were found to be susceptible. Yubico offers free replacement of affected tokens.




</doc>
<doc id="23083" url="https://en.wikipedia.org/wiki?curid=23083" title="Playing card">
Playing card

A playing card is a piece of specially prepared heavy paper, thin cardboard, plastic-coated paper, cotton-paper blend, or thin plastic, marked with distinguishing motifs and used as one of a set for playing card games. Playing cards are typically palm-sized for convenient handling, and were first invented in China during the Tang dynasty.

Playing cards may have been invented during the Tang dynasty around the 9th century AD as a result of the usage of woodblock printing technology. The first possible reference to card games comes from a 9th-century text known as the "Collection of Miscellanea at Duyang", written by Tang dynasty writer Su E. It describes Princess Tongchang, daughter of Emperor Yizong of Tang, playing the "leaf game" in 868 with members of the clan of Wei Baoheng, the family of the princess' husband. The first known book on the "leaf" game was called the "Yezi Gexi" and allegedly written by a Tang woman. It received commentary by writers of subsequent dynasties. The Song dynasty (960–1279) scholar Ouyang Xiu (1007–1072) asserts that the "leaf" game existed at least since the mid-Tang dynasty and associated its invention with the development of printed sheets as a writing medium. However, Ouyang also claims that the "leaves" were pages of a book used in a board game played with dice, and that the rules of the game were lost by 1067.

Other games revolving around alcoholic drinking involved using playing cards of a sort from the Tang dynasty onward. However, these cards did not contain suits or numbers. Instead, they were printed with instructions or forfeits for whomever drew them.

The earliest dated instance of a game involving cards with suits and numerals occurred on 17 July 1294 when "Yan Sengzhu and Zheng Pig-Dog were caught playing cards [zhi pai] and that wood blocks for printing them had been impounded, together with nine of the actual cards."

William Henry Wilkinson suggests that the first cards may have been actual paper currency which doubled as both the tools of gaming and the stakes being played for, similar to trading card games. Using paper money was inconvenient and risky so they were substituted by play money known as "money cards". One of the earliest games in which we know the rules is Madiao, a trick-taking game, which dates to the Ming Dynasty (1368–1644). Fifteenth century scholar Lu Rong described it is as being played with 38 "money cards" divided into four suits: 9 in coins, 9 in strings of coins (which may have been misinterpreted as sticks from crude drawings), 9 in myriads (of coins or of strings), and 11 in tens of myriads (a myriad is 10,000). The two latter suits had "Water Margin" characters instead of pips on them with Chinese characters to mark their rank and suit. The suit of coins is in reverse order with 9 of coins being the lowest going up to 1 of coins as the high card.

Despite the wide variety of patterns, the suits show a uniformity of structure. Every suit contains twelve cards with the top two usually being the court cards of king and vizier and the bottom ten being pip cards. Half the suits use reverse ranking for their pip cards. There are many motifs for the suit pips but some include coins, clubs, jugs, and swords which resemble later Mamluk and Latin suits. Michael Dummett speculated that Mamluk cards may have descended from an earlier deck which consisted of 48 cards divided into four suits each with ten pip cards and two court cards.

By the 11th century, playing cards were spreading throughout the Asian continent and later came into Egypt. The oldest surviving cards in the world are four fragments found in the Keir Collection and one in the Benaki Museum. They are dated to the 12th and 13th centuries (late Fatimid, Ayyubid, and early Mamluk periods).

A near complete pack of Mamluk playing cards dating to the 15th century and of similar appearance to the fragments above was discovered by Leo Aryeh Mayer in the Topkapı Palace, Istanbul, in 1939. It is not a complete set and is actually composed of three different packs, probably to replace missing cards. The Topkapı pack originally contained 52 cards comprising four suits: polo-sticks, coins, swords, and cups. Each suit contained ten pip cards and three court cards, called "malik" (king), "nā'ib malik" (viceroy or deputy king), and "thānī nā'ib" (second or under-deputy). The "thānī nā'ib" is a non-existent title so it may not have been in the earliest versions; without this rank, the Mamluk suits would structurally be the same as a Ganjifa suit. In fact, the word "Kanjifah" appears in Arabic on the king of swords and is still used in parts of the Middle East to describe modern playing cards. Influence from further east can explain why the Mamluks, most of whom were Central Asian Turkic Kipchaks, called their cups "tuman" which means myriad in Turkic, Mongolian and Jurchen languages. Wilkinson postulated that the cups may have been derived from inverting the Chinese and Jurchen ideogram for myriad ().

The Mamluk court cards showed abstract designs or calligraphy not depicting persons possibly due to religious proscription in Sunni Islam, though they did bear the ranks on the cards. "Nā'ib" would be borrowed into French ("nahipi"), Italian ("naibi"), and Spanish ("naipes"), the latter word still in common usage. Panels on the pip cards in two suits show they had a reverse ranking, a feature found in Madiao, Ganjifa, and old European card games like Ombre, Tarot, and Maw.

A fragment of two uncut sheets of Moorish-styled cards of a similar but plainer style were found in Spain and dated to the early 15th century.

Export of these cards (from Cairo, Alexandria, and Damascus), ceased after the fall of the Mamluks in the sixteenth century. The rules to play these games are lost but they are believed to be plain trick games without trumps.

Four-suited playing cards are first attested in Southern Europe in 1365, and are likely derived from the Mamluk suits of cups, coins, swords, and polo-sticks, which are still used in traditional Latin decks. As polo was an obscure sport to Europeans then, the polo-sticks became batons or cudgels. Their presence is attested in Catalonia in 1371, 1377 in Switzerland, and 1380 in many locations including Florence and Paris. Wide use of playing cards in Europe can, with some certainty, be traced from 1377 onward.

In the account books of Johanna, Duchess of Brabant and Wenceslaus I, Duke of Luxembourg, an entry dated May 14, 1379 reads: "Given to Monsieur and Madame four peters, two forms, value eight and a half moutons, wherewith to buy a pack of cards". In his book of accounts for 1392 or 1393, Charles or Charbot Poupart, treasurer of the household of Charles VI of France, records payment for the painting of three sets of cards.

From about 1418 to 1450 professional card makers in Ulm, Nuremberg, and Augsburg created printed decks. Playing cards even competed with devotional images as the most common uses for woodcuts in this period. Most early woodcuts of all types were coloured after printing, either by hand or, from about 1450 onwards, stencils. These 15th-century playing cards were probably painted. The Flemish Hunting Deck, held by the Metropolitan Museum of Art is the oldest complete set of ordinary playing cards made in Europe from the fifteenth century.

As cards spread from Italy to Germanic countries, the Latin suits were replaced with the suits of Leaves (or Shields), Hearts (or Roses), Bells, and Acorns, and a combination of Latin and Germanic suit pictures and names resulted in the French suits of (clovers), (tiles), (hearts), and (pikes) around 1480. The "trèfle" (clover) was probably derived from the acorn and the (pike) from the leaf of the German suits. The names and "spade", however, may have derived from the sword () of the Italian suits. In England, the French suits were eventually used, although the earliest packs circulating may have had Latin suits. This may account for why the English called the clovers "clubs" and the pikes "spades".

In the late 14th century, Europeans changed the Mamluk court cards to represent European royalty and attendants. In a description from 1377, the earliest courts were originally a seated "King", an upper marshal that held his suit symbol up, and a lower marshal that held it down. The latter two correspond with the Ober and Unter cards found in German and Swiss playing cards. The Italians and Iberians replaced the / system with the "Knight" and "" or "" before 1390, perhaps to make the cards more visually distinguishable. In England, the lowest court card was called the "Knave" which originally meant "male child" (compare German ), so in this context the character could represent the "prince", son to the King and Queen; the meaning "servant" developed later. Queens appeared sporadically in packs as early as 1377, especially in Germany. Although the Germans abandoned the Queen before the 1500s, the French permanently picked it up and placed it under the King. Packs of 56 cards containing in each suit a King, Queen, Knight, and Knave (as in tarot) were once common in the 15th century.

During the mid 16th century, Portuguese traders introduced playing cards to Japan. The first indigenous Japanese deck was the named after the period.

Packs with corner and edge indices (i.e. the value of the card printed at the corner(s) of the card) enabled players to hold their cards close together in a fan with one hand (instead of the two hands previously used). The first such pack known with Latin suits was printed by Infirerra and dated 1693, but this feature was commonly used only from the end of the 18th century. The first Anglo-American deck with this innovation was the Saladee's Patent, printed by Samuel Hart in 1864. In 1870, he and his cousins at Lawrence & Cohen followed up with the Squeezers, the first cards with indices that had a large diffusion.

This was followed by the innovation of reversible court cards. This invention is attributed to a French card maker of Agen in 1745. But the French government, which controlled the design of playing cards, prohibited the printing of cards with this innovation. In central Europe (Trappola cards) and Italy (Tarocco Bolognese) the innovation was adopted during the second half of the 18th century. In Great Britain, the pack with reversible court cards was patented in 1799 by Edmund Ludlow and Ann Wilcox. The Anglo-American pack with this design was printed around 1802 by Thomas Wheeler.

Sharp corners wear out more quickly, and could possibly reveal the card's value, so they were replaced with rounded corners. Before the mid-19th century, British, American, and French players preferred blank backs. The need to hide wear and tear and to discourage writing on the back led cards to have designs, pictures, photos, or advertising on the reverse.

The United States introduced the Joker into the deck. It was devised for the game of Euchre, which spread from Europe to America beginning shortly after the American Revolutionary War. In Euchre, the highest trump card is the Jack of the trump suit, called the "right bower" (from the German ""); the second-highest trump, the "left bower", is the Jack of the suit of the same color as trumps. The joker was invented c. 1860 as a third trump, the "imperial" or "best bower", which ranked higher than the other two "bowers". The name of the card is believed to derive from "juker", a variant name for Euchre. The earliest reference to a Joker functioning as a wild card dates to 1875 with a variation of poker.

Contemporary playing cards are grouped into three broad categories based on the suits they use: French, Latin, and Germanic. Latin suits are used in the closely related Spanish and Italian formats. The Swiss-German suits are distinct enough to merit their subcategory. Excluding Jokers and Tarot trumps, the French 52-card deck preserves the number of cards in the original Mamluk deck, while Latin and Germanic decks average fewer. Latin decks usually drop the higher-valued pip cards, while Germanic decks drop the lower-valued ones.

Within suits, there are regional or national variations called "standard patterns." Because these patterns are in the public domain, this allows multiple card manufacturers to recreate them. Pattern differences are most easily found in the face cards but the number of cards per deck, the use of numeric indices, or even minor shape and arrangement differences of the pips can be used to distinguish them. Some patterns have been around for hundreds of years. Jokers are not part of any pattern as they are a relatively recent invention and lack any standardized appearance so each publisher usually puts their own trademarked illustration into their decks. The wide variation of jokers has turned them into collectible items. Any card that bore the stamp duty like the ace of spades in England or the ace of clubs in France are also collectible as that is where the manufacturer's logo is usually placed.

French decks come in a variety of patterns and deck sizes. The 52-card deck is the most popular deck and includes 13 ranks of each suit with reversible "court" or face cards. Each suit includes an Ace, depicting a single symbol of its suit, a King, Queen, and Jack, each depicted with a symbol of their suit; and ranks two through ten, with each card depicting that number of pips of its suit. As well as these 52 cards, commercial packs often include between one and six jokers, most often two.

Decks with less than 52 cards are known as stripped decks. The piquet pack has all values from 2 through 6 in each suit removed for a total of 32 cards. It is popular in France, the Low Countries, Central Europe and Russia and is used to play Piquet, Belote, Bezique and Skat. Forty-card French suited packs are common in northwest Italy; these remove the 8s through 10s like Latin suited decks. 24 card decks, removing 2s through 8s are also sold in Austria and Bavaria to play Schnapsen.

A pinochle deck consists of two copies of each of the 9, 10, jack, queen, king, and ace cards of all four suits. It thus comprises just 48 cards per deck.

The 78 card Tarot Nouveau adds the Knight card between Queens and Jacks along with 21 numbered trumps and the unnumbered Fool.

The Unicode standard for text encoding on computers defines 8 characters for card suits in the Miscellaneous Symbols block, at U+2660–2667. Unicode 7.0 added a unified pack for French-suited Tarot Nouveau's trump cards and the 52 cards of the modern French pack, with 4 Knights, together with a character for "Playing Card Back" and black, red, and white jokers in the block U+1F0A0–1F0FF.

Geographic origin:

Types of decks:

Specific decks:

Uses:

Terminology:

Sources for further information:


Playing card societies (collectors and researchers)

History of playing cards
Playing card iconography
Museums, Institutes and Organisations
Playing card collections online
Manufacturers
Designing Playing cards


</doc>
<doc id="23084" url="https://en.wikipedia.org/wiki?curid=23084" title="Paleontology">
Paleontology

Paleontology or palaeontology () is the scientific study of life that existed prior to, and sometimes including, the start of the Holocene Epoch (roughly 11,700 years before present). It includes the study of fossils to determine organisms' evolution and interactions with each other and their environments (their paleoecology). Paleontological observations have been documented as far back as the 5th century BC. The science became established in the 18th century as a result of Georges Cuvier's work on comparative anatomy, and developed rapidly in the 19th century. The term itself originates from Greek παλαιός, "palaios", "old, ancient", ὄν, "on" (gen. "ontos"), "being, creature" and λόγος, "logos", "speech, thought, study".

Paleontology lies on the border between biology and geology, but differs from archaeology in that it excludes the study of anatomically modern humans. It now uses techniques drawn from a wide range of sciences, including biochemistry, mathematics, and engineering. Use of all these techniques has enabled paleontologists to discover much of the evolutionary history of life, almost all the way back to when Earth became capable of supporting life, about 3.8 billion years ago. As knowledge has increased, paleontology has developed specialised sub-divisions, some of which focus on different types of fossil organisms while others study ecology and environmental history, such as ancient climates.

Body fossils and trace fossils are the principal types of evidence about ancient life, and geochemical evidence has helped to decipher the evolution of life before there were organisms large enough to leave body fossils. Estimating the dates of these remains is essential but difficult: sometimes adjacent rock layers allow radiometric dating, which provides absolute dates that are accurate to within 0.5%, but more often paleontologists have to rely on relative dating by solving the "jigsaw puzzles" of biostratigraphy. Classifying ancient organisms is also difficult, as many do not fit well into the Linnaean taxonomy that is commonly used for classifying living organisms, and paleontologists more often use cladistics to draw up evolutionary "family trees". The final quarter of the 20th century saw the development of molecular phylogenetics, which investigates how closely organisms are related by measuring how similar the DNA is in their genomes. Molecular phylogenetics has also been used to estimate the dates when species diverged, but there is controversy about the reliability of the molecular clock on which such estimates depend.

The simplest definition is "the study of ancient life". Paleontology seeks information about several aspects of past organisms: "their identity and origin, their environment and evolution, and what they can tell us about the Earth's organic and inorganic past".

Paleontology is one of the historical sciences, along with archaeology, geology, astronomy, cosmology, philology and history itself. This means that it aims to describe phenomena of the past and reconstruct their causes. Hence it has three main elements: description of the phenomena; developing a general theory about the causes of various types of change; and applying those theories to specific facts.
When trying to explain past phenomena, paleontologists and other historical scientists often construct a set of hypotheses about the causes and then look for a "smoking gun", a piece of evidence that indicates that one hypothesis is a better explanation than others. Sometimes the smoking gun is discovered by a fortunate accident during other research. For example, the discovery by Luis Alvarez and Walter Alvarez of an iridium-rich layer at the Cretaceous–Tertiary boundary made asteroid impact and volcanism the most favored explanations for the Cretaceous–Paleogene extinction event.

The other main type of science is experimental science, which is often said to work by conducting experiments to "disprove" hypotheses about the workings and causes of natural phenomena – note that this approach cannot confirm a hypothesis is correct, since some later experiment may disprove it. However, when confronted with totally unexpected phenomena, such as the first evidence for invisible radiation, experimental scientists often use the same approach as historical scientists: construct a set of hypotheses about the causes and then look for a "smoking gun".

Paleontology lies on the boundary between biology and geology since paleontology focuses on the record of past life but its main source of evidence is fossils, which are found in rocks. For historical reasons paleontology is part of the geology departments of many universities, because in the 19th century and early 20th century geology departments found paleontological evidence important for estimating the ages of rocks while biology departments showed little interest.

Paleontology also has some overlap with archaeology, which primarily works with objects made by humans and with human remains, while paleontologists are interested in the characteristics and evolution of humans as organisms. When dealing with evidence about humans, archaeologists and paleontologists may work together – for example paleontologists might identify animal or plant fossils around an archaeological site, to discover what the people who lived there ate; or they might analyze the climate at the time when the site was inhabited by humans.

In addition paleontology often uses techniques derived from other sciences, including biology, osteology, ecology, chemistry, physics and mathematics. For example, geochemical signatures from rocks may help to discover when life first arose on Earth, and analyses of carbon isotope ratios may help to identify climate changes and even to explain major transitions such as the Permian–Triassic extinction event. A relatively recent discipline, molecular phylogenetics, often helps by using comparisons of different modern organisms' DNA and RNA to re-construct evolutionary "family trees"; it has also been used to estimate the dates of important evolutionary developments, although this approach is controversial because of doubts about the reliability of the "molecular clock". Techniques developed in engineering have been used to analyse how ancient organisms might have worked, for example how fast "Tyrannosaurus" could move and how powerful its bite was. It is relatively commonplace to study fossils using X-ray microtomography A combination of paleontology, biology, and archaeology, paleoneurobiology is the study of endocranial casts (or endocasts) of species related to humans to learn about the evolution of human brains.

Paleontology even contributes to astrobiology, the investigation of possible life on other planets, by developing models of how life may have arisen and by providing techniques for detecting evidence of life.

As knowledge has increased, paleontology has developed specialised subdivisions. Vertebrate paleontology concentrates on fossils of vertebrates, from the earliest fish to the immediate ancestors of modern mammals. Invertebrate paleontology deals with fossils of invertebrates such as molluscs, arthropods, annelid worms and echinoderms. Paleobotany focuses on the study of fossil plants, but traditionally includes the study of fossil algae and fungi. Palynology, the study of pollen and spores produced by land plants and protists, straddles the border between paleontology and botany, as it deals with both living and fossil organisms. Micropaleontology deals with all microscopic fossil organisms, regardless of the group to which they belong.

Instead of focusing on individual organisms, paleoecology examines the interactions between different organisms, such as their places in food chains, and the two-way interaction between organisms and their environment.  One example is the development of oxygenic photosynthesis by bacteria, which hugely increased the productivity and diversity of ecosystems. This also caused the oxygenation of the atmosphere. Together, these were a prerequisite for the evolution of the most complex eukaryotic cells, from which all multicellular organisms are built.

Paleoclimatology, although sometimes treated as part of paleoecology, focuses more on the history of Earth's climate and the mechanisms that have changed it – which have sometimes included evolutionary developments, for example the rapid expansion of land plants in the Devonian period removed more carbon dioxide from the atmosphere, reducing the greenhouse effect and thus helping to cause an ice age in the Carboniferous period.

Biostratigraphy, the use of fossils to work out the chronological order in which rocks were formed, is useful to both paleontologists and geologists. Biogeography studies the spatial distribution of organisms, and is also linked to geology, which explains how Earth's geography has changed over time.

Fossils of organisms' bodies are usually the most informative type of evidence. The most common types are wood, bones, and shells. Fossilisation is a rare event, and most fossils are destroyed by erosion or metamorphism before they can be observed. Hence the fossil record is very incomplete, increasingly so further back in time. Despite this, it is often adequate to illustrate the broader patterns of life's history. There are also biases in the fossil record: different environments are more favorable to the preservation of different types of organism or parts of organisms. Further, only the parts of organisms that were already mineralised are usually preserved, such as the shells of molluscs. Since most animal species are soft-bodied, they decay before they can become fossilised. As a result, although there are 30-plus phyla of living animals, two-thirds have never been found as fossils.

Occasionally, unusual environments may preserve soft tissues. These lagerstätten allow paleontologists to examine the internal anatomy of animals that in other sediments are represented only by shells, spines, claws, etc. – if they are preserved at all. However, even lagerstätten present an incomplete picture of life at the time. The majority of organisms living at the time are probably not represented because lagerstätten are restricted to a narrow range of environments, e.g. where soft-bodied organisms can be preserved very quickly by events such as mudslides; and the exceptional events that cause quick burial make it difficult to study the normal environments of the animals. The sparseness of the fossil record means that organisms are expected to exist long before and after they are found in the fossil record – this is known as the Signor-Lipps effect.

Trace fossils consist mainly of tracks and burrows, but also include coprolites (fossil feces) and marks left by feeding. Trace fossils are particularly significant because they represent a data source that is not limited to animals with easily fossilised hard parts, and they reflect organisms' behaviours. Also many traces date from significantly earlier than the body fossils of animals that are thought to have been capable of making them. Whilst exact assignment of trace fossils to their makers is generally impossible, traces may for example provide the earliest physical evidence of the appearance of moderately complex animals (comparable to earthworms).

Geochemical observations may help to deduce the global level of biological activity at a certain period, or the affinity of certain fossils. For example, geochemical features of rocks may reveal when life first arose on Earth, and may provide evidence of the presence of eukaryotic cells, the type from which all multicellular organisms are built. Analyses of carbon isotope ratios may help to explain major transitions such as the Permian–Triassic extinction event.

Naming groups of organisms in a way that is clear and widely agreed is important, as some disputes in paleontology have been based just on misunderstandings over names. Linnaean taxonomy is commonly used for classifying living organisms, but runs into difficulties when dealing with newly discovered organisms that are significantly different from known ones. For example: it is hard to decide at what level to place a new higher-level grouping, e.g. genus or family or order; this is important since the Linnaean rules for naming groups are tied to their levels, and hence if a group is moved to a different level it must be renamed.

Paleontologists generally use approaches based on cladistics, a technique for working out the evolutionary "family tree" of a set of organisms. It works by the logic that, if groups B and C have more similarities to each other than either has to group A, then B and C are more closely related to each other than either is to A. Characters that are compared may be anatomical, such as the presence of a notochord, or molecular, by comparing sequences of DNA or proteins. The result of a successful analysis is a hierarchy of clades – groups that share a common ancestor. Ideally the "family tree" has only two branches leading from each node ("junction"), but sometimes there is too little information to achieve this and paleontologists have to make do with junctions that have several branches. The cladistic technique is sometimes fallible, as some features, such as wings or camera eyes, evolved more than once, convergently – this must be taken into account in analyses.

Evolutionary developmental biology, commonly abbreviated to "Evo Devo", also helps paleontologists to produce "family trees", and understand fossils. For example, the embryological development of some modern brachiopods suggests that brachiopods may be descendants of the halkieriids, which became extinct in the Cambrian period.
Paleontology seeks to map out how living things have changed through time. A substantial hurdle to this aim is the difficulty of working out how old fossils are. Beds that preserve fossils typically lack the radioactive elements needed for radiometric dating. This technique is our only means of giving rocks greater than about 50 million years old an absolute age, and can be accurate to within 0.5% or better. Although radiometric dating requires very careful laboratory work, its basic principle is simple: the rates at which various radioactive elements decay are known, and so the ratio of the radioactive element to the element into which it decays shows how long ago the radioactive element was incorporated into the rock. Radioactive elements are common only in rocks with a volcanic origin, and so the only fossil-bearing rocks that can be dated radiometrically are a few volcanic ash layers.

Consequently, paleontologists must usually rely on stratigraphy to date fossils. Stratigraphy is the science of deciphering the "layer-cake" that is the sedimentary record, and has been compared to a jigsaw puzzle. Rocks normally form relatively horizontal layers, with each layer younger than the one underneath it. If a fossil is found between two layers whose ages are known, the fossil's age must lie between the two known ages. Because rock sequences are not continuous, but may be broken up by faults or periods of erosion, it is very difficult to match up rock beds that are not directly next to one another. However, fossils of species that survived for a relatively short time can be used to link up isolated rocks: this technique is called "biostratigraphy". For instance, the conodont "Eoplacognathus pseudoplanus" has a short range in the Middle Ordovician period. If rocks of unknown age are found to have traces of "E. pseudoplanus", they must have a mid-Ordovician age. Such index fossils must be distinctive, be globally distributed and have a short time range to be useful. However, misleading results are produced if the index fossils turn out to have longer fossil ranges than first thought. Stratigraphy and biostratigraphy can in general provide only relative dating ("A" was before "B"), which is often sufficient for studying evolution. However, this is difficult for some time periods, because of the problems involved in matching up rocks of the same age across different continents.

Family-tree relationships may also help to narrow down the date when lineages first appeared. For instance, if fossils of B or C date to X million years ago and the calculated "family tree" says A was an ancestor of B and C, then A must have evolved more than X million years ago.

It is also possible to estimate how long ago two living clades diverged – i.e. approximately how long ago their last common ancestor must have lived – by assuming that DNA mutations accumulate at a constant rate. These "molecular clocks", however, are fallible, and provide only a very approximate timing: for example, they are not sufficiently precise and reliable for estimating when the groups that feature in the Cambrian explosion first evolved, and estimates produced by different techniques may vary by a factor of two.

The evolutionary history of life stretches back to over , possibly as far as . Earth formed about and, after a collision that formed the Moon about 40 million years later, may have cooled quickly enough to have oceans and an atmosphere about . However, there is evidence on the Moon of a Late Heavy Bombardment from . If, as seems likely, such a bombardment struck Earth at the same time, the first atmosphere and oceans may have been stripped away. The oldest clear evidence of life on Earth dates to , although there have been reports, often disputed, of fossil bacteria from and of geochemical evidence for the presence of life . Some scientists have proposed that life on Earth was "seeded" from elsewhere, but most research concentrates on various explanations of how life could have arisen independently on Earth.

For about 2,000 million years microbial mats, multi-layered colonies of different types of bacteria, were the dominant life on Earth. The evolution of oxygenic photosynthesis enabled them to play the major role in the oxygenation of the atmosphere from about . This change in the atmosphere increased their effectiveness as nurseries of evolution. While eukaryotes, cells with complex internal structures, may have been present earlier, their evolution speeded up when they acquired the ability to transform oxygen from a poison to a powerful source of energy in their metabolism. This innovation may have come from primitive eukaryotes capturing oxygen-powered bacteria as endosymbionts and transforming them into organelles called mitochondria. The earliest evidence of complex eukaryotes with organelles such as mitochondria, dates from .

Multicellular life is composed only of eukaryotic cells, and the earliest evidence for it is the Francevillian Group Fossils from , although specialisation of cells for different functions first appears between (a possible fungus) and (a probable red alga). Sexual reproduction may be a prerequisite for specialisation of cells, as an asexual multicellular organism might be at risk of being taken over by rogue cells that retain the ability to reproduce.

The earliest known animals are cnidarians from about , but these are so modern-looking that the earliest animals must have appeared before then. Early fossils of animals are rare because they did not develop mineralised hard parts that fossilise easily until about . The earliest modern-looking bilaterian animals appear in the Early Cambrian, along with several "weird wonders" that bear little obvious resemblance to any modern animals. There is a long-running debate about whether this Cambrian explosion was truly a very rapid period of evolutionary experimentation; alternative views are that modern-looking animals began evolving earlier but fossils of their precursors have not yet been found, or that the "weird wonders" are evolutionary "aunts" and "cousins" of modern groups. Vertebrates remained an obscure group until the first fish with jaws appeared in the Late Ordovician.

The spread of life from water to land required organisms to solve several problems, including protection against drying out and supporting themselves against gravity. The earliest evidence of land plants and land invertebrates date back to about and respectively. The lineage that produced land vertebrates evolved later but very rapidly between and ; recent discoveries have overturned earlier ideas about the history and driving forces behind their evolution. Land plants were so successful that they caused an ecological crisis in the Late Devonian, until the evolution and spread of fungi that could digest dead wood.
During the Permian period synapsids, including the ancestors of mammals, may have dominated land environments, but the Permian–Triassic extinction event came very close to wiping out complex life. The extinctions were apparently fairly sudden, at least among vertebrates. During the slow recovery from this catastrophe a previously obscure group, archosaurs, became the most abundant and diverse terrestrial vertebrates. One archosaur group, the dinosaurs, were the dominant land vertebrates for the rest of the Mesozoic, and birds evolved from one group of dinosaurs. During this time mammals' ancestors survived only as small, mainly nocturnal insectivores, but this apparent set-back may have accelerated the development of mammalian traits such as endothermy and hair. After the Cretaceous–Paleogene extinction event killed off the non-avian dinosaurs – birds are the only surviving dinosaurs – mammals increased rapidly in size and diversity, and some took to the air and the sea.
Fossil evidence indicates that flowering plants appeared and rapidly diversified in the Early Cretaceous, between and . Their rapid rise to dominance of terrestrial ecosystems is thought to have been propelled by coevolution with pollinating insects. Social insects appeared around the same time and, although they account for only small parts of the insect "family tree", now form over 50% of the total mass of all insects.

Humans evolved from a lineage of upright-walking apes whose earliest fossils date from over . Although early members of this lineage had chimp-sized brains, about 25% as big as modern humans', there are signs of a steady increase in brain size after about . There is a long-running debate about whether "modern" humans are descendants of a single small population in Africa, which then migrated all over the world less than 200,000 years ago and replaced previous hominine species, or arose worldwide at the same time as a result of interbreeding.
Life on earth has suffered occasional mass extinctions at least since . Although they are disasters at the time, mass extinctions have sometimes accelerated the evolution of life on earth. When dominance of particular ecological niches passes from one group of organisms to another, it is rarely because the new dominant group is "superior" to the old and usually because an extinction event eliminates the old dominant group and makes way for the new one.

The fossil record appears to show that the rate of extinction is slowing down, with both the gaps between mass extinctions becoming longer and the average and background rates of extinction decreasing. However, it is not certain whether the actual rate of extinction has altered, since both of these observations could be explained in several ways:



Biodiversity in the fossil record, which is
shows a different trend: a fairly swift rise from , a slight decline from , in which the devastating Permian–Triassic extinction event is an important factor, and a swift rise from to the present.

Although paleontology became established around 1800, earlier thinkers had noticed aspects of the fossil record. The ancient Greek philosopher Xenophanes (570–480 BC) concluded from fossil sea shells that some areas of land were once under water. During the Middle Ages the Persian naturalist Ibn Sina, known as "Avicenna" in Europe, discussed fossils and proposed a theory of petrifying fluids on which Albert of Saxony elaborated in the 14th century. The Chinese naturalist Shen Kuo (1031–1095) proposed a theory of climate change based on the presence of petrified bamboo in regions that in his time were too dry for bamboo.

In early modern Europe, the systematic study of fossils emerged as an integral part of the changes in natural philosophy that occurred during the Age of Reason. In the Italian Renaissance, Leonardo Da Vinci made various significant contributions to the field as well designed numerous fossils. At the end of the 18th century Georges Cuvier's work established comparative anatomy as a scientific discipline and, by proving that some fossil animals resembled no living ones, demonstrated that animals could become extinct, leading to the emergence of paleontology. The expanding knowledge of the fossil record also played an increasing role in the development of geology, particularly stratigraphy.

The first half of the 19th century saw geological and paleontological activity become increasingly well organised with the growth of geologic societies and museums and an increasing number of professional geologists and fossil specialists. Interest increased for reasons that were not purely scientific, as geology and paleontology helped industrialists to find and exploit natural resources such as coal.

This contributed to a rapid increase in knowledge about the history of life on Earth and to progress in the definition of the geologic time scale, largely based on fossil evidence. In 1822 Henri Marie Ducrotay de Blanville, editor of "Journal de Physique", coined the word "palaeontology" to refer to the study of ancient living organisms through fossils. As knowledge of life's history continued to improve, it became increasingly obvious that there had been some kind of successive order to the development of life. This encouraged early evolutionary theories on the transmutation of species.
After Charles Darwin published "Origin of Species" in 1859, much of the focus of paleontology shifted to understanding evolutionary paths, including human evolution, and evolutionary theory.
The last half of the 19th century saw a tremendous expansion in paleontological activity, especially in North America. The trend continued in the 20th century with additional regions of the Earth being opened to systematic fossil collection. Fossils found in China near the end of the 20th century have been particularly important as they have provided new information about the earliest evolution of animals, early fish, dinosaurs and the evolution of birds. The last few decades of the 20th century saw a renewed interest in mass extinctions and their role in the evolution of life on Earth. There was also a renewed interest in the Cambrian explosion that apparently saw the development of the body plans of most animal phyla. The discovery of fossils of the Ediacaran biota and developments in paleobiology extended knowledge about the history of life back far before the Cambrian.

Increasing awareness of Gregor Mendel's pioneering work in genetics led first to the development of population genetics and then in the mid-20th century to the modern evolutionary synthesis, which explains evolution as the outcome of events such as mutations and horizontal gene transfer, which provide genetic variation, with genetic drift and natural selection driving changes in this variation over time. Within the next few years the role and operation of DNA in genetic inheritance were discovered, leading to what is now known as the "Central Dogma" of molecular biology. In the 1960s molecular phylogenetics, the investigation of evolutionary "family trees" by techniques derived from biochemistry, began to make an impact, particularly when it was proposed that the human lineage had diverged from apes much more recently than was generally thought at the time. Although this early study compared proteins from apes and humans, most molecular phylogenetics research is now based on comparisons of RNA and DNA.




</doc>
<doc id="23085" url="https://en.wikipedia.org/wiki?curid=23085" title="Plotter">
Plotter

The plotter is a computer printer for printing vector graphics. In the past, plotters were used in applications such as computer-aided design, though they have generally been replaced with wide-format conventional printers. A plotter gives a hard copy of the output. It draws pictures on a paper using a pen. Plotters are used to print designs of ships and machines, plans for buildings and so on.

Digitally controlled plotters evolved from earlier fully analog XY-writers used as output devices for measurement instruments and analog computers.

Pen plotters print by moving a pen or other instrument across the surface of a piece of paper. This means that plotters are vector graphics devices, rather than raster graphics as with other printers. Pen plotters can draw complex line art, including text, but do so slowly because of the mechanical movement of the pens. They are often incapable of efficiently creating a solid region of color, but can hatch an area by drawing a number of close, regular lines.

Plotters offered the fastest way to efficiently produce very large drawings or color high-resolution vector-based artwork when computer memory was very expensive and processor power was very limited, and other types of printers had limited graphic output capabilities.

Pen plotters have essentially become obsolete, and have been replaced by large-format inkjet printers and LED toner based printers. Such devices may still understand vector languages originally designed for plotter use, because in many uses, they offer a more efficient alternative to raster data.

Electrostatic plotters used a dry toner transfer process similar to that in many photocopiers. They were faster than pen plotters and were available in large formats, suitable for reproducing engineering drawings. The quality of image was often not as good as contemporary pen plotters. Electrostatic plotters were made in both flat-bed and drum types.

Cutting plotters use knives to cut into a piece of material (such as paper, mylar or vinyl) that is lying on the flat surface area of the plotter. It is achieved because the cutting plotter is connected to a computer, which is equipped with specialized cutting design or drawing computer software programs. Those computer software programs are responsible for sending the necessary cutting dimensions or designs in order to command the cutting knife to produce the correct project cutting needs.

In recent years the use of cutting plotters (generally called die-cut machines) has become popular with home enthusiasts of paper crafts such as cardmaking and scrapbooking. Such tools allow desired card shapes to be cut out very precisely, and repeated perfectly identically.

A number of printer control languages were created to operate pen plotters, and transmit commands like "lift pen from paper", "place pen on paper", or "draw a line from here to here". Three common ASCII-based plotter control languages are Hewlett-Packard's HP-GL, its successor HP-GL/2 and Houston Instruments DMPL. Here is a simple HP-GL script drawing a line :

This program instructs the plotter, in order, to take the first pen (SP1 = Select Pen 1), to go to coordinates X=500, Y=500 on the paper sheet (PA = Plot Absolute), to lower the pen against the paper (PD = Pen Down), to move 1000 units in the Y direction (thus drawing a vertical line - PR = Plot Relative), to lift the pen (PU = Pen Up) and finally to put it back in its stall.

Programmers using FORTRAN or BASIC generally did not program these directly, but used software packages, such as the Calcomp library, or device independent graphics packages, such as Hewlett-Packard's AGL libraries or BASIC extensions or high end packages such as DISSPLA. These would establish scaling factors from world coordinates to device coordinates, and translate to the low level device commands. For example, to plot X*X in HP 9830 BASIC, the program would be 

Early pen plotters, e.g., the Calcomp 565 of 1959, worked by placing the paper over a roller that moved the paper back and forth for X motion, while the pen moved back and forth on a track for Y motion. The paper was supplied in roll form and had perforations along both edges that were engaged by sprockets on the rollers.

Another approach, e.g. Computervision's Interact I, involved attaching ball-point pens to drafting pantographs and driving the machines with stepper motors controlled by the computer. This had the disadvantage of being somewhat slow to move, as well as requiring floor space equal to the size of the paper, but could double as a digitizer. A later change was the addition of an electrically controlled clamp to hold the pens, which allowed them to be changed, and thus create multi-colored output.

Hewlett Packard and Tektronix produced small, desktop-sized flatbed plotters in the late 1960s and 1970s. The pens were mounted on a traveling bar, whereby the y-axis was represented by motion up and down the length of the bar and the x-axis was represented by motion of the bar back and forth across the plotting table. Due to the mass of the bar, these plotters operated relatively slowly.

In the 1980s, the small and lightweight HP 7470 introduced the "grit wheel" mechanism, eliminating the need for perforations along the edges, unlike the Calcomp plotters two decades earlier. The grit wheels at opposite edges of the sheet press against resilient polyurethane-coated rollers and form tiny indentations in the sheet. As the sheet is moved back and forth, the grit wheels keep the sheet in proper registration due to the grit particles falling into the earlier indentations, much like the teeth of two gears meshing. The pen is mounted on a carriage that moves back and forth in a line between the grit wheels, representing the orthogonal axis. These smaller "home-use" plotters became popular for desktop business graphics and in engineering laboratories, but their low speed meant they were not useful for general printing purposes, and different conventional printer would be required for those jobs. One category, introduced by Hewlett Packard's MultiPlot for the HP 2647, was the "word chart", which used the plotter to draw large letters on a transparency. This was the forerunner of the modern Powerpoint chart. With the widespread availability of high-resolution inkjet and laser printers, inexpensive memory and computers fast enough to rasterize color images, pen plotters have all but disappeared. However, the grit wheel mechanism is still found in inkjet-based, large format engineering plotters.
Plotters were also used in the Create-A-Card kiosks that were available for a while in the greeting card area of supermarkets that used the HP 7475 six-pen plotter.

Plotters are used primarily in technical drawing and CAD applications, where they have the advantage of working on very large paper sizes while maintaining high resolution. Another use has been found by replacing the pen with a cutter, and in this form plotters can be found in many garment and sign shops.

Changing the color or width of a line required the plotter to change pens. This was either done manually on small plotters, but more typically the plotter would have a magazine of four or more pens which could be automatically mounted.

A niche application of plotters is in creating tactile images for visually handicapped people on special thermal cell paper.

Unlike other printer types, pen plotter speed is measured by pen speed and acceleration rate, instead of by page printing speed. A pen plotter's speed is primarily limited by the type of pen used, so the choice of pen is a key factor in pen plotter output speed. Indeed, most modern pen plotters have commands to control slewing speed, depending on the type of pen currently in use.

There are many types of plotter pen, some of which are no longer mass-produced. Technical pen tips are often used, many of which can be renewed using parts and supplies for manual drafting pens. Early HP flatbed and grit wheel plotters used small, proprietary fiber-tipped or plastic nib disposable pens.

One type of plotter pen uses a cellulose fiber rod inserted through a circular foam tube saturated with ink, with the end of the rod sharpened into a conical tip. As the pen moves across the paper surface, capillary wicking draws the ink from the foam, down the rod, and onto the paper. As the ink supply in the foam is depleted, the migration of ink to the tip begins to slow down, resulting in faint lines. Slowing the plotting speed will allow the lines drawn by a worn-out pen to remain dark, but the fading will continue until the foam is completely depleted. Also, as the fiber tip pen is used, the tip slowly wears away on the plotting medium, producing a progressively wider, smudged line.

Ball-point plotter pens with refillable clear plastic ink reservoirs are available. They do not have the fading or wear effects of fiber pens, but are generally more expensive and uncommon. Also, conventional ball-point pens can be modified to work in most pen plotters.

A vinyl cutter (sometimes known as a cutting plotter) is used to create posters, billboards, signs, T-shirt logos, and other weather-resistant graphical designs. The vinyl can also be applied to car bodies and windows for large, bright company advertising and to sailboat transoms. A similar process is used to cut tinted vinyl for automotive windows.

Colors are limited by the collection of vinyl on hand. To prevent creasing of the material, it is stored in rolls. Typical vinyl roll sizes are 15-inch, 24-inch, 36-inch and 48-inch widths, and have a backing material for maintaining the relative placement of all design elements.

Vinyl cutter hardware is similar to a traditional plotter except that the ink pen is replaced by a very sharp knife to outline each shape, and may have a pressure control to adjust how hard the knife presses down into the vinyl film, preventing the cuts from also penetrating the backing material. Besides losing relative placement of separate design elements, loose pieces cut out of the backing material may fall out and jam the plotter roll feed or the cutter head. After cutting, the vinyl material outside of the design is peeled away, leaving the design on the backing material which can be applied using self-adhesion, glue, lamination, or a heat press.

The vinyl knife is usually shaped like a plotter pen and is also mounted on a swivel head so that the knife edge self-rotates to face the correct direction as the plotter head moves.

Vinyl cutters are primarily used to produce single-color line art and lettering. Multiple color designs require cutting separate sheets of vinyl, then overlaying them during application; but this process quickly becomes cumbersome for more than a couple of hues.

Sign cutting plotters are in decline in applications such as general billboard design, where wide-format inkjet printers that use solvent-based inks are employed to print directly onto a variety of materials. Cutting plotters are still relied upon for precision contour-cutting of graphics produced by wide-format inkjet printers – for example to produce window or car graphics, or shaped stickers.

Large-format inkjet printers are increasingly used to print onto heat-shrink plastic sheeting, which is then applied to cover a vehicle surface and shrunk to fit using a heat gun, known as a vehicle wrap.

A static cutting table is a type of cutting plotter used a large flat vacuum table. It is used for cutting non-rigid and porous material such as textiles, foam, or leather, that may be too difficult or impossible to cut with roll-fed plotters. Static cutters can also cut much thicker and heavier materials than a typical roll-fed or sheet-fed plotter is capable of handling.

The surface of the table has a series of small pinholes drilled in it. Material is placed on the table, and a coversheet of plastic or paper is overlaid onto the material to be cut. A vacuum pump is turned on, and air pressure pushes down on the coversheet to hold the material in place. The table then operates like a normal vector plotter, using various cutting tools to cut holes or slits into the fabric. The coversheet is also cut, which may lead to a slight loss of vacuum around the edges of the coversheet, but this loss is not significant.

In the mid-to-late 2000s artists and hackers began to rediscover pen plotters as quirky, customizable output devices. The quality of the lines produced by pens on paper is quite different from other digital output techniques. Even 30-year-old pen plotters typically still function reliably, and many were available for less than $100 on auction and resale websites. While support for driving pen plotters directly or saving files as HPGL has disappeared from most commercial graphics applications, several contemporary software packages make working with HPGL on modern operating systems possible.

As use of plotters has waned, the large-format printers that have largely replaced them have come to be called plotters as well.



</doc>
<doc id="23086" url="https://en.wikipedia.org/wiki?curid=23086" title="Poker equipment">
Poker equipment

The following is a list of equipment used for a game of poker:



</doc>
<doc id="23090" url="https://en.wikipedia.org/wiki?curid=23090" title="Ante">
Ante

Ante or Antes may refer to:



</doc>
<doc id="23093" url="https://en.wikipedia.org/wiki?curid=23093" title="Bug (poker)">
Bug (poker)

A bug in poker is a limited form of wild card. One or both jokers are often added to the deck and played as bugs.

In draw poker played for high and pai gow poker, the bug is considered to be an ace, unless it can be used as a missing card to complete a straight or a flush, in which case it becomes the highest card which can complete the hand.


In California lowball, the bug is the lowest unpaired card in a hand. For example in 8-6-4-3 plus the bug, the bug becomes an ace; in A-2-3-5 plus the bug, the bug becomes a four.

Holding the bug greatly increases the power of certain drawing hands. For example, playing for high, a natural four-straight such as Q-J-10-9 drawing one has nine outs to complete the hand, any king or eight or the bug. By contrast a four-straight including the bug can have as many as sixteen outs to complete the straight - Q-J-10-Joker can catch any ace, king, nine, or eight.


</doc>
<doc id="23094" url="https://en.wikipedia.org/wiki?curid=23094" title="Wild card (cards)">
Wild card (cards)

A wild card in card games is one that may be used to represent any other playing card, sometimes with certain restrictions. These may be jokers, for example in Rummy games, or they may be normal ranked and suited cards pressed into wild card duty ("deuces wild" in Poker is a common variant).

In most cases, the wild card or cards must be agreed upon by all players before the cards are dealt and play commences. There are two common rules regarding wild cards: "fully wild" cards and the "bug".

A card that is fully wild can be designated by its holder as any card they choose with no restrictions. Under this rule, for example, a hand with any natural pair and a wild card becomes three of a kind. Without wild cards in play, the best possible hand is a natural royal flush. The common rule in casinos is that a wild card plays as a bug, which is given the rank of ace unless designating it as a different card would complete a straight, flush, or royal flush. Under this rule, a hand such as K-K-Joker-5-2 is just a pair of kings (with an ace kicker), but any four same-suit cards with a bug make a flush, and a hand such as 7-Joker-5-4-3 makes a straight.

There is also a variation of the "Fully Wild" rule in which the wild card (in this instance they are usually jokers as there are traditionally only two and there is only one black and one red) can be any card of the suits matching the cards colour or current suit. For example in a jokers wild game with these rules, the red joker could be used as any card of hearts or diamonds. Inversely, the black joker would be any card of clubs or spades.

Two exceptions to standard poker practice sometimes seen in home games are the double-ace flush rule, and the natural wins rule. The latter rule states that between hands that would otherwise tie, the hand with fewer wild cards wins. This is not common in casinos and should be treated as an exception to standard practice (as is the double-ace flush).

There is a tendency among some players to regard wild cards as "impure" or treat wild card games as silly or amateurish. While it is certainly true that a game with too many wild cards can become so random that all skill is lost, the occasional use of wild cards is a good way to add variation to a game and add opportunities for skillful play. In particular, five-card draw is traditionally played with a joker in California (which plays as a bug), and also plays well with deuces fully wild. Seven-card stud plays well with one or two bugs, especially when played high-low split. Other games such as Texas hold 'em and Omaha hold'em do not play well with wild cards. For some players, the problem with wild card games is that the winner is almost always the hand with the most wild cards, making the other cards irrelevant, and making skill less important.

Another issue with wild cards is that they distort the hand frequencies. In 5-card stud, the stronger hands are less frequent than the weaker hands; i.e., no pair is most common, followed by one pair, two pair, three of a kind, etc. When wild cards are added, the stronger hands gain frequency while the weaker hands lose frequency. For example, if a player holds a pair and a wild card, they will always choose three of a kind rather than two pair. This causes three of a kind to be more common than two pair. But if two pair ranks above three of a kind, the two pair will become more common.


</doc>
<doc id="23101" url="https://en.wikipedia.org/wiki?curid=23101" title="High-low split">
High-low split

In traditional poker games, the player with the best traditional hand wins the whole pot. Lowball variations award the pot to the lowest hand, by any of several methods (see Low hand (poker)). High-low split games are those in which the pot is divided between the player with the best traditional hand (called the high hand) and the player with the low hand.

There are two common methods for playing high-low split games, called declaration and cards speak. In a declaration game, each player declares (either verbally or using markers such as chips) whether he wishes to contest for the high hand or the low hand. The lowest hand among those who declared low wins that half of the pot, and the highest hand among those who declared high wins that half (for further details, see declaration). In a cards speak game, all players simply reveal their cards at showdown and the hands are evaluated by all players; high hand wins half of the pot and low hand wins the other half.

Especially when using the ace-to-five low method, it is possible for one player to have both the low hand and the high hand, and therefore win all of the pot (called "scooping," "hogging" the pot, or "going pig"). In the event more than one player ties for either high or low, the pot can be further split into quarters or smaller fractions. For example, if one player has the high hand on showdown, and two other players tie for the best low hand, the high hand wins half of the pot and each low hand wins only a quarter of the pot.

It is common, especially in cards speak games, to require a certain hand value or better to win the low half of the pot, called a qualifier. For example in an "eight or better to qualify low" game, a player with an eight-high hand (or better low such as seven-high) is entitled to win the low half of the pot (assuming his hand defeats all other low hands), but a player with a 10-high or 9-high hand cannot win, even if his hand is the lowest. In this case, the high hand wins the entire pot. There is generally no qualifier to win high, although one common variant is any pair/no pair, where a hand of at least a pair is required to win high and any hand with no pair is required to win low.

In high-low split games where each player is dealt more than five cards, each player chooses five of his cards to play as his high hand, and/or five of his cards to play as his low hand. The sets may overlap: for example, in seven-card stud played high-low split, a player dealt 7-7-6-4-4-3-2 can play a high hand of 7-7-4-4-6 (two pair, sevens and fours) and a low hand of 7-6-4-3-2 (seven-high).

Note that bluffs can be especially powerful in high-low split games, because a player making a successful bluff wins the whole pot rather than having to share it. This fact also makes bluffs less likely to succeed.



</doc>
<doc id="23106" url="https://en.wikipedia.org/wiki?curid=23106" title="Kicker (poker)">
Kicker (poker)

A kicker, also called a side card, is a card in a poker hand that does not itself take part in determining the rank of the hand, but that may be used to break ties between hands of the same rank. For example, the hand Q-Q-10-5-2 is ranked as a pair of queens. The 10, 5, and 2 are kickers. This hand would defeat any hand with no pair, or with a lower-ranking pair, and lose to any higher-ranking hand. But the kickers can be used to break ties between other hands that also have a pair of queens. For example, Q-Q-K-3-2 would win (because its K kicker outranks the 10), but Q-Q-10-4-3 would lose (because its 4 is outranked by the 5).

The term is also used in draw poker to denote an unmatched card (often an ace) retained by a player during the draw in the hope that either it will be paired on the draw, or else play as a kicker (in the first sense) on the showdown. A kicker may also be retained in order to deceive an opponent, for example, to represent a three-of-a-kind when the player has only a pair.

Kickers take on special importance in Texas hold 'em, because a common winning hand is one card in a player's hand matched with a card on the board, while the player's second card acts as a kicker. For example, if one player holds A-8, a second player holds A-7, and the board is
A-K-6-5-4, the player with the A-8 will outkick the player with the A-7, since A-8's best hand is A-A-K-8-6, while the A-7's hand is A-A-K-7-6.
However, if the board held A-K-Q-J-3, the players would tie, because both would play the hand A-A-K-Q-J; in this case it is said that the players' kickers "don't play", or that the "kicker on the board plays". In this case, there would be a split pot.



</doc>
<doc id="23118" url="https://en.wikipedia.org/wiki?curid=23118" title="Four of a kind">
Four of a kind

Four of a kind may refer to:



</doc>
<doc id="23124" url="https://en.wikipedia.org/wiki?curid=23124" title="Blind (poker)">
Blind (poker)

The blinds are forced bets posted by players to the left of the dealer button in flop-style poker games. The number of blinds is usually two, but it can range from none to three.

The small blind is placed by the player to the left of the dealer button and the big blind is then posted by the next player to the left. The one exception is when there are only two players (a "heads-up" game), when the player on the button is the small blind, and the other player is the big blind. (Both the player and the bet may be referred to as big or small blind.)

After the cards are dealt, the player to the left of the big blind is the first to act during the first betting round. If any players call the big blind, the big blind is then given an extra opportunity to raise. This is known as a "live blind". If the live blind checks, the betting round then ends.

Generally, the "big blind" is equal to the minimum bet. The "small blind" is normally half the big blind. In cases where posting exactly half the big blind is impractical due to the big blind being some odd-valued denomination, the small blind is rounded (usually down) to the nearest practical value. For example, if the big blind in a live table game is $3, then the small blind will usually be $1 or $2 since most casinos do not distribute large quantities of $0.50 poker chips.

The blinds exist because Omaha and Texas hold 'em are frequently played without antes, allowing a player to fold his hand without placing a bet. The blind bets introduce a regular cost to take part in the game, thus inducing a player to enter pots in an attempt to compensate for that expense.

It is possible to play without blinds. The minimum bet is then the lowest denomination chip in play, and tossing only one chip is considered as a call. Anything higher than that is considered a raise. Poker without blinds is usually played with everyone posting an ante to receive cards.

In cash games, otherwise known as ring games, blinds primarily serve to ensure all players are subject to some minimum, ongoing cost for participating in the game. This encourages players to play hands they otherwise might not, thereby increasing the average size of the pots and, by extension, increasing the amount of rake earned by the cardroom hosting the game.

In cash games, the amount of the blinds are normally fixed for each particular table and will not change for the duration of the game. However, many cardrooms will allow blind levels to change in cases where all players unanimously agree to a change. Larger cardrooms will often include tables with different blind levels to give players the option of playing at whatever stakes they are most comfortable with. In online poker, blinds range from as little as one U.S. cent to USD1,000 or more.

The minimum and maximum buy-in at a table is usually set in relation to the big blind. At live games, the minimum buy-in is usually between 20 and 50 big blinds, while the maximum buy-in is usually between 100 and 250 big blinds. Some online cardrooms offer "short stack" tables where the maximum buy-in is 50 big blinds or less and/or "deep stack" tables where the minimum buy-in is 100 big blinds or more.

In cash games that do not deal cards to players who are absent from the table at the start of the hand (or, in online games, are designated as "sitting out"), special rules are necessary to deal with players who miss their blinds.

In such a situation, if a player misses his or her big blind, he or she will not be dealt in again until the button has passed. At that point, if the player wishes to rejoin the game, he or she must "super-post" - he or she must post both the big and small blinds in order to be dealt cards. Of these, only the big blind is considered "live" while the small blind is "dead" - it is placed in the center of the pot apart from the big blind and will not count towards calling any additional bets or raises by other players. If the player has only missed the small blind, then the same procedure applies except that the player only has to post the "dead" small blind to rejoin the game. Most cardrooms allow players to relieve themselves of these obligations if they wait until they are again due to post the big blind before rejoining the game.

Some cardrooms hosting live cash games do not allow players to miss and/or avoid paying blinds in this manner. In these games, all players with chips on the table are dealt in whether or not they are present at the table. Any blinds due will be posted from the player's stack - depending on the cardroom's rules this will be done either by the dealer, another cardroom employee or a nearby player under staff supervision. Whenever a player has not returned to the table by the time it is his turn to act, his or her hand is automatically folded. Under such rules, if a player wishes to be absent from the table then the only way he or she can avoid paying blinds is to cash out and leave the game altogether.

In poker tournament play, blinds serve a dual purpose. In addition to the purpose explained above, blinds are also used to control how long the tournament will last. Before the tournament begins, the players will agree to a blinds structure, usually set by the tournament organizer. This structure defines how long each round is and how much the blinds increase per round. Typically, they are increased at a smooth rate of between 25% and 50% per round over the previous round. As the blinds increase, players need to increase their chip counts (or "stacks") to stay in the game. The blinds will eventually consume all of a player's stack if he or she does not play to win more.

Unlike many cash games, it is not possible for a player to "miss" blinds in a tournament. If a player is absent from the table, he will continue to have his or her cards dealt and mucked and will have blinds and, if applicable, antes taken from his stack as they are due, either until he or she returns or until his or her stack is completely consumed by blinds and antes. A player who loses his or her chips in this manner is said to have been "blinded off."

There are two main goals for the blinds structure:


If desired, antes can be added to further increase the pressure to win more chips.

If each player in a tournament starts with 5,000 in chips and after four hours, the big blind is 10,000 (with a small blind of 5,000), it will be very difficult for a player with only 15,000 in chips to stay in the game.



</doc>
<doc id="23128" url="https://en.wikipedia.org/wiki?curid=23128" title="Showdown (poker)">
Showdown (poker)

In poker, the showdown is a situation when, if more than one player remains after the last betting round, remaining players expose and compare their hands to determine the winner or winners.

To win any part of a pot if more than one player has a hand, a player must show all of his cards faceup on the table, whether they were used in the final hand played or not. Cards speak for themselves: the actual value of a player's hand prevails in the event a player mis-states the value of his hand. Because exposing a losing hand gives information to an opponent, players may be reluctant to expose their hands until after their opponents have done so and will muck their losing hands without exposing them. "Robert's Rules of Poker" state that the last player to take aggressive action by a bet or raise is the first to show the hand—unless everyone checks (or is all-in) on the last round of betting, then the first player to the left of the dealer button is the first to show the hand.

If there is a side pot, players involved in the side pot should show their hands before anyone who is all-in for only the main pot. To speed up the game, a player holding a probable winner is encouraged to show the hand without delay. Any player who has been dealt in may request to see any hand that is eligible to participate in the showdown, even if the hand has been mucked. This option is generally only used when a player suspects collusion or some other sort of cheating by other players. When the privilege is abused by a player (i.e. the player does not suspect cheating, but asks to see the cards just to get insight on another player's style or betting patterns), he may be warned by the dealer, or even removed from the table.

There has been a recent trend in public cardroom rules to limit the ability of players to request to see mucked losing hands at the showdown. Specifically, some cardrooms only grant the right to view a mucked losing hand if the requesting player articulates a concern about possible collusion. Under such rules, players do not have an inherent right to view mucked hands.


</doc>
<doc id="23134" url="https://en.wikipedia.org/wiki?curid=23134" title="Check-raise">
Check-raise

A check-raise in poker is a common deceptive play in which a player checks early in a betting round, hoping someone else will open. The player who checked then raises in the same round.

This might be done, for example, when the first player believes that an opponent has an inferior hand and will not call a direct bet, but that they may attempt to bluff, allowing the first player to win more money than they would by betting straightforwardly. The key point is that if no one else is keen to bet, then the most a player can raise by (in a limit game) is one single bet. If someone else bets first, they can raise, thus increasing the value of the pot by two bets. In a no-limit game, there is no restriction on the size of one's bet, and a raise is likely to be much larger than the second player's bet. Of course, if no other player chooses to open, the betting will be "checked around" and the play will have failed to elicit additional money for the pot. Like a simple check, a failed check-raise provides other players an opportunity to view the next card or cards dealt without requiring the other players to commit more money to the pot. A check-raise thus contains an element of risk because the check-raising player's advantage may deteriorate when new cards are revealed. 

While it can be an important part of one's poker strategy, this play is not allowed by a house rule in some home games and certain small-stakes casino games. It is also frequently not allowed in the game of California lowball. In older poker material and among stud and draw poker players, it is sometimes referred to as "sandbagging".

Check-raises can also be used as an intimidation technique over the course of a game; a player who has frequently been check-raised may be less likely to attempt to steal the pot.

In online poker games special tracking software can be used to determine the exact percentage of times a player check-raised when they had the opportunity. This information helps to determine if a player who check-raised has a monster hand or is bluffing as part of their routine poker play.

Not all players agree that a check-raise is an especially effective play, however. In "Super/System", poker legend Doyle Brunson claims to check-raise very rarely in no-limit hold 'em; he contends that it is more profitable to simply bet a quality hand, regardless of whether his opponent will try to bluff. His reasoning for this is twofold: First, a failed check-raise gives other players the chance to see free cards that may improve their hand; second, it makes it obvious to other players that you potentially have a very strong hand. The latter, however, may be used as a strong bluff technique, although the opponent could put in a re-raise to scare off a bluff.



</doc>
<doc id="23144" url="https://en.wikipedia.org/wiki?curid=23144" title="Twist (poker)">
Twist (poker)

Twist is poker jargon for a round with specific rules which is sometimes used in the poker variant stud poker.

One can replace any round of (or add a round to) a stud poker game with a twist round, in which each player is offered the option to replace exactly one card in his hand with a new one from the remaining deck stub.
This is similar to the draw phase of draw poker, differing in the following way: if the player chooses to replace a downcard, he discards it and is dealt a replacement card also face down; if he wishes to replace an upcard, he discards it and receives the replacement face up.
On a twist round, players make the decision of which card to replace in turn starting with the player who bet first on the preceding round (usually the player whose upcards make the best hand), discarding the card they choose to replace, if any.
After everyone has made their decision, the replacement cards are dealt starting at the dealer's left as usual.

Sometimes replacement cards are "bought" by requiring a player to add a fixed amount to the pot to be able to get a replacement.


</doc>
<doc id="23145" url="https://en.wikipedia.org/wiki?curid=23145" title="Stripped deck">
Stripped deck

A stripped deck (US) or shortened pack (UK) is a set of playing cards from which some cards have been removed. The removed cards are usually the pip cards. Many card games use stripped decks, and stripped decks for popular games are commercially available.

When playing cards first arrived in Europe during the 1370s, they had the same format as the modern standard 52-card deck, consisting of four suits each with ten pip cards and three face cards. During the late 14th and 15th centuries, the Spanish and Portuguese decks dropped the 10s while the German and Swiss packs removed the Aces to create 48-card decks. It is far easier to print 48 cards using two woodblocks than 52 cards. While the removal of the above cards was motivated by manufacturing considerations, later expulsions are the result of trying to speed up card games to make them more exciting. Trappola is the first known card game to be played with a deck that was stripped for game play. It removed all the cards from 6 to 3 to create a 36-card deck.

The most popular card game in 16th-century Europe was Piquet, played with a 36-card deck that dropped ranks from 5 to 2. Around 1700, it dropped the 6s as well to create the 32-card deck which is now the most popular format in France. 32 and 36-card decks are the most widespread in countries that were once part of the Holy Roman (the Low Countries, Germany, and Switzerland), Austro-Hungarian, and Russian empires. 24-card decks to play Schnapsen are widely available in central Europe although it may be shortened to 20 in the future as that is how the modern variant is now commonly played.

The Spanish, Portuguese, Italians, and Latin Americans use mostly 40-card decks. Unlike the countries above, they drop the higher-ranking numerals so that the 7 is located immediately under the face cards. This was due to the popularity of Ombre, the game that introduced the concept of bidding.

The British and the Scandinavians are the most resistant against stripped decks, having maintained the 52-card format since receiving them in the 15th century. The British have also propagated that deck size through whist, the most popular card game of the 19th century. In the 20th century, this has been followed by contract bridge, gin rummy, canasta, and poker which all require that deck size. The British prefer games involving four players as opposed to the continental three-player games which uses smaller decks.

Asian countries also created stripped decks using their traditional playing cards. In contrast to the Western practice of removing "ranks", Asians remove "suits". During the Qing dynasty, the Chinese money-suited cards dropped one suit as shedding-type games became more popular. In India, the gambling game of Naqsha overtook the Ganjifa trick-taking game and many decks were made with only half of the traditional suits.

The opposite of a stripped deck is an expanded deck. Many commercial attempts have tried and failed to increase the standard deck above 52 cards. The most successful addition to the standard deck is the Joker which first appeared during the American Civil War as a Euchre trump card. The Joker has since been adopted as a wild card in a few other standard playing card games with different values and quantities depending on which game is being played. 500 is a Euchre offshoot invented by the United States Playing Card Company (USPCC) during the early 20th century. To play the six-handed version, USPCC created a deck with ranks 11, 12, and 13. 500 decks are now produced by other manufacturers and are sold primarily in English-speaking countries where the game is played. A much older expanded deck is tarot, invented in 15th-century Italy, with an extra suit of trumps. Tarot card games were the most popular card games of the 18th century but have since declined. They are still played in various continental European countries with France having the largest community. Tarot decks are not immune to stripping either. The Tarocco Bolognese, Tarocco Siciliano, Industrie und Glück, and Cego decks have excised some pip cards.

A French-suited deck of 32 cards, consisting of 7, 8, 9, 10, Jack, Queen, King and Ace in four suits each, is used in the two-player game Piquet, which dates back to the 16th century. 
Games played with a piquet deck (or the equivalent German- or Swiss-suited decks) are still among the most popular in some parts of Europe. This includes belote and klaverjas (the national games of France and the Netherlands, respectively) and skat (the German national game, which is also played with the equivalent German-suited decks in some regions). Bezique is played with two piquet decks.

Stripped decks are used in certain poker variants. The earliest form of poker was played with only 20 cards. The Australian game of Manila uses a piquet deck, and Mexican stud is played with the 8s, 9s, and 10s removed from the deck (and a joker added). This may require adjusting hand values: in both of these games, a flush ranks above a full house, because having fewer cards of each suit available makes flushes rarer.

A hand such as 6-7-J-Q-K plays as a straight in Mexican stud, skipping over the removed ranks. Some places may allow a hand such as 10-9-8-7-A to play as a straight (by analogy to a wheel) in the 32-card game, the A playing low and skipping over the removed ranks (although this is not the case in Manila). Finally, the relative frequency of straights versus three of a kind is also sensitive to the deck composition (and to the number of cards dealt), so some places may consider three of a kind to be superior to a straight, but the difference is small enough that this complication is not necessary for most games. Similarly, a full house tends to occur more often than a flush in a piquet deck, due to the increased frequency of each playing card rank, creating a change in poker combination ranking.

Five-card stud is also often played with a piquet deck. In lively home games it might work better to only strip three ranks (2s through 4s) with seven or eight players; with only two or three players 7s and 8s could be stripped as well, leaving the same 24-card deck used in euchre. In any of these cases, a flush should rank above a full house (in a 24-card deck it is actually rarer than four of a kind, but is rarely played that flushes are superior to four of a kind). Stripped deck five-card stud is a game particularly well-suited to cheating by collusion, because it is easy for partners to signal a single hole card and the relative value of knowing the location of a single card is higher than with a full deck.

The game of euchre is also played with a 24-card stripped deck, consisting of only 9-10-J-Q-K-A of each suit, the 2-8 being stripped from the deck. The game of pinochle is played with 48 cards, consisting of a doubled euchre deck (that is, two copies of 9-A of each suit).

In some games, a small number of cards are stripped from the deck to make the deal exact. For example, it is customary to remove the 2 when three people play Hearts.



</doc>
<doc id="23146" url="https://en.wikipedia.org/wiki?curid=23146" title="Roll-your-own cigarette">
Roll-your-own cigarette

Roll-your-own cigarettes (also called RYO, MYO, rollies, roll-ups, burns, bingie, hand-rolled cigarettes, or simply rolls) refer to cigarettes made from loose tobacco and rolling paper. Roll-your-own products are sold in pouches or as tins of tobacco, sometimes including the rolling papers or cigarette tubes. Loose filters are available for purchase and can be added to the rolled cigarettes. Some people use a machine to assist them and some people use pre-rolled cones or cigarette tubes.

Hand-rolled cigarettes give smokers the ability to roll cigarettes of any diameter, thereby varying the strength of the cigarette. Technological aids—from hand injectors to large in-store machines—aid in the process.

In the United States, the Internal Revenue section of the tax code includes a personal exemption for people who make their own cigarettes and tobacco (done by shredding blended strips of tobacco leaves).

An amendment to the 2012 federal transportation bill caused roll-your-own cigarette shops to struggle and consider closing. In order for shops to continue using machines, owners must obtain a manufacturer's permit, file a bond, pay the applicable federal cigarette tax rate, keep records, print required markings on packages used for manufactured cigarettes, affix the U.S. Surgeon General's warning labels to packages and comply with the U.S. Food and Drug Administration's minimum cigarette package size.

In Europe, EU regulations for tar and nicotine levels in cigarettes do not apply to rolling tobacco. Hand-rolling tobacco is taxed and priced at a lower level – about half that of packaged cigarettes. In countries where cigarettes are cheap or rolling tobacco is expensive, very few people use RYO cigarettes. By contrast, in the Netherlands about half of all tobacco smoked in the country is RYO because of price differences.

The least amount of supplies needed to roll one's own cigarettes includes tobacco and rolling papers. However, some prefer to use equipment to aid them in rolling. These can include mechanical rolling machines and cigarette injectors (both mechanical and electric). Filters can also be added when using a rolling machine, and filter tubes are used when making cigarettes with an injector.

Rolling tobacco, or cigarette tobacco, is the primary tobacco used for R-Y-O cigarettes. It is generally packaged in pouches. After 2009, the (United States) federal tax rate on R-Y-O tobacco was raised from $1.0969 per pound to $24.78 per pound. This increase has caused many people to switch to using pipe tobacco to make cigarettes, since the pipe tobacco tax rate was also increased, but only to $2.83 per pound.

Backroll is a widely used method for hand-rolling a cigarette. The method involves inversion of the rolling paper, so that the gum strip faces the inside. Once rolled, the gum can then be licked through the paper and torn off, thus removing any excess paper. This technique was developed due to the alleged increased heat (possibly caused by additives) generated by smoking tobacco wrapped in multiple layers of paper.

R-Y-O has become more popular in the United States in recent years, but relatively few smokers, only 6.7%, actually roll their own cigarettes. In contrast, this rate was 15% in Canada, 22% in Australia, and 30% in the UK. Reasons for this difference include the generally lower price of traditional cigarettes in most states in the US compared to Canada and Europe.




</doc>
<doc id="23147" url="https://en.wikipedia.org/wiki?curid=23147" title="Rollout (poker)">
Rollout (poker)

Rollout or roll 'em out is poker jargon used for a game phase in certain poker variants. It is often incorrectly called "roll your own", to which it has similarities but from which it is fundamentally different.

Poker games with a rollout phase resemble stud poker but have significantly different strategies, because players generally receive all of their cards up front (sometimes with a draw phase), and know the final value of their hand in early betting rounds. They resemble stud poker only in that cards are revealed to other players one at a time for each betting round.

There are the same three variations on the idea as with roll your own, depending on when players are allowed to choose which card to reveal. They can either be forced to arrange the order of their cards before any betting begins ("choose before"), or they can also be allowed to choose cards in later rounds based on information found in earlier rounds ("choose after"). In the latter case, the revealing can be made simultaneously or in turn.

In the game of show five, for example, each player is dealt seven cards before any betting begins, and each of the game's five betting rounds begins with the players simultaneously revealing one of their cards ("simultaneous choose-after rollout"). Rollout games are frequently played high-low split, and players choose which cards to reveal in order to delay as long as possible revealing which half of the pot they intend to win.


</doc>
<doc id="23149" url="https://en.wikipedia.org/wiki?curid=23149" title="One player to a hand">
One player to a hand

One player to a hand is an important poker rule designed to promote fair play that is universally applied in casino play. It states that all game decisions about the play of each hand must be made by one player without any assistance. This means, for example, that a player may not ask for advice from any other player or non-player during the play of the hand, nor should anyone offer such advice. The phrase is often used as a warning to players making what might be perceived as minor violations, such as commenting upon other players' possible hands.

Note that any player correcting an error on a declared holding once the hands are exposed is not a violation of this rule, since no further decisions can be made. Some rulebooks declare it an ethical obligation of a player to point out any error in the awarding of a pot or the reading of hands shown down. See Cards speak.



</doc>
<doc id="23150" url="https://en.wikipedia.org/wiki?curid=23150" title="Cards speak">
Cards speak

Cards speak ("for themselves"), also known as "cards read" is used in two poker contexts:

First, it is used to describe a high-low split game without a declaration. That is, in a cards speak game, players all reveal their hands at the showdown, and whoever has the highest hand wins the high half of the pot and whoever has the lowest hand wins the low half.

The second is as a house rule in casino cardrooms. "Cards speak" means that any verbal declaration as to the content of a player’s hand is not binding. If Mary says she has no pair, but in fact she has a flush, her cards speak and her hand is viewed for its genuine value, that of a flush. Likewise if John says he has a flush, but in fact he does not, his hand is judged on its actual merits, not his verbal declaration. At the discretion of management, a player deemed to be deliberately miscalling his hand may incur a penalty.

The "cards speak" rule does not address the awarding of a pot, player responsibilities, or the similar one player to a hand rule. It merely means that verbal statements do not make a hand value, but the cards do.


</doc>
<doc id="23151" url="https://en.wikipedia.org/wiki?curid=23151" title="Declaration (poker)">
Declaration (poker)

There are several actions in poker called declaration, in which a player formally expresses his intent to take some action (which he may perform at a later point).
For example, one may verbally declare an action (fold, call, raise) while in turn, which obligates the player to complete that action.
One may declare a number of cards to draw in a draw poker game (which is typically not binding), or one may declare some other choice specific to the variant being played.

But most commonly, the term refers to the declaration in the final phase of a high-low split game, in which players indicate whether their hands are to be evaluated as high hands, low hands, or both at showdown. This is only one option for high-low split games; the other is known as "cards speak", in which players simply reveal their hands at showdown and award the pot to the highest and lowest hands shown (possibly subject to qualifications). Cards speak is used commonly in casinos because it is the much simpler method. High-low with declaration is common in home games.

First, declarations can be made either in turn or simultaneously.
Games with verbal in-turn declarations are uncommon, because the positional value of declaring last is so great that it makes the game unfair.
Simultaneous declarations are commonly done by the "chips in hand" method.
Each player remaining in the game takes two chips or coins below the table, then brings up a closed hand containing zero, one, or two of the chips.
After all players have brought their closed hands above the table, they all then open their hands to reveal their choices: for example, no chips in the hand means the player is declaring "low", one chip "high", and two chips "swing" (both ways).

After declaration and showdown, half of the pot is awarded to the highest hand among those players who declared high, and half to the lowest hand among those who declared low. If no one declared in one direction, the whole pot is awarded to the other (for example, if all players declared low, the lowest hand is awarded the whole pot).

If any player declared "swing", then that player must have both the high and low hands to take any part of the pot, though there are several rule variations covering the specifics. First, if the rules specify that ties are acceptable, then a player declaring swing must win or tie both directions to win anything, but if he does, he is entitled to his appropriate share. For example, if the swing player has the clearly highest hand but shares the lowest hand with another player, he wins three-fourths of the pot and the other low hand wins one-fourth. If the rules specify that ties are not acceptable, then a swing player must clearly win both directions: even a tie in one direction means he wins nothing.

If a swing player fails for half the pot, the half that he would have otherwise won can be awarded either to the second-best hand in that direction, or to the player who defeated him in the other. The latter rule affords more strategic possibilities in declaration. For example, if a player declaring swing has the best high hand but loses for low (or ties for low with a no-ties rule), the whole pot is awarded to the low hand that defeated him.

A rule must be adopted for the case where no player is eligible to win the pot (for example, if all players declare swing, and no player winds both ways). Some possible rules include playing the hand as a no-declare hand, or having the pot ride over to the next hand.


</doc>
<doc id="23153" url="https://en.wikipedia.org/wiki?curid=23153" title="Closed (poker)">
Closed (poker)

In the game of poker, a betting round is said to be closed if no player will have the right to raise in the round. Normally this occurs when a player calls, and the next player whose turn it is to act is the one who made the last raise, so he cannot raise further (this ends the betting round). The round can also said to be closed before it has actually ended if there are still players remaining to act, but they will not be entitled to raise either because the last raise was a sub-minimum all-in raise (see poker table stakes rules) or because the limit ("cap") on allowed raises has been reached.

The term is also used to describe a category of poker game in which no cards held by individual players are visible to any other player before the showdown. Most forms of draw poker are closed games (draw games with a rollout are an exception). Most forms of stud poker, in contrast, are open games, because some players' cards are dealt face up or are exposed during play (blind stud games are an exception). Most community card poker games like Texas hold 'em are considered closed as well, because the only cards exposed before showdown belong to everyone; the individual players' cards are never seen until showdown.

A player who closes the betting round by calling or overcalling is entitled to greater freedom by doing so, since he does not face the threat of subsequent raises. This is especially true when comparing limit hold'em games with a standard cap (3 raises) to an elevated cap (4 raises) or capless game. A player can cap with as much as 80% of his flat calling range when he knows he cannot be forced out of the pot and no opponent can make his hand appear much stronger by raising. This is particularly correct when closing the action on the river in Texas hold'em or on the 7th street in stud poker, where a player can make calldowns with hands that are unlikely to win simply because of the pot odds he is getting and the fact he cannot be bluffed out of the pot.


</doc>
<doc id="23157" url="https://en.wikipedia.org/wiki?curid=23157" title="Value (poker)">
Value (poker)

In poker, the strength of a hand (how likely it is to be the best according to the rules of the game being played) is often called its value; however, in the context of poker strategy the term is more often used to describe a betting tactic, a bet for value. This bet (or raise) is intended to increase the size of the pot, by inducing opponents to call. A bet for value is in contrast to a "bluff" or a "protection bet" (though some bets may have a combination of these motives).

For a bet for value to be correct, a player must have a positive expectation, that is, he will win more than one bet for every bet he puts in the pot. Note that pot odds do not matter in this situation, because the factor here is whether it is more profitable to "raise" or "call", rather than to "call" or "fold". Betting for value can apply to both made hand and drawing hand situations, although in the latter situation it is less often correct, as the drawing hand's chances of winning are generally lower. Many made hands will win the pot more than 50% of the time, therefore a value bet is usually correct, even heads up.

For example, in a game of Texas hold 'em, a player has 8♣ 6♠ with a flop of 9♥ 7♦ 2♣, The player has an open-ended straight draw and so has eight outs (four 10s and four 5s). With 47 unknown cards, the player will make the straight approximately one time for every five times he doesn't, thus a bet is profitable if six or more of his opponents will call the bet (he will win once (+6 bets) and lose five time (-5 bets) out of every six hands like this, resulting in an expectation of +1 bet). If he thinks that fewer than six opponents will call the bet, he would lose money and must simply call.



</doc>
<doc id="23158" url="https://en.wikipedia.org/wiki?curid=23158" title="Nut hand">
Nut hand

In poker, the nut hand is the strongest possible hand in a given situation. The second-nut hand or third-nut hand (and so on) may refer to the second and third best possible hands. The term applies mostly to community card poker games where the individual holding the strongest possible hand, with the given board of community cards, is capable of knowing that they have the nut hand.

In Texas hold 'em, if the board is 5♠ 6♠ A♣ 9♠ 5♥, a player holding 7♠ 8♠ has the nut hand because those hole cards complete a 9-high straight flush of spades, which cannot be beaten by any other possible combination of hole cards and community cards. On the same board, the hand 5♣ 5♦ would be the second-nut hand, four of a kind fives; the third-nut hand would be any pair of the remaining three aces, making a full house, aces full of fives.

It is important to note that the "actual" nut hand may not be the same as the "absolute" nut hand; for example, if the board is 7♥ 2♣ K♠ K♥ 3♦ a player with K♣ K♦ has the absolute nut hand. However, any player with K-7 knows that he has the nut hand as it is impossible for another player to have two kings. The phrase may also refer to a hand in progress with cards yet to be dealt, as the player can be said to have the nuts at that time. For example if a player holds 7♠ 8♠ on a board of 5♣ 6♠ 9♥ he can be said to have the nuts, however if the next card comes 7♥ then 8-10 becomes the nuts. This makes some nut hands very vulnerable in nine-card games, such as Omaha hold 'em.

In high-low split games one often speaks of "nut-low" and "nut-high" hands separately. In Omaha hold 'em, if the board is, 5♠ 6♠ A♣ 9♠ 5♥, any player with 2-3 makes the nut-low hand, 6-5-3-2-A, while a player with 2-4 makes the second-nut-low hand, 6-5-4-2-A (the nut-high hands remain the same as in Texas hold 'em, in this case 7♠ 8♠ to make a straight flush, although one can go as low as aces full by introducing quads and straight flush blockers). Similarly, one can sometimes hear the term "nut-nut", which refers to a hand that makes both the best possible high and low. In Omaha, with the same board as above, a player holding 7♠ 8♠ plus 2-3 of any suit has the nut-nut and is guaranteed no worse than a split of the low pot plus a win of the high pot.

A common and certainly apocryphal folk etymology is that the term originated from the historical poker games in the colonial west of America, where if a player bet everything he possessed, he would place the nuts of his wagon wheels on the table to ensure that, should he lose, he would be unable to flee and would have to make good on the bet. Since it would be expected that a player would only make such a bet when he had the best possible hand, the folk lore says that this is how the best possible hand came to be known as the nuts. It is also rumored that these historical games were played only in the winter, and therefore, the nuts that were placed on the table were "stone cold", hence coining the term "stone-cold-nuts".

Another explanation is that "the nuts" originated from the old English usage of "nuts", meaning "any source of pleasure".

Another seemingly fitting explanation is that the term was derived from the UK English slang "the dog's bollocks" or "the mutt's nuts", meaning "the absolute best". However, this phrase originated around 1949, and the term "the nuts" pre-dates it.



</doc>
<doc id="23159" url="https://en.wikipedia.org/wiki?curid=23159" title="Protection (poker)">
Protection (poker)

Protection in poker is a bet made with a strong but vulnerable hand, such as top pair when straight or flush draws are possible. The bet forces opponents with draws to either call with insufficient pot odds, or to fold, both of which are profitable for the betting player. By contrast, if he failed to protect his hand, another player could draw out on him at no cost, meaning he gets no value from his made hand.

A protection play differs from a bluff in that the bluff can win "only" when the opponent folds, while protection bet is made with a hand that is likely to win a showdown, but isn't strong enough for slow playing.

The importance of protection increases when there are multiple opponents. For example, if a hand is currently the best, but each of four opponents has a 1-in-6 chance of drawing an out, the four opponents "combined" become the favorite to win, even though each one is individually an underdog. With a protection bet, some or all of them may fold, leaving fewer opponents and a better chance of winning.

The term "protection" is also often heard in the context of an "all-in" player (see poker table stakes rules). A bet by an opponent serves to protect the all-in player by reducing the number of opponents the all-in player must beat. To deliberately make such a bet solely to protect another player's hand constitutes collusion.

A player may also be said to "protect" his or her cards by placing an object like a specialty chip or miniature figure upon them. This prevents the player from having his cards accidentally collected by the dealer or being fouled by other players' discards.



</doc>
<doc id="23160" url="https://en.wikipedia.org/wiki?curid=23160" title="Draw (poker)">
Draw (poker)

A poker player is drawing if they have a hand that is incomplete and needs further cards to become valuable. The hand itself is called a draw or drawing hand. For example, in seven-card stud, if four of a player's first five cards are all spades, but the hand is otherwise weak, they are "drawing to" a flush. In contrast, a made hand already has value and does not necessarily need to draw to win. A made starting hand with no help can lose to an inferior starting hand with a favorable draw. If an opponent has a made hand that will beat the player's draw, then the player is "drawing dead"; even if they make their desired hand, they will lose. Not only draws benefit from additional cards; many made hands can be improved by catching an out — and may have to in order to win.

An unseen card that would improve a drawing hand to a likely winner is an out. "Playing a drawing hand has a positive expectation if the probability of catching an out is greater than the pot odds offered by the pot."

The probability formula_1 of catching an out with one card to come is:

The probability formula_3 of catching at least one out with two cards to come is:

A dead out is a card that would normally be considered an out for a particular drawing hand, but should be excluded when calculating the probability of catching an out. Outs can be dead for two reasons:

A flush draw, or four flush, is a hand with four cards of the same suit that may improve to a flush. For example, K♣ 9♣ 8♣ 5♣ x. A flush draw has nine outs (thirteen cards of the suit less the four already in the hand). If you have a flush draw in Hold'em, the probability to flush the hand in the end is 34.97 percent if there are two more cards to come, and 19.56 percent (9 live cards divided by 46 unseen cards) if there is only one more card to come.

An outside straight draw, also called up and down, double-ended straight draw or open-end(ed) straight draw, is a hand with four of the five needed cards in sequence (and could be completed on either end) that may improve to a straight. For example, x-9-8-7-6-x. An outside straight draw has eight outs (four cards to complete the top of the straight and four cards to complete the bottom of the straight). Straight draws including an ace are not outside straight draws, because the straight can only be completed on one end (has four outs).

An inside straight draw, or gutshot draw or belly buster draw, is a hand with four of the five cards needed for a straight, but missing one in the middle. For example, 9-x-7-6-5. An inside straight draw has four outs (four cards to fill the missing internal rank). Because straight draws including an ace only have four outs, they are also considered inside straight draws. For example, A-K-Q-J-x or A-2-3-4-x. The probability of catching an out for an inside straight draw is half that of catching an out for an outside straight draw.

A double inside straight draw, or double gutshot draw or double belly buster draw can occur when either of two ranks will make a straight, but both are "inside" draws. For example in 11-card games, 9-x-7-6-5-x-3, or 9-8-x-6-5-x-3-2, or in Texas Hold'em when holding 9-J hole cards on a 7-10-K flop. The probability of catching an out for a double inside straight draw is the same as for an outside straight draw.

Sometimes a made hand needs to draw to a better hand. For example, if a player has two pair or three of a kind, but an opponent has a straight or flush, to win the player must draw an out to improve to a full house (or four of a kind). There are a multitude of potential situations where one hand needs to improve to beat another, but the expected value of most drawing plays can be calculated by counting outs, computing the probability of winning, and comparing the probability of winning to the pot odds.

A backdoor draw, or runner-runner draw, is a drawing hand that needs to catch two outs to win. For example, a hand with three cards of the same suit has a "backdoor flush draw" because it needs two more cards of the suit. The probability formula_6 of catching two outs with two cards to come is:

For example, if after the flop in Texas hold 'em, a player has a backdoor flush draw (e.g., three spades), the probability of catching two outs on the turn and river is (10 ÷ 47) × (9 ÷ 46) = 4.16 percent. Backdoor draws are generally unlikely; with 43 unseen cards, it is equally likely to catch two out of seven outs as to catch one out of one. A backdoor outside straight draw (such as J-10-9) is equally likely as a backdoor flush, but any other 3-card straight combination isn't worth even one out.

A player is said to be "drawing dead" when the hand he hopes to complete will nonetheless lose to a player who already has a better one. For example, drawing to a straight or flush when the opponent already has a full house. In games with community cards, the term can also refer to a situation where no possible additional community card draws results in a win for a player. (This may be because another player has folded the cards that would complete his hand, his opponent's hand is already stronger than any hand he can possibly draw to or that the card that completes his hand also augments his opponent's.)



</doc>
<doc id="23162" url="https://en.wikipedia.org/wiki?curid=23162" title="Out (poker)">
Out (poker)

In a poker game with more than one betting round, an out is any unseen card that, if drawn, will improve a player's hand to one that is likely to win. Knowing the number of outs a player has is an important part of poker strategy. For example, in draw poker, a hand with four diamonds has nine outs to make a flush: there are 13 diamonds in the deck, and four of them have been seen. If a player has two small pairs, and he believes that it will be necessary for him to make a full house to win, then he has four outs: the two remaining cards of each rank that he holds.

One's number of outs is often used to describe a drawing hand: "I had a two-outer" meaning you had a hand that only two cards in the deck could improve to a winner, for example. In draw poker, one also hears the terms "12-way" or "16-way" straight draw for hands such as 6♥ 7♥ 8♠ (Joker), in which any of sixteen cards (4 fours, 4 fives, 4 nines, 4 tens) can fill a straight.

The number of outs can be converted to the probability of making the hand on the next card by dividing the number of outs by the number of unseen cards. For example, say a Texas Holdem player holds two spades, and two more appear in the flop. He has seen five cards (regardless of the number of players, as there are no upcards in Holdem except the board), of which four are spades. He thus has 9 outs for a flush out of 47 cards yet to be drawn, giving him a 9/47 chance to fill his flush on the turn. If he fails on the turn, he then has a 9/46 chance to fill on the river. Calculating the combined odds of filling on "either" the turn or river is more complicated: it is (1 - ((38/47) * (37/46))), or about 35%. A common approximation used is to double the number of outs and add one for the percentage to hit on the next card, or to multiply outs by four for the either-of-two case. This approximation works out to within a 1% error margin for up to 14 outs.

Note that the hidden cards of a player's opponents may affect the calculation of outs. For example, assume that a Texas hold 'em board looks like this after the third round: 5♠ K♦ 7♦ J♠, and that a player is holding A♦ 10♦. The player's current hand is just a high ace, which is not likely to win unimproved, so the player has a drawing hand. He has a minimum of nine outs for certain, called "nut outs", because they will make his hand the best possible: those are the 2♦, 3♦, 4♦, 6♦, 8♦, 9♦, and Q♦ (which will give him an ace-high flush with no possible better hand on the board) and the Q♣ and Q♥, which will give him an ace-high straight with no higher hand possible. The 5♦ and J♦ will also make him an ace-high flush, so those are "possible outs" since they give him a hand that is likely to win, but they also make it possible for an opponent to have a full house (if the opponent has something like K♠ K♣, for example). Likewise, the Q♠ will fill his ace-high straight, but will also make it possible for an opponent to have a spade flush. It is possible that an opponent could have as little as something like 7♣ 9♣ (making a pair of sevens); in this case even catching any of the three remaining aces or tens will give the player a pair to beat the opponent's, so those are even more "potential outs". In sum, the player has 9 guaranteed outs, and possibly as many as 18, depending on what cards he expects his opponents to have.



</doc>
<doc id="23163" url="https://en.wikipedia.org/wiki?curid=23163" title="Pot odds">
Pot odds

In poker, pot odds are the ratio of the current size of the pot to the cost of a contemplated call. Pot odds are often compared to the probability of winning a hand with a future card in order to estimate the call's expected value.

Odds are most commonly expressed as ratios, but converting them to percentages will often make them easier to work with. The ratio has two numbers: the size of the pot and the cost of the call. To convert this ratio to the equivalent percentage, we add these two numbers together and then divide the cost of the call by this sum. For example, the pot is $30, and the cost of the call is $10. The pot odds in this situation are 30:10, or 3:1 when simplified. To get the percentage, we add 30 and 10 to get a sum of 40 and then divide 10 by 40, giving us 0.25, or 25%.

To convert any percentage or fraction to the equivalent odds, we subtract the numerator from the denominator and then divide this difference by the numerator. For example, to convert 25%, or 1/4, we subtract 1 from 4 to get 3 (or 25 from 100 to get 75) and then divide 3 by 1 (or 75 by 25), giving us 3, or 3:1.

When a player holds a drawing hand (a hand that is behind now but is likely to win if a certain card is drawn) pot odds are used to determine the expected value of that hand when the player is faced with a bet.

The expected value of a call is determined by comparing the pot odds to the odds of drawing a card that wins the pot. When the odds of drawing a card that wins the pot are numerically higher than the pot odds, the call has a positive expectation; on average, you win a portion of the pot that is greater than the cost of the call. Conversely, if the odds of drawing a winning card are numerically lower than the pot odds, the call has a negative expectation, and you can expect to win less money on average than it costs to call the bet.

Implied pot odds, or simply implied odds, are calculated the same way as pot odds, but take into consideration estimated future betting. Implied odds are calculated in situations where the player expects to fold in the following round if the draw is missed, thereby losing no additional bets, but expects to gain additional bets when the draw is made. Since the player expects to always gain additional bets in later rounds when the draw is made, and never lose any additional bets when the draw is missed, the extra bets that the player expects to gain, excluding his own, can fairly be added to the current size of the pot. This adjusted pot value is known as the implied pot.

On the turn, Alice's hand is certainly behind, and she faces a $1 call to win a $10 pot against a single opponent. There are four cards remaining in the deck that make her hand a certain winner. Her probability of drawing one of those cards is therefore 4/47 (8.5%), which when converted to odds is 10.75:1. Since the pot lays 10:1 (9.1%), Alice will on average lose money by calling if there is no future betting. However, Alice expects her opponent to call her additional $1 bet on the final betting round if she makes her draw. Alice will fold if she misses her draw and thus lose no additional bets. Alice's implied pot is therefore $11 ($10 plus the expected $1 call to her additional $1 bet), so her implied pot odds are 11:1 (8.3%). Her call now has a positive expectation.

Reverse implied pot odds, or simply reverse implied odds, apply to situations where a player will win the minimum if holding the best hand but lose the maximum if not having the best hand. Aggressive actions (bets and raises) are subject to reverse implied odds, because they win the minimum if they win immediately (the current pot), but may lose the maximum if called (the current pot plus the called bet or raise). These situations may also occur when a player has a made hand with little chance of improving what is believed to be currently the best hand, but an opponent continues to bet. An opponent with a weak hand will be likely to give up after the player calls and not call any bets the player makes. An opponent with a superior hand, will, on the other hand, continue, (extracting additional bets or calls from the player).

With one card to come, Alice holds a made hand with little chance of improving and faces a $10 call to win a $30 pot. If her opponent has a weak hand or is bluffing, Alice expects no further bets or calls from her opponent. If her opponent has a superior hand, Alice expects the opponent to bet another $10 on the end. Therefore, if Alice wins, she only expects to win the $30 currently in the pot, but if she loses, she expects to lose $20 ($10 call on the turn plus $10 call on the river). Because she is risking $20 to win $30, Alice's "reverse implied pot odds" are 1.5-to-1 ($30/$20) or 40 percent (1/(1.5+1)). For calling to have a positive expectation, Alice must believe the probability of her opponent having a weak hand is over 40 percent.

Often a player will bet to manipulate the pot odds offered to other players. A common example of manipulating pot odds is make a bet to protect a made hand that discourages opponents from chasing a drawing hand.

With one card to come, Bob has a made hand, but the board shows a potential flush draw. Bob wants to bet enough to make it wrong for an opponent with a flush draw to call, but Bob does not want to bet more than he has to in the event the opponent already has him beat. How much should Bob bet?

Assume a $20 pot and one opponent. If Bob bets $10 (half the pot), when his opponent acts, the pot will be $30 and it will cost $10 to call. The opponent's pot odds will be 3-to-1, or 25 percent. If the opponent is on a flush draw (9/46, approximately 19.565 percent or 4.11-to-1 odds against with one card to come), the pot is not offering adequate pot odds for the opponent to call unless the opponent thinks he can induce additional final round betting from Bob if the opponent completes his flush draw (see implied pot odds).

A bet of $6.43, resulting in pot odds of 4.11-to-1, would make his opponent mathematically indifferent to calling if implied odds are disregarded.

According to David Sklansky, game theory shows that a player should bluff a percentage of the time equal to his opponent's pot odds to call the bluff. For example, in the final betting round, if the pot is $30 and a player is contemplating a $30 bet (which will give his opponent 2-to-1 pot odds for the call), the player should bluff half as often as he would bet for value (one out of three times). 

However, this conclusion does not take into account some of the context of specific situations. A player's bluffing frequency often accounts for many different factors, particularly the tightness or looseness of their opponents. Bluffing against a tight player is more likely to induce a fold than bluffing against a loose player, who is more likely to call the bluff. Sklansky's strategy is an equilibrium strategy in the sense that it is optimal against someone playing an optimal strategy against it.




</doc>
<doc id="23165" url="https://en.wikipedia.org/wiki?curid=23165" title="Position (poker)">
Position (poker)

Position in poker refers to the order in which players are seated around the table and the related poker strategy implications. Players who act first are in "early position"; players who act later are in "late position"; players who act in between are in "middle position". A player "has position" on opponents acting before him and is "out of position" to opponents acting after him. Because players act in clockwise order, a player "has position" on opponents seated to his right, except when the opponent has the button and certain cases in the first betting round of games with blinds.

The primary advantage held by a player in late position is that he will have more information with which to make better decisions than players in early position, who will have to act first, without the benefit of this extra information. This advantage has led to many players in heads-up play raising on the button with an extremely wide range of hands because of this positional advantage. Also, as earlier opponents fold, the probability of a hand being the best goes up as the number of opponents goes down. 

The blinds are the least desirable position because a player is forced to contribute to the pot and they must act first on all betting rounds after the flop. Although the big blind has a big advantage on the first round of betting, it is on average the biggest money losing position.

There are 10 players playing $4/$8 fixed limit. Alice pays the $2 small blind. Bob pays the $4 big blind. Carol is under the gun (first to act). If Carol has a hand like K♥ J♠, she may choose to fold. With 9 opponents remaining to act, there is approximately a 40% chance that at least one of them will have a better hand than Carol's like A-A, K-K, Q-Q, J-J, A-K, A-Q, A-J or K-Q. And even if no one does, seven of them (all but the two players in the blind) will have position on Carol in the next three betting rounds.

Now instead, suppose David in the cut-off position (to the right of the button) has the same K♥ J♠ and all players fold to him. In this situation, there are only three opponents left to act, so the odds that one of them has a better hand are considerably less (only around 16%). Secondly, two of those three (Alice and Bob) will be out of position to David on later betting rounds. A common play would be for David to raise and hope that the button (the only player who has position on David) folds. David's raise might simply steal the blinds if they don't have playable hands, but if they do play, David will be in good shape to take advantage of his position in later betting rounds.




</doc>
<doc id="23166" url="https://en.wikipedia.org/wiki?curid=23166" title="Steal (poker)">
Steal (poker)

In poker, a steal is a type of a bluff, a raise during the first betting round made with an inferior hand and meant to make other players fold superior hands because of shown strength. A steal is normally either an "ante steal" or "blind steal" (depending on whether the game being played uses antes or blinds).

Steals are done with hands less valuable than what might normally be considered a raising hand, normally a below average one, with the hope that the few players remaining will not have a hand worth calling the raise, thereby winning the antes or blinds without further action. This play is used either in late position after several people have folded, or when the game is short-handed. Steals happen more often in tournament situations due to the escalating ante/blind structure making the starting pot quite valuable.

While steals don't win much money per hand, they can accumulate to considerable profit if the players to the left of the stealer are tight enough not to contest enough steals. Of course, skilled players will recognize repeated steal plays and frequently reraise for defense.

Steals being made in late position when everyone folds to the stealer, or when the game is short-handed, are the most common steals, but a raise under other conditions can also act as a steal. An aggressive player, especially one with a large stack of chips, might reraise, also known as re-steal, someone he knows might be trying to steal. The objective here is twofold: the re-raiser hopes to pick up both the blinds and antes and the original raiser's chips when the raiser folds, and he also hopes to keep that player from constantly raising before she or he can act because that cuts down on the reraiser's own stealing opportunities.
If one or more players have called a raise pre-flop, a player can re-raise as a bluff in what is called a squeeze play. The original raiser will often only continue with a truly premium holding as several other players have shown signs of strength, and he may well be playing out of position. The players that have just called the original raise are unlikely to have very strong hands as they have not re-raised. 



</doc>
<doc id="23169" url="https://en.wikipedia.org/wiki?curid=23169" title="Dead money (poker)">
Dead money (poker)

In poker, dead money is the amount of money in the pot other than the equal amounts bet by active remaining players in that pot. Examples of dead money include money contributed to the pot by players who have folded, a dead blind posted by a player returning to a game after missing blinds, or an odd chip left in the pot from a previous deal. For example, eight players each ante $1, one player opens for $2, and gets two callers, making the pot total $14. Three players are now in the pot having contributed $3 each, for $9 "live" money; the remaining $5 (representing the antes of the players who folded) is dead money. The amount of dead money in a pot affects the pot odds of plays or rules of thumb that are based on the number of players.

The term "dead money" is also used in a derogatory sense to refer to money put in the pot by players who are still legally eligible to win it, but who are unlikely to do so because they are unskilled, increasing the expected return of other players. This can also be applied to the player himself: "Let's invite John every week; he's dead money". The term "dead money" also applies in tournaments, when many casual players enter events with virtually no chance of winning.



</doc>
<doc id="23170" url="https://en.wikipedia.org/wiki?curid=23170" title="Isolation (poker)">
Isolation (poker)

In poker, an isolation play is usually a raise designed to encourage one or more players to fold, specifically for the purpose of making the hand a one-on-one contest with a specific opponent. For example, if an opponent raises and a player suspects he is holding a weak, but playable hand, they may reraise to pressure other opponents to fold, with the aim of getting heads up with the opening raiser.

Isolation plays are most common against overly-aggressive players ("maniacs") who frequently play inferior hands, or with players who may have a drawing hand. Isolation plays are also common in tournaments to isolate a player who is "short stacked", that is, one who is in imminent danger of elimination, and so is likely to be playing aggressively out of desperation. However, when a player is extremely short stacked compared to the rest of the field in a tournament, making him bust will sometimes be more profitable than winning his chips, so inducing overcalls from other players trumps isolation play.

Isolating is encouraged when holding a hand that fares better heads up than in a multi-way pot. For instance, when a player has a small pocket pair he may raise a large amount simply to knock out other players because typically a small pocket pair is about 50–60% likely to win an all-in pot in a heads up situation, but less likely when facing multiple opponents.



</doc>
<doc id="23172" url="https://en.wikipedia.org/wiki?curid=23172" title="Freeroll">
Freeroll

In poker, a freeroll has two distinct meanings. One applies to the play of a single hand, and the other describes an entire poker tournament.

In playing a particular hand of poker, a freeroll is a situation that arises (usually when only two players remain) before the last card has been dealt, in which one player is guaranteed to at least split the pot with his opponent no matter what the final cards are, but where there is some chance he can win the whole pot if certain final cards are dealt. This most commonly occurs in a high-low split game where one player knows that he has a guaranteed low hand made, his opponent cannot make a better low no matter what the last card is, but the player who is low might possibly catch a lucky card that gives him a straight or flush, winning high as well.

Here's an example from Texas hold'em: Angie holds , and Burt holds . After the fourth card is dealt, the board is . Both players have an ace-high straight, the current nut hand, and so they will most likely split the pot. But if the final card happens to be a club, Burt's straight will lose to Angie's flush. There is no other possible final card that will give Burt more than a split; only Angie can improve, so she is "freerolling" Burt.

If a player knows he has a freeroll, he can raise the pot with impunity, and often a less-skilled opponent with a good hand who does not realize that he is on the wrong end of the freeroll will continue to put in raises with no possible hope of gain.

A freeroll tournament is a tournament with no entry fee, although some freerolls require a payment at some point to gain entry to the tournament.

In a typical pay-to-play tournament, the prize pool consists of an accumulation of the entry fees minus a "fee" which is retained by the house. In a freeroll (at least from the players' perspective) the prize pool is essentially a "donation" provided by the house. Of course, in most freerolls the house is able defray a significant portion of the prize pool (or even turn a profit) by charging for food and beverages, sponsorship fees, admission to spectators, broadcast rights fees, or any combination of these. Sometimes a particular cardroom or casino (either traditional or online) will offer a freeroll tournament to frequent players. Invitation-only tournaments are frequently freerolls.

Freerolls at Internet poker sites should not be confused with their close counterpart -- play money tournaments. Freerolls are different from play-money tournaments in two respects. Play money tournaments usually require the 'payment' of play money and the tournament winnings are play money. Freeroll tournaments can be genuinely free, may require a payment of points (from a point system developed by the site), or on some occasions require a deposit of funds into the player's account. The winnings are either real money, points, merchandise or entry tickets (invitations) to other tournaments.

Most if not all Internet poker sites have freeroll tournaments although in many cases require a payment of points to play. These points typically can only be earned by paying and playing real money hands which in essence is a payment required to play their 'freerolls' and therefore a loose use of the term 'freeroll'. There are Internet sites that allow playing in freerolls without payment of any kind and with the chance to win real money.

It is not unusual to pay to play in a feeder tournament that gives the winner(s) a free entry to another tournament but it is debatable whether these second level tournaments can be called 'freerolls', since they require a buy-in, albeit smaller than the major tournament one. More often, such tournaments are called 'satellites'. This format is typical of freeroll tournaments both on the Internet and in the 'brick and mortar' sites. 

The Professional Poker Tour is one such 'freeroll', with entrants being required to qualify through their results in previous tournaments. Sponsorship and broadcast-rights fees fund the prize pools.

Freeroll tournaments are not exclusive to poker. Casinos frequently offer them to frequent and/or high-value players in games such as craps, blackjack, video poker and slot machines.

Many believe the term comes from early 1950s Las Vegas, when guests would often be given a "free roll" of nickels to play at the slot machines upon check-in. Guests would often ask for their "free rolls" and the words became fused together and expanded to mean any complimentary gaming bonus.



</doc>
<doc id="23173" url="https://en.wikipedia.org/wiki?curid=23173" title="Starting hand">
Starting hand

In poker, the starting hand is the initial set of cards dealt to each player before any voluntary betting takes place. For example, in seven-card stud this is two downcards and one upcard; in Texas hold 'em it is two downcards; in five-card draw it is five cards.

The one decision made by every poker player on every deal of every game is whether to continue playing that hand after seeing that first set of cards. Since making this decision correctly will lead to the most long-run profit for a skilled player, players often put considerable study into what the appropriate starting hand "standards" are for the game being played.

Optimal starting hand standards can be very sensitive to factors such as the betting structure of a game, position, and the character of the other players, as well as the rules of the game being played.



</doc>
<doc id="23174" url="https://en.wikipedia.org/wiki?curid=23174" title="Omaha hold 'em">
Omaha hold 'em

Omaha hold 'em (also known as Omaha holdem or simply Omaha) is a community card poker game similar to Texas hold 'em, where each player is dealt four cards and must make his or her best hand using exactly two of them, plus exactly three of the five community cards. The exact origin of the game is unknown, but casino executive Robert Turner first brought Omaha into a casino setting when he introduced the game to Bill Boyd, who offered it as a game at the Las Vegas Golden Nugget Casino (calling it "Nugget Hold'em"). Omaha uses a 52-card French deck. Limit Omaha hold 'em 8-or-better is the "O" game featured in H.O.R.S.E. Both limit Omaha/8 and pot limit Omaha high are featured in the 8-Game.

Omaha hold 'em gets its name from two types of games.

In the original Omaha poker game, players were only dealt two hole cards and had to use both to make a hand combined with community cards. This version of Omaha is defined in the glossary of "Super/System" (under Omaha) as being interchangeable with "Tight hold 'em". Across all the variations of the game, the requirement of using exactly two hole cards is the only consistent rule. The "Omaha" part of the name represents this aspect of the game.

"Hold'em" refers to a game using community cards that are shared by all players. This is opposed to Draw games where each player's hand is composed only by hole cards and Stud games where each player hand contains both non-community cards that are visible to the other players and concealed hole cards.

In North American casinos, the term "Omaha" can refer to several poker games. The original game is also commonly known as "Omaha high". A high-low split version called "Omaha Hi-Lo", or sometimes "Omaha eight-or-better" or "Omaha/8", is also played. In Europe, "Omaha" still typically refers to the high version of the game, usually played pot-limit. Pot-limit Omaha is often abbreviated as "PLO." Pot-limit and no-limit Omaha eight-or-better can be found in some casinos and online, though no-limit is rarer.

It is often said that Omaha is a game of "the nuts", i.e. the best possible high or low hand, because it frequently takes "the nuts" to win a showdown. It is also a game where between the cards in his hand and the community cards a player may have drawing possibilities to multiple different types of holdings. For example, a player may have both a draw to a flush and a full house using different combinations of cards. At times, even seasoned players may need additional time to figure what draws are possible for their hand.

The basic differences between Omaha and Texas hold 'em are these: first, each player is dealt four hole cards instead of two. The betting rounds and layout of community cards are identical. At showdown, each player's hand is the best five-card hand made from "exactly three" of the five cards on the board, plus "exactly two" of the player's own cards. Unlike Texas hold 'em, a player cannot play four or five of the cards on the board with fewer than two of his own, nor can a player use three or four hole cards to disguise a strong hand.

Some specific things to notice about Omaha hands are:

In Omaha hi-low split-8 or better (simply Omaha/8), each player makes a separate five-card high hand and five-card ace-to-five low hand (eight-high or lower to qualify), and the pot is split between the high and low (which may be the same player). To qualify for low, a player must be able to play an 8-7-6-5-4 or lower (this is why it is called "eight or better"). A few casinos play with a 9-low qualifier instead, but this is rare. Each player can play any two of his four hole cards to make his high hand, and any two of his four hole cards to make his low hand. If there is no qualifying low hand, the high hand wins ("scoops") the whole pot. This game is usually played in the fixed limit version, although pot limit Omaha/8 is becoming more popular. A few low-stakes online tournaments feature no limit Omaha/8.

The brief explanation above belies the complexity of the game, so a number of examples will be useful here to clarify it. The table below shows a five-card board of community cards at the end of play, and then lists for each player the initial private four-card hand dealt to him or her, and the best five-card high hand and low hand each player can play on showdown:

In the deal above, Chris wins the high-hand half of the pot with his J-high straight, and Bryan and Eve split the low half (getting a quarter of the pot each) with 7-5-3-2-A.

Some specific things to notice about Omaha/8 hands are:

Pot-limit Omaha (shortened to PLO) is popular in Europe, online, and in high-stakes "mixed games" played in some American casinos. It is more often played high only, but can also be played high-low. Even more so than Limit Omaha Hi-Lo, PLO is a game of drawing, if you are drawing, to the nut hand. Second best flushes and straights can be, and frequently are, beaten. Furthermore, because of the exponential growth of the pot size in pot-limit play, seeing one of these hands to the end can be very expensive and carry immense reverse implied odds.

In poker, an out is any unseen card in the deck that will give a player the best hand. A wrap is a straight draw with 9 or more outs. It’s called a wrap because the player’s hole cards are said to wrap-around the board cards. In hold-em, where players have 2 hole cards, the most straight outs possible is 8. But in Omaha, there are 4 hole cards and this results in straight draws that can have up to 20 outs. An example of a twenty out wrap is on a flop of . To hit a straight one of the following cards is needed: .

A desirable hand to have in PLO is the current best hand with a redraw. For example, if the board is , and the player has , then not only do they have the current best hand possible (their ace-king makes the ace-high straight), but they also have a redraw with the two queens in their hand because if the board pairs, they will make a full house, or four queens. would be an even better hand because it has flush and royal flush redraws as well. In fact, with the board, is approximately an 80-20 money favorite over a random hand containing ace-king (see freerolling). Even a pair of queens with any two spades is better than 55-45 against a random ace-king hand.

The most common variations of Pot Limit Omaha high are Five-card Omaha, commonly referred as "Big O" very popular in Southeastern United States as a home game and Six-card Omaha or 6-O which can be found in many casinos across the UK. Some online poker rooms support Five-card Omaha, Six-card Omaha and Courchevel.

"Big O" (occasionally called Five-card Omaha or 5-O) began appearing in Southern California in 2008, and had spread to most of the card rooms in the area by the end of the year.

Sometimes the high-low split game is played with a 9 or a 7-high qualifier instead of 8-high. It can also be played with five cards dealt to each player instead of four. In that case, the same rules for making a hand apply: exactly two from the player's hand, and exactly three from the board.

In the game of Courchevel, players are dealt five hole cards rather than four. Simultaneously, the first community card is dealt. Following an opening round of betting, two additional community cards are dealt, creating a 3-card flop, where the structure of the game is then identical to standard Omaha. Still, exactly two of the five hole cards must be used. Courchevel is popular in France but its popularity has expanded in other parts of Europe, particularly the United Kingdom.




</doc>
<doc id="23184" url="https://en.wikipedia.org/wiki?curid=23184" title="Aggression (poker)">
Aggression (poker)

In the game of poker, opens and raises are considered aggressive plays, while calls and checks are considered passive (though a check-raise would be considered a very aggressive play). It is said that "aggression has its own value", meaning that often aggressive plays can make money with weak hands because of bluff value. In general, opponents must respond to aggressive play by playing more loosely, which offers more opportunities to make mistakes.

While it is true that aggressive play is generally superior to passive play, using any play exclusively can lead to predictability. A player who is constantly aggressive and plays many inferior hands is called a "maniac", and skilled players will take advantage of him by calling him more often, using isolation plays, and by other means.

If a player is not aggressive with his weaker hands, the opponents can safely fold whenever the player does bet or raise. The appropriate amount of aggression can be computed using game theory, and depends on the game being played and the tendencies of the opponents.

Most theorists, like David Sklansky and Doyle Brunson, suggest aggression as an important tool. Aggressive play should not be confused with loose play. Loose players may play passively, resulting in a calling station, while tight players may play aggressively, referred to as a TAG. Aggression is called for in particular circumstances. Very strong starting hands should be played very aggressively most of the time. A very strong propositional hand – one that is more likely to win with a straight or a flush – is one of the hands that can be played for effect with an aggressive style. Such aggression is deceptive, as the low and unpaired ranks of the starting hand require much improvement to win. This is beneficial for two reasons:

The second reasoning is what is known as "advertising" in poker. It can be very profitable for a player to convince the other players at the table that he is willing to gamble with less than premium cards. The result is larger pots when the aggressive player has tremendously strong hands.



</doc>
<doc id="23189" url="https://en.wikipedia.org/wiki?curid=23189" title="Shuffling">
Shuffling

Shuffling is a procedure used to randomize a deck of playing cards to provide an element of chance in card games. Shuffling is often followed by a cut, to help ensure that the shuffler has not manipulated the outcome.

One of the easiest shuffles to accomplish after a little practice is the overhand shuffle. Johan Jonasson wrote, "The overhand shuffle... is the shuffling technique where you gradually transfer the deck from, say, your right hand to your left hand by sliding off small packets from the top of the deck." In detail as normally performed, with the pack initially held in the left hand (say), most of the cards are grasped as a group from the bottom of the pack between the thumb and fingers of the right hand and lifted clear of the small group that remains in the left hand. Small packets are then released from the right hand a packet at a time so that they drop on the top of the pack accumulating in the left hand. The process is repeated several times. The randomness of the whole shuffle is increased by the number of small packets in each shuffle and the number of repeat shuffles performed.

The overhand shuffle offers sufficient opportunity for sleight of hand techniques to be used to affect the ordering of cards, creating a stacked deck. The most common way that players cheat with the overhand shuffle is by having a card at the top or bottom of the pack that they require, and then slipping it to the bottom at the start of a shuffle (if it was on top to start), or leaving it as the last card in a shuffle and just dropping it on top (if it was originally on the bottom of the deck).

A common shuffling technique is called the "riffle," or "dovetail" shuffle or "leafing the cards", in which half of the deck is held in each hand with the thumbs inward, then cards are released by the thumbs so that they fall to the table interleaved. Many also lift the cards up after a riffle, forming what is called a bridge which puts the cards back into place; it can also be done by placing the halves flat on the table with their rear corners touching, then lifting the back edges with the thumbs while pushing the halves together. While this method is more difficult, it is often used in casinos because it minimizes the risk of exposing cards during the shuffle. There are two types of perfect riffle shuffles: if the top card moves to be second from the top then it is an in shuffle, otherwise it is known as an out shuffle (which preserves both the top and bottom cards).

The Gilbert–Shannon–Reeds model provides a mathematical model of the random outcomes of riffling, that has been shown experimentally to be a good fit to human shuffling and that forms the basis for a recommendation that card decks be riffled seven times in order to randomize them thoroughly.

Also known as the "Indian", "Kattar", "Kenchi" or "Kutti Shuffle" (Hindi for scissor). The deck is held face down, with the middle finger on one long edge and the thumb on the other on the bottom half of the deck. The other hand draws off a packet from the top of the deck. This packet is allowed to drop into the palm. The maneuver is repeated over and over, with newly drawn packets dropping onto previous ones, until the deck is all in the second hand. Indian shuffle differs from stripping in that all the action is in the hand "taking" the cards, whereas in stripping, the action is performed by the hand with the original deck, "giving" the cards to the resulting pile. This is the most common shuffling technique in Asia and other parts of the world, while the overhand shuffle is primarily used in Western countries.

Cards are simply dealt out into a number of piles, then the piles are stacked on top of each other. Though this is deterministic and does not randomize the cards at all, it ensures that cards that were next to each other are now separated. Some variations on the pile shuffle attempt to make it slightly random by dealing to the piles in a random order each circuit.

Also known as the Chemmy, Irish, scramble, beginner shuffle, smooshing, schwirsheling, et al or washing the cards, this involves simply spreading the cards out face down, and sliding them around and over each other with one's hands. Then the cards are moved into one pile so that they begin to intertwine and are then arranged back into a stack. This method is useful for beginners and small children or if one is inept at shuffling cards. However, the beginner shuffle requires a large surface for spreading out the cards and takes longer than the other methods.

The Mongean shuffle, or Monge's shuffle, is performed as follows (by a right-handed person): Start with the unshuffled deck in the left hand and transfer the top card to the right. Then repeatedly take the top card from the left hand and transfer it to the right, putting the second card at the top of the new deck, the third at the bottom, the fourth at the top, the fifth at the bottom, etc. The result, if one started with cards numbered consecutively formula_1, would be a deck with the cards in the following order: formula_2.

For a deck of given size, the number of Mongean shuffles that it takes to return a deck to starting position, is known . Twelve perfect Mongean shuffles restore a 52-card deck.

"Weaving" is the procedure of pushing the ends of two halves of a deck against each other in such a way that they naturally intertwine. Sometimes the deck is split into equal halves of 26 cards which are then pushed together in a certain way so as to make them perfectly interweave. This is known as a "Faro Shuffle".

The faro shuffle is performed by cutting the deck into two, preferably equal, packs in both hands as follows (right-handed):
The cards are held from above in the right and from below in the left hand. Separation of the deck is done simply lifting up half the cards with the right hand thumb slightly and pushing the left hand's packet forward away from the right hand. The two packets are often crossed and slammed into each other as to align them. They are then pushed together by the short sides and bent (either up or down). The cards then alternately fall into each other, much like a zipper. A flourish can be added by springing the packets together by applying pressure and bending them from above, as called the bridge finish. The faro is a controlled shuffle which does not randomize a deck when performed properly.

A perfect faro shuffle, where the cards are perfectly alternated, is considered one of the most difficult sleights by card magicians, simply because it requires the shuffler to be able to cut the deck into two equal packets and apply just the right amount of pressure when pushing the cards into each other. Performing eight perfect faro shuffles in a row restores the order of the deck to the original order only if there are 52 cards in the deck and if the original top and bottom cards remain in their positions (1st and 52nd) during the eight shuffles. If the top and bottom cards are weaved in during each shuffle, it takes 52 shuffles to return the deck back into original order (or 26 shuffles to reverse the order).

The Mexican spiral shuffle is performed by cyclic actions of moving the top card onto the table, then the new top card under the deck, the next onto the table, next under the deck, and so on until the last card is dealt onto the table. It takes quite a long time, compared with riffle or overhand shuffles, but allows other players to fully control cards which are on the table. The Mexican spiral shuffle was popular at the end of the 19th century in some areas of Mexico as a protection from gamblers and con men arriving from the United States.

Magicians, sleight-of-hand artists, and card cheats employ various methods of shuffling whereby the deck appears to have been shuffled fairly, when in reality one or more cards (up to and including the entire deck) stays in the same position. It is also possible, though generally considered very difficult, to "stack the deck" (place cards into a desirable order) by means of one or more riffle shuffles; this is called "riffle stacking".

Both performance magicians and card sharps regard the Zarrow shuffle as a particularly effective example of the false shuffle. In this shuffle, the entire deck remains in its original order, although spectators think they see an honest riffle shuffle.

Casinos often equip their tables with shuffling machines instead of having croupiers shuffle the cards, as it gives the casino a few advantages, including an increased complexity to the shuffle and therefore an increased difficulty for players to make predictions, even if they are collaborating with croupiers. The shuffling machines are carefully designed to avoid biasing the shuffle and are typically computer-controlled. Shuffling machines also save time that would otherwise be wasted on manual shuffling, thereby increasing the profitability of the table. These machines are also used to lessen repetitive-motion-stress injuries to a dealer.

Players with superstitions often regard with suspicion any electronic equipment, so casinos sometimes still have the croupiers perform the shuffling at tables that typically attract those crowds (Baccarat tables).

There are exactly 52 factorial (expressed in shorthand as 52!) possible orderings of the cards in a 52-card deck. In other words, there are 52 × 51 × 50 × 49 × ··· × 4 × 3 × 2 × 1 possible combinations of card sequence. This is approximately 8 possible orderings or specifically 80,658,175,170,943,878,571,660,636,856,403,766,975,289,505,440,883,277,824,000,000,000,000. The magnitude of this number means that it is exceedingly improbable that two randomly selected, truly randomized decks will be the same. However, while the exact sequence of all cards in a randomized deck is unpredictable, it may be possible to make some probabilistic predictions about a deck that is not sufficiently randomized.

The number of shuffles which are sufficient for a "good" level of randomness depends on the type of shuffle and the measure of "good enough randomness", which in turn depends on the game in question. For most games, four to seven riffle shuffles are sufficient: for unsuited games such as blackjack, four riffle shuffles are sufficient, while for suited games, seven riffle shuffles are necessary. There are some games, however, for which even seven riffle shuffles are insufficient.

In practice the number of shuffles required depends both on the quality of the shuffle and how significant non-randomness is, particularly how good the people playing are at noticing and using non-randomness. Two to four shuffles is good enough for casual play. But in club play, good bridge players take advantage of non-randomness after four shuffles, and top blackjack players supposedly track aces through the deck; this is known as "ace tracking", or more generally, as "shuffle tracking".

Following early research at Bell Labs, which was abandoned in 1955, the question of how many shuffles was required remained open until 1990, when it was convincingly solved as "seven shuffles," as elaborated below. Some results preceded this, and refinements have continued since.

A leading figure in the mathematics of shuffling is mathematician and magician Persi Diaconis, who began studying the question around 1970, and has authored many papers in the 1980s, 1990s, and 2000s on the subject with numerous co-authors. Most famous is , co-authored with mathematician Dave Bayer, which analyzed the Gilbert–Shannon–Reeds model of random riffle shuffling and concluded that the deck did not start to become random until five good riffle shuffles, and was truly random after seven, in the precise sense of variation distance described in Markov chain mixing time; of course, you would need more shuffles if your shuffling technique is poor. Recently, the work of Trefethen et al. has questioned some of Diaconis' results, concluding that six shuffles are enough. The difference hinges on how each measured the randomness of the deck. Diaconis used a very sensitive test of randomness, and therefore needed to shuffle more. Even more sensitive measures exist, and the question of what measure is best for specific card games is still open. Diaconis released a response indicating that you only need four shuffles for un-suited games such as blackjack.

On the other hand, variation distance may be too forgiving a measure and seven riffle shuffles may be many too few. For example, seven shuffles of a new deck leaves an 81% probability of winning New Age Solitaire where the probability is 50% with a uniform random deck. One sensitive test for randomness uses a standard deck without the jokers divided into suits with two suits in ascending order from ace to king, and the other two suits in reverse. (Many decks already come ordered this way when new.) After shuffling, the measure of randomness is the number of rising sequences that are left in each suit.

If a computer has access to purely random numbers, it is capable of generating a "perfect shuffle", a random permutation of the cards; beware that this terminology (an algorithm that perfectly randomizes the deck) differs from "a perfectly executed single shuffle", notably a perfectly interleaving faro shuffle. The Fisher–Yates shuffle, popularized by Donald Knuth, is simple (a few lines of code) and efficient (O("n") on an "n"-card deck, assuming constant time for fundamental steps) algorithm for doing this. Shuffling can be seen as the opposite of sorting.

There are other, less-desirable algorithms in common use. For example, one can assign a random number to each card, and then sort the cards in order of their random numbers. This will generate a random permutation, unless any of the random numbers generated are the same as any others (i.e. pairs, triplets etc.). This can be eliminated either by adjusting one of the pair's values randomly up or down by a small amount, or reduced to an arbitrarily low probability by choosing a sufficiently wide range of random number choices. If using efficient sorting such as mergesort or heapsort this is an O("n" log "n") average and worst-case algorithm.

These issues are of considerable commercial importance in online gambling, where the randomness of the shuffling of packs of simulated cards for online card games is crucial. For this reason, many online gambling sites provide descriptions of their shuffling algorithms and the sources of randomness used to drive these algorithms, with some gambling sites also providing auditors' reports of the performance of their systems.


Physical card shuffling:
Mathematics of shuffling:

Real world (historical) application:


</doc>
<doc id="23190" url="https://en.wikipedia.org/wiki?curid=23190" title="Cut (cards)">
Cut (cards)

In many card games, to cut the cards (or to cut the deck) is a procedure used just prior to the cards being dealt to the players. 

A common procedure is that after the cards have been shuffled, the dealer sets the cards face-down on the table near the player designated to make the cut, typically the player to the dealer's right. That player initiates a cut of the deck by taking a contiguous range of cards off the top of the deck and placing it face-down on the table farther from the dealer; the dealer completes the cut by taking the original bottom portion of the deck and placing it on top of the just-moved cards. Once the cut is complete, the dealer picks up the deck, straightens or "squares" it, and deals the cards.

Rules of procedure or etiquette may vary concerning who makes the cut, the minimum or maximum number of cards which may be cut off the top, whether the dealer or the cutter restacks the cards, whether a cut card is employed, and whether a cut is mandatory.

The practice of cutting is primarily a method of reducing the likelihood of someone cheating by manipulating the order of cards to gain advantage. Even if the dealer (or the shuffler, if he is not the dealer) does not plan on cheating, cutting will prevent suspicions, thus many rules require it. Some players also consider the cut to be lucky.

The contiguous section may also be taken from the middle of the deck. This is called "Scarne's cut", though in some settings this is considered poor etiquette or against the rules. A cut involving a very small number of cards, such as taking only the top card (or some cards from the bottom) as a cut, is often acceptable according to rules. The same is true when a player takes every top card save for one on the cut.

During informal card games, the dealer is typically not required to offer the cut, and even if offered, the designated player can decline the request. On the other hand, any player may specifically request to cut the cards before they are dealt. If a cut is requested by a player, it must be granted by the dealer.

In formal player dealt settings, such as in a casino or during a tournament, an offer to cut the deck is mandatory and the designated player must perform the cut, generally by inserting a cut card (a plastic card about the size of a playing card, usually solid-colored) into the deck; the dealer then makes the actual cut at that point in the deck. When the dealer is not a player (i.e. a casino employee), the cut is mandatory and is usually performed by the dealer. In this instance, the deck is cut onto the aforementioned cut card, and the cut completed; this prevents players from seeing the bottom card of the deck.

A cut should always be completed with one hand to limit possibility of a false cut.

Scarne's cut was developed by John Scarne during World War II to help protect servicemen against cheating by unscrupulous dealers. First one pulls out a portion of the middle of the stack and places it back on top of the deck; one then performs a regular cut described earlier.

It can be demonstrated that multiple top-to-bottom (non-Scarne's) cuts are equivalent to some single cut. In fact, knowing the size of the deck and the size of the cuts, the formula for the composite single cut is given as the sum of the sizes of the cuts modulo the size of the deck. For example, in a 10 card deck, if a 7 card cut and a 4 card cut are made, that is, 7 cards are moved from the top of the deck to the bottom and then the resulting top 4 cards are also moved to the bottom, then those two consecutive cuts are equivalent to a cut the size of (7 + 4 = 11 (mod 10)) = 1. The deck will be in the order (2,3...,10,1).

A false cut is a move used either in magic, or for cheating when playing card games. It appears to be a real cut, but leaves the deck in the same order as when it began. More sophisticated versions may make specific desired changes to the deck's order, while still appearing to be an innocuous normal cut.

There are many ways to accomplish a false cut, involving misdirection or using complex moves to conceal the real result.

Cutting cards is usually a prelude to a game, but it can be a game unto itself. Each player, in turn, removes a selection of cards from the top and reveals the bottom card to all the players, and then replaces the cards in the original position. Whoever has revealed the highest (or sometimes lowest) card is the winner. This is often used in an informal setting, much like flipping coins; it is also sometimes used to determine who will play first in a card game.

The command to "cut the cards", followed by someone literally chopping the deck in half with an axe, is a none-too-subtle gag that has been used many times in popular media, going back to at least the vaudeville days. Examples include Harpo Marx in "Horse Feathers", Curly Howard in "Ants in the Pantry", and Bugs Bunny in "Bugs Bunny Rides Again".


</doc>
<doc id="23193" url="https://en.wikipedia.org/wiki?curid=23193" title="Philology">
Philology

Philology is the study of language in oral and written historical sources; it is a combination of literary criticism, history, and linguistics. Philology is more commonly defined as the study of literary texts as well as oral and written records, the establishment of their authenticity and their original form, and the determination of their meaning. A person who pursues this kind of study is known as a philologist.

In older usage, especially British, philology is more general, covering comparative and historical linguistics.

Classical philology studies classical languages. Classical philology principally originated from the Library of Pergamum and the Library of Alexandria around the fourth century BCE, continued by Greeks and Romans throughout the Roman/Byzantine Empire. It was preserved and promoted during the Islamic Golden Age, and eventually resumed by European scholars of the Renaissance, where it was soon joined by philologies of other non-Asian (European) (Germanic, Celtic), Eurasian (Slavistics, etc.) and Asian (Arabic, Persian, Sanskrit, Chinese, etc.) languages. Indo-European studies involves the comparative philology of all Indo-European languages.

Philology, with its focus on historical development (diachronic analysis), is contrasted with linguistics due to Ferdinand de Saussure's insistence on the importance of synchronic analysis. The contrast continued with the emergence of structuralism and Chomskyan linguistics alongside its emphasis on syntax.
The term "philology" is derived from the Greek ("philología"), from the terms ("phílos") "love, affection, loved, beloved, dear, friend" and ("lógos") "word, articulation, reason", describing a love of learning, of literature, as well as of argument and reasoning, reflecting the range of activities included under the notion of . The term changed little with the Latin "philologia", and later entered the English language in the 16th century, from the Middle French "philologie", in the sense of "love of literature".

The adjective ("philólogos") meant "fond of discussion or argument, talkative", in Hellenistic Greek, also implying an excessive ("sophistic") preference of argument over the love of true wisdom, ("philósophos").

As an allegory of literary erudition, "philologia" appears in fifth-century postclassical literature (Martianus Capella, "De nuptiis Philologiae et Mercurii"), an idea revived in Late Medieval literature (Chaucer, Lydgate).

The meaning of "love of learning and literature" was narrowed to "the study of the historical development of languages" (historical linguistics) in 19th-century usage of the term. Due to the rapid progress made in understanding sound laws and language change, the "golden age of philology" lasted throughout the 19th century, or "from Giacomo Leopardi and Friedrich Schlegel to Nietzsche". In the Anglo-Saxon world, the term philology to describe work on languages and literatures, which had become synonymous with the practices of German scholars, was abandoned as a consequence of anti-German feeling following World War I. Most continental European countries still maintain the term to designate departments, colleges, position titles, and journals. J. R. R. Tolkien opposed the nationalist reaction against philological practices, claiming that "the philological instinct" was "universal as is the use of language". In British English usage, and in British academia, "philology" remains largely synonymous with "historical linguistics", while in US English, and US academia, the wider meaning of "study of a language's grammar, history and literary tradition" remains more widespread. Based on the harsh critique of Friedrich Nietzsche, US scholars since the 1980s have viewed philology as responsible for a narrowly scientistic study of language and literature.

The comparative linguistics branch of philology studies the relationship between languages. Similarities between Sanskrit and European languages were first noted in the early 16th century and led to speculation of a common ancestor language from which all these descended. It is now named Proto-Indo-European. Philology's interest in ancient languages led to the study of what were, in the 18th century, "exotic" languages, for the light they could cast on problems in understanding and deciphering the origins of older texts.

Philology also includes the study of texts and their history. It includes elements of textual criticism, trying to reconstruct an author's original text based on variant copies of manuscripts. This branch of research arose among Ancient scholars in the 4th century BC Greek-speaking world, who desired to establish a standard text of popular authors for the purposes of both sound interpretation and secure transmission. Since that time, the original principles of textual criticism have been improved and applied to other widely distributed texts such as the Bible. Scholars have tried to reconstruct the original readings of the Bible from the manuscript variants. This method was applied to Classical Studies and to medieval texts as a way to reconstruct the author's original work. The method produced so-called "critical editions", which provided a reconstructed text accompanied by a "critical apparatus", i.e., footnotes that listed the various manuscript variants available, enabling scholars to gain insight into the entire manuscript tradition and argue about the variants.

A related study method known as higher criticism studies the authorship, date, and provenance of text to place such text in historical context. As these philological issues are often inseparable from issues of interpretation, there is no clear-cut boundary between philology and hermeneutics. When text has a significant political or religious influence (such as the reconstruction of Biblical texts), scholars have difficulty reaching objective conclusions.

Some scholars avoid all critical methods of textual philology, especially in historical linguistics, where it is important to study the actual recorded materials. The movement known as New Philology has rejected textual criticism because it injects editorial interpretations into the text and destroys the integrity of the individual manuscript, hence damaging the reliability of the data. Supporters of New Philology insist on a strict "diplomatic" approach: a faithful rendering of the text exactly as found in the manuscript, without emendations.

Another branch of philology, cognitive philology, studies written and oral texts. Cognitive philology considers these oral texts as the results of human mental processes. This science compares the results of textual science with the results of experimental research of both psychology and artificial intelligence production systems.

In the case of Bronze Age literature, philology includes the prior decipherment of the language under study. This has notably been the case with the Egyptian, Sumerian, Assyrian, Hittite, Ugaritic and Luwian languages. Beginning with the famous decipherment and translation of the Rosetta Stone by Jean-François Champollion in 1822, a number of individuals attempted to decipher the writing systems of the Ancient Near East and Aegean. In the case of Old Persian and Mycenaean Greek, decipherment yielded older records of languages already known from slightly more recent traditions (Middle Persian and Alphabetic Greek).

Work on the ancient languages of the Near East progressed rapidly. In the mid-19th century, Henry Rawlinson and others deciphered the Behistun Inscription, which records the same text in Old Persian, Elamite, and Akkadian, using a variation of cuneiform for each language. The elucidation of cuneiform led to the decipherment of Sumerian. Hittite was deciphered in 1915 by Bedřich Hrozný.

Linear B, a script used in the ancient Aegean, was deciphered in 1952 by Michael Ventris, who demonstrated that it recorded an early form of Greek, now known as Mycenaean Greek. Linear A, the writing system that records the still-unknown language of the Minoans, resists deciphering, despite many attempts.

Work continues on scripts such as the Maya, with great progress since the initial breakthroughs of the phonetic approach championed by Yuri Knorozov and others in the 1950s. Since the late 20th century, the Maya code has been almost completely deciphered, and the Mayan languages are among the most documented and studied in Mesoamerica. The code is described as a logosyllabic style of writing, which could be used to fully express any spoken thought.

In the "Space Trilogy" by C.S. Lewis, the main character, Elwin Ransom, is a philologist - as was Lewis' close friend J. R. R. Tolkien.

Dr. Edward Morbius, one of the main characters in the science-fiction film "Forbidden Planet", is a philologist.

Moritz-Maria von Igelfeld, the main character in Alexander McCall Smith's 1997 comic novel "Portuguese Irregular Verbs" is a philologist, educated at Cambridge.

The main character in the Academy Award Nominee for Best Foreign Language film in 2012, "Footnote", is a Hebrew philologist, and a significant part of the film deals with his work.




</doc>
<doc id="23194" url="https://en.wikipedia.org/wiki?curid=23194" title="Phonetics">
Phonetics

Phonetics (pronounced ) is the branch of linguistics that studies the sounds of human speech, or—in the case of sign languages—the equivalent aspects of sign. It is concerned with the physical properties of speech sounds or signs (phones): their physiological production, acoustic properties, auditory perception, and neurophysiological status. Phonology, on the other hand, is concerned with the abstract, grammatical characterization of systems of sounds or signs.

In the case of oral languages, phonetics has three basic areas of study:


Phonetics was studied by 4th century BCE, and possibly as early as the 6th century BCE, in the Indian subcontinent, with Pāṇini's account of the place and manner of articulation of consonants in his treatise on Sanskrit. The major Indic alphabets today order their consonants according to Pāṇini's classification.

Modern phonetics begins with attempts—such as those of Joshua Steele (in "Prosodia Rationalis", 1779) and Alexander Melville Bell (in "Visible Speech", 1867)—to introduce systems of precise notation for speech sounds.

The study of phonetics grew quickly in the late 19th century partly due to the invention of the phonograph, which allowed the speech signal to be recorded. Phoneticians were able to replay the speech signal several times and apply acoustic filters to the signal. By doing so, they were able to more carefully deduce the acoustic nature of the speech signal.

Using an Edison phonograph, Ludimar Hermann investigated the spectral properties of vowels and consonants. It was in these papers that the term "formant" was first introduced. Hermann also played vowel recordings made with the Edison phonograph at different speeds in order to test Willis', and Wheatstone's theories of vowel production.

In contrast to phonetics, phonology is the study of how sounds and gestures pattern in and across languages, relating such concerns with other levels and aspects of language. Phonetics deals with the articulatory and acoustic properties of speech sounds, how they are produced, and how they are perceived. As part of this investigation, phoneticians may concern themselves with the physical properties of meaningful sound contrasts or the social meaning encoded in the speech signal (socio-phonetics) (e.g. gender, sexuality, ethnicity, etc.). However, a substantial portion of research in phonetics is not concerned with the meaningful elements in the speech signal.

While it is widely agreed that phonology is grounded in phonetics, phonology is a distinct branch of linguistics, concerned with sounds and gestures as abstract units (e.g., distinctive features, phonemes, morae, syllables, etc.) and their conditioned variation (via, e.g., allophonic rules, constraints, or derivational rules). Phonology has been argued to relate to phonetics via the set of distinctive features, which map the abstract representations of speech units to articulatory gestures, acoustic signals or perceptual representations.

Phonetics as a research discipline has three main branches:

Phonetic transcription is a system for transcribing sounds that occur in a language, whether oral or sign. The most widely known system of phonetic transcription, the International Phonetic Alphabet (IPA), provides a standardized set of symbols for oral phones. The standardized nature of the IPA enables its users to transcribe accurately and consistently the phones of different languages, dialects, and idiolects. The IPA is a useful tool not only for the study of phonetics, but also for language teaching, professional acting, and speech pathology.

Applications of phonetics include:

Studying phonetics involves not only learning theoretical material but also undergoing training in the production and perception of speech sounds. The latter is often known as "ear-training". Students must learn control of articulatory variables and develop their ability to recognize fine differences between different vowels and consonants. As part of the training, they must become expert in using phonetic symbols, usually those of the International Phonetic Alphabet.




</doc>
<doc id="23195" url="https://en.wikipedia.org/wiki?curid=23195" title="Petroleum">
Petroleum

Petroleum () is a naturally occurring, yellow-to-black liquid found in geological formations beneath the Earth's surface. It is commonly refined into various types of fuels. Components of petroleum are separated using a technique called fractional distillation i.e. separation of a liquid mixture into fractions differing in boiling point by means of distillation, typically using a fractionating column.

It consists of hydrocarbons of various molecular weights and other organic compounds. The name "petroleum" covers both naturally occurring unprocessed crude oil and petroleum products that are made up of refined crude oil. A fossil fuel, petroleum is formed when large quantities of dead organisms, usually zooplankton and algae, are buried underneath sedimentary rock and subjected to both intense heat and pressure.

Petroleum has mostly been recovered by oil drilling (natural petroleum springs are rare). Drilling is carried out after studies of structural geology (at the reservoir scale), sedimentary basin analysis, and reservoir characterisation (mainly in terms of the porosity and permeability of geologic reservoir structures) have been completed. It is refined and separated, most easily by distillation, into a large number of consumer products, from gasoline (petrol) and kerosene to asphalt and chemical reagents used to make plastics and pharmaceuticals. Petroleum is used in manufacturing a wide variety of materials, and it is estimated that the world consumes about 95 million barrels each day.

Concern over the depletion of the earth's finite reserves of oil, and the effect this would have on a society dependent on it, is a concept known as peak oil. The use of fossil fuels, such as petroleum, has a negative impact on Earth's biosphere, damaging ecosystems through events such as oil spills and releasing a range of pollutants into the air including ground-level ozone and sulfur dioxide from sulfur impurities in fossil fuels. The burning of fossil fuels plays a major role in the current episode of global warming.

The word "petroleum" comes from , "rock" and Latin "oleum", "oil" from .

The term was found (in the spelling "petraoleum") in 10th-century Old English sources. It was used in the treatise "De Natura Fossilium", published in 1546 by the German mineralogist Georg Bauer, also known as Georgius Agricola. In the 19th century, the term "petroleum" was often used to refer to mineral oils produced by distillation from mined organic solids such as cannel coal (and later oil shale), and refined oils produced from them; in the United Kingdom, storage (and later transport) of these oils were regulated by a series of Petroleum Acts, from the "Petroleum Act 1863" onwards.

Petroleum, in one form or another, has been used since ancient times, and is now important across society, including in economy, politics and technology. The rise in importance was due to the invention of the internal combustion engine, the rise in commercial aviation, and the importance of petroleum to industrial organic chemistry, particularly the synthesis of plastics, fertilisers, solvents, adhesives and pesticides.

More than 4000 years ago, according to Herodotus and Diodorus Siculus, asphalt was used in the construction of the walls and towers of Babylon; there were oil pits near Ardericca (near Babylon), and a pitch spring on Zacynthus. Great quantities of it were found on the banks of the river Issus, one of the tributaries of the Euphrates. Ancient Persian tablets indicate the medicinal and lighting uses of petroleum in the upper levels of their society.

The use of petroleum in ancient China dates back to more than 2000 years ago. In I Ching, one of the earliest Chinese writings cites that oil in its raw state, without refining, was first discovered, extracted, and used in China in the first century BCE. In addition, the Chinese were the first to use petroleum as fuel as the early as the fourth century BCE.

By 347 AD, oil was produced from bamboo-drilled wells in China. Early British explorers to Myanmar documented a flourishing oil extraction industry based in Yenangyaung that, in 1795, had hundreds of hand-dug wells under production.

Pechelbronn (Pitch fountain) is said to be the first European site where petroleum has been explored and used. The still active Erdpechquelle, a spring where petroleum appears mixed with water has been used since 1498, notably for medical purposes. Oil sands have been mined since the 18th century.

In Wietze in lower Saxony, natural asphalt/bitumen has been explored since the 18th century. Both in Pechelbronn as in Wietze, the coal industry dominated the petroleum technologies.

Chemist James Young noticed a natural petroleum seepage in the Riddings colliery at Alfreton, Derbyshire from which he distilled a light thin oil suitable for use as lamp oil, at the same time obtaining a more viscous oil suitable for lubricating machinery. In 1848 Young set up a small business refining the crude oil.

Young eventually succeeded, by distilling cannel coal at a low heat, in creating a fluid resembling petroleum, which when treated in the same way as the seep oil gave similar products. Young found that by slow distillation he could obtain a number of useful liquids from it, one of which he named "paraffine oil" because at low temperatures it congealed into a substance resembling paraffin wax.

The production of these oils and solid paraffin wax from coal formed the subject of his patent dated 17 October 1850. In 1850 Young & Meldrum and Edward William Binney entered into partnership under the title of E.W. Binney & Co. at Bathgate in West Lothian and E. Meldrum & Co. at Glasgow; their works at Bathgate were completed in 1851 and became the first truly commercial oil-works in the world with the first modern oil refinery, using oil extracted from locally mined torbanite, shale, and bituminous coal to manufacture naphtha and lubricating oils; paraffin for fuel use and solid paraffin were not sold until 1856.
The world's first oil refinery was built in 1856 by Ignacy Łukasiewicz. His achievements also included the discovery of how to distill kerosene from seep oil, the invention of the modern kerosene lamp (1853), the introduction of the first modern street lamp in Europe (1853), and the construction of the world's first modern oil well (1854).

The demand for petroleum as a fuel for lighting in North America and around the world quickly grew. Edwin Drake's 1859 well near Titusville, Pennsylvania, is popularly considered the first modern well. Already 1858 Georg Christian Konrad Hunäus had found a significant amount of petroleum while drilling for lignite 1858 in Wietze, Germany. Wietze later provided about 80% of the German consumption in the Wilhelminian Era. The production stopped in 1963, but Wietze has hosted a Petroleum Museum since 1970.

Drake's well is probably singled out because it was drilled, not dug; because it used a steam engine; because there was a company associated with it; and because it touched off a major boom. However, there was considerable activity before Drake in various parts of the world in the mid-19th century. A group directed by Major Alexeyev of the Bakinskii Corps of Mining Engineers hand-drilled a well in the Baku region in 1848. There were engine-drilled wells in West Virginia in the same year as Drake's well. An early commercial well was hand dug in Poland in 1853, and another in nearby Romania in 1857. At around the same time the world's first, small, oil refinery was opened at Jasło in Poland, with a larger one opened at Ploiești in Romania shortly after. Romania is the first country in the world to have had its annual crude oil output officially recorded in international statistics: 275 tonnes for 1857.

The first commercial oil well in Canada became operational in 1858 at Oil Springs, Ontario (then Canada West). Businessman James Miller Williams dug several wells between 1855 and 1858 before discovering a rich reserve of oil four metres below ground. Williams extracted 1.5 million litres of crude oil by 1860, refining much of it into kerosene lamp oil. Williams's well became commercially viable a year before Drake's Pennsylvania operation and could be argued to be the first commercial oil well in North America. The discovery at Oil Springs touched off an oil boom which brought hundreds of speculators and workers to the area. Advances in drilling continued into 1862 when local driller Shaw reached a depth of 62 metres using the spring-pole drilling method. On January 16, 1862, after an explosion of natural gas Canada's first oil gusher came into production, shooting into the air at a recorded rate of 3,000 barrels per day. By the end of the 19th century the Russian Empire, particularly the Branobel company in Azerbaijan, had taken the lead in production.

Access to oil was and still is a major factor in several military conflicts of the twentieth century, including World War II, during which oil facilities were a major strategic asset and were extensively bombed. The German invasion of the Soviet Union included the goal to capture the Baku oilfields, as it would provide much needed oil-supplies for the German military which was suffering from blockades. Oil exploration in North America during the early 20th century later led to the US becoming the leading producer by mid-century. As petroleum production in the US peaked during the 1960s, however, the United States was surpassed by Saudi Arabia and the Soviet Union.

Today, about 90 percent of vehicular fuel needs are met by oil. Petroleum also makes up 40 percent of total energy consumption in the United States, but is responsible for only 1 percent of electricity generation. Petroleum's worth as a portable, dense energy source powering the vast majority of vehicles and as the base of many industrial chemicals makes it one of the world's most important commodities. Viability of the oil commodity is controlled by several key parameters, number of vehicles in the world competing for fuel, quantity of oil exported to the world market (Export Land Model), net energy gain (economically useful energy provided minus energy consumed), political stability of oil exporting nations and ability to defend oil supply lines.ci

The top three oil producing countries are Russia, Saudi Arabia and the United States. About 80 percent of the world's readily accessible reserves are located in the Middle East, with 62.5 percent coming from the Arab 5: Saudi Arabia, United Arab Emirates, Iraq, Qatar and Kuwait. A large portion of the world's total oil exists as unconventional sources, such as bitumen in Athabasca oil sands and extra heavy oil in the Orinoco Belt. While significant volumes of oil are extracted from oil sands, particularly in Canada, logistical and technical hurdles remain, as oil extraction requires large amounts of heat and water, making its net energy content quite low relative to conventional crude oil. Thus, Canada's oil sands are not expected to provide more than a few million barrels per day in the foreseeable future.

In its strictest sense, petroleum includes only crude oil, but in common usage it includes all liquid, gaseous and solid hydrocarbons. Under surface pressure and temperature conditions, lighter hydrocarbons methane, ethane, propane and butane occur as gases, while pentane and heavier hydrocarbons are in the form of liquids or solids. However, in an underground oil reservoir the proportions of gas, liquid, and solid depend on subsurface conditions and on the phase diagram of the petroleum mixture.

An oil well produces predominantly crude oil, with some natural gas dissolved in it. Because the pressure is lower at the surface than underground, some of the gas will come out of solution and be recovered (or burned) as "associated gas" or "solution gas". A gas well produces predominantly natural gas. However, because the underground temperature and pressure are higher than at the surface, the gas may contain heavier hydrocarbons such as pentane, hexane, and heptane in the gaseous state. At surface conditions these will condense out of the gas to form "natural gas condensate", often shortened to "condensate." Condensate resembles gasoline in appearance and is similar in composition to some volatile light crude oils.

The proportion of light hydrocarbons in the petroleum mixture varies greatly among different oil fields, ranging from as much as 97 percent by weight in the lighter oils to as little as 50 percent in the heavier oils and bitumens.

The hydrocarbons in crude oil are mostly alkanes, cycloalkanes and various aromatic hydrocarbons, while the other organic compounds contain nitrogen, oxygen and sulfur, and trace amounts of metals such as iron, nickel, copper and vanadium. Many oil reservoirs contain live bacteria. The exact molecular composition of crude oil varies widely from formation to formation but the proportion of chemical elements varies over fairly narrow limits as follows:

Four different types of hydrocarbon molecules appear in crude oil. The relative percentage of each varies from oil to oil, determining the properties of each oil.

Crude oil varies greatly in appearance depending on its composition. It is usually black or dark brown (although it may be yellowish, reddish, or even greenish). In the reservoir it is usually found in association with natural gas, which being lighter forms a "gas cap" over the petroleum, and saline water which, being heavier than most forms of crude oil, generally sinks beneath it. Crude oil may also be found in a semi-solid form mixed with sand and water, as in the Athabasca oil sands in Canada, where it is usually referred to as crude bitumen. In Canada, bitumen is considered a sticky, black, tar-like form of crude oil which is so thick and heavy that it must be heated or diluted before it will flow. Venezuela also has large amounts of oil in the Orinoco oil sands, although the hydrocarbons trapped in them are more fluid than in Canada and are usually called extra heavy oil. These oil sands resources are called unconventional oil to distinguish them from oil which can be extracted using traditional oil well methods. Between them, Canada and Venezuela contain an estimated of bitumen and extra-heavy oil, about twice the volume of the world's reserves of conventional oil.

Petroleum is used mostly, by volume, for producing fuel oil and gasoline, both important ""primary energy"" sources. 84 percent by volume of the hydrocarbons present in petroleum is converted into energy-rich fuels (petroleum-based fuels), including gasoline, diesel, jet, heating, and other fuel oils, and liquefied petroleum gas. The lighter grades of crude oil produce the best yields of these products, but as the world's reserves of light and medium oil are depleted, oil refineries are increasingly having to process heavy oil and bitumen, and use more complex and expensive methods to produce the products required. Because heavier crude oils have too much carbon and not enough hydrogen, these processes generally involve removing carbon from or adding hydrogen to the molecules, and using fluid catalytic cracking to convert the longer, more complex molecules in the oil to the shorter, simpler ones in the fuels.

Due to its high energy density, easy transportability and relative abundance, oil has become the world's most important source of energy since the mid-1950s. Petroleum is also the raw material for many chemical products, including pharmaceuticals, solvents, fertilizers, pesticides, and plastics; the 16 percent not used for energy production is converted into these other materials. Petroleum is found in porous rock formations in the upper strata of some areas of the Earth's crust. There is also petroleum in oil sands (tar sands). Known oil reserves are typically estimated at around 190 km (1.2 trillion (short scale) barrels) without oil sands, or 595 km (3.74 trillion barrels) with oil sands. Consumption is currently around per day, or 4.9 km per year, yielding a remaining oil supply of only about 120 years, if current demand remains static.

Petroleum is a mixture of a very large number of different hydrocarbons; the most commonly found molecules are alkanes (paraffins), cycloalkanes (naphthenes), aromatic hydrocarbons, or more complicated chemicals like asphaltenes. Each petroleum variety has a unique mix of molecules, which define its physical and chemical properties, like color and viscosity.

The "alkanes", also known as "paraffins", are saturated hydrocarbons with straight or branched chains which contain only carbon and hydrogen and have the general formula CH. They generally have from 5 to 40 carbon atoms per molecule, although trace amounts of shorter or longer molecules may be present in the mixture.

The alkanes from pentane (CH) to octane (CH) are refined into gasoline, the ones from nonane (CH) to hexadecane (CH) into diesel fuel, kerosene and jet fuel. Alkanes with more than 16 carbon atoms can be refined into fuel oil and lubricating oil. At the heavier end of the range, paraffin wax is an alkane with approximately 25 carbon atoms, while asphalt has 35 and up, although these are usually cracked by modern refineries into more valuable products. The shortest molecules, those with four or fewer carbon atoms, are in a gaseous state at room temperature. They are the petroleum gases. Depending on demand and the cost of recovery, these gases are either flared off, sold as liquefied petroleum gas under pressure, or used to power the refinery's own burners. During the winter, butane (CH), is blended into the gasoline pool at high rates, because its high vapor pressure assists with cold starts. Liquified under pressure slightly above atmospheric, it is best known for powering cigarette lighters, but it is also a main fuel source for many developing countries. Propane can be liquified under modest pressure, and is consumed for just about every application relying on petroleum for energy, from cooking to heating to transportation.

The "cycloalkanes", also known as "naphthenes", are saturated hydrocarbons which have one or more carbon rings to which hydrogen atoms are attached according to the formula CH. Cycloalkanes have similar properties to alkanes but have higher boiling points.

The "aromatic hydrocarbons" are unsaturated hydrocarbons which have one or more planar six-carbon rings called benzene rings, to which hydrogen atoms are attached with the formula CH. They tend to burn with a sooty flame, and many have a sweet aroma. Some are carcinogenic.

These different molecules are separated by fractional distillation at an oil refinery to produce gasoline, jet fuel, kerosene, and other hydrocarbons. For example, 2,2,4-trimethylpentane (isooctane), widely used in gasoline, has a chemical formula of CH and it reacts with oxygen exothermically:

The number of various molecules in an oil sample can be determined by laboratory analysis. The molecules are typically extracted in a solvent, then separated in a gas chromatograph, and finally determined with a suitable detector, such as a flame ionization detector or a mass spectrometer. Due to the large number of co-eluted hydrocarbons within oil, many cannot be resolved by traditional gas chromatography and typically appear as a hump in the chromatogram. This unresolved complex mixture (UCM) of hydrocarbons is particularly apparent when analysing weathered oils and extracts from tissues of organisms exposed to oil. Some of the component of oil will mix with water: the water associated fraction of the oil.

Incomplete combustion of petroleum or gasoline results in production of toxic byproducts. Too little oxygen during combustion results in the formation of carbon monoxide. Due to the high temperatures and high pressures involved, exhaust gases from gasoline combustion in car engines usually include nitrogen oxides which are responsible for creation of photochemical smog.

At a constant volume, the heat of combustion of a petroleum product can be approximated as follows:

where formula_2 is measured in calories per gram and formula_3 is the specific gravity at .

The thermal conductivity of petroleum based liquids can be modeled as follows:

where formula_5 is measured in BTU°Fhrft , formula_6 is measured in °F and formula_7 is degrees API gravity.

The specific heat of petroleum oils can be modeled as follows:

where formula_9 is measured in BTU/(lb °F), formula_6 is the temperature in Fahrenheit and formula_3 is the specific gravity at .

In units of kcal/(kg·°C), the formula is:

where the temperature formula_6 is in Celsius and formula_3 is the specific gravity at 15 °C.

The latent heat of vaporization can be modeled under atmospheric conditions as follows:

where formula_16 is measured in BTU/lb, formula_6 is measured in °F and formula_3 is the specific gravity at .

In units of kcal/kg, the formula is:

where the temperature formula_6 is in Celsius and formula_3 is the specific gravity at 15 °C.

Petroleum is a fossil fuel derived from ancient fossilized organic materials, such as zooplankton and algae. Vast amounts of these remains settled to sea or lake bottoms where they were covered in stagnant water (water with no dissolved oxygen) or sediments such as mud and silt faster than they could decompose aerobically. Approximately 1 m below this sediment or water oxygen concentration was low, below 0.1 mg/l, and anoxic conditions existed. Temperatures also remained constant.

As further layers settled to the sea or lake bed, intense heat and pressure built up in the lower regions. This process caused the organic matter to change, first into a waxy material known as kerogen, found in various oil shales around the world, and then with more heat into liquid and gaseous hydrocarbons via a process known as catagenesis. Formation of petroleum occurs from hydrocarbon pyrolysis in a variety of mainly endothermic reactions at high temperature or pressure, or both. These phases are described in detail below.

In the absence of plentiful oxygen, "aerobic" bacteria were prevented from decaying the organic matter after it was buried under a layer of sediment or water. However, "anaerobic" bacteria were able to reduce sulfates and nitrates among the matter to HS and N respectively by using the matter as a source for other reactants. Due to such anaerobic bacteria, at first this matter began to break apart mostly via hydrolysis: polysaccharides and proteins were hydrolyzed to simple sugars and amino acids respectively. These were further anaerobically oxidized at an accelerated rate by the enzymes of the bacteria: e.g. amino acids went through oxidative deamination to imino acids, which in turn reacted further to ammonia and α-keto acids. Monosaccharides in turn ultimately decayed to CO and methane. The anaerobic decay products of amino acids, monosaccharides, phenols and aldehydes combined to fulvic acids. Fats and waxes were not extensively hydrolyzed under these mild conditions.

Some phenolic compounds produced from previous reactions worked as bactericides and actinomycetales order of bacteria produced antibiotic compounds (e.g. streptomycin). Thus the action of anaerobic bacteria ceased at about 10 m below the water or sediment. The mixture at this depth contained fulvic acids, unreacted and partially reacted fats and waxes, slightly modified lignin, resins and other hydrocarbons. As more layers of organic matter settled to the sea or lake bed, intense heat and pressure built up in the lower regions. As a consequence, compounds of this mixture the began to combine in poorly understood ways to kerogen. Combination happened in a similar fashion as phenol and formaldehyde molecules react to urea-formaldehyde resins, but kerogen formation occurred in a more complex manner due to a bigger variety of reactants. The total process of kerogen formation from the beginning of anaerobic decay is called diagenesis, a word that means a transformation of materials by dissolution and recombination of their constituents.

Kerogen formation continued to the depth of about 1 km from the Earth's surface where temperatures may reach around 50 °C. Kerogen formation represents a halfway point between organic matter and fossil fuels: kerogen can be exposed to oxygen, oxidize and thus be lost or it could be buried deeper inside the Earth's crust and be subjected to conditions which allow it to slowly transform into fossil fuels like petroleum. The latter happened through catagenesis in which the reactions were mostly radical rearrangements of kerogen. These reactions took thousands to millions of years and no external reactants were involved. Due to radical nature of these reactions, kerogen reacted towards two classes of products: those with low H/C ratio (anthracene or products similar to it) and those with high H/C ratio (methane or products similar to it); i.e. carbon-rich or hydrogen-rich products. Because catagenesis was closed off from external reactants, the resulting composition of the fuel mixture was dependent on the composition of the kerogen via reaction stoichiometry. 3 main types of kerogen exist: type I (algal), II (liptinic) and III (humic), which were formed mainly from algae, plankton and woody plants (this term includes trees, shrubs and lianas) respectively.

Catagenesis was pyrolytic despite of the fact that it happened at relatively low temperatures (when compared to commercial pyrolysis plants) of 60 to several hundred °C. Pyrolysis was possible because of the long reaction times involved. Heat for catagenesis came from the decomposition of radioactive materials of the crust, especially K, Th, U and U. The heat varied with geothermal gradient and was typically 10-30 °C per km of depth from the Earth's surface. Unusual magma intrusions, however, could have created greater localized heating.

Geologists often refer to the temperature range in which oil forms as an "oil window". Below the minimum temperature oil remains trapped in the form of kerogen. Above the maximum temperature the oil is converted to natural gas through the process of thermal cracking. Sometimes, oil formed at extreme depths may migrate and become trapped at a much shallower level. The Athabasca Oil Sands are one example of this.

An alternative mechanism to the one described above was proposed by Russian scientists in the mid-1850s, the hypothesis of abiogenic petroleum origin (petroleum formed by inorganic means), but this is contradicted by geological and geochemical evidence. Abiogenic sources of oil have been found, but never in commercially profitable amounts. "The controversy isn't over whether abiogenic oil reserves exist," said Larry Nation of the American Association of Petroleum Geologists. "The controversy is over how much they contribute to Earth's overall reserves and how much time and effort geologists should devote to seeking them out."

Three conditions must be present for oil reservoirs to form:

The reactions that produce oil and natural gas are often modeled as first order breakdown reactions, where hydrocarbons are broken down to oil and natural gas by a set of parallel reactions, and oil eventually breaks down to natural gas by another set of reactions. The latter set is regularly used in petrochemical plants and oil refineries.

Wells are drilled into oil reservoirs to extract the crude oil. "Natural lift" production methods that rely on the natural reservoir pressure to force the oil to the surface are usually sufficient for a while after reservoirs are first tapped. In some reservoirs, such as in the Middle East, the natural pressure is sufficient over a long time. The natural pressure in most reservoirs, however, eventually dissipates. Then the oil must be extracted using "artificial lift" means. Over time, these "primary" methods become less effective and "secondary" production methods may be used. A common secondary method is "waterflood" or injection of water into the reservoir to increase pressure and force the oil to the drilled shaft or "wellbore." Eventually "tertiary" or "enhanced" oil recovery methods may be used to increase the oil's flow characteristics by injecting steam, carbon dioxide and other gases or chemicals into the reservoir. In the United States, primary production methods account for less than 40 percent of the oil produced on a daily basis, secondary methods account for about half, and tertiary recovery the remaining 10 percent. Extracting oil (or "bitumen") from oil/tar sand and oil shale deposits requires mining the sand or shale and heating it in a vessel or retort, or using "in-situ" methods of injecting heated liquids into the deposit and then pumping the liquid back out saturated with oil.

Oil-eating bacteria biodegrade oil that has escaped to the surface. Oil sands are reservoirs of partially biodegraded oil still in the process of escaping and being biodegraded, but they contain so much migrating oil that, although most of it has escaped, vast amounts are still present—more than can be found in conventional oil reservoirs. The lighter fractions of the crude oil are destroyed first, resulting in reservoirs containing an extremely heavy form of crude oil, called crude bitumen in Canada, or extra-heavy crude oil in Venezuela. These two countries have the world's largest deposits of oil sands.

On the other hand, oil shales are source rocks that have not been exposed to heat or pressure long enough to convert their trapped hydrocarbons into crude oil. Technically speaking, oil shales are not always shales and do not contain oil, but are fined-grain sedimentary rocks containing an insoluble organic solid called kerogen. The kerogen in the rock can be converted into crude oil using heat and pressure to simulate natural processes. The method has been known for centuries and was patented in 1694 under British Crown Patent No. 330 covering, "A way to extract and make great quantities of pitch, tar, and oil out of a sort of stone." Although oil shales are found in many countries, the United States has the world's largest deposits.

The petroleum industry generally classifies crude oil by the geographic location it is produced in (e.g. West Texas Intermediate, Brent, or Oman), its API gravity (an oil industry measure of density), and its sulfur content. Crude oil may be considered "light" if it has low density or "heavy" if it has high density; and it may be referred to as sweet if it contains relatively little sulfur or "sour" if it contains substantial amounts of sulfur.

The geographic location is important because it affects transportation costs to the refinery. "Light" crude oil is more desirable than "heavy" oil since it produces a higher yield of gasoline, while "sweet" oil commands a higher price than "sour" oil because it has fewer environmental problems and requires less refining to meet sulfur standards imposed on fuels in consuming countries. Each crude oil has unique molecular characteristics which are revealed by the use of Crude oil assay analysis in petroleum laboratories.

Barrels from an area in which the crude oil's molecular characteristics have been determined and the oil has been classified are used as pricing references throughout the world. Some of the common reference crudes are:

There are declining amounts of these benchmark oils being produced each year, so other oils are more commonly what is actually delivered. While the reference price may be for West Texas Intermediate delivered at Cushing, the actual oil being traded may be a discounted Canadian heavy oil—Western Canadian Select— delivered at Hardisty, Alberta, and for a Brent Blend delivered at Shetland, it may be a discounted Russian Export Blend delivered at the port of Primorsk.

The petroleum industry is involved in the global processes of exploration, extraction, refining, transporting (often with oil tankers and pipelines), and marketing petroleum products. The largest volume products of the industry are fuel oil and gasoline. Petroleum is also the raw material for many chemical products, including pharmaceuticals, solvents, fertilizers, pesticides, and plastics. The industry is usually divided into three major components: upstream, midstream and downstream. Midstream operations are usually included in the downstream category.

Petroleum is vital to many industries, and is of importance to the maintenance of industrialized civilization itself, and thus is a critical concern to many nations. Oil accounts for a large percentage of the world's energy consumption, ranging from a low of 32 percent for Europe and Asia, up to a high of 53 percent for the Middle East, South and Central America (44%), Africa (41%), and North America (40%). The world at large consumes 30 billion barrels (4.8 km³) of oil per year, and the top oil consumers largely consist of developed nations. In fact, 24 percent of the oil consumed in 2004 went to the United States alone, though by 2007 this had dropped to 21 percent of world oil consumed.

In the US, in the states of Arizona, California, Hawaii, Nevada, Oregon and Washington, the Western States Petroleum Association (WSPA) represents companies responsible for producing, distributing, refining, transporting and marketing petroleum. This non-profit trade association was founded in 1907, and is the oldest petroleum trade association in the United States.

In the 1950s, shipping costs made up 33 percent of the price of oil transported from the Persian Gulf to USA, but due to the development of supertankers in the 1970s, the cost of shipping dropped to only 5 percent of the price of Persian oil in USA. Due to the increase of the value of the crude oil during the last 30 years, the share of the shipping cost on the final cost of the delivered commodity was less than 3% in 2010. For example, in 2010 the shipping cost from the Persian Gulf to the USA was in the range of 20 $/t and the cost of the delivered crude oil around 800 $/t.

After the collapse of the OPEC-administered pricing system in 1985, and a short-lived experiment with netback pricing, oil-exporting countries adopted a market-linked pricing mechanism. First adopted by PEMEX in 1986, market-linked pricing was widely accepted, and by 1988 became and still is the main method for pricing crude oil in international trade. The current reference, or pricing markers, are Brent, WTI, and Dubai/Oman.

The chemical structure of petroleum is heterogeneous, composed of hydrocarbon chains of different lengths. Because of this, petroleum may be taken to oil refineries and the hydrocarbon chemicals separated by distillation and treated by other chemical processes, to be used for a variety of purposes. The total cost of a plant is about 9 billion dollars per plant.

The most common distillation fractions of petroleum are fuels. Fuels include (by increasing boiling temperature range):

Petroleum classification according to chemical composition.
Certain types of resultant hydrocarbons may be mixed with other non-hydrocarbons, to create other end products:

Since the 1940s, agricultural productivity has increased dramatically, due largely to the increased use of energy-intensive mechanization, fertilizers and pesticides.

According to the US Energy Information Administration (EIA) estimate for 2011, the world consumes 87.421 million barrels of oil each day.
This table orders the amount of petroleum consumed in 2011 in thousand barrels (1000 bbl) per day and in thousand cubic metres (1000 m) per day:

Source: US Energy Information Administration

Population Data:

In petroleum industry parlance, "production" refers to the quantity of crude extracted from reserves, not the literal creation of the product.

In order of net exports in 2011, 2009 and 2006 in thousand bbl/d and thousand m³/d:

Source: US Energy Information Administration

Countries whose oil production is 10% or less of their consumption.
Source: CIA World Factbook

Because petroleum is a naturally occurring substance, its presence in the environment need not be the result of human causes such as accidents and routine activities (seismic exploration, drilling, extraction, refining and combustion). Phenomena such as seeps and tar pits are examples of areas that petroleum affects without man's involvement. Regardless of source, petroleum's effects when released into the environment are similar.

Ocean acidification is the increase in the acidity of the Earth's oceans caused by the uptake of carbon dioxide () from the atmosphere. This increase in acidity inhibits all marine life – having a greater impact on smaller organisms as well as shelled organisms (see scallops).

When burned, petroleum releases carbon dioxide, a greenhouse gas. Along with the burning of coal, petroleum combustion may be the largest contributor to the increase in atmospheric CO. Atmospheric CO has risen over the last 150 years to current levels of over 390 ppmv, from the 180 – 300 ppmv of the prior 800 thousand years This rise in temperature may have reduced the Arctic ice cap to , smaller than ever recorded. Because of this melt, more oil reserves have been revealed. About 13 percent of the world's undiscovered oil resides in the Arctic.

Oil extraction is simply the removal of oil from the reservoir (oil pool). Oil is often recovered as a water-in-oil emulsion, and specialty chemicals called demulsifiers are used to separate the oil from water. Oil extraction is costly and sometimes environmentally damaging. Offshore exploration and extraction of oil disturbs the surrounding marine environment.

Crude oil and refined fuel spills from tanker ship accidents have damaged natural ecosystems in Alaska, the Gulf of Mexico, the Galápagos Islands, France and many other places.

The quantity of oil spilled during accidents has ranged from a few hundred tons to several hundred thousand tons (e.g., Deepwater Horizon oil spill, SS Atlantic Empress, Amoco Cadiz). Smaller spills have already proven to have a great impact on ecosystems, such as the "Exxon Valdez" oil spill.

Oil spills at sea are generally much more damaging than those on land, since they can spread for hundreds of nautical miles in a thin oil slick which can cover beaches with a thin coating of oil. This can kill sea birds, mammals, shellfish and other organisms it coats. Oil spills on land are more readily containable if a makeshift earth dam can be rapidly bulldozed around the spill site before most of the oil escapes, and land animals can avoid the oil more easily.

Control of oil spills is difficult, requires ad hoc methods, and often a large amount of manpower. The dropping of bombs and incendiary devices from aircraft on the wreck produced poor results; modern techniques would include pumping the oil from the wreck, like in the "Prestige" oil spill or the "Erika" oil spill.

Though crude oil is predominantly composed of various hydrocarbons, certain nitrogen heterocylic compounds, such as pyridine, picoline, and quinoline are reported as contaminants associated with crude oil, as well as facilities processing oil shale or coal, and have also been found at legacy wood treatment sites. These compounds have a very high water solubility, and thus tend to dissolve and move with water. Certain naturally occurring bacteria, such as "Micrococcus", "Arthrobacter", and "Rhodococcus" have been shown to degrade these contaminants.

A tarball is a blob of crude oil (not to be confused with tar, which is a man-made product derived from pine trees or refined from petroleum) which has been weathered after floating in the ocean. Tarballs are an aquatic pollutant in most environments, although they can occur naturally, for example in the Santa Barbara Channel of California or in the Gulf of Mexico off Texas. Their concentration and features have been used to assess the extent of oil spills. Their composition can be used to identify their sources of origin, and tarballs themselves may be dispersed over long distances by deep sea currents. They are slowly decomposed by bacteria, including "Chromobacterium violaceum", "Cladosporium resinae", "Bacillus submarinus", "Micrococcus varians", "Pseudomonas aeruginosa", "Candida marina" and "Saccharomyces estuari".

James S. Robbins has argued that the advent of petroleum-refined kerosene saved some species of great whales from extinction by providing an inexpensive substitute for whale oil, thus eliminating the economic imperative for open-boat whaling.

In the United States in 2007 about 70 percent of petroleum was used for transportation (e.g. gasoline, diesel, jet fuel), 24 percent by industry (e.g. production of plastics), 5 percent for residential and commercial uses, and 2 percent for electricity production. Outside of the US, a higher proportion of petroleum tends to be used for electricity.

Alternative fuel vehicles refers to both:


Biological feedstocks do exist for industrial uses such as Bioplastic production.

In oil producing countries with little refinery capacity, oil is sometimes burned to produce electricity. Renewable energy technologies such as solar power, wind power, micro hydro, biomass and biofuels are used, but the primary alternatives remain large-scale hydroelectricity, nuclear and coal-fired generation.

Consumption in the twentieth and twenty-first centuries has been abundantly pushed by automobile sector growth. The 1985–2003 oil glut even fueled the sales of low fuel economy vehicles in OECD countries. The 2008 economic crisis seems to have had some impact on the sales of such vehicles; still, in 2008 oil consumption showed a small increase.

In 2016 Goldman Sachs predicted lower demand for oil due to emerging economies concerns, especially China. The BRICS (Brasil, Russia, India, China, South Africa) countries might also kick in, as China briefly was the first automobile market in December 2009. The immediate outlook still hints upwards. In the long term, uncertainties linger; the OPEC believes that the OECD countries will push low consumption policies at some point in the future; when that happens, it will definitely curb oil sales, and both OPEC and the Energy Information Administration (EIA) kept lowering their 2020 consumption estimates during the past five years. A detailed review of International Energy Agency oil projections have revealed that revisions of world oil production, price and investments have been motivated by a combination of demand and supply factors. All together, Non-OPEC conventional projections have been fairly stable the last 15 years, while downward revisions were mainly allocated to OPEC. Recent upward revisions are primarily a result of US tight oil.

Production will also face an increasingly complex situation; while OPEC countries still have large reserves at low production prices, newly found reservoirs often lead to higher prices; offshore giants such as Tupi, Guara and Tiber demand high investments and ever-increasing technological abilities. Subsalt reservoirs such as Tupi were unknown in the twentieth century, mainly because the industry was unable to probe them. Enhanced Oil Recovery (EOR) techniques (example: DaQing, China ) will continue to play a major role in increasing the world's recoverable oil.

The expected availability of petroleum resources has always been around 35 years or even less since the start of the modern exploration. The oil constant, an insider pun in the German industry, refers to that effect.

Peak oil is a term applied to the projection that future petroleum production (whether for individual oil wells, entire oil fields, whole countries, or worldwide production) will eventually peak and then decline at a similar rate to the rate of increase before the peak as these reserves are exhausted. The peak of oil discoveries was in 1965, and oil production per year has surpassed oil discoveries every year since 1980. However, this does not mean that potential oil production has surpassed oil demand.

Hubbert applied his theory to accurately predict the peak of U.S. conventional oil production at a date between 1966 and 1970. This prediction was based on data available at the time of his publication in 1956. In the same paper, Hubbert predicts world peak oil in "half a century" after his publication, which would be 2006.

It is difficult to predict the oil peak in any given region, due to the lack of knowledge and/or transparency in accounting of global oil reserves. Based on available production data, proponents have previously predicted the peak for the world to be in years 1989, 1995, or 1995–2000. Some of these predictions date from before the recession of the early 1980s, and the consequent reduction in global consumption, the effect of which was to delay the date of any peak by several years. Just as the 1971 U.S. peak in oil production was only clearly recognized after the fact, a peak in world production will be difficult to discern until production clearly drops off. The peak is also a moving target as it is now measured as "liquids", which includes synthetic fuels, instead of just conventional oil.

The International Energy Agency (IEA) said in 2010 that production of conventional crude oil had peaked in 2006 at 70 MBBL/d, then flattened at 68 or 69 thereafter. Since virtually all economic sectors rely heavily on petroleum, peak oil, if it were to occur, could lead to a "partial or complete failure of markets". In the mid-2000s, widespread fears of an imminent peak led to the "peak oil movement," in which over one hundred thousand Americans prepared, individually and collectively, for the "post-carbon" future.

The calculus for peak oil has changed with the introduction of unconventional production methods. In particular, the combination of horizontal drilling and hydraulic fracturing has resulted in a significant increase in production from previously uneconomic plays. Analysts expect that $150 billion will be spent on further developing North American tight oil fields in 2015. The large increase in tight oil production is one of the reasons behind the price drop in late 2014. Certain rock strata contain hydrocarbons but have low permeability and are not thick from a vertical perspective. Conventional vertical wells would be unable to economically retrieve these hydrocarbons. Horizontal drilling, extending horizontally through the strata, permits the well to access a much greater volume of the strata. Hydraulic fracturing creates greater permeability and increases hydrocarbon flow to the wellbore.





</doc>
<doc id="23196" url="https://en.wikipedia.org/wiki?curid=23196" title="Particular">
Particular

In metaphysics, particulars are defined as concrete, spatiotemporal entities as opposed to abstract entities, such as properties or numbers. There are, however, theories of "abstract particulars" or "tropes". For example, Socrates is a particular (there's only one Socrates-the-teacher-of-Plato and one cannot make copies of him, e.g., by cloning him, without introducing new, distinct particulars). Redness, by contrast, is not a particular, because it is abstract and multiply instantiated (my bicycle, this apple, and that woman's hair are all red).

Sybil Wolfram writes 
Particulars include only individuals of a certain kind: as a first approximation individuals with a definite place in space and time, such as persons and material objects or events, or which must be identified through such individuals, like smiles or thoughts.

Some terms are used by philosophers with a rough-and-ready idea of their meaning. This can occur if there is lack of agreement about the best definition of the term. In formulating a solution to the problem of universals, the term 'particular' can be used to describe the "particular" instance of redness of a certain apple as opposed to the 'universal' 'redness' (being abstract).

The term particular is also used as a modern equivalent of the Aristotelian notion of individual substance. Used in this sense, particular can mean any concrete (individual) entity, irrespective of whether it is spatial and temporal or not.



</doc>
<doc id="23197" url="https://en.wikipedia.org/wiki?curid=23197" title="Poultry">
Poultry

Poultry () are domesticated birds kept by humans for their eggs, their meat or their feathers. These birds are most typically members of the superorder Galloanserae (fowl), especially the order Galliformes (which includes chickens, quails, and turkeys).

Poultry also includes other birds that are killed for their meat, such as the young of pigeons (known as squabs) but does not include similar wild birds hunted for sport or food and known as game. The word "poultry" comes from the French/Norman word "poule", itself derived from the Latin word "pullus", which means small animal.

The domestication of poultry took place several thousand years ago. This may have originally been as a result of people hatching and rearing young birds from eggs collected from the wild, but later involved keeping the birds permanently in captivity. Domesticated chickens may have been used for cockfighting at first and quail kept for their songs, but soon it was realised how useful it was having a captive-bred source of food. 

Selective breeding for fast growth, egg-laying ability, conformation, plumage and docility took place over the centuries, and modern breeds often look very different from their wild ancestors. Although some birds are still kept in small flocks in extensive systems, most birds available in the market today are reared in intensive commercial enterprises. 

Poultry is the second most widely eaten type of meat globally and, along with eggs, provides nutritionally beneficial food containing high-quality protein accompanied by a low proportion of fat. All poultry meat should be properly handled and sufficiently cooked in order to reduce the risk of food poisoning.

The word "poultry" comes from the Middle English "pultrie", from Old French "pouletrie", from "pouletier", poultry dealer, from "poulet", pullet. The word "pullet" itself comes from Middle English "pulet", from Old French "polet", both from Latin "pullus", a young fowl, young animal or chicken. The word "fowl" is of Germanic origin (cf. Old English "Fugol", German "Vogel", Danish "Fugl").

"Poultry" is a term used for any kind of domesticated bird, captive-raised for its utility, and traditionally the word has been used to refer to wildfowl (Galliformes) and waterfowl (Anseriformes) but not to cagebirds such as songbirds and parrots. "Poultry" can be defined as domestic fowls, including chickens, turkeys, geese and ducks, raised for the production of meat or eggs and the word is also used for the flesh of these birds used as food. 

The Encyclopædia Britannica lists the same bird groups but also includes guinea fowl and squabs (young pigeons). In R. D. Crawford's "Poultry breeding and genetics", squabs are omitted but Japanese quail and common pheasant are added to the list, the latter frequently being bred in captivity and released into the wild. In his 1848 classic book on poultry, "Ornamental and Domestic Poultry: Their History, and Management", Edmund Dixon included chapters on the peafowl, guinea fowl, mute swan, turkey, various types of geese, the muscovy duck, other ducks and all types of chickens including bantams. 

In colloquial speech, the term "fowl" is often used near-synonymously with "domesticated chicken" ("Gallus gallus"), or with "poultry" or even just "bird", and many languages do not distinguish between "poultry" and "fowl". Both words are also used for the flesh of these birds. Poultry can be distinguished from "game", defined as wild birds or mammals hunted for food or sport, a word also used to describe the flesh of these when eaten.

Chickens are medium-sized, chunky birds with an upright stance and characterised by fleshy red combs and wattles on their heads. Males, known as cocks, are usually larger, more boldly coloured, and have more exaggerated plumage than females (hens). Chickens are gregarious, omnivorous, ground-dwelling birds that in their natural surroundings search among the leaf litter for seeds, invertebrates, and other small animals. They seldom fly except as a result of perceived danger, preferring to run into the undergrowth if approached. Today's domestic chicken ("Gallus gallus domesticus") is mainly descended from the wild red junglefowl of Asia, with some additional input from grey junglefowl. Domestication is believed to have taken place between 7,000 and 10,000 years ago, and what are thought to be fossilized chicken bones have been found in northeastern China dated to around 5,400 BC. Archaeologists believe domestication was originally for the purpose of cockfighting, the male bird being a doughty fighter. By 4,000 years ago, chickens seem to have reached the Indus Valley and 250 years later, they arrived in Egypt. They were still used for fighting and were regarded as symbols of fertility. The Romans used them in divination, and the Egyptians made a breakthrough when they learned the difficult technique of artificial incubation. Since then, the keeping of chickens has spread around the world for the production of food with the domestic fowl being a valuable source of both eggs and meat.

Since their domestication, a large number of breeds of chickens have been established, but with the exception of the white Leghorn, most commercial birds are of hybrid origin. In about 1800, chickens began to be kept on a larger scale, and modern high-output poultry farms were present in the United Kingdom from around 1920 and became established in the United States soon after the Second World War. By the mid-20th century, the poultry meat-producing industry was of greater importance than the egg-laying industry. Poultry breeding has produced breeds and strains to fulfil different needs; light-framed, egg-laying birds that can produce 300 eggs a year; fast-growing, fleshy birds destined for consumption at a young age, and utility birds which produce both an acceptable number of eggs and a well-fleshed carcase. Male birds are unwanted in the egg-laying industry and can often be identified as soon as they are hatch for subsequent culling. In meat breeds, these birds are sometimes castrated (often chemically) to prevent aggression. The resulting bird, called a capon, has more tender and flavorful meat, as well.

A bantam is a small variety of domestic chicken, either a miniature version of a member of a standard breed, or a "true bantam" with no larger counterpart. The name derives from the town of Bantam in Java where European sailors bought the local small chickens for their shipboard supplies. Bantams may be a quarter to a third of the size of standard birds and lay similarly small eggs. They are kept by small-holders and hobbyists for egg production, use as broody hens, ornamental purposes, and showing.

Cockfighting is said to be the world's oldest spectator sport and may have originated in Persia 6,000 years ago. Two mature males (cocks or roosters) are set to fight each other, and will do so with great vigour until one is critically injured or killed. Breeds such as the Aseel were developed in the Indian subcontinent for their aggressive behaviour. The sport formed part of the culture of the ancient Indians, Chinese, Greeks, and Romans, and large sums were won or lost depending on the outcome of an encounter. Cockfighting has been banned in many countries during the last century on the grounds of cruelty to animals.

Ducks are medium-sized aquatic birds with broad bills, eyes on the side of the head, fairly long necks, short legs set far back on the body, and webbed feet. Males, known as drakes, are often larger than females (simply known as ducks) and are differently coloured in some breeds. Domestic ducks are omnivores, eating a variety of animal and plant materials such as aquatic insects, molluscs, worms, small amphibians, waterweeds, and grasses. They feed in shallow water by dabbling, with their heads underwater and their tails upended. Most domestic ducks are too heavy to fly, and they are social birds, preferring to live and move around together in groups. They keep their plumage waterproof by preening, a process that spreads the secretions of the preen gland over their feathers.
Clay models of ducks found in China dating back to 4000 BC may indicate the domestication of ducks took place there during the Yangshao culture. Even if this is not the case, domestication of the duck took place in the Far East at least 1500 years earlier than in the West. Lucius Columella, writing in the first century BC, advised those who sought to rear ducks to collect wildfowl eggs and put them under a broody hen, because when raised in this way, the ducks "lay aside their wild nature and without hesitation breed when shut up in the bird pen". Despite this, ducks did not appear in agricultural texts in Western Europe until about 810 AD, when they began to be mentioned alongside geese, chickens, and peafowl as being used for rental payments made by tenants to landowners.

It is widely agreed that the mallard ("Anas platyrhynchos") is the ancestor of all breeds of domestic duck (with the exception of the Muscovy duck ("Cairina moschata"), which is not closely related to other ducks). Ducks are farmed mainly for their meat, eggs, and down. As is the case with chickens, various breeds have been developed, selected for egg-laying ability, fast growth, and a well-covered carcase. The most common commercial breed in the United Kingdom and the United States is the Pekin duck, which can lay 200 eggs a year and can reach a weight of in 44 days. In the Western world, ducks are not as popular as chickens, because the latter produce larger quantities of white, lean meat and are easier to keep intensively, making the price of chicken meat lower than that of duck meat. While popular in "haute cuisine", duck appears less frequently in the mass-market food industry. However, things are different in the East. Ducks are more popular there than chickens and are mostly still herded in the traditional way and selected for their ability to find sufficient food in harvested rice fields and other wet environments.

The greylag goose ("Anser anser") was domesticated by the Egyptians at least 3000 years ago, and a different wild species, the swan goose ("Anser cygnoides"), domesticated in Siberia about a thousand years later, is known as a Chinese goose. The two hybridise with each other and the large knob at the base of the beak, a noticeable feature of the Chinese goose, is present to a varying extent in these hybrids. The hybrids are fertile and have resulted in several of the modern breeds. Despite their early domestication, geese have never gained the commercial importance of chickens and ducks.

Domestic geese are much larger than their wild counterparts and tend to have thick necks, an upright posture, and large bodies with broad rear ends. The greylag-derived birds are large and fleshy and used for meat, while the Chinese geese have smaller frames and are mainly used for egg production. The fine down of both is valued for use in pillows and padded garments. They forage on grass and weeds, supplementing this with small invertebrates, and one of the attractions of rearing geese is their ability to grow and thrive on a grass-based system. They are very gregarious and have good memories and can be allowed to roam widely in the knowledge that they will return home by dusk. The Chinese goose is more aggressive and noisy than other geese and can be used as a guard animal to warn of intruders. The flesh of meat geese is dark-coloured and high in protein, but they deposit fat subcutaneously, although this fat contains mostly monounsaturated fatty acids. The birds are killed either around 10 or about 24 weeks. Between these ages, problems with dressing the carcase occur because of the presence of developing pin feathers.

In some countries, geese and ducks are force-fed to produce livers with an exceptionally high fat content for the production of "foie gras". Over 75% of world production of this product occurs in France, with lesser industries in Hungary and Bulgaria and a growing production in China. "Foie gras" is considered a luxury in many parts of the world, but the process of feeding the birds in this way is banned in many countries on animal welfare grounds.

Turkeys are large birds, their nearest relatives being the pheasant and the guineafowl. Males are larger than females and have spreading, fan-shaped tails and distinctive, fleshy wattles, called a snood, that hang from the top of the beak and are used in courtship display. Wild turkeys can fly, but seldom do so, preferring to run with a long, stratling gait. They roost in trees and forage on the ground, feeding on seeds, nuts, berries, grass, foliage, invertebrates, lizards, and small snakes.

The modern domesticated turkey is descended from one of six subspecies of wild turkey ("Meleagris gallopavo") found in the present Mexican states of Jalisco, Guerrero and Veracruz. Pre-Aztec tribes in south-central Mexico first domesticated the bird around 800 BC, and Pueblo Indians inhabiting the Colorado Plateau in the United States did likewise around 200 BC. They used the feathers for robes, blankets, and ceremonial purposes. More than 1,000 years later, they became an important food source. The first Europeans to encounter the bird misidentified it as a guineafowl, a bird known as a "turkey fowl" at that time because it had been introduced into Europe via Turkey.

Commercial turkeys are usually reared indoors under controlled conditions. These are often large buildings, purpose-built to provide ventilation and low light intensities (this reduces the birds' activity and thereby increases the rate of weight gain). The lights can be switched on for 24-hrs/day, or a range of step-wise light regimens to encourage the birds to feed often and therefore grow rapidly. Females achieve slaughter weight at about 15 weeks of age and males at about 19. Mature commercial birds may be twice as heavy as their wild counterparts. Many different breeds have been developed, but the majority of commercial birds are white, as this improves the appearance of the dressed carcass, the pin feathers being less visible. Turkeys were at one time mainly consumed on special occasions such as Christmas (10 million birds in the United Kingdom) or Thanksgiving (60 million birds in the United States). However, they are increasingly becoming part of the everyday diet in many parts of the world.

The quail is a small to medium-sized, cryptically coloured bird. In its natural environment, it is found in bushy places, in rough grassland, among agricultural crops, and in other places with dense cover. It feeds on seeds, insects, and other small invertebrates. Being a largely ground-dwelling, gregarious bird, domestication of the quail was not difficult, although many of its wild instincts are retained in captivity. It was known to the Egyptians long before the arrival of chickens and was depicted in hieroglyphs from 2575 BC. It migrated across Egypt in vast flocks and the birds could sometimes be picked up off the ground by hand. These were the common quail ("Coturnix coturnix"), but modern domesticated flocks are mostly of Japanese quail ("Coturnix japonica") which was probably domesticated as early as the 11th century AD in Japan. They were originally kept as songbirds, and they are thought to have been regularly used in song contests.

In the early 20th century, Japanese breeders began to selectively breed for increased egg production. By 1940, the quail egg industry was flourishing, but the events of World War II led to the complete loss of quail lines bred for their song type, as well as almost all of those bred for egg production. After the war, the few surviving domesticated quail were used to rebuild the industry, and all current commercial and laboratory lines are considered to have originated from this population. Modern birds can lay upward of 300 eggs a year and countries such as Japan, India, China, Italy, Russia, and the United States have established commercial Japanese quail farming industries. Japanese quail are also used in biomedical research in fields such as genetics, embryology, nutrition, physiology, pathology, and toxicity studies. These quail are closely related to the common quail, and many young hybrid birds are released into the wild each year to replenish dwindling wild populations.

Guinea fowl originated in southern Africa, and the species most often kept as poultry is the helmeted guineafowl ("Numida meleagris"). It is a medium-sized grey or speckled bird with a small naked head with colourful wattles and a knob on top, and was domesticated by the time of the ancient Greeks and Romans. Guinea fowl are hardy, sociable birds that subsist mainly on insects, but also consume grasses and seeds. They will keep a vegetable garden clear of pests and will eat the ticks that carry Lyme disease. They happily roost in trees and give a loud vocal warning of the approach of predators. Their flesh and eggs can be eaten in the same way as chickens, young birds being ready for the table at the age of about four months.
A squab is the name given to the young of domestic pigeons that are destined for the table. Like other domesticated pigeons, birds used for this purpose are descended from the rock pigeon ("Columba livia"). Special utility breeds with desirable characteristics are used. Two eggs are laid and incubated for about 17 days. When they hatch, the squabs are fed by both parents on "pigeon's milk", a thick secretion high in protein produced by the crop. Squabs grow rapidly, but are slow to fledge and are ready to leave the nest at 26 to 30 days weighing about . By this time, the adult pigeons will have laid and be incubating another pair of eggs and a prolific pair should produce two squabs every four weeks during a breeding season lasting several months.

Worldwide, more chickens are kept than any other type of poultry, with over 50 billion birds being raised each year as a source of meat and eggs. Traditionally, such birds would have been kept extensively in small flocks, foraging during the day and housed at night. This is still the case in developing countries, where the women often make important contributions to family livelihoods through keeping poultry. However, rising world populations and urbanization have led to the bulk of production being in larger, more intensive specialist units. These are often situated close to where the feed is grown or near to where the meat is needed, and result in cheap, safe food being made available for urban communities. Profitability of production depends very much on the price of feed, which has been rising. High feed costs could limit further development of poultry production.

In free-range husbandry, the birds can roam freely outdoors for at least part of the day. Often, this is in large enclosures, but the birds have access to natural conditions and can exhibit their normal behaviours. A more intensive system is yarding, in which the birds have access to a fenced yard and poultry house at a higher stocking rate. Poultry can also be kept in a barn system, with no access to the open air, but with the ability to move around freely inside the building. The most intensive system for egg-laying chickens is battery cages, often set in multiple tiers. In these, several birds share a small cage which restricts their ability to move around and behave in a normal manner. The eggs are laid on the floor of the cage and roll into troughs outside for ease of collection. Battery cages for hens have been illegal in the EU since January 1, 2012.

Chickens raised intensively for their meat are known as "broilers". Breeds have been developed that can grow to an acceptable carcass size () in six weeks or less. Broilers grow so fast, their legs cannot always support their weight and their hearts and respiratory systems may not be able to supply enough oxygen to their developing muscles. Mortality rates at 1% are much higher than for less-intensively reared laying birds which take 18 weeks to reach similar weights. Processing the birds is done automatically with conveyor-belt efficiency. They are hung by their feet, stunned, killed, bled, scalded, plucked, have their heads and feet removed, eviscerated, washed, chilled, drained, weighed, and packed, all within the course of little over two hours.

Both intensive and free-range farming have animal welfare concerns. In intensive systems, cannibalism, feather pecking and vent pecking can be common, with some farmers using beak trimming as a preventative measure. Diseases can also be common and spread rapidly through the flock. In extensive systems, the birds are exposed to adverse weather conditions and are vulnerable to predators and disease-carrying wild birds. Barn systems have been found to have the worst bird welfare. In Southeast Asia, a lack of disease control in free-range farming has been associated with outbreaks of avian influenza.

In many countries, national and regional poultry shows are held where enthusiasts exhibit their birds which are judged on certain phenotypical breed traits as specified by their respective breed standards. The idea of poultry exhibition may have originated after cockfighting was made illegal, as a way of maintaining a competitive element in poultry husbandry. Breed standards were drawn up for egg-laying, meat-type, and purely ornamental birds, aiming for uniformity. Sometimes, poultry shows are part of general livestock shows, and sometimes they are separate events such as the annual "National Championship Show" in the United Kingdom organised by the Poultry Club of Great Britain. 

Poultry is the second most widely eaten type of meat in the world, accounting for about 30% of total meat production worldwide compared to pork at 38%. Sixteen billion birds are raised annually for consumption, more than half of these in industrialised, factory-like production units. Global broiler meat production rose to 84.6 million tonnes in 2013. The largest producers were the United States (20%), China (16.6%), Brazil (15.1%) and the European Union (11.3%). There are two distinct models of production; the European Union supply chain model seeks to supply products which can be traced back to the farm of origin. This model faces the increasing costs of implementing additional food safety requirements, welfare issues and environmental regulations. In contrast, the United States model turns the product into a commodity.

World production of duck meat was about 4.2 million tonnes in 2011 with China producing two thirds of the total, some 1.7 billion birds. Other notable duck-producing countries in the Far East include Vietnam, Thailand, Malaysia, Myanmar, Indonesia and South Korea (12% in total). France (3.5%) is the largest producer in the West, followed by other EU nations (3%) and North America (1.7%). China was also by far the largest producer of goose and guinea fowl meat, with a 94% share of the 2.6 million tonne global market.

Global egg production was expected to reach 65.5 million tonnes in 2013, surpassing all previous years. Between 2000 and 2010, egg production was growing globally at around 2% per year, but since then growth has slowed down to nearer 1%.

Poultry is available fresh or frozen, as whole birds or as joints (cuts), bone-in or deboned, seasoned in various ways, raw or ready cooked. The meatiest parts of a bird are the flight muscles on its chest, called "breast" meat, and the walking muscles on the legs, called the "thigh" and "drumstick". The wings are also eaten (Buffalo wings are a popular example in the United States) and may be split into three segments, the meatier "drumette", the "wingette" (also called the "flat"), and the wing tip (also called the "flapper"). In Japan, the wing is frequently separated, and these parts are referred to as 手羽元 ("teba-moto" "wing base") and 手羽先 ("teba-saki" "wing tip").

Dark meat, which avian myologists refer to as "red muscle", is used for sustained activity—chiefly walking, in the case of a chicken. The dark colour comes from the protein myoglobin, which plays a key role in oxygen uptake and storage within cells. White muscle, in contrast, is suitable only for short bursts of activity such as, for chickens, flying. Thus, the chicken's leg and thigh meat are dark, while its breast meat (which makes up the primary flight muscles) is white. Other birds with breast muscle more suitable for sustained flight, such as ducks and geese, have red muscle (and therefore dark meat) throughout. Some cuts of meat including poultry expose the microscopic regular structure of intracellular muscle fibrils which can diffract light and produce iridescent colours, an optical phenomenon sometimes called structural colouration.

Poultry meat and eggs provide nutritionally beneficial food containing protein of high quality. This is accompanied by low levels of fat which have a favourable mix of fatty acids. Chicken meat contains about two to three times as much polyunsaturated fat as most types of red meat when measured by weight. However, for boneless, skinless chicken breast, the amount is much lower. A 100-g serving of baked chicken breast contains 4 g of fat and 31 g of protein, compared to 10 g of fat and 27 g of protein for the same portion of broiled, lean skirt steak.

A 2011 study by the Translational Genomics Research Institute showed that 47% of the meat and poultry sold in United States grocery stores was contaminated with "Staphylococcus aureus", and 52% of the bacteria concerned showed resistance to at least three groups of antibiotics. Thorough cooking of the product would kill these bacteria, but a risk of cross-contamination from improper handling of the raw product is still present. Also, some risk is present for consumers of poultry meat and eggs to bacterial infections such as "Salmonella" and "Campylobacter". Poultry products may become contaminated by these bacteria during handling, processing, marketing, or storage, resulting in food-borne illness if the product is improperly cooked or handled.

In general, avian influenza is a disease of birds caused by bird-specific influenza A virus that is not normally transferred to people; however, people in contact with live poultry are at the greatest risk of becoming infected with the virus and this is of particular concern in areas such as Southeast Asia, where the disease is endemic in the wild bird population and domestic poultry can become infected. The virus possibly could mutate to become highly virulent and infectious in humans and cause an influenza pandemic.

Bacteria can be grown in the laboratory on nutrient culture media, but viruses need living cells in which to replicate. Many vaccines to infectious diseases can be grown in fertilised chicken eggs. Millions of eggs are used each year to generate the annual flu vaccine requirements, a complex process that takes about six months after the decision is made as to what strains of virus to include in the new vaccine. A problem with using eggs for this purpose is that people with egg allergies are unable to be immunised, but this disadvantage may be overcome as new techniques for cell-based rather than egg-based culture become available. Cell-based culture will also be useful in a pandemic when it may be difficult to acquire a sufficiently large quantity of suitable sterile, fertile eggs.



</doc>
<doc id="23203" url="https://en.wikipedia.org/wiki?curid=23203" title="Propaganda">
Propaganda

Propaganda is information that is not objective and is used primarily to influence an audience and further an agenda, often by presenting facts selectively to encourage a particular synthesis or perception, or using loaded language to produce an emotional rather than a rational response to the information that is presented. Propaganda is often associated with material prepared by governments, but activist groups, companies and the media can also produce propaganda. 

In the twentieth century, the term propaganda has been associated with a manipulative approach, but propaganda historically was a neutral descriptive term. A wide range of materials and media are used for conveying propaganda messages, which changed as new technologies were invented, including paintings, cartoons, posters, pamphlets, films, radio shows, TV shows, and websites.

In a 1929 literary debate with Edward Bernays, Everett Dean Martin argues that, “Propaganda is making puppets of us. We are moved by hidden strings which the propagandist manipulates.”

"Propaganda" is a modern Latin word, the gerundive form of "propagare", meaning "to spread" or "to propagate", thus "propaganda" means "that which is to be propagated". Originally this word derived from a new administrative body of the Catholic church (congregation) created in 1622, called the "Congregatio de Propaganda Fide" ("Congregation for Propagating the Faith"), or informally simply "Propaganda". Its activity was aimed at "propagating" the Catholic faith in non-Catholic countries.

From the 1790s, the term began being used also to refer to "propaganda" in secular activities. The term began taking a pejorative or negative connotation in the mid-19th century, when it was used in the political sphere.

Primitive forms of propaganda have been a human activity as far back as reliable recorded evidence exists. The Behistun Inscription (c. 515 BC) detailing the rise of Darius I to the Persian throne is viewed by most historians as an early example of propaganda. 
Another striking example of propaganda during Ancient History is the last Roman civil wars (44-30 BC) during which Octavian and Mark Antony blame each other for obscure and degrading origins, cruelty, cowardice, oratorical and literary incompetence, debaucheries, luxury, drunkenness and other slanders. This defamation took the form of "uituperatio" (Roman rhetorical genre of the invective) which was decisive for shaping the Roman public opinion at this time.

Propaganda during the Reformation, helped by the spread of the printing press throughout Europe, and in particular within Germany, caused new ideas, thoughts, and doctrine to be made available to the public in ways that had never been seen before the 16th century. During the era of the American Revolution, the American colonies had a flourishing network of newspapers and printers who specialized in the topic on behalf of the Patriots (and to a lesser extent on behalf of the Loyalists).
The first large-scale and organised propagation of government propaganda was occasioned by the outbreak of war in 1914. After the defeat of Germany in the First World War, military officials such as Erich Ludendorff suggested that British propaganda had been instrumental in their defeat. Adolf Hitler came to echo this view, believing that it had been a primary cause of the collapse of morale and the revolts in the German home front and Navy in 1918 (see also: Dolchstoßlegende). In "Mein Kampf" (1925) Hitler expounded his theory of propaganda, which provided a powerful base for his rise to power in 1933. Historian Robert Ensor explains that "Hitler...puts no limit on what can be done by propaganda; people will believe anything, provided they are told it often enough and emphatically enough, and that contradicters are either silenced or smothered in calumny." Most propaganda in Nazi Germany was produced by the Ministry of Public Enlightenment and Propaganda under Joseph Goebbels. World War II saw continued use of propaganda as a weapon of war, building on the experience of WWI, by Goebbels and the British Political Warfare Executive, as well as the United States Office of War Information.
In the early 20th century, the invention of motion pictures gave propaganda-creators a powerful tool for advancing political and military interests when it came to reaching a broad segment of the population and creating consent or encouraging rejection of the real or imagined enemy. In the years following the October Revolution of 1917, the Soviet government sponsored the Russian film industry with the purpose of making propaganda films (e.g. the 1925 film "The Battleship Potemkin" glorifies Communist ideals.) In WWII, Nazi filmmakers produced highly emotional films to create popular support for occupying the Sudetenland and attacking Poland. The 1930s and 1940s, which saw the rise of totalitarian states and the Second World War, are arguably the "Golden Age of Propaganda". Leni Riefenstahl, a filmmaker working in Nazi Germany, created one of the best-known propaganda movies, "Triumph of the Will". In the US, animation became popular, especially for winning over youthful audiences and aiding the U.S. war effort, e.g.,"Der Fuehrer's Face" (1942), which ridicules Hitler and advocates the value of freedom. US war films in the early 1940s were designed to create a patriotic mindset and convince viewers that sacrifices needed to be made to defeat the Axis Powers. Polish filmmakers in Great Britain created anti-nazi color film "Calling mr. Smith" (1943) about current nazi crimes in occupied Europe and about lies of nazi propaganda.

The West and the Soviet Union both used propaganda extensively during the Cold War. Both sides used film, television, and radio programming to influence their own citizens, each other, and Third World nations. George Orwell's novels "Animal Farm" and "Nineteen Eighty-Four" are virtual textbooks on the use of propaganda. During the Cuban Revolution, Fidel Castro stressed the importance of propaganda. Propaganda was used extensively by Communist forces in the Vietnam War as means of controlling people's opinions.

During the Yugoslav wars, propaganda was used as a military strategy by governments of Federal Republic of Yugoslavia and Croatia. Propaganda was used to create fear and hatred, and particularly incite the Serb population against the other ethnicities (Bosniaks, Croats, Albanians and other non-Serbs). Serb media made a great effort in justifying, revising or denying mass war crimes committed by Serb forces during these wars.

In the early 20th century the term propaganda was used by the founders of the nascent public relations industry to refer to their people. This image died out around the time of World War II, as the industry started to avoid the word, given the pejorative connotation it had acquired. Literally translated from the Latin gerundive as "things that must be disseminated", in some cultures the term is neutral or even positive, while in others the term has acquired a strong negative connotation. The connotations of the term "propaganda" can also vary over time. For example, in Portuguese and some Spanish language speaking countries, particularly in the Southern Cone, the word "propaganda" usually refers to the most common manipulative media — "advertising".
In English, "propaganda" was originally a neutral term for the dissemination of information in favor of any given cause. During the 20th century, however, the term acquired a thoroughly negative meaning in western countries, representing the intentional dissemination of often false, but certainly "compelling" claims to support or justify political actions or ideologies. According to Harold Lasswell, the term began to fall out of favor due to growing public suspicion of propaganda in the wake of its use during World War I by the Creel Committee in the United States and the Ministry of Information in Britain: Writing in 1928, Lasswell observed, "In democratic countries the official propaganda bureau was looked upon with genuine alarm, for fear that it might be suborned to party and personal ends. The outcry in the United States against Mr. Creel's famous Bureau of Public Information (or 'Inflammation') helped to din into the public mind the fact that propaganda existed. … The public's discovery of propaganda has led to a great of lamentation over it. Propaganda has become an epithet of contempt and hate, and the propagandists have sought protective coloration in such names as 'public relations council,' 'specialist in public education,' 'public relations adviser.' "

Identifying propaganda has always been a problem. The main difficulties have involved differentiating propaganda from other types of persuasion, and avoiding a biased approach. Richard Alan Nelson provides a definition of the term: "Propaganda is neutrally defined as a systematic form of purposeful persuasion that attempts to influence the emotions, attitudes, opinions, and actions of specified target audiences for ideological, political or commercial purposes through the controlled transmission of one-sided messages (which may or may not be factual) via mass and direct media channels." The definition focuses on the communicative process involved — or more precisely, on the purpose of the process, and allow "propaganda" to be considered objectively and then interpreted as positive or negative behavior depending on the perspective of the viewer or listener. 
According to historian Zbyněk Zeman, propaganda is defined as either white, grey or black. White propaganda openly discloses its source and intent. Grey propaganda has an ambiguous or non-disclosed source or intent. Black propaganda purports to be published by the enemy or some organization besides its actual origins (compare with black operation, a type of clandestine operation in which the identity of the sponsoring government is hidden). In scale, these different types of propaganda can also be defined by the potential of true and correct information to compete with the propaganda. For example, opposition to white propaganda is often readily found and may slightly discredit the propaganda source. Opposition to grey propaganda, when revealed (often by an inside source), may create some level of public outcry. Opposition to black propaganda is often unavailable and may be dangerous to reveal, because public cognizance of black propaganda tactics and sources would undermine or backfire the very campaign the black propagandist supported.

The propagandist seeks to change the way people understand an issue or situation for the purpose of changing their actions and expectations in ways that are desirable to the interest group. Propaganda, in this sense, serves as a corollary to censorship in which the same purpose is achieved, not by filling people's minds with approved information, but by preventing people from being confronted with opposing points of view. What sets propaganda apart from other forms of advocacy is the willingness of the propagandist to change people's understanding through deception and confusion rather than persuasion and understanding. The leaders of an organization know the information to be one sided or untrue, but this may not be true for the rank and file members who help to disseminate the propaganda.

Propaganda was often used to influence opinions and beliefs on religious issues, particularly during the split between the Roman Catholic Church and the Protestant churches.

More in line with the religious roots of the term, propaganda is also used widely in the debates about new religious movements (NRMs), both by people who defend them and by people who oppose them. The latter pejoratively call these NRMs cults. Anti-cult activists and Christian countercult activists accuse the leaders of what they consider cults of using propaganda extensively to recruit followers and keep them. Some social scientists, such as the late Jeffrey Hadden, and CESNUR affiliated scholars accuse ex-members of "cults" and the anti-cult movement of making these unusual religious movements look bad without sufficient reasons.

In post–World War II usage of the word "propaganda" more typically refers to political or nationalist uses of these techniques or to the promotion of a set of ideas.

Propaganda is a powerful weapon in war; it is used to dehumanize and create hatred toward a supposed enemy, either internal or external, by creating a false image in the mind of soldiers and citizens. This can be done by using derogatory or racist terms (e.g., the racist terms "Jap" and "gook" used during World War II and the Vietnam War, respectively), avoiding some words or language or by making allegations of enemy atrocities. Most propaganda efforts in wartime require the home population to feel the enemy has inflicted an injustice, which may be fictitious or may be based on facts (e.g., the sinking of the passenger ship RMS Lusitania by the German Navy in World War I). The home population must also believe that the cause of their nation in the war is just. In NATO doctrine, propaganda is defined as "Any information, ideas, doctrines, or special appeals disseminated to influence the opinion, emotions, attitudes, or behaviour of any specified group in order to benefit the sponsor either directly or indirectly." Within this perspective, information provided does not need to be necessarily false, but must be instead relevant to specific goals of the "actor" or "system" that performs it.

Propaganda is also one of the methods used in psychological warfare, which may also involve false flag operations in which the identity of the operatives is depicted as those of an enemy nation (e.g., The Bay of Pigs invasion used CIA planes painted in Cuban Air Force markings). The term propaganda may also refer to false information meant to reinforce the mindsets of people who already believe as the propagandist wishes (e.g., During the First World War, the main purpose of British propaganda was to encourage men join the army, and women to work in the country’s industry. The propaganda posters were used, because radios and TVs were not very common at that time.). The assumption is that, if people believe something false, they will constantly be assailed by doubts. Since these doubts are unpleasant (see cognitive dissonance), people will be eager to have them extinguished, and are therefore receptive to the reassurances of those in power. For this reason propaganda is often addressed to people who are already sympathetic to the agenda or views being presented. This process of reinforcement uses an individual's predisposition to self-select "agreeable" information sources as a mechanism for maintaining control over populations.

Propaganda may be administered in insidious ways. For instance, disparaging disinformation about the history of certain groups or foreign countries may be encouraged or tolerated in the educational system. Since few people actually double-check what they learn at school, such disinformation will be repeated by journalists as well as parents, thus reinforcing the idea that the disinformation item is really a "well-known fact", even though no one repeating the myth is able to point to an authoritative source. The disinformation is then recycled in the media and in the educational system, without the need for direct governmental intervention on the media. Such permeating propaganda may be used for political goals: by giving citizens a false impression of the quality or policies of their country, they may be incited to reject certain proposals or certain remarks or ignore the experience of others.

In the Soviet Union during the Second World War, the propaganda designed to encourage civilians was controlled by Stalin, who insisted on a heavy-handed style that educated audiences easily saw was inauthentic. On the other hand, the unofficial rumours about German atrocities were well founded and convincing. Stalin was a Georgian who spoke Russian with a heavy accent. That would not do for a national hero so starting in the 1930s all new visual portraits of Stalin were retouched to erase his Georgian facial characteristics and make him a more generalized Soviet hero. Only his eyes and famous mustache remained unaltered. Zhores Medvedev and Roy Medvedev say his "majestic new image was devised appropriately to depict the leader of all times and of all peoples."

Article 20 of the International Covenant on Civil and Political Rights prohibits any propaganda for war as well as any advocacy of national or religious hatred that constitutes incitement to discrimination, hostility or violence by law.

Propaganda shares techniques with advertising and public relations, each of which can be thought of as propaganda that promotes a commercial product or shapes the perception of an organization, person, or brand.
Journalistic theory generally holds that news items should be objective, giving the reader an accurate background and analysis of the subject at hand. On the other hand, advertisements evolved from the traditional commercial advertisements to include also a new type in the form of paid articles or broadcasts disguised as news. These generally present an issue in a very subjective and often misleading light, primarily meant to persuade rather than inform. Normally they use only subtle propaganda techniques and not the more obvious ones used in traditional commercial advertisements. If the reader believes that a paid advertisement is in fact a news item, the message the advertiser is trying to communicate will be more easily "believed" or "internalized". Such advertisements are considered obvious examples of "covert" propaganda because they take on the appearance of objective information rather than the appearance of propaganda, which is misleading. Federal law specifically mandates that any advertisement appearing in the format of a news item must state that the item is in fact a paid advertisement.

Propaganda has become more common in political contexts, in particular to refer to certain efforts sponsored by governments, political groups, but also often covert interests. In the early 20th century, propaganda was exemplified in the form of party slogans. Propaganda also has much in common with public information campaigns by governments, which are intended to encourage or discourage certain forms of behavior (such as wearing seat belts, not smoking, not littering and so forth). Again, the emphasis is more political in propaganda. Propaganda can take the form of leaflets, posters, TV and radio broadcasts and can also extend to any other medium. In the case of the United States, there is also an important legal (imposed by law) distinction between advertising (a type of overt propaganda) and what the Government Accountability Office (GAO), an arm of the United States Congress, refers to as "covert propaganda".

Roderick Hindery argues that propaganda exists on the political left, and right, and in mainstream centrist parties. Hindery further argues that debates about most social issues can be productively revisited in the context of asking "what is or is not propaganda?" Not to be overlooked is the link between propaganda, indoctrination, and terrorism/counterterrorism. He argues that threats to destroy are often as socially disruptive as physical devastation itself.

Since 9/11 and the appearance of greater media fluidity, propaganda institutions, practices and legal frameworks have been evolving in the US and Britain. Dr Emma Louise Briant shows how this included expansion and integration of the apparatus cross-government and details attempts to coordinate the forms of propaganda for foreign and domestic audiences, with new efforts in strategic communication. These were subject to contestation within the US Government, resisted by Pentagon Public Affairs and critiqued by some scholars. The National Defense Authorization Act for Fiscal Year 2013 (section 1078 (a)) amended the US Information and Educational Exchange Act of 1948 (popularly referred to as the Smith-Mundt Act) and the Foreign Relations Authorization Act of 1987, allowing for materials produced by the State Department and the Broadcasting Board of Governors (BBG) to be released within U.S. borders for the Archivist of the United States. The Smith-Mundt Act, as amended, provided that “the Secretary and the Broadcasting Board of Governors shall make available to the Archivist of the United States, for domestic distribution, motion pictures, films, videotapes, and other material 12 years after the initial dissemination of the material abroad (...) Nothing in this section shall be construed to prohibit the Department of State or the Broadcasting Board of Governors from engaging in any medium or form of communication, either directly or indirectly, because a United States domestic audience is or may be thereby exposed to program material, or based on a presumption of such exposure.” Public concerns were raised upon passage due to the relaxation of prohibitions of domestic propaganda in the United States.

Common media for transmitting propaganda messages include news reports, government reports, historical revision, junk science, books, leaflets, movies, radio, television, and posters. Some propaganda campaigns follow a strategic transmission pattern to indoctrinate the target group. This may begin with a simple transmission, such as a leaflet or advertisement dropped from a plane or an advertisement. Generally these messages will contain directions on how to obtain more information, via a web site, hot line, radio program, etc. (as it is seen also for selling purposes among other goals). The strategy intends to initiate the individual from information recipient to information seeker through reinforcement, and then from information seeker to opinion leader through indoctrination.

A number of techniques based in social psychological research are used to generate propaganda. Many of these same techniques can be found under logical fallacies, since propagandists use arguments that, while sometimes convincing, are not necessarily valid.

Some time has been spent analyzing the means by which the propaganda messages are transmitted. That work is important but it is clear that information dissemination strategies become propaganda strategies only when coupled with "propagandistic messages". Identifying these messages is a necessary prerequisite to study the methods by which those messages are spread. 

The field of social psychology includes the study of persuasion. Social psychologists can be sociologists or psychologists. The field includes many theories and approaches to understanding persuasion. For example, communication theory points out that people can be persuaded by the communicator's credibility, expertise, trustworthiness, and attractiveness. The elaboration likelihood model as well as heuristic models of persuasion suggest that a number of factors (e.g., the degree of interest of the recipient of the communication), influence the degree to which people allow superficial factors to persuade them. Nobel Prize–winning psychologist Herbert A. Simon won the Nobel prize for his theory that people are cognitive misers. That is, in a society of mass information, people are forced to make decisions quickly and often superficially, as opposed to logically.

According to William W. Biddle's 1931 article "A psychological definition of propaganda", "[t]he four principles followed in propaganda are: (1) rely on emotions, never argue; (2) cast propaganda into the pattern of "we" versus an "enemy"; (3) reach groups as well as individuals; (4) hide the propagandist as much as possible."

The propaganda model is a theory advanced by Edward S. Herman and Noam Chomsky which argues systemic biases in the mass media and seeks to explain them in terms of structural economic causes:

First presented in their 1988 book "", the propaganda model views the private media as businesses selling a product — readers and audiences (rather than news) — to other businesses (advertisers) and relying primarily on government and corporate information and propaganda. The theory postulates five general classes of "filters" that determine the type of news that is presented in news media: Ownership of the medium, the medium's Funding, Sourcing of the news, Flak, and anti-communist ideology.

The first three (ownership, funding, and sourcing) are generally regarded by the authors as being the most important. Although the model was based mainly on the characterization of United States media, Chomsky and Herman believe the theory is equally applicable to any country that shares the basic economic structure and organizing principles the model postulates as the cause of media bias.

Of all the potential targets for propaganda, children are the most vulnerable because they are the least prepared with the critical reasoning and contextual comprehension they need to determine whether a message is propaganda or not. The attention children give their environment during development, due to the process of developing their understanding of the world, causes them to absorb propaganda indiscriminately. Also, children are highly imitative: studies by Albert Bandura, Dorothea Ross and Sheila A. Ross in the 1960s indicated that, to a degree, socialization, formal education and standardized television programming can be seen as using propaganda for the purpose of indoctrination. The use of propaganda in schools was highly prevalent during the 1930s and 1940s in Germany, as well as in Stalinist Russia. 
John Taylor Gatto asserts that modern schooling in the USA is designed to "dumb us down" in order to turn children into material suitable to work in factories. This ties into the Herman & Chomsky thesis of rise of Corporate Power, and its use in creating educational systems which serve its purposes against those of democracy.

In Nazi Germany, the education system was thoroughly co-opted to indoctrinate the German youth with anti-Semitic ideology. This was accomplished through the National Socialist Teachers League, of which 97% of all German teachers were members in 1937. The League encouraged the teaching of racial theory. Picture books for children such as "Don't Trust A Fox in A Green Meadow or The Word of A Jew", "Der Giftpilz" (translated into English as "The Poisonous Mushroom") and "The Poodle-Pug-Dachshund-Pincher" were widely circulated (over 100,000 copies of "Don't Trust A Fox"... were circulated during the late 1930s) and contained depictions of Jews as devils, child molesters and other morally charged figures. Slogans such as "Judas the Jew betrayed Jesus the German to the Jews" were recited in class. The following is an example of a propagandistic math problem recommended by the National Socialist Essence of Education: "The Jews are aliens in Germany—in 1933 there were 66,606,000 inhabitants in the German Reich, of whom 499,682 (.75%) were Jews."




</doc>
<doc id="23204" url="https://en.wikipedia.org/wiki?curid=23204" title="Physical quantity">
Physical quantity

A physical quantity is a physical property of a phenomenon, body, or substance, that can be quantified by measurement or we can say that quantities which we come across during our scientific studies are called as the physical quantities. A physical quantity can be expressed as the combination of a magnitude expressed by a number – usually a real number – and a unit: formula_1 where formula_2 is the magnitude and formula_3 is the unit. For example, (the mass of the neutron), or (the speed of light). The same physical quantity formula_4 can be represented equivalently in many unit systems, "i.e." formula_5.

Symbols for quantities should be chosen according to the international recommendations of ISO/IEC 80000, the IUPAP red book and the IUPAC green book. For example, the recommended symbol for the physical quantity "mass" is "m", and the recommended symbol for the quantity "charge" is "Q".

Subscripts are used for two reasons, to simply attach a name to the quantity or associate it with another quantity, or represent a specific vector, matrix, or tensor component.

The type of subscripts is expressed by their style, e.g. by italic style: 'k' and 'p' are abbreviations of the words "kinetic" and "potential", whereas "p" (italic) is the symbol for the physical quantity "pressure" rather than an abbreviation of the word.


A scalar is a physical quantity that has magnitude but no direction. Symbols for physical quantities are usually chosen to be a single letter of the Latin or Greek alphabet, and are printed in italic type.

Vectors are physical quantities that possess both magnitude and direction. Symbols for physical quantities that are vectors are in bold type, underlined or with an arrow above. If, e.g., "u" is the speed of a particle, then the straightforward notation for its velocity is u, u, or formula_6.

Numerical quantities, even those denoted by letters, are usually printed in roman (upright) type, though sometimes can be italic. Symbols for elementary functions (circular trigonometric, hyperbolic, logarithmic etc.), changes in a quantity like Δ in Δ"y" or operators like d in d"x", are also recommended to be printed in roman type.

Examples:

Most physical quantities include a unit, but not all – some are dimensionless. Neither the name of a physical quantity, nor the symbol used to denote it, implies a particular choice of unit, though SI units are usually preferred and assumed today due to their ease of use and all-round applicability. For example, a quantity of mass might be represented by the symbol "m", and could be expressed in the units kilograms (kg), pounds (lb), or daltons (Da).

The notion of "physical dimension" of a physical quantity was introduced by Joseph Fourier in 1822. By convention, physical quantities are organized in a dimensional system built upon base quantities, each of which is regarded as having its own dimension.

Base quantities are those quantities which are distinct in nature and cannot be defined by other quantities. Base quantities are those quantities on the basis of which other quantities can be expressed. The seven base quantities of the International System of Quantities (ISQ) and their corresponding SI units and dimensions are listed in the following table. Other conventions may have a different number of base units (e.g. the CGS and MKS systems of units).

The last two angular units, plane angle and solid angle, are subsidiary units used in the SI, but are treated as dimensionless. The subsidiary units are used for convenience to differentiate between a "truly dimensionless" quantity (pure number) and an "angle", which are different measurements.

Derived quantities are those whose definitions are based on other physical quantities(base quantities).

Important applied base units for space and time are below. Area and volume are of course derived from length, but included for completeness as they occur frequently in many derived quantities, in particular densities.

Important and convenient derived quantities such as densities, fluxes, flows, currents are associated with many quantities. Sometimes different terms such as "current density" and "flux density", "rate", "frequency" and "current", are used interchangeably in the same context, sometimes they are used uniqueley.

To clarify these effective template derived quantities, we let "q" be "any" quantity within some scope of context (not necessarily base quantities) and present in the table below some of the most commonly used symbols where applicable, their definitions, usage, SI units and SI dimensions – where [q] is the dimension of "q".

For time derivatives, specific, molar, and flux densities of quantities there is no one symbol, nomenclature depends on subject, though time derivatives can be generally written using overdot notation. For generality we use "q", "q", and F respectively. No symbol is necessarily required for the gradient of a scalar field, since only the nabla/del operator ∇ or grad needs to be written. For spatial density, current, current density and flux, the notations are common from one context to another, differing only by a change in subscripts.

For current density, formula_7 is a unit vector in the direction of flow, i.e. tangent to a flowline. Notice the dot product with the unit normal for a surface, since the amount of current passing through the surface is reduced when the current is not normal to the area. Only the current passing perpendicular to the surface contributes to the current passing "through" the surface, no current passes "in" the (tangential) plane of the surface.

The calculus notations below can be used synonymously.

If "X" is a "n"-variable function formula_8, then:

The meaning of the term physical "quantity" is generally well understood (everyone understands what is meant by "the frequency of a periodic phenomenon", or "the resistance of an electric wire"). The term "physical quantity" does not imply a physically "invariant quantity". "Length" for example is a "physical quantity", yet it is variant under coordinate change in special and general relativity. The notion of physical quantities is so basic and intuitive in the realm of science, that it does not need to be explicitly "spelled out" or even "mentioned". It is universally understood that scientists will (more often than not) deal with quantitative data, as opposed to qualitative data. Explicit mention and discussion of "physical quantities" is not part of any standard science program, and is more suited for a "philosophy of science" or "philosophy" program.
The notion of "physical quantities" is seldom used in physics, nor is it part of the standard physics vernacular. The idea is often misleading, as its name implies "a quantity that can be physically measured", yet is often incorrectly used to mean a physical invariant. Due to the rich complexity of physics, many different fields possess different physical invariants. There is no known physical invariant sacred in all possible fields of physics. Energy, space, momentum, torque, position, and length (just to name a few) are all found to be experimentally variant in some particular scale and system. Additionally, the notion that it is possible to measure "physical quantities" comes into question, particular in quantum field theory and normalization techniques. As infinities are produced by the theory, the actual “measurements” made are not really those of the physical universe (as we cannot measure infinities), they are those of the renormalization scheme which is expressly dependent on our measurement scheme, coordinate system and metric system.





</doc>
<doc id="23205" url="https://en.wikipedia.org/wiki?curid=23205" title="Physical constant">
Physical constant

A physical constant, sometimes fundamental physical constant or universal constant, is a physical quantity that is generally believed to be both universal in nature and have constant value in time. It is contrasted with a mathematical constant, which has a fixed numerical value, but does not directly involve any physical measurement.

There are many physical constants in science, some of the most widely recognized being the speed of light in vacuum "c", the gravitational constant "G", Planck's constant "h", the electric constant "ε", and the elementary charge "e". Physical constants can take many dimensional forms: the speed-of-light signifies a maximum speed for any object and is expressed dimensionally as length divided by time; while the fine-structure constant "α", which characterizes the strength of the electromagnetic interaction, is dimensionless.

The term "fundamental physical constant" is sometimes used to refer to universal but dimensioned physical constants such as those mentioned above.
Increasingly, however, physicists reserve the use of the term "fundamental physical constant" for dimensionless physical constants, such as the fine-structure constant "α".

Physical constant in the sense under discussion in this article should not be confused with other quantities called "constants" that are assumed to be constant in a given context without the implication that they are in any way fundamental, such as the "time constant" characteristic of a given system, or material constants, such as the Madelung constant, electrical resistivity, heat capacity.

Whereas the physical quantity indicated by a physical constant does not depend on the unit system used to express the quantity, the numerical values of dimensional physical constants do depend on choice of unit system. 
The term "physical constant" refers to the physical quantity, and not to the numerical value within any given system of units. For example, the speed of light is defined as having the numerical value of 299,792,458 in SI units, and as having the numerical value of 1 in natural units. While its numerical value can be defined at will by the choice of units, the speed of light itself is a single physical constant.

Any ratio between physical constants of the same dimensions results in a dimensionless physical constant, for example, the proton-to-electron mass ratio. Any relation between physical quantities can be expressed as a relation between dimensionless ratios via a process known as nondimensionalisation.

The term of "fundamental physical constant" is reserved for physical quantities which, according to the current state of knowledge, are regarded as immutable and as non-derivable from more fundamental principles. Notable examples are the speed of light "c", and the gravitational constant "G".

The fine-structure constant "α" is the best known dimensionless fundamental physical constant. It is the value of the elementary charge squared expressed in Planck units. This value has become a standard example when discussing the derivability or non-derivability of physical constants. Introduced by Arnold Sommerfeld, its value as determined at the time was consistent with 1/137. This motivated Arthur Eddington (1929) to construct an argument why its value might be 1/137 precisely, which related to the Eddington number, his estimate of the number of protons in the Universe. By the 1940s, it became clear that the value of the fine-structure constant deviates significantly from the precise value of 1/137, refuting Eddington's argument.

With the development of quantum chemistry in the 20th century, however, a vast number of previously inexplicable dimensionless physical constants "were" successfully computed from theory. In light of that, some theoretical physicists still hope for continued progress in explaining the values of other dimensionless physical constants.

It is known that the Universe would be very different if these constants took values significantly different from those we observe. For example, a few percent change in the value of the fine structure constant would be enough to eliminate stars like our Sun. This has prompted attempts at anthropic explanations of the values of some of the dimensionless fundamental physical constants.

Using dimensional analysis, it is possible to combine dimensional universal physical constants to define a system of units of measurement that has no reference to any human construct. Depending on the choice and arrangement of constants used, the resulting natural units may have useful physical meaning. For example, Planck units, shown in the table below, use "c", "G", "ħ", "ε" and "k" in such a manner to derive units relevant to unified theories such as quantum gravity.

The number of fundamental physical constants depends on the physical theory accepted as "fundamental". 
Currently, this is the theory of general relativity for gravitation and the Standard Model for electromagnetic, weak and strong nuclear interactions and the matter fields.
Between them, these theories account for a total of 19 independent fundamental constants. 
There is, however, no single "correct" way of enumerating them, as it is a matter of arbitrary choice which quantities are considered "fundamental" and which as "derived". Uzan (2011) lists 22 "unknown constants" in the fundamental theories, which give rise to 19 "unknown dimensionless parameters", as follows: 
The number of 19 independent fundamental physical constants is subject to change under possible extensions of the Standard Model, notably by the introduction of neutrino mass (equivalent to seven additional constants, i.e. 3 Yukawa couplings and 4 lepton mixing parameters).

The discovery of variability in any of these constants would be equivalent to the discovery of "new physics".

The question as to which constants are "fundamental" is neither straightforward nor meaningless, but a question of interpretation of the physical theory regarded as fundamental; as pointed out by Lévy-Leblond (1979), not all physical constants 
are of the same importance, with some having a deeper role than others.
Lévy-Leblond (1979) proposed a classification schemes of three types of fundamental constant: 
The same physical constant may move from one category to another as the understanding of its role deepens; this has notably happened to the speed of light, which was a class A constant (characteristic of light) when it was first measured, but became a class B constant (characteristic of electromagnetic phenomena) with the development of classical electromagnetism, and finally a class C constant with the discovery of special relativity.

By definition, fundamental physical constants are subject to measurement, so that their being constant (independent on both the time and position of the performance of the measurement) is necessarily an experimental result and subject to verification.

Paul Dirac in 1937 speculated that physical constants such as the gravitational constant or the fine-structure constant might be subject to change over time in proportion of the age of the universe. Experiments can in principle only put an upper bound on the relative change per year. For the fine-structure constant, this upper bound is comparatively low, at
roughly 10 per year (as of 2008).

The gravitational constant is much more difficult to measure with precision, and conflicting measurements in the 2000s have inspired the controversial suggestions of a periodic variation of its value in a 2015 paper. However, while its value is not known to great precision, the possibility of observing type Ia supernovae which happened in the universe's remote past, paired with the assumption that the physics involved in these events is universal, allows for an upper bound of less than 10 per year for the gravitational constant over the last nine billion years.

Similarly, an upper bound of the change in the proton-to-electron mass ratio has been placed at 10 over a period of 7 billion years (or 10 per year) in a 2012 study based on the observation of methanol in a distant galaxy.

It is problematic to discuss the proposed rate of change (or lack thereof) of a single "dimensional" physical constant in isolation. The reason for this is that the choice of a system of units may arbitrarily select as its basis, making the question of which constant is undergoing change an artefact of the choice of units.

For example, in SI units, the speed of light has been given a "defined" value in 1983. Thus, it was meaningful to experimentally measure the speed of light in SI units prior to 1983, but it is not so now. The proposed redefinition of SI base units, scheduled for 2018, seeks to express all SI base units in terms of fundamental physical constants.

Tests on the immutability of physical constants look at "dimensionless" quantities, i.e. ratios between quantities of like dimensions, in order to escape this problem. Changes in physical constants are not meaningful if they result in an "observationally indistinguishable" universe. For example, a "change" in the speed of light "c" would be meaningless if accompanied by a corresponding change in the elementary charge "e" so that the ratio (the fine-structure constant) remained unchanged.

Some physicists have explored the notion that if the dimensionless physical constants had sufficiently different values, our Universe would be so radically different that intelligent life would probably not have emerged, and that our Universe therefore seems to be fine-tuned for intelligent life. The anthropic principle states a logical truism: the fact of our existence as intelligent beings who can measure physical constants requires those constants to be such that beings like us can exist. There are a variety of interpretations of the constants' values, including that of a divine creator (the apparent fine-tuning is actual and intentional), or that ours is one universe of many in a multiverse (e.g. the Many-worlds interpretation of quantum mechanics), or even that, if information is an innate property of the universe and logically inseparable from consciousness, a universe without the capacity for conscious beings cannot exist.





</doc>
<doc id="23206" url="https://en.wikipedia.org/wiki?curid=23206" title="Parsley">
Parsley

Parsley or garden parsley (Petroselinum crispum) is a species of flowering plant in the family Apiaceae, native to the central Mediterranean region (southern Italy, Greece, Portugal, Spain, Malta, Morocco, Algeria, and Tunisia), naturalized elsewhere in Europe, and widely cultivated as an herb, a spice, and a vegetable.

Where it grows as a biennial, in the first year, it forms a rosette of tripinnate leaves long with numerous leaflets, and a taproot used as a food store over the winter.

Parsley is widely used in European, Middle Eastern, and American cooking. Curly leaf parsley is often used as a garnish. In central Europe, eastern Europe, and southern Europe, as well as in western Asia, many dishes are served with fresh green chopped parsley sprinkled on top. Root parsley is very common in central, eastern, and southern European cuisines, where it is used as a snack or a vegetable in many soups, stews, and casseroles.

The word "parsley" is a merger of the Old English" petersilie" (which is identical to the contemporary German word for "parsley": "Petersilie") and the Old French "peresil", both derived from Medieval Latin "petrosilium", from Latin "petroselinum", which is the latinization of the Greek πετροσέλινον ("petroselinon"), "rock-celery", from πέτρα ("petra"), "rock, stone", + σέλινον ("selinon"), "celery". Mycenaean Greek se-ri-no, in Linear B, is the earliest attested form of the word "selinon".

Garden parsley is a bright green, biennial plant in temperate climates, or an annual herb in subtropical and tropical areas.

Where it grows as a biennial, in the first year, it forms a rosette of tripinnate leaves 10–25 cm long with numerous 1–3 cm leaflets, and a taproot used as a food store over the winter. In the second year, it grows a flowering stem to tall with sparser leaves and flat-topped 3–10 cm diameter umbels with numerous 2 mm diameter yellow to yellowish-green flowers. The seeds are ovoid, 2–3 mm long, with prominent style remnants at the apex. One of the compounds of the essential oil is apiol. The plant normally dies after seed maturation.

Parsley is a source of flavonoids and antioxidants, especially luteolin, apigenin, folic acid, vitamin K, vitamin C, and vitamin A. Half a tablespoon (a gram) of dried parsley contains about 6.0 µg of lycopene and 10.7 µg of alpha carotene as well as 82.9 µg of lutein+zeaxanthin and 80.7 µg of beta carotene.

Excessive consumption of parsley should be avoided by pregnant women. Normal food quantities are safe for them to consume, but consuming excessively large amounts may have uterotonic effects.

Parsley grows best in moist, well-drained soil, with full sun. It grows best between , and usually is grown from seed. Germination is slow, taking four to six weeks, and it often is difficult because of furanocoumarins in its seed coat. Typically, plants grown for the leaf crop are spaced 10 cm apart, while those grown as a root crop are spaced 20 cm apart to allow for the root development.

Parsley attracts several species of wildlife. Some swallowtail butterflies use parsley as a host plant for their larvae; their caterpillars are black and green striped with yellow dots, and will feed on parsley for two weeks before turning into butterflies. Bees and other nectar-feeding insects also visit the flowers. Birds such as the goldfinch feed on the seeds.

In cultivation, parsley is subdivided into several cultivar groups, depending on the form of the plant, which is related to its end use. Often these are treated as botanical varieties, but they are cultivated selections, not of natural botanical origin.

The two main groups of parsley used as herbs are curly leaf (i.e.) ("P. crispum" crispum group; syn. "P. crispum" var. "crispum") and Italian, or flat leaf ("P. crispum" neapolitanum group; syn. "P. crispum" var. "neapolitanum"); of these, the neapolitanum group more closely resembles the natural wild species. Flat-leaved parsley is preferred by some gardeners as it is easier to cultivate, being more tolerant of both rain and sunshine, and is said to have a stronger flavor—although this is disputed—while curly leaf parsley is preferred by others because of its more decorative appearance in garnishing. A third type, sometimes grown in southern Italy, has thick leaf stems resembling celery.

Another type of parsley is grown as a root vegetable, the Hamburg root parsley ("P. crispum" radicosum group, syn. "P. crispum" var. "tuberosum"). This type of parsley produces much thicker roots than types cultivated for their leaves. Although seldom used in Britain and the United States, root parsley is common in central and eastern European cuisine, where it is used in soups and stews, or simply eaten raw, as a snack (similar to carrots).

Although root parsley looks similar to the parsnip, which is among its closest relatives in the family Apiaceae, its taste is quite different.

Parsley is widely used in Middle Eastern, European, Brazilian, and American cooking. Curly leaf parsley is used often as a garnish. Green parsley is used frequently as a garnish on potato dishes (boiled or mashed potatoes), on rice dishes (risotto or pilaf), on fish, fried chicken, lamb, goose, and steaks, as well in meat or vegetable stews (including shrimp creole, beef bourguignon, goulash, or chicken paprikash).

In central Europe, eastern Europe, and southern Europe, as well as in western Asia, many dishes are served with fresh green, chopped parsley sprinkled on top. In southern and central Europe, parsley is part of "bouquet garni", a bundle of fresh herbs used as an ingredient in stocks, soups, and sauces. Freshly chopped green parsley is used as a topping for soups such as chicken soup, green salads, or salads such as "salade Olivier", and on open sandwiches with cold cuts or "pâtés".

"Persillade" is a mixture of chopped garlic and chopped parsley in French cuisine.

Parsley is the main ingredient in Italian salsa verde, which is a mixed condiment of parsley, capers, anchovies, garlic, and sometimes bread, soaked in vinegar. It is an Italian custom to serve it with bollito misto or fish. "Gremolata", a mixture of parsley, garlic, and lemon zest, is a traditional accompaniment to the Italian veal stew, "ossobuco alla milanese".

In England, parsley sauce is a roux-based sauce, commonly served over fish or gammon.

Root parsley is very common in Central, Eastern, and Southern European cuisines, where it is used as a snack or a vegetable in many soups, stews, and casseroles, and as ingredient for broth.

In Brazil, freshly chopped parsley () and freshly chopped scallion () are the main ingredients in the herb seasoning called (literally "green aroma"), which is used as key seasoning for major Brazilian dishes, including meat, chicken, fish, rice, beans, stews, soups, vegetables, salads, condiments, sauces, and stocks. is sold in food markets as a bundle of both types of fresh herbs. In some Brazilian regions, chopped parsley may be replaced by chopped coriander (also called cilantro, in Portuguese) in the mixture.

Parsley is a key ingredient in several Middle Eastern salads such as Lebanese "tabbouleh"; it is also often mixed in with the chickpeas and/or fava beans while making falafel (that gives the inside of the falafel its green color).




</doc>
<doc id="23209" url="https://en.wikipedia.org/wiki?curid=23209" title="Peppermint">
Peppermint

Peppermint (Mentha" × "piperita, also known as "Mentha balsamea" Wild.) is a hybrid mint, a cross between watermint and spearmint. Indigenous to Europe and the Middle East, the plant is now widely spread and cultivated in many regions of the world. It is occasionally found in the wild with its parent species.

Peppermint was first described in 1753 by Carl Linnaeus from specimens that had been collected in England; he treated it as a species, but it is now universally agreed to be a hybrid.
It is a herbaceous rhizomatous perennial plant that grows to be tall, with smooth stems, square in cross section. The rhizomes are wide-spreading, fleshy, and bear fibrous roots. The leaves can be long and broad. They are dark green with reddish veins, and they have an acute apex and coarsely toothed margins. The leaves and stems are usually slightly fuzzy. The flowers are purple, long, with a four-lobed corolla about diameter; they are produced in whorls (verticillasters) around the stem, forming thick, blunt spikes. Flowering season lasts from mid- to late summer. The chromosome number is variable, with 2n counts of 66, 72, 84, and 120 recorded. Peppermint is a fast-growing plant; once it sprouts, it spreads very quickly.

Peppermint typically occurs in moist habitats, including stream sides and drainage ditches. Being a hybrid, it is usually sterile, producing no seeds and reproducing only vegetatively, spreading by its runners. If placed, it can grow almost anywhere.

Outside of its native range, areas where peppermint was formerly grown for oil often have an abundance of feral plants, and it is considered invasive in Australia, the Galápagos Islands, New Zealand, and the United States in the Great Lakes region, noted since 1843.

Peppermint generally grows best in moist, shaded locations, and expands by underground rhizomes. Young shoots are taken from old stocks and dibbled into the ground about 1.5 feet apart. They grow quickly and cover the ground with runners if it is permanently moist. For the home gardener, it is often grown in containers to restrict rapid spreading. It grows best with a good supply of water, without being water-logged, and planted in areas with part-sun to shade.

The leaves and flowering tops are used; they are collected as soon as the flowers begin to open and can be dried. The wild form of the plant is less suitable for this purpose, with cultivated plants having been selected for more and better oil content. They may be allowed to lie and wilt a little before distillation, or they may be taken directly to the still.

A number of cultivars have been selected for garden use:

Commercial cultivars may include

In 2014, world production of peppermint was 92,296 tonnes, led by Morocco with 92% of the world total reported by FAOSTAT of the United Nations. Argentina accounted for 8% of the world total.

In the United States, Oregon and Washington produce most of the country's peppermint, the leaves of which are processed for the essential oil to produce flavorings mainly for chewing gum and toothpaste.

Peppermint has a high menthol content. The oil also contains menthone and carboxyl esters, particularly menthyl acetate. Dried peppermint typically has 0.3–0.4% of volatile oil containing menthol (7–48%), menthone (20–46%), menthyl acetate (3–10%), menthofuran (1–17%) and 1,8-cineol (3–6%). Peppermint oil also contains small amounts of many additional compounds including limonene, pulegone, caryophyllene and pinene.

Peppermint contains terpenoids and flavonoids such as eriocitrin, hesperidin, and kaempferol 7-O-rutinoside.

Peppermint oil has a high concentration of natural pesticides, mainly pulegone (found mainly in "Mentha arvsis" var. "piperascens" cornmint, field mint, Japanese mint, and to a lesser extent (6,530 ppm) in "Mentha" × "piperita" subsp. "nothosubsp. piperita") and menthone. It is known to repel some pest insects, including mosquitos, and has uses in organic gardening.

The chemical composition of the essential oil from peppermint ("Mentha" x "piperita" L.) was analyzed by GC/FID and GC-MS. The main constituents were menthol (40.7%) and menthone (23.4%). Further components were (+/-)-menthyl acetate, 1,8-cineole, limonene, beta-pinene and beta-caryophyllene.

Peppermint oil is under preliminary research for its potential as a short-term treatment for irritable bowel syndrome, and has supposed uses in traditional medicine for minor ailments. Peppermint oil and leaves have a cooling effect when used topically for muscle pain, nerve pain, relief from itching, or as a fragrance. High oral doses of peppermint oil (500 mg) can cause mucosal irritation and mimic heartburn. As an aroma, peppermint may have memory- and alertness-enhancing properties.

Fresh or dried peppermint leaves are often used alone in peppermint tea or with other herbs in herbal teas (tisanes, infusions). Peppermint is used for flavouring ice cream, candy, fruit preserves, alcoholic beverages, chewing gum, toothpaste, and some shampoos, soaps and skin care products.

Menthol activates cold-sensitive TRPM8 receptors in the skin and mucosal tissues, and is the primary source of the cooling sensation that follows the topical application of peppermint oil.

Peppermint oil is also used in construction and plumbing to test for the tightness of pipes and disclose leaks by its odor.

Medicinal uses of peppermint have not been approved as effective or safe by the US Food and Drug Administration. With caution that the concentration of the peppermint constituent pulegone should not exceed 1% (140 mg), peppermint preparations are considered safe by the European Medicines Agency when used in topical formulations for adult subjects. Undiluted peppermint essential oil is safe for oral intake when only a few drops are used.

Although peppermint is commonly available as a herbal supplement, there are no established, consistent manufacturing standards for it, and some peppermint products may be contaminated with toxic metals or other substituted compounds. Skin rashes, irritation, or an allergic reaction may result from applying peppermint oil to the skin, and its use on the face or chest of young children may cause side effects if the oil menthol is inhaled. A common side effect from oral intake of peppermint oil or capsules is heartburn. Oral use of peppermint products may have adverse effects when used with iron supplements, cyclosporine, medicines for heart conditions or high blood pressure, or medicines to decrease stomach acid.



</doc>
<doc id="23210" url="https://en.wikipedia.org/wiki?curid=23210" title="Pseudorandomness">
Pseudorandomness

A pseudorandom process is a process that appears to be random but is not. Pseudorandom sequences typically exhibit statistical randomness while being generated by an entirely deterministic causal process. Such a process is easier to produce than a genuinely random one, and has the benefit that it can be used again and again to produce exactly the same numbers, which is useful for testing and fixing software.

To generate truly random numbers would require precise, accurate, and repeatable system measurements of absolutely non-deterministic processes. Linux uses, for example, various system timings (like user keystrokes, I/O, or least-significant digit voltage measurements) to produce a pool of random numbers. It attempts to constantly replenish the pool, depending on the level of importance, and so will issue a random number. This system is an example, and similar to those of dedicated hardware random number generators.

The generation of random numbers has many uses (mostly in statistics, for random sampling, and simulation). Before modern computing, researchers requiring random numbers would either generate them through various means (dice, cards, roulette wheels, etc.) or use existing random number tables.

The first attempt to provide researchers with a ready supply of random digits was in 1927, when the Cambridge University Press published a table of 41,600 digits developed by L.H.C. Tippett. In 1947, the RAND Corporation generated numbers by the electronic simulation of a roulette wheel; the results were eventually published in 1955 as "A Million Random Digits with 100,000 Normal Deviates".

John von Neumann was a pioneer in computer-based random number generators. In 1949, Derrick Henry Lehmer invented the linear congruential generator, which was for a long time used in most pseudorandom number generators. Today, most generators in use are based on linear recurrence (for instance, the Xorshift family). With the spread of the use of computers, algorithmic pseudorandom number generators replaced random number tables, and "true" random number generators (hardware random number generators) are used in only a few cases.

A pseudorandom variable is a variable which is created by a deterministic algorithm, often a computer program or subroutine, which in most cases takes random bits as input. The pseudorandom string will typically be longer than the original random string, but less random (less entropic in the information theory sense). This can be useful for randomized algorithms.

Pseudorandom number generators are widely used in such applications as computer modeling (e.g., Markov chains), statistics, experimental design, etc.

In theoretical computer science, a distribution is pseudorandom against a class of adversaries if no adversary from the class can distinguish it from the uniform distribution with significant advantage.
This notion of pseudorandomness is studied in computational complexity theory and has applications to cryptography.

Formally, let "S" and "T" be finite sets and let F = {"f": "S" → "T"} be a class of functions. A distribution D over "S" is ε-pseudorandom against F if for every "f" in F, the statistical distance between the distributions "f"("X"), where "X" is sampled from D, and "f"("Y"), where "Y" is sampled from the uniform distribution on "S", is at most ε.

In typical applications, the class F describes a model of computation with bounded resources
and one is interested in designing distributions D with certain properties that are pseudorandom against F. The distribution D is often specified as the output of a pseudorandom generator.

Though random numbers are needed in cryptography, the use of pseudorandom number generators (whether hardware or software or some combination) is insecure. When random values are required in cryptography, the goal is to make a message as hard to crack as possible, by eliminating or obscuring the parameters used to encrypt the message (the key) from the message itself or from the context in which it is carried. Pseudorandom sequences are deterministic and reproducible; all that is required in order to discover and reproduce a pseudorandom sequence is the algorithm used to generate it and the initial seed. So the entire sequence of numbers is only as powerful as the randomly chosen parts - sometimes the algorithm and the seed, but usually only the seed.

There are many examples in cryptographic history of ciphers, otherwise excellent, in which random choices were not random enough and security was lost as a direct consequence. The World War II Japanese PURPLE cipher machine used for diplomatic communications is a good example. It was consistently broken throughout World War II, mostly because the "key values" used were insufficiently random. They had patterns, and those patterns made any intercepted traffic readily decryptable. Had the keys (i.e. the initial settings of the stepping switches in the machine) been made unpredictably (i.e. randomly), that traffic would have been much harder to break, and perhaps even secure in practice.

Users and designers of cryptography are strongly cautioned to treat their randomness needs with the utmost care. Absolutely nothing has changed with the era of computerized cryptography, except that patterns in pseudorandom data are easier to discover than ever before. Randomness is, if anything, more important than ever.

Since pseudorandom numbers are in fact deterministic, a given seed will always determine the same pseudorandom number. This attribute is used in security, in the form of rolling code to avoid replay attacks, in which a command would be intercepted to be used by a thief at a later time. This is defeated using a pseudorandom number generator to generate a different key each time. Since the pseudorandom can be synchronized between the two systems, an intercepted key would not work a second time, since the interceptor cannot guess the next number from an intercepted number.

A Monte Carlo method simulation is defined as any method that utilizes sequences of random numbers to perform the simulation. Monte Carlo simulations are applied to many topics including quantum chromodynamics, cancer radiation therapy, traffic flow, stellar evolution and VLSI design. All these simulations require the use of random numbers and therefore pseudorandom number generators, which makes creating random-like numbers very important.

A simple example of how a computer would perform a Monte Carlo simulation is the calculation of π. If a square enclosed a circle and a point were randomly chosen inside the square the point would either lie inside the circle or outside it. If the process were repeated many times, the ratio of the random points that lie inside the circle to the total number of random points in the square would approximate the ratio of the area of the circle to the area of the square. From this we can estimate pi, as shown in the Python code below utilizing a SciPy package to generate pseudorandom numbers with the MT19937 algorithm. Note that this method is a computationally inefficient way to numerically approximate π.




</doc>
<doc id="23212" url="https://en.wikipedia.org/wiki?curid=23212" title="Poales">
Poales

The Poales are a large order of flowering plants in the monocotyledons, and includes families of plants such as the grasses, bromeliads, and sedges. Sixteen plant families are currently recognized by botanists to be part of Poales.

The flowers are typically small, enclosed by bracts, and arranged in inflorescences (except in three species of the genus "Mayaca", which possess very reduced, one-flowered inflorescences). The flowers of many species are wind pollinated; the seeds usually contain starch.

The APG III system (2009) accepts the order within a monocot clade called commelinids, and accepts the following 16 families:
The earlier APG system (1998) adopted the same placement of the order, although it used the spelling "commelinoids". It did not include the Bromeliaceae and Mayaceae, but had the additional families Prioniaceae (now included in Thurniaceae), Sparganiaceae (now in Typhaceae), and Hydatellaceae (now transferred out of the monocots; recently discovered to be an 'early-diverging' lineage of flowering plants).

The morphology-based Cronquist system did not include an order named Poales, assigning these families to the orders Bromeliales, Cyperales, Hydatellales, Juncales, Restionales and Typhales.

In early systems, an order including the grass family did not go by the name Poales but by a descriptive botanical name such as Graminales in the Engler system (update of 1964) and in the Hutchinson system (first edition, first volume, 1926), Glumiflorae in the Wettstein system (last revised 1935) or Glumaceae in the Bentham & Hooker system (third volume, 1883).

The earliest fossils attributed to the Poales date to the late Cretaceous period about million years ago, though some studies (e.g., Bremer, 2002) suggest the origin of the group may extend to nearly 115 million years ago, likely in South America. The earliest known fossils include pollen and fruits.

The phylogenetic position of Poales within the commelinids was difficult to resolve, but an analysis using complete chloroplast DNA found support for Poales as sister group of Commelinales plus Zingiberales. Major lineages within the Poales have been referred to as bromeliad, cyperid, xyrid, graminid, and restiid clades. A phylogenetic analysis resolved most relationships within the order but found weak support for the monophyly of the cyperid clade. The relationship between Centrolepidaceae and Restoniaceae within the restiid clade remains unclear; the first may actually be embedded in the latter.

The four most species-rich families in the order are: 
The Poales are the most economically important order of monocots and possibly the most important order of plants in general. Within the order, by far the most important family economically is the family of grasses (Poaceae, syn. Gramineae), which includes the starch staples barley, maize, millet, rice, and wheat as well as bamboos (mostly used structurally, like wood, but somewhat as vegetables), and a few "seasonings" like sugarcane and lemongrass. Graminoids, especially the grasses, are typically dominant in open (low moisture but not yet arid, or also fire climax) habitats like prairie/steppe and savannah and thus form a large proportion of the forage of grazing livestock. Possibly due to pastoral nostalgia or simply a desire for open areas for play, they dominate most Western yards as lawns, which consume vast sums of money in upkeep (artificial grazing—mowing—for aesthetics and to keep the allergenic flowers suppressed, irrigation, and fertilizer). Many Bromeliaceae are used as ornamental plants (and one, the pineapple, is internationally grown in the tropics for fruit). Many wetland species of sedges, rushes, grasses, and cattails are important habitat plants for waterfowl, are used in weaving chair seats, and (especially cattails) were important pre-agricultural food sources for man. Two sedges, chufa (Cyperus esculentus, also a significant weed) and water chestnut (Eleocharis dulcis) are still at least locally important wetland starchy root crops.




</doc>
<doc id="23213" url="https://en.wikipedia.org/wiki?curid=23213" title="Political correctness">
Political correctness

The term political correctness (adjectivally: politically correct; commonly abbreviated to PC or P.C.) is used to describe language, policies, or measures that are intended to avoid offense or disadvantage to members of particular groups in society. Since the late 1980s, the term has come to refer to avoiding language or behavior that can be seen as excluding, marginalizing, or insulting groups of people considered disadvantaged or discriminated against, especially groups defined by sex or race. In public discourse and the media, it is generally used as a pejorative, implying that these policies are excessive.

The contemporary usage of the term emerged from conservative criticism of the New Left in the late 20th century. This usage was popularized by a number of articles in the "The New York Times" and other media throughout the 1990s, and was widely used in the debate about Allan Bloom's 1987 book "The Closing of the American Mind", and gained further currency in response to Roger Kimball's "Tenured Radicals" (1990), and conservative author Dinesh D'Souza's 1991 book "Illiberal Education", in which he condemned what he saw as liberal efforts to advance self-victimization and multiculturalism through language, affirmative action, and changes to the content of school and university curricula.

Commentators on the political left contend that conservatives use the concept of political correctness to downplay and divert attention from substantively discriminatory behavior against disadvantaged groups. They also argue that the political right enforces its own forms of political correctness to suppress criticism of its favored constituencies and ideologies. The term has played a major role in the United States culture war between liberals and conservatives.

The term "politically correct" was used infrequently until the latter part of the 20th century. This earlier use did not communicate the social disapproval usually implied in more recent usage. In 1793, the term "politically correct" appeared in a U.S. Supreme Court judgment of a political lawsuit. The term also had use in other English-speaking countries in the 1800s. William Safire states that the first recorded use of the term in the typical modern sense is by Toni Cade Bambara in the 1970 anthology "The Black Woman". The term probably entered use in the United Kingdom around 1975.

In the early-to-mid 20th century, the phrase "politically correct" was used to describe strict adherence to a range of ideological orthodoxies. In 1934, "The New York Times" reported that Nazi Germany was granting reporting permits "only to pure 'Aryans' whose opinions are politically correct."

As Marxist-Leninist movements gained political power, the phrase came to be associated with accusations of dogmatic application of doctrine, in debates between American Communists and American Socialists. This usage referred to the Communist party line which, in the eyes of the Socialists, provided "correct" positions on all political matters. According to American educator Herbert Kohl, writing about debates in New York in the late 1940s and early 1950s,

In the 1970s, the American New Left began using the term "politically correct". In the essay "The Black Woman: An Anthology" (1970), Toni Cade Bambara said that "a man cannot be politically correct and a [male] chauvinist, too." Thereafter, the term was often used as self-critical satire. Debra L. Shultz said that "throughout the 1970s and 1980s, the New Left, feminists, and progressives... used their term 'politically correct' ironically, as a guard against their own orthodoxy in social change efforts." "PC" is used in the comic book "Merton of the Movement", by Bobby London, which was followed by the term "ideologically sound", in the comic strips of Bart Dickon. In her essay "Toward a feminist Revolution" (1992) Ellen Willis said: "In the early eighties, when feminists used the term 'political correctness', it was used to refer sarcastically to the anti-pornography movement's efforts to define a 'feminist sexuality'."

Stuart Hall suggests one way in which the original use of the term may have developed into the modern one:

Allan Bloom's 1987 book "The Closing of the American Mind" heralded a debate about "political correctness" in American higher education in the 1980s and 1990s. Professor of English literary and cultural studies at CMU Jeffrey J. Williams wrote that the "assault on ... political correctness that simmered through the Reagan years, gained bestsellerdom with Bloom's Closing of the American Mind." According to Z.F. Gamson, Bloom's book "attacked the faculty for 'political correctness'." Prof. of Social Work at CSU Tony Platt says the "campaign against 'political correctness'" was launched by Bloom's book in 1987.

An October 1990 "New York Times" article by Richard Bernstein is credited with popularizing the term. At this time, the term was mainly being used within academia: "Across the country the term p.c., as it is commonly abbreviated, is being heard more and more in debates over what should be taught at the universities". Nexis citations in "arcnews/curnews" reveal only seventy total citations in articles to "political correctness" for 1990; but one year later, Nexis records 1532 citations, with a steady increase to more than 7000 citations by 1994. In May 1991, "The New York Times" had a follow-up article, according to which the term was increasingly being used in a wider public arena:

The previously obscure far-left term became common currency in the lexicon of the conservative social and political challenges against progressive teaching methods and curriculum changes in the secondary schools and universities of the U.S. Policies, behavior, and speech codes that the speaker or the writer regarded as being the imposition of a liberal orthodoxy, were described and criticized as "politically correct". In May 1991, at a commencement ceremony for a graduating class of the University of Michigan, then U.S. President George H.W. Bush used the term in his speech: "The notion of political correctness has ignited controversy across the land. And although the movement arises from the laudable desire to sweep away the debris of racism and sexism and hatred, it replaces old prejudice with new ones. It declares certain topics off-limits, certain expression off-limits, even certain gestures off-limits."

After 1991, its use as a pejorative phrase became widespread amongst conservatives in the US. It became a key term encapsulating conservative concerns about the left in culture and political debate more broadly, as well as in academia. Two articles on the topic in late 1990 in "Forbes" and "Newsweek" both used the term "thought police" in their headlines, exemplifying the tone of the new usage, but it was Dinesh D'Souza's "Illiberal Education: The Politics of Race and Sex on Campus" (1991) which "captured the press's imagination." Similar critical terminology was used by D'Souza for a range of policies in academia around victimization, supporting multiculturalism through affirmative action, sanctions against anti-minority hate speech, and revising curricula (sometimes referred to as "canon busting"). These trends were at least in part a response to multiculturalism and the rise of identity politics, with movements such as feminism, gay rights movements and ethnic minority movements. That response received funding from conservative foundations and think tanks such as the John M. Olin Foundation, which funded several books such as D'Souza's.

Herbert Kohl, in 1992, commented that a number of neoconservatives who promoted the use of the term "politically correct" in the early 1990s were former Communist Party members, and, as a result, familiar with the Marxist use of the phrase. He argued that in doing so, they intended "to insinuate that egalitarian democratic ideas are actually authoritarian, orthodox and Communist-influenced, when they oppose the right of people to be racist, sexist, and homophobic."

During the 1990s, conservative and right-wing politicians, think-tanks, and speakers adopted the phrase as a pejorative descriptor of their ideological enemies – especially in the context of the Culture Wars about language and the content of public-school curricula. Roger Kimball, in "Tenured Radicals", endorsed Frederick Crews's view that PC is best described as "Left Eclecticism", a term defined by Kimball as "any of a wide variety of anti-establishment modes of thought from structuralism and poststructuralism, deconstruction, and Lacanian analyst to feminist, homosexual, black, and other patently political forms of criticism." 

Liberal commentators have argued that the conservatives and reactionaries who used the term did so in effort to divert political discussion away from the substantive matters of resolving societal discrimination – such as racial, social class, gender, and legal inequality – against people whom conservatives do not consider part of the social mainstream. Jan Narveson wrote that "that phrase was born to live between scare-quotes: it suggests that the operative considerations in the area so called are "merely" political, steamrolling the genuine reasons of principle for which we ought to be acting..." Commenting in 2001, one such British journalist, Polly Toynbee, said "the phrase is an empty, right-wing smear, designed only to elevate its user", and, in 2010, "the phrase 'political correctness' was born as a coded cover for all who still want to say "Paki", "spastic", or "queer"". Another British journalist, Will Hutton, wrote in 2001:

Glenn Loury wrote in 1994 that: "to address the subject of "political correctness" when power and authority within the academic community is being contested by parties on either side of that issue, is to invite scrutiny of one's arguments by would-be "friends" and "enemies." Combatants from the left and the right will try to assess whether a writer is "for them" or "against them."

In the US, the term has been widely used in books and journals, but in Britain, usage has been confined mainly to the popular press. Many such authors and popular-media figures, particularly on the right, have used the term to criticize what they see as bias in the media. William McGowan argues that journalists get stories wrong or ignore stories worthy of coverage, because of what McGowan perceives to be their liberal ideologies and their fear of offending minority groups. Robert Novak, in his essay "Political Correctness Has No Place in the Newsroom", used the term to blame newspapers for adopting language use policies that he thinks tend to excessively avoid the appearance of bias. He argued that political correctness in language not only destroys meaning but also demeans the people who are meant to be protected. Authors David Sloan and Emily Hoff claim that in the US, journalists shrug off concerns about political correctness in the newsroom, equating the political correctness criticisms with the old "liberal media bias" label.

Much of the modern debate on the term was sparked by conservative critiques of liberal bias in academia and education, and conservatives have used it as a major line of attack since.
University of Pennsylvania professor Alan Charles Kors and lawyer Harvey A. Silverglate connect speech codes in US universities to philosopher Herbert Marcuse. They claim that speech codes create a "climate of repression", arguing that they are based on "Marcusean logic". The speech codes, "mandate a redefined notion of "freedom", based on the belief that the imposition of a moral agenda on a community is justified", a view which, "requires less emphasis on individual rights and more on assuring "historically oppressed" persons the means of achieving equal rights." Kors and Silverglate later established the Foundation for Individual Rights in Education (FIRE), which campaigns against infringement of rights of due process, in particular "speech codes". Similarly, a common conservative criticism of higher education in the United States is that the political views of the faculty are much more liberal than the general population, and that this situation contributes to an atmosphere of political correctness.

Groups who oppose certain generally accepted scientific views about evolution, second-hand tobacco smoke, AIDS, global warming, race, and other politically contentious scientific matters have used the term "political correctness" to describe what they view as unwarranted rejection of their perspective on these issues by a scientific community they feel is corrupted by liberal politics.

"Political correctness" is a label typically used to describe liberal terms and actions, but not for equivalent attempts to mold language and behavior on the right. However, the term "right-wing political correctness" is sometimes applied by commentators, especially when drawing parallels: in 1995, one author used the term "conservative correctness" arguing, in relation to higher education, that "critics of political correctness show a curious blindness when it comes to examples of conservative correctness. Most often, the case is entirely ignored or censorship of the Left is justified as a positive virtue. [...] A balanced perspective was lost, and everyone missed the fact that people on all sides were sometimes censored."

In 2003, French fries and French toast were renamed "Freedom fries" and "Freedom toast" in three U.S. House of Representatives cafeterias in response to France's opposition to the proposed invasion of Iraq; this was described as "polluting the already confused concept of political correctness." In 2004, then Australian Labor leader Mark Latham described conservative calls for "civility" in politics as "the new political correctness."

In 2012, Paul Krugman wrote: "the big threat to our discourse is right-wing political correctness, which – unlike the liberal version – has lots of power and money behind it. And the goal is very much the kind of thing Orwell tried to convey with his notion of Newspeak: to make it impossible to talk, and possibly even think, about ideas that challenge the established order."

After Mike Pence was booed at a November 2016 performance of Hamilton, president-elect Trump called it harassment and asked for "safe place". Chrissy Teigen commented that it was "the very thing him [sic] and his supporters make fun of as liberal political correctness."

Alex Nowrasteh of the Cato Institute defined the right's own version of political correctness as “patriotic correctness”. Vox editor Dara Lind summarized the definition as "a brand of right-wing hypersensitivity that gets just as offended by insults to American pride and patriotism (like protests against the president-elect or “The Star-Spangled Banner”) as any college activist gets over insults to diversity." Jim Geraghty of National Review replied to Nowrasteh, stating that "There is no right-wing equivalent to political correctness."

In 2015 and 2016, leading up to the 2016 United States presidential election, Republican candidate Donald Trump used political correctness as a common target in his rhetoric. According to Trump, Barack Obama and Hillary Clinton were willing to let ordinary Americans suffer because their first priority was political correctness.

In a column for "the Huffington Post", Eric Mink characterized Trump's concept of "political correctness":

Political correctness is a controversial social force in a nation with a constitutional guarantee of freedom of expression, and it raises legitimate issues well worth discussing and debating. But that’s not what Trump is doing. He’s not a rebel speaking unpopular truths to power. He’s not standing up for honest discussions of deeply contentious issues. He’s not out there defying rules handed down by elites to control what we say. All Trump’s defying is common decency.
Following the 2016 election, "Los Angeles Times" columnist Jessica Roy wrote that "political correctness" is one of the key terms used by the American alt-right.

Some conservative commentators in the West argue that "political correctness" and multiculturalism are part of a conspiracy with the ultimate goal of undermining Judeo-Christian values. This theory, which holds that political correctness originates from the critical theory of the Frankfurt School as part of a conspiracy that its proponents call "Cultural Marxism", is generally known as the Frankfurt School conspiracy theory by academics. The theory originated with Michael Minnicino's 1992 essay "New Dark Age: Frankfurt School and 'Political Correctness'", published in a Lyndon LaRouche movement journal. In 2001, conservative commentator Patrick Buchanan wrote in "The Death of the West" that "political correctness is cultural Marxism", and that "its trademark is intolerance".

In the United States, left forces of "political correctness" have been blamed for censorship, with "Time" citing campaigns against violence on network television as contributing to a "mainstream culture [which] has become cautious, sanitized, scared of its own shadow" because of "the watchful eye of the p.c. police", even though in John Wilson's view protests and advertiser boycotts targeting TV shows are generally organized by right-wing religious groups campaigning against violence, sex, and depictions of homosexuality on television.

In the United Kingdom, some newspapers reported that a nursery school had altered the nursery rhyme "Baa Baa Black Sheep" to read "Baa Baa Rainbow Sheep" and had banned the original. But it was later reported that in fact the Parents and Children Together (PACT) nursery had the children "turn the song into an action rhyme... They sing happy, sad, bouncing, hopping, pink, blue, black and white sheep etc." This story was widely circulated and later extended to suggest that other language bans applied to the terms "black coffee" and "blackboard". "Private Eye" magazine reported that similar stories had been published in the British press since "The Sun" first ran them in 1986.

Political correctness is often satirized, for example in "The PC Manifesto" (1992) by Saul Jerushalmy and Rens Zbignieuw X, and "Politically Correct Bedtime Stories" (1994) by James Finn Garner, which presents fairy tales re-written from an exaggerated politically correct perspective. In 1994, the comedy film "PCU" took a look at political correctness on a college campus.

Other examples include the television program "Politically Incorrect", George Carlin’s "Euphemisms" routine, and "The Politically Correct Scrapbook". The popularity of the "South Park" cartoon program led to the creation of the term ""South Park" Republican" by Andrew Sullivan, and later the book "South Park Conservatives" by Brian C. Anderson. In its Season 19 (2015), "South Park" introduced the character PC Principal, who embodies the principle, to poke fun at the principle of political correctness.

"The Colbert Report"'s host Stephen Colbert often talked, satirically, about the "PC Police".

Graham Good, an academic at the University of British Columbia, wrote that the term was widely used in debates on university education in Canada. Writing about a 1995 report on the Political Science department at his university, he concluded:
"Political correctness" has become a popular phrase because it catches a certain kind of self-righteous and judgmental tone in some and a pervasive anxiety in others – who, fearing that they may do something wrong, adjust their facial expressions, and pause in their speech to make sure they are not doing or saying anything inappropriate. The climate this has created on campuses is at least as bad in Canada as in the United States.

In Hong Kong, as the 1997 handover drew nearer, greater control over the press was exercised by both owners and the Chinese state. This had a direct impact on news coverage of relatively sensitive political issues. The Chinese authorities exerted pressure on individual newspapers to take pro-Beijing stances on controversial issues. Tung Chee-hwa's policy advisers and senior bureaucrats increasingly linked their actions and remarks to "political correctness." Zhaojia Liu and Siu-kai Lau, writing in "The first Tung Chee-hwa administration : the first five years of the Hong Kong Special Administrative Region", said that "Hong Kong has traditionally been characterized as having freedom of speech and freedom of press, but that an unintended consequence of emphasizing political 'correctness' is to limit the space for such freedom of expression."

In New Zealand, controversies over PC surfaced during the 1990s regarding the social studies school curriculum.




</doc>
<doc id="23219" url="https://en.wikipedia.org/wiki?curid=23219" title="Ploidy">
Ploidy

Ploidy () is the number of complete sets of chromosomes in a cell, and hence the number of possible alleles for autosomal and pseudoautosomal genes.

Somatic cells, tissues and individuals can be described according to the number of sets present (the ploidy level): monoploid (1 set), diploid (2 sets), triploid (3 sets), tetraploid (4 sets), pentaploid (5 sets), hexaploid (6 sets), heptaploid or septaploid (7 sets), etc. The generic term polyploid is used to describe cells with three or more chromosome sets.

Humans are diploid organisms, carrying two complete sets of chromosomes: one set of 23 chromosomes from their father and one set of 23 chromosomes from their mother. The two sets combined provide a full complement of 46 chromosomes. This total number of chromosomes is called the chromosome number. The zygotic number is defined as the number of chromosomes in zygotic cells. Human zygotes are diploid, hence with a zygotic number of 46.

When a species has a varying chromosome number, e.g. a diploid and tetraploid form, the chromosome number is called diploid number in the diploid form, and tetraploid number in the tetraploid form.

The number of chromosomes found in a single complete set of chromosomes is called the monoploid number ("x"). In most animals, the haploid number ("n") is unique to gametes (sperm or egg cells), and refers to the total number of chromosomes found in a gamete, which under normal conditions is half the total number of chromosomes in a somatic cell.

The haploid number for humans (half of 46) is 23; and the monoploid number equals 46 divided by the ploidy level of 2, which is also 23. When a human germ cell undergoes meiosis the two sets of 23 chromosomes are split in half to form gametes. After fusion of a male and a female gamete (fertilization) both containing 1 set of 23 chromosomes, the resulting zygote has 46 chromosomes: 2 sets of 23 chromosomes (22 autosomes, and 1 allosome).

The common potato ("Solanum tuberosum") is an example of a tetraploid organism, carrying four sets of chromosomes. The potato plant inherits two sets of 12 chromosomes from the pollen parent, and two sets of 12 chromosomes from the ovule parent. The four sets combined provide a full complement of 48 chromosomes. The haploid number (half of 48) is 24. The monoploid number equals the chromosome number divided by the ploidy level: 48 chromosomes in total divided by a ploidy level of 4 equals a monoploid number of 12.

The commercial common potato crop is propagated vegetatively (asexual reproduction through mitosis), in which case new individuals are produced from a single parent, without the involvement of gametes and fertilization, and all the offspring are genetically equal to each other and to the parent.

Because the chromosome number is generally reduced only by the specialized process of meiosis, the somatic cells of the body inherit and maintain the chromosome number of the zygote. However, in many situations somatic cells double their copy number by means of endoreduplication as an aspect of cellular differentiation. For example, the hearts of two-year-old children contain 85% diploid and 15% tetraploid nuclei, but by 12 years of age the proportions become approximately equal, and adults examined contained 27% diploid, 71% tetraploid and 2% octaploid nuclei.

When a germ cell with an uneven number of chromosomes undergoes meiosis, the chromosomes can't be evenly divided between two cells resulting in aneuploid gametes. Triploid organisms for instance are usually sterile. Because of this, triploidy is a common way of making seedless fruit such as bananas and watermelons. If the fertilization of human gametes results in 3 sets of chromosomes the condition is called triploid syndrome.

The term "ploidy" is a back-formation from "haploidy" and "diploidy". Ploid is a combination of Ancient Greek -παλτος (-paltos), -πλος (-plos), -πλόος (-plóos, "fold"), and -oid from Ancient Greek -ειδής (-eidḗs), -οειδής (-oeidḗs), from εἶδος (eîdos, "form, likeness"). The principal meaning of the Greek word ἁπλόος "haplóos" is "two-fold", from ἅμα, which means, "at once, at the same time". From this comes the secondary sense of "single", since folding double produces a unity. It is in this latter sense that it appears in modern genetics. διπλόος "diplóos" means "duplex" or "two-fold". Diploid therefore means "duplex-shaped" (compare 'humanoid', "human-shaped").

Eduard Strasburger, coined the terms "haploid" and "diploid" in 1905:

Some authors suggest that Strasburger based the terms on Weismann's conception of the id (or germ plasm), hence haplo-"id" and diplo-"id". The two terms were brought into the English language from German through William Henry Lang's 1908 translation of a 1906 textbook by Strasburger and colleagues.

Technically, ploidy refers to the nucleus. Though at times authors may report the total ploidy of all nuclei present within the cell membrane of a syncytium, usually the ploidy of the nuclei present will be described. For example, a fungal dikaryon with two haploid nuclei is distinguished from the diploid in which the chromosomes share a nucleus and can be shuffled together. Nonetheless, because in most situations there is only one nucleus, it is commonplace to speak of the ploidy of a cell.

It is possible on rare occasions for ploidy to increase in the germline, which can result in polyploid offspring and ultimately polyploid "species". This is an important evolutionary mechanism in both plants and animals. As a result, it becomes desirable to distinguish between the ploidy of a species or variety as it presently breeds and that of an ancestor. The number of chromosomes in the ancestral (non-homologous) set is called the monoploid number ("x"), and is distinct from the haploid number ("n") in the organism as it now reproduces. Both numbers "n", and "x", apply to every cell of a given organism.

Common wheat is an organism where "x" and "n" differ. It has six sets of chromosomes, two sets from each of three different diploid species that are its distant ancestors. The somatic cells are hexaploid, with six sets of chromosomes, 2"n" = 6"x" = 42 (where the monoploid number "x" = 7 and the haploid number "n" = 21). The gametes are haploid for their own species, but triploid, with three sets of chromosomes, by comparison to a probable evolutionary ancestor, einkorn wheat. 

Tetraploidy (four sets of chromosomes, 2"n" = 4"x") is common in plants, and also occurs in amphibians, reptiles, and insects. For example, species of "Xenopus" (African toads) form a ploidy series, featuring diploid ("X.tropicalis", 2n=20), tetraploid (for example "X.laevis", 4n=36), octaploid (for example "X.wittei", 8n=72) and dodecaploid (for example "X.ruwenzoriensis", 12n=108) species.

Over evolutionary time scales in which chromosomal polymorphisms accumulate, these changes become less apparent by karyotype - for example, humans are generally regarded as diploid, but the 2R hypothesis has confirmed two rounds of whole genome duplication in early vertebrate ancestors.

Ploidy can also differ with life cycle. In some insects it differs by caste. In humans, only the gametes are haploid, but in the Australian bulldog ant, "Myrmecia pilosula", a haplodiploid species, haploid individuals of this species have a single chromosome, and diploid individuals have two chromosomes. In "Entamoeba", the ploidy level varies from 4"n" to 40"n" in a single population. Alternation of generations occurs in many plants.

Some studies suggest that selection is more likely to favor diploidy in host species and haploidy in parasite species.

The nucleus of a eukaryotic cell is haploid if it has a single set of chromosomes, each one not being part of a pair. By extension a cell may be called haploid if its nucleus is haploid, and an organism may be called haploid if its body cells (somatic cells) are haploid. The number of chromosomes in a single set is called the haploid number, given the symbol "n". If the number of chromosomes in the set is 1 (n=1) then the nucleus (or cell, organism) may be called monoploid.

Gametes (sperm and ova) are haploid cells. The haploid gametes produced by most organisms combine to form a zygote with "n" pairs of chromosomes, i.e. 2"n" chromosomes in total. The chromosomes in each pair, one of which comes from the sperm and one from the egg, are said to be homologous. Cells and organisms with pairs of homologous chromosomes are called diploid. For example, most animals are diploid and produce haploid gametes. During meiosis, sex cell precursors have their number of chromosomes halved by randomly "choosing" one member of each pair of chromosomes, resulting in haploid gametes. Because homologous chromosomes usually differ genetically, gametes usually differ genetically from one another.

All plants and many fungi and algae switch between a haploid and a diploid state, with one of the stages emphasized over the other. This is called alternation of generations. Most fungi and algae are haploid during the principal stage of their lifecycle, as are plants like mosses. Most animals are diploid, but male bees, wasps, and ants are haploid organisms because they develop from unfertilized, haploid eggs, while females (workers and queens) are diploid, making their system haplodiploid.

In some cases there is evidence that the "n" chromosomes in a haploid set have resulted from duplications of an originally smaller set of chromosomes. This "base" number – the number of apparently originally unique chromosomes in a haploid set – is called the monoploid number, also known as basic or cardinal number, or fundamental number. As an example, the chromosomes of common wheat are believed to be derived from three different ancestral species, each of which had 7 chromosomes in its haploid gametes. The monoploid number is thus 7 and the haploid number is 3 × 7 = 21. In general "n" is a multiple of "x". The somatic cells in a wheat plant have six sets of 7 chromosomes: three sets from the egg and three sets from the sperm which fused to form the plant, giving a total of 42 chromosomes. As a formula, for wheat 2"n" = 6"x" = 42, so that the haploid number "n" is 21 and the monoploid number "x" is 7. The gametes of common wheat are considered to be haploid, since they contain half the genetic information of somatic cells, but they are not monoploid, as they still contain three complete sets of chromosomes ("n" = 3"x").

In the case of wheat, the origin of its haploid number of 21 chromosomes from three sets of 7 chromosomes can be demonstrated. In many other organisms, although the number of chromosomes may have originated in this way, this is no longer clear, and the monoploid number is regarded as the same as the haploid number. Thus in humans, "x" = "n" = 23.

Diploid cells have two homologous copies of each chromosome, usually one from the mother and one from the father. All or nearly all mammals are diploid organisms. The suspected tetraploid (possessing four chromosome sets) plains viscacha rat ("Tympanoctomys barrerae") and golden vizcacha rat ("Pipanacoctomys aureus") have been regarded as the only known exceptions (as of 2004). However, some genetic studies have rejected any polyploidism in mammals as unlikely, and suggest that amplification and dispersion of repetitive sequences best explain the large genome size of these two rodents. All normal diploid individuals have some small fraction of cells that display polyploidy. Human diploid cells have 46 chromosomes (the somatic number, "2n") and human haploid gametes (egg and sperm) have 23 chromosomes ("n").
Retroviruses that contain two copies of their RNA genome in each viral particle are also said to be diploid. Examples include human foamy virus, human T-lymphotropic virus, and HIV.

"Homoploid" means "at the same ploidy level", i.e. having the same number of homologous chromosomes. For example, homoploid hybridization is hybridization where the offspring have the same ploidy level as the two parental species. This contrasts with a common situation in plants where chromosome doubling accompanies, or happens soon after hybridization. Similarly, homoploid speciation contrasts with polyploid speciation.

Zygoidy is the state where the chromosomes are paired and can undergo meiosis. The zygoid state of a species may be diploid or polyploid. In the azygoid state the chromosomes are unpaired. It may be the natural state of some asexual species or may occur after meiosis. In diploid organisms the azygoid state is monoploid. (see below for dihaploidy)

Polyploidy is the state where all cells have multiple sets of chromosomes beyond the basic set, usually 3 or more. Specific terms are triploid (3 sets), tetraploid (4 sets), pentaploid (5 sets), hexaploid (6 sets), heptaploid or septaploid (7 sets) octoploid (8 sets), nonaploid (9 sets), decaploid (10 sets), undecaploid (11 sets), dodecaploid (12 sets), tridecaploid (13 sets), tetradecaploid (14 sets) etc. Some higher ploidies include hexadecaploid (16 sets), dotriacontaploid (32 sets), and tetrahexacontaploid (64 sets), though Greek terminology may be set aside for readability in cases of higher ploidy (such as "16-ploid"). Polytene chromosomes of plants and fruit flies can be 1024-ploid. Ploidy of systems such as the salivary gland, elaiosome, endosperm, and trophoblast can exceed this, up to 1048576-ploid in the silk glands of the commercial silkworm "Bombyx mori".

The chromosome sets may be from the same species or from closely related species. In the latter case, these are known as allopolyploids (or amphidiploids, which are allopolyploids that behave as if they were normal diploids). Allopolyploids are formed from the hybridization of two separate species. In plants, this probably most often occurs from the pairing of meiotically unreduced gametes, and not by diploid–diploid hybridization followed by chromosome doubling. The so-called "Brassica" triangle is an example of allopolyploidy, where three different parent species have hybridized in all possible pair combinations to produce three new species.

Polyploidy occurs commonly in plants, but rarely in animals. Even in diploid organisms, many somatic cells are polyploid due to a process called endoreduplication where duplication of the genome occurs without mitosis (cell division).
The extreme in polyploidy occurs in the fern genus "Ophioglossum", the adder's-tongues, in which polyploidy results in chromosome counts in the hundreds, or, in at least one case, well over one thousand.

It is possible for polyploid organisms to revert to lower ploidy by haploidisation.

Polyploidy is a characteristic of the bacterium "Deinococcus radiodurans" and of the archaeon "Halobacterium salinarum". These two species are highly resistant to ionizing radiation and desiccation, conditions that induce DNA double-strand breaks. This resistance appears to be due to efficient homologous recombinational repair.

Depending on growth conditions, prokaryotes such as bacteria may have a chromosome copy number of 1 to 4, and that number is commonly fractional, counting portions of the chromosome partly replicated at a given time. This is because under exponential growth conditions the cells are able to replicate their DNA faster than they can divide.

In ciliates, the macronucleus is called ampliploid, because only part of the genome is amplified.

Mixoploidy is the case where two cell lines, one diploid and one polyploid, coexist within the same organism. Though polyploidy in humans is not viable, mixoploidy has been found in live adults and children. There are two types: diploid-triploid mixoploidy, in which some cells have 46 chromosomes and some have 69, and diploid-tetraploid mixoploidy, in which some cells have 46 and some have 92 chromosomes. It is a major topic of cytology.

Dihaploid and polyhaploid cells are formed by haploidisation of polyploids, i.e., by halving the chromosome constitution.

Dihaploids (which are diploid) are important for selective breeding of tetraploid crop plants (notably potatoes), because selection is faster with diploids than with tetraploids. Tetraploids can be reconstituted from the diploids, for example by somatic fusion.

The term "dihaploid" was coined by Bender to combine in one word the number of genome copies (diploid) and their origin (haploid). The term is well established in this original sense, but it has also been used for doubled monoploids or doubled haploids, which are homozygous and used for genetic research.

Euploidy (Greek "eu", true or even) is the state of a cell or organism having one or more than one set of the same set of chromosomes, possibly excluding the sex-determining chromosomes. For example, most human cells have 2 of each of the 23 homologous monoploid chromosomes, for a total of 46 chromosomes. A human cell with an extra set out of the 23 normal ones would be considered euploid. Euploid karyotypes would consequentially be a multiple of the haploid number, which in humans is 23. Aneuploidy is the state where one or more chromosomes of a normal set are missing or present in more than their usual number of copies. Unlike euploidy, aneuploid karyotypes will not be a multiple of the haploid number. In humans, examples of aneuploidy include having a single extra chromosome (such as Down syndrome), or missing a chromosome (such as Turner syndrome). Aneuploid karyotypes are given names with the suffix "-somy" (rather than "-ploidy", used for euploid karyotypes), such as trisomy and monosomy.

A study comparing the karyotypes of endangered or invasive plants with those of their relatives found that being polyploid as opposed to diploid is associated with a 14% lower risk of being endangered, and a 20% greater chance of being invasive. Polyploidy may be associated with increased vigor and adaptability.


Some eukaryotic genome-scale or genome size databases and other sources which may contain the ploidy of many organisms:


</doc>
<doc id="23221" url="https://en.wikipedia.org/wiki?curid=23221" title="Playboy">
Playboy

Playboy is an American men's lifestyle and entertainment magazine. It was founded in Chicago in 1953, by Hugh Hefner and his associates, and funded in part by a $1,000 loan from Hefner's mother. Notable for its centerfolds of nude and semi-nude models (Playmates), "Playboy" played an important role in the sexual revolution and remains one of the world's best-known brands, having grown into Playboy Enterprises, Inc., with a presence in nearly every medium. In addition to the flagship magazine in the United States, special nation-specific versions of "Playboy" are published worldwide.

The magazine has a long history of publishing short stories by notable novelists such as Arthur C. Clarke, Ian Fleming, Vladimir Nabokov, Saul Bellow, Chuck Palahniuk, P. G. Wodehouse, Roald Dahl, Haruki Murakami, and Margaret Atwood. With a regular display of full-page color cartoons, it became a showcase for notable cartoonists, including Harvey Kurtzman, Jack Cole, Eldon Dedini, Jules Feiffer, Shel Silverstein, Erich Sokol, Roy Raymonde, Gahan Wilson, and Rowland B. Wilson. "Playboy" features monthly interviews of notable public figures, such as artists, architects, economists, composers, conductors, film directors, journalists, novelists, playwrights, religious figures, politicians, athletes, and race car drivers. The magazine generally reflects a liberal editorial stance, although it often interviews conservative celebrities.
After a year-long removal of most nude photos in "Playboy" magazine, the March–April 2017 issue brought back nudity.

By spring 1953, Hugh Hefner—a 1949 University of Illinois psychology graduate who had worked in Chicago for "Esquire" magazine writing promotional copy; Publisher's Development Corporation in sales and marketing; and "Children's Activities" magazine as circulation promotions manager—had planned out the elements of his own magazine, that he would call "Stag Party". He formed HMH Publishing Corporation, and recruited his friend Eldon Sellers to find investors. Hefner eventually raised just over $8,000, including from his brother and mother. However, the publisher of an unrelated men's adventure magazine, "Stag," contacted Hefner and informed him it would file suit to protect their trademark if he were to launch his magazine with that name. Hefner, his wife Millie, and Sellers met to seek a new name, considering "Top Hat", "Gentleman", "Sir'", "Satyr", "Pan" and "Bachelor" before Sellers suggested "Playboy".

The first issue, in December 1953, was undated, as Hefner was unsure there would be a second. He produced it in his Hyde Park kitchen. The first centerfold was Marilyn Monroe, although the picture used originally was taken for a calendar rather than for "Playboy". Hefner chose what he deemed the "sexiest" image, a previously unused nude study of Marilyn stretched with an upraised arm on a red velvet background with closed eyes and mouth open. The heavy promotion centered around Marilyn's nudity on the already-famous calendar, together with the teasers in marketing, made the new Playboy magazine a success.
The first issue sold out in weeks. Known circulation was 53,991. The cover price was 50¢. Copies of the first issue in mint to near mint condition sold for over $5,000 in 2002.

The novel "Fahrenheit 451", by Ray Bradbury, was published in 1953 and serialized in the March, April and May 1954 issues of "Playboy".

An urban legend started about Hefner and the Playmate of the Month because of markings on the front covers of the magazine. From 1955 to 1979 (except for a six-month gap in 1976), the "P" in "Playboy" had stars printed in or around the letter. The legend stated that this was either a rating that Hefner gave to the Playmate according to how attractive she was, the number of times that Hefner had slept with her, or how good she was in bed. The stars, between zero and 12, actually indicated the domestic or international advertising region for that printing.

From 1966 to 1976, Robie Macauley was the Fiction Editor at "Playboy". During this period the magazine published fiction by Saul Bellow, Seán Ó Faoláin, John Updike, James Dickey, John Cheever, Doris Lessing, Joyce Carol Oates, Vladimir Nabokov, Michael Crichton, John le Carré, Irwin Shaw, Jean Shepherd, Arthur Koestler, Isaac Bashevis Singer, Bernard Malamud, John Irving, Anne Sexton, Nadine Gordimer, Kurt Vonnegut and J. P. Donleavy, as well as poetry by Yevgeny Yevtushenko.

In 1968 at the feminist Miss America protest, protestors symbolically threw a number of feminine products into a "Freedom Trash Can." These included copies of "Playboy" and "Cosmopolitan" magazines. One of the key pamphlets produced by the protesters was "No More Miss America!", by Robin Morgan which listed ten characteristics of the Miss America pageant that the authors believed degraded women; it compared the pageant to "Playboy"'s centerfold as sisters under the skin, describing this as "The Unbeatable Madonna-Whore Combination."

Macauley contributed all of the popular "Ribald Classics" series published between January 1978 and March 1984.

Since reaching its peak in the 1970s, "Playboy" saw a decline in circulation and cultural relevance due to competition in the field it founded—first from "Penthouse," then "Oui" (which was published as a spin-off of "Playboy") and "Gallery" in the 1970s; later from pornographic videos; and more recently from lad mags such as "Maxim," "FHM," and "Stuff." In response, "Playboy" has attempted to re-assert its hold on the 18–35 male demographic through slight changes to content and focusing on issues and personalities more appropriate to its audience—such as hip-hop artists being featured in the ""Playboy" Interview".

Christie Hefner, daughter of the founder Hugh Hefner, joined Playboy in 1975 and became head of the company in 1988. She announced in December 2008 that she would be stepping down from leading the company, effective in January 2009, and said that the election of Barack Obama as the next President had inspired her to give more time to charitable work, and that the decision to step down was her own. "Just as this country is embracing change in the form of new leadership, I have decided that now is the time to make changes in my own life as well," she said.

The magazine celebrated its 50th anniversary with the January 2004 issue. Celebrations were held at Las Vegas, Los Angeles, New York, and Moscow during the year to commemorate this event. Playboy also launched limited-edition products designed by a number of notable fashion-houses such as Versace, Vivienne Westwood and Sean Jean. As a hommage to the magazine 50th anniversary, MAC Cosmetics released two limited-edition products, namely a lipstick and a glitter cream.

The magazine runs several annual features and ratings. One of the most popular is its annual ranking of the top "party schools" among all U.S. universities and colleges. For 2009, the magazine used five considerations: bikini, brains, campus, sex and sports in the development of its list. The top ranked party school by "Playboy" for 2009 was the University of Miami.

In June 2009, the magazine reduced its publication schedule to 11 issues per year, with a combined July/August issue and on August 11, 2009. London's "Daily Telegraph" newspaper reported that Hugh Hefner had sold his English Manor house (next door to the famous Playboy Mansion) for $18 m ($10 m less than the reported asking price) to another American Daren Metropoulos the President and co-owner of Pabst Blue Ribbon. Also that due to significant losses in the company's value (down from $1 billion in 2000 to $84 million in 2009) the Playboy publishing empire is up for sale for $300 million. In December 2009, they further reduced the publication schedule to 10 issues per year, with a combined January/February issue.

On July 12, 2010, Playboy Enterprises Inc. announced Hefner's $5.50 per share offer ($122.5 million based on shares outstanding on April 30 and the closing price on July 9) to buy the portion of the company he did not already own and take the company private with the help of Rizvi Traverse Management LLC. The company derives much of its income from licensing rather than the magazine. On July 15, "Penthouse" owner FriendFinder Networks Inc. offered $210 million (the company is valued at $185 million), though Hefner, who already owned 70 percent of voting stock, did not want to sell. In January 2011, the publisher of "Playboy" magazine agreed to an offer by Hefner to take the company private for $6.15 per share, an 18 percent premium over the price of the last previous day of trading. The buyout was completed in March 2011.

In October 2015, Playboy announced that starting with their March 2016 issue, the magazine would no longer feature full frontal nudity. "Playboy" CEO Scott Flanders acknowledged the magazine's inability to compete with freely available Internet pornography and nudity; according to him, "You're now one click away from every sex act imaginable for free. And so it's just passé at this juncture". Hefner agreed with the decision. The redesigned "Playboy", however, would still feature a Playmate of the Month and pictures of women, but they would be rated as not appropriate for children under 13. The move would not affect PlayboyPlus.com (which features nudity at a paid subscription). Josh Horwitz of Quartz argued that the motivation for the decision to remove nudity from the magazine was to give Playboy Licensing a less inappropriate image in India and China, where the brand is a popular item on apparel and thus generates significant revenue.

Among other changes to the magazine included ending the popular jokes section and the various cartoons that appeared throughout the magazine. The redesign eliminated the use of jump copy (articles continuing on non-consecutive pages), which in turn eliminated most of the space for cartoons. Hefner, himself a former cartoonist, reportedly resisted dropping the cartoons more than the nudity, but ultimately obliged. "Playboy"s plans were to market itself as a competitor to "Vanity Fair" as opposed to more traditional competitors "GQ" and "Maxim".

Playboy announced in February 2017, however, that the dropping of nudity had been a mistake, and furthermore for its March/April issue reestablished some of its franchises, including The Playboy Philosophy and Party Jokes, but dropped the subtitle "Entertainment for Men" inasmuch as gender roles have evolved. The announcement was made by the company's chief creative officer on Twitter with the hashtag #NakedIsNormal.

In early 2018, and according to Jim Puzzanghera of the "Los Angeles Times", "Playboy" was reportedly "considering killing the print magazine," as the publication "has lost as much as $7 million annually in recent years." However, in the July/August 2018 issue a reader asked if the print magazine would discontinue, and Playboy responded that it was not going anywhere.

The best-selling "Playboy" edition was the November 1972 edition, which sold 7,161,561 copies. One-quarter of all American college men were buying or subscribing to the magazine every month. On the cover was model Pam Rawlings, photographed by Rowland Scherman.

Perhaps coincidentally, a cropped image of the issue's centerfold (which featured Lena Söderberg) became a de facto standard image for testing image processing algorithms. It is known simply as the "Lenna" (also "Lena") image in that field.

In 1970, "Playboy" became the first gentleman's magazine to be printed in braille. It is also one of the few magazines whose microfilm format was in color, not black and white.

Playboy's iconic and enduring mascot, a stylized silhouette of a rabbit wearing a tuxedo bow tie, was created by "Playboy" art director Art Paul for the second issue as an endnote, but was adopted as the official logo and has appeared ever since. A running joke in the magazine involves hiding the logo somewhere in the cover art or photograph. Hefner said he chose the rabbit for its "humorous sexual connotation", and because the image was "frisky and playful".

In an interview Hefner explained his choice of a rabbit as "Playboy's" logo to the Italian journalist Oriana Fallaci:

The jaunty rabbit was quickly a popular symbol of extroverted male culture, becoming a lucrative source of merchandizing revenue for Playboy. In the 1950s, it was adopted as military aircraft insignia for the Navy's VX-4 fighter-evaluation squadron.

Besides its centerfold, a major part of "Playboy" for much of its existence has been the "Playboy" Interview, an extensive (usually several thousand-word) discussion between a notable individual and an interviewer (historian Alex Haley, for example, served as a "Playboy" interviewer on a few occasions; one of his interviews was with Martin Luther King Jr.; he also interviewed Malcolm X and American Nazi Party founder George Lincoln Rockwell in the April 1966 issue, then coauthored Malcolm X's autobiography). One of the magazine's most notable interviews was a discussion with then-presidential candidate Jimmy Carter in the November 1976 issue, in which he stated "I've committed adultery in my heart many times." David Sheff's interview with John Lennon and Yoko Ono appeared in the January 1981 issue, which was on newsstands at the time of Lennon's murder; the interview was later published in book format.

Another interview type section, entitled "20Q" (a play on the game of Twenty Questions), was added in October 1978. Cheryl Tiegs was the first interviewee for the section.

"Rock the Rabbit" was an annual music news and pictorial feature published in the March edition. The pictorial featured images of rock bands photographed by music photographer Mick Rock. Fashion designers participated in the Rock the Rabbit event by designing T-shirts inspired by Playboy's rabbit head logo for each band. The shirts were sold at Playboy's retailers and auctioned off to raise money for AIDS at LIFEbeat: The Music Industry Fights AIDS. Notable bands who were featured include: MGMT, Daft Punk, Iggy Pop, Duran Duran, Flaming Lips, Snow Patrol, and The Killers.

Many notable photographers have contributed to "Playboy", including Ken Marcus, Richard Fegley, Arny Freytag, Ron Harris, Tom Kelley, David Mecey, Russ Meyer, Pompeo Posar, Suze Randall, Herb Ritts, Stephen Wayda, Sam Wu, Mario Casilli, Annie Leibovitz, Helmut Newton, and Bunny Yeager.

Many celebrities (singers, actresses, models, etc.) have posed for "Playboy" over the years. This list is only a small portion of those who have posed. Some of them are:

The success of "Playboy" magazine has led PEI to market other versions of the magazine, the Special Editions (formerly called "Newsstand Specials"), such as "Playboy's College Girls" and "Playboy's Book of Lingerie", as well as the "Playboy" video collection.

The National Library Service for the Blind and Physically Handicapped (NLS) has published a Braille edition of "Playboy" since 1970. The Braille version includes all the written words in the non-Braille magazine, but no pictorial representations. Congress cut off funding for the Braille magazine translation in 1985, but U.S. District Court Judge Thomas Hogan reversed the decision on First Amendment grounds.

The growth of the Internet prompted the magazine to develop an official web presence called "Playboy" Online or Playboy.com, which is the official website for Playboy Enterprises, and an online companion to "Playboy" magazine. The site has been available online since 1994. As part of the online presence, Playboy developed a pay web site called the "Playboy Cyber Club" in 1995 which features online chats, additional pictorials, videos of Playmates and Playboy Cyber Girls that are not featured in the magazine. Archives of past "Playboy" articles and interviews are also included. In September 2005, Playboy launched the online edition of the magazine "Playboy Digital".

In 2010, Playboy introduced "The Smoking Jacket", a safe-for-work website designed to appeal to young men, while avoiding nude images or key words that would cause the site to be filtered or otherwise prohibited in the workplace.

In May 2011, Playboy introduced i.playboy.com, a complete, uncensored version of its near 700 issue archive, targeting the Apple iPad. By launching the archive as a web app, Playboy was able to circumvent both Apple's App Store content restrictions and their 30% subscription fee.

On January 14, 2004, the Ninth Circuit U.S. Court of Appeals ruled that Playboy Enterprises Inc.'s (PEI) trademark terms "Playboy" and "Playmate" should be protected in the situation where a user typing "Playboy" or "Playmate" in a browser search was instead shown advertisements of companies that competed with PEI. (The decision reversed an earlier district court ruling.) The suit started on April 15, 1999, when Playboy sued Excite Inc. and Netscape for trademark infringement.

Many in the American religious community opposed the publication of "Playboy". The Louisiana pastor and author L. L. Clover wrote in his 1974 treatise "Evil Spirits Intellectualism and Logic" that "Playboy" encouraged young men to view themselves as "pleasure-seeking individuals for whom sex is fun and women are play things."

In many parts of Asia, including India, mainland China, Myanmar, Malaysia, Thailand, Singapore, and Brunei, sale and distribution of "Playboy" is banned. In addition, sale and distribution is banned in most Muslim countries (except Lebanon and Turkey) in Asia and Africa, including Iran, Saudi Arabia, and Pakistan. Despite the ban on the magazine in these countries, the official "Playboy" brand itself can still appear on various merchandise such as perfume and deodorants.

While banned in mainland China, the magazine is sold in Hong Kong. In Japan, where genitals of models cannot be shown, a separate edition was published under license by Shueisha. An Indonesian edition was launched in April 2006, but controversy started before the first issue hit the stands. Though the publisher said the content of the Indonesian edition will be different from the original edition, the government tried to ban it by using anti-pornography rules. A Muslim organization, the Islamic Defenders Front (IDF), opposed "Playboy" on the grounds of pornography. On April 12, about 150 IDF members clashed with police and stoned the editorial offices. Despite this, the edition quickly sold out. On April 6, 2007, the chief judge of the case dismissed the charges because they had been incorrectly filed.

In 1986, the American convenience store chain 7-Eleven removed the magazine. The store returned "Playboy" to its shelves in late 2003. 7-Eleven had also been selling "Penthouse" and other similar magazines before the ban.

In 1995, "Playboy" was returned to shelves in the Republic of Ireland after a 36-year ban, despite staunch opposition from many women's groups.

"Playboy" was not sold in the state of Queensland, Australia during 2004 and 2005 but returned as of 2006. Due to declining sales, the last Australia-wide edition of "Playboy" was the January 2000 issue.

In 2013, "Playboy" was cleared by the Pentagon of violating its rule against selling sexually explicit material on military property, but the base exchanges stopped selling it anyway.

In March 2018, Playboy announced they would be deactivating their Facebook accounts due to the "sexually repressive" nature of the social media platform and their mismanagement of user data resulting from the Cambridge Analytica problem.

General compilations

Anniversary collections

Interview compilations


Official

Others


</doc>
<doc id="23222" url="https://en.wikipedia.org/wiki?curid=23222" title="Pennsylvanian (geology)">
Pennsylvanian (geology)

The Pennsylvanian (also known as Upper Carboniferous or Late Carboniferous) is, in the ICS geologic timescale, the younger of two subperiods (or upper of two subsystems) of the Carboniferous Period. It lasted from roughly Ma (million years ago). As with most other geochronologic units, the rock beds that define the Pennsylvanian are well identified, but the exact date of the start and end are uncertain by a few hundred thousand years. The Pennsylvanian is named after the U.S. state of Pennsylvania, where the coal-productive beds of this age are widespread.

The division between Pennsylvanian and Mississippian comes from North American stratigraphy. In North America, where the early Carboniferous beds are primarily marine limestones, the Pennsylvanian was in the past treated as a full-fledged geologic period between the Mississippian and the Permian. In Europe, the Mississippian and Pennsylvanian are one more-or-less continuous sequence of lowland continental deposits and are grouped together as the Carboniferous Period. The current internationally used geologic timescale of the ICS gives the Mississippian and Pennsylvanian the rank of subperiods, subdivisions of the Carboniferous Period.

All modern classes of fungi have been found in rocks of Pennsylvanian age.

Amphibians were diverse and common; some were several meters long as adults. The collapse of the rainforest ecology in the mid-Pennsylvanian (between the Moscovian and the Kasimovian) removed many amphibian species that did not survive as well in the cooler, drier conditions. Reptiles, however, prospered due to specific key adaptations. One of the greatest evolutionary innovations of the Carboniferous was the amniote egg, which allowed for the further exploitation of the land by certain tetrapods. These included the earliest sauropsid reptiles ("Hylonomus"), and the earliest known synapsid ("Archaeothyris"). Small lizard-like animals quickly gave rise to many descendants. Reptiles underwent a major evolutionary radiation, in response to the drier climate that followed the rainforest collapse.

The major forms of life at this time were the arthropods. Due to the high levels of oxygen, arthropods were far larger than modern ones. "Arthropleura", a giant millipede relative, was a common sight and the giant dragonfly "Meganeura" "flew the skies".

The Pennsylvanian has been variously subdivided. The international timescale of the ICS follows the Russian subdivision into four stages:


North American subdivision is into five stages, but not precisely the same, with additional (older) Appalachian series names following:

The Virgilian or Conemaugh corresponds to the Gzhelian plus the uppermost Kasimovian.
The Missourian or Monongahela corresponds to the rest of the Kasimovian.
The Desmoinesian or Allegheny corresponds to the upper half of the Moscovian.
The Atokan or upper Pottsville corresponds to the lower half of the Moscovian.
The Morrowan corresponds to the Bashkirian.

In the European subdivision, the Carboniferous is divided into two epochs: Dinantian (early) and Silesian (late). The Silesian starts earlier than the Pennsylvanian and is divided in three ages: 



</doc>
<doc id="23224" url="https://en.wikipedia.org/wiki?curid=23224" title="Paul Héroult">
Paul Héroult

Paul (Louis-Toussaint) Héroult (10 April 1863 – 9 May 1914) was a French scientist. He was the inventor of the aluminium electrolysis and developed the first successful commercial electric arc furnace. He lived in Thury-Harcourt, Normandy.

Paul Héroult read Henri Sainte-Claire Deville's treatise on aluminium, when he was 15 years old. At that time, aluminium was as expensive as silver and was used mostly for luxury items and jewellery. Héroult wanted to make it cheaper.
He succeeded in doing so when he discovered the electrolytic aluminium process in 1886.
The same year, in the United States, Charles Martin Hall (1863–1914) was
discovering the same process. Because of this, the process was called the Hall-Heroult process.

Héroult's second most important contribution is the first commercially successful electric arc furnace (EAF) for steel in 1900. The Héroult furnace gradually replaced the giant smelters for the production of a variety of steels. In 1905, Paul Héroult was invited to the United States as a technical adviser to several companies, and in particular to the United States Steel Corporation and the Halcomb Steel Company. Halcomb installed the first Héroult furnace in the US.

The invention of the electric arc furnace probably began when Humphry Davy discovered the carbon arc in 1800. Then in 1878 Carl Wilhelm Siemens patented, constructed and operated both direct and indirect EAFs. Commercial use still needed to wait for larger supplies of electricity and better carbon electrodes.

Paul Héroult is renowned for other major inventions, among them a
self-supporting conduit still used to bring water down from mountain heights and across rivers to hydraulic power plants, avoiding the need to build expensive bridges.

Christian Bickert said of him 
Héroult's death on 9 May 1914 followed his reaching the age of 51 by twenty-nine days.




</doc>
<doc id="23226" url="https://en.wikipedia.org/wiki?curid=23226" title="Permian">
Permian

The Permian is a geologic period and system which spans 47 million years from the end of the Carboniferous Period million years ago (Mya), to the beginning of the Triassic period 251.902 Mya. It is the last period of the Paleozoic era; the following Triassic period belongs to the Mesozoic era. The concept of the Permian was introduced in 1841 by geologist Sir Roderick Murchison, who named it after the city of Perm.

The Permian witnessed the diversification of the early amniotes into the ancestral groups of the mammals, turtles, lepidosaurs, and archosaurs. The world at the time was dominated by two continents known as Pangaea and Siberia, surrounded by a global ocean called Panthalassa. The Carboniferous rainforest collapse left behind vast regions of desert within the continental interior. Amniotes, who could better cope with these drier conditions, rose to dominance in place of their amphibian ancestors.

The Permian (along with the Paleozoic) ended with the Permian–Triassic extinction event, the largest mass extinction in Earth's history, in which nearly 90% of marine species and 70% of terrestrial species died out. It would take well into the Triassic for life to recover from this catastrophe. Recovery from the Permian–Triassic extinction event was protracted; on land, ecosystems took 30 million years to recover.

The term "Permian" was introduced into geology in 1841 by Sir R. I. Murchison, president of the Geological Society of London, who identified typical strata in extensive Russian explorations undertaken with Édouard de Verneuil. The region now lies in the Perm Krai of Russia.

Official ICS 2017 subdivisions of the Permian System from most recent to most ancient rock layers are:




Sea levels in the Permian remained generally low, and near-shore environments were reduced as almost all major landmasses collected into a single continent – Pangaea. This could have in part caused the widespread extinctions of marine species at the end of the period by severely reducing shallow coastal areas preferred by many marine organisms.

During the Permian, all the Earth's major landmasses were collected into a single supercontinent known as Pangaea. Pangaea straddled the equator and extended toward the poles, with a corresponding effect on ocean currents in the single great ocean ("Panthalassa", the "universal sea"), and the Paleo-Tethys Ocean, a large ocean that was between Asia and Gondwana. The Cimmeria continent rifted away from Gondwana and drifted north to Laurasia, causing the Paleo-Tethys Ocean to shrink. A new ocean was growing on its southern end, the Tethys Ocean, an ocean that would dominate much of the Mesozoic era. Large continental landmass interiors experience climates with extreme variations of heat and cold ("continental climate") and monsoon conditions with highly seasonal rainfall patterns. Deserts seem to have been widespread on Pangaea. Such dry conditions favored gymnosperms, plants with seeds enclosed in a protective cover, over plants such as ferns that disperse spores in a wetter environment. The first modern trees (conifers, ginkgos and cycads) appeared in the Permian.

Three general areas are especially noted for their extensive Permian deposits – the Ural Mountains (where Perm itself is located), China, and the southwest of North America, including the Texas red beds. The Permian Basin in the U.S. states of Texas and New Mexico is so named because it has one of the thickest deposits of Permian rocks in the world.

The climate in the Permian was quite varied. At the start of the Permian, the Earth was still in an ice age, which began in the Carboniferous. Glaciers receded around the mid-Permian period as the climate gradually warmed, drying the continent's interiors. In the late Permian period, the drying continued although the temperature cycled between warm and cool cycles.

Permian marine deposits are rich in fossil mollusks, echinoderms, and brachiopods. Fossilized shells of two kinds of invertebrates are widely used to identify Permian strata and correlate them between sites: fusulinids, a kind of shelled amoeba-like protist that is one of the foraminiferans, and ammonoids, shelled cephalopods that are distant relatives of the modern nautilus. By the close of the Permian, trilobites and a host of other marine groups became extinct.

Terrestrial life in the Permian included diverse plants, fungi, arthropods, and various types of tetrapods. The period saw a massive desert covering the interior of Pangaea. The warm zone spread in the northern hemisphere, where extensive dry desert appeared. The rocks formed at that time were stained red by iron oxides, the result of intense heating by the sun of a surface devoid of vegetation cover. A number of older types of plants and animals died out or became marginal elements.

The Permian began with the Carboniferous flora still flourishing. About the middle of the Permian a major transition in vegetation began. The swamp-loving lycopod trees of the Carboniferous, such as "Lepidodendron" and "Sigillaria", were progressively replaced in the continental interior by the more advanced seed ferns and early conifers. At the close of the Permian, lycopod and equisete swamps reminiscent of Carboniferous flora survived only on a series of equatorial islands in the Paleo-Tethys Ocean that later would become South China.

The Permian saw the radiation of many important conifer groups, including the ancestors of many present-day families. Rich forests were present in many areas, with a diverse mix of plant groups. The southern continent saw extensive seed fern forests of the "Glossopteris " flora. Oxygen levels were probably high there. The ginkgos and cycads also appeared during this period.

From the Pennsylvanian subperiod of the Carboniferous period until well into the Permian, the most successful insects were primitive relatives of cockroaches. Six fast legs, four well-developed folding wings, fairly good eyes, long, well-developed antennae (olfactory), an omnivorous digestive system, a receptacle for storing sperm, a chitin-based exoskeleton that could support and protect, as well as a form of gizzard and efficient mouth parts, gave it formidable advantages over other herbivorous animals. About 90% of insects at the start of the Permian were cockroach-like insects ("Blattopterans").

Primitive forms of dragonflies (Odonata) were the dominant aerial predators and probably dominated terrestrial insect predation as well. True Odonata appeared in the Permian, and all are effectively semi-aquatic insects (aquatic immature stages, and terrestrial adults), as are all modern odonates. Their prototypes are the oldest winged fossils, dating back to the Devonian, and are different in several respects from the wings of other insects. Fossils suggest they may have possessed many modern attributes even by the late Carboniferous, and it is possible that they captured small vertebrates, for at least one species had a wing span of . Several other insect groups appeared or flourished during the Permian, including the Coleoptera (beetles) and Hemiptera (true bugs).

Early Permian terrestrial faunas were dominated by pelycosaurs, diadectids and amphibians, the middle Permian by primitive therapsids such as the dinocephalia, and the late Permian by more advanced therapsids such as gorgonopsians and dicynodonts. Towards the very end of the Permian the first archosaurs appeared, a group that would give rise to the crurotarsans and the dinosaurs in the following period. Also appearing at the end of the Permian were the first cynodonts, which would go on to evolve into mammals during the Triassic. Another group of therapsids, the therocephalians (such as "Lycosuchus"), arose in the Middle Permian. There were no aerial vertebrates (with the exception of gliding reptiles, the avicephalans).

The Permian period saw the development of a fully terrestrial fauna and the appearance of the first large herbivores and carnivores. It was the high tide of the anapsids in the form of the massive Pareiasaurs and host of smaller, generally lizard-like groups. A group of small reptiles, the diapsids, started to abound. These were the ancestors to most modern reptiles and the ruling dinosaurs as well as pterosaurs and crocodiles.

The synapsid, early ancestors to mammals, also thrived at this time. Synapsids included some large members such as "Dimetrodon". The special adaptations of reptiles enabled them to flourish in the drier climate of the Permian and they grew to dominate the vertebrates.

Permian amphibians consisted of temnospondyli, lepospondyli and batrachosaurs.

The Permian ended with the most extensive extinction event recorded in paleontology: the Permian–Triassic extinction event. 90% to 95% of marine species became extinct, as well as 70% of all land organisms. It is also the only known mass extinction of insects. Recovery from the Permian-Triassic extinction event was protracted; on land, ecosystems took 30 million years to recover. Trilobites, which had thrived since Cambrian times, finally became extinct before the end of the Permian. Nautiluses, a species of cephalopods, surprisingly survived this occurrence.

There is evidence that magma, in the form of flood basalt, poured onto the surface in what is now called the Siberian Traps, for thousands of years, contributing to the environmental stress that led to mass extinction. The reduced coastal habitat and highly increased aridity probably also contributed. Based on the amount of lava estimated to have been produced during this period, the worst-case scenario is the release of enough carbon dioxide from the eruptions to raise world temperatures five degrees Celsius.

Another hypothesis involves ocean venting of hydrogen sulfide gas. Portions of the deep ocean will periodically lose all of its dissolved oxygen allowing bacteria that live without oxygen to flourish and produce hydrogen sulfide gas. If enough hydrogen sulfide accumulates in an anoxic zone, the gas can rise into the atmosphere. Oxidizing gases in the atmosphere would destroy the toxic gas, but the hydrogen sulfide would soon consume all of the atmospheric gas available. Hydrogen sulfide levels might have increased dramatically over a few hundred years. Models of such an event indicate that the gas would destroy ozone in the upper atmosphere allowing ultraviolet radiation to kill off species that had survived the toxic gas. There are species that can metabolize hydrogen sulfide.

Another hypothesis builds on the flood basalt eruption theory. An increase in temperature of five degrees Celsius would not be enough to explain the death of 95% of life. But such warming could slowly raise ocean temperatures until frozen methane reservoirs below the ocean floor near coastlines melted, expelling enough methane (among the most potent greenhouse gases) into the atmosphere to raise world temperatures an additional five degrees Celsius. The frozen methane hypothesis helps explain the increase in carbon-12 levels found midway in the Permian–Triassic boundary layer. It also helps explain why the first phase of the layer's extinctions was land-based, the second was marine-based (and starting right after the increase in C-12 levels), and the third land-based again.

An even more speculative hypothesis is that intense radiation from a nearby supernova was responsible for the extinctions.

It has been hypothesised that huge meteorite impact crater (Wilkes Land crater) with a diameter of around 500 kilometers in Antarctica represents an impact event that may be related to the extinction. The crater is located at a depth of 1.6 kilometers beneath the ice of Wilkes Land in eastern Antarctica. The scientists speculate that this impact may have caused the Permian–Triassic extinction event, although its age is bracketed only between 100 million and 500 million years ago. They also speculate that it may have contributed in some way to the separation of Australia from the Antarctic landmass, which were both part of a supercontinent called Gondwana. Levels of iridium and quartz fracturing in the Permian-Triassic layer do not approach those of the Cretaceous–Paleogene boundary layer. Given that a far greater proportion of species and individual organisms became extinct during the former, doubt is cast on the significance of a meteorite impact in creating the latter. Further doubt has been cast on this theory based on fossils in Greenland that show the extinction to have been gradual, lasting about eighty thousand years, with three distinct phases.

Many scientists argue that the Permian–Triassic extinction event was caused by a combination of some or all of the hypotheses above and other factors; the formation of Pangaea decreased the number of coastal habitats and may have contributed to the extinction of many clades.




</doc>
<doc id="23227" url="https://en.wikipedia.org/wiki?curid=23227" title="Pisces (constellation)">
Pisces (constellation)

Pisces is a constellation of the zodiac. Its name is the Latin plural for fish. It lies between Aquarius to the west and Aries to the east. The ecliptic and the celestial equator intersect within this constellation and in Virgo. Its symbol is (Unicode ♓).

The vernal equinox is currently located in Pisces, due south of ω Psc, and, due to precession, slowly drifting below the western fish towards Aquarius.


M74 is a loosely wound (type Sc) spiral galaxy in Pisces, found at a distance of 30 million light years (redshift 0.0022). It has many clusters of young stars and the associated nebulae, showing extensive regions of star formation. It was discovered by Pierre Méchain, a French astronomer, in 1780. A type II-P supernova was discovered in the outer regions of M74 by Robert Evans in June 2003; the star that underwent the supernova was later identified as a red supergiant with a mass of 8 solar masses.

NGC 488 is an isolated face-on prototypical spiral galaxy.

NGC 520 is a pair of colliding galaxies located 90 million lightyears away.

CL 0024+1654 is a massive galaxy cluster that lenses the galaxy behind it, creating arc-shaped images of the background galaxy. The cluster is primarily made up of yellow elliptical and spiral galaxies, at a distance of 3.6 billion light-years from Earth (redshift 0.4), half as far away as the background galaxy, which is at a distance of 5.7 billion light-years (redshift 1.67).

3C 31 is an active galaxy and radio source in Perseus located at a distance of 237 million light-years from Earth (redshift 0.0173). Its jets, caused by the supermassive black hole at its center, extend several million light-years in both directions, making them some of the largest objects in the universe.

Pisces originates from some composition of the Babylonian constellations "Šinunutu" "the great swallow" in current western Pisces, and "Anunitum" the "Lady of the Heaven", at the place of the northern fish. In the first-millennium BC texts known as the "Astronomical Diaries", part of the constellation was also called DU.NU.NU ("Rikis-nu.mi", "the fish cord or ribbon").

Pisces is associated with Aphrodite and Eros, who escaped from the monster Typhon by leaping into the sea and transforming themselves into fish. In order not to lose each other, they tied themselves together with rope. The Romans adopted the Greek legend, with Venus and Cupid acting as the counterparts for Aphrodite and Eros. The knot of the rope is marked by Alpha Piscium (α Psc), also called Al-Rischa ("the cord" in Arabic).
In 1690, the astronomer Johannes Hevelius in his "Firmamentum Sobiescianum" regarded the constellation Pisces as being composed of four subdivisions:

In 1754, the astronomer John Hill proposed to treat part of Pisces as a separate constellation, called Testudo (the Turtle) 24 – 27 – YY(30) – 33 – 29 Psc., centred a natural but faint asterism in which the star 20 Psc is intended to be the head of the turtle. However the proposal was largely neglected by other astronomers with the exception of Admiral Smyth, who mentioned it in his book "The Bedford Catalogue", and it is now obsolete.

The Fishes are also associated with the German legend of Antenteh, who owned just a tub and a crude cabin when he met a magical fish. They offered him a wish, which he refused. However, his wife begged him to return to the fish and ask for a beautiful furnished home. This wish was granted, but her desires were not satisfied. She then asked to be a queen and have a palace, but when she asked to become a goddess, the fish became angry and took the palace and home, leaving the couple with the tub and cabin once again. The tub in the story is sometimes recognized as the Great Square of Pegasus.

The stars of Pisces were incorporated into several constellations in Chinese astronomy. Wai-ping ("Outer Enclosure") was a fence that kept a pig farmer from falling into the marshes and kept the pigs where they belonged. It was represented by Alpha, Delta, Epsilon, Zeta, Mu, Nu, and Xi Piscium. The marshes were represented by the four stars designated Phi Ceti. The northern fish of Pisces was a part of the House of the Sandal, Koui-siou.

Pisces is a dim constellation located next to Aquarius, and Aries. 
While the "astrological sign" Pisces per definition runs from ecliptical longitude 330° to 0, this position is now mostly covered by the constellation of Aquarius, due to the precession from when the constellation and the sign coincided.




</doc>
<doc id="23229" url="https://en.wikipedia.org/wiki?curid=23229" title="Paul Robeson">
Paul Robeson

Paul Leroy Robeson ( ; April 9, 1898 – January 23, 1976) was an American bass baritone concert artist and stage and film actor who became famous both for his cultural accomplishments and for his political activism. Educated at Rutgers College and Columbia University, he was also a star athlete in his youth. His political activities began with his involvement with unemployed workers and anti-imperialist students whom he met in Britain and continued with support for the Loyalist cause in the Spanish Civil War and his opposition to fascism. In the United States he also became active in the Civil Rights Movement and other social justice campaigns. His sympathies for the Soviet Union and for communism, and his criticism of the United States government and its foreign policies, caused him to be blacklisted during the McCarthy era.

In 1915, Robeson won an academic scholarship to Rutgers College, where he was twice named a consensus All-American and was the class valedictorian. Almost 80 years later, he was inducted into the College Football Hall of Fame. He received his LL.B. from Columbia Law School, while playing in the National Football League (NFL). At Columbia, he sang and acted in off-campus productions. After graduating, he became a figure in the Harlem Renaissance with performances in "The Emperor Jones" and "All God's Chillun Got Wings".

Between 1925 and 1961, Robeson recorded and released some 276 distinct songs, many of which were recorded several times. The first of these were the spirituals "Steal Away" backed with "Were You There" in 1925. Robeson's recorded repertoire spanned many styles, including Americana, popular standards, classical music, European folk songs, political songs, poetry and spoken excerpts from plays.

Robeson performed in Britain in a touring melodrama, "Voodoo", in 1922, and in "Emperor Jones" in 1925, and scored a major success in the London premiere of "Show Boat" in 1928, settling in London for several years with his wife Eslanda. While continuing to establish himself as a concert artist, Robeson also starred in a London production of "Othello", the first of three productions of the play over the course of his career. He also gained attention in the film production of "Show Boat" and other films such as "Sanders of the River" and "The Proud Valley". During this period, Robeson became increasingly attuned to the sufferings of people of other cultures, notably the British working class and the colonized peoples of the British Empire. He advocated for Republican forces during the Spanish Civil War and became active in the Council on African Affairs (CAA).

Returning to the United States in 1939, during World War II Robeson supported the American and Allied war efforts. However, his history of supporting civil rights causes and pro-Soviet policies brought scrutiny from the FBI. After the war ended, the CAA was placed on the Attorney General's List of Subversive Organizations and Robeson was investigated during the age of McCarthyism. Due to his decision not to recant his public advocacy, he was denied a passport by the U.S. State Department, and his income, consequently, plummeted. He moved to Harlem and published a periodical critical of United States policies. His right to travel was eventually restored as a result of the 1958 United States Supreme Court decision, "Kent v. Dulles". In the early 1960s he retired and lived the remaining years of his life privately in Philadelphia.

Paul Leroy Robeson was born in Princeton, New Jersey, in 1898, to Reverend William Drew Robeson and Maria Louisa Bustill. His mother was from a prominent Quaker family of mixed ancestry: African, Anglo-American, and Lenape. His father, William, was born into slavery, escaped from a plantation in his teens and eventually became the minister of Princeton's Witherspoon Street Presbyterian Church in 1881. Robeson had three brothers: William Drew Jr. (born 1881), Reeve (born c. 1887), and Ben (born c. 1893); and one sister, Marian (born c. 1895).

In 1900, a disagreement between William and white financial supporters of Witherspoon arose with apparent racial undertones, which were prevalent in Princeton. William, who had the support of his entirely black congregation, resigned in 1901. The loss of his position forced him to work menial jobs. Three years later when Robeson was six, his mother, who was nearly blind, died in a house fire. Eventually, William became financially incapable of providing a house for himself and his children still living at home, Ben and Paul, so they moved into the attic of a store in Westfield, New Jersey.

William found a stable parsonage at the St. Thomas A. M. E. Zion in 1910, where Robeson would fill in for his father during sermons when he was called away. In 1912, Robeson attended Somerville High School in Somerville, New Jersey, where he performed in "Julius Caesar" and "Othello", sang in the chorus, and excelled in football, basketball, baseball and track. His athletic dominance elicited racial taunts which he ignored. Prior to his graduation, he won a statewide academic contest for a scholarship to Rutgers. He took a summer job as a waiter in Narragansett Pier, Rhode Island, where he befriended Fritz Pollard, later to be the first African-American coach in the National Football League.

In late 1915, Robeson became the third African-American student ever enrolled at Rutgers, and the only one at the time. He tried out for the Rutgers Scarlet Knights football team, and his resolve to make the squad was tested as his teammates engaged in excessive play, during which his nose was broken and his shoulder dislocated. The coach, Foster Sanford, decided he had overcome the provocation and announced that he had made the team.

Robeson joined the debating team and sang off-campus for spending money, and on-campus with the Glee Club informally, as membership required attending all-white mixers. He also joined the other collegiate athletic teams. As a sophomore, amidst Rutgers' sesquicentennial celebration, he was benched when a Southern team refused to take the field, because the Scarlet Knights had fielded a Negro, Robeson.

After a standout junior year of football, he was recognized in "The Crisis" for his athletic, academic, and singing talents. At this time his father fell grievously ill. Robeson took the sole responsibility in caring for him, shuttling between Rutgers and Somerville. His father, who was the "glory of <nowiki>his</nowiki> boyhood years" soon died, and at Rutgers, Robeson expounded on the incongruity of African Americans fighting to protect America in World War I but, contemporaneously, being without the same opportunities in the United States as whites.
He finished university with four annual oratorical triumphs and varsity letters in multiple sports. His play at end won him first-team All-American selection, in both his junior and senior years. Walter Camp considered him the greatest end ever. Academically, he was accepted into Phi Beta Kappa and Cap and Skull. His classmates recognized him by electing him class valedictorian. "The Daily Targum" published a poem featuring his achievements. In his valedictory speech, he exhorted his classmates to work for equality for all Americans.

Robeson entered New York University School of Law in fall 1919. To support himself, he became an assistant football coach at Lincoln, where he joined the Alpha Phi Alpha fraternity. However, Robeson felt uncomfortable at NYU and moved to Harlem and transferred to Columbia Law School in February 1920. Already known in the black community for his singing, he was selected to perform at the dedication of the Harlem YWCA.

Robeson began dating Eslanda "Essie" Goode and after her coaxing, he gave his theatrical debut as "Simon" in Ridgely Torrence's "Simon of Cyrene". After a year of courtship, they were married in August 1921.

Robeson was recruited by Pollard to play for the NFL's Akron Pros while he continued his law studies. In the spring, Robeson postponed school to portray Jim in Mary Hoyt Wiborg's play "Taboo". He then sang in a chorus in an Off-Broadway production of "Shuffle Along" before he joined "Taboo" in Britain. The play was adapted by Mrs. Patrick Campbell to highlight his singing. After the play ended, he befriended Lawrence Brown, a classically trained musician, before returning to Columbia while playing for the NFL's Milwaukee Badgers. He ended his football career after 1922, and months later, he graduated from law school.

Robeson worked briefly as a lawyer, but he renounced a career in law due to extant racism. Essie financially supported them and they frequented the social functions at the future Schomburg Center. In December 1924 he landed the lead role of Jim in Eugene O'Neill's "All God's Chillun Got Wings", which culminated with Jim metaphorically consummating his marriage with his white wife by symbolically emasculating himself. "Chillun's" opening was postponed due to nationwide controversy over its plot.

"Chillun's" delay led to a revival of "The Emperor Jones" with Robeson as Brutus, a role pioneered by Charles Sidney Gilpin. The role terrified and galvanized Robeson, as it was practically a 90-minute soliloquy. Reviews declared him an unequivocal success. Though arguably clouded by its controversial subject, his Jim in "Chillun" was less well received. He deflected criticism of its plot by writing that fate had drawn him to the "untrodden path" of drama and the true measure of a culture is in its artistic contributions, and the only true American culture was African-American.

The success of his acting placed him in elite social circles and his ascension to fame, which was forcefully aided by Essie, had occurred at a startling pace. Essie's ambition for Robeson was a startling dichotomy to his indifference. She quit her job, became his agent, and negotiated his first movie role in a silent race film directed by Oscar Micheaux, "Body and Soul". To support a charity for single mothers, he headlined a concert singing spirituals. He performed his repertoire of spirituals on the radio.

Lawrence Brown, who had become renowned while touring as a pianist with gospel singer Roland Hayes, stumbled upon Robeson in Harlem. The two ad-libbed a set of spirituals, with Robeson as lead and Brown as accompanist. This so enthralled them that they booked Provincetown Playhouse for a concert. The pair's rendition of African-American folk songs and spirituals was captivating, and Victor Records signed Robeson to a contract.

The Robesons went to London for a revival of "Jones", before spending the rest of the fall on holiday on the French Riviera, socializing with Gertrude Stein and Claude McKay. Robeson and Brown performed a series of concert tours in America from January 1926 until May 1927.

During a hiatus in New York, Robeson learned that Essie was several months pregnant. Paul Robeson Jr. was born in November 1927 in New York, while Robeson and Brown toured Europe. Essie experienced complications from the birth, and by mid-December, her health had deteriorated dramatically. Ignoring Essie's objections, her mother wired Robeson and he immediately returned to her bedside. Essie completely recovered after a few months.

In 1928, Robeson played "Joe" in the London production of the American musical "Show Boat", at the Theatre Royal, Drury Lane. His rendition of "Ol' Man River" became the benchmark for all future performers of the song. Some black critics were not pleased with the play due to its usage of the word "nigger". It was, nonetheless, immensely popular with white audiences. He was summoned for a Royal Command Performance at Buckingham Palace and Robeson was befriended by MPs from the House of Commons. "Show Boat" continued for 350 performances and, as of 2001, it remained the Royal's most profitable venture. The Robesons bought a home in Hampstead. He reflected on his life in his diary and wrote that it was all part of a "higher plan" and "God watches over me and guides me. He's with me and lets me fight my own battles and hopes I'll win." However, an incident at the Savoy Grill, in which he was refused seating, sparked him to issue a press release describing the insult which subsequently became a matter of public debate.

Essie had learned early in their marriage that Robeson had been involved in extramarital affairs, but she tolerated them. However, when she discovered that he was having another affair, she unfavorably altered the characterization of him in his biography, and defamed him by describing him with "negative racial stereotypes". Despite her uncovering of this tryst, there was no public evidence that their relationship had soured.

In early 1930, they both appeared in the experimental Swiss film "Borderline", and Robeson then returned to the Savoy Theatre, in London's West End to play the lead in Shakespeare's "Othello", opposite Peggy Ashcroft as Desdemona. Robeson was the first black actor to play Othello in Britain since Ira Aldridge. The production received mixed reviews which noted Robeson's "highly civilized quality <nowiki>[but lacking the]</nowiki> grand style." Robeson stated the best way to diminish the oppression African Americans faced was for his artistic work to be an example of what "men of my colour" could accomplish rather than to "be a propagandist and make speeches and write articles about what they call the Colour Question."

After Essie discovered Robeson had been having an affair with Ashcroft, she decided to seek a divorce and they split up. Robeson returned to Broadway as Joe in the 1932 revival of "Show Boat", to critical and popular acclaim. Subsequently, he received, with immense pride, an honorary master's degree from Rutgers. Thereabout, his former football coach, Foster Sanford, advised him that divorcing Essie and marrying Ashcroft would do irreparable damage to his reputation. Ashcroft and Robeson's relationship ended in 1932, following which Robeson and Essie reconciled, although their relationship was permanently scarred.

In 1933, Robeson played the role of Jim in the London production of "Chillun", virtually gratis; then returned to the United States to star as Brutus in the film "The Emperor Jones", "a feat not repeated for more than two decades in the U.S." His acting in "Jones"—the first film to feature an African American in a starring role—was well received. On the film set he rejected any slight to his dignity, despite the widespread Jim Crow atmosphere in the United States. Upon returning to England he publicly criticized African Americans' rejection of their own culture. Despite negative reactions from the press, such as a "New York Amsterdam News" retort that Robeson had made a "jolly well <nowiki>[ass of himself]</nowiki>", he also announced that he would reject any offers to perform European opera, because the music had no connection to his heritage.

In early 1934 Robeson enrolled in the School of Oriental and African Studies in London, where he studied some 20 African dialects. His "sudden interest" in African history and its impact on culture coincided with his essay "I Want to be African", wherein he wrote of his desire to embrace his ancestry.
He undertook the role of Bosambo in the movie "Sanders of the River", which he felt would render a realistic view of colonial African culture. His friends in the anti-imperialism movement and association with British socialists led him to visit the Soviet Union. Robeson, Essie, and Marie Seton traveled to the Soviet Union on an invitation from Sergei Eisenstein in December 1934. A stopover in Berlin enlightened Robeson to the racism in Nazi Germany and, on his arrival in Moscow, in the Soviet Union, Robeson said, "Here I am not a Negro but a human being for the first time in my life .. I walk in full human dignity." Waldemar ("Wally") Hille, who subsequently went on to do arrangements on the People's Songs Bulletin, got his start as an early touring pianist for Robeson.

"Sanders of the River", released in 1935, made Robeson an international movie star; but the stereotypical portrayal of a colonial African was seen as embarrassing to his stature as an artist and damaging to his reputation. The Commissioner of Nigeria to London protested the film as slanderous to his country, and Robeson thereafter became more politically conscious of his roles. He appeared in the play "Stevedore" at the Embassy Theatre in London in May 1935, which was favorably reviewed in "The Crisis" by Nancy Cunard, who concluded: ""Stevedore" is extremely valuable in the racial–social question — it is straight from the shoulder". In early 1936, he decided to send his son to school in the Soviet Union to shield him from racist attitudes. He then played the role of Toussaint Louverture in the eponymous play by C. L. R. James at the Westminster Theatre, and appeared in the films "Song of Freedom", "Show Boat", "Big Fella", "My Song Goes Forth", and "King Solomon's Mines". In 1938, he was named by Motion Picture Herald as the 10th most popular star in British cinema.

Robeson believed that the struggle against fascism during the Spanish Civil War was a turning point in his life and transformed him into a political activist. In 1937, he used his concert performances to advocate the Republican cause and the war's refugees. He permanently modified his renditions of "Ol' Man River" from a tragic "song of resignation with a hint of protest implied" into a battle hymn of unwavering defiance. His business agent expressed concern about his political involvement, but Robeson overruled him and decided that contemporary events trumped commercialism. In Wales, he commemorated the Welsh killed while fighting for the Republicans, where he recorded a message which would become his epitaph: "The artist must take sides. He must elect to fight for freedom or slavery. I have made my choice. I had no alternative."

After an invitation from J. B. S. Haldane, he traveled to Spain in 1938 because he believed in the International Brigades's cause, visited the hospital of the Benicàssim, singing to the wounded soldiers. Also visited the battlefront and provided a morale boost to the Republicans at a time when their victory was unlikely. Back in England, he hosted Jawaharlal Nehru to support Indian independence, whereat Nehru expounded on imperialism's affiliation with Fascism. Robeson reevaluated the direction of his career and decided to focus on the ordeals of "common people", He appeared in the pro-labor play "Plant in the Sun", in which he played an Irishman, his first "white" role. With Max Yergan, and the CAA, Robeson became an advocate in the aspirations of African nationalists for political independence.

Robeson also developed a sympathy for China's side in the Second Sino-Japanese War. In 1940, the Chinese progressive activist, Liu Liangmo taught Robeson the patriotic song ""Chee Lai!"" ("Arise!"), known as the March of the Volunteers. Robeson memorized the words in Chinese. Robeson premiered the song at a large concert in New York City's Lewisohn Stadium. and recorded it in both English and Chinese for Keynote Records in early 1941. Its 3-disc album included a booklet whose preface was written by Soong Ching-ling, widow of Sun Yat-sen, Robeson gave further performances at benefits for the China Aid Council and United China Relief at their sold-out concert at Washington's Uline Arena on April 24, 1941. The Washington Committee for Aid to China had booked Constitution Hall but been blocked by the Daughters of the American Revolution owing to Robeson's race. The indignation was great enough that President Roosevelt's wife Eleanor and Hu Shih, the Chinese ambassador, joined as sponsors. However, when the organizers offered tickets on generous terms to the National Negro Congress to help fill the larger venue, these sponsors withdrew, in objection to the NNC's Communist ties. 

Partly because of the favorable international reputation Robeson gave to the song, it became China's National Anthem after 1949. The Chinese lyricist died in a Beijing prison in 1968, but Robeson continued to send royalties to his family. 

Robeson's last British film was "The Proud Valley" (released 1940), set in a Welsh coal-mining town. After the outbreak of World War II, Robeson returned to the United States and became America's "no.1 entertainer" with a radio broadcast of "Ballad for Americans". Nevertheless, during an ensuing tour, the Beverly Wilshire Hotel was the only hotel willing to accommodate him due to his race, providing he registered under an assumed name, and he therefore dedicated two hours every afternoon sitting in the lobby "to ensure that the next time Black come through, they'll have a place to stay."

Furthermore, "Native Land" was labeled by the FBI as communist propaganda. After an appearance in "Tales of Manhattan", a production that he felt was "very offensive to my people", he announced that he would no longer act in films because of the demeaning roles available to blacks.

Robeson participated in benefit concerts on behalf of the war effort and at a concert at the Polo Grounds, he met two emissaries from the Jewish Anti-Fascist Committee, Solomon Mikhoels and Itzik Feffer Subsequently, Robeson reprised his role of Othello at the Shubert Theatre in 1943, and became the first African American to play the role with a white supporting cast on Broadway. During the same period of time, he addressed a meeting with Kenesaw Mountain Landis in a failed attempt to convince him to admit black players to Major League Baseball. He toured North America with "Othello" until 1945, and subsequently, his political efforts with the CAA to get colonial powers to discontinue their exploitation of Africa were short-circuited by the United Nations.

After the mass lynching of four African Americans on July 25, 1946, Robeson met with President Truman and admonished Truman by stating that if he did not enact legislation to end lynching, "the Negroes will defend themselves". Truman immediately terminated the meeting and declared that the time was not right to propose anti-lynching legislation. Subsequently, Robeson publicly called upon all Americans to demand that Congress pass civil rights legislation. Taking a stance against lynching, Robeson founded the American Crusade Against Lynching organization in 1946. This organization was thought to be a threat to the NAACP antiviolence movement. Robeson received support from W. E. B. Du Bois regarding this matter and officially launched this organization on the anniversary day of the Emancipation Proclamation, September 23.

About this time, Robeson's belief that trade unionism was crucial to civil rights became a mainstay of his political beliefs as he became proponent of the union activist Revels Cayton. Robeson was later called before the Tenney Committee where he responded to questions about his affiliation with the Communist Party USA (CPUSA) by testifying that he was not a member of the CPUSA. Nevertheless, two organizations with which Robeson was intimately involved, the Civil Rights Congress (CRC) and the CAA, were placed on the Attorney General's List of Subversive Organizations (AGLOSO). Subsequently, he was summoned before the United States Senate Committee on the Judiciary, and when questioned about his affiliation with the Communist Party, he refused to answer, stating: "Some of the most brilliant and distinguished Americans are about to go to jail for the failure to answer that question, and I am going to join them, if necessary."

In 1948, Robeson was preeminent in Henry A. Wallace's bid for the President of the United States, during which Robeson traveled to the Deep South, at risk to his own life, to campaign for him. In the ensuing year, Robeson was forced to go overseas to work because his concert performances were canceled at the FBI's behest. While on tour, he spoke at the World Peace Council, at which his speech was publicly reported as equating America with a Fascist state—a depiction that he flatly denied. Nevertheless, the speech publicly attributed to him was a catalyst for his becoming an enemy of mainstream America. Robeson refused to bow to public criticism when he advocated in favor of twelve defendants, including his long-time friend, Benjamin J. Davis Jr., charged during the Smith Act trials of Communist Party leaders.
Robeson traveled to Moscow in June, and was unable to find Itzik Feffer. He let Soviet authorities know that he wanted to see him. Reluctant to lose Robeson as a propagandist for the Soviet Union, the Soviets brought Feffer from prison to him. Feffer told him that Mikhoels had been murdered, and he would be summarily executed. To protect the Soviet Union's reputation, and to keep the right wing of the United States from gaining the moral high ground, Robeson denied that any persecution existed in the Soviet Union, and kept the meeting secret for the rest of his life, except from his son. On June 20, 1949, Robeson spoke at the Paris Peace Congress saying that "We in America do not forget that it was on the backs of the white workers from Europe and on the backs of millions of Blacks that the wealth of America was built. And we are resolved to share it equally. We reject any hysterical raving that urges us to make war on anyone. Our will to fight for peace is strong. We shall not make war on anyone. We shall not make war on the Soviet Union. We oppose those who wish to build up imperialist Germany and to establish fascism in Greece. We wish peace with Franco's Spain despite her fascism. We shall support peace and friendship among all nations, with Soviet Russia and the people's Republics." He was blacklisted for saying this in the mainstream press within the United States, including in many periodicals of the Negro press such as "The Crisis".

In order to isolate Robeson politically, the House Un-American Activities Committee (HUAC) subpoenad Jackie Robinson to comment on Robeson's Paris speech. Robinson testified that Robeson's statements, "'if accurately reported', <nowiki>were</nowiki> silly'". Days later, the announcement of a concert headlined by Robeson in New York City provoked the local press to decry the use of their community to support "subversives" and the Peekskill Riots ensued.

A book reviewed in early 1950 as "the most complete record on college football" failed to list Robeson as ever having played on the Rutgers team and as ever having been an All-American. Months later, NBC canceled Robeson's appearance on Eleanor Roosevelt's television program. Subsequently, the State Department denied Robeson a passport to travel abroad and issued a "stop notice" at all ports because it believed that an isolated existence inside United States borders would not only afford him less freedom of expression but also avenge his "<nowiki>extreme advocacy on</nowiki> behalf of the independence <nowiki>of</nowiki> the colonial peoples of Africa." However, when Robeson met with State Department officials and asked why he was denied a passport, he was told that "his frequent criticism of the treatment of blacks in the United States should not be aired in foreign countries".

In 1951, an article titled "Paul Robeson – the Lost Shepherd" was published in "The Crisis" although Paul Jr. suspected it was authored by Amsterdam News columnist Earl Brown. J. Edgar Hoover and the United States State Department arranged for the article to be printed and distributed in Africa in order to defame Robeson's reputation and reduce his and Communists' popularity in colonial countries. Another article by Roy Wilkins (now thought to have been the real author of "Paul Robeson - the Lost Shepherd") denounced Robeson as well as the Communist Party USA (CPUSA) in terms consistent with the anti-Communist FBI propaganda.

On December 17, 1951, Robeson presented to the United Nations an anti-lynching petition, "We Charge Genocide". The document asserted that the United States federal government, by its failure to act against lynching in the United States, was "guilty of genocide" under Article II of the UN Genocide Convention.

In 1952, Robeson was awarded the International Stalin Prize by the Soviet Union. Unable to travel to Moscow, he accepted the award in New York. In April 1953, shortly after Stalin's death, Robeson penned "To You My Beloved Comrade", praising Stalin as dedicated to peace and a guide to the world: "Through his deep humanity, by his wise understanding, he leaves us a rich and monumental heritage." Robeson's opinion on the Soviet Union kept his passport out of reach and stopped his return to the entertainment industry and the civil rights movement. In his opinion, the Soviet Union was the guarantor of political balance in the world.

In a symbolic act of defiance against the travel ban, in May 1952, labor unions in the United States and Canada organized a concert at the International Peace Arch on the border between Washington state and the Canadian province of British Columbia. Robeson returned to perform a second concert at the Peace Arch in 1953, and over the next two years, two further concerts took place. In this period, with the encouragement of his friend the Welsh politician Aneurin Bevan, Robeson recorded a number of radio concerts for supporters in Wales.

In 1956, Robeson was called before HUAC after he refused to sign an affidavit affirming that he was not a Communist. In his testimony, he invoked the Fifth Amendment and refused to reveal his political affiliations. When asked why he had not remained in the Soviet Union because of his affinity with its political ideology, he replied, "because my father was a slave and my people died to build <nowiki>[the United States and]</nowiki>, I am going to stay here, and have a part of it just like you and no fascist-minded people will drive me from it!" At that hearing, Robeson stated "Whether I am or not a Communist is irrelevant. The question is whether American citizens, regardless of their political beliefs or sympathies, may enjoy their constitutional rights." Robeson's passport was subsequently revoked. Campaigns were launched to protest the passport ban and the restriction of his right to travel over the next four years, but it was to no avail. In 1957, unable to accept invitations to perform abroad, Paul Robeson sang for audiences in London, where 1,000 concert tickets for his telephone concert at St Pancras Town Hall sold out within an hour, and Wales via the transatlantic telephone cable TAT-1: "We have to learn the hard way that there is another way to sing".

In 1956, in the United Kingdom, Topic Records, at that time part of the Workers Music Association, released a single of Robeson singing "Joe Hill", written by Alfred Hayes and Earl Robinson, backed with "John Brown's Body". Joe Hill (1879–1915) was a labor activist in the early 20th century, and "Joe Hill" sung by Robeson is the third most popular selection on the BBC radio programme "Desert Island Discs" among British Labour Party politicians, and in the top 14 most popular selections for all British politicians.

Nikita Khrushchev's denunciation of Stalinism at the 1956 Party Congress silenced Robeson on Stalin, though Robeson continued to praise the Soviet Union. In 1956, after public pressure brought a one-time exemption to the travel ban, Robeson performed two concerts in Canada in February, one in Toronto and the other at a union convention in Sudbury, Ontario. That year Robeson, along with close friend W. E. B. Du Bois, compared the anti-Soviet uprising in Hungary to the "same sort of people who overthrew the Spanish Republican Government" and supported the Soviet invasion and suppression of the revolt.

An appeal to the Supreme Court of the United States to reinstate his confiscated passport had been rejected, but over the telephone Robeson was able to sing to the 5,000 gathered there as he had earlier in the year to London. Due to the reaction to the promulgation of Robeson's political views, his recordings and films were removed from public distribution, and he was universally condemned in the U.S press. During the height of the Cold War, it became increasingly difficult in the United States to hear Robeson sing on commercial radio, buy his music or see his films.

1958 saw the publication of Robeson's "manifesto-autobiography", "Here I Stand". His passport was restored in June 1958 via "Kent v. Dulles", and he embarked on a world tour using London as his base. In Moscow in August 1959, he received a tumultuous reception at the Lenin Stadium (Khabarovsk) where he sang classic Russian songs along with American standards. Robeson and Essie then flew to Yalta to rest and spend time with Nikita Khrushchev.

On October 11, 1959, Robeson took part in a service at St. Paul's Cathedral, the first black performer to sing there. On a trip to Moscow, Robeson experienced bouts of dizziness and heart problems and was hospitalized for two months while Essie was diagnosed with operable cancer. He recovered and returned to the UK to visit the National Eisteddfod.

Meanwhile, the State Department had circulated negative literature about him throughout the media in India.

During his run at the Royal Shakespeare Company playing Othello in Tony Richardson's 1959 production at Stratford-upon-Avon, he befriended actor Andrew Faulds, whose family hosted him in the nearby village of Shottery. In 1960, in what would prove to be his final concert performance in Great Britain, Robeson sang to raise money for the Movement for Colonial Freedom at the Royal Festival Hall.

In October 1960, Robeson embarked on a two-month concert tour of Australia and New Zealand with Essie, primarily to generate money, at the behest of Australian politician Bill Morrow. While in Sydney, he became the first major artist to perform at the construction site of the future Sydney Opera House. After appearing at the Brisbane Festival Hall, they went to Auckland where Robeson reaffirmed his support of Marxism, denounced the inequality faced by the Māori and efforts to denigrate their culture. Thereabouts, Robeson publicly stated "..the people of the lands of Socialism want peace dearly".

During the tour he was introduced to Faith Bandler who interested the Robesons in the plight of the Australian Aborigines. Robeson, consequently, became enraged and demanded the Australian government provide the Aborigines citizenship and equal rights. He attacked the view of the Aborigines as being unsophisticated and uncultured, and declared, "there's no such thing as a "backward" human being, there is only a society which says they are backward."

Back in London, he decided to return to the United States, where he hoped to resume participation in the civil rights movement, stopping off in Africa and Cuba along the way. Essie argued to stay in London, fearing that he'd be "killed" if he returned and would be "unable to make any money" due to harassment by the United States government. Robeson disagreed and made his own travel arrangements, arriving in Moscow in March 1961.

During an uncharacteristically wild party in his Moscow hotel room, Robeson locked himself in his bedroom and attempted suicide by cutting his wrists. Three days later, under Soviet medical care, he told his son that he felt extreme paranoia, thought that the walls of the room were moving and, overcome by a powerful sense of emptiness and depression, tried to take his own life.

Paul Jr. believed that his father's health problems stemmed from attempts by the CIA and MI5 to "neutralize" his father. He remembered that his father had had such fears prior to his prostate operation. He said that three doctors treating Robeson in London and New York had been CIA contractors, and that his father's symptoms resulted from being "subjected to mind depatterning under MKULTRA", a secret CIA programme. Martin Duberman wrote that Robeson's health breakdown was probably brought on by a combination of factors including extreme emotional and physical stress, bipolar depression, exhaustion and the beginning of circulatory and heart problems. "[E]ven without an organic predisposition and accumulated pressures of government harassment he might have been susceptible to a breakdown."

Robeson stayed at the Barvikha Sanatorium until September 1961, when he left for London. There his depression reemerged, and after another period of recuperation in Moscow, he returned to London. Three days after arriving back, he became suicidal and suffered a panic attack while passing the Soviet Embassy. He was admitted to the Priory Hospital, where he underwent electroconvulsive therapy (ECT) and was given heavy doses of drugs for nearly two years, with no accompanying psychotherapy. During his treatment at the Priory, Robeson was being monitored by the British MI5. Both intelligence services were well aware of Robeson's suicidal state of mind. An FBI memo described Robeson's debilitated condition, remarking that his "death would be much publicized" and would be used for Communist propaganda, necessitating continued surveillance. Numerous memos advised that Robeson should be denied a passport renewal, an obstacle that was likely to further jeopardize his recovery process.

In August 1963, disturbed about his treatment, friends and family had Robeson transferred to the Buch Clinic in East Berlin. Given psychotherapy and less medication, his physicians found him still "completely without initiative" and they expressed "doubt and anger" about the "high level of barbiturates and ECT" that had been administered in London. He rapidly improved, though his doctor stressed that "what little is left of Paul's health must be quietly conserved."

In 1963, Robeson returned to the United States and for the remainder of his life lived in seclusion. He momentarily assumed a role in the civil rights movement, making a few major public appearances before falling seriously ill during a tour. Double pneumonia and a kidney blockage in 1965 nearly killed him.

Robeson was contacted by both Bayard Rustin and James Farmer about the possibility of becoming involved with the mainstream of the Civil Rights Movement. Because of Rustin's past anti-Communist stances, Robeson declined to meet with him. Robeson eventually met with Farmer, but because he was asked to denounce Communism and the Soviet Union in order to assume a place in the mainstream, Robeson adamantly declined.

After Essie, who had been his spokesperson to the media, died in December 1965, Robeson moved in with his son's family in New York City. He was rarely seen strolling near his Harlem apartment on Jumal Place (sic), and his son responded to press inquiries that his "father's health does not permit him to perform or answer questions."

In 1968, he settled at his sister's home in Philadelphia. Numerous celebrations were held in honor of Robeson over the next several years, including at public arenas that had previously shunned him, but he saw few visitors aside from close friends and gave few statements apart from messages to support current civil rights and international movements, feeling that his record "spoke for itself". At a Carnegie Hall tribute to mark his 75th birthday in 1973, he was unable to attend, but a taped message from him was played that said: "Though I have not been able to be active for several years, I want you to know that I am the same Paul, dedicated as ever to the worldwide cause of humanity for freedom, peace and brotherhood."

On January 23, 1976, following complications of a stroke, Robeson died in Philadelphia at the age of 77. He lay in state in Harlem and his funeral was held at his brother Ben's former parsonage, Mother Zion AME Zion Church, where Bishop J. Clinton Hoggard performed the eulogy. His twelve pall bearers included Harry Belafonte and Fritz Pollard . He was interred in the Ferncliff Cemetery in Hartsdale, New York. According to biographer, Martin Duberman, contemporary post-mortem reflections on Robeson's life in "<nowiki>[t</nowiki>he] white <nowiki>[American]</nowiki> press..ignored the continuing inability of white America to tolerate a black maverick who refused to bend, ..downplayed the racist component central to his persecution <nowiki>[during his life]</nowiki>", as they "paid him gingerly respect and tipped their hat to him as a 'great American,'" while the black American press, "which had never, overall, been as hostile to Robeson <nowiki>[as the white American press had],</nowiki> opined that his life '..would always be a challenge to white and Black America.'"

Early in his life, Robeson was one of the most influential participants in the Harlem Renaissance. His achievements in sport and culture were all the more incredible given the barriers of racism he had to surmount. Robeson brought Negro spirituals into the American mainstream . His theatrical performances have been recognized as the first to display dignity for black actors and pride in African heritage, and he was among the first artists to refuse to play live to segregated audiences.

After McCarthyism, <nowiki>[Robeson's stand]</nowiki> on anti-colonialism in the 1940s would never again have a voice in American politics, but the <nowiki>[African independence movements]</nowiki> of the late 1950s and 1960s would vindicate <nowiki>his</nowiki> anti-colonial <nowiki>[agenda].</nowiki>

Subsequently, in 1945 he received the Spingarn medal from the NAACP. Several public and private establishments he was associated with have been landmarked, or named after him. His efforts to end Apartheid in South Africa were posthumously rewarded in 1978 by the United Nations General Assembly. "" won an Academy Award for best short documentary in 1980. In 1995, he was named to the College Football Hall of Fame. In the centenary of his birth, which was commemorated around the world, he was awarded a Lifetime Achievement Grammy Award, as well as a star on the Hollywood Walk of Fame. Robeson is also a member of the American Theater Hall of Fame.

, the run of "Othello" starring Robeson was the longest-running production of a Shakespeare play ever staged on Broadway. He received a Donaldson Award for his performance. His Othello was characterised by Michael A. Morrison in 2011 as a high point in Shakespearean theatre in the 20th century.

Robeson left Australia as a respected, albeit controversial, figure and his support for Aboriginal rights had a profound effect in Australia over the next decade.

Robeson archives exist at the Academy of Arts; Howard University, and the Schomburg Center. In 2010, Susan Robeson launched a project by Swansea University and the Welsh Assembly to create an online learning resource in her grandfather's memory.

Robeson connected his own life and history not only to his fellow Americans and to his people in the South, but to all the people of Africa and its diaspora whose lives had been fundamentally shaped by the same processes that had brought his ancestors to America. While a consensus definition of his legacy remains controversial, to deny his courage in the face of public and governmental pressure would be to defame his courage.

In 1976, the apartment building on Edgecombe Avenue in the Washington Heights section of Manhattan where Robeson lived during the early 1940s was officially renamed the Paul Robeson Residence, and declared a National Historic Landmark. In 1993, the building was designated a New York City landmark as well. Edgecombe Avenue itself was later co-named Paul Robeson Boulevard.

In 1978, TASS announced that the Latvian Shipping Company had named one of its new 40,000-ton tankers "Paul Robeson" in honor of the singer. TASS said the ship's crew would establish a Robeson museum aboard the tanker.

In 2002, a blue plaque was unveiled by English Heritage on the house in Hampstead where Robeson lived in 1929–30.

In 2004, the U.S. Postal Service issued a 37-cent stamp honoring Robeson.

In 2007, the Criterion Collection, a company that specializes in releasing special-edition versions of classic and contemporary films, released a DVD boxed set of Robeson films.

In 2009, Robeson was inducted into the New Jersey Hall of Fame.

The main campus library at Rutgers University-Camden is named after Robeson, as is the campus center at Rutgers University-Newark. The Paul Robeson Cultural Center is on the campus of Rutgers University-New Brunswick.

In 1972, Penn State established a formal cultural center on the University Park campus. Students and staff chose to name the center for Robeson.

A street in Princeton is named after him. In addition, the block of Davenport Street in Somerville, New Jersey, where St. Thomas AME Zion Church still stands is called Paul Robeson Boulevard.
Tom Rob Smith's novel "Agent 6" (2012) includes the character Jesse Austin, "a black singer, political activist and communist sympathizer modeled after real-life actor/activist Paul Robeson."

In 1954, the Kurdish poet Abdulla Goran wrote the poem "Bangêk bo Pol Ropsin" ("A Call for Paul Robeson"). In the same year, another Kurdish poet, Cegerxwîn, also wrote a poem about him, "Heval Pol Robson" ("Comrade Paul Robeson"), which was put to music by singer Şivan Perwer in 1976.

Black 47's album "Home of The Brave" includes the song "Paul Robeson (Born To Be Free)", which features spoken quotes of Robeson as part of the song. These quotes are drawn from Robeson's testimony before the House Un-American Activities Committee in June 1956.

In January 1978, James Earl Jones performed the one-man show "Paul Robeson", written by Phillip Hayes Dean, on Broadway. This stage drama was made into a TV movie in 1979, starring Jones and directed by Lloyd Richards.

At the 2007 Edinburgh Fringe Festival, British-Nigerian actor Tayo Aluko, himself a baritone soloist, premiered his one-man show, "Call Mr. Robeson: A Life with Songs", which has since toured various countries. 

In November 2014, it was reported that film director Steve McQueen's next film would be a biographical film about Paul Robeson. As of 2017, the film has not been made.

In 2001, Welsh rock band Manic Street Preachers released a song titled "Let Robeson Sing" as a tribute to Robeson, which reached number 19 on the UK singles chart.












</doc>
<doc id="23230" url="https://en.wikipedia.org/wiki?curid=23230" title="Polaris">
Polaris

Polaris, designated Alpha Ursae Minoris ( Ursae Minoris, abbreviated Alpha UMi, UMi), commonly the North Star or Pole Star, is the brightest star in the constellation of Ursa Minor. It is very close to the north celestial pole, making it the current northern pole star. The revised Hipparcos parallax gives a distance to Polaris of about 433 light-years (133 parsecs), while calculations by other methods derive distances around 30% closer.

Polaris is a multiple star, comprising the main star (Polaris Aa, a yellow supergiant) in orbit with a smaller companion (Polaris Ab); the pair in orbit with Polaris B (discovered in August 1779 by William Herschel). There were once thought to be two more distant components—Polaris C and Polaris D—but these have been shown not to be physically associated with the Polaris system.

Polaris Aa is a 5.4 solar mass () F7 yellow supergiant of spectral type Ib. This is the first classical Cepheid to have a mass determined from its orbit. The two smaller companions are Polaris B, a F3 main-sequence star orbiting at a distance of 2400 astronomical units (au), and Polaris Ab (or P), a very close F6 main-sequence star with an 18.8 au radius orbit and .

Polaris B can be seen even with a modest telescope. William Herschel discovered the star in August 1779 using a reflecting telescope of his own, one of the best telescopes of the time. By examining the spectrum of Polaris A, it was also discovered in 1929 that it was a very close binary, with the secondary being a dwarf (variously UMi P, UMi an or UMi Ab), which had been theorized in earlier observations (Moore, J. H. and Kholodovsky, E. A.). In January 2006, NASA released images, from the Hubble telescope, that showed the three members of the Polaris ternary system. The nearest dwarf star is in an orbit of only 18.5 au (2.8 billion km) from Polaris A, about the distance between the Sun and Uranus), which explains why its light is swamped by its close and much brighter companion.

Polaris A, the supergiant primary component, is a low-amplitude Population I classical Cepheid variable, although it was once thought to be a type II Cepheid due to its high galactic latitude. Cepheids constitute an important standard candle for determining distance, so Polaris, as the closest such star, is heavily studied. The variability of Polaris had been suspected since 1852; this variation was confirmed by Ejnar Hertzsprung in 1911.

The range of brightness of Polaris during its pulsations is given as 1.86–2.13, but the amplitude has changed since discovery. Prior to 1963, the amplitude was over 0.1 magnitude and was very gradually decreasing. After 1966 it very rapidly decreased until it was less than 0.05 magnitude; since then, it has erratically varied near that range. It has been reported that the amplitude is now increasing again, a reversal not seen in any other Cepheid.

The period, roughly 4 days, has also changed over time. It has steadily increased by around 4.5 seconds per year except for a hiatus in 1963–1965. This was originally thought to be due to secular redward evolution across the Cepheid instability strip, but it may be due to interference between the primary and the first-overtone pulsation modes. Authors disagree on whether Polaris is a fundamental or first-overtone pulsator and on whether it is crossing the instability strip for the first time or not.

The temperature of Polaris varies by only a small amount during its pulsations, but the amount of this variation is variable and unpredictable. The erratic changes of temperature and the amplitude of temperature changes during each cycle, from less than 50 K to at least 170 K, may be related to the orbit with Polaris Ab.

Research reported in "Science" suggests that Polaris is 2.5 times brighter today than when Ptolemy observed it, changing from third to second magnitude. Astronomer Edward Guinan considers this to be a remarkable change and is on record as saying that "if they are real, these changes are 100 times larger than [those] predicted by current theories of stellar evolution".

The modern name "Polaris" is shortened from New Latin "stella polaris" "polar star", coined in the Renaissance era, when the star had approached the celestial pole to within a few degrees. 
Gemma Frisius, writing in 1547, referred to it as "stella illa quae polaris dicitur" ("that star which is called 'polar'"), placing it 3° 7' from the celestial pole.

In antiquity, Polaris was not yet the closest naked-eye star to the celestial pole, and the entire constellation of Ursa Minor was used for navigation rather than any single star. Polaris moved close enough to the pole to be the closest naked-eye star, even though still at a distance of several degrees, in the early medieval period, and numerous names referring to this characteristic as polar star have been in use since the medieval period. In Old English, it was known as "scip-steorra" ("ship-star"); 
In the Old English rune poem, the T-rune is apparently associated with "a circumpolar constellation", compared to the quality of steadfastness or honour.

In the Hindu Puranas, it became personified under the name "Dhruva" ("immovable, fixed"). In the later medieval period, it became associated with the Marian title of "Stella Maris" "Star of the Sea" (so in Bartholomeus Anglicus, c. 1270s)
An older English name, attested since the 14th century, is lodestar "guiding star", cognate with the Old Norse "leiðarstjarna", Middle High German "leitsterne".

The ancient name of the constellation Ursa Minor, "Cynosura" (from the Greek "the dog’s tail" ) became associated with the pole star in particular by the early modern period. An explicit identification of Mary as "stella maris" with the polar star ("Stella Polaris"), as well as the use of "Cyonsura" as a name of the star, is evident in the title "Cynosura seu Mariana Stella Polaris" (i.e. "Cynosure, or the Marian Polar Star"), a collection of Marian poetry published by Nicolaus Lucensis (Niccolo Barsotti de Lucca) in 1655. 

In the Tuareg Berber language in North Africa the North Star is known as ""Tatrit tan Tamasna"" which means: "star of the plains" or "star of the desert" and that indicates its central role in navigating the vast deserts. 

Its name in traditional pre-Islamic Arab astronomy was "Al-Judeyy" الجدي, and that name was used in medieval Islamic astronomy as well
. In those times, it was not yet as close to the north celestial pole as it is now, and used to rotate around the pole.

It was invoked as a symbol of steadfastness in poetry, as "steadfast star" by Spenser.
Shakespeare's sonnet 116 is an example of the symbolism of the north star as a guiding principle: "[Love] is the star to every wandering bark / Whose worth's unknown, although his height be taken." In "Julius Caesar", he has Caesar explain his refusal to grant a pardon by saying, "I am as constant as the northern star/Of whose true-fixed and resting quality/There is no fellow in the firmament./The skies are painted with unnumbered sparks,/They are all fire and every one doth shine,/But there’s but one in all doth hold his place;/So in the world" (III, i, 65-71). Of course, Polaris will not "constantly" remain as the north star due to precession, but this is only noticeable over centuries.
In Inuit astronomy, Polaris is known as "Niqirtsuituq". It is depicted on the flag and coat of arms of the Canadian Inuit territory of Nunavut, as well as on the flag of the U.S. state of Alaska.

Because Polaris lies nearly in a direct line with the axis of the Earth's rotation "above" the North Pole—the north celestial pole—Polaris stands almost motionless in the sky, and all the stars of the northern sky appear to rotate around it. Therefore, it makes an excellent fixed point from which to draw measurements for celestial navigation and for astrometry. The moving of Polaris towards and, in the future, away from the celestial pole, is due to the precession of the equinoxes.
The celestial pole will move away from UMi after the 21st century, passing close by Gamma Cephei by about the 41st century, moving towards Deneb by about the 91st century. Historically, the celestial pole was close to Thuban around 2750 BC, and
during classical antiquity it was closer to Kochab (β UMi) than to Polaris. It was about the same angular distance from β UMi as to α UMi by the end of late antiquity. The Greek navigator Pytheas in ca. 320 BC described the celestial pole as devoid of stars. However, as one of the brighter stars close to the celestial pole, Polaris was used for navigation at least from late antiquity, and described as ἀεί φανής ("aei phanēs") "always visible" by Stobaeus (5th century), and it could reasonably be described as "stella polaris" from about the High Middle Ages.

In more recent history, in Shakespeare's play Julius Caesar, written around 1599, Caesar describes himself as being "as constant as the northern star", though in Caesar's time there was no constant northern star. It was referenced in Nathaniel Bowditch's 1802 book, "American Practical Navigator", where it is listed as one of the navigational stars. At present, Polaris is 0.75° away from the pole of rotation (1.4 times the Moon disc) and hence revolves around the pole in a small circle 1.5° in diameter. Only twice during every sidereal day does Polaris accurately define the true north azimuth; the rest of the time, it is slightly displaced eastward or westward, and the bearing must be corrected using tables or a rough rule of thumb. The best approximate was made using the leading edge of the "Big Dipper" asterism in the constellation Ursa Major as a point of reference. The leading edge (defined by the stars Dubhe and Merak) was referenced to a clock face, and the true azimuth of Polaris worked out for different latitudes.

Many recent papers calculate the distance to Polaris at about 433 light-years (133 parsecs), in agreement with parallax measurements from the Hipparcos astrometry satellite. Older distance estimates were often slightly less, and recent research based on high resolution spectral analysis suggests it may be up to 100 light years closer (323 ly/99 pc). Polaris is the closest Cepheid variable to Earth so its physical parameters are of critical importance to the whole astronomical distance scale. It is also the only one with a dynamically measured mass.

The "Hipparcos" spacecraft used stellar parallax to take measurements from 1989 and 1993 with the accuracy of 0.97 milliarcseconds (970 microarcseconds), and it obtained accurate measurements for stellar distances up to 1,000 pc away. The Hipparcos data was examined again with more advanced error correction and statistical techniques. Despite the advantages of Hipparcos astrometry, the uncertainty in its Polaris data has been pointed out and some researchers have questioned the accuracy of Hipparcos when measuring binary Cepheids like Polaris. The Hipparcos reduction specifically for Polaris has been re-examined and reaffirmed but there is still not widespread agreement about the distance.

The next major step in high precision parallax measurements will come from "Gaia", a space astrometry mission launched in 2013 and intended to measure stellar parallax to within 25 microarcseconds (μas). It was expected that Gaia would not be able to take measurements on bright stars like Polaris, but it may help with measurements of other members of assumed associations and with the general galactic distance scale. Radio telescopes have also been used to produce accurate parallax measurements at large distances, but these require a compact radio source in close association with the star which is typically only the case for cool supergiants with masers in their circumstellar material. Gaia was launched in 2013 and began its mission to record data

Although it was originally planned to limit Gaia's observations to stars fainter than magnitude 5.7, tests carried out during the commissioning phase indicated that Gaia could autonomously identify stars as bright as magnitude 3. When Gaia entered regular scientific operations in July 2014, it was configured to routinely process stars in the magnitude range 3 – 20. Beyond that limit, special procedures are used to download raw scanning data for the remaining 230 stars brighter than magnitude 3; methods to reduce and analyse these data are being developed; and it is expected that there will be "complete sky coverage at the bright end" with standard errors of "a few dozen µas".

Determining an accurate distance to Polaris is important for the cosmic distance ladder because, until new data comes, it is the only Cepheid variable for which precision distance data exists, which has a ripple effect on distance measurements that use this "ruler".

Polaris in stellar catalogues and atlases



</doc>
<doc id="23231" url="https://en.wikipedia.org/wiki?curid=23231" title="Parabola">
Parabola

In mathematics, a parabola is a plane curve which is mirror-symmetrical and is approximately U-shaped. It fits any of several superficially different mathematical descriptions, which can all be proved to define exactly the same curves.
One description of a parabola involves a point (the focus) and a line (the directrix). The focus does not lie on the directrix. The parabola is the locus of points in that plane that are equidistant from both the directrix and the focus. Another description of a parabola is as a conic section, created from the intersection of a right circular conical surface and a plane which is parallel to another plane that is tangential to the conical surface. A third description is algebraic. A parabola is a graph of a quadratic function, , for example.

The line perpendicular to the directrix and passing through the focus (that is, the line that splits the parabola through the middle) is called the "axis of symmetry". The point on the parabola that intersects the axis of symmetry is called the "vertex", and is the point where the parabola is most sharply curved. The distance between the vertex and the focus, measured along the axis of symmetry, is the "focal length". The "latus rectum" is the chord of the parabola which is parallel to the directrix and passes through the focus. Parabolas can open up, down, left, right, or in some other arbitrary direction. Any parabola can be repositioned and rescaled to fit exactly on any other parabola — that is, all parabolas are geometrically similar.

Parabolas have the property that, if they are made of material that reflects light, then light which travels parallel to the axis of symmetry of a parabola and strikes its concave side is reflected to its focus, regardless of where on the parabola the reflection occurs. Conversely, light that originates from a point source at the focus is reflected into a parallel ("collimated") beam, leaving the parabola parallel to the axis of symmetry. The same effects occur with sound and other forms of energy. This reflective property is the basis of many practical uses of parabolas.

The parabola has many important applications, from a parabolic antenna or parabolic microphone to automobile headlight reflectors to the design of ballistic missiles. They are frequently used in physics, engineering, and many other areas.

The earliest known work on conic sections was by Menaechmus in the fourth century BC. He discovered a way to solve the problem of doubling the cube using parabolas. (The solution, however, does not meet the requirements of compass-and-straightedge construction.) The area enclosed by a parabola and a line segment, the so-called "parabola segment", was computed by Archimedes via the method of exhaustion in the third century BC, in his "The Quadrature of the Parabola". The name "parabola" is due to Apollonius who discovered many properties of conic sections. It means "application", referring to "application of areas" concept, that has a connection with this curve, as Apollonius had proved. The focus–directrix property of the parabola and other conic sections is due to Pappus.

Galileo showed that the path of a projectile follows a parabola, a consequence of uniform acceleration due to gravity.

The idea that a parabolic reflector could produce an image was already well known before the invention of the reflecting telescope. Designs were proposed in the early to mid seventeenth century by many mathematicians including René Descartes, Marin Mersenne, and James Gregory. When Isaac Newton built the first reflecting telescope in 1668, he skipped using a parabolic mirror because of the difficulty of fabrication, opting for a spherical mirror. Parabolic mirrors are used in most modern reflecting telescopes and in satellite dishes and radar receivers.
A parabola can be defined geometrically as a set of points (locus of points) in the Euclidean plane:


The midpoint formula_7 of the perpendicular from the focus formula_3 onto the directrix formula_5 is called "vertex" and the line formula_10 the "axis of symmetry" of the parabola.

If one introduces cartesian coordinates, such that formula_11 and the directrix has the equation formula_12 one obtains for a point formula_13 from formula_14 the equation formula_15. Solving for formula_16 yields

The parabola is U-shaped ("opening to the top").

The horizontal chord through the focus (see picture) is called the "latus rectum"; one half of it is the "semi-latus rectum". The latus rectum is parallel to the directrix. The semi-latus rectum is designated by the letter formula_18. From the picture one obtains 

The latus rectum is defined similarly for the other two conics, namely the ellipse and the hyperbola, respectively. The latus rectum is the line drawn through a focus of a conic section parallel to the directrix and terminated both ways by the curve. For any case, formula_18 is the radius of the osculating circle at the vertex. For a parabola, the semi-latus rectum, formula_18, is the distance of the focus from the directrix. Using the parameter formula_18, the equation of the parabola can be rewritten as

More generally, if the vertex is formula_24, the focus formula_25 and the directrix formula_26, one obtains the equation

"Remark:" 

If the focus is formula_34 and the directrix formula_35 one obtains the equation

For a parametric equation of a parabola in general position see .

The implicit equation of a parabola is defined by an irreducible polynomial of degree two
such that formula_39 or, equivalently, such that formula_40 is the square of a linear polynomial.

The previous section shows: any parabola with the origin as vertex and the y-axis as axis of symmetry
can be considered as the graph of a function

For formula_42 the parabolas are opening to the top and for formula_43 opening to the bottom (see picture). From the section above one obtains:


For formula_51 the parabola is the unit parabola with equation formula_52.
Its focus is formula_53, the semi-latus rectum formula_54 and the directrix has the equation formula_55.

The general function of degree 2 is 
Completing the square yields
which is the equation of a parabola with


Two objects in the Euclidean plane are "similar" if one can be transformed to the other by a "similarity", that is, an arbitrary composition of rigid motions (translations and rotations) and uniform scalings.

A parabola formula_66 with vertex formula_24 can be transformed by the translation formula_68 to one with the origin as vertex. A suitable rotation around the origin can then transform the parabola to one that has the -axis as axis of symmetry. Hence the parabola formula_66 can be transformed by a rigid motion to a parabola with an equation formula_70. Such a parabola can then be transformed by the uniform scaling formula_71 into the unit parabola with equation formula_72. Thus, any parabola can be mapped to the unit parabola by a similarity.

A synthetic approach, using similar triangles, can also be used to establish this result.
The general result is that two conic sections (necessarily of the same type) are similar if and only if they have the same eccentricity. Thus, only circles (all having eccentricity 0) share this property with parabolas (all having eccentricity 1), while general ellipses and hyperbolas do not. 

There are other simple affine transformations that map the parabola formula_73 onto the unit parabola, such as formula_74. But this mapping is not a similarity, and only shows that all parabolas are affinely equivalent (see ).

The pencil of conic sections with the x-axis as axis of symmetry, one vertex at the origin (0,0) and the same semi-latus rectum formula_18 can be represented by the equation 
with formula_77 the eccentricity.

If , the parabola with equation formula_83 (opening to the right) has the polar coordinate representation:

Its vertex is formula_86 and its focus is formula_87.

If one shifts the origin into the focus, i.e., formula_88, one obtains the equation

"Remark 1:" Inverting this polar form shows: a parabola is the inverse of a cardioid.

"Remark 2:" The second polar form is a special case of a pencil of conics with focus formula_88 (see picture):

The diagram represents a cone with its axis vertical. The point A is its apex. An inclined cross-section of the cone, shown in pink, is inclined from the vertical by the same angle, , as the side of the cone. According to the definition of a parabola as a conic section, the boundary of this pink cross-section, EPD, is a parabola.

A horizontal cross-section of the cone passes through the vertex, P, of the parabola. This cross-section is circular, but appears elliptical when viewed obliquely, as is shown in the diagram. Its centre is V, and is a diameter. We will call its radius .

Another horizontal, circular cross-section of the cone is farther from the apex, A, than the one just described. It has a chord , which joins the points where the parabola intersects the circle. Another chord, , is the perpendicular bisector of , and is consequently a diameter of the circle. These two chords and the parabola's axis of symmetry, , all intersect at the point M.

All the labelled points, except D and E, are coplanar. They are in the plane of symmetry of the whole figure. This includes the point F, which is not mentioned above. It is defined and discussed below, in the paragraph "Position of the focus".

Let us call the length of and of , and the length of  .

The lengths of and are:

Using the intersecting chords theorem on the chords and , we get:

Substituting:

Rearranging:

For any given cone and parabola, and are constants, but and are variables which depend on the arbitrary height at which the horizontal cross-section BECD is made. This last equation shows the relationship between these variables. They can be interpreted as Cartesian coordinates of the points D and E, in a system in the pink plane with P as its origin. Since is squared in the equation, the fact that D and E are on opposite sides of the -axis is unimportant. If the horizontal cross-section moves up or down, toward or away from the apex of the cone, D and E move along the parabola, always maintaining the relationship between and shown in the equation. The parabolic curve is therefore the locus of points where the equation is satisfied, which makes it a Cartesian graph of the quadratic function in the equation.

This discussion started from the definition of a parabola as a conic section, but it has now led to a description as a graph of a quadratic function. This shows that these two descriptions are equivalent. They both define curves of exactly the same shape.

It is proved in a preceding section that if a parabola has its vertex at the origin, and if it opens in the positive direction, then its equation is , where is its focal length. Comparing this with the last equation above shows that the focal length of the parabola in the cone is .

In the diagram above, the point V is the foot of the perpendicular from the vertex of the parabola to the axis of the cone. The point F is the foot of the perpendicular from the point V to the plane of the parabola. By symmetry, F is on the axis of symmetry of the parabola. Angle VPF is complementary to , and angle PVF is complementary to angle VPF, therefore angle PVF is . Since the length of is , the distance of F from the vertex of the parabola is . It is shown above that this distance equals the focal length of the parabola, which is the distance from the vertex to the focus. The focus and the point F are therefore equally distant from the vertex, along the same line, which implies that they are the same point. Therefore, the point F, defined above, is the focus of the parabola.

An alternative proof can be done using Dandelin spheres. It works without calculation and uses elementary geometric considerations, only. (see German article on )

The reflective property states that, if a parabola can reflect light, then light which enters it travelling parallel to the axis of symmetry is reflected toward the focus. This is derived from the wave nature of light in the paragraph "description of final diagram", which describes a diagram just above it, at the end of this article. This derivation is valid, but may not be satisfying to readers who would prefer a mathematical approach. In the following proof, the fact that every point on the parabola is equidistant from the focus and from the directrix is taken as axiomatic.

Consider the parabola . Since all parabolas are similar, this simple case represents all others. The right-hand side of the diagram shows part of this parabola.

The point E is an arbitrary point on the parabola, with coordinates . The focus is F, the vertex is A (the origin), and the line (the -axis) is the axis of symmetry. The line is parallel to the axis of symmetry, and intersects the -axis at D. The point C is located on the directrix (which is not shown, to minimize clutter). The point B is the midpoint of the line segment .

Measured along the axis of symmetry, the vertex, A, is equidistant from the focus, F, and from the directrix. According to the Intercept theorem, since C is on the directrix, the -coordinates of F and C are equal in absolute value and opposite in sign. B is the midpoint of FC, so its -coordinate is zero, so it lies on the -axis. Its -coordinate is half that of E, D, and C, i.e., . The slope of the line is the quotient of the lengths of and , which is , which comes to . But is also the slope (first derivative) of the parabola at E. Therefore, the line is the tangent to the parabola at E.

The distances and are equal because E is on the parabola, F is the focus and C is on the directrix. Therefore, since B is the midpoint of , triangles △FEB and △CEB are congruent (three sides), which implies that the angles marked are congruent. (The angle above E is vertically opposite angle ∠BEC.) This means that a ray of light which enters the parabola and arrives at E travelling parallel to the axis of symmetry will be reflected by the line so it travels along the line , as shown in red in the diagram (assuming that the lines can somehow reflect light). Since is the tangent to the parabola at E, the same reflection will be done by an infinitesimal arc of the parabola at E. Therefore, light that enters the parabola and arrives at E travelling parallel to the axis of symmetry of the parabola is reflected by the parabola toward its focus.

The point E has no special characteristics. This conclusion about reflected light applies to all points on the parabola, as is shown on the left side of the diagram. This is the reflective property.

There are other theorems that can be deduced simply from the above argument.

The above proof and the accompanying diagram show that the tangent bisects the angle ∠FEC. In other words, the tangent to the parabola at any point bisects the angle between the lines joining the point to the focus, and perpendicularly to the directrix.

Since triangles △FBE and △CBE are congruent, is perpendicular to the tangent . Since B is on the -axis, which is the tangent to the parabola at its vertex, it follows that the point of intersection between any tangent to a parabola and the perpendicular from the focus to that tangent lies on the line that is tangential to the parabola at its vertex. See animated diagram and pedal curve.

If light travels along the line , it moves parallel to the axis of symmetry and strikes the convex side of the parabola at E. It is clear from the above diagram that this light will be reflected directly away from the focus, along an extension of the segment .

The above proofs of the reflective and tangent bisection properties use a line of calculus. For readers who are not comfortable with calculus, the following alternative is presented.

In this diagram, F is the focus of the parabola, and T and U lie on its directrix. P is an arbitrary point on the parabola. is perpendicular to the directrix, and the line bisects angle ∠FPT. Q is another point on the parabola, with perpendicular to the directrix. We know that  =  and  = . Clearly,  > , so  > . All points on the bisector are equidistant from F and T, but Q is closer to F than to T. This means that Q is to the left of , i.e., on the same side of it as the focus. The same would be true if Q were located anywhere else on the parabola (except at the point P), so the entire parabola, except the point P, is on the focus side of . Therefore, is the tangent to the parabola at P. Since it bisects the angle ∠FPT, this proves the tangent bisection property.

The logic of the last paragraph can be applied to modify the above proof of the reflective property. It effectively proves the line to be the tangent to the parabola at E if the angles are equal. The reflective property follows as shown previously.

A parabola can be considered as the affine part of a non degenerated projective conic with a point formula_98 on the line of infinity formula_99, which is the tangent at formula_98. The 5-,4- and 3- point degenerations of Pascal's theorem are properties of a conic dealing with at least one tangent. If one considers this tangent as the line at infinity and its point of contact as the point at infinity of the "y"-axis, one obtains three statements for a parabola.

The following properties of a parabola deal only with terms "connect, intersect, parallel", which are invariants of similarities. So, it is sufficient to prove any property for the "unit parabola" with equation formula_72.

Any parabola can be described in a suitable coordinate system by an equation formula_73.

"Proof:" straightforward calculation for the unit parabola formula_52.

"Application:" The 4-points-property of a parabola can be used for the construction of point formula_116, while formula_117 and formula_105 are given.

"Remark:" the 4-points-property of a parabola is an affine version of the 5-point-degeneration of Pascal's theorem.


"Proof:" can be performed for the unit parabola formula_52. A short calculation shows: line formula_112 has slope formula_133 which is the slope of the tangent at point formula_127.

"Application:" The 3-points-1-tangent-property of a parabola can be used for the construction of the tangent at point formula_127, while formula_136 are given.

"Remark:" The 3-points-1-tangent-property of a parabola is an affine version of the 4-point-degeneration of Pascal's theorem.


"Proof:" straight forward calculation for the unit parabola formula_72.

"Application:" The 2-points-2-tangents-property can be used for the construction of the tangent of a parabola at point formula_143 while formula_151 and the tangent at formula_140 are given.

"Remark 1:" The 2-points-2-tangents-property of a parabola is an affine version of the 3-point-degeneration of Pascal's theorem.

"Remark 2:" The 2-points-2-tangents-property should not be confused with the following property of a parabola, which deals with 2 points and 2 tangents, too, but is "not" related to Pascal's theorem.

The statements above presume the knowledge of the axis-direction of the parabola, in order to construct the points formula_153. The following property determines the points formula_153 by two given points and their tangents only, and the result is: the line formula_112 is parallel to the axis of the parabola.


"Proof:" can be done (like the properties above) for the unit parabola formula_52.

"Application:" This property can be used to determine the direction of the axis of a parabola, if two points and their tangents are given. An alternative way is to determine the midpoints of two parallel chords, see section on parallel chords.

"Remark:" This property is an affine version of the theorem of two "perspective triangles" of a non-degenerate conic.

Steiner established the following procedure for the construction of a non-degenerate conic (see Steiner conic):


This procedure can be used for a simple construction of points on the parabola formula_73:


"Proof:" straightforward calculation.

"Remark:" Steiner's generation is also available for ellipses and hyperbolas.


The Steiner generation of a conic can be applied to the generation of a dual conic by changing the meanings of points and lines:


In order to generate elements of a dual parabola, one starts with 

The "proof" is a consequence of the "de Casteljau algorithm" for a Bezier curve of degree 2.

A parabola with equation formula_197 is uniquely determined by three points formula_198 with different x-coordinates. The usual procedure to determine the coefficients formula_199 is to insert the point coordinates into the equation. The result is a linear system of three equations, which can be solved by Gaussian elimination or Cramer's rule, for example. An alternative way uses the "inscribed angle theorem" for parabolas:

In the following, the angle of two lines will be measured by the difference of the slopes of the line with respect to the directrix of the parabola. That is, for a parabola of equation formula_200 the angle between two lines of equations formula_201 is measured by formula_202

Analogous to the inscribed angle theorem for circles one has the "Inscribed angle theorem for parabolas":

A consequence is that the equation (in formula_210) of the parabola determined by 3 points formula_211 with different -coordinates is (if two -coordinates are equal there is no parabola with directrix parallel to the -axis, which passes through the points)

The "focal length" can be determined by a suitable parameter transformation (which does not change the geometric shape of the parabola). The focal length is
Hence 

"Remark: "The advantage of this definition is, one obtains a simple parametric representation of an arbitrary parabola, even in the space, if the vectors formula_215 are vectors of the Euclidean space.

A quadratic Bézier curve is a curve formula_216 defined by three points formula_217, formula_218 and formula_219, its "control points":

This curve is an arc of a parabola (see ).

For numerical integration one replaces the graph of a function by arcs of parabolas and integrates the parabola arcs. A parabola is determined by three points. The formula for one arc is

The method is called Simpson's rule.

The following quadrics contain parabolas as plane sections:

A parabola can be used as a trisectrix, that is it allows the exact trisection of an arbitrary angle with straightedge and compass. Note that this is not in contradiction to the impossibility of an angle trisection with compass-and-straightedge constructions alone, as the use of parabolas is not allowed in the classic rules for compass-and-straightedge constructions.

To trisect formula_222 place its leg formula_223 on the "x"-axis such that the vertex formula_224 is in the coordinate system's origin. The coordinate system also contains the parabola formula_225. The unit circle with radius 1 around the origin intersects the angle's other leg formula_226 and from this point of intersection draw the perpendicular onto the "y"-axis. The parallel to "y"-axis through the midpoint of that perpendicular and the tangent on the unit circle in formula_227 intersect in formula_228. The circle around formula_228 with radius formula_230 intersects the parabola in formula_231. The perpendicular from formula_231 onto the "x"-axis intersects the unit circle in formula_233 and formula_234 is exactly one third of formula_222.

The correctness of this construction can be seen by showing that the "x"-coordinate of formula_140 is formula_237. Solving the equation system given by the circle around formula_228 and the parabola leads to the cubic equation formula_239. The triple angle formula formula_240 then shows that formula_237 is indeed a solution of that cubic equation.

This trisection goes back to René Descartes who described it in his book "La Geometria" (1637).

If one replaces the real numbers by an arbitrary field, many geometric properties of the parabola formula_72 are still valid: 1) a line intersects in at most two points. 2) At any point formula_243 the line formula_244 is the tangent... Essentially new phenomena arise, if the field has characteristic 2 (i.e., formula_245) : the tangents are all parallel.

In algebraic geometry, the parabola is generalized by the rational normal curves, which have coordinates ; the standard parabola is the case , and the case is known as the twisted cubic. A further generalization is given by the Veronese variety, when there is more than one input variable.

In the theory of quadratic forms, the parabola is the graph of the quadratic form (or other scalings), while the elliptic paraboloid is the graph of the positive-definite quadratic form (or scalings) and the hyperbolic paraboloid is the graph of the indefinite quadratic form . Generalizations to more variables yield further such objects.

The curves for other values of are traditionally referred to as the higher parabolas, and were originally treated implicitly, in the form for and both positive integers, in which form they are seen to be algebraic curves. These correspond to the explicit formula for a positive fractional power of . Negative fractional powers correspond to the implicit equation , and are traditionally referred to as higher hyperbolas. Analytically, can also be raised to an irrational power (for positive values of ); the analytic properties are analogous to when is raised to rational powers, but the resulting curve is no longer algebraic, and cannot be analyzed via algebraic geometry.

In nature, approximations of parabolas and paraboloids are found in many diverse situations. The best-known instance of the parabola in the history of physics is the trajectory of a particle or body in motion under the influence of a uniform gravitational field without air resistance (for instance, a ball flying through the air, neglecting air friction).

The parabolic trajectory of projectiles was discovered experimentally by Galileo in the early 17th century, who performed experiments with balls rolling on inclined planes. He also later proved this mathematically in his book "Dialogue Concerning Two New Sciences". For objects extended in space, such as a diver jumping from a diving board, the object itself follows a complex motion as it rotates, but the center of mass of the object nevertheless forms a parabola. As in all cases in the physical world, the trajectory is always an approximation of a parabola. The presence of air resistance, for example, always distorts the shape, although at low speeds, the shape is a good approximation of a parabola. At higher speeds, such as in ballistics, the shape is highly distorted and doesn't resemble a parabola.

Another hypothetical situation in which parabolas might arise, according to the theories of physics described in the 17th and 18th centuries by Sir Isaac Newton, is in two-body orbits; for example the path of a small planetoid or other object under the influence of the gravitation of the Sun. Parabolic orbits do not occur in nature; simple orbits most commonly resemble hyperbolas or ellipses. The parabolic orbit is the degenerate intermediate case between those two types of ideal orbit. An object following a parabolic orbit would travel at the exact escape velocity of the object it orbits; objects in elliptical or hyperbolic orbits travel at less or greater than escape velocity, respectively. Long-period comets travel close to the Sun's escape velocity while they are moving through the inner solar system, so their paths are near parabolic.

Approximations of parabolas are also found in the shape of the main cables on a simple suspension bridge. The curve of the chains of a suspension bridge is always an intermediate curve between a parabola and a catenary, but in practice the curve is generally nearer to a parabola, and in calculations the second degree parabola is used. Under the influence of a uniform load (such as a horizontal suspended deck), the otherwise catenary-shaped cable is deformed toward a parabola. Unlike an inelastic chain, a freely hanging spring of zero unstressed length takes the shape of a parabola. Suspension-bridge cables are, ideally, purely in tension, without having to carry other, e.g., bending, forces. Similarly, the structures of parabolic arches are purely in compression.

Paraboloids arise in several physical situations as well. The best-known instance is the parabolic reflector, which is a mirror or similar reflective device that concentrates light or other forms of electromagnetic radiation to a common focal point, or conversely, collimates light from a point source at the focus into a parallel beam. The principle of the parabolic reflector may have been discovered in the 3rd century BC by the geometer Archimedes, who, according to a dubious legend, constructed parabolic mirrors to defend Syracuse against the Roman fleet, by concentrating the sun's rays to set fire to the decks of the Roman ships. The principle was applied to telescopes in the 17th century. Today, paraboloid reflectors can be commonly observed throughout much of the world in microwave and satellite-dish receiving and transmitting antennas.

In parabolic microphones, a parabolic reflector is used to focus sound onto a microphone, giving it highly directional performance.

Paraboloids are also observed in the surface of a liquid confined to a container and rotated around the central axis. In this case, the centrifugal force causes the liquid to climb the walls of the container, forming a parabolic surface. This is the principle behind the liquid mirror telescope.

Aircraft used to create a weightless state for purposes of experimentation, such as NASA's "Vomit Comet", follow a vertically parabolic trajectory for brief periods in order to trace the course of an object in free fall, which produces the same effect as zero gravity for most purposes.

In the United States, vertical curves in roads are usually parabolic by design.

Click on any image to enlarge it.




</doc>
<doc id="23233" url="https://en.wikipedia.org/wiki?curid=23233" title="Problem of the criterion">
Problem of the criterion

In the field of epistemology, the problem of the criterion is an issue regarding the starting point of knowledge. This is a separate and more fundamental issue than the regress argument found in discussions on justification of knowledge.

American philosopher Roderick Chisholm in his "Theory of Knowledge" details the problem of the criterion with two sets of questions:

1. What do we know? or What is the extent of our knowledge?

2. How do we know? or What is the criterion of knowing?

An answer to either set of questions will allow us to devise a means of answering the other. Answering the former question set first is called "particularism", whereas answering the latter set first is called "methodism". A third solution is "skepticism", which proclaims that since one cannot have an answer to the first set of questions without first answering the second set, and one cannot hope to answer the second set of questions without first knowing the answers to the first set, we are, therefore, unable to answer either. This has the result of our being unable to justify any of our beliefs.

Particularist theories organize things already known and attempt to use these particulars of knowledge to find a method of how we know, thus answering the second question set. Methodist theories propose an answer to question set two and proceed to use this to establish what we, in fact, know. Classical empiricism embraces the Methodist approach.

Meno's paradox


</doc>
<doc id="23234" url="https://en.wikipedia.org/wiki?curid=23234" title="Paleozoic">
Paleozoic

The Paleozoic (or Palaeozoic) Era (; from the Greek "palaios" (παλαιός), "old" and "zoe" (ζωή), "life", meaning "ancient life") is the earliest of three geologic eras of the Phanerozoic Eon. It is the longest of the Phanerozoic eras, lasting from , and is subdivided into six geologic periods (from oldest to youngest): the Cambrian, Ordovician, Silurian, Devonian, Carboniferous, and Permian. The Paleozoic comes after the Neoproterozoic Era of the Proterozoic Eon and is followed by the Mesozoic Era.

The Paleozoic was a time of dramatic geological, climatic, and evolutionary change. The Cambrian witnessed the most rapid and widespread diversification of life in Earth's history, known as the Cambrian explosion, in which most modern phyla first appeared. Fish, arthropods, amphibians, anapsids, synapsids, euryapsids, and diapsids all evolved during the Paleozoic. Life began in the ocean but eventually transitioned onto land, and by the late Paleozoic, it was dominated by various forms of organisms. Great forests of primitive plants covered the continents, many of which formed the coal beds of Europe and eastern North America. Towards the end of the era, large, sophisticated diapsids and synapsids were dominant and the first modern plants (conifers) appeared.

The Paleozoic Era ended with the largest extinction event in the history of Earth, the Permian–Triassic extinction event. The effects of this catastrophe were so devastating that it took life on land 30 million years into the Mesozoic Era to recover. Recovery of life in the sea may have been much faster.

The Paleozoic era began and ended with supercontinents and in between were the rise of mountains along the continental margins, and flooding and draining of shallow seas between. At its start, the supercontinent Pannotia broke up. Paleoclimatic studies and evidence of glaciers indicate that central Africa was most likely in the polar regions during the early Paleozoic. During the early Paleozoic, the huge continent Gondwana () formed or was forming. By mid-Paleozoic, the collision of North America and Europe produced the Acadian-Caledonian uplifts, and a subduction plate uplifted eastern Australia. By the late Paleozoic, continental collisions formed the supercontinent of Pangaea and resulted in some of the great mountain chains, including the Appalachians, Ural Mountains, and mountains of Tasmania.

There are six periods in the Paleozoic Era: Cambrian, Ordovician, Silurian, Devonian, Carboniferous (alternatively subdivided into the Mississippian Period and the Pennsylvanian Period), and the Permian.

The Cambrian spans from 541 million years to 485 million years and is the first period of the Paleozoic era of the Phanerozoic. The Cambrian marked a boom in evolution in an event known as the Cambrian explosion in which the largest number of creatures evolved in any single period of the history of the Earth. Creatures like algae evolved, but the most ubiquitous of that period were the armored arthropods, like trilobites. Almost all marine phyla evolved in this period. During this time, the supercontinent Pannotia begins to break up, most of which later became the supercontinent Gondwana.

The Ordovician spanned from approximately 485 million years to approximately 443 million years ago. The Ordovician was a time in Earth's history in which many of the biological classes still prevalent today evolved, such as primitive fish, cephalopods, and coral. The most common forms of life, however, were trilobites, snails and shellfish. More importantly, the first arthropods went ashore to colonize the empty continent of Gondwana. By the end of the Ordovician, Gondwana was at the south pole, early North America had collided with Europe, closing the Atlantic Ocean. Glaciation of Africa resulted in a major drop in sea level, killing off all life that had established along coastal Gondwana. Glaciation may have caused the Ordovician–Silurian extinction events, in which 60% of marine invertebrates and 25% of families became extinct, and is considered the first mass extinction event and the second deadliest.

The Silurian spanned from 443 to 416 million years ago. The Silurian saw the rejuvenation of life as the Earth recovered from the previous glaciation. This period saw the mass evolution of fish, as jawless fish became more numerous, jawed fish evolved, and the first freshwater fish evolved, though arthropods, such as sea scorpions, were still apex predators. Fully terrestrial life evolved, including early arachnids, fungi, and centipedes. The evolution of vascular plants (Cooksonia) allowed plants to gain a foothold on land. These early plants are the forerunners of all plant life on land. During this time, there were four continents: Gondwana (Africa, South America, Australia, Antarctica, Siberia), Laurentia (North America), Baltica (Northern Europe), and Avalonia (Western Europe). The recent rise in sea levels allowed many new species to thrive in water.

The Devonian spanned from 416 million years to 359 million years. Also known as "The Age of the Fish", the Devonian featured a huge diversification of fish, including armored fish like "Dunkleosteus" and lobe-finned fish which eventually evolved into the first tetrapods. On land, plant groups diversified incredibly in an event known as the Devonian Explosion when plants made lignin allowing taller growth and vascular tissue: the first trees evolved, as well as seeds. This event also diversified arthropod life, by providing them new habitats. The first amphibians also evolved, and the fish were now at the top of the food chain. Near the end of the Devonian, 70% of all species became extinct in an event known as the Late Devonian extinction and was the second mass extinction event the world has seen.

The Carboniferous spanned from 359 million to 299 million years. During this time, average global temperatures were exceedingly high; the early Carboniferous averaged at about 20 degrees Celsius (but cooled to 10 °C during the Middle Carboniferous). Tropical swamps dominated the Earth, and the lignin stiffened trees grew to greater heights and number. As the bacteria and fungi capable of eating the lignin had not yet evolved, their remains were left buried, which created much of the carbon that became the coal deposits of today (hence the name "Carboniferous"). Perhaps the most important evolutionary development of the time was the evolution of amniotic eggs, which allowed amphibians to move farther inland and remain the dominant vertebrates for the duration of this period. Also, the first reptiles and synapsids evolved in the swamps. Throughout the Carboniferous, there was a cooling trend, which led to the Permo-Carboniferous glaciation or the Carboniferous Rainforest Collapse. Gondwana was glaciated as much of it was situated around the south pole.

The Permian spanned from 299 to 252 million years ago and was the last period of the Paleozoic Era. At the beginning of this period, all continents joined together to form the supercontinent Pangaea, which was encircled by one ocean called Panthalassa. The land mass was very dry during this time, with harsh seasons, as the climate of the interior of Pangaea was not regulated by large bodies of water. Diapsids and synapsids flourished in the new dry climate. Creatures such as Dimetrodon and Edaphosaurus ruled the new continent. The first conifers evolved, and dominated the terrestrial landscape. Near the end of the Permian, however, Pangaea grew drier. The interior was desert, and new species such as Scutosaurus and Gorgonopsids filled it. Eventually they disappeared, along with 95% of all life on Earth, in a cataclysm known as "The Great Dying", the third and most severe mass extinction.

Geologically, the Paleozoic started shortly after the breakup of the supercontinent Pannotia. Throughout the early Paleozoic, that landmass was broken into a substantial number of continents. Towards the end of the era, the continents gathered together into a supercontinent called Pangaea, which included most of the Earth's land area.

The earliest two periods, the Ordovician and Silurian, were warm greenhouse periods, with the highest sea levels of the Paleozoic (200 m above today's); the warm climate was interrupted only by a cool period, the Early Palaeozoic Icehouse, culminating in the Huronian glaciation, at the end of the Ordovician.

The early Cambrian climate was probably moderate at first, becoming warmer over the course of the Cambrian, as the second-greatest sustained sea level rise in the Phanerozoic got underway. However, as if to offset this trend, Gondwana moved south, so that, in Ordovician time, most of West Gondwana (Africa and South America) lay directly over the South Pole. The early Paleozoic climate was also strongly zonal, with the result that the "climate", in an abstract sense, became warmer, but the living space of most organisms of the time—the continental shelf marine environment—became steadily colder. However, Baltica (Northern Europe and Russia) and Laurentia (eastern North America and Greenland) remained in the tropical zone, while China and Australia lay in waters which were at least temperate. The early Paleozoic ended, rather abruptly, with the short, but apparently severe, late Ordovician ice age. This cold spell caused the second-greatest mass extinction of Phanerozoic time. Over time, the warmer weather moved into the Paleozoic Era.

The middle Paleozoic was a time of considerable stability. Sea levels had dropped coincident with the ice age, but slowly recovered over the course of the Silurian and Devonian. The slow merger of Baltica and Laurentia, and the northward movement of bits and pieces of Gondwana created numerous new regions of relatively warm, shallow sea floor. As plants took hold on the continental margins, oxygen levels increased and carbon dioxide dropped, although much less dramatically. The north–south temperature gradient also seems to have moderated, or metazoan life simply became hardier, or both. At any event, the far southern continental margins of Antarctica and West Gondwana became increasingly less barren. The Devonian ended with a series of turnover pulses which killed off much of middle Paleozoic vertebrate life, without noticeably reducing species diversity overall.

There are many unanswered questions about the late Paleozoic. The Mississippian (early Carboniferous Period) began with a spike in atmospheric oxygen, while carbon dioxide plummeted to new lows. This destabilized the climate and led to one, and perhaps two, ice ages during the Carboniferous. These were far more severe than the brief Late Ordovician ice age; but, this time, the effects on world biota were inconsequential. By the Cisuralian Epoch, both oxygen and carbon dioxide had recovered to more normal levels. On the other hand, the assembly of Pangaea created huge arid inland areas subject to temperature extremes. The Lopingian Epoch is associated with falling sea levels, increased carbon dioxide and general climatic deterioration, culminating in the devastation of the Permian extinction.

While macroscopic plant life appeared early in the Paleozoic Era and possibly late in the Neoproterozoic Era of the earlier eon. Plants mostly remained aquatic until sometime in the Silurian and Devonian Periods, about 420 million years ago, when they began to transition onto dry land. Terrestrial flora reached its climax in the Carboniferous, when towering lycopsid rainforests dominated the tropical belt of Euramerica. Climate change caused the Carboniferous Rainforest Collapse which fragmented this habitat, diminishing the diversity of plant life in the late Carboniferous and Permian.

A noteworthy feature of Paleozoic life is the sudden appearance of nearly all of the invertebrate animal phyla in great abundance at the beginning of the Cambrian. The first vertebrates appeared in the form of primitive fish, which greatly diversified in the Silurian and Devonian Periods. The first animals to venture onto dry land were the arthropods. Some fish had lungs, and powerful bony fins that in the late Devonian, 367.5 million years ago, allowed them to crawl onto land. The bones in their fins eventually evolved into legs and they became the first tetrapods, , and began to develop lungs. Amphibians were the dominant tetrapods until the mid-Carboniferous, when climate change greatly reduced their diversity. Later, reptiles prospered and continued to increase in number and variety by the late Permian.





</doc>
