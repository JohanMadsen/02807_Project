<doc id="15538" url="https://en.wikipedia.org/wiki?curid=15538" title="Inclusion body myositis">
Inclusion body myositis

Inclusion body myositis (IBM) [my-oh-SIGH-tis] is the most common inflammatory muscle disease in older adults. The disease is characterized by slowly progressive weakness and wasting of both distal and proximal muscles, most apparent in the finger flexors and knee extensors. There are two types of IBM: sporadic (sIBM), which is more common, and hereditary (hIBM). In sIBM, two processes appear to occur in the muscles in parallel, one autoimmune and the other degenerative. Inflammation is evident from the invasion of muscle fibers by immune cells. Degeneration is characterized by the appearance of holes, deposits of abnormal proteins, and filamentous inclusions in the muscle fibers. sIBM is a rare disease, with a prevalence ranging from 1 to 71 individuals per million.

Weakness comes on slowly (over months to years) in an asymmetric manner and progresses steadily, leading to severe weakness and wasting of arm and leg muscles. IBM is more common in men than women. Patients may become unable to perform activities of daily living and most require assistive devices within 5 to 10 years of symptom onset. sIBM is not considered a fatal disorder, but the risk of serious injury due to falls is increased. Death in IBM is sometimes related to malnutrition and respiratory failure. There is no effective treatment for the disease.

How sIBM affects individuals is quite variable as is the age of onset (which generally varies from the forties upwards). Because sIBM affects different people in different ways and at different rates, there is no "textbook case."

Eventually, sIBM results in general, progressive muscle weakness. The muscles in the thighs called the quadriceps and the muscles in the arms that control finger flexion—making a fist—are usually affected early on. Common early symptoms include frequent tripping and falling, weakness going up stairs and trouble manipulating the fingers (including difficulty with tasks such as turning doorknobs or gripping keys). Foot drop in one or both feet has been a symptom of IBM and advanced stages of polymyositis (PM).

During the course of the illness, the patient's mobility is progressively restricted as it becomes hard for him or her to bend down, reach for things, walk quickly and so on. Many patients say they have balance problems and fall easily, as the muscles cannot compensate for an off-balanced posture. Because sIBM makes the leg muscles weak and unstable, patients are very vulnerable to serious injury from tripping or falling down. Although pain has not been traditionally part of the "textbook" description, many patients report severe muscle pain, especially in the thighs.

When present, difficulty swallowing (dysphagia) is a progressive condition in those with inclusion body myositis and often leads to death from aspiration pneumonia. Dysphagia is present in 40 to 85% of IBM cases.

IBM can also result in diminished capacity for aerobic exercise. This decline is most likely a consequence of the sedentary lifestyle that is often associated with the symptoms of IBM (i.e. progressive muscle weakness, decreased mobility, and increased level of fatigue). Therefore, one focus of treatment should be the improvement of aerobic capacity.

Patients with sIBM usually eventually need to resort to a cane or a walker and in most cases, a wheelchair eventually becomes a necessity.

"The progressive course of s-IBM leads slowly to severe disability. Finger functions can become very impaired, such as for manipulating pens, keys, buttons, and zippers, pulling handles, and firmly grasping handshakes. Arising from a chair becomes difficult. Walking becomes more precarious. Sudden falls, sometimes resulting in major injury to the skull or other bones, can occur, even from walking on minimally-irregular ground or from other minor imbalances outside or in the home, due to weakness of quadriceps and gluteus muscles depriving the patient of automatic posture maintenance. A foot-drop can increase the likelihood of tripping. Dysphagia can occur, usually caused by upper esophageal constriction that often can be symptomatically improved, for several months to years, by bougie dilation per a GI or ENT physician. Respiratory muscle weakness can sometimes eventuate."

The cause of IBM is unknown. IBM likely results from the interaction of a number of genetic and environmental factors.

There are two major theories about how sIBM is caused. One hypothesis suggests that the inflammation-immune reaction, caused by an unknown trigger – likely an undiscovered virus or an autoimmune disorder– is the primary cause of sIBM and that the degeneration of muscle fibers and protein abnormalities are secondary features. Despite the arguments "in favor of an adaptive immune response in sIBM, a purely autoimmune hypothesis for sIBM is untenable because of the disease's resistance to most immunotherapy."

The second school of thought advocates the theory that sIBM is a degenerative disorder related to aging of the muscle fibers and that abnormal, potentially pathogenic protein accumulations in myofibrils play a key causative role in sIBM (apparently before the immune system comes into play). This hypothesis emphasizes the abnormal intracellular accumulation of many proteins, protein aggregation and misfolding, proteosome inhibition, and endoplasmic reticulum (ER) stress.

One review discusses the "limitations in the beta-amyloid-mediated theory of IBM myofiber injury."

Dalakas (2006) suggested that a chain of events causes IBM—some sort of virus, likely a retrovirus, triggers the cloning of T cells. These T cells appear to be driven by specific antigens to invade muscle fibers. In people with sIBM, the muscle cells display “flags” telling the immune system that they are infected or damaged (the muscles ubiquitously express MHC class I antigens) and this immune process leads to the death of muscle cells. The chronic stimulation of these antigens also causes stress inside the muscle cell in the endoplasmic reticulum (ER) and this ER stress may be enough to cause a self-sustaining T cell response (even after a virus has dissipated). In addition, this ER stress may cause the misfolding of protein. The ER is in charge of processing and folding molecules carrying antigens. In IBM, muscle fibers are overloaded with these major histocompatibility complex (MHC) molecules that carry the antigen protein pieces, leading to more ER stress and more protein misfolding.

A self-sustaining T cell response would make sIBM a type of autoimmune disorder. When studied carefully, it has not been impossible to detect an ongoing viral infection in the muscles. One theory is that a chronic viral infection might be the initial triggering factor setting IBM in motion. There have been a handful of IBM cases—approximately 15—that have shown clear evidence of a virus called HTLV-1. The HTLV-1 virus can cause leukemia, but in most cases lies dormant and most people end up being lifelong carriers of the virus. One review says that the best evidence points towards a connection with some type of retrovirus and that a retroviral infection combined with immune recognition of the retrovirus is enough to trigger the inflammation process.


sIBM is not inherited and is not passed on to the children of IBM patients. There are genetic features that do not directly cause IBM but that appear to predispose a person to getting IBM — having this particular combination of genes increases one's susceptibility to getting IBM. Some 67% of IBM patients have a particular combination of human leukocyte antigen genes in a section of the 8.1 ancestral haplotype in the center of the MHC class II region. sIBM is not passed on from generation to generation, although the susceptibility region of genes may be.

There are also several rare forms of hereditary inclusion body myopathy that are linked to specific genetic defects and that are passed on from generation to generation. Since these forms do not show features of muscle inflammation, they are classified as myopathies rather than forms of myositis. Because they do not display inflammation as a primary symptom, they may in fact be similar, but different diseases to sporadic inclusion body myositis. There are several different types, each inherited in different ways. See hereditary inclusion body myopathy.

A 2007 review concluded there is no indication that the genes responsible for the familial or hereditary conditions are involved in sIBM.

Elevated creatine kinase (CK) levels in the blood (at most ~10 times normal) are typical in sIBM but affected individuals can also present with normal CK levels. Electromyography (EMG) studies usually display abnormalities. Muscle biopsy may display several common findings including; inflammatory cells invading muscle cells, vacuolar degeneration, inclusions or plaques of abnormal proteins. sIBM is a challenge to the pathologist and even with a biopsy, diagnosis can be ambiguous.

A diagnosis of inclusion body myositis was historically dependent on muscle biopsy results. Antibodies to cytoplasmic 5'-nucleotidase (cN1A; NT5C1A) have been strongly associated with the condition. In the clinical context of a classic history and positive antibodies, a muscle biopsy might be unnecessary.

IBM is often initially misdiagnosed as polymyositis. A course of prednisone is typically completed with no improvement and eventually sIBM is confirmed. sIBM weakness comes on over months or years and progresses steadily, whereas polymyositis has an onset of weeks or months. Other forms of muscular dystrophy (e.g. limb girdle) must be considered as well.


There is no standard course of treatment to slow or stop the progression of the disease. sIBM patients do not reliably respond to the anti-inflammatory, immunosuppressant, or immunomodulatory medications. Management is symptomatic. Prevention of falls is an important consideration. Specialized exercise therapy may supplement treatment to enhance quality of life. Physical therapy is recommended to teach the patient a home exercise program, to teach how to compensate during mobility-gait training with an assistive device, transfers and bed mobility.

When sIBM was originally described, the major feature noted was muscle inflammation. Two other disorders were also known to display muscle inflammation, and sIBM was classified along with them. They are dermatomyositis (DM) and polymyositis (PM) and all three illnesses were called idiopathic (of unknown origin) myositis or inflammatory myopathies.

It appears that sIBM and polymyositis share some features, especially the initial sequence of immune system activation, however, polmyositis comes on over weeks or months, does not display the subsequent muscle degeneration and protein abnormalities as seen in IBM, and as well, polymyositis tends to respond well to treatments, IBM does not. IBM is often confused with (misdiagnosed as) polymyositis. Polymyositis that does not respond to treatment is likely IBM.

Dermatomyositis shares a number of similar physical symptoms and histopathological traits as polymyositis, but exhibits a skin rash not seen in polymyositis or sIBM. It may have different root causes unrelated to either polymyositis or sIBM.



</doc>
<doc id="15539" url="https://en.wikipedia.org/wiki?curid=15539" title="Ion implantation">
Ion implantation

Ion implantation is low-temperature process by which ions of one element are accelerated into a solid target, thereby changing the physical, chemical, or electrical properties of the target. Ion implantation is used in semiconductor device fabrication and in metal finishing, as well as in materials science research. The ions can alter the elemental composition of the target (if the ions differ in composition from the target) if they stop and remain in the target. Ion implantation cause also chemical and physical changes when the ions impinge on the target at high energy. The crystal structure of the target can be damaged or even destroyed by the energetic collision cascades, and ions of sufficiently high energy (10s of MeV) can cause nuclear transmutation.

Ion implantation equipment typically consists of an ion source, where ions of the desired element are produced, an accelerator, where the ions are electrostatically accelerated to a high energy, and a target chamber, where the ions impinge on a target, which is the material to be implanted. Thus ion implantation is a special case of particle radiation. Each ion is typically a single atom or molecule, and thus the actual amount of material implanted in the target is the integral over time of the ion current. This amount is called the dose. The currents supplied by implanters are typically small (microamperes), and thus the dose which can be implanted in a reasonable amount of time is small. Therefore, ion implantation finds application in cases where the amount of chemical change required is small.

Typical ion energies are in the range of 10 to 500 keV (1,600 to 80,000 aJ). Energies in the range 1 to 10 keV (160 to 1,600 aJ) can be used, but result in a penetration of only a few nanometers or less. Energies lower than this result in very little damage to the target, and fall under the designation ion beam deposition. Higher energies can also be used: accelerators capable of 5 MeV (800,000 aJ) are common. However, there is often great structural damage to the target, and because the depth distribution is broad (Bragg peak), the net composition change at any point in the target will be small.

The energy of the ions, as well as the ion species and the composition of the target determine the depth of penetration of the ions in the solid: A monoenergetic ion beam will generally have a broad depth distribution. The average penetration depth is called the range of the ions. Under typical circumstances ion ranges will be between 10 nanometers and 1 micrometer. Thus, ion implantation is especially useful in cases where the chemical or structural change is desired to be near the surface of the target. Ions gradually lose their energy as they travel through the solid, both from occasional collisions with target atoms (which cause abrupt energy transfers) and from a mild drag from overlap of electron orbitals, which is a continuous process. The loss of ion energy in the target is called stopping and can be simulated with the binary collision approximation method.

Accelerator systems for ion implantation are generally classified into medium current (ion beam currents between 10 μA and ~2 mA), high current (ion beam currents up to ~30 mA), high energy (ion energies above 200 keV and up to 10 MeV), and very high dose (efficient implant of dose greater than 10ions/cm).

All varieties of ion implantation beamline designs contain certain general groups of functional components (see image). The first major segment of an ion beamline includes a device known as an ion source to generate the ion species. The source is closely coupled to biased electrodes for extraction of the ions into the beamline and most often to some means of selecting a particular ion species for transport into the main accelerator section. The "mass" selection is often accompanied by passage of the extracted ion beam through a magnetic field region with an exit path restricted by blocking apertures, or "slits", that allow only ions with a specific value of the product of mass and velocity/charge to continue down the beamline. If the target surface is larger than the ion beam diameter and a uniform distribution of implanted dose is desired over the target surface, then some combination of beam scanning and wafer motion is used. Finally, the implanted surface is coupled with some method for collecting the accumulated charge of the implanted ions so that the delivered dose can be measured in a continuous fashion and the implant process stopped at the desired dose level.

Semiconductor doping with boron, phosphorus, or arsenic is a common application of ion implantation. When implanted in a semiconductor, each dopant atom can create a charge carrier in the semiconductor after annealing. A hole can be created for a p-type dopant, and an electron for an n-type dopant. This modifies the conductivity of the semiconductor in its vicinity. The technique is used, for example, for adjusting the threshold of a MOSFET.

Ion implantation was developed as a method of producing the p-n junction of photovoltaic devices in the late 1970s and early 1980s, along with the use of pulsed-electron beam for rapid annealing, although it has not to date been used for commercial production.

One prominent method for preparing silicon on insulator (SOI) substrates from conventional silicon substrates is the "SIMOX" (separation by implantation of oxygen) process, wherein a buried high dose oxygen implant is converted to silicon oxide by a high temperature annealing process.

Mesotaxy is the term for the growth of a crystallographically matching phase underneath the surface of the host crystal (compare to epitaxy, which is the growth of the matching phase on the surface of a substrate). In this process, ions are implanted at a high enough energy and dose into a material to create a layer of a second phase, and the temperature is controlled so that the crystal structure of the target is not destroyed. The crystal orientation of the layer can be engineered to match that of the target, even though the exact crystal structure and lattice constant may be very different. For example, after the implantation of nickel ions into a silicon wafer, a layer of nickel silicide can be grown in which the crystal orientation of the silicide matches that of the silicon.

Nitrogen or other ions can be implanted into a tool steel target (drill bits, for example). The structural change caused by the implantation produces a surface compression in the steel, which prevents crack propagation and thus makes the material more resistant to fracture. The chemical change can also make the tool more resistant to corrosion.

In some applications, for example prosthetic devices such as artificial joints, it is desired to have surfaces very resistant to both chemical corrosion and wear due to friction. Ion implantation is used in such cases to engineer the surfaces of such devices for more reliable performance. As in the case of tool steels, the surface modification caused by ion implantation includes both a surface compression which prevents crack propagation and an alloying of the surface to make it more chemically resistant to corrosion.

Ion implantation can be used to achieve ion beam mixing, i.e. mixing up atoms of different elements at an interface. This may be useful for achieving graded interfaces or strengthening adhesion between layers of immiscible materials.

Ion implantation may be used to induce nano-dimensional particles in oxides such as sapphire and silica. The particles may be formed as a result of precipitation of the ion implanted species, they may be formed as a result of the production of an mixed oxide species that contains both the ion-implanted element and the oxide substrate, and they may be formed as a result of a reduction of the substrate, first reported by Hunt and Hampikian. Typical ion beam energies used to produce nanoparticles range from 50 to 150 keV, with ion fluences that range from 10 to 10 ions/cm. The table below summarizes some of the work that has been done in this field for a sapphire substrate. A wide variety of nanoparticles can be formed, with size ranges from 1 nm on up to 20 nm and with compositions that can contain the implanted species, combinations of the implanted ion and substrate, or that are comprised solely from the cation associated with the substrate.

Composite materials based on dielectrics such as sapphire that contain dispersed metal nanoparticles are promising materials for optoelectronics and nonlinear optics.

Each individual ion produces many point defects in the target crystal on impact such as vacancies and interstitials. Vacancies are crystal lattice points unoccupied by an atom: in this case the ion collides with a target atom, resulting in transfer of a significant amount of energy to the target atom such that it leaves its crystal site. This target atom then itself becomes a projectile in the solid, and can cause successive collision events.
Interstitials result when such atoms (or the original ion itself) come to rest in the solid, but find no vacant space in the lattice to reside. These point defects can migrate and cluster with each other, resulting in dislocation loops and other defects.

Because ion implantation causes damage to the crystal structure of the target which is often unwanted, ion implantation processing is often followed by a thermal annealing. This can be referred to as damage recovery.

The amount of crystallographic damage can be enough to completely amorphize the surface of the target: i.e. it can become an amorphous solid (such a solid produced from a melt is called a glass). In some cases, complete amorphization of a target is preferable to a highly defective crystal: An amorphized film can be regrown at a lower temperature than required to anneal a highly damaged crystal. Amorphisation of the substrate can occur as a result of the beam damage. For example, yttrium ion implantation into sapphire at an ion beam energy of 150 keV to a fluence of 5*10 Y/cm produces an amorphous glassy layer approximately 110 nm in thickness, measured from the outer surface. [Hunt, 1999]

Some of the collision events result in atoms being ejected (sputtered) from the surface, and thus ion implantation will slowly etch away a surface. The effect is only appreciable for very large doses.

If there is a crystallographic structure to the target, and especially in semiconductor substrates where the crystal structure is more open, particular crystallographic directions offer much lower stopping than other directions. The result is that the range of an ion can be much longer if the ion travels exactly along a particular direction, for example the <110> direction in silicon and other diamond cubic materials. This effect is called "ion channelling", and, like all the channelling effects, is highly nonlinear, with small variations from perfect orientation resulting in extreme differences in implantation depth. For this reason, most implantation is carried out a few degrees off-axis, where tiny alignment errors will have more predictable effects.

Ion channelling can be used directly in Rutherford backscattering and related techniques as an analytical method to determine the amount and depth profile of damage in crystalline thin film materials.

In fabricating wafers, toxic materials such as arsine and phosphine are often used in the ion implanter process. Other common carcinogenic, corrosive, flammable, or toxic elements include antimony, arsenic, phosphorus, and boron. Semiconductor fabrication facilities are highly automated, but residue of hazardous elements in machines can be encountered during servicing and in vacuum pump hardware.

High voltage power supplies used in ion accelerators necessary for ion implantation can pose a risk of electrocution. In addition, high-energy atomic collisions can generate X-rays and, in some cases, other ionizing radiation and radionuclides. In addition to high voltage, particle accelerators such as radio frequency linear particle accelerators and laser wakefield plasma accelerators other hazards.



</doc>
<doc id="15570" url="https://en.wikipedia.org/wiki?curid=15570" title="John Ford (disambiguation)">
John Ford (disambiguation)

John Ford (1894–1973) was an American film director who won four Academy Awards.

John or Johnny Ford may also refer to:








</doc>
<doc id="15571" url="https://en.wikipedia.org/wiki?curid=15571" title="John Woo">
John Woo

John Woo SBS (Wu Yu-seng; Ng Yu-sum (Cantonese); born May 1, 1946) is a Chinese-born Hong Kong film director, writer, and producer. He is the owner of Lion Rock Productions. He is considered a major influence on the action genre, known for his highly chaotic action sequences, Mexican standoffs, and frequent use of slow motion. 

Woo has directed several notable Hong Kong action films, among them, "A Better Tomorrow" (1986), "The Killer" (1989), "Hard Boiled" (1992), and "Red Cliff" (2008/2009). 

Woo's Hollywood films include the action films "Hard Target" (1993) and "Broken Arrow" (1996), the sci-fi action thriller "Face/Off" (1997) and the action spy film "" (2000). He created the comic series "Seven Brothers", published by Virgin Comics. He cites his three favorite films as David Lean's "Lawrence of Arabia", Akira Kurosawa's "Seven Samurai" and Jean-Pierre Melville's "Le Samouraï".

Woo was born Wu Yu-seng (Ng Yu-sum in Cantonese) in Guangzhou, China, amidst the chaos of the Chinese Civil War at the end of October 1946. Due to school age restrictions, his mother changed his birth date to 22 September 1948, which is what remains on his passport. The Woo family, Christians faced with persecution during Mao Zedong's early anti-bourgeois purges after the communist revolution in China, fled to Hong Kong when he was five.

Impoverished, the Woo family lived in the slums at Shek Kip Mei. His father was a teacher, though rendered unable to work by tuberculosis, and his mother was a manual laborer on construction sites. The family was rendered homeless by the big Shek Kip Mei fire of 1953. Charitable donations from disaster relief efforts enabled the family to relocate; however, violent crime had by then become commonplace in Hong Kong housing projects. At age three he was diagnosed with a serious medical condition. Following surgery on his spine, he was unable to walk correctly until eight years old, and as a result his right leg is shorter than his left leg. 

His Christian upbringing shows influences in his films. As a young boy, Woo had wanted to be a Christian minister. He later found a passion for movies influenced by the French New Wave especially Jean-Pierre Melville. Woo has said he was shy and had difficulty speaking, but found making movies a way to explore his feelings and thinking and would "use movies as a language". 

The local cinema would prove a haven of retreat. Woo found respite in musical films, such as "The Wizard of Oz" and in American Westerns. He has stated the final scene of "Butch Cassidy and the Sundance Kid" made a particular impression on him in his youth: the device of two comrades, each of whom fire pistols from each hand, is a recurrent spectacle later found in his own work.

In 1969, Woo was hired as a script supervisor at Cathay Studios. In 1971, he became an assistant director at Shaw Studios. His directorial debut in 1974 was the feature film "The Young Dragons" (鐵漢柔情, "Tiě hàn róu qíng"). 

In the Kung fu action genre, it was choreographed by Jackie Chan and featured dynamic camera-work and elaborate action scenes. The film was picked up by Golden Harvest Studio where he went on to direct more martial arts films. He later had success as a comedy director with "Money Crazy" (發錢寒, "Fā qián hàn") (1977), starring Hong Kong comedian Ricky Hui.

By the mid-1980s, Woo was experiencing occupational burnout. Several of his films were commercial disappointments, and he felt a distinct lack of creative control. It was during this period of self-imposed exile that director/producer Tsui Hark provided the funding for Woo to film a longtime pet project, "A Better Tomorrow" (1986).

The story of two brothers—one a law enforcement officer, the other a criminal—the film was a financial blockbuster. "A Better Tomorrow" became a defining achievement in Hong Kong action cinema for its combination of emotional drama, slow-motion gunplay, and gritty atmospherics. Its signature visual device of two-handed, two-gunned shootouts within confined quarters—often referred to as "gun fu" was novel, and its diametrical inversion of the "good-guys-bad guys" formula in its characterization would influence later American films.

Woo would make several more Heroic Bloodshed films in the late 1980s and early 1990s, nearly all starring Chow Yun-Fat. These violent gangster thrillers typically focus on men bound by honor and loyalty, at odds with contemporary values of impermanence and expediency. The protagonists of these films, therefore, may be said to present a common lineage with the Chinese literary tradition of loyalty among generals depicted in classics such as "Romance of the Three Kingdoms".

Woo gained international recognition with the release of "The Killer", which became the most successful Hong Kong film in American release since Bruce Lee's "Enter the Dragon" (1973) and garnered Woo an American cult following. "Bullet in the Head" followed a year later failed to find an audience that accepted its political undertones, and failed to recoup its massive budget.

His last Hong Kong film before emigrating to the United States was "Hard Boiled" (1992), a police thriller that served as the antithesis of his previous glorification of gangsters. Most notable of its numerous action scenes is a 30-minute climax set within a hospital. One particular long take follows two characters for exactly 2 minutes and 42 seconds as they fight their way between hospital floors. On the Criterion DVD and laserdisc, this chapter is referenced as "2 minutes, 42 seconds." The film was considerably darker than most of Woo's previous films, depicting a police force nearly helpless to stop the influx of gangsters in the city, and the senseless slaughter of innocents. As a result, it did not match the success of his other films.

"John Woo: Interviews" () includes a new 36-page interview with Woo by editor Robert K. Elder, which documents the years 1968 to 1990, from Woo's early career in working on comedies and kung fu films (in which he gave Jackie Chan in one of his first major film roles), to his gunpowder morality plays in Hong Kong.

An émigré in 1993, the director experienced difficulty in cultural adjustment while contracted with Universal Studios to direct Jean-Claude Van Damme in "Hard Target". As characteristics of other foreign national film directors confronted the Hollywood environment, Woo was unaccustomed to pervasive management concerns, such as limitations on violence and completion schedules. When initial cuts failed to yield an "R" rated film, the studio assumed control of the project and edited footage to produce a cut "suitable for American audiences". A "rough cut" of the film, supposedly the original unrated version, is still circulated among his admirers.

A three-year hiatus saw Woo next direct John Travolta and Christian Slater in "Broken Arrow." A frenetic chase-themed film, the director once again found himself hampered by studio management and editorial concerns. Despite a larger budget than his previous "Hard Target," the final feature lacked the trademark Woo style. Public reception saw modest financial success.

Reluctant to pursue projects which would necessarily entail front-office controls, the director cautiously rejected the script for "Face/Off" several times until it was rewritten to suit him. (The futuristic setting was changed to a contemporary one.) Paramount Pictures also offered the director significantly more freedom to exercise his speciality: emotional characterisation and elaborate action. A complex story of adversaries—each of whom surgically alters their identity—law enforcement agent John Travolta and terrorist Nicolas Cage play a cat-and-mouse game, trapped in each other's outward appearance. "Face/Off" opened in 1997 to critical acclaim and strong attendance. Grosses in the United States exceeded $100 million. "Face/Off" was also nominated for an Academy Award in the category Sound Effects Editing (Mark Stoeckinger) at the 70th Academy Awards.

In 2003, Mr. Woo directed a television pilot entitled "The Robinsons: Lost in Space" for The WB Television Network, based on the 1960s television series "Lost in Space". The pilot was not purchased, although bootleg copies have been made available by fans.

John Woo has made three additional films in Hollywood: "", "Windtalkers" and "Paycheck". "Mission: Impossible 2" was the highest-grossing film in America in 2000, but received mixed reviews. "Windtalkers" and "Paycheck" fared poorly at the box office and were summarily dismissed by critics. Woo directed and produced a videogame called "Stranglehold" for games consoles and PC. It is a sequel to his 1992 film, "Hard Boiled". He also produced the 2007 anime movie, "", the sequel to Shinji Aramaki's 2004 film "Appleseed".

In 2008, Woo returned to Asian cinema with the completion of the two-part epic war film "Red Cliff", based on a historical battle from "Records of the Three Kingdoms". Produced on a grand scale, it is his first film in China since he emigrated from Hong Kong to the United States in 1993. Part 1 of the film was released throughout Asia in July, 2008, to generally favourable reviews and strong attendance. Part 2 was released in China in January, 2009.

John Woo was presented with a Golden Lion award for lifetime achievement at the Venice Film Festival in 2010.

He followed "Red Cliff" with another two-part film, "The Crossing", in 2014 and 2015. Featuring an all-star cast, the four-hour epic tells the parallel stories of several characters who all ultimately find themselves passengers on the doomed Taiping steamer, which sank in 1949 en route from mainland China to Taiwan and has been described as "China's "Titanic"".

Following the box-office disappointment of "The Crossing", Woo and producer Terence Chang disbanded Lion Rock Productions.

A CGI Mighty Mouse film was announced in 2003 although, , nothing has yet been produced. There have been rumours that Woo will direct a film version of the videogame "Metroid", however the rights he optioned have since expired.

Woo's next projects are "The Divide", a western concerning the friendship between two workers, one Chinese, the other Irish, on the transcontinental rail-road, while "The Devil's Soldier" is a biopic on Frederick Townsend Ward, an American brought to China in the mid 19th century by the Emperor to suppress rebellion. "Rendezvous in Black" will be an adaptation of the drama/thriller novel of the same name, and "Psi-Ops" is a science fiction thriller about a telepathic agent, and a remake of "Blind Spot".

In May 2008, Woo announced in Cannes that his next movie would be "1949", an epic love story set between the end of World War II and Chinese Civil War to the founding of the People's Republic of China, the shooting of which would take place in China and Taiwan. Its production was due to begin by the end of 2008, with a theatrical release planned in December 2009. However, in early April 2009, the film was cancelled due to script right issues. Reports indicated that Woo might be working on another World War II film, this time about the American Volunteer Group, or the Flying Tigers. The movie was tentatively titled "Flying Tiger Heroes" and Woo is reported as saying it will feature "The most spectacular aerial battle scenes ever seen in Chinese cinema." It was not clear whether Woo would not be directing the earlier war film, or whether it was put on the back burner. Woo has stated that Flying Tiger Heroes would be an "extremely important production" and will "emphasise US-Chinese friendship and the contributions of the Flying Tigers and the Yunnan people during the war of resistance." Woo has announced he will be using IMAX cameras to film the "Flying Tigers" project. “It has always been a dream of mine to explore shooting with IMAX cameras and to work in the IMAX format, and the strong visual element of this film is incredibly well-suited to the tastes of cinemagoers today [...] Using IMAX for Flying Tigers would create a new experience for the audience, and I think it would be another breakthrough for Chinese movies.”

After the death of Japanese actor Ken Takakura in 2014, Woo announced his next film "Manhunt", a film based on the novel by Juko Nishimura. The novel had previously been adapted by Junya Satō in 1976 as "Kimi yo Fundo no Kawa o Watare", starring Takakura. Andy Lau, Takeshi Kaneshiro and Shu Qi were in discussion to star in the film. In March 2016, it was confirmed that Zhang Hanyu, Masaharu Fukuyama, and Qi Wei would be starring in the film. Ha Ji-won was additionally confirmed as being attached to the project. Lee Byung-hun was slated to join, but had to drop out due to scheduling conflicts. Taking place and being shot in Japan, the film will have Chinese, Korean, and English dialogue. It was set for a tentative 2017 release.

Woo married Annie Woo Ngau Chun-lung in 1976, with whom he has three children. He is a Christian and told the BBC in a September 2014 interview that he has the utmost admiration for Jesus, whom he calls "a great philosopher".









</doc>
<doc id="15573" url="https://en.wikipedia.org/wiki?curid=15573" title="Japan">
Japan

Japan (; "Nippon" or "Nihon" ; formally "" or "Nihon-koku", lit. "State of Japan") is a sovereign island country in East Asia. Located in the Pacific Ocean, it lies off the eastern coast of the Asian mainland and stretches from the Sea of Okhotsk in the north to the East China Sea and China in the southwest.

The kanji that make up Japan's name mean "sun origin", and it is often called the "Land of the Rising Sun". Japan is a stratovolcanic archipelago consisting of about 6,852 islands. The four largest are Honshu, Hokkaido, Kyushu, and Shikoku, which make up about ninety-seven percent of Japan's land area and often are referred to as home islands. The country is divided into 47 prefectures in eight regions, with Hokkaido being the northernmost prefecture and Okinawa being the southernmost one. The population of 127 million is the world's tenth largest. Japanese people make up 98.5% of Japan's total population. About 9.1 million people live in Tokyo, the capital of Japan.

Archaeological research indicates that Japan was inhabited as early as the Upper Paleolithic period. The first written mention of Japan is in Chinese history texts from the 1st century AD. Influence from other regions, mainly China, followed by periods of isolation, particularly from Western Europe, has characterized Japan's history.

From the 12th century until 1868, Japan was ruled by successive feudal military "shōguns" who ruled in the name of the Emperor. Japan entered into a long period of isolation in the early 17th century, which was ended in 1853 when a United States fleet pressured Japan to open to the West. After nearly two decades of internal conflict and insurrection, the Imperial Court regained its political power in 1868 through the help of several clans from Chōshū and Satsuma—and the Empire of Japan was established. In the late 19th and early 20th centuries, victories in the First Sino-Japanese War, the Russo-Japanese War and World War I allowed Japan to expand its empire during a period of increasing militarism. The Second Sino-Japanese War of 1937 expanded into part of World War II in 1941, which came to an end in 1945 following the atomic bombings of Hiroshima and Nagasaki and the Japanese surrender. Since adopting its revised constitution on May 3, 1947, during the occupation by the SCAP, Japan has maintained a unitary parliamentary constitutional monarchy with an Emperor and an elected legislature called the National Diet.

Japan is a member of the ASEAN Plus mechanism, UN, the OECD, the G7, the G8 and the G20—and is considered a great power. The country has the world's third-largest economy by nominal GDP and the world's fourth-largest economy by purchasing power parity. It is also the world's fourth-largest exporter and fourth-largest importer.

The country benefits from a highly skilled workforce and is among the most highly educated countries in the world, with one of the highest percentages of its citizens holding a tertiary education degree. Although Japan has officially renounced its right to declare war, it maintains a modern military with the world's eighth-largest military budget, used for self-defense and peacekeeping roles. Japan is a highly developed country with a very high standard of living and Human Development Index. Its population enjoys the highest life expectancy and the third lowest infant mortality rate in the world. Japan is renowned for its historical and extensive cinema, influential music industry, rich cuisine and its major contributions to science and modern-day technology.

The Japanese word for Japan is , which is pronounced "Nihon" or "Nippon" and literally means "the origin of the sun". The character means "sun" or "day"; means "base" or "origin". The compound therefore means "origin of the sun" and is the source of the popular Western epithet "Land of the Rising Sun".

The earliest record of the name "Nihon" appears in the Chinese historical records of the Tang dynasty, the "Old Book of Tang". At the end of the seventh century, a delegation from Japan requested that "Nihon" be used as the name of their country. This name may have its origin in a letter sent in 607 and recorded in the official history of the Sui dynasty. Prince Shōtoku, the Regent of Japan, sent a mission to China with a letter in which he called himself "the Emperor of the Land where the Sun rises" (日出處天子). The message said: "Here, I, the emperor of the country where the sun rises, send a letter to the emperor of the country where the sun sets. How are you[?]”.
Prior to the adoption of "Nihon", other terms such as and were used. The term is a homophone of "Wo" 倭 (pronounced "Wa" by the Japanese), which has been used by the Chinese as a designation for the Japanese as early as the third century Three Kingdoms period. Another form "Wei" (委) was used for an early state in Japan called Nakoku during the Han dynasty. However, the Japanese disliked some connotation of Wa 倭 (which has been associated in China with concepts like "dwarf" or "pygmy"), and it was therefore replaced with the substitute character , meaning "togetherness, harmony".

The English word Japan possibly derives from the historical Chinese pronunciation of 日本. The Old Mandarin or possibly early Wu Chinese pronunciation of Japan was recorded by Marco Polo as "Cipangu". In modern Shanghainese, a Wu dialect, the pronunciation of characters Japan is "Zeppen" . The old Malay word for Japan, "Japun" or "Japang", was borrowed from a southern coastal Chinese dialect, probably Fukienese or Ningpo—and this Malay word was encountered by Portuguese traders in Southeast Asia in the 16th century. These Early Portuguese traders then brought the word to Europe. The first record of this name in English is in a book published in 1577 and spelled "Giapan", in a translation of a 1565 letter written by a Portuguese Jesuit Luís Fróis.

From the Meiji Restoration until the end of World War II, the full title of Japan was , meaning "the Empire of Great Japan". Today, the name is used as a formal modern-day equivalent with the meaning of "the State of Japan". Countries like Japan whose long form does not contain a descriptive designation are generally given a name appended by the character , meaning "country", "nation" or "state".

A Paleolithic culture around 30,000 BC constitutes the first known habitation of the Japanese archipelago. This was followed from around 14,000 BC (the start of the Jōmon period) by a Mesolithic to Neolithic semi-sedentary hunter-gatherer culture characterized by pit dwelling and rudimentary agriculture, including by ancestors of contemporary Ainu people and Yamato people. Decorated clay vessels from this period are some of the oldest surviving examples of pottery in the world. Around 300 BC, the Yayoi people began to enter the Japanese islands, intermingling with the Jōmon. The Yayoi period, starting around 500 BC, saw the introduction of practices like wet-rice farming, a new style of pottery and metallurgy, introduced from China and Korea.

Japan first appears in written history in the Chinese "Book of Han". According to the "Records of the Three Kingdoms", the most powerful kingdom on the archipelago during the third century was called Yamataikoku. Buddhism was introduced to Japan from Baekje, Korea and was promoted by Prince Shōtoku, but the subsequent development of Japanese Buddhism was primarily influenced by China. Despite early resistance, Buddhism was promoted by the ruling class and gained widespread acceptance beginning in the Asuka period (592–710).

The Nara period (710–784) marked an emergence of the centralized Japanese state centered on the Imperial Court in Heijō-kyō (modern Nara). The Nara period is characterized by the appearance of a nascent literature as well as the development of Buddhist-inspired art and architecture. The smallpox epidemic of 735–737 is believed to have killed as much as one-third of Japan's population. In 784, Emperor Kanmu moved the capital from Nara to Nagaoka-kyō, then to Heian-kyō (modern Kyoto) in 794.

This marked the beginning of the Heian period (794–1185), during which a distinctly indigenous Japanese culture emerged, noted for its art, poetry and prose. Murasaki Shikibu's "The Tale of Genji" and the lyrics of Japan's national anthem "Kimigayo" were written during this time. 

Buddhism began to spread during the Heian era chiefly through two major sects, Tendai by Saichō and Shingon by Kūkai. Pure Land Buddhism (Jōdo-shū, Jōdo Shinshū) became greatly popular in the latter half of the 11th century.

Japan's feudal era was characterized by the emergence and dominance of a ruling class of warriors, the samurai. In 1185, following the defeat of the Taira clan in the Genpei War, sung in the epic Tale of Heike, samurai Minamoto no Yoritomo was appointed "shōgun" by Emperor Go-Toba, and Yoritomo established a base of power in Kamakura. After his death, the Hōjō clan came to power as regents for the "shōguns". The Zen school of Buddhism was introduced from China in the Kamakura period (1185–1333) and became popular among the samurai class. The Kamakura shogunate repelled Mongol invasions in 1274 and 1281, but was eventually overthrown by Emperor Go-Daigo. Emperor Go-Daigo was himself defeated by Ashikaga Takauji in 1336.
Ashikaga Takauji established the shogunate in Muromachi, Kyoto. This was the start of the Muromachi period (1336–1573). The Ashikaga shogunate achieved glory at the age of Ashikaga Yoshimitsu, and the culture based on Zen Buddhism (the art of "Miyabi") prospered. This evolved to Higashiyama Culture, and prospered until the 16th century. On the other hand, the succeeding Ashikaga shogunate failed to control the feudal warlords ("daimyōs") and a civil war (the Ōnin War) began in 1467, opening the century-long Sengoku period ("Warring States").

During the 16th century, traders and Jesuit missionaries from Portugal reached Japan for the first time, initiating direct commercial and cultural exchange between Japan and the West. This allowed Oda Nobunaga to obtain European technology and firearms, which he used to conquer many other "daimyōs". His consolidation of power began what was known as the Azuchi–Momoyama period (1573–1603). After Nobunaga was assassinated in 1582 by Akechi Mitsuhide, his successor Toyotomi Hideyoshi unified the nation in 1590 and launched two unsuccessful invasions of Korea in 1592 and 1597.

Tokugawa Ieyasu served as regent for Hideyoshi's son and used his position to gain political and military support. When open war broke out, Ieyasu defeated rival clans in the Battle of Sekigahara in 1600. Tokugawa Ieyasu was appointed "shōgun" by Emperor Go-Yōzei in 1603 and established the Tokugawa shogunate in Edo (modern Tokyo). The shogunate enacted measures including "buke shohatto", as a code of conduct to control the autonomous "daimyōs"; and in 1639 the isolationist "sakoku" ("closed country") policy that spanned the two and a half centuries of tenuous political unity known as the Edo period (1603–1868). The study of Western sciences, known as "rangaku", continued through contact with the Dutch enclave at Dejima in Nagasaki. The Edo period also gave rise to "kokugaku" ("national studies"), the study of Japan by the Japanese.

On March 31, 1854, Commodore Matthew Perry and the "Black Ships" of the United States Navy forced the opening of Japan to the outside world with the Convention of Kanagawa. Subsequent similar treaties with Western countries in the Bakumatsu period brought economic and political crises. The resignation of the "shōgun" led to the Boshin War and the establishment of a centralized state nominally unified under the Emperor (the Meiji Restoration).

Plunging itself through an active process of Westernization during the Meiji Restoration in 1868, Japan adopted Western political, judicial and military institutions and Western cultural influences integrated with its traditional culture for modern industrialization. The Cabinet organized the Privy Council, introduced the Meiji Constitution, and assembled the Imperial Diet. The Meiji Restoration transformed the Empire of Japan into an industrialized world power that pursued military conflict to expand its sphere of influence. Although France and Britain showed some interest, the European powers largely ignored Japan and instead concentrated on the much greater attractions of China. France was also set back by its failures in Mexico and defeat by the Germans. After victories in the First Sino-Japanese War (1894–1895) and the Russo-Japanese War (1904–1905), Japan gained control of Taiwan, Korea and the southern half of Sakhalin. In addition to imperialistic success, Japan also invested much more heavily in its own economic growth, leading to a period of economic flourishing in the country which lasted until the Great Depression. Japan's population grew from 35 million in 1873 to 70 million in 1935.

In World War I, Japan joined the Allies and captured German possessions, and made advances into China. The early 20th century saw a period of Taishō democracy (1912–1926), but the 1920s saw a fragile democracy buckle under a political shift towards statism, the passing of laws against political dissent and a series of attempted coups. This process accelerated during the 1930s, spawning a number of new Radical Nationalist groups which shared a hostility to liberal democracy and a dedication to expansion in Asia. Japanese expansionism and militarization along with the totalitarianism and ultranationalism reshaped the country. In 1931 Japan invaded and occupied Manchuria and following international condemnation of this occupation, it quit the League of Nations in 1933. In 1936, Japan signed the Anti-Comintern Pact with Germany and the 1940 Tripartite Pact made it one of the Axis Powers.
The Empire of Japan invaded other parts of China in 1937, precipitating the Second Sino-Japanese War (1937–1945). The Imperial Japanese Army swiftly captured the capital Nanjing and conducted the Nanking Massacre. In 1940, the Empire invaded French Indochina, after which the United States placed an oil embargo on Japan. On December 7–8, 1941, Japanese forces carried out surprise attacks on Pearl Harbor, British forces in Malaya, Singapore and Hong Kong and declared war on the United States and the British Empire, bringing the United States and the United Kingdom into World War II in the Pacific. After Allied victories across the Pacific during the next four years, which culminated in the Soviet invasion of Manchuria and the atomic bombings of Hiroshima and Nagasaki in 1945, Japan agreed to an unconditional surrender on August 15. The war cost Japan, its colonies, China and the war's other combatants tens of millions of lives and left much of Japan's industry and infrastructure destroyed. The Allies (led by the United States) repatriated millions of ethnic Japanese from colonies and military camps throughout Asia, largely eliminating the Japanese empire and restoring the independence of its conquered territories. The Allies also convened the International Military Tribunal for the Far East on May 3, 1946, to prosecute some senior generals for war crimes.

In 1947, Japan adopted a new constitution emphasizing liberal democratic practices. The Allied occupation ended with the Treaty of San Francisco in 1952 and Japan was granted membership in the United Nations in 1956. Japan later achieved rapid growth to become the second-largest economy in the world, until surpassed by China in 2010. This ended in the mid-1990s when Japan suffered a major recession. In the beginning of the 21st century, positive growth has signaled a gradual economic recovery. On March 11, 2011, Japan suffered one of the largest earthquakes in its recorded history; this triggered the Fukushima Daiichi nuclear disaster, one of the worst disasters in the history of nuclear power.

Japan has a total of 6,852 islands extending along the Pacific coast of East Asia. The country, including all of the islands it controls, lies between latitudes 24° and 46°N, and longitudes 122° and 146°E. The main islands, from north to south, are Hokkaido, Honshu, Shikoku and Kyushu. The Ryukyu Islands, which include Okinawa, are a chain to the south of Kyushu. Together they are often known as the Japanese archipelago.

About 73 percent of Japan is forested, mountainous and unsuitable for agricultural, industrial or residential use. As a result, the habitable zones, mainly located in coastal areas, have extremely high population densities. Japan is one of the most densely populated countries in the world.

The islands of Japan are located in a volcanic zone on the Pacific Ring of Fire. They are primarily the result of large oceanic movements occurring over hundreds of millions of years from the mid-Silurian to the Pleistocene as a result of the subduction of the Philippine Sea Plate beneath the continental Amurian Plate and Okinawa Plate to the south, and subduction of the Pacific Plate under the Okhotsk Plate to the north. The Boso Triple Junction off the coast of Japan is a triple junction where the North American Plate, the Pacific Plate and the Philippine Sea Plate meets. Japan was originally attached to the eastern coast of the Eurasian continent. The subducting plates pulled Japan eastward, opening the Sea of Japan around 15 million years ago.

Japan has 108 active volcanoes. During the twentieth century several new volcanoes emerged, including Shōwa-shinzan on Hokkaido and Myōjin-shō off the Bayonnaise Rocks in the Pacific. Destructive earthquakes, often resulting in tsunami, occur several times each century. The 1923 Tokyo earthquake killed over 140,000 people. More recent major quakes are the 1995 Great Hanshin earthquake and the 2011 Tōhoku earthquake, a 9.1-magnitude quake which hit Japan on March 11, 2011, and triggered a large tsunami. Japan is substantially prone to earthquakes, tsunami and volcanoes due to its location along the Pacific Ring of Fire. It has the 15th highest natural disaster risk as measured in the 2013 World Risk Index.

The climate of Japan is predominantly temperate, but varies greatly from north to south. Japan's geographical features divide it into six principal climatic zones: Hokkaido, Sea of Japan, Central Highland, Seto Inland Sea, Pacific Ocean, and Ryukyu Islands. The northernmost zone, Hokkaido, has a humid continental climate with long, cold winters and very warm to cool summers. Precipitation is not heavy, but the islands usually develop deep snowbanks in the winter.

In the Sea of Japan zone on Honshu's west coast, northwest winter winds bring heavy snowfall. In the summer, the region is cooler than the Pacific area, though it sometimes experiences extremely hot temperatures because of the foehn. The Central Highland has a typical inland humid continental climate, with large temperature differences between summer and winter seasons, as well as large diurnal variation; precipitation is light, though winters are usually snowy. The mountains of the Chūgoku and Shikoku regions shelter the Seto Inland Sea from seasonal winds, bringing mild weather year-round.

The Pacific coast features a humid subtropical climate that experiences milder winters with occasional snowfall and hot, humid summers because of the southeast seasonal wind. The Ryukyu Islands have a subtropical climate, with warm winters and hot summers. Precipitation is very heavy, especially during the rainy season.

The average winter temperature in Japan is and the average summer temperature is . The highest temperature ever measured in Japan was recorded on July 23, 2018. The main rainy season begins in early May in Okinawa, and the rain front gradually moves north until reaching Hokkaido in late July. In most of Honshu, the rainy season begins before the middle of June and lasts about six weeks. In late summer and early autumn, typhoons often bring heavy rain.

Japan has nine forest ecoregions which reflect the climate and geography of the islands. They range from subtropical moist broadleaf forests in the Ryūkyū and Bonin Islands, to temperate broadleaf and mixed forests in the mild climate regions of the main islands, to temperate coniferous forests in the cold, winter portions of the northern islands. Japan has over 90,000 species of wildlife, including the brown bear, the Japanese macaque, the Japanese raccoon dog, the large Japanese field mouse, and the Japanese giant salamander. A large network of national parks has been established to protect important areas of flora and fauna as well as thirty-seven Ramsar wetland sites. Four sites have been inscribed on the UNESCO World Heritage List for their outstanding natural value.

In the period of rapid economic growth after World War II, environmental policies were downplayed by the government and industrial corporations; as a result, environmental pollution was widespread in the 1950s and 1960s. Responding to rising concern about the problem, the government introduced several environmental protection laws in 1970. The oil crisis in 1973 also encouraged the efficient use of energy because of Japan's lack of natural resources. Current environmental issues include urban air pollution (NOx, suspended particulate matter, and toxics), waste management, water eutrophication, nature conservation, climate change, chemical management and international co-operation for conservation.

As of June 2015, more than 40 coal-fired power plants are planned or under construction in Japan. The NGO Climate Action Network announced Japan as the winner of its "Fossil of the Day" award for "doing the most to block progress on climate action".

Japan ranks 20th in the 2018 Environmental Performance Index, which measures a nation's commitment to environmental sustainability. As the host and signatory of the 1997 Kyoto Protocol, Japan is under treaty obligation to reduce its carbon dioxide emissions and to take other steps to curb climate change.

Japan is a constitutional monarchy whereby the power of the Emperor is very limited. As a ceremonial figurehead, he is defined by the constitution to be "the symbol of the State and of the unity of the people". Executive power is wielded chiefly by the Prime Minister and his cabinet, while sovereignty is vested in the Japanese people.

Japan's legislative body is the National Diet, seated in Chiyoda, Tokyo. The Diet is a bicameral body, comprising the lower House of Representatives with 465 seats, elected by popular vote every four years or when dissolved; and the upper House of Councillors with 242 seats, whose popularly elected members serve six-year terms. There is universal suffrage for adults over 18 years of age, with a secret ballot for all elected offices. The Diet is dominated by the social liberal Constitutional Democratic Party (CDP) and the conservative Liberal Democratic Party (LDP). The LDP has enjoyed near-continuous electoral success since 1955, except for brief periods between 1993 and 1994 and from 2009 to 2012. As of November 2017, it holds 283 seats in the lower house and 125 seats in the upper house.

The Prime Minister of Japan is the head of government and is appointed by the Emperor after being designated by the Diet from among its members. The Prime Minister is the head of the Cabinet, and appoints and dismisses the Ministers of State. Following the LDP's landslide victory in the 2012 general election, Shinzō Abe replaced Yoshihiko Noda as the Prime Minister on December 26, 2012.

Historically influenced by Chinese law, the Japanese legal system developed independently during the Edo period through texts such as "Kujikata Osadamegaki". However, since the late 19th century the judicial system has been largely based on the civil law of Europe, notably Germany. For example, in 1896, the Japanese government established a civil code based on a draft of the German Bürgerliches Gesetzbuch; with the code remaining in effect with post–World War II modifications. Statutory law originates in Japan's legislature and has the rubber stamp of the Emperor. Japan's court system is divided into four basic tiers: the Supreme Court and three levels of lower courts. The main body of Japanese statutory law is called the Six Codes.

Japan is divided into 47 prefectures, each overseen by an elected governor, legislature and administrative bureaucracy. Each prefecture is further divided into cities, towns and villages. The nation is currently undergoing administrative reorganization by merging many of the cities, towns and villages with each other. This process will reduce the number of sub-prefecture administrative regions and is expected to cut administrative costs.

Japan has diplomatic relations with nearly all independent nations and has been an active member of the United Nations since December 1956. Japan is a member of the G8, APEC, and "ASEAN Plus Three", and is a participant in the East Asia Summit. Japan signed a security pact with Australia in March 2007 and with India in October 2008. It is the world's fifth largest donor of official development assistance, donating US$9.2 billion in 2014.

Japan has close ties to the United States. Since Japan's defeat by the United States and allies in World War II, the two countries have maintained close economic and defense relations. The United States is a major market for Japanese exports and the primary source of Japanese imports, and is committed to defending the country, having military bases in Japan for partially that purpose.

Japan contests Russia's control of the Southern Kuril Islands (including Etorofu, Kunashiri, Shikotan, and the Habomai group) which were occupied by the Soviet Union in 1945. South Korea's control of Liancourt Rocks (Japanese: "Takeshima", Korean: "Dokdo") are acknowledged, but not accepted and are claimed by Japan. Japan has strained relations with the People's Republic of China (PRC) and the Republic of China (ROC) over the Senkaku Islands; and with the People's Republic of China over the status of Okinotorishima.

Japan's relationship with South Korea has been strained due to Japan's treatment of Koreans during Japanese colonial rule, particularly over the issue of comfort women. These women were essentially sex slaves, and although there is no exact number on how many women were subjected to this treatment, experts believe it could be in the tens or hundreds of thousands. Between 1910 and 1945, the Japanese government rebuilt Korean infrastructure. Despite this, modernization in Korea was always linked to Japanese interests and therefore did not imply a "revolutionization" of social structures. For instance, Japan kept Korea's primitive feudalistic agriculture because it served Japanese interests. Further developments on Japan's imperialism in Korea included establishing a slew of police stations all over the country, replacing taxes in kind with taxes in fixed money, and taking much of the communal land which had belonged to villages to give them to private companies in Japan (causing many peasants to loose their land.) Japan also introduced over 800,000 Japanese immigrants onto the peninsula and carried out a campaign of cultural suppression through efforts to ban the Korean language in schools and force Koreans to adopt Japanese names. With the surrender of Japan and the Axis at the end of WWII in 1945, the Korean Peninsula was once again independent. Despite their historical tensions, in December 2015, Japan agreed to settle the comfort women dispute with South Korea by issuing a formal apology, taking responsibility for the issue and paying money to the surviving comfort women. Today, South Korea and Japan have a stronger and more economically-driven relationship. Since the 1990s, the Korean Wave has created a large fanbase in East Asia, but most notably in Japan. Japan is the number one importer of Korean music (K-pop), television (K-dramas), and films, but this was only made possible after the South Korean government lifted the 30-year ban on cultural exchange with Japan that had been in place since 1948. Korean pop cultural products' success in the Japanese market is partially explained by the borrowing of Japanese ideas such as the star-marketing system and heavy promotion of new television shows and music. Korean dramas such as "Winter Sonata" and "Coffee Prince," as well as K-pop artists such as BIGBANG and SHINee are extremely popular with Japanese consumers. Most recently, South Korean President Moon Jae-in met with Japanese Prime Minister Shinzo Abe at the 2017 G-20 Summit in Hamburg, Germany to discuss the future of their relationship and specifically how to cooperate on finding solutions for North Korean aggression in the region. Both leaders restated their commitment to solving the comfort women dispute, building positive relations in the region, and pressuring China to be more assertive with North Korea as it continues to test nuclear weapons and isolate themselves further form the international community.

Japan maintains one of the largest military budgets of any country in the world. The country's military (the Japan Self-Defense Forces – JSDF) is restricted by Article 9 of the Japanese Constitution, which renounces Japan's right to declare war or use military force in international disputes. Accordingly, Japan's Self-Defense Forces is an unusual military that has never fired shots outside Japan. Japan is the highest-ranked Asian country in the Global Peace Index.
The military is governed by the Ministry of Defense, and primarily consists of the Japan Ground Self-Defense Force (JGSDF), the Japan Maritime Self-Defense Force (JMSDF) and the Japan Air Self-Defense Force (JASDF). The Japan Maritime Self-Defense Force (JMSDF) is a regular participant in RIMPAC maritime exercises. The forces have been recently used in peacekeeping operations; the deployment of troops to Iraq marked the first overseas use of Japan's military since World War II. Japan Business Federation has called on the government to lift the ban on arms exports so that Japan can join multinational projects such as the Joint Strike Fighter.

The 21st century is witnessing a rapid change in global power balance along with globalization. The security environment around Japan has become increasingly severe as represented by nuclear and missile development by North Korea. Transnational threats grounded on technological progress including international terrorism and cyber attacks are also increasing their significance. Japan, including its Self-Defense Forces, has contributed to the maximum extent possible to the efforts to maintain and restore international peace and security, such as UN peacekeeping operations. Building on the ongoing efforts as a peaceful state, the Government of Japan has been making various efforts on its security policy which include: the establishment of the National Security Council (NSC), the adoption of the National Security Strategy (NSS), and the National Defense Program Guidelines (NDPG). These efforts are made based on the belief that Japan, as a "Proactive Contributor to Peace", needs to contribute more actively to the peace and stability of the region and the international community, while coordinating with other countries including its ally, the United States.

Japan has close economic and military relations with the United States; the US-Japan security alliance acts as the cornerstone of the nation's foreign policy. A member state of the United Nations since 1956, Japan has served as a non-permanent Security Council member for a total of 20 years, most recently for 2009 and 2010. It is one of the G4 nations seeking permanent membership in the Security Council.

In May 2014, Prime Minister Shinzō Abe said Japan wanted to shed the passiveness it has maintained since the end of World War II and take more responsibility for regional security. He said Japan wanted to play a key role and offered neighboring countries Japan's support. In recent years, they have been engaged in international peacekeeping operations including the UN peacekeeping. Recent tensions, particularly with North Korea, have reignited the debate over the status of the JSDF and its relation to Japanese society. New military guidelines, announced in December 2010, will direct the JSDF away from its Cold War focus on the former Soviet Union to a focus on China, especially regarding the territorial dispute over the Senkaku Islands.

Japan is the third largest national economy in the world, after the United States and China, in terms of nominal GDP, and the fourth largest national economy in the world, after the United States, China and India, in terms of purchasing power parity. , Japan's public debt was estimated at more than 230 percent of its annual gross domestic product, the largest of any nation in the world. In August 2011, Moody's rating has cut Japan's long-term sovereign debt rating one notch from Aa3 to Aa2 inline with the size of the country's deficit and borrowing level. The large budget deficits and government debt since the 2009 global recession and followed by the earthquake and tsunami in March 2011 caused the rating downgrade. The service sector accounts for three quarters of the gross domestic product.

Japan has a large industrial capacity, and is home to some of the largest and most technologically advanced producers of motor vehicles, electronics, machine tools, steel and nonferrous metals, ships, chemical substances, textiles, and processed foods. Agricultural businesses in Japan cultivate 13 percent of Japan's land, and Japan accounts for nearly 15 percent of the global fish catch, second only to China. , Japan's labor force consisted of some 65.9 million workers. Japan has a low unemployment rate of around four percent. Some 20 million people, around 17 per cent of the population, were below the poverty line in 2007. Housing in Japan is characterized by limited land supply in urban areas.

Japan's exports amounted to US$4,210 per capita in 2005. , Japan's main export markets were the United States (20.2 percent), China (17.5 percent), South Korea (7.1 percent), Hong Kong (5.6 percent) and Thailand (4.5 percent). Its main exports are transportation equipment, motor vehicles, iron and steel products, semiconductors and auto parts. Japan's main import markets were China (24.8 percent), the United States (10.5 percent), Australia (5.4 percent) and South Korea (4.1 percent).

Japan's main imports are machinery and equipment, fossil fuels, foodstuffs (in particular beef), chemicals, textiles and raw materials for its industries. By market share measures, domestic markets are the least open of any OECD country. Junichirō Koizumi's administration began some pro-competition reforms, and foreign investment in Japan has soared.

Japan ranks 27th of 189 countries in the 2014 ease of doing business index and has one of the smallest tax revenues of the developed world. The Japanese variant of capitalism has many distinct features: keiretsu enterprises are influential, and lifetime employment and seniority-based career advancement are relatively common in the Japanese work environment. Japanese companies are known for management methods like "The Toyota Way", and shareholder activism is rare. Japan's top global brands include Toyota, Honda, Canon, Nissan, Sony, Mitsubishi UFJ (MUFG), Panasonic, Uniqlo, Lexus, Subaru, Nintendo, Bridgestone, Mazda and Suzuki.

Modern Japan's economic growth began in the Edo period. Some of the surviving elements of the Edo period are roads and water transportation routes, as well as financial instruments such as futures contracts, banking and insurance of the Osaka rice brokers. During the Meiji period from 1868, Japan expanded economically with the embrace of the market economy. Many of today's enterprises were founded at the time, and Japan emerged as the most developed nation in Asia. The period of overall real economic growth from the 1960s to the 1980s has been called the Japanese post-war economic miracle: it averaged 7.5 percent in the 1960s and 1970s, and 3.2 percent in the 1980s and early 1990s.

Growth slowed in the 1990s during the "Lost Decade" due to after-effects of the Japanese asset price bubble and government policies intended to wring speculative excesses from the stock and real estate markets. Efforts to revive economic growth were unsuccessful and further hampered by the global slowdown in 2000. The economy recovered after 2005; GDP growth for that year was 2.8 percent, surpassing the growth rates of the US and European Union during the same period.

Today, Japan ranks highly for competitiveness and economic freedom. It is ranked sixth in the Global Competitiveness Report for 2015–2016.

The Japanese agricultural sector accounts for about 1.4% of the total country's GDP. Only 12% of Japan's land is suitable for cultivation. Due to this lack of arable land, a system of terraces is used to farm in small areas. This results in one of the world's highest levels of crop yields per unit area, with an overall agricultural self-sufficiency rate of about 50% on fewer than cultivated.

Japan's small agricultural sector, however, is also highly subsidized and protected, with government regulations that favor small-scale cultivation instead of large-scale agriculture as practiced in North America. There has been a growing concern about farming as the current farmers are aging with a difficult time finding successors.

Rice accounts for almost all of Japan's cereal production. Japan is the second-largest agricultural product importer in the world. Rice, the most protected crop, is subject to tariffs of 777.7%.

In 1996, Japan ranked fourth in the world in tonnage of fish caught. Japan captured 4,074,580 metric tons of fish in 2005, down from 4,987,703 tons in 2000, 9,558,615 tons in 1990, 9,864,422 tons in 1980, 8,520,397 tons in 1970, 5,583,796 tons in 1960 and 2,881,855 tons in 1950. In 2003, the total aquaculture production was predicted at 1,301,437 tonnes. In 2010, Japan's total fisheries production was 4,762,469 fish. Offshore fisheries accounted for an average of 50% of the nation's total fish catches in the late 1980s although they experienced repeated ups and downs during that period.

Today, Japan maintains one of the world's largest fishing fleets and accounts for nearly 15% of the global catch, prompting some claims that Japan's fishing is leading to depletion in fish stocks such as tuna. Japan has also sparked controversy by supporting quasi-commercial whaling.

Japan's industrial sector makes up approximately 27.5% of its GDP. Japan's major industries are motor vehicles, electronics, machine tools, metals, ships, chemicals and processed foods; some major Japanese industrial companies include Toyota, Canon Inc., Toshiba and Nippon Steel.

Japan is the third largest automobile producer in the world, and is home to Toyota, the world's largest automobile company. The Japanese consumer electronics industry, once considered the strongest in the world, is currently in a state of decline as competition arises in countries like South Korea, the United States and China. However, despite also facing similar competition from South Korea and China, the Japanese shipbuilding industry is expected to remain strong due to an increased focus on specialized, high-tech designs.

Japan's service sector accounts for about three-quarters of its total economic output. Banking, insurance, real estate, retailing, transportation, and telecommunications are all major industries, with companies such as Mitsubishi UFJ, Mizuho, NTT, TEPCO, Nomura, Mitsubishi Estate, ÆON, Mitsui Sumitomo, Softbank, JR East, Seven & I, KDDI and Japan Airlines listed as some of the largest in the world. Four of the five most circulated newspapers in the world are Japanese newspapers. Japan Post Holdings, one of the country's largest providers of savings and insurance services, was slated for privatization by 2015. The six major keiretsus are the Mitsubishi, Sumitomo, Fuyo, Mitsui, Dai-Ichi Kangyo and Sanwa Groups.

Japan attracted 19.73 million international tourists in 2015 and increased by 21.8% to attract 24.03 million international tourists in 2016. Tourism from abroad is one of the few promising businesses in Japan. Foreign visitors to Japan doubled in last decade and reached 10 million people for the first time in 2013, led by increase of Asian visitors.

In 2008, the Japanese government has set up Japan Tourism Agency and set the initial goal to increase foreign visitors to 20 million in 2020. In 2016, having met the 20 million target, the government has revised up its target to 40 million by 2020 and to 60 million by 2030.

Japan has 20 World Heritage Sites, including Himeji Castle, Historic Monuments of Ancient Kyoto and Nara. Popular tourist attractions include Tokyo and Hiroshima, Mount Fuji, ski resorts such as Niseko in Hokkaido, Okinawa, riding the shinkansen and taking advantage of Japan's hotel and hotspring network.

For inbound tourism, Japan was ranked 16th in the world in 2015. In 2009, the "Yomiuri Shimbun" published a modern list of famous sights under the name "Heisei Hyakkei" (the Hundred Views of the Heisei period). The "Travel and Tourism Competitiveness Report 2017" ranks Japan 4th out of 141 countries overall, which was the best in Asia. Japan gained relatively high scores in almost all aspects, especially health and hygiene, safety and security, cultural resources and business travel.

In 2016, 24,039,053 foreign tourists visited Japan. Neighbouring South Korea is Japan's most important source of foreign tourists. In 2010, the 2.4 million arrivals made up 27% of the tourists visiting Japan. Chinese travelers are the highest spenders in Japan by country, spending an estimated 196.4 billion yen (US$2.4 billion) in 2011, or almost a quarter of total expenditure by foreign visitors, according to data from the Japan Tourism Agency.

The Japanese government hopes to receive 40 million foreign tourists every year by 2020.

Japan is a leading nation in scientific research, particularly in fields related to the natural sciences and engineering. The country ranks second among the most innovative countries in the Bloomberg Innovation Index. Nearly 700,000 researchers share a US$130 billion research and development budget. The amount spent on research and development relative to gross domestic product is the third highest in the world. The country is a world leader in fundamental scientific research, having produced twenty-two Nobel laureates in either physics, chemistry or medicine and three Fields medalists.

Japanese scientists and engineers have contributed to the advancement of agricultural sciences, electronics, industrial robotics, optics, chemicals, semiconductors, life sciences and various fields of engineering. Japan leads the world in robotics production and use, possessing more than 20% (300,000 of 1.3 million) of the world's industrial robots —though its share was historically even higher, representing one-half of all industrial robots worldwide in 2000. Japan boasts the third highest number of scientists, technicians, and engineers per capita in the world with 83 scientists, technicians and engineers per 10,000 employees.

The Japanese electronics and automotive manufacturing industry is well known throughout the world, and the country's electronic and automotive products account for a large share in the global market, compared to a majority of other countries. Brands such as Fujifilm, Canon, Sony, Nintendo, Panasonic, Toyota, Nissan and Honda are internationally famous. It is estimated that 16% of the world's gold and 22% of the world's silver is contained in Japanese electronics.

Japan has started a project to build the world's fastest supercomputer by the end of 2017.

The Japan Aerospace Exploration Agency (JAXA) is Japan's national space agency; it conducts space, planetary, and aviation research, and leads development of rockets and satellites. It is a participant in the International Space Station: the Japanese Experiment Module (Kibo) was added to the station during Space Shuttle assembly flights in 2008. The space probe "Akatsuki" was launched May 20, 2010, and achieved orbit around Venus on December 9, 2015. Japan's plans in space exploration include: developing the "Mercury Magnetospheric Orbiter" to be launched in 2018; and building a moon base by 2030.

On September 14, 2007, it launched lunar explorer SELENE (Selenological and Engineering Explorer) on a H-IIA (Model H2A2022) carrier rocket from Tanegashima Space Center. SELENE is also known as Kaguya, after the lunar princess of "The Tale of the Bamboo Cutter". Kaguya is the largest lunar mission since the Apollo program. Its purpose is to gather data on the moon's origin and evolution. It entered a lunar orbit on October 4, flying at an altitude of about . The probe's mission was ended when it was deliberately crashed by JAXA into the Moon on June 11, 2009.

Japan has received the most science Nobel Prizes in Asia and ranked 8th in the world. Hideki Yukawa, educated at Kyoto University, was awarded the prize in physics in 1949. Shin'ichirō Tomonaga followed in 1965. Solid-state physicist Leo Esaki, educated at the University of Tokyo, received the prize in 1973. Kenichi Fukui of Kyoto University shared the 1981 prize in chemistry, and Susumu Tonegawa, also educated at Kyoto University, became Japan's first laureate in physiology or medicine in 1987. Japanese chemists took prizes in 2000 and 2001: first Hideki Shirakawa (Tokyo Institute of Technology) and then Ryōji Noyori (Kyoto University). In 2002, Masatoshi Koshiba (University of Tokyo) and Koichi Tanaka (Tohoku University) won in physics and chemistry, respectively. Makoto Kobayashi, Toshihide Masukawa and Yoichiro Nambu, who was an American citizen when awarded, shared the physics prize and Osamu Shimomura also won the chemistry prize in 2008. Isamu Akasaki, Hiroshi Amano and Shuji Nakamura, who is an American citizen when awarded, shared the physics prize in 2014 and the Nobel Prize in Physiology or Medicine was awarded to Yoshinori Ohsumi in 2016.

Japan's road spending has been extensive. Its of paved road are the main means of transportation. As of April 2012, Japan has approximately of roads made up of of city, town and village roads, of prefectural roads, of general national highways and of national expressways. A single network of high-speed, divided, limited-access toll roads connects major cities on Honshu, Shikoku and Kyushu. Hokkaido has a separate network, and Okinawa Island has a highway of this type. A single network of high-speed, divided, limited-access toll roads connects major cities and is operated by toll-collecting enterprises. New and used cars are inexpensive; car ownership fees and fuel levies are used to promote energy efficiency. However, at just 50 percent of all distance traveled, car usage is the lowest of all G8 countries.

Since privatisation in 1987, dozens of Japanese railway companies compete in regional and local passenger transportation markets; major companies include seven JR enterprises, Kintetsu, Seibu Railway and Keio Corporation. Some 250 high-speed Shinkansen trains connect major cities and Japanese trains are known for their safety and punctuality. Proposals for a new Maglev route between Tokyo and Osaka are at an advanced stage.

There are 175 airports in Japan; the largest domestic airport, Haneda Airport, is Asia's second-busiest airport. The largest international gateways are Narita International Airport, Kansai International Airport and Chūbu Centrair International Airport. Nagoya Port is the country's largest and busiest port, accounting for 10 percent of Japan's trade value.

, 46.1% of energy in Japan was produced from petroleum, 21.3% from coal, 21.4% from natural gas, 4.0% from nuclear power and 3.3% from hydropower. Nuclear power produced 9.2 percent of Japan's electricity, , down from 24.9 percent the previous year. However, by May 2012 all of the country's nuclear power plants had been taken offline because of ongoing public opposition following the Fukushima Daiichi nuclear disaster in March 2011, though government officials continued to try to sway public opinion in favor of returning at least some of Japan's 50 nuclear reactors to service. , two reactors at Sendai are likely to restart in early 2015. Japan lacks significant domestic reserves and so has a heavy dependence on imported energy. Japan has therefore aimed to diversify its sources and maintain high levels of energy efficiency.

The government took responsibility for regulating the water and sanitation sector is shared between the Ministry of Health, Labor and Welfare in charge of water supply for domestic use; the Ministry of Land, Infrastructure, Transport and Tourism in charge of water resources development as well as sanitation; the Ministry of the Environment in charge of ambient water quality and environmental preservation; and the Ministry of Internal Affairs and Communications in charge of performance benchmarking of utilities.

Access to an improved water source is universal in Japan. 97% of the population receives piped water supply from public utilities and 3% receive water from their own wells or unregulated small systems, mainly in rural areas.

Access to improved sanitation is also universal, either through sewers or on-site sanitation. All collected waste water is treated at secondary-level treatment plants. All effluents discharged to closed or semi-closed water bodies, such as Tokyo Bay, Osaka Bay, or Lake Biwa, are further treated to tertiary level. This applies to about 15% of waste water. The effluent quality is remarkably good at 3–10 mg/l of BOD for secondary-level treatment, well below the national effluent standard of 20 mg/l.

Water supply and sanitation in Japan is facing some challenges, such as a decreasing population, declining investment, fiscal constraints, ageing facilities, an ageing workforce, a fragmentation of service provision among thousands of municipal utilities, and the vulnerability of parts of the country to droughts that are expected to become more frequent due to climate change.

Japan's population is estimated at around /1e6 round 0 million, with 80% of the population living on Honshū. Japanese society is linguistically, ethnically and culturally homogeneous, composed of 98.5% ethnic Japanese, with small populations of foreign workers. Zainichi Koreans, Chinese, Filipinos, Brazilians mostly of Japanese descent, Peruvians mostly of Japanese descent and Americans are among the small minority groups in Japan. In 2003, there were about 134,700 non-Latin American Western (not including more than 33,000 American military personnel and their dependents stationed throughout the country) and 345,500 Latin American expatriates, 274,700 of whom were Brazilians (said to be primarily Japanese descendants, or "nikkeijin", along with their spouses), the largest community of Westerners.

The most dominant native ethnic group is the Yamato people; primary minority groups include the indigenous Ainu and Ryukyuan peoples, as well as social minority groups like the "burakumin". There are persons of mixed ancestry incorporated among the Yamato, such as those from Ogasawara Archipelago. In 2014, foreign-born non-naturalized workers made up only 1.5% of the total population. Japan is widely regarded as ethnically homogeneous, and does not compile ethnicity or race statistics for Japanese nationals; sources varies regarding such claim, with at least one analysis describing Japan as a multiethnic society while another analysis put the number of Japanese nationals of recent foreign descent to be minimal. Most Japanese continue to see Japan as a monocultural society. Former Japanese Prime Minister and current Finance Minister Tarō Asō described Japan as being a nation of "one race, one civilization, one language and one culture", which drew criticism from representatives of ethnic minorities such as the Ainu.

Japan has the second longest overall life expectancy at birth of any country in the world: 83.5 years for persons born in the period 2010–2015. The Japanese population is rapidly aging as a result of a post–World War II baby boom followed by a decrease in birth rates. In 2012, about 24.1 percent of the population was over 65, and the proportion is projected to rise to almost 40 percent by 2050.

Japan has full religious freedom based on Article 20 of its Constitution. Upper estimates suggest that 84–96 percent of the Japanese population subscribe to Shinto as its indigenous religion (50% to 80% of which considering degrees of syncretism with Buddhism, shinbutsu-shūgō). However, these estimates are based on people affiliated with a temple, rather than the number of true believers. The number of Shinto shrines in Japan is estimated to be around 100,000. Other studies have suggested that only 30 percent of the population identify themselves as belonging to a religion. According to Edwin Reischauer and Marius Jansen, some 70–80% of the Japanese do not consider themselves believers in any religion. Nevertheless, the level of participation remains high, especially during festivals and occasions such as the first shrine visit of the New Year. Taoism and Confucianism from China have also influenced Japanese beliefs and customs. Japanese streets are decorated on Tanabata, Obon and Christmas.

Shinto is the largest religion in Japan, practiced by nearly 80% of the population, yet only a small percentage of these identify themselves as "Shintoists" in surveys. This is due to the fact that "Shinto" has different meanings in Japan: most of the Japanese attend Shinto shrines and beseech kami without belonging to Shinto organisations, and since there are no formal rituals to become a member of folk Shinto, Shinto membership is often estimated counting those who join organised Shinto sects. Shinto has 100,000 shrines and 78,890 priests in the country. Buddhism first arrived in Japan in the 6th century; it was introduced in the year 538 or 552 from the kingdom of Baekje in Korea.

Christianity was first introduced into Japan by Jesuit missions starting in 1549. Today, fewer than 1% to 2.3% are Christians, most of them living in the western part of the country, where the missionaries' activities were greatest during the 16th century. Nagasaki Prefecture has the highest percentage of Christians: about 5.1% in 1996. , there are 32,036 Christian priests and pastors in Japan. Throughout the latest century, some Western customs originally related to Christianity (including Western style weddings, Valentine's Day and Christmas) have become popular as secular customs among many Japanese.

Islam in Japan is estimated to constitute, about 80–90%, of foreign born migrants and their children, primarily from Indonesia, Pakistan, Bangladesh, and Iran. Much of the ethnic Japanese Muslims are those who convert upon marrying immigrant Muslims. The Pew Research Center estimated that there were 185,000 Muslims in Japan in 2010.

Other minority religions include Hinduism, Sikhism and Judaism, and since the mid-19th century numerous new religious movements have emerged in Japan.

More than 99 percent of the population speaks Japanese as their first language. Japanese is an agglutinative language distinguished by a system of honorifics reflecting the hierarchical nature of Japanese society, with verb forms and particular vocabulary indicating the relative status of speaker and listener. Japanese writing uses kanji (Chinese characters) and two sets of kana (syllabaries based on cursive script and radical of kanji), as well as the Latin alphabet and Arabic numerals.

Besides Japanese, the Ryukyuan languages (Amami, Kunigami, Okinawan, Miyako, Yaeyama, Yonaguni), also part of the Japonic language family, are spoken in the Ryukyu Islands chain. Few children learn these languages, but in recent years the local governments have sought to increase awareness of the traditional languages. The Okinawan Japanese dialect is also spoken in the region. The Ainu language, which has no proven relationship to Japanese or any other language, is moribund, with only a few elderly native speakers remaining in Hokkaido. Public and private schools generally require students to take Japanese language classes as well as English language courses.

The changes in demographic structure have created a number of social issues, particularly a potential decline in workforce population and increase in the cost of social security benefits such as the public pension plan. A growing number of younger Japanese are not marrying or remain childless. In 2011, Japan's population dropped for a fifth year, falling by 204,000 people to 126.24 million people. This was the greatest decline since at least 1947, when comparable figures were first compiled. This decline was made worse by the March 2011 earthquake and tsunami, which killed nearly 16,000 people.

Japan's population is expected to drop to 95 million by 2050; demographers and government planners are currently in a heated debate over how to cope with this problem. Immigration and birth incentives are sometimes suggested as a solution to provide younger workers to support the nation's ageing population. Japan accepts an average flow of 9,500 new Japanese citizens by naturalization per year. According to the UNHCR, in 2012 Japan accepted just 18 refugees for resettlement, while the United States took in 76,000.

Japan suffers from a high suicide rate. In 2009, the number of suicides exceeded 30,000 for the twelfth successive year. Suicide is the leading cause of death for people under 30.

Primary schools, secondary schools and universities were introduced in 1872 as a result of the Meiji Restoration. Since 1947, compulsory education in Japan comprises elementary and junior high school, which together last for nine years (from age 6 to age 15). Almost all children continue their education at a three-year senior high school.

Japan's education system played a central part in the country's recovery and rapid economic growth in the decades following the end of World War II. After World War II, the Fundamental Law of Education and the School Education Law were enacted. The latter law defined the school system that would be in effect for many decades: six years of elementary school, three years of junior high school, three years of high school, and two or four years of university. Starting in April 2016, various schools began the academic year with elementary school and junior high school integrated into one nine-year compulsory schooling program, in hopes to mitigate bullying and truancy; MEXT plans for this approach to be adopted nationwide in the coming years. In Japan, having a strong educational background greatly improves the likelihood of finding a job and earning enough money to support oneself. Highly educated individuals are less affected by unemployment trends as higher levels of educational attainment make an individual more attractive in the workforce. The lifetime earnings also increase with each level of education attained. Furthermore, skills needed in the modern 21st century labor market are becoming more knowledge-based and strong aptitude in science and mathematics are more strong predictors of employment prospects in Japan's highly technological economy.

Japan is one of the top-performing OECD countries in reading literacy, maths and sciences with the average student scoring 540 and has one of the worlds highest-educated labor forces among OECD countries. The Japanese populace is well educated and its society highly values education as a platform for social mobility and for gaining employment in the country's competitive high-tech economy. The country's large pool of highly educated and skilled individuals is largely responsible for ushering Japan's post-war economic growth. Tertiary-educated adults in Japan, particularly graduates in sciences and engineering benefit economically and socially from their education and skills in the country's high tech economy. Spending on education as a proportion of GDP is below the OECD average. Although expenditure per student is comparatively high in Japan, total expenditure relative to GDP remains small. In 2015, Japan's public spending on education amounted to just 3.5 percent of its GDP, below the OECD average of 4.7%. In 2014, the country ranked fourth for the percentage of 25- to 64-year-olds that have attained tertiary education with 48 percent. In addition, bachelor's degrees are held by 59 percent of Japanese aged 25–34, the second most in the OECD after South Korea. As the Japanese economy is largely scientific and technological based, the labor market demands people who have achieved some form of higher education, particularly related to science and engineering in order to gain a competitive edge when searching for employment opportunities. About 75.9 percent of high school graduates attended a university, junior college, trade school, or other higher education institution.

The two top-ranking universities in Japan are the University of Tokyo and Kyoto University, which have produced 16 Nobel Prize laureates. The Programme for International Student Assessment coordinated by the OECD currently ranks the overall knowledge and skills of Japanese 15-year-olds as sixth best in the world.

In Japan, health care is provided by national and local governments. Payment for personal medical services is offered through a universal health insurance system that provides relative equality of access, with fees set by a government committee. People without insurance through employers can participate in a national health insurance program administered by local governments. Since 1973, all elderly persons have been covered by government-sponsored insurance. Patients are free to select the physicians or facilities of their choice.

Japanese culture has evolved greatly from its origins. Contemporary culture combines influences from Asia, Europe and North America. Traditional Japanese arts include crafts such as ceramics, textiles, lacquerware, swords and dolls; performances of bunraku, kabuki, noh, dance, and rakugo; and other practices, the tea ceremony, ikebana, martial arts, calligraphy, origami, onsen, Geisha and games. Japan has a developed system for the protection and promotion of both tangible and intangible Cultural Properties and National Treasures. Nineteen sites have been inscribed on the UNESCO World Heritage List, fifteen of which are of cultural significance.

Japanese architecture is a combination between local and other influences. It has traditionally been typified by wooden structures, elevated slightly off the ground, with tiled or thatched roofs. Sliding doors ("fusuma") were used in place of walls, allowing the internal configuration of a space to be customized for different occasions. People usually sat on cushions or otherwise on the floor, traditionally; chairs and high tables were not widely used until the 20th century. Since the 19th century, however, Japan has incorporated much of Western, modern, and post-modern architecture into construction and design, and is today a leader in cutting-edge architectural design and technology.

The introduction of Buddhism during the sixth century was a catalyst for large-scale temple building using complicated techniques in wood. Influence from the Chinese Tang and Sui dynasties led to the foundation of the first permanent capital in Nara. Its checkerboard street layout used the Chinese capital of Chang'an as a template for its design. A gradual increase in the size of buildings led to standard units of measurement as well as refinements in layout and garden design. The introduction of the tea ceremony emphasised simplicity and modest design as a counterpoint to the excesses of the aristocracy.

During the Meiji Restoration of 1868 the history of Japanese architecture was radically changed by two important events. The first was the Kami and Buddhas Separation Act of 1868, which formally separated Buddhism from Shinto and Buddhist temples from Shinto shrines, breaking an association between the two which had lasted well over a thousand years.

Second, it was then that Japan underwent a period of intense Westernization in order to compete with other developed countries. Initially architects and styles from abroad were imported to Japan but gradually the country taught its own architects and began to express its own style. Architects returning from study with western architects introduced the International Style of modernism into Japan. However, it was not until after the Second World War that Japanese architects made an impression on the international scene, firstly with the work of architects like Kenzō Tange and then with theoretical movements like Metabolism.

The Shrines of Ise have been celebrated as the prototype of Japanese architecture. Largely of wood, traditional housing and many temple buildings see the use of tatami mats and sliding doors that break down the distinction between rooms and indoor and outdoor space. Japanese sculpture, largely of wood, and Japanese painting are among the oldest of the Japanese arts, with early figurative paintings dating back to at least 300 BC. The history of Japanese painting exhibits synthesis and competition between native Japanese aesthetics and adaptation of imported ideas.

The interaction between Japanese and European art has been significant: for example ukiyo-e prints, which began to be exported in the 19th century in the movement known as Japonism, had a significant influence on the development of modern art in the West, most notably on post-Impressionism. Famous ukiyo-e artists include Hokusai and Hiroshige.

Japanese comics, known as manga, developed in the 20th century and have become popular worldwide. Rakuten Kitazawa was first to use the word "manga" in the modern sense. Japanese-made video game consoles have been popular since the 1980s.

Japanese animated films and television series, known as anime for short, were largely influenced by Japanese manga comic books and have been extensively popular in the West. Japan is a world-renowned powerhouse of animation. Famous anime directors include Hayao Miyazaki, Osamu Tezuka and Isao Takahata.

Japan has one of the oldest and largest film industries in the world; movies have been produced in Japan since 1897. Three Japanese films ("Rashomon", "Seven Samurai" and "Tokyo Story") made the "Sight & Sound"'s 2002 Critics and Directors Poll for the best films of all time. Ishirō Honda's "Godzilla" became an international icon of Japan and spawned an entire subgenre of "kaiju" films, as well as the longest-running film franchise in history. The most acclaimed Japanese film directors include Akira Kurosawa, Kenji Mizoguchi, Yasujiro Ozu and Shohei Imamura. Japan has won the Academy Award for the Best Foreign Language Film four times, more than any other Asian country.

Japanese music is eclectic and diverse. Many instruments, such as the koto, were introduced in the 9th and 10th centuries. The accompanied recitative of the Noh drama dates from the 14th century and the popular folk music, with the guitar-like shamisen, from the sixteenth. Western classical music, introduced in the late 19th century, now forms an integral part of Japanese culture. The imperial court ensemble Gagaku has influenced the work of some modern Western composers.

Notable classical composers from Japan include Toru Takemitsu and Rentarō Taki. Popular music in post-war Japan has been heavily influenced by American and European trends, which has led to the evolution of J-pop, or Japanese popular music. Karaoke is the most widely practiced cultural activity in Japan. A 1993 survey by the Cultural Affairs Agency found that more Japanese had sung karaoke that year than had participated in traditional pursuits such as flower arranging (ikebana) or tea ceremonies.

The earliest works of Japanese literature include the "Kojiki" and "Nihon Shoki" chronicles and the "Man'yōshū" poetry anthology, all from the 8th century and written in Chinese characters. In the early Heian period, the system of phonograms known as "kana" (hiragana and katakana) was developed. "The Tale of the Bamboo Cutter" is considered the oldest Japanese narrative. An account of Heian court life is given in "The Pillow Book" by Sei Shōnagon, while "The Tale of Genji" by Murasaki Shikibu is often described as the world's first novel.

During the Edo period, the chōnin ("townspeople") overtook the samurai aristocracy as producers and consumers of literature. The popularity of the works of Saikaku, for example, reveals this change in readership and authorship, while Bashō revivified the poetic tradition of the Kokinshū with his haikai (haiku) and wrote the poetic travelogue "Oku no Hosomichi". The Meiji era saw the decline of traditional literary forms as Japanese literature integrated Western influences. Natsume Sōseki and Mori Ōgai were the first "modern" novelists of Japan, followed by Ryūnosuke Akutagawa, Jun'ichirō Tanizaki, Yukio Mishima and, more recently, Haruki Murakami. Japan has two Nobel Prize-winning authors—Yasunari Kawabata (1968) and Kenzaburō Ōe (1994).

Japanese Philosophy has historically been a fusion of both foreign; particularly Chinese and Western, and uniquely Japanese elements. In its literary forms, Japanese philosophy began about fourteen centuries ago.

Archaeological evidence and early historical accounts suggest that Japan was originally an animistic culture, which viewed the world as infused with "kami" (神) or sacred presence as taught by Shinto, though it is not a philosophy as such, but has greatly influenced all other philosophies in their Japanese interpretations.

Confucianism entered Japan from China around the 5th century A.D., as did Buddhism. Confucian ideals are still evident today in the Japanese concept of society and the self, and in the organization of the government and the structure of society. Buddhism has profoundly impacted Japanese psychology, metaphysics, and aesthetics.

Indigenous ideas of loyalty and honour have been held since the 16th century. Western philosophy has had its major impact in Japan only since the middle of the 19th century.

Japanese cuisine is based on combining staple foods, typically Japanese rice or noodles, with a soup and "okazu"—dishes made from fish, vegetable, tofu and the like—to add flavor to the staple food. In the early modern era ingredients such as red meats that had previously not been widely used in Japan were introduced. Japanese cuisine is known for its emphasis on seasonality of food, quality of ingredients and presentation. Japanese cuisine offers a vast array of regional specialties that use traditional recipes and local ingredients. The phrase refers to the makeup of a typical meal served, but has roots in classic "kaiseki", "honzen", and "yūsoku" cuisine. The term is also used to describe the first course served in standard "kaiseki" cuisine nowadays.

Traditional Japanese sweets are known as "wagashi". Ingredients such as red bean paste and mochi are used. More modern-day tastes includes green tea ice cream, a very popular flavor. Almost all manufacturers produce a version of it. Kakigori is a shaved ice dessert flavored with syrup or condensed milk. It is usually sold and eaten at summer festivals. Popular Japanese beverages such as sake, which is a brewed rice beverage that, typically, contains 15%~17% alcohol and is made by multiple fermentation of rice. Beer has been brewed in Japan since the late 1800s and is produced in many regions by companies including Asahi Breweries, Kirin Brewery, and Sapporo Brewery – the oldest brand of beer in Japan.

Officially, Japan has 16 national, government-recognized holidays. Public holidays in Japan are regulated by the Public Holiday Law (国民の祝日に関する法律 "Kokumin no Shukujitsu ni Kansuru Hōritsu") of 1948. Beginning in 2000, Japan implemented the Happy Monday System, which moved a number of national holidays to Monday in order to obtain a long weekend. In 2006, the country decided to add Shōwa Day, a new national holiday, in place of Greenery Day on April 29, and to move Greenery Day to May 4. These changes took effect in 2007. In 2014, the House of Councillors decided to add to the Japanese calendar on August 11, after lobbying by the Japanese Alpine Club. It is intended to coincide with the Bon Festival vacation time, giving Japanese people an opportunity to appreciate Japan's mountains.

The national holidays in Japan are New Year's Day on January 1, Coming of Age Day on Second Monday of January, National Foundation Day on February 11, Vernal Equinox Day on March 20 or 21, Shōwa Day on April 29, Constitution Memorial Day on May 3, Greenery Day on May 4, Children's Day on May 5, Marine Day on Third Monday of July, Mountain Day on August 11, Respect for the Aged Day on Third Monday of September, Autumnal Equinox on September 23 or 24, Health and Sports Day on Second Monday of October, Culture Day on November 3, Labour Thanksgiving Day on November 23, and The Emperor's Birthday on December 23.

There are many festivals in Japan, which are called in Japanese as "matsuri" (祭) which celebrate annually. There are no specific festival days for all of Japan; dates vary from area to area, and even within a specific area, but festival days do tend to cluster around traditional holidays such as Setsubun or Obon. Festivals are often based around one event, with food stalls, entertainment, and carnival games to keep people entertained. Its usually sponsored by a local shrine or temple, though they can be secular.

Notable festival often feature processions which may include elaborate floats. Preparation for these processions is usually organised at the level of neighborhoods, or "machi" (町). Prior to these, the local kami may be ritually installed in mikoshi and paraded through the streets, such as Gion in Kyoto, and Hadaka in Okayama.

Traditionally, sumo is considered Japan's national sport. Japanese martial arts such as judo, karate and kendo are also widely practiced and enjoyed by spectators in the country. After the Meiji Restoration, many Western sports were introduced in Japan and began to spread through the education system.

Japan hosted the Summer Olympics in Tokyo in 1964 and the Winter Olympics in Sapporo in 1972 and Nagano in 1998. Further, the country hosted the official 2006 Basketball World Championship. Tokyo will host the 2020 Summer Olympics, making Tokyo the first Asian city to host the Olympics twice. The country gained the hosting rights for the official Women's Volleyball World Championship on five occasions (1967, 1998, 2006, 2010, 2018), more than any other nation. Japan is the most successful Asian Rugby Union country, winning the Asian Five Nations a record 6 times and winning the newly formed IRB Pacific Nations Cup in 2011. Japan will host the 2019 IRB Rugby World Cup.

Baseball is currently the most popular spectator sport in the country. Japan's top professional league, now known as Nippon Professional Baseball, was established in 1936 and is widely considered to be the highest level of professional baseball in the world outside of the North American Major Leagues. Since the establishment of the Japan Professional Football League in 1992, association football has also gained a wide following. Japan was a venue of the Intercontinental Cup from 1981 to 2004 and co-hosted the 2002 FIFA World Cup with South Korea. Japan has one of the most successful football teams in Asia, winning the Asian Cup four times. Also, Japan recently won the FIFA Women's World Cup in 2011. Golf is also popular in Japan, as are forms of auto racing like the Super GT series and Formula Nippon. The country has produced one NBA player, Yuta Tabuse.

Television and newspapers take an important role in Japanese mass media, though radio and magazines also take a part. For a long time, newspapers were regarded as the most influential information medium in Japan, although audience attitudes towards television changed with the emergence of commercial news broadcasting in the mid-1980s. Over the last decade, television has clearly come to surpass newspapers as Japan's main information and entertainment medium.

There are 6 nationwide television networks: NHK (public broadcasting), Nippon Television (NTV), Tokyo Broadcasting System (TBS), Fuji Network System (FNS), TV Asahi (EX) and TV Tokyo Network (TXN). For the most part, television networks were established based on capital investments by existing radio networks. Variety shows, serial dramas, and news constitute a large percentage of Japanese television show. According to the 2015 NHK survey on television viewing in Japan, 79 percent of Japanese watch television every day. The average daily duration of television viewing was three hours.

Japanese readers have a choice of approximately 120 daily newspapers with a total of 50 million copies of set paper with an average subscription rate of 1.13 newspapers per household. The main newspaper's publishers are Yomiuri Shimbun, Asahi Shimbun, Mainichi Shimbun, Nikkei Shimbun and Sankei Shimbun. According to a survey conducted by the Japanese Newspaper Association in June 1999, 85.4 per cent of men and 75 per cent of women read a newspaper every day. Average daily reading times vary with 27.7 minutes on weekdays and 31.7 minutes on holidays and Sunday.





</doc>
<doc id="15575" url="https://en.wikipedia.org/wiki?curid=15575" title="Geography of Japan">
Geography of Japan

Japan is an island nation in East Asia comprising a volcanic archipelago extending along the continent's Pacific coast. It lies between 24° to 46° north latitude and from 123° to 146° east longitude. Japan is southeast of the Russian Far East, separated by the Sea of Okhotsk; slightly east of the Korean Peninsula, separated by the Sea of Japan; and east-northeast of China and Taiwan, separated by the East China Sea. The closest neighboring country to Japan is Russia.

The major islands, sometimes called the "Home Islands", are (from east to west) Hokkaido, Honshu (the "mainland"), Shikoku and Kyushu. There are 6,852 islands in total, including the Nansei Islands, the Nanpō Islands and islets, with 430 islands being inhabited and others uninhabited. In total, as of 2006, Japan's territory is , of which is land and water.

Location: Eastern Asia, island chain between the North Pacific Ocean and the Sea of Japan, east of the Korean Peninsula.

Map references: Asia, Oceania

Area:

Land boundaries: the ocean

Coastline: 

Maritime claims: 

Climate: varies from tropical in south to cool temperate in north

Terrain: mostly rugged and mountainous, can easily be compared to Norway, both having about 70% of their land in the mountains.

Natural resources: small deposits of coal, oil, iron, and minerals. Major fishing industry.

Land use: 

Irrigated land: 25,000 km² (2010)

Total renewable water resources: 430 km (2011)

Freshwater withdrawal (domestic/industrial/agricultural):

About 73% of Japan is mountainous, with a mountain range running through each of the main islands. Japan's highest mountain is Mount Fuji, with an elevation of . Japan's forest cover rate is 68.55% since the mountains are heavily forested. The only other developed nations with such a high forest cover percentage are Finland and Sweden.

Since there is little level ground, many hills and mountainsides at lower elevations around towns and cities are often cultivated. As Japan is situated in a volcanic zone along the Pacific deeps, frequent low-intensity earth tremors and occasional volcanic activity are felt throughout the islands. Destructive earthquakes occur several times a century. Hot springs are numerous and have been exploited as an economic capital by the leisure industry.

The mountainous islands of the Japanese archipelago form a crescent off the eastern coast of Asia. They are separated from the mainland by the Sea of Japan, which historically served as a protective barrier. The country consists of four major islands: Hokkaido, Honshu, Shikoku, and Kyushu; with more than 6,500 adjacent smaller islands and islets ("island" defined as land more than 100 m in circumference), including the Izu Islands and Ogasawara Islands in the Nanpō Islands, and the Satsunan Islands, Okinawa Islands, and Sakishima Islands of the Ryukyu Islands.

The national territory also includes the Volcano Islands (Kazan Retto) such as Iwo Jima, located some 1,200 kilometers south of mainland Tokyo. A territorial dispute with Russia, dating from the end of World War II, over the two southernmost of the Kuril Islands, Etorofu and Kunashiri, and the smaller Shikotan Island and Habomai Islands northeast of Hokkaido remains a sensitive spot in Japanese–Russian relations (). Excluding disputed territory, the archipelago covers about 377,000 square kilometers. No point in Japan is more than 150 kilometers from the sea.

The four major islands are separated by narrow straits and form a natural entity. The Ryukyu Islands curve 970 kilometers southward from Kyūshū.

The distance between Japan and the Korean Peninsula, the nearest point on the Asian continent, is about 200 kilometers at the Korea Strait. Japan has always been linked with the continent through trade routes, stretching in the north toward Siberia, in the west through the Tsushima Islands to the Korean Peninsula, and in the south to the ports on the south China coast.

The Japanese islands are the summits of mountain ridges uplifted near the outer edge of the continental shelf. About 73 percent of Japan's area is mountainous, and scattered plains and intermontane basins (in which the population is concentrated) cover only about 27 percent. A long chain of mountains runs down the middle of the archipelago, dividing it into two halves, the "face", fronting on the Pacific Ocean, and the "back", toward the Sea of Japan. On the Pacific side are steep mountains 1,500 to 3,000 meters high, with deep valleys and gorges.

Central Japan is marked by the convergence of the three mountain chains—the Hida, Kiso, and Akaishi mountains—that form the Japanese Alps (Nihon Arupusu), several of whose peaks are higher than 3,000 meters. The highest point in the Japanese Alps is Mount Kita at 3,193 meters. The highest point in the country is Mount Fuji (Fujisan, also erroneously called Fujiyama), a volcano dormant since 1707 that rises to 3,776 meters above sea level in Shizuoka Prefecture. On the Sea of Japan side are plateaus and low mountain districts, with altitudes of 500 to 1,500 meters.

None of the populated plains or mountain basins are extensive in area. The largest, the Kantō Plain, where Tokyo is situated, covers only 13,000 square kilometers. Other important plains are the Nōbi Plain surrounding Nagoya, the Kinai Plain in the Osaka–Kyoto area, the Sendai Plain around the city of Sendai in northeastern Honshū, and the Ishikari Plain on Hokkaidō. Many of these plains are along the coast, and their areas have been increased by reclamation throughout recorded history.

The small amount of habitable land has prompted significant human modification of the terrain over many centuries. Land was reclaimed from the sea and from river deltas by building dikes and drainage, and rice paddies were built on terraces carved into mountainsides. The process continued in the modern period with extension of shorelines and building of artificial islands for industrial and port development, such as Port Island in Kobe and the new Kansai International Airport in Osaka Bay. Hills and even mountains have been razed to provide flat areas for housing.

Rivers are generally steep and swift, and few are suitable for navigation except in their lower reaches. Although most rivers are less than 300 kilometers in length, their rapid flow from the mountains provides a valuable, renewable resource: hydroelectric power generation. Japan's hydroelectric power potential has been exploited almost to capacity. Seasonal variations in flow have led to extensive development of flood control measures. Most of the rivers are very short. The longest, the Shinano River, which winds through Nagano Prefecture to Niigata Prefecture and flows into the Sea of Japan, is only 367 kilometers long. The largest freshwater lake is Lake Biwa, northeast of Kyoto.

Extensive coastal shipping, especially around the Seto Inland Sea (Seto Naikai), compensates for the lack of navigable rivers. The Pacific coastline south of Tokyo is characterized by long, narrow, gradually shallowing inlets produced by sedimentation, which has created many natural harbors. The Pacific coastline north of Tokyo, the coast of Hokkaidō, and the Sea of Japan coast are generally unindented, with few natural harbors.

In November 2008, Japan filed a request to expand its claimed continental shelf. In April 2012, the U.N. Commission on the Limits of the Continental Shelf recognized around of seabed around Okinotorishima, giving Japan priority over access to seabed resources in nearby areas. According to U.N. Commission on the Limits of the Continental Shelf, the approved expansion is equal to about 82% of Japan's total land area. The People's Republic of China and South Korea have opposed Japan's claim because they view Okinotorishima not as an island, but as a group of rocks.

Most regions of Japan, such as much of Honshu, Shikoku and Kyushu, belong to the temperate zone with humid subtropical climate (Köppen climate classification "Cfa") characterized by four distinct seasons. However, its climate varies from cool humid continental climate (Köppen climate classification "Dfb") in the north such as northern Hokkaido, to warm tropical rainforest climate (Köppen climate classification "Af") in the south such as Ishigaki in the Yaeyama Islands.

The two primary factors influences in Japan's climate are a location near the Asian continent and the existence of major oceanic currents. Two major ocean currents affect Japan: the warm Kuroshio Current (Black Current; also known as the Japan Current); and the cold Oyashio Current (Parent Current; also known as the Okhotsk Current). The Kuroshio Current flows northward on the Pacific side of Japan and warms areas as north as the South Kantō region; a small branch, the Tsushima Current, flows up the Sea of Japan side. The Oyashio Current, which abounds in plankton beneficial to coldwater fish, flows southward along the northern Pacific, cooling adjacent coastal areas. The intersection of these currents at 36 north latitude is a bountiful fishing ground.

Japan's varied geographical features divide it into six principal climatic zones.

Japan is generally a rainy country with high humidity. Because of its wide range of latitude, seasonal winds and different types of ocean currents, Japan has a variety of climates, with a latitude range of the inhabited islands from 24° to 46° north, which is comparable to the range between Nova Scotia and The Bahamas in the east coast of North America. Tokyo is at about 35 degrees north latitude, comparable to that of Tehran, Athens, or Las Vegas.

Regional climatic variations range from humid continental in the northern island of Hokkaido extending down through northern Japan to the Central Highland, then blending with and eventually changing to a humid subtropical climate on the Pacific Coast and ultimately reaching tropical rainforest climate on the Yaeyama Islands of the Ryukyu Islands. Climate also varies dramatically with altitude and with location on the Pacific Ocean or on the Sea of Japan.

Northern Japan has warm summers but long, cold winters with heavy snow. Central Japan in its elevated position, has hot summers and moderate to short winters with some areas having very heavy snow, and southwestern Japan has long hot summer and short mild winters. The generally temperate climate exhibits marked seasonal variation such as the blooming of the spring cherry blossoms, the calls of the summer cicada and fall foliage colors that are celebrated in art and literature.

The climate from June to September is marked by hot, wet weather brought by tropical airflows from the Pacific Ocean and Southeast Asia. These airflows are full of moisture and deposit substantial amounts of rain when they reach land. There is a marked rainy season, beginning in early June and continuing for about a month. It is followed by hot, sticky weather. Five or six typhoons pass over or near Japan every year from early August to early October, sometimes resulting in significant damage. Annual precipitation averages between except for the areas such as Kii Peninsula and Yakushima Island which is Japan's wettest place with the annual precipitation being one of the world's highest at 4,000 to 10,000 mm.

Maximum precipitation, like the rest of East Asia, occurs in the summer months except on the Sea of Japan coast where strong northerly winds produce a maximum in late autumn and early winter. Except for a few sheltered inland valleys during December and January, precipitation in Japan is above of rainfall equivalent in all months of the year, and in the wettest coastal areas it is above per month throughout the year.

In winter, the Siberian High develops over the Eurasian land mass and the Aleutian Low develops over the northern Pacific Ocean. The result is a flow of cold air southeastward across Japan that brings freezing temperatures and heavy snowfalls to the central mountain ranges facing the Sea of Japan, but clear skies to areas fronting on the Pacific.

Mid June to mid July is generally the rainy season in Honshu, Shikoku and Kyushu, excluding Hokkaidō since the seasonal rain front or dissipates in northern Honshu before reaching Hokkaido. In Okinawa, the rainy season starts early in May and continues until mid June. Unlike the rainy season in mainland Japan, it rains neither everyday nor all day long during the rainy season in Okinawa. Between July and October, typhoons, grown from tropical depressions generated near the equator, can attack Japan with furious rainstorms.

The warmest winter temperatures are found in the Nanpō and Bonin Islands, which enjoy a tropical climate due to the combination of latitude, distance from the Asian mainland, and warming effect of winds from the Kuroshio, as well as the Volcano Islands (at the latitude of the southernmost of the Ryukyu Islands, 24° N). The coolest summer temperatures are found on the northeastern coast of Hokkaidō in Kushiro and Nemuro Subprefectures.

Sunshine, in accordance with Japan’s uniformly heavy rainfall, is generally modest in quantity, though no part of Japan receives the consistently gloomy fogs that envelope the Sichuan Basin or Taipei. Amounts range from about six hours per day in the Inland Sea coast and sheltered parts of the Pacific Coast and Kantō Plain to four hours per day on the Sea of Japan coast of Hokkaidō. In December there is a very pronounced sunshine gradient between the Sea of Japan and Pacific coasts, as the former side can receive less than 30 hours and the Pacific side as much as 180 hours. In summer, however, sunshine hours are lowest on exposed parts of the Pacific coast where fogs from the Oyashio current create persistent cloud cover similar to that found on the Kuril Islands and Sakhalin.

As an island nation, Japan has the 7th longest coastline in the world. A few prefectures are landlocked: Gunma, Tochigi, Saitama, Nagano, Yamanashi, Gifu, Shiga, and Nara. As Mt. Fuji and the coastal Japanese Alps provide a rain shadow, Nagano and Yamanashi Prefectures receive the least precipitation in Honshu, though it still exceeds annually. A similar effect is found in Hokkaido, where Okhotsk Subprefecture receives as little as per year. All other prefectures have coasts on the Pacific Ocean, Sea of Japan, Seto Inland Sea or have a body of salt water connected to them. Two prefectures—Hokkaido and Okinawa—are composed entirely of islands.

The hottest temperature ever measured in Japan, , occurred in Kumagaya on 23 July 2018. and the coldest was recorded at Asahikawa, Hokkaidō on 25 January 1902.

Environment - current issues: In the 2006 environment annual report, the Ministry of Environment reported that current major issues are: global warming and preservation of the ozone layer, conservation of the atmospheric environment, water and soil, waste management and recycling, measures for chemical substances, conservation of the natural environment and the participation in the international cooperation.

Environment - international agreements: <br>
"party to": Antarctic-Environmental Protocol, Antarctic Treaty, Biodiversity, Climate Change, Desertification, Endangered Species, Environmental Modification, Hazardous Wastes (Basel Convention), Law of the Sea, Marine Dumping, Ozone Layer Protection (Montreal Protocol), Ship Pollution (MARPOL 73/78), Tropical Timber 83, Tropical Timber 94, Wetlands (Ramsar Convention), Whaling <br>
"signed and ratified": Climate Change-Kyoto Protocol

Ten percent of the world's active volcanoes—forty in the early 1990s (another 148 were dormant)—are found in Japan, which lies in a zone of extreme crustal instability. As many as 1,500 earthquakes are recorded yearly, and magnitudes of 4 to 7 are common. Minor tremors occur almost daily in one part of the country or another, causing slight shaking of buildings. Major earthquakes occur infrequently; the most famous in the twentieth century was the great Kantō earthquake of 1923, in which 130,000 people died. Undersea earthquakes also expose the Japanese coastline to danger from . 

On March 11, 2011 Japan was subject to a devastating magnitude 9.0 earthquake and a massive tsunami as a result. The March 11 quake was the largest ever recorded in Japan and is the world's fourth largest earthquake to strike since 1900, according to the U.S. Geological Service. It struck offshore about northeast of Tokyo and east of the city of Sendai, and created a massive tsunami that devastated Japan's northeastern coastal areas. At least 100 aftershocks registering a 6.0 magnitude or higher have followed the main shock. At least 15,000 people died as a result.

Japan has become a world leader in research on causes and prediction of earthquakes. The development of advanced technology has permitted the construction of skyscrapers even in earthquake-prone areas. Extensive civil defence efforts focus on training in protection against earthquakes, in particular against accompanying fire, which represents the greatest danger.

Another common hazard are several typhoons that reach Japan from the Pacific every year and heavy snowfall during winter in the snow country regions, causing landslides, flooding, and avalanches.
Japan is informally divided into eight regions. Each contains several prefectures, except the Hokkaido region, which covers only Hokkaido Prefecture.

The region is not an official administrative unit, but has been traditionally used as the regional division of Japan in a number of contexts: for example, maps and geography textbooks divide Japan into the eight regions, weather reports usually give the weather by region, and many businesses and institutions use their home region as part of their name (Kinki Nippon Railway, Chūgoku Bank, Tohoku University, etc.). While Japan has eight High Courts, their jurisdictions do not correspond to the eight regions.

This is a list of the extreme points of Japan, the points that are farther north, south, east or west than any other location.




The only part of Japan with antipodes over land are the Ryukyu Islands, though the islands off the western coast of Kyūshū are close.

The northernmost antipodal island in the Ryukyu Island chain, Nakanoshima, is opposite the Brazilian coast near Capão da Canoa. The other islands south to the Straits of Okinawa correspond to southern Brazil, with Gaja Island being opposite the outskirts of Santo Antônio da Patrulha, Takarajima with Jua, Amami Ōshima covering the villages of Carasinho and Fazenda Pae João, Ginoza, Okinawa with Palmas, Paraná, the Kerama Islands with Pato Branco, Tonaki Island with São Lourenço do Oeste, and Kume Island corresponding to Palma Sola. The main Daitō Islands correspond to near Guaratuba, with Oki Daitō Island near Apiaí.

The Sakishima Islands beyond the straits are antipodal to Paraguay, from the Brazilian border almost to Asunción, with Ishigaki overlapping San Isidro de Curuguaty, and the uninhabited Senkaku Islands surrounding Villarrica.





</doc>
<doc id="15576" url="https://en.wikipedia.org/wiki?curid=15576" title="Demography of Japan">
Demography of Japan

The demographic features of the population of Japan include population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects regarding the population.

For information on historical demographic data in Japan prior to 1945 refer to:

Based on the census from October 2010, Japan's population was at its peak at 128,057,352. As of October 1, 2015, the population was 127,094,745 making it the world's tenth-most populous country at the time. It had declined by 0.8 percent from the time of the census five years ago, the first time it had declined since the 1945 census. Japan's population size can be attributed to high growth rates experienced during the late 19th and early 20th centuries.

Since 2010, Japan has experienced net population loss due to falling birth rates and almost no immigration, despite having one of the highest life expectancies in the world, at 85.00 years as of 2016 (it was 81.25 as of 2006). Using the annual estimate for October of each year, the population peaked in 2008 at 128,083,960 and had fallen 285,256 by October 2011. Japan's population density was 336 people per square kilometer.

Based on 2012 data from the National Institute of Population and Social Security Research, Japan's population will keep declining by about one million people every year in the coming decades, which will leave Japan with a population of 42 million in 2110. More than 40% of the population is expected to be over age 65 in 2060. In 2012, the population had for six consecutive years declined by 212,000, the largest drop on record since 1947 and also a record low of 1.03 million births. In 2014, a new record of population drop happened with 268,000 people. In 2013, more than 20 percent of the population are age 65 and over.

The population ranking of Japan dropped from 7th to 8th in 1990, to 9th in 1998, and to 10th in the early 21st century. In 2015 it dropped further to 11th place, according to both the UN and PRB. Over the period of 2010–2015, the population shrank by almost a million.

Japan collects census information every five years. The exercise is conducted by the Statistics Bureau of the Ministry of Internal Affairs.

Japan's population density was 340.8 people per square kilometer (874 people per square mile) according to World Development Indicators. It ranks 35th in a list of countries by population density, ranking directly above Philippines (347 per km) and directly below Curacao (359 per km). Between 1955 and 1989, land prices in the six largest cities increased 15,000% (+12% a year). Urban land prices generally increased 40% from 1980 to 1987; in the six largest cities, the price of land doubled over that period. For many families, this trend put housing in central cities out of reach.
The result was lengthy commutes for many workers in the big cities, especially in Tokyo area where daily commutes of two hours each way are common. In 1991, as the bubble economy started to collapse, land prices began a steep decline, and within a few years fell 60% below their peak. After a decade of declining land prices, residents began moving back into central city areas (especially Tokyo's 23 wards), as evidenced by 2005 census figures. Despite nearly 70% of Japan being covered by forests, parks in many major cities—especially Tokyo and Osaka—are smaller and scarcer than in major West European or North American cities. As of 2014, parkland per inhabitant in Tokyo is 5.78 square meters, which is roughly half of the 11.5 square meters of Madrid.

National and regional governments devote resources to making regional cities and rural areas more attractive by developing transportation networks, social services, industry, and educational institutions in attempts to decentralize settlement and improve the quality of life. Nevertheless, major cities, especially Tokyo, Yokohama, and Fukuoka, and to a lesser extent Kyoto, Osaka and Nagoya, remain attractive to young people seeking education and jobs.

Japan has a high population concentration in urban areas on the plains since 75% of Japan’s land area is made up of mountains, and also Japan has a forest cover rate of 68.5% (the only other developed countries with such a high forest cover percentage are Finland and Sweden).
The 2010 census shows 90.7% of the total Japanese population live in cities.

Japan is an urban society with about only 5% of the labor force working in agriculture. Many farmers supplement their income with part-time jobs in nearby towns and cities. About 80 million of the urban population is heavily concentrated on the Pacific shore of Honshu.

Metropolitan Tokyo-Yokohama, with its population of 35 million residents, is the world's most populous city. Japan faces the same problems that confront urban industrialized societies throughout the world: overcrowded cities and congested highways.

Japan's population is aging faster than any other nation. The population of those 65 years or older roughly doubled in 24 years, from 7.1% of the population in 1970 to 14.1% in 1994. The same increase took 61 years in Italy, 85 years in Sweden, and 115 years in France. In 2014, 26% of Japan's population was estimated to be 65 years or older, and the Health and Welfare Ministry has estimated that over-65s will account for 40% of the population by 2060. The demographic shift in Japan's age profile has triggered concerns about the nation's economic future and the viability of its welfare state.

Population in 5 households, 78.7% in urban areas (July 2000). High population density; 329.5 people per square kilometer for total area; 1,523 persons per square kilometer for habitable land. More than 50% of the population lives on 2% of the land. (July 1993)

Adult prevalence rate

People living with HIV/AIDS

Deaths

Naturalized Japanese citizens and native-born Japanese nationals with multi-ethnic background are all considered to be Japanese in the population census of Japan, therefore no ethnicity census data is available.

Live births, birth and death rates and overall fertility rate in Japan from 1899 to present.

2012 (and 2011) update:

Japan's total fertility rate (TFR) in 2012 was estimated at 1.41 children per woman, increasing slightly from 1.32 in the 2001–05 period. In 2012, the highest TFR was 1.90, in Okinawa, and the lowest was 1.09, in Tokyo. TFR by prefecture for 2000–05, as well as future estimates, have been released.

Between 6 million and 7 million people moved their residences each year during the 1980s. About 50% of these moves were within the same prefecture; the others were relocations from one prefecture to another. During Japan's economic development in the twentieth century, and especially during the 1950s and 1960s, migration was characterized by urbanization as people from rural areas in increasing numbers moved to the larger metropolitan areas in search of better jobs and education. Out-migration from rural prefectures continued in the late 1980s, but more slowly than in previous decades.

In the 1980s, government policy provided support for new urban development away from the large cities, particularly Tokyo, and assisted regional cities to attract young people to live and work there. Regional cities offered familiarity to those from nearby areas, lower costs of living, shorter commutes, and, in general, a more relaxed lifestyle than could be had in larger cities. Young people continued to move to large cities, however, to attend universities and find work, but some returned to regional cities (a pattern known as U-turn) or to their prefecture of origin (a pattern referred to as "J-turn").

Government statistics show that in the 1980s significant numbers of people left the largest central cities (Tokyo and Osaka) to move to suburbs within their metropolitan areas. In 1988 more than 500,000 people left Tokyo, which experienced a net loss through migration of nearly 73,000 for the year. Osaka had a net loss of nearly 36,000 in the same year.

Due to decreasing total population of the country, only 8 prefectures show increase in population. These are Okinawa(2.9%), Tokyo(2.7%), Aichi(1.0%), Saitama(1.0%), Kanagawa(0.9%), Fukuoka(0.6%),Shiga(0.2%), and Chiba(0.1%).

About 663,300 Japanese were living abroad, approximately 75,000 of whom had permanent foreign residency, more than six times the number who had that status in 1975. More than 200,000 Japanese went abroad in 1990 for extended periods of study, research, or business assignments. As the government and private corporations have stressed internationalization, greater numbers of individuals have been directly affected, decreasing Japan's historical insularity. By the late 1980s, these problems, particularly the bullying of returnee children in schools, had become a major public issue both in Japan and in Japanese communities abroad.

Cities with significant populations of Japanese nationals

Note: The above data shows the number of Japanese nationals living overseas. It was published by the Ministry of Foreign Affairs of Japan and relates to 2015.

According to the Japanese immigration centre, the number of foreign residents in Japan has steadily increased, and the number of foreign residents (excluding a small number of illegal immigrants and short-term visitors, such as foreign nationals staying less than 90 days in Japan), exceeded 2.2 million people in 2008.

In 2010, the number of foreigners in Japan was 2,134,151. This includes 209,373 Filipinos, many of whom are married to Japanese nationals, 210,032 Brazilians, the majority possessing some degree of Japanese ancestry, 687,156 Chinese and 565,989 Koreans. Chinese, Filipinos, Koreans, and Brazilians account for about 69.5% of foreign residents in Japan.

The current issue of the shrinking workforce in Japan alongside its aging population has resulted in a recent need to attract foreign labour to the country. Reforms which took effect in 2015 relax visa requirements for "Highly Skilled Foreign Professionals" and create a new type of residence status with an unlimited period of stay.

The number of naturalizations peaked in 2008 at 16,000, declining to over 9,000 in the most recent year for which data are available. Most of the decline is accounted for by a steep reduction in the number of Japan-born Koreans taking Japanese citizenship. Historically the bulk of those taking Japanese citizenship have not been foreign-born immigrants but rather Japanese-born descendants of Koreans and Taiwanese who lost their citizenship in the Japanese Empire in 1947 as part of the American Occupation policy for Japan.

Japanese statistical authorities do not collect information on ethnicity, only nationality. As a result, both native and naturalized Japanese citizens are counted in a single group. Although official statistics show near homogeneity, one analysis describe the population as “multi-ethnic”, although unofficial statistics still show that ethnic minorities are small compared with many other countries.

In 2015 the Japanese government under prime minister Shinzō Abe announced that its policy of restricting immigration would not change despite the current declining population. In the long term, its plan is to improve technology to address the labour shortage, while increasing Japanese fertility rates from the current level of 1.4 to 1.8, eventually stabilizing the population at approximately 100 million.

The Japanese society of Yamato people is linguistically homogeneous with small populations of Koreans (0.9 million), Chinese/Taiwanese (0.65 million), Filipino (306,000 some being Japanese Filipino; children of Japanese and Filipino parentage). Brazilians (300,000, many of whom are ethnically Japanese) as well as Peruvians and Argentineans of both Latin American and Japanese descent. Japan has indigenous minority groups such as the Ainu and Ryukyuans, who generally speak Japanese.

Japanese citizenship is conferred "jus sanguinis", and monolingual Japanese-speaking minorities often reside in Japan for generations under permanent residency status without acquiring citizenship in their country of birth, although legally they are allowed to do so. This is because Japanese law does not recognise dual citizenship after the age of adulthood, and so people becoming naturalised Japanese citizens must relinquish citizenship of other countries when they reach the age of 20. Some ethnic Koreans and Chinese and their descendants (who may speak only Japanese and may never have even visited the country whose nationality they hold) do not wish to abandon this other citizenship.

In addition, people taking Japanese citizenship must take a name using the Japanese character sets hiragana, katakana, and/or kanji. Names using Western alphabet, Korean alphabet, Arabic characters, etc. are not acceptable as legal names. Chinese characters are usually legally acceptable as nearly all Chinese characters are recognized as valid by the Japanese government. Transliterations of non-Japanese names using katakana (e.g. 　"" for "Smith") are also legally acceptable.

However, some naturalizing foreigners feel that becoming a Japanese citizen should mean that they have a Japanese name and that they should abandon their foreign name, and some foreign residents do not wish to do this—although most Special Permanent Resident Koreans and Chinese already use Japanese names. Nonetheless, some 10,000 Zainichi Koreans naturalize every year. Approximately 98.6% of the population are Japanese citizens, and 99% of the population speak Japanese as their first language. Non-ethnic Japanese in the past, and to an extent in the present, also live in small numbers in the Japanese archipelago.

Japanese people enjoy a high standard of living, and nearly 90% of the population consider themselves part of the middle class. However, many studies on happiness and satisfaction with life tend to find that Japanese people average relatively low levels of life satisfaction and happiness when compared with most of the highly developed world; the levels have remained consistent if not declining slightly over the last half century. Japanese have been surveyed to be relatively lacking in financial satisfaction.

The suicide rates per 100,000 in Japan in 2009 were 29.2 for men and 10.5 for women. In 2010, 32,000 Japanese committed suicide, which translates to an average of 88 Japanese suicides a day in 2010.

Three native Japanese minority groups can be identified. The largest are the "hisabetsu buraku" or "discriminated communities", also known as the "burakumin". These descendants of premodern outcast hereditary occupational groups, such as butchers, leatherworkers, funeral directors, and certain entertainers, may be considered a Japanese analog of India's Dalits. Discrimination against these occupational groups arose historically because of Buddhist prohibitions against killing and Shinto notions of pollution, as well as governmental attempts at social control.

During the Tokugawa period, such people were required to live in special "buraku" and, like the rest of the population, were bound by sumptuary laws based on the inheritance of social class. The Meiji government abolished most derogatory names applied to these discriminated communities in 1871, but the new laws had little effect on the social discrimination faced by the former outcasts and their descendants. The laws, however, did eliminate the economic monopoly they had over certain occupations. The "buraku" continued to be treated as social outcasts and some casual interactions with the majority caste were perceived taboo until the era after World War II.

Estimates of their number range from 2 to 4 million (about 2% to 3% of the national population). Although members of these discriminated communities are physically indistinguishable from other Japanese, they often live in urban ghettoes or in the traditional special hamlets in rural areas, and membership can be surmised from the location of the family home, occupation, dialect, or mannerisms. Checks on family background designed to ferret out "buraku" were commonly performed as part of marriage arrangements and employment applications, but have been illegal since 1985 in Osaka.

Past and current discrimination has resulted in lower educational attainment and socioeconomic status among "hisabetsu buraku" than among the majority of Japanese. Movements with objectives ranging from "liberation" to encouraging integration have tried to change this situation, with some success. Nadamoto Masahisa of the Buraku History Institute estimates that as of 1998, between 60 and 80% of burakumin marry a non-burakumin.

One of the largest minority groups among Japanese citizens is the Ryukyuan people. They are primarily distinguished from their use of several distinct Ryukyuan languages though use of Ryukyuan is dying out. The Ryukyuan people and language originated in the Ryukyu Islands, which are in Okinawa prefecture.

The third largest minority group among Japanese citizens is the Ainu, whose language is an isolate. Historically, the Ainu were an indigenous hunting and gathering population who occupied most of northern Honshū as late as the Nara period (A.D. 710–94). As Japanese settlement expanded, the Ainu were pushed northward, by the Tokugawa shogunate, the Ainu were pushed into the island of Hokkaido.

Characterized as remnants of a primitive circumpolar culture, the fewer than 20,000 Ainu in 1990 were considered racially distinct and thus not fully Japanese. Disease and a low birth rate had severely diminished their numbers over the past two centuries, and intermarriage had brought about an almost completely mixed population.

Although no longer in daily use, the Ainu language is preserved in epics, songs, and stories transmitted orally over succeeding generations. Distinctive rhythmic music and dances and some Ainu festivals and crafts are preserved, but mainly in order to take advantage of tourism.

In 2005, there were 1,555,505 foreign residents in Japan, representing 1.22% of the Japanese population. Foreign Army personnel, of which there were up to 430,000 from the SCAP (post-occupation, United States Forces Japan) and 40,000 BCOF in the immediate post-war years, have not been at any time included in Japanese foreign resident statistics. Most foreign residents in Japan come from Brazil or from other Asian countries, particularly from China, South Korea, the Philippines, Vietnam and Nepal.

A number of long-term resident Koreans in Japan today retain familial links with the descendants of Koreans, that either immigrated voluntarily or were forcibly relocated during the Japanese Occupation of the Korea. Within this group, a number hold Special Permanent Resident status, granted under the terms of the Normalisation Treaty (22. June 1965) between South Korea and Japan. In many cases special residents, despite being born in Japan and speaking Japanese, have chosen not to take advantage of the mostly automatic granting of citizenship to special resident applicants.

Beginning in 1947 the Japanese government started to repatriate Korean nationals, who had nominally been granted Japanese citizenship during the years of military occupation. When the Treaty of San Francisco came into force many ethnic Koreans lost their Japanese citizenship from April 28, 1952 and with it the right to welfare grants, to hold a government job of any kind or to attend Japanese schools. In the following year the government contrived, with the help of the Red Cross, a scheme to "repatriate" Korean residents, who mainly were from the Southern Provinces, to their "home" of North Korea. Between 1959 and 1984 93,430 people used this route. 6,737 were Japanese or Chinese dependents. Most of these departures – 78,276 – occurred before 1962.

All non-Japanese without special residential status (people whose residential roots go back to before WWII) are required by law to register with the government and carry alien registration cards. From the early 1980s, a civil disobedience movement encouraged refusal of the fingerprinting that accompanied registration every five years.

Opponents of fingerprinting argued that it was discriminatory because the only Japanese who were fingerprinted were criminals. The courts upheld fingerprinting, but the law was changed so that fingerprinting was done once rather than with each renewal of the registration, which until a law reform in 1989 was usually required every six months for anybody from the age of 16. Those refusing fingerprinting were denied re-entry permits, thus depriving them of freedom of movement.

Of these foreign residents below, the new wave started 2014 comes to Japan as students or trainees. These foreigners are registered under student visa or trainee visa which gives them the student residency status, Most of these new foreigners are under this visa. Almost all of these foreign students and trainees will return to their home country after 3–4 years (one valid period), few students extend their visa. Vietnamese makes the largest increase, however Burmese, Cambodians, Filipinos and Chinese are also increasing.

Asian migrant wives of Japanese men have also contributed to the foreign-born population in the country. Many young single Japanese male farmers choose foreign wives, mainly from the Philippines, Sri Lanka, Thailand, China and South Korea, due to a lack of interest from Japanese women living a farming life. Migrant wives often travel as mail-order brides as a result of arranged marriages with Japanese men. Additionally, Japanese men in urban parts of the country have also begun marrying foreign Asian women.

There was an increase of 110,358 foreign residents from 2014 to 2015. Vietnamese made the largest proportion of these new foreign residents, whilst Nepalese, Filipino, Chinese and Taiwanese are also significant in numbers. Together these countries makes up 91,126 or 82.6% of all new residents from 2014 to 2015. However, the majority of these immigrants will only remain in Japan for a maximum of five years, as many of them have entered the country in order to complete trainee programmes. Once they complete their programmes, they will be required to return to their home countries.

As of December 2014 there were 2,121,831 foreigners residing in Japan, 677,019 of whom were long-term residents in Japan, according to national demographics figures. The majority of long-term residents were from Asia, totalling 478,953. Chinese made up the largest portion of them with 215,155, followed by Filipinos with 115,857, and Koreans with 65,711. Thai, Vietnamese, and Taiwanese long-term residents totaled 47,956, and those from other Asian countries totaled 34,274. The Korean figures do not include zainichi Koreans with "tokubetsu eijusha" ("special permanent resident") visas, of whom there were 354,503 (of a total of 358,409 of all nationalities with such visas). The total number of permanent residents had declined over the previous 5 years due to high cost of living.

A significant number of foreign residents of Japan are employed on a short term contractual basis under programs administered by the Japanese government. Well known programs include:


In the light of current demographic trends Japan is likely to experience a decrease in tax revenue without a corresponding decrease in welfare expenses for an increasingly elderly population. Given growing manpower shortages, immigrant workers continue to play an important role taking low skilled and manual labour jobs. A recent growth in blue collar employment using documented short term contractual labour from developing countries has also contributed to the rise in the resident foreign population. The government administered Technical Intern Training Program, first established in 1993, provided over 190,000 short term contracted workers in 2015. However, it has been claimed that many of these workers often work at reduced pay and are required to undertake significant amounts of overtime in order to make up for labor shortages. As trainees, labor standards law and minimum wage legislation has on occasion been ignored by unscrupulous employers. The Japanese government has begun to examine this problem and has sought to both strengthen the vocational training aspect of the work program oversight.

Foreign residents were recorded only in an alien registration system separate from the "koseki" (family registry) and "jūminhyō" (resident registry) systems in which Japanese citizens were registered until a new registration system was enacted in July 2012. Since then, all residents are recorded by municipal offices in the "jūminhyō" system. The "koseki" system continues for Japanese citizens, while foreigners are recorded in a separate residency management system administered by immigration offices which combines the previous immigration status and local alien registration systems.

The Japanese Ministry of Justice maintains a website and hotline for "receiving report on illegal stay foreigner." The criteria for reporting include "feeling anxious about a foreigner", and anonymous submissions are permitted. Japanese immigration authorities work in unison with police to investigate those reported, and human rights groups such as Amnesty International have argued that those reported do not receive proper legal protection.

The Daiyo Kangoku system allows police to detain suspects without charges, access to legal counsel or telephone calls for up to 23 days. In October 2006, the foreigner reporting hotline's operating hours were extended to include Saturday, Sunday and national holidays.

As of November 20, 2007, all foreigners entering Japan must be biometrically registered (photograph and fingerprints) on arrival; this includes people living in Japan on visas as well as permanent residents, but excludes people with special permanent resident permission, diplomats, and those under 16.

Shinto and Buddhism are Japan's two major religions. They have co-existed for more than a thousand years. However, most Japanese identify as either atheists, irreligious, or do not identify themselves as adherents of one religion, but rather incorporate various elements in a syncretic fashion. There are small Christian and other minorities as well, with the Christian population dating to as early as the 1500s, as a result of European missionary work before sakoku was implemented from 1635–1853.





</doc>
<doc id="15577" url="https://en.wikipedia.org/wiki?curid=15577" title="Politics of Japan">
Politics of Japan

The politics of Japan are conducted in a framework of a multi-party bicameral parliamentary representative democratic constitutional monarchy whereby the Emperor is the ceremonial head of state and the Prime Minister is the head of government and the head of the Cabinet, which directs the executive branch.

Legislative power is vested in the National Diet, which consists of the House of Representatives and the House of Councillors. Judicial power is vested in the Supreme Court and lower courts, and sovereignty is vested in the Japanese people by the Constitution. Japan is considered a constitutional monarchy with a system of civil law.

The Economist Intelligence Unit rated Japan as a "flawed democracy" in 2016.

The Constitution of Japan defines the Emperor to be "the symbol of the State and of the unity of the people". He performs ceremonial duties and holds no real power. Political power is held mainly by the Prime Minister and other elected members of the Diet. The Imperial Throne is succeeded by a member of the Imperial House as designated by the Imperial Household Law.

The chief of the executive branch, the Prime Minister, is appointed by the Emperor as directed by the Diet. He is a member of either house of the Diet and must be a civilian. The Cabinet members are nominated by the Prime Minister, and are also required to be civilian. With the Liberal Democratic Party (LDP) in power, it has been convention that the President of the party serves as the Prime Minister.

Several political parties exist in Japan, however, the politics of Japan have primarily been dominated by the LDP since 1955, with the DPJ playing an important role as opposition several times. LDP was a ruling party during decades since 1955. Despite of existence of multiple parties, other parties were completely ignored. Most of the prime ministers were elected from inner factions of LDP. 

Despite an increasingly unpredictable domestic and international environment, policy making conforms to well established postwar patterns. The close collaboration of the ruling party, the elite bureaucracy and important interest groups often make it difficult to tell who exactly is responsible for specific policy decisions.

After a largely informal process within elite circles in which ideas were discussed and developed, steps might be taken to institute more formal policy development. This process often took place in deliberation councils ("shingikai"). There were about 200 "shingikai", each attached to a ministry; their members were both officials and prominent private individuals in business, education, and other fields. The "shingikai" played a large role in facilitating communication among those who ordinarily might not meet.

Given the tendency for real negotiations in Japan to be conducted privately (in the "nemawashi", or root binding, process of consensus building), the "shingikai" often represented a fairly advanced stage in policy formulation in which relatively minor differences could be thrashed out and the resulting decisions couched in language acceptable to all. These bodies were legally established but had no authority to oblige governments to adopt their recommendations. The most important deliberation council during the 1980s was the Provisional Commission for Administrative Reform, established in March 1981 by Prime Minister Suzuki Zenko. The commission had nine members, assisted in their deliberations by six advisers, twenty-one "expert members," and around fifty "councillors" representing a wide range of groups. Its head, Keidanren president Doko Toshio, insisted that government agree to take its recommendations seriously and commit itself to reforming the administrative structure and the tax system.

In 1982, the commission had arrived at several recommendations that by the end of the decade had been actualized. These implementations included tax reform, a policy to limit government growth, the establishment in 1984 of the Management and Coordination Agency to replace the Administrative Management Agency in the Office of the Prime Minister, and privatization of the state-owned railroad and telephone systems. In April 1990, another deliberation council, the Election Systems Research Council, submitted proposals that included the establishment of single-seat constituencies in place of the multiple-seat system.

Another significant policy-making institution in the early 1990s were the Liberal Democratic Party's Policy Research Council. It consisted of a number of committees, composed of LDP Diet members, with the committees corresponding to the different executive agencies. Committee members worked closely with their official counterparts, advancing the requests of their constituents, in one of the most effective means through which interest groups could state their case to the bureaucracy through the channel of the ruling party. 
"See also:" Industrial policy of Japan; Monetary and fiscal policy of Japan; Mass media and politics in Japan

Political parties had begun to revive almost immediately after the occupation began. Left-wing organizations, such as the Japan Socialist Party and the Japanese Communist Party, quickly reestablished themselves, as did various conservative parties. The old Rikken Seiyūkai and Rikken Minseitō came back as, respectively, the Liberal Party (Nihon Jiyūtō) and the Japan Progressive Party (Nihon Shimpotō). The first postwar elections were held in 1948 (women were given the franchise for the first time in 1947), and the Liberal Party's vice president, Yoshida Shigeru (1878–1967), became prime minister.

For the 1947 elections, anti-Yoshida forces left the Liberal Party and joined forces with the Progressive Party to establish the new Democratic Party (Minshutō). This divisiveness in conservative ranks gave a plurality to the Japan Socialist Party, which was allowed to form a cabinet, which lasted less than a year. Thereafter, the socialist party steadily declined in its electoral successes. After a short period of Democratic Party administration, Yoshida returned in late 1948 and continued to serve as prime minister until 1954.

Even before Japan regained full sovereignty, the government had rehabilitated nearly 80,000 people who had been purged, many of whom returned to their former political and government positions. A debate over limitations on military spending and the sovereignty of the Emperor ensued, contributing to the great reduction in the Liberal Party's majority in the first post-occupation elections (October 1952). After several reorganizations of the armed forces, in 1954 the Japan Self-Defense Forces were established under a civilian director. Cold War realities and the hot war in nearby Korea also contributed significantly to the United States-influenced economic redevelopment, the suppression of communism, and the discouragement of organized labor in Japan during this period.

Continual fragmentation of parties and a succession of minority governments led conservative forces to merge the Liberal Party (Jiyūtō) with the Japan Democratic Party (Nihon Minshutō), an offshoot of the earlier Democratic Party, to form the Liberal Democratic Party (Jiyū-Minshutō; LDP) in November 1955, called 1955 System. This party continuously held power from 1955 through 1993, except for short when it was replaced by a new minority government. LDP leadership was drawn from the elite who had seen Japan through the defeat and occupation. It attracted former bureaucrats, local politicians, businessmen, journalists, other professionals, farmers, and university graduates.

In October 1955, socialist groups reunited under the Japan Socialist Party, which emerged as the second most powerful political force. It was followed closely in popularity by the Kōmeitō, founded in 1964 as the political arm of the Soka Gakkai (Value Creation Society), until 1991, a lay organization affiliated with the Nichiren Shoshu Buddhist sect. The Komeito emphasized the traditional Japanese beliefs and attracted urban laborers, former rural residents, and women. Like the Japan Socialist Party, it favored the gradual modification and dissolution of the Japan-United States Mutual Security Assistance Pact.

The LDP domination lasted until the Diet Lower House elections on 18 July 1993, in which LDP failed to win a majority. A coalition of new parties and existing opposition parties formed a governing majority and elected a new prime minister, Morihiro Hosokawa, in August 1993. His government's major legislative objective was political reform, consisting of a package of new political financing restrictions and major changes in the electoral system. The coalition succeeded in passing landmark political reform legislation in January 1994.

In April 1994, Prime Minister Hosokawa resigned. Prime Minister Tsutomu Hata formed the successor coalition government, Japan's first minority government in almost 40 years. Prime Minister Hata resigned less than two months later. Prime Minister Tomiichi Murayama formed the next government in June 1994 with the coalition of Japan Socialist Party (JSP), the LDP, and the small New Party Sakigake. The advent of a coalition containing the JSP and LDP shocked many observers because of their previously fierce rivalry.

Prime Minister Murayama served from June 1994 to January 1996. He was succeeded by Prime Minister Ryutaro Hashimoto, who served from January 1996 to July 1998. Prime Minister Hashimoto headed a loose coalition of three parties until the July 1998 Upper House election, when the two smaller parties cut ties with the LDP. Hashimoto resigned due to a poor electoral performance by the LDP in the Upper House elections. He was succeeded as party president of the LDP and prime minister by Keizo Obuchi, who took office on 30 July 1998. The LDP formed a governing coalition with the Liberal Party in January 1999, and Keizo Obuchi remained prime minister. The LDP-Liberal coalition expanded to include the New Komeito Party in October 1999.

Prime Minister Obuchi suffered a stroke in April 2000 and was replaced by Yoshirō Mori. After the Liberal Party left the coalition in April 2000, Prime Minister Mori welcomed a Liberal Party splinter group, the New Conservative Party, into the ruling coalition. The three-party coalition made up of the LDP, New Komeito, and the New Conservative Party maintained its majority in the Diet following the June 2000 Lower House elections.

After a turbulent year in office in which he saw his approval ratings plummet to the single digits, Prime Minister Mori agreed to hold early elections for the LDP presidency in order to improve his party's chances in crucial July 2001 Upper House elections. On 24 April 2001, riding a wave of grassroots desire for change, maverick politician Junichiro Koizumi defeated former Prime Minister Hashimoto and other party stalwarts on a platform of economic and political reform.

Koizumi was elected as Japan's 87th Prime Minister on 26 April 2001. On 11 October 2003, Prime Minister Koizumi dissolved the lower house and he was re-elected as the president of the LDP. Likewise, that year, the LDP won the election, even though it suffered setbacks from the new opposition party, the liberal and social-democratic Democratic Party (DPJ). A similar event occurred during the 2004 Upper House elections as well.

In a strong move, on 8 August 2005, Prime Minister Junichiro Koizumi called for a snap election to the lower house, as threatened, after LDP stalwarts and opposition DPJ parliamentarians defeated his proposal for a large-scale reform and privatization of Japan Post, which besides being Japan's state-owned postal monopoly is arguably the world's largest financial institution, with nearly 331 trillion yen of assets. The election was scheduled for 11 September 2005, with the LDP achieving a landslide victory under Junichiro Koizumi's leadership.

The ruling LDP started losing hold since 2006. No prime minister except Koizumi had good public support. On 26 September 2006, new LDP President Shinzō Abe was elected by a special session of the Diet to succeed Junichiro Koizumi as Prime Minister. He was the Japan's youngest post-World War II prime minister and the first born after the war. On 12 September 2007, Abe surprised Japan by announcing his resignation from office. He was replaced by Yasuo Fukuda, a veteran of LDP.

In the meantime, on 4 November 2007, leader of the main opposition party, Ichirō Ozawa announced his resignation from the post of party president, after controversy over an offer to the DPJ to join the ruling coalition in a grand coalition, but has since, with some embarrassment, rescinded his resignation.

On 11 January 2008, Prime Minister Yasuo Fukuda forced a bill allowing ships to continue a refueling mission in the Indian Ocean in support of US-led operations in Afghanistan. To do so, PM Fukuda used the LDP's overwhelming majority in the Lower House to ignore a previous "no-vote" of the opposition-controlled Upper House. This was the first time in 50 years that the Lower House voted to ignore the opinion of the Upper House. Fukuda resigned suddenly on 1 September 2008, just a few weeks after reshuffling his cabinet. On 1 September 2008, Fukuda's resignation was designed so that the LDP did not suffer a "power vacuum". It thus caused a leadership election within the LDP, and the winner, Tarō Asō was chosen as the new party president and on 24 September 2008, he was appointed as 92nd Prime Minister after the House of Representatives voted in his favor in the extraordinary session of Diet.

Later, on 21 July 2009, Prime Minister Asō dissolved the House of Representatives and elections were held on 30 August.
The election results for the House of Representatives were announced on 30 and 31 August 2009. The opposition party DPJ led by Yukio Hatoyama, won a majority by gaining 308 seats (10 seats were won by its allies the Social Democratic Party and the People's New Party). On 16 September 2009, president of DPJ, Hatoyama was elected by the House of Representatives as the 93rd Prime Minister of Japan.

On 2 June 2010, Hatoyama resigned due to lack of fulfillments of his policies, both domestically and internationally and soon after, on 8 June, Akihito, Emperor of Japan ceremonially swore in the newly elected DPJ's president, Naoto Kan as prime minister. Kan suffered an early setback in the Japanese House of Councillors election, 2010. In a routine political change in Japan, DPJ’s new president and former finance minister of Naoto Kan’s cabinet, Yoshihiko Noda was cleared and elected by the Diet as 95th prime minister on 30 August 2011. He was officially appointed as prime minister in the attestation ceremony at imperial palace on 2 September 2011.

In an undesired move, Noda dissolved the lower house on 16 November 2012 (as he fails to get support outside the Diet on various domestic issues i.e. tax, nuclear energy) and elections were held on 16 December. The results were in the favor of LDP, which won absolute majority in the leadership of former Prime Minister Shinzō Abe. He was appointed as the 96th Prime Minister of Japan on 26 December 2012. With the changing political situation, earlier in November 2014, Prime Minister Abe called for fresh mandate for the Lower House. In an opinion poll the government failed to win the public trust due to bad economic achievements in the two consecutive quarters and on the tax reforms.

The election was held on 14 December 2014, and the results were in the favor of LDP and its ally New Komeito. Together they managed to secure a huge majority by winning 325 seats for the Lower House. The opposition, DPJ, could not manage to provide the alternatives to the voters with its policies and programs. "Abenomics", the ambitious self-titled fiscal policy of the current prime minister, managed to attract more voters in this election, many Japanese voters supported the policies. Shinzō Abe was sworn as the 97th prime minister on 24 December 2014 and would like go ahead with his agenda of economic revitalization and structural reforms in Japan.

Japan is a member state of the United Nations and pursues a permanent membership of the Security Council - Japan is one of the "G4 nations" seeking permanent membership. Japan plays an important role in East Asia. The Japanese Constitution prohibits the use of military forces to wage war against other countries. The government maintains a "Self-Defense Force", which include air, land and sea components. Japan's deployment of non-combat troops to Iraq marked the first overseas use of its military since World War II.

As an economic power, Japan is a member of the G8 and Asia-Pacific Economic Cooperation (APEC), and has developed relations with ASEAN as a member of "ASEAN plus three" and the East Asia Summit. Japan is a major donor in international aid and development efforts, donating 0.19% of its Gross National Income in 2004.

Japan has territorial disputes with Russia over the Kuril Islands (Northern Territories), with South Korea over Liancourt Rocks (known as "Dokdo" in Korea, "Takeshima" in Japan), with China and Taiwan over the Senkaku Islands and with China over the status of Okinotorishima. These disputes are in part about the control of marine and natural resources, such as possible reserves of crude oil and natural gas. Japan has an ongoing dispute with North Korea over its abduction of Japanese citizens and nuclear weapons program.




</doc>
<doc id="15578" url="https://en.wikipedia.org/wiki?curid=15578" title="Economy of Japan">
Economy of Japan

The economy of Japan is a highly developed and market-oriented economy. It is the third-largest in the world by nominal GDP and the fourth-largest by purchasing power parity (PPP). and is the world's second largest developed economy. Japan is a member of the G7. According to the International Monetary Fund, the country's per capita GDP (PPP) was at $37,519, the 28th highest in 2014, down from the 22nd position in 2012. Due to a volatile currency exchange rate, Japan's GDP as measured in dollars fluctuates widely. Accounting for these fluctuations through use of the Atlas method, Japan is estimated to have a GDP per capita of around $38,490. The Japanese economy is forecasted by the Quarterly Tankan survey of business sentiment conducted by the Bank of Japan. The Nikkei 225 presents the monthly report of top Blue chip (stock market) equities on Japan Exchange Group.

Japan is the world's third largest automobile manufacturing country, has the largest electronics goods industry, and is often ranked among the world's most innovative countries leading several measures of global patent filings. Facing increasing competition from China and South Korea, manufacturing in Japan today now focuses primarily on high-tech and precision goods, such as optical instruments, hybrid vehicles, and robotics. Besides the Kantō region, the Kansai region is one of the leading industrial clusters and manufacturing centers for the Japanese economy.<ref name="Profile of Osaka/Kansai"></ref> The size and industrial structure of cities in Japan have maintained tight regularities despite substantial churning of population and industries across cities overtime. Japan is the world's largest creditor nation. Japan generally runs an annual trade surplus and has a considerable net international investment surplus. As of 2010, Japan possesses 13.7% of the world's private financial assets (the third largest in the world) at an estimated $13.5 trillion. As of 2015, 54 of the Fortune Global 500 companies are based in Japan, down from 62 in 2013.

Japan has the highest ratio of public debt to GDP of any developed nation. However, the national debt is predominantly owned by Japanese nationals. The Japanese economy faces considerable challenges posed by a declining population. Statistics showed an official decline for the first time in 2015, while projections suggest that it will continue to fall from 127 million down to below 100 million by the middle of the 21st century.

In the three decades of economic development following 1960, Japan ignored defense spending in favor of economic growth, thus allowing for a rapid economic growth referred to as the Japanese post-war economic miracle. By the guidance of Ministry of Economy, Trade and Industry, with average growth rates of 10% in the 1960s, 5% in the 1970s, and 4% in the 1980s, Japan was able to establish and maintain itself as the world's second largest economy from 1978 until 2010, when it was surpassed by the People's Republic of China. By 1990, income per capita in Japan equalled or surpassed that in most countries in the West.

However, in the second half of the 1980s, rising stock and real estate prices caused the economic bubble to the Japanese economy by Bank of Japan. The economic bubble came to an abrupt end as the Tokyo Stock Exchange crashed in 1990–92 and real estate prices peaked in 1991. Growth in Japan throughout the 1990s at 1.5% was slower than growth in other major developed economies, giving rise to the term Lost Decade. After another decade of low growth rate, the term became the Lost 20 Years. Nonetheless, GDP per capita growth from 2001 to 2010 has still managed to outpace Europe and the United States.

Although 1.0% annual GDP growth is considered an aberration, Theodore Breton shows that this is the expected growth rate for a country with a shrinking work force and rates of investment in physical and human capital that have not increased since the 1970s. His analysis indicates that Japan has converged on its steady-state growth rate.

With this low growth rate, national debt of Japan is difficult for the government to manage due to its considerable social welfare spending related to an aging society. The scenario of "Abandoned homes" continues to spread from rural areas to urban areas in Japan. The Japanese economy rebounded under Junichirō Koizumi by acting against bad debts with commercial banks and privatizing the postal savings system. Abenomics has thus far given the economy of Japan a low and unsteady growth rate.

The ICT industry has generated the major outputs to the Japanese economy. Japan is the second largest music market in the world (for more, see Japan Hot 100). Manga cafe business expands to 2000 stores nationwide in Japan in 2002. With fewer children in the aging Japan, Japanese anime industry is facing growing Chinese competition in the targeted Chinese market. Japanese manga industry (from the Japanese manga (and anime) profession) enjoys popularity in most of the Asian markets, and anime industry share in Japan GDP has exceeded 10 percent. Local gourmets of 47 Prefectures of Japan are guided and promoted by Ministry of Agriculture, Forestry and Fisheries (Japan). Japan Cup in the Tokyo Racecourse is one of the richest horse racing in the world.

A mountainous, volcanic island country, Japan has inadequate natural resources to support its growing economy and large population, and therefore exports goods in which it has a comparative advantage such as engineering-oriented, research and development-led industrial products in exchange for the import of raw materials and petroleum. Japan is among the top-three importers for agricultural products in the world next to the European Union and United States in total volume for covering of its own domestic agricultural consumption. Japan is the world's largest single national importer of fish and fishery products. Tokyo Metropolitan Central Wholesale Market is the largest wholesale market for primary products in Japan, including the renowned Tsukiji fish market. Japanese whaling, ostensibly for research purposes, has been sued as illegal under international law.

Although many kinds of minerals were extracted throughout the country, most mineral resources had to be imported in the postwar era. Local deposits of metal-bearing ores were difficult to process because they were low grade. The nation's large and varied forest resources, which covered 70 percent of the country in the late 1980s, were not utilized extensively. Because of political decisions on local, prefectural, and national levels, Japan decided not to exploit its forest resources for economic gain. Domestic sources only supplied between 25 and 30 percent of the nation's timber needs. Agriculture and fishing were the best developed resources, but only through years of painstaking investment and toil. The nation therefore built up the manufacturing and processing industries to convert raw materials imported from abroad. This strategy of economic development necessitated the establishment of a strong economic infrastructure to provide the needed energy, transportation, communications, and technological know-how.

Deposits of gold, magnesium, and silver meet current industrial demands, but Japan is dependent on foreign sources for many of the minerals essential to modern industry. Iron ore, copper, bauxite, and alumina must be imported, as well as many forest products.

The economic history of Japan is one of the most studied economies for its spectacular growth in three different periods. First was the foundation of Edo (in 1603) to whole inland economical developments, second was the Meiji Restoration (in 1868) to be the first non-European power, third was after the defeat of World War II (in 1945) when the island nation rose to become the world's second largest economy.

Japan was considered as a country rich in precious metals, mainly owing to Marco Polo's accounts of gilded temples and palaces, but also due to the relative abundance of surface ores characteristic of a massive huge volcanic country, before large-scale deep-mining became possible in Industrial times. Japan was to become a major exporter of silver, copper, and gold during the period until exports for those minerals were banned.

Renaissance Japan was also perceived as a sophisticated feudal society with a high culture and a strong pre-industrial technology. It was densely populated and urbanized. Prominent European observers of the time seemed to agree that the Japanese ""excel not only all the other Oriental peoples, they surpass the Europeans as well"" (Alessandro Valignano, 1584, "Historia del Principo y Progresso de la Compania de Jesus en las Indias Orientales).

Early European visitors were amazed by the quality of Japanese craftsmanship and metalsmithing. This stems from the fact that Japan itself is rather rich in natural resources found commonly in Europe, especially iron.

The cargo of the first Portuguese ships (usually about 4 smaller-sized ships every year) arriving in Japan almost entirely consisted of Chinese goods (silk, porcelain). The Japanese were very much looking forward to acquiring such goods, but had been prohibited from any contacts with the Emperor of China, as a punishment for Wakō pirate raids. The Portuguese (who were called "Nanban", lit. Southern Barbarians) therefore found the opportunity to act as intermediaries in Asian trade.

The beginning of the Edo period coincides with the last decades of the Nanban trade period, during which intense interaction with European powers, on the economic and religious plane, took place. It is at the beginning of the Edo period that Japan built her first ocean-going Western-style warships, such as the "San Juan Bautista", a 500-ton galleon-type ship that transported a Japanese embassy headed by Hasekura Tsunenaga to the Americas, which then continued to Europe. Also during that period, the "bakufu" commissioned around 350 Red Seal Ships, three-masted and armed trade ships, for intra-Asian commerce. Japanese adventurers, such as Yamada Nagamasa, were active throughout Asia.

In order to eradicate the influence of Christianization, Japan entered in a period of isolation called sakoku, during which its economy enjoyed stability and mild progress.

Economic development during the Edo period included urbanization, increased shipping of commodities, a significant expansion of domestic and, initially, foreign commerce, and a diffusion of trade and handicraft industries. The construction trades flourished, along with banking facilities and merchant associations. Increasingly, "han" authorities oversaw the rising agricultural production and the spread of rural handicrafts.

By the mid-eighteenth century, Edo had a population of more than 1 million and Osaka and Kyoto each had more than 400,000 inhabitants. Many other castle towns grew as well. Osaka and Kyoto became busy trading and handicraft production centers, while Edo was the center for the supply of food and essential urban consumer goods.

Rice was the base of the economy, as the daimyo collected the taxes from the peasants in the form of rice. Taxes were high, about 40% of the harvest. The rice was sold at the "fudasashi" market in Edo. To raise money, the daimyo used forward contracts to sell rice that was not even harvested yet. These contracts were similar to modern futures trading.

During the period, Japan progressively studied Western sciences and techniques (called "rangaku", literally "Dutch studies") through the information and books received through the Dutch traders in Dejima. The main areas that were studied included geography, medicine, natural sciences, astronomy, art, languages, physical sciences such as the study of electrical phenomena, and mechanical sciences as exemplified by the development of Japanese clockwatches, or wadokei, inspired from Western techniques.

Since the mid-19th century, after the Meiji Restoration, the country was opened up to Western commerce and influence and Japan has gone through two periods of economic development. The first began in earnest in 1868 and extended through to World War II; the second began in 1945 and continued into the mid-1980s.

Economic developments of the prewar period began with the "Rich State and Strong Army Policy" by the Meiji government. During the Meiji period (1868–1912), leaders inaugurated a new Western-based education system for all young people, sent thousands of students to the United States and Europe, and hired more than 3,000 Westerners to teach modern science, mathematics, technology, and foreign languages in Japan (Oyatoi gaikokujin). The government also built railroads, improved road, and inaugurated a land reform program to prepare the country for further development.

To promote industrialization, the government decided that, while it should help private business to allocate resources and to plan, the public sector was best equipped to stimulate economic growth. The greatest role of government was to help provide good economic conditions for business. In short, government was to be the guide and business the producer. In the early Meiji period, the government built factories and shipyards that were sold to entrepreneurs at a fraction of their value. Many of these businesses grew rapidly into the larger conglomerates. Government emerged as chief promoter of private enterprise, enacting a series of probusiness policies.

In the mid-1930s, the Japanese nominal wage rates were "10 times less" than the one of the U.S (based on mid-1930s exchange rates), while the price level is estimated to have been about 44% the one of the U.S.

From the 1960s to the 1980s, overall real economic growth was extremely large: a 10% average in the 1960s, a 5% average in the 1970s and a 4% average in the 1980s. By the end of said period, Japan had moved into being a high-wage economy.

Growth slowed markedly in the late 1990s also termed the Lost Decade after the collapse of the Japanese asset price bubble. As a consequence Japan ran massive budget deficits (added trillions in Yen to Japanese financial system) to finance large public works programs.

By 1998, Japan's public works projects still could not stimulate demand enough to end the economy's stagnation. In desperation, the Japanese government undertook "structural reform" policies intended to wring speculative excesses from the stock and real estate markets. Unfortunately, these policies led Japan into deflation on numerous occasions between 1999 and 2004. In his 1998 paper, Japan's Trap, Princeton economics professor Paul Krugman argued that based on a number of models, Japan had a new option. Krugman's plan called for a rise in inflation expectations to, in effect, cut long-term interest rates and promote spending.

Japan used another technique, somewhat based on Krugman's, called Quantitative easing. As opposed to flooding the money market with newly printed money, the Bank of Japan expanded the money supply internally to raise expectations of inflation. Initially, the policy failed to induce any growth, but it eventually began to affect inflationary expectations. By late 2005, the economy finally began what seems to be a sustained recovery. GDP growth for that year was 2.8%, with an annualized fourth quarter expansion of 5.5%, surpassing the growth rates of the US and European Union during the same period. Unlike previous recovery trends, domestic consumption has been the dominant factor of growth.

Despite having interest rates down near zero for a long period of time, the Quantitative easing strategy did not succeed in stopping price deflation. This led some economists, such as Paul Krugman, and some Japanese politicians, to advocate the generation of higher inflation expectations. In July 2006, the zero-rate policy was ended. In 2008, the Japanese Central Bank still had the lowest interest rates in the developed world, but deflation had still not been eliminated and the Nikkei 225 has fallen over approximately 50% (between June 2007 and December 2008). However, on 5 April 2013, the Bank of Japan announced that it would be purchasing 60–70 trillion yen in bonds and securities in an attempt to eliminate deflation by doubling the money supply in Japan over the course of two years. Markets around the world have responded positively to the government's current proactive policies, with the Nikkei 225 adding more than 42% since November 2012. The Economist has suggested that improvements to bankruptcy law, land transfer law, and tax laws will aid Japan's economy. In recent years, Japan has been the top export market for almost 15 trading nations worldwide.

In 2005, one half of Japan's energy was produced from petroleum, a fifth from coal, and 14% from natural gas. Nuclear power in Japan made a quarter of electricity production but due to the Fukushima Daiichi nuclear disaster there has been a large desire to end Japan's nuclear power program. In September 2013, Japan closed its last 50 nuclear power plants nationwide, causing the nation to be nuclear free.

Japan's spendings on roads has been considered large. The 1.2 million kilometers of paved road are one of the major means of transportation. Japan has left-hand traffic. A single network of speed, divided, limited-access toll roads connects major cities and are operated by toll-collecting enterprises. New and used cars are inexpensive, and the Japanese government has encouraged people to buy hybrid vehicles. Car ownership fees and fuel levies are used to promote energy-efficiency.

Rail transport is a major means of transport in Japan. Dozens of Japanese railway companies compete in regional and local passenger transportation markets; for instance, 6 passenger JR enterprises, Kintetsu Railway, Seibu Railway, and Keio Corporation. Often, strategies of these enterprises contain real estate or department stores next to stations, and many major stations have major department stores near them. The Japanese cities of Fukuoka, Kobe, Kyoto, Nagoya, Osaka, Sapporo, Sendai, Tokyo and Yokohama all have subway systems. Some 250 high-speed Shinkansen trains connect major cities. All trains are known for punctuality, and a delay of 90 seconds can be considered late for some train services.

There are 98 passenger and 175 total airports in Japan, and flying is a popular way to travel. The largest domestic airport, Tokyo International Airport, is Asia's second busiest airport. The largest international gateways are Narita International Airport (Tokyo area), Kansai International Airport (Osaka/Kobe/Kyoto area), and Chūbu Centrair International Airport (Nagoya area). The largest ports in Japan include Nagoya Port, the Port of Yokohama, the Port of Tokyo and the Port of Kobe.

About 84% of Japan's energy is imported from other countries. Japan is the world's largest liquefied natural gas importer, second largest coal importer, and third largest net oil importer. Given its heavy dependence on imported energy, Japan has aimed to diversify its sources. Since the oil shocks of the 1970s, Japan has reduced dependence on petroleum as a source of energy from 77.4% in 1973 to about 43.7% in 2010 and increased dependence on natural gas and nuclear power. Other important energy source includes coal, and hydroelectricity is Japan's biggest renewable energy source. Japan's solar market is also currently booming. Kerosene is also used extensively for home heating in portable heaters, especially farther north. Many taxi companies run their fleets on liquefied natural gas. A recent success towards greater fuel economy was the introduction of mass-produced Hybrid vehicles. Prime Minister Shinzō Abe, who was working on Japan's economic revival, signed a treaty with Saudi Arabia and UAE about the rising prices of oil, ensuring Japan's stable deliveries from that region.

This is a chart of trend of gross domestic product of Japan at market prices estimated by the International Monetary Fund with figures in millions of Japanese Yen. See also

For purchasing power parity comparisons, the US dollar was exchanged at ￥109 in 2010.

Industries by GDP value-added 2012. Values are converted using the exchange rate on 13 April 2013.

The Japanese agricultural sector accounts for about 1.4% of the total country's GDP. Only 12% of Japan's land is suitable for cultivation. Due to this lack of arable land, a system of terraces is used to farm in small areas. This results in one of the world's highest levels of crop yields per unit area, with an overall agricultural self-sufficiency rate of about 50% on fewer than 56,000 km² (14 million acres) cultivated.

Japan's small agricultural sector, however, is also highly subsidized and protected, with government regulations that favor small-scale cultivation instead of large-scale agriculture as practiced in North America. There has been a growing concern about farming as the current farmers are aging with a difficult time finding successors.

Rice accounts for almost all of Japan's cereal production. Japan is the second-largest agricultural product importer in the world. Rice, the most protected crop, is subject to tariffs of 777.7%.

Although Japan is usually self-sufficient in rice (except for its use in making rice crackers and processed foods) and wheat, the country must import about 50% of its requirements of other grain and fodder crops and relies on imports for half of its supply of meat. Japan imports large quantities of wheat and soybeans. Japan is the 5th largest market for EU agricultural exports. Over 90% of mandarin oranges in Japan are grown in Japan. Apples are also grown due to restrictions on apple imports.

Japan ranked fourth in the world in 1996 in tonnage of fish caught. Japan captured 4,074,580 metric tons of fish in 2005, down from 4,987,703 tons in 2000, 9,558,615 tons in 1990, 9,864,422 tons in 1980, 8,520,397 tons in 1970, 5,583,796 tons in 1960 and 2,881,855 tons in 1950. In 2003, the total aquaculture production was predicted at 1,301,437 tonnes. In 2010, Japan's total fisheries production was 4,762,469 fish. Offshore fisheries accounted for an average of 50% of the nation's total fish catches in the late 1980s although they experienced repeated ups and downs during that period.

Coastal fishing by small boats, set nets, or breeding techniques accounts for about one third of the industry's total production, while offshore fishing by medium-sized boats makes up for more than half the total production. Deep-sea fishing from larger vessels makes up the rest. Among the many species of seafood caught are sardines, skipjack tuna, crab, shrimp, salmon, pollock, squid, clams, mackerel, sea bream, sauries, tuna and Japanese amberjack. Freshwater fishing, including salmon, trout and eel hatcheries and fish farms, takes up about 30% of Japan's fishing industry. Among the nearly 300 fish species in the rivers of Japan are native varieties of catfish, chub, herring and goby, as well as such freshwater crustaceans as crabs and crayfish. Marine and freshwater aquaculture is conducted in all 47 prefectures in Japan.

Japan maintains one of the world's largest fishing fleets and accounts for nearly 15% of the global catch, prompting some claims that Japan's fishing is leading to depletion in fish stocks such as tuna. Japan has also sparked controversy by supporting quasi-commercial whaling.

Japanese manufacturing and industry is very diversified, with a variety of advanced industries that are highly successful. Industry accounts for 24% of the nation's GDP.

Industry is concentrated in several regions, with the Kantō region surrounding Tokyo, (the Keihin industrial region) as well as the Kansai region surrounding Osaka (the Hanshin industrial region) and the Tōkai region surrounding Nagoya (the Chūkyō–Tōkai industrial region) the main industrial centers. Other industrial centers include the southwestern part of Honshū and northern Shikoku around the Seto Inland Sea (the Setouchi industrial region); and the northern part of Kyūshū (Kitakyūshū). In addition, a long narrow belt of industrial centers called the Taiheiyō Belt is found between Tokyo and Fukuoka, established by particular industries, that have developed as mill towns.

Japan enjoys high technological development in many fields, including consumer electronics, automobile manufacturing, semiconductor manufacturing, optical fibers, optoelectronics, optical media, facsimile and copy machines, and fermentation processes in food and biochemistry. However, many Japanese companies are facing emerging rivals from the United States, South Korea, and China.

Japan is the third biggest producer of automobiles in the world. Toyota is currently the world largest car maker, and the Japanese car makers Nissan, Honda, Suzuki, and Mazda also count for some of the largest car makers in the world.

Japan's mining production has been minimal, and Japan has very little mining deposits. However, massive deposits of rare earths have been found off the coast of Japan. In the 2011 fiscal year, the domestic yield of crude oil was 820 thousand kiloliters, which was 0.4% of Japan's total crude processing volume.

Japan's service sector accounts for about three-quarters of its total economic output. Banking, insurance, real estate, retailing, transportation, and telecommunications are all major industries such as Mitsubishi UFJ, Mizuho, NTT, TEPCO, Nomura, Mitsubishi Estate, ÆON, Mitsui Sumitomo, Softbank, JR East, Seven & I, KDDI and Japan Airlines counting as one of the largest companies in the world. Four of the five most circulated newspapers in the world are Japanese newspapers. The Koizumi government set Japan Post, one of the country's largest providers of savings and insurance services for privatization by 2015. The six major keiretsus are the Mitsubishi, Sumitomo, Fuyo, Mitsui, Dai-Ichi Kangyo and Sanwa Groups. Japan is home to 251 companies from the Forbes Global 2000 or 12.55% (as of 2013).

In 2012, Japan was the fifth most visited country in Asia and the Pacific, with over 8.3 million tourists. In 2013, due to the weaker yen and easier visa requirements for southwest Asian countries, Japan received a record 11.25 million visitors, which was higher than the government's projected goal of 10 million visitors. The government hopes to attract 40 million visitors a year by the 2020 Summer Olympics in Tokyo. Some of the most popular visited places include the Shinjuku, Ginza, Shibuya and Asakusa areas in Tokyo, and the cities of Osaka, Kobe and Kyoto, as well as Himeji Castle. Hokkaido is also a popular winter destination for visitors with several ski resorts and luxury hotels being built there.

The Tokyo Stock Exchange is the third largest stock exchange in the world by market capitalization, as well as the 2nd largest stock market in Asia, with 2,292 listed companies. The Nikkei 225 and the TOPIX are the two important stock market indexes of the Tokyo Stock Exchange. The Tokyo Stock Exchange and the Osaka Stock Exchange, another major stock exchange in Japan, merged on 1 January 2013, creating one of the world's largest stock exchanges. Other stock exchanges in Japan include the Nagoya Stock Exchange, Fukuoka Stock Exchange and Sapporo Securities Exchange.

The unemployment rate in December 2013 was 3.7%, down 1.5 percentage points from the claimed unemployment rate of 5.2% in June 2009 due to the strong economic recovery.

In 2008 Japan's labor force consisted of some 66 million workers—40% of whom were women—and was rapidly shrinking.
One major long-term concern for the Japanese labor force is its low birthrate. In the first half of 2005, the number of deaths in Japan exceeded the number of births, indicating that the decline in population, initially predicted to start in 2007, had already started. While one countermeasure for a declining birthrate would be to remove barriers to immigration, despite taking new steps towards it, the Japanese government has been reluctant to do so, since foreign immigration to Japan has been unpopular among citizens.

In 1989, the predominantly public sector union confederation, SOHYO (General Council of Trade Unions of Japan), merged with RENGO (Japanese Private Sector Trade Union Confederation) to form the Japanese Trade Union Confederation. Labor union membership is about 12 million.

Japan ranks 27th of 185 countries in the ease of doing business index 2013.

Japan has one of the smallest tax rates in the developed world. After deductions, the majority of workers are free from personal income taxes. Consumption tax rate is only 8%, while corporate tax rates are high, second highest corporate tax rate in the world, at 36.8%. However, the House of Representatives has passed a bill which will increase the consumption tax to 10% in October 2015. The government has also decided to reduce corporate tax and to phase out automobile tax.

In 2016, the IMF encouraged Japan to adopt an income policy that pushes firms to raise employee wages in combination with reforms to tackle the labor market dual tiered employment system to drive higher wages, on top of monetary and fiscal stimulus. Shinzo Abe has encouraged firms to raise wages by at least three percent annually (the inflation target plus average productivity growth).

Shareholder activism is rare despite the fact that the corporate law gives shareholders strong powers over managers. Under Prime Minister Shinzō Abe, corporate governance reform has been a key initiative to encourage economic growth. In 2012 around 40% of leading Japanese companies had any independent directors while in 2016 most all have begun to appoint independent directors.

The government's liabilities include the second largest public debt of any nation with debt of over one quadrillion yen, or $8,535,340,000,000 in USD. Former Prime Minister Naoto Kan has called the situation 'urgent'.

Japan's central bank has the second largest foreign-exchange reserves after the People's Republic of China, with over one trillion US Dollars in foreign reserves.

Nemawashi (根回し), or "consensus building", in Japanese culture is an informal process of quietly laying the foundation for some proposed change or project, by talking to the people concerned, gathering support and feedback, and so forth. It is considered an important element in any major change, before any formal steps are taken, and successful "nemawashi" enables changes to be carried out with the consent of all sides.

Japanese companies are known for management methods such as "The Toyota Way". Kaizen (改善, Japanese for "improvement") is a Japanese philosophy that focuses on continuous improvement throughout all aspects of life. When applied to the workplace, Kaizen activities continually improve all functions of a business, from manufacturing to management and from the CEO to the assembly line workers. By improving standardized activities and processes, Kaizen aims to eliminate waste (see Lean manufacturing). Kaizen was first implemented in several Japanese businesses during the country's recovery after World War II, including Toyota, and has since spread to businesses throughout the world. Within certain value systems, it is ironic that Japanese workers labor amongst the most hours per day, even though kaizen is supposed to improve all aspects of life.

Some companies have powerful enterprise unions and "shuntō". The Nenko System or Nenko Joretsu, as it is called in Japan, is the Japanese system of promoting an employee based on his or her proximity to retirement. The advantage of the system is that it allows older employees to achieve a higher salary level before retirement and it usually brings more experience to the executive ranks. The disadvantage of the system is that it does not allow new talent to be combined with experience and those with specialized skills cannot be promoted to the already crowded executive ranks. It also does not guarantee or even attempt to bring the "right person for the right job".

Relationships between government bureaucrats and companies are often close. is the institutionalised practice where Japanese senior bureaucrats retire to high-profile positions in the private and public sectors. The practice is increasingly viewed as corrupt and a limitation on efforts to reduce ties between the private sector and the state that prevent economic and political reforms. Lifetime employment ("shūshin koyō") and seniority-based career advancement have been common in the Japanese work environment. Japan has begun to gradually move away from some of these norms.

An office lady, often abbreviated OL (Japanese: オーエル "Ōeru"), is a female office worker in Japan who performs generally pink collar tasks such as serving tea and secretarial or clerical work. Like many unmarried Japanese, OLs often live with their parents well into early adulthood. Office ladies are usually full-time permanent staff, although the jobs they do usually have little opportunity for promotion, and there is usually the tacit expectation that they leave their jobs once they get married.

, which can be translated quite literally from Japanese as "death from overwork", is occupational sudden death. The major medical causes of karōshi deaths are heart attack and stroke due to stress.

, (sometimes also translated as "corporate bouncers", "meeting-men", or "corporate blackmailers") are a form of specialized racketeer unique to Japan, and often associated with the yakuza that extort money from or blackmail companies by threatening to publicly humiliate companies and their management, usually in their . is a Japanese term for moneylender, or loan shark. It is a contraction of the Japanese words for salaryman and cash. Around 14 million people, or 10% of the Japanese population, have borrowed from a "sarakin". In total, there are about 10,000 firms (down from 30,000 a decade ago); however, the top seven firms make up 70% of the market. The value of outstanding loans totals $100 billion. The biggest "sarakin" are publicly traded and often allied with big banks.

The first "Western-style" department store in Japan was Mitsukoshi, founded in 1904, which has its root as a kimono store called Echigoya from 1673. When the roots are considered, however, Matsuzakaya has an even longer history, dated from 1611. The kimono store changed to a department store in 1910. In 1924, Matsuzakaya store in Ginza allowed street shoes to be worn indoors, something innovative at the time. These former kimono shop department stores dominated the market in its earlier history. They sold, or rather displayed, luxurious products, which contributed for their sophisticated atmospheres. Another origin of Japanese department store is that from railway company. There have been many private railway operators in the nation, and from the 1920s, they started to build department stores directly linked to their lines' termini. Seibu and Hankyu are the typical examples of this type. From the 1980s onwards, Japanese department stores face fierce competition from supermarkets and convenience stores, gradually losing their presences. Still, "depāto" are bastions of several aspects of cultural conservatism in the country. Gift certificates for prestigious department stores are frequently given as formal presents in Japan. Department stores in Japan generally offer a wide range of services and can include foreign exchange, travel reservations, ticket sales for local concerts and other events.

A is a set of companies with interlocking business relationships and shareholdings. It is a type of business group. The prototypical "keiretsu" appeared in Japan during the "economic miracle" following World War II. Before Japan's surrender, Japanese industry was controlled by large family-controlled vertical monopolies called "zaibatsu". The Allies dismantled the "zaibatsu" in the late 1940s, but the companies formed from the dismantling of the "zaibatsu" were reintegrated. The dispersed corporations were re-interlinked through share purchases to form horizontally integrated alliances across many industries. Where possible, "keiretsu" companies would also supply one another, making the alliances vertically integrated as well. In this period, official government policy promoted the creation of robust trade corporations that could withstand pressures from intensified world trade competition.

The major "keiretsu" were each centered on one bank, which lent money to the "keiretsu's" member companies and held equity positions in the companies. Each central bank had great control over the companies in the "keiretsu" and acted as a monitoring entity and as an emergency bail-out entity. One effect of this structure was to minimize the presence of hostile takeovers in Japan, because no entities could challenge the power of the banks.

There are two types of "keiretsu": vertical and horizontal. Vertical "keiretsu" illustrates the organization and relationships within a company (for example all factors of production of a certain product are connected), while a horizontal "keiretsu" shows relationships between entities and industries, normally centered on a bank and trading company. Both are complexly woven together and sustain each other.

The Japanese recession in the 1990s had profound effects on the keiretsu. Many of the largest banks were hit hard by bad loan portfolios and forced to merge or go out of business. This had the effect of blurring the lines between the keiretsu: Sumitomo Bank and Mitsui Bank, for instance, became Sumitomo Mitsui Banking Corporation in 2001, while Sanwa Bank (the banker for the Hankyu-Toho Group) became part of Bank of Tokyo-Mitsubishi UFJ. Additionally, many companies from outside the keiretsu system, such as Sony, began outperforming their counterparts within the system.

Generally, these causes gave rise to a strong notion in the business community that the old keiretsu system was not an effective business model, and led to an overall loosening of keiretsu alliances. While the keiretsu still exist, they are not as centralized or integrated as they were before the 1990s. This, in turn, has led to a growing corporate acquisition industry in Japan, as companies are no longer able to be easily "bailed out" by their banks, as well as rising derivative litigation by more independent shareholders.

Japanese companies have been involved in 50,759 deals between 1985 and 2018. This cumulates to a total value of 2,636 bil. USD which translates to 281,469.9 bil. YEN. In the year 1999 there was an all-time high in terms of value of deals with almost 220 bil. USD. The most active year so far was 2017 with over 3,150 deals, but only a total value of 114 bil. USD (see graph "M&A in Japan by number and value").

Here is a list of the most important deals (ranked by value in bil. USD) in Japanese history:

Among the top 50 deals by value, 92% of the time the acquiring nation is Japan. Foreign direct investment is playing a much smaller role than national M&A in Japan.

Net international investment position: 266,223 \ billion (1st)

Industrial Production Growth Rate: 7.5% (2010 est.)

Investment (gross fixed): 20.3% of GDP (2010 est.)

Household income or consumption by percentage share:
Agriculture – Products: rice, sugar beets, vegetables, fruit, pork, poultry, dairy products, eggs, fish

Exports – Commodities: machinery and equipment, motor vehicles, semiconductors, chemicals

Imports – Commodities: machinery and equipment, fuels, foodstuffs, chemicals, textiles, raw materials (2001)

Exchange rates:<br>
"Japanese Yen per US$1" – 88.67 (2010), 93.57 (2009), 103.58 (2008), 117.99 (2007), 116.18 (2006), 109.69 (2005), 115.93 (2003), 125.39 (2002), 121.53 (2001), 105.16 (January 2000), 113.91 (1999), 130.91 (1998), 120.99 (1997), 108.78 (1996), 94.06 (1995)

Electricity:

Electricity – Production by source:

Electricity – Standards:

Oil:




</doc>
<doc id="15579" url="https://en.wikipedia.org/wiki?curid=15579" title="Communications in Japan">
Communications in Japan

The nation of Japan currently possesses one of the most advanced communication networks in the world. For example, by 2008 the Japanese government's Internal Affairs and Communications Ministry stated that about 75 million people used cellphones to access the internet, said total accounting for about 82% of individual internet users.

Telephones and ISDN - main lines in use: 52.3981 million (2007)

IP phone lines in use: 16.766 million (2007)

Mobile and PHS lines in use: 105.297 million (2007)

There are five nationwide mobile phone service providers: NTT DoCoMo, KDDI, SoftBank Mobile, EMOBILE, and Willcom.

Radio broadcast stations: AM 190, FM 88, shortwave 24 (1999)

Radios: 120.5 million (1997)

Television broadcast stations: 7,108 (plus 441 repeaters; note - in addition, US Forces are served by 3 TV stations and 2 TV cable services) (1999)

Televisions: 86.5 million (1997)

Amateur radio: 446,602 licensed stations as of October 2011. See Amateur radio call signs of Japan.


Number of Broadband Users by Access (April 2005)

Number of Broadband Users by Access (June 2004)

Number of Broadband Users by Access (June 2002)

Country code (Top-level domain): JP

Japan's first modern postal service got started in 1871, with mail professionally traveling between Kyoto and Tokyo as well as the latter city and Osaka. This took place in the midst of the rapid industrialization and social reorganization that the Meiji period symbolized in Japanese history. Given how the nation's railroad technology was in its infancy, Japan's growing postal system relied heavily on human-powered transport, including rickshaws, as well as horse-drawn methods of delivery. For example, while commemorating the 50th anniversary of Japan's postal service, the country's 1921 government released decorative postcards depicting intrepid horseback riders carrying the mail.

In communication terms, British technicians had already been employed in assisting with Japanese lighthouses, and the country's budding mail system looked to hybridize British ideas with local practicalities. Shipping along the nation's coastline in particular demonstrates a key instance of how the Japanese economy developed: the government closely working with private companies to industrially expand in a way that met social needs while also allowing for large profits. Mitsubishi's contract for mail transport by sea proved lucrative enough that it assisted with the firm becoming one of the famous "zaibatsu".

Since 2007, the nation's post offices have been managed by the firm Japan Post Network, which is itself a part of the larger Japan Post Holdings conglomerate. As of December 2017, the smaller company has been managed by CEO Koji Furukawa. The simple Japanese postal mark, predating mass literacy in the nation, is still used to this day.

An example of the dawn of modern Japanese communications is the shift in newspaper publication. News vendors of the Tokugawa period, taking place from 1603 to 1867, typically promoted publications by reading the contents aloud and handed out papers that were printed from hand-graven blocks. Widespread adoption of movable type took place as Japanese society modernized. In particular, "Yomiuri Shimbun", a national daily newspaper that became the country's largest by circulation, was founded in 1874 and designed to be read in detail using standard Japanese vernacular. Five such dailies got started early in the Meiji period, taking place from 1868 to 1912. "Yomiuri" specifically took direct influence from American publications controlled by William Randolph Hearst.

The first such mass newspaper to be founded was the "Nagasaki Shipping List & Advertiser", established in 1861 in Nagasaki by the Englishman A.W. Hansard. Its first issue ran 22 June of that year. The newspaper, which notably discussed matters in the English language, laid the groundwork for Hansard's later publication "Japan Herald".

The broadcast industry has been dominated by the Japan Broadcasting Corporation (Nippon Hoso Kyokai—NHK) since its founding in 1925.

In the postwar period, NHK's budget and operations were under the purview of the Ministry of Posts and Telecommunications, the Broadcasting Law of 1950 provides for independent management and programming by NHK. Television broadcasting began in 1953, and color television was introduced in 1960. Cable television was introduced in 1969. In 1978 an experimental broadcast satellite with two color television channels was launched. Operational satellites for television use were launched between 1984 and 1990. Television viewing spread so rapidly that, by 1987, 99 percent of Japan's households had color television sets and the average family had its set on at least five hours a day. Starting in 1987, NHK began full-scale experimental broadcasting on two channels using satellite-to-audience signals, thus bringing service to remote and mountainous parts of the country that earlier had experienced poor reception. The new system also provided twenty-four hours a day, nonstop service.

In the late 1980s, NHK operated two public television and three radio networks nationally, producing about 1,700 programs per week. Its general and education programs were broadcast through more than 6,900 television stations and nearly 330 AM and more than 500 FM radio transmitting stations. Comprehensive service in twenty-one languages is available throughout the world.

Rapid improvements, innovations, and diversification in communications technology, including optical fiber cables, communications satellites, and fax machines, led to rapid growth of the communications industry in the 1980s. Nippon Telegraph and Telephone Corporation, owned by the government until 1985, had dominated the communications industry until April 1985, when new common carriers, including Daini Denden, were permitted to enter the field. NTT Worldwide Telecommunications Corp (Kokusai Denshin Denwa Company, commonly known as KDD, now part of KDDI Inc.) lost its monopoly hold on international communications activities in 1989, when Nihon Kokusai Tsushin and other private overseas communications firms began operations.

In 1992 Japan also had more than 12,000 televisions stations, and the country had more than 350 radio stations, 300 AM radio stations and 58 FM. Broadcasting innovations in the 1980s included sound multiplex (two-language or stereo) broadcasting, satellite broadcasting, and in 1985 the University of the Air and teletext services were inaugurated.

Japan has been the world leader in telecommunications in the 1980s, but this position that has been challenged by the United States' dot-com industry in the 1990s and the emerging tiger states in Asia. While the United States is leading in digital content, South Korea is leading in broadband access, India is leading in software, and Taiwan is leading in research and development.

Japan went into the 21st century after achieving widespread saturation with telecommunication devices. For instance, by 2008 the government's Internal Affairs and Communications Ministry stated that about 75 million people used cellphones to access the internet, said total accounting for about 82% of individual internet users.



</doc>
<doc id="15580" url="https://en.wikipedia.org/wiki?curid=15580" title="Transport in Japan">
Transport in Japan

Transport in Japan is modern and highly developed. Japan's transport sector stands out for its energy efficiency: it uses less energy per person compared to other countries, thanks to a high share of rail transport and low overall travel distances. Transport in Japan is also very expensive in international comparison, reflecting high tolls and taxes, particularly on automobile transport.

Japan's spending on roads has been large. The 1.2 million kilometres of paved road are the main means of transport. Japan has left-hand traffic. A single network of high-speed, divided, limited-access toll roads connects major cities, which are operated by toll-collecting enterprises.

Dozens of Japanese railway companies compete in regional and local passenger transport markets; for instance, seven JR Group companies, Kintetsu Railway, Seibu Railway, and Keio Corporation. Often, strategies of these enterprises contain real estate or department stores next to stations. Some 250 high-speed Shinkansen trains connect major cities. All trains are known for punctuality.

There are 176 airports, and the largest domestic airport, Haneda Airport, is Asia's busiest airport. The largest international gateways are Narita International Airport (Tokyo area), Kansai International Airport (Osaka/Kobe/Kyoto area), and Chūbu Centrair International Airport (Nagoya area). The largest ports include Nagoya Port.

In Japan, railways are a major means of passenger transport, especially for mass and high-speed transport between major cities and for commuter transport in metropolitan areas. Seven Japan Railways Group companies, state-owned until 1987, cover most parts of Japan. There also are railway services operated by private rail companies, regional governments, and companies funded by both regional governments and private companies.

Total railways of 27,182 km include several track gauges, the most common of which is narrow gauge, with 22,301 km of track of which 15,222 km is electrified.

Fukuoka, Kobe, Kyoto, Nagoya, Osaka, Sapporo, Sendai, Tokyo, and Yokohama have subway systems.

Most Japanese people travelled on foot until the later part of the 19th century. The first railway was built between Tokyo's Shimbashi Station and Yokohama's former Yokohama Station (now Sakuragichō Station) in 1872. Many more railways developed soon afterward. Japan, as we know it today, is home to one of the world's most developed transport networks. Mass transport is well developed in Japan, but the road system lags behind and is inadequate for the number of cars owned in Japan. This is often attributed to the fact that road construction is difficult in Japan because of its uniquely high population density, and the limited amount of available usable land for road construction.

The Shinkansen, or "bullet trains", as they are often known, are the high-speed rail trains that run across Japan. The of 8 Shinkansen lines run on completely separate lines from their commuting train counterparts, with a few exceptions. Shinkansen take up a large portion of the long distance travel in Japan, with the whole system carrying over 10 billion passengers in its lifetime. 1,114 journeys are made daily, with the fastest train being the JR East E5 and E6 series trains, which operate at a maximum speed of . Shinkansen trains are known to be very safe, with no accident-related deaths or injuries from passengers in its 50-plus year history. Shinkansen trains are also known to be very punctual, following suit with all other Japanese transport; in 2003, the average delay per train on the Tokaido Shinkansen was a mere 6 seconds. Japan has been trying to sell its Shinkansen technology overseas, and has struck deals to help build systems in India, Thailand, and the United States.

The first Shinkansen line opened between Tokyo and Osaka in 1964, and trains can now make the journey in 2 hours and 25 minutes. Additional Shinkansen lines connect Tokyo to Aomori, Niigata, Kanazawa, and Hakodate and Osaka to Fukuoka and Kagoshima, with new lines under construction to Tsuruga, Sapporo and Nagasaki.

Japan has been developing maglev technology trains, and broke the world maglev speed record in April 2015 with a train travelling at the speed of . The Chūō Shinkansen, a commercial maglev service, is currently under construction from Tokyo to Nagoya and Osaka, and when completed in 2045 will cover the distance in 67 minutes, half the time of the current Shinkansen.

According to Japan Statistical Yearbook 2015, Japan in April 2012 had approximately 1,215,000 km of roads made up of 1,022,000 km of city, town and village roads, 129,000 km of prefectural roads, 55,000 km of general national highways and 8,050 km of national expressways. The Foreign Press Center/Japan cites a total length of expressways at 7,641 km (fiscal 2008). A single network of high-speed, divided, limited-access toll roads connects major cities on Honshu, Shikoku and Kyushu. Hokkaido has a separate network, and Okinawa Island has a highway of this type. In the year 2005, the toll collecting companies, formerly Japan Highway Public Corporation, have been transformed into private companies in public ownership, and there are plans to sell parts of them. The aim of this policy is to encourage competition and decrease tolls.

Road passenger and freight transport expanded considerably during the 1980s as private ownership of motor vehicles greatly increased along with the quality and extent of the nation's roads. Bus companies including the JR Bus companies operate long-distance bus services on the nation's expanding expressway network. In addition to relatively low fares and deluxe seating, the buses are well utilised because they continue service during the night, when air and train services are limited.

The cargo sector grew rapidly in the 1980s, recording 274.2 billion tonne-kilometres in 1990. The freight handled by motor vehicles, mainly trucks, in 1990, was over 6 billion tonnes, accounting for 90 percent of domestic freight tonnage and about 50 percent of tonne-kilometres.

Recent large infrastructure projects were the construction of the Great Seto Bridge and the Tokyo Bay Aqua-Line (opened 1997).

Although road fatalities have been decreasing, due in part to stricter enforcement of drunk driving laws, 2004 still saw 7,358 deaths on Japanese roads.

In 2013 Japan had the fourth largest passenger air market in the world with 105,913,000 passengers. In 2013 Japan had 98 airports. The main international gateways are Narita International Airport (Tokyo area), Kansai International Airport (Osaka/Kobe/Kyoto area), and Chūbu Centrair International Airport (Nagoya area). The main domestic hub is Tokyo International Airport (Haneda Airport), Asia's busiest airport and the world's 4th busiest airport; other major traffic hubs include Osaka International Airport, New Chitose Airport outside Sapporo, and Fukuoka Airport. 14 heliports are estimated to exist (1999).

The two main airlines are Japan Airlines and All Nippon Airways. Other passenger carriers include Skymark Airlines, Skynet Asia Airways, Air Do, Star Flyer and Fuji Dream Airlines. United Airlines and Delta Air Lines, formerly Northwest Airlines, are major international operators from Narita Airport.

Domestic air travel in Japan has historically been highly regulated. From 1972, the three major domestic airlines (JAL, ANA, and JAS) were allocated certain routes, with JAL and ANA sharing trunk routes, and ANA and JAS sharing local feeder routes. JAL and JAS have since been merged to help compete with ANA. JAL also had a flag-carrier monopoly on international routes until 1986. Airfares were set by the government until 2000, although carriers had freedom to adjust the standard fares starting in 1995 (when discounts of up to 50% were permitted). Today, fares can be set by carriers, but the government retains the ability to veto fares that are too high.

There are 1770 km of waterways in Japan; seagoing craft ply all coastal inland seas.

There are some 994 ports in Japan as of April 2014. There are overlapping classifications of these ports, some of which are multi-purpose, e.g. cargo, passenger, naval, and fishery. The five designated "super" container ports are: Yokkaichi, Yokohama, Nagoya, Kobe and Osaka. 23 are designated major/international, 125 designated as important, while there are also purely fisherman ports.

The twenty-three major seaports designated as special, important ports by the Ministry of Land, Infrastructure, Transport and Tourism : Chiba, Fushiki/Toyama, Himeji, Hiroshima, Kawasaki, Kitakyūshū, Kobe, Kudamatsu, Muroran, Nagoya, Niigata, Osaka, Sakai/Senpoku, Sendai/Shiogama, Shimizu, Shimonoseki, Tokyo, Tomakomai, Wakayama, Yokkaichi, and Yokohama.

Japan has 662 ships with a volume of or over, totaling or . There are 146 bulk ships, 49 cargo ships, 13 chemical tankers, 16 combination bulk, 4 with combination of ore and oil, 25 container, 45 liquefied gas, 9 passenger, 2 passenger and cargo combination ships, 214 petroleum tankers, 22 refrigerated cargo, 48 roll-on/roll-off ships, 9 short-sea passenger, and 60 vehicle carriers (1999 est.).

Ferries connect Hokkaido to Honshu, and Okinawa Island to Kyushu and Honshu. They also connect other smaller islands and the main islands. The scheduled international passenger routes are to China, Russia, South Korea and Taiwan. Coastal and cross-channel ferries on the main islands decreased in routes and frequencies following the development of bridges and expressways but some are still operating (as of 2007).

Japan has 84 km of pipelines for crude oil, 322 km for petroleum products, and 1,800 km for natural gas.





</doc>
<doc id="15582" url="https://en.wikipedia.org/wiki?curid=15582" title="Foreign relations of Japan">
Foreign relations of Japan

The are handled by the Ministry of Foreign Affairs of Japan.

Japanese foreign relations had to begin anew in 1945, when it was defeated in war and stripped of all of its foreign conquests and possessions. See History of Japanese foreign relations. The United States, acting for the Allied powers, occupied Japan 1945-51. Since gaining full independence with the Treaty of San Francisco, Japanese diplomatic policy has been based on close partnership with the United States and the emphasis on the international cooperation such as the United Nations. In the Cold War, Japan took a part in the Western world's confrontation of the Soviet Union in East Asia. In the rapid economic developments in the 1960s and 1970s, Japan recovered its influences and became regarded as one of the major powers in the world. However, Japanese influences are regarded as negative by two particular countries: China and South Korea.

During the Cold War, Japanese foreign policy was not self-assertive, relatively focused on their economic growth. However, the end of the Cold War and bitter lessons from the Gulf War changed the policy slowly. Japanese government decided to participate in the Peacekeeping operations by the UN, and sent their troops to Cambodia, Mozambique, Golan Heights and the East Timor in the 1990s and 2000s. After the September 11 attacks in 2001, Japanese naval vessels have been assigned to resupply duties in the Indian Ocean to the present date. The Ground Self-Defense Force also dispatched their troops to Southern Iraq for the restoration of basic infrastructures.

Beyond its immediate neighbors, Japan has pursued a more active foreign policy in recent years, recognizing the responsibility which accompanies its economic strength. Prime Minister Yasuo Fukuda stressed a changing direction in a policy speech to the National Diet: "Japan aspires to become a hub of human resource development as well as for research and intellectual contribution to further promote cooperation in the field of peace-building." This follows the modest success of a Japanese-conceived peace plan which became the foundation for nationwide elections in Cambodia in 1998.


Japan is increasingly active in Africa. In May 2008, the first Hideyo Noguchi Africa Prize will be awarded at Fourth Tokyo International Conference on African Development (TICAD IV), which signals a changing emphasis in bilateral relations.

Japan has continued to extend significant support to development and technical assistance projects in Latin America.

By 1990 Japan's interaction with the vast majority of Asia-Pacific countries, especially its burgeoning economic exchanges, was multifaceted and increasingly important to the recipient countries. The developing countries of the Association of Southeast Asian Nations (ASEAN) regarded Japan as critical to their development. Japan's aid to the ASEAN countries totaled US $1.9 billion in Japanese fiscal year (FY) 1988 versus about US $333 million for the United States during U.S. FY 1988. Japan was the number one foreign investor in the ASEAN countries, with cumulative investment as of March 1989 of about US $14.5 billion, more than twice that of the United States. Japan's share of total foreign investment in ASEAN countries in the same period ranged from 70 to 80 percent in Thailand to 20 percent in Indonesia.

In the late 1980s, the Japanese government was making a concerted effort to enhance its diplomatic stature, especially in Asia. Toshiki Kaifu's much publicized spring 1991 tour of five Southeast Asian nations—Malaysia, Brunei, Thailand, Singapore, and the Philippines—culminated in a 3 May major foreign policy address in Singapore, in which he called for a new partnership with the ASEAN and pledged that Japan would go beyond the purely economic sphere to seek an "appropriate role in the political sphere as a nation of peace." As evidence of this new role, Japan took an active part in promoting negotiations to resolve the Cambodian conflict.

In 1997, the ASEAN member nations and the People's Republic of China, South Korea and Japan agreed to hold yearly talks to further strengthen regional cooperation, the ASEAN Plus Three meetings. In 2005 the ASEAN plus Three countries together with India, Australia and New Zealand held the inaugural East Asia Summit (EAS).

In South Asia, Japan's role is mainly that of an aid donor. Japan's aid to seven South Asian countries totaled US$1.1 billion in 1988 and 1989, dropping to just under US$900 million in 1990. Except for Pakistan, which received heavy inputs of aid from the United States, all other South Asian countries receive most of their aid from Japan. Four South Asian nations—India, Pakistan, Bangladesh, and Sri Lanka—are in the top ten list of Tokyo's aid recipients worldwide. A point to note is that Indian Government has a no receive aid policy since the tsunami that struck India but Indian registerred NGOs look to Japan for much investment in their projects

Prime Minister Toshiki Kaifu signaled a broadening of Japan's interest in South Asia with his swing through the region in April 1990. In an address to the Indian parliament, Kaifu stressed the role of free markets and democracy in bringing about "a new international order," and he emphasized the need for a settlement of the Kashmir territorial dispute between India and Pakistan and for economic liberalization to attract foreign investment and promote dynamic growth. To India, which was very short of hard currency, Kaifu pledged a new concessional loan of ¥100 billion (about US$650 million) for the coming year.

In what became known as the Tenshō embassy, the first ambassadors from Japan to European powers reached Lisbon, Portugal in August 1584. From Lisbon, the ambassadors left for the Vatican in Rome, which was the main goal of their journey. The embassy returned to Japan in 1590, after which time the four nobleman ambassadors were ordained by Alessandro Valignano as the first Japanese Jesuit fathers.

A second embassy, headed by Hasekura Tsunenaga and sponsored by Date Masamune, was also a diplomatic mission to the Vatican. The embassy left 28 October 1613 from Ishinomaki, Miyagi Prefecture, in the northern Tōhoku region of Japan, where Date was "daimyō". It traveled to Europe by way of New Spain, arriving in Acapulco on 25 January 1614, Mexico City in March, Havana in July, and finally Seville on 23 October 1614. After a short stop-over in France, the embassy reached Rome in November 1615, where it was received by Pope Paul V. After return travel by way of New Spain and the Philippines, the embassy reached the harbor of Nagasaki in August 1620. While the embassy was gone, Japan had undergone significant change, starting with the 1614 Osaka Rebellion, leading to a 1616 decree from the Tokugawa shogunate that all interaction with non-Chinese foreigners was confined to Hirado and Nagasaki. In fact, the only western country that was allowed to trade with Japan was the Dutch Republic. This was the beginning of "sakoku", where Japan was essentially closed to the western world until 1854.

Although cultural and non-economic ties with Western Europe grew significantly during the 1980s, the economic nexus remained by far the most important element of Japanese – West European relations throughout the decade. Events in West European relations, as well as political, economic, or even military matters, were topics of concern to most Japanese commentators because of the immediate implications for Japan. The major issues centred on the effect of the coming West European economic unification on Japan's trade, investment, and other opportunities in Western Europe. Some West European leaders were anxious to restrict Japanese access to the newly integrated European Union, but others appeared open to Japanese trade and investment. In partial response to the strengthening economic ties among nations in Western Europe and to the United States–Canada–Mexico North American Free Trade Agreement, Japan and other countries along the Asia-Pacific rim began moving in the late 1980s toward greater economic cooperation.

On 18 July 1991, after several months of difficult negotiations, Prime Minister Toshiki Kaifu signed a joint statement with the Dutch prime minister and head of the European Community Council, Ruud Lubbers, and with the European Commission president, Jacques Delors, pledging closer Japanese – European Community consultations on foreign relations, scientific and technological cooperation, assistance to developing countries, and efforts to reduce trade conflicts. Japanese Ministry of Foreign Affairs officials hoped that this agreement would help to broaden Japanese – European Community political links and raise them above the narrow confines of trade disputes.

Japan has formally issued apologies for its military occupations before and during World War II, but that has done little in helping to improve its relationships with neighboring countries, especially the People's Republic of China, North Korea and South Korea. These countries still insist that Japan has yet to formally express remorse for its wrongdoings in the 20th century, despite some formal statements of regret from Prime Ministers Hosokawa Morihiro and Murayama Tomiichi. Japan’s official stance is that all war-related reparation claims have been resolved (except with North Korea). Unofficial visits to the controversial Yasukuni Jinja by past Prime Ministers belonging to the Liberal Democratic Party and the exclusion or generalization of some elements of Japan’s military history in a number school textbooks have also clouded the issue.

In 2004 the People’s Republic of China, North Korea, and South Korea also criticized Japan for sending its Ground Self Defence Forces to Iraq, which was seen as signalling a return to militarism. The government of Japan insisted that its forces would only participate in reconstruction and humanitarian aid missions.

There is a strong anti-Japanese sentiment in the People’s Republic of China, North Korea and South Korea. Antagonism is not inevitable however. South Korea and Japan successfully dual-hosted the 2002 FIFA World Cup, bridging a physical and political gap between the two countries. The great popularity in Japan of Bae Yong-joon, a South Korean actor, has also been seen as a sign that the two countries have moved closer together.

Japan has several territorial disputes with its neighbors concerning the control of certain outlying islands.

Japan contests Russia's control of the Southern Kuril Islands (including Etorofu, Kunashiri, Shikotan, and the Habomai group) which were occupied by the Soviet Union in 1945. South Korea's assertions concerning Liancourt Rocks (Japanese: "Takeshima", Korean: "Dokdo") are acknowledged, but not accepted by Japan. Japan has strained relations with the People's Republic of China (PRC) and the Republic of China (Taiwan) over the Senkaku Islands; and with the People's Republic of China over the status of Okinotorishima.

These disputes are in part about irredentism; and they are also about the control of marine and natural resources, such as possible reserves of crude oil and natural gas.





</doc>
<doc id="15587" url="https://en.wikipedia.org/wiki?curid=15587" title="Joshua Jackson">
Joshua Jackson

Joshua Browning Carter Jackson (born June 11, 1978) is a Canadian-American actor. He has appeared in primetime television and in over 30 film roles. His well-known roles include Pacey Witter in "Dawson's Creek", Charlie Conway in "The Mighty Ducks" film series, Peter Bishop in "Fringe", and Cole Lockhart in "The Affair". Jackson won the Genie Award for Best Performance by an Actor in a Leading Role for his performance in the Canadian independent film "One Week".

Jackson was born in Vancouver, British Columbia, to parents John Carter and Fiona Jackson. His mother is a casting director. Jackson's father is from Texas; and his mother is a native of Ballyfermot, Dublin, Ireland, having immigrated to North America in the late 1960s. He has a younger sister, Aisleagh, and two older half brothers, Jonathan and Lyman. He was raised Catholic.

Jackson grew up in California until the age of 8. He moved to Vancouver with his mother and younger sister. He attended Ideal Mini School and later switched to Kitsilano Secondary School. In an interview with "The New York Times", Jackson said he was kicked out of high school once because of "The Jon Stewart Show": "[The show] played, at least where I grew up, at 1:30 in the morning, so I would stay up at night to watch Jon Stewart, but then I'd be too tired—or too lazy—to go to school in the morning. So I'd just take the first couple of classes off, 'cause I wanted to be fresh when I got there." He claims that the first time was because of "attitude" problems and that he "wasn't in the school spirit".

Jackson started acting in a small role in the film "Crooked Hearts" in 1991. The next year, he played the role of Charlie in a musical version of "Willie Wonka and the Chocolate Factory". At this point, with the help of the play's casting director Laura Kennedy, he joined the William Morris Agency. Soon after, he landed the role of Charlie (#96) in "The Mighty Ducks" series, playing a young and aspiring hockey player.

Joshua Jackson went on to appear as Pacey Witter on "Dawson's Creek", which ran on the WB network from 1998–2003, and also starred James Van Der Beek, Michelle Williams and Katie Holmes. While the show was on hiatus, he appeared in several movies including "Cruel Intentions" (a New York yuppie adaptation of "Les Liaisons dangereuses" that also starred Sarah Michelle Gellar and Ryan Phillippe), "The Skulls", "The Safety of Objects", "The Laramie Project" and a short cameo in the remake of "Ocean's Eleven" in which he appears as himself in a poker scene with Brad Pitt, George Clooney and Holly Marie Combs, among others. In 2000, he also guest-starred in Season 12 of "The Simpsons", voicing the character of Jesse Grass, a "hunky environmentalist" and love interest for Lisa Simpson in the episode "Lisa the Tree Hugger".

Shortly after "Dawson's Creek" ended in 2003, Jackson played the lead role in films alongside Dennis Hopper ("Americano"), Harvey Keitel ("Shadows in the Sun"), and Donald Sutherland ("Aurora Borealis"). In 2005, Jackson moved to the UK and made his stage debut on the London West End with Patrick Stewart in David Mamet's two-man play, "A Life in the Theatre". The play was a critical and popular success, and ran from February to April of that year. Jackson said that he would consider returning to the stage, to try his hand on Broadway. His next film role was in "Bobby", directed by Emilio Estevez, Jackson's co-star from "The Mighty Ducks". He played a lead role in "Shutter", a US remake of a Thai horror film of the same name. He starred and acted as executive producer in the Canadian independent film "One Week", which opened on March 6, 2009.

From 2008 to 2013, Jackson played the lead role of Peter Bishop in the science-fiction series "Fringe", created by JJ Abrams, Roberto Orci and Alex Kurtzman. The series appeared on the Fox TV network and was the second-highest rated new show of the 2008–2009 season after CBS's "The Mentalist". BuddyTV ranked him #9 on its "TV's 100 Sexiest Men of 2010" list, #19 in 2011 and #14 in 2012.

Jackson was nominated for Genie Award for Best Performance by an Actor in a Leading Role for the film "One Week". He won the award on April 12, 2010.
He held and hosted Pacey-Con in 2010, directly across the street from the San Diego Comic-Con, sporting a bowling shirt and giving out fan fiction he wrote himself to those waiting in the Comic-Con entrance line. Footage of the event was recorded for a video, entitled 'Pacey-Con', which he was filming for Will Ferrell's Funny or Die celebrity humor website.
In 2013 Jackson appeared in the IFC film "Inescapable" with Marisa Tomei and Alexander Siddig. Jackson wrote the first story from the comic book trilogy "Beyond the Fringe", titled "Peter and the Machine". Jackson currently stars in the fourth season of the successful Showtime television show, "The Affair", where he plays Cole Lockhart, the protagonist husband of the unfaithful Alison Lockhart.

In March 2018, Jackson makes a theatrical debut in Broadway, "Children of a Lesser God", where he plays James Leeds, an unconventional teacher at school for the deaf who got in a conflicted professional and romantic relationship with a former deaf student, Sarah Norman (Lauren Ridloff). The play runs through May 2018. 

Jackson was in a relationship with "Dawson's Creek" co-star Katie Holmes during the first two seasons of the show's run. Holmes claims Jackson was her first love.

Jackson began dating German actress Diane Kruger in 2006; the couple shared residences in Paris, Los Angeles and Vancouver. Jackson and Kruger ended their relationship in 2016, after 10 years together. He owns his childhood home in Topanga, California. He previously lived in Wilmington, North Carolina, where "Dawson's Creek" was filmed; and in New York, where "Fringe" filmed its first season. In 2009, he moved back to Vancouver for the shooting of the four following seasons before the show aired its last episode on January 18, 2013.

Jackson is a fan of the Vancouver Canucks ice hockey team. He was arrested on November 9, 2002, at a Carolina Hurricanes ice hockey game in Raleigh, North Carolina, after a quarrel with a security guard. He was charged with assault, affray and being intoxicated and disruptive, having a 0.14 blood alcohol content. Prosecutors agreed to dismiss the assault charge, and Jackson agreed to attend an alcohol education program and perform 24 hours of community service in order to have the remaining charge dropped.

In June 2018 it was reported that the star is dating actress & Cheddar TV host, Alyssa Julya Smith.



</doc>
<doc id="15588" url="https://en.wikipedia.org/wiki?curid=15588" title="Jung (disambiguation)">
Jung (disambiguation)

Carl Gustav Jung (1875–1961) was the founder of analytical psychology.

Jung may also refer to:





</doc>
<doc id="15593" url="https://en.wikipedia.org/wiki?curid=15593" title="JFK (disambiguation)">
JFK (disambiguation)

JFK are the initials of John F. Kennedy (1917–1963), the 35th President of the United States.
JFK may also refer to:



</doc>
<doc id="15596" url="https://en.wikipedia.org/wiki?curid=15596" title="John Ray">
John Ray

John Ray FRS (29 November 1627 – 17 January 1705) was an English naturalist widely regarded as one of the earliest of the English parson-naturalists. Until 1670, he wrote his name as John Wray. From then on, he used 'Ray', after "having ascertained that such had been the practice of his family before him".

He published important works on botany, zoology, and natural theology. His classification of plants in his "Historia Plantarum", was an important step towards modern taxonomy. Ray rejected the system of dichotomous division by which species were classified according to a pre-conceived, either/or type system , and instead classified plants according to similarities and differences that emerged from observation. He was the first to give a biological definition of the term "species".

John Ray was born in the village of Black Notley in Essex. He is said to have been born in the smithy, his father having been the village blacksmith. He was sent at the age of sixteen to Cambridge University: studying at Trinity College. His tutor at Trinity was James Duport, and his intimate friend and fellow-pupil the celebrated Isaac Barrow. Ray was chosen minor fellow of Trinity in 1649, and later major fellow. He held many college offices, becoming successively lecturer in Greek (1651), mathematics (1653), and humanity (1655), "praelector" (1657), frias (1657), and college steward (1659 and 1660); and according to the habit of the time, he was accustomed to preach in his college chapel and also at Great St Mary's, long before he took holy orders on 23 December 1660. Among these sermons were his discourses on "The wisdom of God manifested in the works of the creation", and "Deluge and Dissolution of the World". Ray was also highly regarded as a tutor and he communicated his own passion for natural history to several pupils. It has been generally asserted that Willughby was Ray's pupil at Cambridge, but what little evidence exists on the matter is rather against this supposition. 

After leaving Cambridge in 1662 he spent some time travelling both in Britain and the continent. In 1673, Ray married Margaret Oakley of Launton; in 1676 he went to Middleton Hall near Tamworth, and in 1677 to Falborne (or Faulkbourne) Hall in Essex. Finally, in 1679, he removed to his birthplace at Black Notley, where he afterwards remained. His life there was quiet and uneventful, although he had poor health, including chronic sores. Ray kept writing books and corresponded widely on scientific matters. He lived, in spite of his infirmities, to the age of seventy-seven, dying at Black Notley. He is buried in the churchyard of St Peter and St Paul where there is a memorial to him.

At Cambridge, Ray spent much of his time in the study of natural history, a subject which would occupy him for most of his life. When Ray found himself unable to subscribe as required by the ‘Bartholomew Act’ of 1662 he, along with 13 other college fellows, resigned his fellowship on 24 August 1662 rather than swear to the declaration that the Solemn League and Covenant was not binding on those who had taken it. Tobias Smollett quoted the reasoning given in the biography of Ray by William Derham:
"The reason of his refusal was not (says his biographer) as some have imagined, his having taken the solemn league and covenant; for that he never did, and often declared that he ever thought it an unlawful oath: but he said he could not say, for those that had taken the oath, that no obligation lay upon them, but feared there might."
His religious views were generally in accord with those imposed under the restoration of Charles II of England, and (though technically a nonconformist) he continued as a layman in the Established Church of England.

From this time onwards he seems to have depended chiefly on the bounty of his pupil Francis Willughby, who made Ray his constant companion while he lived. Willughby arranged that after his death, Ray would have 6 shillings a year for educating Willughby's two sons.

In the spring of 1663 Ray started together with Willughby and two other pupils (Philip Skippon and Nathaniel Bacon) on a tour through Europe, from which he returned in March 1666, parting from Willughby at Montpellier, whence the latter continued his journey into Spain. He had previously in three different journeys (1658, 1661, 1662) travelled through the greater part of Great Britain, and selections from his private notes of these journeys were edited by George Scott in 1760, under the title of "Mr Ray's Itineraries". Ray himself published an account of his foreign travel in 1673, entitled "Observations topographical, moral, and physiological, made on a Journey through part of the Low Countries, Germany, Italy, and France". From this tour Ray and Willughby returned laden with collections, on which they meant to base complete systematic descriptions of the animal and vegetable kingdoms. Willughby undertook the former part, but, dying in 1672, left only an ornithology and ichthyology for Ray to edit; while Ray used the botanical collections for the groundwork of his "Methodus plantarum nova" (1682), and his great "Historia generalis plantarum" (3 vols., 1686, 1688, 1704). The plants gathered on his British tours had already been described in his "Catalogus plantarum Angliae" (1670), which formed the basis for later English floras.

In 1667 Ray was elected Fellow of the Royal Society, and in 1669 he and Willughby published a paper on "Experiments concerning the Motion of Sap in Trees". In 1671, he presented the research of Francis Jessop on formic acid to the Royal Society.

In the 1690s, he published three volumes on religion—the most popular being "The Wisdom of God Manifested in the Works of the Creation" (1691), an essay describing evidence that all in nature and space is God's creation as in Bible is affirmed. In this volume, he moved on from the naming and cataloguing of species like his successor Carl Linnaeus. Instead, Ray considered species' lives and how nature worked as a whole, giving facts that are arguments for God's will expressed in His creation of all 'visible and invisible' (Colossians 1:16).
Ray gave an early description of dendrochronology, explaining for the ash tree how to find its age from its tree-rings.

Ray was the first person to produce a biological definition of species, in his 1686 "History of plants":

As outlined in his "Historia Plantarum" (1685–1703):

Ray published about 23 works, depending on how they are counted. The biological works were usually in Latin, the rest in English. His first publication, while at Cambridge, was the "Catalogus plantarum circa Cantabrigiam nascentium" (1660), followed by many works, botanical, zoological, theological and literary.



Including the various editions, there are 172 works of Ray, of which most are rare. The only libraries with substantial holdings are all in England. The list in order of holdings is:

Ray's biographer, Charles Raven, commented that "Ray sweeps away the litter of mythology and fable... and always insists upon accuracy of observation and description and the testing of every new discovery". Ray's works were directly influential on the development of taxonomy by Carl Linnaeus.

The Ray Society, named after John Ray, was founded in 1844. It is a scientific text publication society and registered charity, based at the Natural History Museum, London, which exists to publish books on natural history, with particular (but not exclusive) reference to the flora and fauna of the British Isles. As of 2017, the Society had published 179 volumes. 

The John Ray Society (a separate organisation) is the Natural Sciences Society at St Catharine's College, Cambridge. It organises a programme of events of interest to science students in the college. 

In 1986, to mark the 300th anniversary of the publication of Ray's "Historia Plantarum", there was a celebration of Ray's legacy in Braintree, Essex. A "John Ray Gallery" was opened in the Braintree Museum.

The John Ray Initiative (JRI) is an educational charity that seeks to reconcile scientific and Christian understandings of the environment. It was formed in 1997 in response to the global environmental crisis and the challenges of sustainable development and environmental stewardship. John Ray's writings proclaimed God as creator whose wisdom is "manifest in the works of creation", and as redeemer of all things. JRI aims to teach appreciation of nature, increase awareness of the state of the global environment, and to promote a Christian understanding of environmental issues.






</doc>
<doc id="15600" url="https://en.wikipedia.org/wiki?curid=15600" title="James Joyce">
James Joyce

James Augustine Aloysius Joyce (2 February 1882 – 13 January 1941) was an Irish novelist, short story writer, and poet. He contributed to the modernist avant-garde and is regarded as one of the most influential and important authors of the 20th century. Joyce is best known for "Ulysses" (1922), a landmark work in which the episodes of Homer's "Odyssey" are paralleled in a variety of literary styles, most famously stream of consciousness. Other well-known works are the short-story collection "Dubliners" (1914), and the novels "A Portrait of the Artist as a Young Man" (1916) and "Finnegans Wake" (1939). His other writings include three books of poetry, a play, his published letters and occasional journalism.

Joyce was born in 41 Brighton Square, Rathgar, Dublin, into a middle-class family. A brilliant student, he briefly attended the Christian Brothers-run O'Connell School before excelling at the Jesuit schools Clongowes and Belvedere, despite the chaotic family life imposed by his father's alcoholism and unpredictable finances. He went on to attend University College Dublin.

In 1904, in his early twenties, Joyce emigrated to continental Europe with his partner (and later wife) Nora Barnacle. They lived in Trieste, Paris, and Zurich. Although most of his adult life was spent abroad, Joyce's fictional universe centres on Dublin, and is populated largely by characters who closely resemble family members, enemies and friends from his time there. "Ulysses" in particular is set with precision in the streets and alleyways of the city. Shortly after the publication of "Ulysses", he elucidated this preoccupation somewhat, saying, "For myself, I always write about Dublin, because if I can get to the heart of Dublin I can get to the heart of all the cities of the world. In the particular is contained the universal."

On 2 February 1882, Joyce was born in Rathgar, Dublin, Ireland. Joyce's father was John Stanislaus Joyce and his mother was Mary Jane "May" Murray. He was the eldest of ten surviving siblings; two died of typhoid. James was baptised according to the Rites of the Catholic Church in the nearby St Joseph's Church in Terenure on 5 February 1882 by Rev. John O'Mulloy. Joyce's godparents were Philip and Ellen McCann.

John Stanislaus Joyce's family came from Fermoy in County Cork, and had owned a small salt and lime works. Joyce's paternal grandfather, James Augustine Joyce, married Ellen O'Connell, daughter of John O'Connell, a Cork Alderman who owned a drapery business and other properties in Cork City. Ellen's family claimed kinship with Daniel O'Connell, "The Liberator". The Joyce family's purported ancestor, Seán Mór Seoighe (fl. 1680) was a stonemason from Connemara.

In 1887, his father was appointed rate collector by Dublin Corporation; the family subsequently moved to the fashionable adjacent small town of Bray, from Dublin. Around this time Joyce was attacked by a dog, leading to his lifelong cynophobia. He suffered from astraphobia; a superstitious aunt had described thunderstorms as a sign of God's wrath.

In 1891 Joyce wrote a poem on the death of Charles Stewart Parnell. His father was angry at the treatment of Parnell by the Catholic Church, the Irish Home Rule Party and the British Liberal Party and the resulting collaborative failure to secure Home Rule for Ireland. The Irish Party had dropped Parnell from leadership. But the Vatican's role in allying with the British Conservative Party to prevent Home Rule left a lasting impression on the young Joyce. The elder Joyce had the poem printed and even sent a part to the Vatican Library. In November, John Joyce was entered in "Stubbs' Gazette" (a publisher of bankruptcies) and suspended from work. In 1893, John Joyce was dismissed with a pension, beginning the family's slide into poverty caused mainly by his drinking and financial mismanagement.

Joyce had begun his education at Clongowes Wood College, a Jesuit boarding school near Clane, County Kildare, in 1888 but had to leave in 1892 when his father could no longer pay the fees. Joyce then studied at home and briefly at the Christian Brothers O'Connell School on North Richmond Street, Dublin, before he was offered a place in the Jesuits' Dublin school, Belvedere College, in 1893. This came about because of a chance meeting his father had with a Jesuit priest who knew the family and Joyce was given a reduction in fees to attend Belvedere. In 1895, Joyce, now aged 13, was elected to join the Sodality of Our Lady by his peers at Belvedere. The philosophy of Thomas Aquinas continued to have a strong influence on him for most of his life.

Joyce enrolled at the recently established University College Dublin (UCD) in 1898, studying English, French and Italian. He became active in theatrical and literary circles in the city. In 1900 his laudatory review of Henrik Ibsen's "When We Dead Awaken" was published in "The Fortnightly Review"; it was his first publication and, after learning basic Norwegian to send a fan letter to Ibsen, he received a letter of thanks from the dramatist. Joyce wrote a number of other articles and at least two plays (since lost) during this period. Many of the friends he made at University College Dublin appeared as characters in Joyce's works. His closest colleagues included leading figures of the generation, most notably, Tom Kettle, Francis Sheehy-Skeffington and Oliver St. John Gogarty. Joyce was first introduced to the Irish public by Arthur Griffith in his newspaper, "United Irishman", in November 1901. Joyce had written an article on the Irish Literary Theatre and his college magazine refused to print it. Joyce had it printed and distributed locally. Griffith himself wrote a piece decrying the censorship of the student James Joyce. In 1901, the National Census of Ireland lists James Joyce (19) as an English- and Irish-speaking scholar living with his mother and father, six sisters and three brothers at Royal Terrace (now Inverness Road), Clontarf, Dublin.

After graduating from UCD in 1902, Joyce left for Paris to study medicine, but he soon abandoned this. Richard Ellmann suggests that this may have been because he found the technical lectures in French too difficult. Joyce had already failed to pass chemistry in English in Dublin. But Joyce claimed ill health as the problem and wrote home that he was unwell and complained about the cold weather. He stayed on for a few months, appealing for finance his family could ill afford and reading late in the Bibliothèque Sainte-Geneviève. When his mother was diagnosed with cancer, his father sent a telegram which read, "NOTHER DYING COME HOME FATHER". Joyce returned to Ireland. Fearing for her son's impiety, his mother tried unsuccessfully to get Joyce to make his confession and to take communion. She finally passed into a coma and died on 13 August, James and his brother Stanislaus having refused to kneel with other members of the family praying at her bedside. After her death he continued to drink heavily, and conditions at home grew quite appalling. He scraped together a living reviewing books, teaching, and singing—he was an accomplished tenor, and won the bronze medal in the 1904 Feis Ceoil.

On 7 January 1904 Joyce attempted to publish "A Portrait of the Artist", an essay-story dealing with aesthetics, only to have it rejected by the free-thinking magazine "Dana". He decided, on his twenty-second birthday, to revise the story into a novel he called "Stephen Hero". It was a fictional rendering of Joyce's youth, but he eventually grew frustrated with its direction and abandoned this work. It was never published in this form, but years later, in Trieste, Joyce completely rewrote it as "A Portrait of the Artist as a Young Man". The unfinished "Stephen Hero" was published after his death.

Also in 1904 he met Nora Barnacle, a young woman from Galway city who was working as a chambermaid. On 16 June 1904 they had their first outing together, they walked to the Dublin suburb of Ringsend, where Nora masturbated him. This event was commemorated by providing the date for the action of "Ulysses" (as "Bloomsday").

Joyce remained in Dublin for some time longer, drinking heavily. After one of his drinking binges, he got into a fight over a misunderstanding with a man in St Stephen's Green; he was picked up and dusted off by a minor acquaintance of his father, Alfred H. Hunter, who took him into his home to tend to his injuries. Hunter was rumoured to be a Jew and to have an unfaithful wife, and would serve as one of the models for Leopold Bloom, the protagonist of "Ulysses". He took up with the medical student Oliver St. John Gogarty, who informed the character for Buck Mulligan in "Ulysses". After six nights in the Martello Tower that Gogarty was renting in Sandycove, he left in the middle of the night following an altercation which involved another student he lived with, the unstable Dermot Chenevix Trench (Haines in "Ulysses"), who fired a pistol at some pans hanging directly over Joyce's bed. Joyce walked the back to Dublin to stay with relatives for the night, and sent a friend to the tower the next day to pack his trunk. Shortly after, the couple left Ireland to live on the continent.

Joyce and Nora went into self-imposed exile, moving first to Zurich in Switzerland, where he ostensibly taught English at the Berlitz Language School through an agent in England. It later came to fact that the agent had been swindled; the director of the school sent Joyce on to Trieste, which was then part of Austria-Hungary (until the First World War), and is today part of Italy. Once again, he found there was no position for him, but with the help of Almidano Artifoni, director of the Trieste Berlitz School, he finally secured a teaching position in Pola, then also part of Austria-Hungary (today part of Croatia). He stayed there, teaching English mainly to Austro-Hungarian naval officers stationed at the Pola base, from October 1904 until March 1905, when the Austrians—having discovered an espionage ring in the city—expelled all aliens. With Artifoni's help, he moved back to Trieste and began teaching English there. He remained in Trieste for most of the next ten years.

Later that year Nora gave birth to their first child, George (known as Giorgio). Joyce persuaded his brother, Stanislaus, to join him in Trieste, and secured a teaching position for him at the school. Joyce sought to augment his family's meagre income with his brother's earnings. Stanislaus and Joyce had strained relations while they lived together in Trieste, arguing about Joyce's drinking habits and frivolity with money.

Joyce became frustrated with life in Trieste and moved to Rome in late 1906, taking employment as a clerk in a bank. He disliked Rome and returned to Trieste in early 1907. His daughter Lucia was born later that year.

Joyce returned to Dublin in mid-1909 with George, to visit his father and work on getting "Dubliners" published. He visited Nora's family in Galway and liked Nora's mother very much. While preparing to return to Trieste he decided to take one of his sisters, Eva, back with him to help Nora run the home. He spent a month in Trieste before returning to Dublin, this time as a representative of some cinema owners and businessmen from Trieste. With their backing he launched Ireland's first cinema, the Volta Cinematograph, which was well-received, but fell apart after Joyce left. He returned to Trieste in January 1910 with another sister, Eileen, in tow. Eva became homesick for Dublin and returned there a few years later, but Eileen spent the rest of her life on the continent, eventually marrying the Czech bank cashier Frantisek Schaurek.

Joyce returned to Dublin again briefly in mid-1912 during his years-long fight with Dublin publisher George Roberts over the publication of "Dubliners". His trip was once again fruitless, and on his return he wrote the poem "Gas from a Burner", an invective against Roberts. After this trip, he never again came closer to Dublin than London, despite many pleas from his father and invitations from his fellow Irish writer William Butler Yeats.

One of his students in Trieste was Ettore Schmitz, better known by the pseudonym Italo Svevo. They met in 1907 and became lasting friends and mutual critics. Schmitz was a Catholic of Jewish origin and became a primary model for Leopold Bloom; most of the details about the Jewish faith in "Ulysses" came from Schmitz's responses to queries from Joyce. While living in Trieste, Joyce was first beset with eye problems that ultimately required over a dozen surgical operations.

Joyce concocted a number of money-making schemes during this period, including an attempt to become a cinema magnate in Dublin. He frequently discussed but ultimately abandoned a plan to import Irish tweed to Trieste. Correspondence relating to that venture with the Irish Woollen Mills were for a long time displayed in the windows of their premises in Dublin. Joyce's skill at borrowing money saved him from indigence. What income he had came partially from his position at the Berlitz school and partially from teaching private students.

In 1915, after most of his students in Trieste were conscripted to fight in the First World War, Joyce moved to Zurich. Two influential private students, Baron Ambrogio Ralli and Count Francesco Sordina, petitioned officials for an exit permit for the Joyces, who in turn agreed not to take any action against the emperor of Austria-Hungary during the war.

Joyce set himself to finishing "Ulysses" in Paris, delighted to find that he was gradually gaining fame as an avant-garde writer. A further grant from Harriet Shaw Weaver meant he could devote himself full-time to writing again, as well as consort with other literary figures in the city. During this time, Joyce's eyes began to give him more and more problems and he often wore an eyepatch. He was treated by Louis Borsch in Paris, undergoing nine operations before Borsch's death in 1929. Throughout the 1930s he travelled frequently to Switzerland for eye surgeries and for treatments for his daughter Lucia, who, according to the Joyces, suffered from schizophrenia. Lucia was analysed by Carl Jung at the time, who after reading "Ulysses" is said to have concluded that her father had schizophrenia. Jung said that she and her father were two people heading to the bottom of a river, except that Joyce was diving and Lucia was sinking.

In Paris, Maria and Eugene Jolas nursed Joyce during his long years of writing "Finnegans Wake". Were it not for their support (along with Harriet Shaw Weaver's constant financial support), there is a good possibility that his books might never have been finished or published. In their literary magazine "transition", the Jolases published serially various sections of "Finnegans Wake" under the title "Work in Progress". Joyce returned to Zurich in late 1940, fleeing the Nazi occupation of France.

The issue of Joyce's relationship with religion is somewhat controversial. Early in life, he lapsed from Catholicism, according to first-hand testimonies coming from himself, his brother Stanislaus Joyce, and his wife:

My mind rejects the whole present social order and Christianity—home, the recognised virtues, classes of life and religious doctrines. [...] Six years ago I left the Catholic church, hating it most fervently. I found it impossible for me to remain in it on account of the impulses of my nature. I made secret war upon it when I was a student and declined to accept the positions it offered me. By doing this I made myself a beggar but I retained my pride. Now I make open war upon it by what I write and say and do.

When the arrangements for Joyce's burial were being made, a Catholic priest offered a religious service, which Joyce's wife Nora declined, saying: "I couldn't do that to him."

Leonard Strong, William T. Noon, Robert Boyle and others have argued that Joyce, later in life, reconciled with the faith he rejected earlier in life and that his parting with the faith was succeeded by a not so obvious reunion, and that "Ulysses" and "Finnegans Wake" are essentially Catholic expressions. Likewise, Hugh Kenner and T. S. Eliot believed they saw between the lines of Joyce's work the outlook of a serious Christian and that beneath the veneer of the work lies a remnant of Catholic belief and attitude. Kevin Sullivan maintains that, rather than reconciling with the faith, Joyce never left it. Critics holding this view insist that Stephen, the protagonist of the semi-autobiographical "A Portrait of the Artist as a Young Man" as well as "Ulysses", is not Joyce. Somewhat cryptically, in an interview after completing Ulysses, in response to the question "When did you leave the Catholic Church", Joyce answered, "That's for the Church to say." Eamonn Hughes maintains that Joyce takes a dialectic approach, both affirming and denying, saying that Stephen's much noted "non-serviam" is qualified—"I will not serve "that which I no longer believe"...", and that the "non-serviam" will always be balanced by Stephen's "I am a servant..." and Molly's "yes". He attended Catholic Mass and Orthodox Sacred Liturgy, especially during Holy Week, purportedly for aesthetic reasons. His sisters noted his Holy Week attendance and that he did not seek to dissuade them. One friend witnessed him cry "secret tears" upon hearing Jesus' words on the cross and another accused him of being a "believer at heart" because of his frequent attendance at church.

Umberto Eco compares Joyce to the ancient "episcopi vagantes" (wandering bishops) in the Middle Ages. They left a discipline, not a cultural heritage or a way of thinking. Like them, the writer retains the sense of blasphemy held as a liturgical ritual.

Some critics and biographers have opined along the lines of Andrew Gibson: "The modern James Joyce may have vigorously resisted the oppressive power of Catholic tradition. But there was another Joyce who asserted his allegiance to that tradition, and never left it, or wanted to leave it, behind him." Gibson argues that Joyce "remained a Catholic intellectual if not a believer" since his thinking remained influenced by his cultural background, even though he lived apart from that culture. His relationship with religion was complex and not easily understood, even perhaps by himself. He acknowledged the debt he owed to his early Jesuit training. Joyce told the sculptor August Suter, that from his Jesuit education, he had 'learnt to arrange things in such a way that they become easy to survey and to judge.'

On 11 January 1941, Joyce underwent surgery in Zurich for a perforated duodenal ulcer. He fell into a coma the following day. He awoke at 2 a.m. on 13 January 1941, and asked a nurse to call his wife and son, before losing consciousness again. They were en route when he died 15 minutes later, less than a month short of his 59th birthday.

His body was interred in the Fluntern Cemetery, Zurich. The Swiss tenor Max Meili sang "Addio terra, addio cielo" from Monteverdi's "L'Orfeo" at the burial service. Although two senior Irish diplomats were in Switzerland at the time, neither attended Joyce's funeral, and the Irish government later declined Nora's offer to permit the repatriation of Joyce's remains. When Joseph Walshe secretary at the Department of External Affairs in Dublin was informed of Joyce's death by Frank Cremins, chargé d'affaires at Berne, Walshe responded "Please wire details of Joyce's death. If possible find out did he die a Catholic? Express sympathy with Mrs Joyce and explain inability to attend funeral". Buried originally in an ordinary grave, Joyce was moved in 1966 to a more prominent "honour grave," with a seated portrait statue by American artist Milton Hebald nearby. Nora, whom he had married in 1931, survived him by 10 years. She is buried by his side, as is their son Giorgio, who died in 1976.

"Dubliners" is a collection of fifteen short stories by Joyce, first published in 1914. They form a naturalistic depiction of Irish middle class life in and around Dublin in the early years of the 20th century.

The stories were written when Irish nationalism was at its peak and a search for a national identity and purpose was raging; at a crossroads of history and culture, Ireland was jolted by converging ideas and influences. The stories centre on Joyce's idea of an epiphany: a moment when a character experiences a life-changing self-understanding or illumination. Many of the characters in "Dubliners" later appear in minor roles in Joyce's novel "Ulysses". The initial stories in the collection are narrated by child protagonists. Subsequent stories deal with the lives and concerns of progressively older people. This aligns with Joyce's tripartite division of the collection into childhood, adolescence and maturity.

"A Portrait of the Artist as a Young Man" is a nearly complete rewrite of the abandoned novel "Stephen Hero". Joyce attempted to burn the original manuscript in a fit of rage during an argument with Nora, though to his subsequent relief it was rescued by his sister. A "Künstlerroman", "Portrait" is a heavily autobiographical coming-of-age novel depicting the childhood and adolescence of the protagonist Stephen Dedalus and his gradual growth into artistic self-consciousness. Some hints of the techniques Joyce frequently employed in later works, such as stream of consciousness, interior monologue, and references to a character's psychic reality rather than to his external surroundings are evident throughout this novel.

Despite early interest in the theatre, Joyce published only one play, "Exiles", begun shortly after the outbreak of the First World War in 1914 and published in 1918. A study of a husband and wife relationship, the play looks back to "The Dead" (the final story in "Dubliners") and forward to "Ulysses", which Joyce began around the time of the play's composition.

Joyce published a number of books of poetry. His first mature published work was the satirical broadside "The Holy Office" (1904), in which he proclaimed himself to be the superior of many prominent members of the Celtic Revival. His first full-length poetry collection "Chamber Music" (1907; referring, Joyce joked, to the sound of urine hitting the side of a chamber pot) consisted of 36 short lyrics. This publication led to his inclusion in the "Imagist Anthology", edited by Ezra Pound, who was a champion of Joyce's work. Other poetry Joyce published in his lifetime include "Gas From A Burner" (1912), "Pomes Penyeach" (1927), and "Ecce Puer" (written in 1932 to mark the birth of his grandson and the recent death of his father). It was published by the Black Sun Press in "Collected Poems" (1936).

As he was completing work on "Dubliners" in 1906, Joyce considered adding another story featuring a Jewish advertising canvasser called Leopold Bloom under the title "Ulysses". Although he did not pursue the idea further at the time, he eventually commenced work on a novel using both the title and basic premise in 1914. The writing was completed in October 1921. Three more months were devoted to working on the proofs of the book before Joyce halted work shortly before his self-imposed deadline, his 40th birthday (2 February 1922).

Thanks to Ezra Pound, serial publication of the novel in the magazine "The Little Review" began in March 1918. This magazine was edited by Margaret C. Anderson and Jane Heap, with the intermittent financial backing of John Quinn, a successful New York commercial lawyer with an interest in contemporary experimental art and literature. Unfortunately, this publication encountered problems with New York Postal Authorities; serialisation ground to a halt in December 1920; the editors were convicted of publishing obscenity in February 1921. Although the conviction was based on the "Nausicaä" episode of "Ulysses", "The Little Review" had fuelled the fires of controversy with dada poet Elsa von Freytag-Loringhoven's defence of "Ulysses" in an essay "The Modest Woman." Joyce's novel was not published in the United States until 1934.

Partly because of this controversy, Joyce found it difficult to get a publisher to accept the book, but it was published in 1922 by Sylvia Beach from her well-known Rive Gauche bookshop, "Shakespeare and Company". An English edition published the same year by Joyce's patron, Harriet Shaw Weaver, ran into further difficulties with the United States authorities, and 500 copies that were shipped to the States were seized and possibly destroyed. The following year, John Rodker produced a print run of 500 more intended to replace the missing copies, but these were burned by English customs at Folkestone. A further consequence of the novel's ambiguous legal status as a banned book was that a number of "bootleg" versions appeared, most notably a number of pirate versions from the publisher Samuel Roth. In 1928, a court injunction against Roth was obtained and he ceased publication.

With the appearance of both "Ulysses" and T. S. Eliot's poem, "The Waste Land", 1922 was a key year in the history of English-language literary modernism. In "Ulysses", Joyce employs stream of consciousness, parody, jokes, and virtually every other literary technique to present his characters. The action of the novel, which takes place in a single day, 16 June 1904, sets the characters and incidents of the Odyssey of Homer in modern Dublin and represents Odysseus (Ulysses), Penelope and Telemachus in the characters of Leopold Bloom, his wife Molly Bloom and Stephen Dedalus, parodically contrasted with their lofty models. The book explores various areas of Dublin life, dwelling on its squalor and monotony. Nevertheless, the book is also an affectionately detailed study of the city, and Joyce claimed that if Dublin were to be destroyed in some catastrophe it could be rebuilt, brick by brick, using his work as a model. In order to achieve this level of accuracy, Joyce used the 1904 edition of Thom's Directory—a work that listed the owners and/or tenants of every residential and commercial property in the city. He also bombarded friends still living there with requests for information and clarification.

The book consists of 18 chapters, each covering roughly one hour of the day, beginning around about 8 a.m. and ending sometime after 2 a.m. the following morning. Each of the 18 chapters of the novel employs its own literary style. Each chapter also refers to a specific episode in Homer's Odyssey and has a specific colour, art or science and bodily organ associated with it. This combination of kaleidoscopic writing with an extreme formal, schematic structure represents one of the book's major contributions to the development of 20th century modernist literature. The use of classical mythology as a framework for his book and the near-obsessive focus on external detail in a book in which much of the significant action is happening inside the minds of the characters are others. Nevertheless, Joyce complained that, "I may have oversystematised "Ulysses"," and played down the mythic correspondences by eliminating the chapter titles that had been taken from Homer. Joyce was reluctant to publish the chapter titles because he wanted his work to stand separately from the Greek form. It was only when Stuart Gilbert published his critical work on "Ulysses" in 1930 that the schema was supplied by Joyce to Gilbert. But as Terrence Killeen points out this schema was developed after the novel had been written and was not something that Joyce consulted as he wrote the novel.

Having completed work on "Ulysses", Joyce was so exhausted that he did not write a line of prose for a year. On 10 March 1923 he informed a patron, Harriet Weaver: "Yesterday I wrote two pages—the first I have since the final "Yes" of "Ulysses". Having found a pen, with some difficulty I copied them out in a large handwriting on a double sheet of foolscap so that I could read them. "Il lupo perde il pelo ma non il vizio", the Italians say. 'The wolf may lose his skin but not his vice' or 'the leopard cannot change his spots.'" Thus was born a text that became known, first, as "Work in Progress" and later "Finnegans Wake".

By 1926 Joyce had completed the first two parts of the book. In that year, he met Eugene and Maria Jolas who offered to serialise the book in their magazine "transition". For the next few years, Joyce worked rapidly on the new book, but in the 1930s, progress slowed considerably. This was due to a number of factors, including the death of his father in 1931, concern over the mental health of his daughter Lucia, and his own health problems, including failing eyesight. Much of the work was done with the assistance of younger admirers, including Samuel Beckett. For some years, Joyce nursed the eccentric plan of turning over the book to his friend James Stephens to complete, on the grounds that Stephens was born in the same hospital as Joyce exactly one week later, and shared the first name of both Joyce and of Joyce's fictional alter-ego, an example of Joyce's superstitions.

Reaction to the work was mixed, including negative comment from early supporters of Joyce's work, such as Pound and the author's brother, Stanislaus Joyce. To counteract this hostile reception, a book of essays by supporters of the new work, including Beckett, William Carlos Williams and others was organised and published in 1929 under the title "Our Exagmination Round His Factification for Incamination of Work in Progress". At his 57th birthday party at the Jolases' home, Joyce revealed the final title of the work and "Finnegans Wake" was published in book form on 4 May 1939. Later, further negative comments surfaced from doctor and author Hervey Cleckley, who questioned the significance others had placed on the work. In his book, "The Mask of Sanity", Cleckley refers to "Finnegans Wake" as "a 628-page collection of erudite gibberish indistinguishable to most people from the familiar word salad produced by hebephrenic patients on the back wards of any state hospital."

Joyce's method of stream of consciousness, literary allusions and free dream associations was pushed to the limit in "Finnegans Wake", which abandoned all conventions of plot and character construction and is written in a peculiar and obscure English, based mainly on complex multi-level puns. This approach is similar to, but far more extensive than that used by Lewis Carroll in "Jabberwocky". This has led many readers and critics to apply Joyce's oft-quoted description in the "Wake" of "Ulysses" as his "usylessly unreadable Blue Book of Eccles" to the "Wake" itself. However, readers have been able to reach a consensus about the central cast of characters and general plot.

Much of the wordplay in the book stems from the use of multilingual puns which draw on a wide range of languages. The role played by Beckett and other assistants included collating words from these languages on cards for Joyce to use and, as Joyce's eyesight worsened, of writing the text from the author's dictation.

The view of history propounded in this text is very strongly influenced by Giambattista Vico, and the metaphysics of Giordano Bruno of Nola are important to the interplay of the "characters." Vico propounded a cyclical view of history, in which civilisation rose from chaos, passed through theocratic, aristocratic, and democratic phases, and then lapsed back into chaos. The most obvious example of the influence of Vico's cyclical theory of history is to be found in the opening and closing words of the book. "Finnegans Wake" opens with the words "riverrun, past Eve and Adam's, from swerve of shore to bend of bay, brings us by a commodius vicus of recirculation back to Howth Castle and Environs." ("vicus" is a pun on Vico) and ends "A way a lone a last a loved a long the." In other words, the book ends with the beginning of a sentence and begins with the end of the same sentence, turning the book into one great cycle. Indeed, Joyce said that the ideal reader of the "Wake" would suffer from "ideal insomnia" and, on completing the book, would turn to page one and start again, and so on in an endless cycle of reading.

Joyce's work has been an important influence on writers and scholars such as Samuel Beckett, Seán Ó Ríordáin, Jorge Luis Borges, Flann O'Brien, Salman Rushdie, Robert Anton Wilson, John Updike, David Lodge and Joseph Campbell. "Ulysses" has been called "a demonstration and summation of the entire [Modernist] movement". The Bulgarian-French literary theorist Julia Kristéva characterised Joyce's novel writing as "polyphonic" and a hallmark of postmodernity alongside the poets Mallarmé and Rimbaud.
Some scholars, notably Vladimir Nabokov, have reservations, often championing some of his fiction while condemning other works. In Nabokov's opinion, "Ulysses" was brilliant, while "Finnegans Wake" was horrible.

Joyce's influence is also evident in fields other than literature. The sentence "Three quarks for Muster Mark!" in Joyce's "Finnegans Wake" is the source of the word "quark", the name of one of the elementary particles proposed by the physicist Murray Gell-Mann in 1963.

The work and life of Joyce is celebrated annually on 16 June, known as Bloomsday, in Dublin and in an increasing number of cities worldwide, and critical studies in scholarly publications, such as the "James Joyce Quarterly", continue. Both popular and academic uses of Joyce's work were hampered by restrictions imposed by Stephen J. Joyce, Joyce's grandson and executor of his literary estate. On 1 January 2012, those restrictions were lessened by the expiry of copyright protection of much of the published work of James Joyce.

In April 2013 the Central Bank of Ireland issued a silver €10 commemorative coin in honour of Joyce that misquoted a famous line from "Ulysses".


Posthumous publications

Joyce Papers, National Library of Ireland

Electronic editions
Resources


</doc>
<doc id="15601" url="https://en.wikipedia.org/wiki?curid=15601" title="Judo">
Judo

The philosophy and subsequent pedagogy developed for judo became the model for other modern Japanese martial arts that developed from .

The early history of judo is inseparable from its founder, Japanese polymath and educator , born . Kano was born into a relatively affluent family. His father, Jirosaku, was the second son of the head priest of the Shinto Hiyoshi shrine in Shiga Prefecture. He married Sadako Kano, daughter of the owner of Kiku-Masamune sake brewing company and was adopted by the family, changing his name to Kano. He ultimately became an official in the Shogunal government.

Jigoro Kano had an academic upbringing and, from the age of seven, he studied English, and the under a number of tutors. When he was fourteen, Kano began boarding at an English-medium school, Ikuei-Gijuku in Shiba, Tokyo. The culture of bullying endemic at this school was the catalyst that caused Kano to seek out a at which to train.

Early attempts to find a jujutsu teacher who was willing to take him on met with little success. With the fall of the Tokugawa shogunate in the Meiji Restoration of 1868, jujutsu had become unfashionable in an increasingly westernised Japan. Many of those who had once taught the art had been forced out of teaching or become so disillusioned with it that they had simply given up. Nakai Umenari, an acquaintance of Kanō's father and a former soldier, agreed to show him "kata", but not to teach him. The caretaker of Jirosaku's second house, Katagiri Ryuji, also knew jujutsu, but would not teach it as he believed it was no longer of practical use. Another frequent visitor, Imai Genshiro of school of jujutsu, also refused. Several years passed before he finally found a willing teacher.

In 1877, as a student at the Tokyo-"Kaisei" school (soon to become part of the newly founded Tokyo Imperial University), Kano learned that many jujutsu teachers had been forced to pursue alternative careers, frequently opening . After inquiring at a number of these, Kano was referred to Fukuda Hachinosuke (c.1828–1880), a teacher of the of jujutsu, who had a small nine mat dojo where he taught five students. Fukuda is said to have emphasized technique over formal exercise, sowing the seeds of Kano's emphasis on in judo.

On Fukuda's death in 1880, Kano, who had become his keenest and most able student in both "randori" and , was given the of the Fukuda dojo. Kano chose to continue his studies at another "Tenjin Shin'yō-ryū" school, that of Iso Masatomo (c.1820–1881). Iso placed more emphasis on the practice of "kata", and entrusted "randori" instruction to assistants, increasingly to Kano. Iso died in June 1881 and Kano went on to study at the dojo of Iikubo Tsunetoshi (1835–1889) of . Like Fukuda, Iikubo placed much emphasis on "randori", with "Kitō-ryū" having a greater focus on .

In February 1882, Kano founded a school and dojo at the , a Buddhist temple in what was then the Shitaya ward of Tokyo (now the Higashi Ueno district of Taitō ward). Iikubo, Kano's "Kitō-ryū" instructor, attended the dojo three days a week to help teach and, although two years would pass before the temple would be called by the name , and Kano had not yet received his in "Kitō-ryū", this is now regarded as the Kodokan founding.

The "Eisho-ji" dojo was a relatively small affair, consisting of a twelve mat training area. Kano took in resident and non-resident students, the first two being Tomita Tsunejirō and Shiro Saigo. In August, the following year, the pair were granted grades, the first that had been awarded in any martial art.

Central to Kano's vision for judo were the principles of and . He illustrated the application of "seiryoku zen'yō" with the concept of :

Kano realised that "seiryoku zen'yō", initially conceived as a jujutsu concept, had a wider philosophical application. Coupled with the Confucianist-influenced "jita kyōei", the wider application shaped the development of judo from a to a . Kano rejected techniques that did not conform to these principles and emphasised the importance of efficiency in the execution of techniques. He was convinced that practice of jujutsu while conforming to these ideals was a route to self-improvement and the betterment of society in general. He was, however, acutely conscious of the Japanese public's negative perception of jujutsu:

Kano believed that ""jūjutsu"" was insufficient to describe his art: although means "art" or "means", it implies a method consisting of a collection of physical techniques. Accordingly, he changed the second character to , meaning way, road or path, which implies a more philosophical context than "jutsu" and has a common origin with the Chinese concept of "tao". Thus Kano renamed it .

There are three basic categories of in judo: , and . Judo is most known for "nage-waza" and "katame-waza".

Judo practitioners typically devote a portion of each practice session to , in order that "nage-waza" can be practiced without significant risk of injury. Several distinct types of "ukemi" exist, including ; ; ; and 

The person who performs a "Waza" is known as and the person to whom it is performed is known as .

"Nage waza" include all techniques in which "tori" attempts to throw or trip "uke", usually with the aim of placing "uke" on his back. Each technique has three distinct stages:

Before an effective "kuzushi" can be performed, it is important to establish a firm grip (組み方, "kumi kata").

"Nage waza" are typically drilled by the use of , repeated turning-in, taking the throw up to the point of "kake".

Traditionally, "nage waza" are further categorised into , throws that are performed with "tori" maintaining an upright position, and , throws in which "tori" sacrifices his upright position in order to throw "uke".

"Tachi-waza" are further subdivided into , in which "tori" predominantly uses his arms to throw "uke"; throws that predominantly use a lifting motion from the hips; and , throws in which "tori" predominantly utilises his legs.

"Katame-waza" is further categorised into , in which "tori" traps and pins "uke" on his back on the floor; , in which "tori" attempts to force a submission by choking or strangling "uke"; and , in which "tori" attempts to submit "uke" by painful manipulation of his joints.

A related concept is that of , in which "waza" are applied from a non-standing position.

In competitive judo, "Kansetsu-waza" is currently limited to elbow joint manipulation. Manipulation and locking of other joints can be found in various kata, such as "Katame-no-kata" and "Kodokan goshin jutsu".

"Atemi-waza" are techniques in which "tori" disables "uke" with a strike to a vital point. "Atemi-waza" are not permitted outside of "kata".

Judo pedagogy emphasizes . This term covers a variety of forms of practice, and the intensity at which it is carried out varies depending on intent and the level of expertise of the participants. At one extreme, is a compliant style of randori, known as , in which neither participant offers resistance to their partner's attempts to throw. A related concept is that of , in which an experienced judoka allows himself to be thrown by his less-experienced partner. At the opposite extreme from "yakusoku geiko" is the hard style of randori that seeks to emulate the style of judo seen in competition. While hard randori is the cornerstone of judo, over-emphasis of the competitive aspect is seen as undesirable by traditionalists if the intent of the randori is to "win" rather than to learn.

 are pre-arranged patterns of techniques and in judo, with the exception of the "Seiryoku-Zen'yō Kokumin-Taiiku", they are all practised with a partner. Their purposes include illustrating the basic principles of judo, demonstrating the correct execution of a technique, teaching the philosophical tenets upon which judo is based, allowing for the practice of techniques that are not allowed in randori, and to preserve ancient techniques that are historically important but are no longer used in contemporary judo.

There are ten kata that are recognized by the Kodokan today:


In addition, there are a number of commonly practiced kata that are not recognised by the Kodokan. Some of the more common kata include:



 is a vitally important aspect of judo. In 1899, Kano was asked to chair a committee of the Dai Nippon Butoku Kai to draw up the first formal set of contest rules for jujutsu. These rules were intended to cover contests between different various traditional schools of jujutsu as well as practitioners of Kodokan judo. Contests were 15 minutes long and were judged on the basis of nage waza and katame waza, excluding atemi waza. Wins were by two ippons, awarded in every four-main different path of winning alternatives, by "Throwing", where the opponent's back strikes flat onto the mat with sufficient force, by "Pinning" them on their back for a "sufficient" amount of time, or by Submission, which could be achieved via "Shime-waza" or "Kansetsu-waza", in which the opponent was forced to give himself or herself up or summon a referee's or corner-judge's stoppage. Finger, toe and ankle locks were prohibited. In 1900, these rules were adopted by the Kodokan with amendments made to prohibit all joint locks for kyu grades and added wrist locks to the prohibited kansetsu-waza for dan grades. It was also stated that the ratio of tachi-waza to ne-waza should be between 70% to 80% for kyu grades and 60% to 70% for dan grades.

In 1916, additional rulings were brought in to further limit "kansetsu waza" with the prohibition of "ashi garami" and neck locks, as well as "do jime". These were further added to in 1925.

The first time judo was seen in the Olympic Games was in an informal demonstration hosted by Kano at the 1932 Games. However, Kano was ambivalent about judo's potential inclusion as an Olympic sport:
Nevertheless, judo became an Olympic sport for men in the 1964 Games in Tokyo. The Olympic Committee initially dropped judo for the 1968 Olympics, meeting protests. Dutchman Anton Geesink won the first Olympic gold medal in the open division of judo by defeating Akio Kaminaga of Japan. The women's event was introduced at the Olympics in 1988 as a demonstration event, and an official medal event in 1992.

Penalties may be given for: passivity or preventing progress in the match; for safety infringements for example by using prohibited techniques, or for behavior that is deemed to be against the spirit of judo. Fighting must be stopped if a participant is outside the designated area on the mat.

There are currently seven weight divisions, subject to change by governing bodies, and may be modified based on the age of the competitors:

A throw that places the opponent on his back with impetus and control scores an , winning the contest. A lesser throw, where the opponent is thrown onto his back, but with insufficient force to merit an ippon, scores a . Formerly, two scores of waza-ari equalled an ippon and a throw that places the opponent onto his side scores a .

The International Judo Federation recently announced changes in evaluation of points. There will only be ippon and waza-ari scores given during a match with yuko scores now included within waza-ari. Multiple waza-ari scores are no longer converted into ippon scores.

Ippon is scored in "ne-waza" for pinning an opponent on his back with a recognised "osaekomi-waza" for 20 seconds or by forcing a submission through "shime-waza" or "kansetsu-waza". A submission is signalled by tapping the mat or the opponent at least twice with the hand or foot, or by saying . A pin lasting for less than 20 seconds, but more than 10 seconds scores waza-ari (formerly waza-ari was awarded for holds of longer than 15 seconds and yuko for holds of longer than 10 seconds).

Formerly, there was an additional score that was lesser to yuko, that of . This has since been removed.

If the scores are identical at the end of the match, the contest is resolved by the "Golden Score" rule. "Golden Score" is a sudden death situation where the clock is reset to match-time, and the first contestant to achieve any score wins. If there is no score during this period, then the winner is decided by , the majority opinion of the referee and the two corner judges.

There have been changes to the scoring. In January 2013, the Hantei was removed and the "Golden Score" no longer has a time limit. The match would continue until a judoka scored through a technique or if the opponent is penalised (Shido).

Two types of penalties may be awarded. A shido (指導 - literally "guidance") is awarded for minor rule infringements. A shido can also be awarded for a prolonged period of non-aggression. Recent rule changes allow for the first shidos to result in only warnings. If there is a tie, then and only then, will the number of shidos (if less than three) be used to determine the winner. After three shidos are given, the victory is given to the opponent, constituting an indirect hansoku-make (反則負け - literally "foul-play defeat"), but does not result in expulsion from the tournament. Note: Prior to 2017, the 4th shido was hansoku make. If hansoku make is awarded for a major rule infringement, it results not just in loss of the match, but in the expulsion from the tournament of the penalized player.

Several judo practitioners have made an impact in mixed martial arts. Notable judo-trained MMA fighters include Olympic medalists Hidehiko Yoshida (Gold, 1992), Naoya Ogawa (Silver, 1992), Paweł Nastula (Gold, 1996), Makoto Takimoto (Gold, 2000), Satoshi Ishii (Gold, 2008) and Ronda Rousey (Bronze, 2008), former Russian national judo championship Bronze medalist Fedor Emelianenko, Karo Parisyan, Don Frye, Antônio Silva, Oleg Taktarov, Rick Hawn, Hector Lombard, Daniel Kelly, Yoshihiro Akiyama and Dong-Sik Yoon.

Kano Jigoro's Kodokan judo is the most popular and well-known style of judo, but is not the only one. The terms judo and jujutsu were quite interchangeable in the early years, so some of these forms of judo are still known as jujutsu or jiu-jitsu either for that reason, or simply to differentiate them from mainstream judo. From Kano's original style of judo, several related forms have evolved—some now widely considered to be distinct arts:


Kano's vision for judo was one of a martial way that could be practiced realistically. Randori (free practice) was a central part of judo pedagogy and shiai (competition) a crucial test of a judoka's understanding of judo. Safety necessitated some basic innovations that shaped judo's development. Atemi waza (striking techniques) were entirely limited to kata (prearranged forms) early in judo's history. Kansetsu waza (joint manipulation techniques) were limited to techniques that focused on the elbow joint. Various throwing techniques that were judged to be too dangerous to practice safely were also prohibited in shiai. To maximise safety in nage waza (throwing techniques), judoka trained in ukemi (break falls) and practiced on tatami (rice straw mats).

The application of joint manipulation and strangulation/choking techniques is generally safe under controlled conditions typical of judo dojo and in competition. It is usual for there to be age restrictions on the practice and application of these types of techniques, but the exact nature of these restrictions will vary from country to country and from organization to organization.

Safety in the practice of throwing techniques depends on the skill level of both tori and uke. Inexpertly applied throws have the potential to injure both tori and uke, for instance when tori compensates for poor technique by powering through the throw. Similarly, poor ukemi can result in injury, particularly from more powerful throws that uke lacks the skill to breakfall from. For these reasons, throws are normally taught in order of difficulty for both tori and uke. This is exemplified in the "Gokyo" , a traditional grouping of throws arranged in order of difficulty of ukemi. Those grouped in are relatively simple to breakfall from whereas those grouped in are difficult to breakfall from.

A practitioner of judo is known as a . The modern meaning of "judoka" in English is a judo practitioner of any level of expertise, but traditionally those below the rank of 4th "dan" were called ; and only those of 4th "dan" or higher were called "judoka". (The suffix , when added to a noun, means a person with expertise or special knowledge on that subject).

A judo teacher is called . The word "sensei" comes from "sen" or "saki" (before) and "sei" (life) – i.e. one who has preceded you. In Western dojo, it is common to call an instructor of any "dan" grade "sensei". Traditionally, that title was reserved for instructors of 4th "dan" and above.

Judo practitioners traditionally wear white uniforms called or . sometimes abbreviated in the west as "gi". It comprises a heavy cotton kimono-like jacket called an , similar to traditional fastened by an , coloured to indicate rank, and cotton draw-string . Early examples of keikogi had short sleeves and trouser legs and the modern long-sleeved judogi was adopted in 1906.

The modern use of the blue judogi for high level competition was first suggested by Anton Geesink at the 1986 Maastricht IJF DC Meeting.
For competition, a blue judogi is worn by one of the two competitors for ease of distinction by judges, referees, and spectators. In Japan, both judoka use a white judogi and the traditional red obi (based on the colors of the Japanese flag) is affixed to the belt of one competitor. Outside Japan, a colored obi may also be used for convenience in minor competitions, the blue judogi only being mandatory at the regional or higher levels, depending on organization. Japanese practitioners and traditionalists tend to look down on the use of blue because of the fact that judo is considered a pure sport, and replacing the pure white judogi for the impure blue is an offense.

For events organized under the auspices of the International judo Federation (IJF), judogi have to bear the IJF Official Logo Mark Label. This label demonstrates that the judogi has passed a number of quality control tests to ensure it conforms to construction regulations ensuring it is not too stiff, flexible, rigid or slippery to allow the opponent to grip or to perform techniques.
The international governing body for judo is the International Judo Federation (IJF), founded in 1951. Members of the IJF include the African Judo Union (AJU), the Pan-American Judo Confederation (PJC), the Judo Union of Asia (JUA), the European Judo Union (EJU) and the Oceania Judo Union (OJU), each comprising a number of national judo associations. The IJF is responsible for organising international competition and hosts the World Judo Championships and is involved in running the Olympic Judo events.

Judo is a hierarchical art, where seniority of judoka is designated by what is known as the - ranking system. This system was developed by Jigoro Kano and was based on the ranking system in the board game Go. 
Beginning students progress through kyu grades towards dan grades.

A judoka's position within the kyu-dan ranking system is displayed by the color of their belt. Beginning students typically wear a white belt, progressing through descending kyu ranks until they are deemed to have achieved a level of competence sufficient to be a dan grade, at which point they wear the . The kyu-dan ranking system has since been widely adopted by modern martial arts.

The ninth degree black belt "kudan", and higher ranks, have no formal requirements and are decided by the president of the Kodokan, currently Kano Jigoro's grandson Yukimitsu Kano. As of 2011, fifteen Japanese men have been promoted to the tenth degree black belt "judan" by the Kodokan, three of whom are still alive; the IJF and Western and Asian national federations have promoted another eleven who are not recognized (at that level of rank) by the Kodokan. On July 28, 2011, the promotion board of USA Judo awarded Keiko Fukuda the rank of 10th "dan", who was the first woman to be promoted to judo's highest level, albeit not a Kodokan-recognized rank.

Although "dan" ranks tend to be consistent between national organizations there is more variation in the "kyū" grades, with some countries having more "kyū" grades. Although initially "kyū" grade belt colours were uniformly white, today a variety of colours are used. The first black belts to denote a dan rank in the 1880s, initially the wide obi was used; as practitioners trained in kimono, only white and black obi were used. It was not until the early 1900s, after the introduction of the judogi, that an expanded colored belt system of awarding rank was created.






</doc>
<doc id="15604" url="https://en.wikipedia.org/wiki?curid=15604" title="James Bond">
James Bond

The James Bond series focuses on a fictional British Secret Service agent created in 1953 by writer Ian Fleming, who featured him in twelve novels and two short-story collections. Since Fleming's death in 1964, eight other authors have written authorised Bond novels or novelizations: Kingsley Amis, Christopher Wood, John Gardner, Raymond Benson, Sebastian Faulks, Jeffery Deaver, William Boyd and Anthony Horowitz. The latest novel is "Trigger Mortis" by Anthony Horowitz, published in September 2015. Additionally Charlie Higson wrote a series on a young James Bond, and Kate Westbrook wrote three novels based on the diaries of a recurring series character, Moneypenny.

The character has also been adapted for television, radio, comic strip, video games and film. The films are the longest continually running film series of all time and have grossed over $7.040 billion in total, making it the fourth-highest-grossing film series to date, which started in 1962 with "Dr. No", starring Sean Connery as Bond. As of 2018, there have been twenty-four films in the Eon Productions series. The most recent Bond film, "Spectre" (2015), stars Daniel Craig in his fourth portrayal of Bond; he is the sixth actor to play Bond in the Eon series. There have also been two independent productions of Bond films: "Casino Royale" (a 1967 spoof) and "Never Say Never Again" (a 1983 remake of an earlier Eon-produced film, "Thunderball"). In 2015 the series was estimated to be worth $19.9 billion, making "James Bond" one of the highest-grossing media franchises of all time.

The Bond films are renowned for a number of features, including the musical accompaniment, with the theme songs having received Academy Award nominations on several occasions, and two wins. Other important elements which run through most of the films include Bond's cars, his guns, and the gadgets with which he is supplied by Q Branch. The films are also noted for Bond's relationships with various women, who are sometimes referred to as "Bond girls".

Ian Fleming created the fictional character of James Bond as the central figure for his works. Bond is an intelligence officer in the Secret Intelligence Service, commonly known as MI6. Bond is known by his code number, 007, and was a Royal Naval Reserve Commander. Fleming based his fictional creation on a number of individuals he came across during his time in the Naval Intelligence Division during the Second World War, admitting that Bond "was a compound of all the secret agents and commando types I met during the war". Among those types were his brother, Peter, who had been involved in behind-the-lines operations in Norway and Greece during the war. Aside from Fleming's brother, a number of others also provided some aspects of Bond's make up, including Conrad O'Brien-ffrench, Patrick Dalzel-Job and Bill "Biffy" Dunderdale.

The name James Bond came from that of the American ornithologist James Bond, a Caribbean bird expert and author of the definitive field guide "Birds of the West Indies". Fleming, a keen birdwatcher himself, had a copy of Bond's guide and he later explained to the ornithologist's wife that "It struck me that this brief, unromantic, Anglo-Saxon and yet very masculine name was just what I needed, and so a second James Bond was born". He further explained that:

On another occasion, Fleming said: "I wanted the simplest, dullest, plainest-sounding name I could find, 'James Bond' was much better than something more interesting, like 'Peregrine Carruthers'. Exotic things would happen to and around him, but he would be a neutral figure—an anonymous, blunt instrument wielded by a government department."
Fleming decided that Bond should resemble both American singer Hoagy Carmichael and himself and in "Casino Royale", Vesper Lynd remarks, "Bond reminds me rather of Hoagy Carmichael, but there is something cold and ruthless." Likewise, in "Moonraker", Special Branch Officer Gala Brand thinks that Bond is "certainly good-looking ... Rather like Hoagy Carmichael in a way. That black hair falling down over the right eyebrow. Much the same bones. But there was something a bit cruel in the mouth, and the eyes were cold."

Fleming endowed Bond with many of his own traits, including sharing the same golf handicap, the taste for scrambled eggs and using the same brand of toiletries. Bond's tastes are also often taken from Fleming's own as was his behaviour, with Bond's love of golf and gambling mirroring Fleming's own. Fleming used his experiences of his espionage career and all other aspects of his life as inspiration when writing, including using names of school friends, acquaintances, relatives and lovers throughout his books.

It was not until the penultimate novel, "You Only Live Twice", that Fleming gave Bond a sense of family background. The book was the first to be written after the release of "Dr. No" in cinemas and Sean Connery's depiction of Bond affected Fleming's interpretation of the character, to give Bond both a sense of humour and Scottish antecedents that were not present in the previous stories. In a fictional obituary, purportedly published in "The Times", Bond's parents were given as Andrew Bond, from the village of Glencoe, Scotland, and Monique Delacroix, from the canton of Vaud, Switzerland. Fleming did not provide Bond's date of birth, but John Pearson's fictional biography of Bond, "", gives Bond a birth date on 11 November 1920, while a study by John Griswold puts the date at 11 November 1921.

Whilst serving in the Naval Intelligence Division, Fleming had planned to become an author and had told a friend, "I am going to write the spy story to end all spy stories." On 17 February 1952, he began writing his first James Bond novel, "Casino Royale", at his Goldeneye estate in Jamaica, where he wrote all his Bond novels during the months of January and February each year. He started the story shortly before his wedding to his pregnant girlfriend, Ann Charteris, in order to distract himself from his forthcoming nuptials.

After completing the manuscript for "Casino Royale", Fleming showed the manuscript to his friend (and later editor) William Plomer to read. Plomer liked it and submitted it to the publishers, Jonathan Cape, who did not like it as much. Cape finally published it in 1953 on the recommendation of Fleming's older brother Peter, an established travel writer. Between 1953 and 1966, two years after his death, twelve novels and two short-story collections were published, with the last two books – "The Man with the Golden Gun" and "Octopussy and The Living Daylights" – published posthumously. All the books were published in the UK through Jonathan Cape.

After Fleming's death a continuation novel, "Colonel Sun", was written by Kingsley Amis (as Robert Markham) and published in 1968. Amis had already written a literary study of Fleming's Bond novels in his 1965 work "The James Bond Dossier". Although novelizations of two of the Eon Productions Bond films appeared in print, "James Bond, The Spy Who Loved Me" and "James Bond and Moonraker", both written by screenwriter Christopher Wood, the series of novels did not continue until the 1980s. In 1981 the thriller writer John Gardner picked up the series with "Licence Renewed". Gardner went on to write sixteen Bond books in total; two of the books he wrote – "Licence to Kill" and "GoldenEye" – were novelizations of Eon Productions films of the same name. Gardner moved the Bond series into the 1980s, although he retained the ages of the characters as they were when Fleming had left them. In 1996 Gardner retired from writing James Bond books due to ill health.

In 1996 the American author Raymond Benson became the author of the Bond novels. Benson had previously been the author of "The James Bond Bedside Companion", first published in 1984.
By the time he moved on to other, non-Bond related projects in 2002, Benson had written six Bond novels, three novelizations and three short stories.

After a gap of six years, Sebastian Faulks was commissioned by Ian Fleming Publications to write a new Bond novel, which was released on 28 May 2008, the 100th anniversary of Fleming's birth. The book—titled "Devil May Care"—was published in the UK by Penguin Books and by Doubleday in the US. American writer Jeffery Deaver was then commissioned by Ian Fleming Publications to produce "Carte Blanche", which was published on 26 May 2011. The book updated Bond into a post-9/11 agent, independent of MI5 or MI6. On 26 September 2013 "Solo", written by William Boyd, was published, set in 1969. In October 2014 it was announced that Anthony Horowitz was to write a "Bond" continuation novel. Set in the 1950s two weeks after the events of "Goldfinger", it contains material written, but previously unreleased, by Fleming. "Trigger Mortis" was released on 8 September 2015.

The "Young Bond" series of novels was started by Charlie Higson and, between 2005 and 2009, five novels and one short story were published. The first Young Bond novel, "SilverFin" was also adapted and released as a graphic novel on 2 October 2008 by Puffin Books. In October 2013 Ian Fleming Publications announced that Stephen Cole would continue the series, with the first edition scheduled to be released in Autumn 2014.

"The Moneypenny Diaries" are a trilogy of novels chronicling the life of Miss Moneypenny, M's personal secretary. The novels are penned by Samantha Weinberg under the pseudonym Kate Westbrook, who is depicted as the book's "editor". The first instalment of the trilogy, subtitled "", was released on 10 October 2005 in the UK. A second volume, subtitled "" was released on 2 November 2006 in the UK, published by John Murray. A third volume, subtitled "" was released on 1 May 2008.

In 1954 CBS paid Ian Fleming $1,000 ($ in dollars) to adapt his novel "Casino Royale" into a one-hour television adventure as part of its "Climax!" series. The episode aired live on 21 October 1954 and starred Barry Nelson as "Card Sense" James Bond and Peter Lorre as Le Chiffre. The novel was adapted for American audiences to show Bond as an American agent working for "Combined Intelligence", while the character Felix Leiter—American in the novel—became British onscreen and was renamed "Clarence Leiter".

In 1973 a BBC documentary "Omnibus: The British Hero" featured Christopher Cazenove playing a number of such title characters (e.g. Richard Hannay and Bulldog Drummond). The documentary included James Bond in dramatised scenes from
"Goldfinger"—notably featuring 007 being threatened with the novel's circular saw, rather than the film's laser beam—and "Diamonds Are Forever". In 1991 a TV cartoon series "James Bond Jr." was produced with Corey Burton in the role of Bond's nephew, also called James Bond.

In 1956 the novel "Moonraker" was adapted for broadcast on South African radio, with Bob Holness providing the voice of Bond. According to "The Independent", "listeners across the Union thrilled to Bob's cultured tones as he defeated evil master criminals in search of world domination".

The BBC have adapted five of the Fleming novels for broadcast: in 1990 "You Only Live Twice" was adapted into a 90-minute radio play for BBC Radio 4 with Michael Jayston playing James Bond. The production was repeated a number of times between 2008 and 2011. On 24 May 2008 BBC Radio 4 broadcast an adaptation of "Dr. No". The actor Toby Stephens, who played Bond villain Gustav Graves in the Eon Productions version of "Die Another Day", played Bond, while Dr. No was played by David Suchet. Following its success, a second story was adapted and on 3 April 2010 BBC Radio 4 broadcast "Goldfinger" with Stephens again playing Bond. Sir Ian McKellen was Goldfinger and Stephens' "Die Another Day" co-star Rosamund Pike played Pussy Galore. The play was adapted from Fleming's novel by Archie Scottney and was directed by Martin Jarvis.
In 2012 the novel "From Russia, with Love" was dramatized for Radio 4; it featured a full cast again starring Stephens as Bond. In May 2014 Stephens again played Bond, in "On Her Majesty's Secret Service", with Alfred Molina as Blofeld, and Joanna Lumley as Irma Bunt.

In 1957 the "Daily Express" approached Ian Fleming to adapt his stories into comic strips, offering him £1,500 per novel and a share of takings from syndication. After initial reluctance, Fleming, who felt the strips would lack the quality of his writing, agreed. To aid the "Daily Express" in illustrating Bond, Fleming commissioned an artist to create a sketch of how he believed James Bond looked. The illustrator, John McLusky, however, felt that Fleming's 007 looked too "outdated" and "pre-war" and changed Bond to give him a more masculine look. The first strip, "Casino Royale" was published from 7 July 1958 to 13 December 1958 and was written by Anthony Hern and illustrated by John McLusky.

Most of the Bond novels and short stories have since been adapted for illustration, as well as Kingsley Amis's "Colonel Sun"; the works were written by Henry Gammidge or Jim Lawrence with Yaroslav Horak replacing McClusky as artist in 1966. After the Fleming and Amis material had been adapted, original stories were produced, continuing in the "Daily Express" and "Sunday Express" until May 1977.

Several comic book adaptations of the James Bond films have been published through the years: at the time of "Dr. No"'s release in October 1962, a comic book adaptation of the screenplay, written by Norman J. Nodel, was published in Britain as part of the "Classics Illustrated" anthology series. It was later reprinted in the United States by DC Comics as part of its "Showcase" anthology series, in January 1963. This was the first American comic book appearance of James Bond and is noteworthy for being a relatively rare example of a British comic being reprinted in a fairly high-profile American comic. It was also one of the earliest comics to be censored on racial grounds (some skin tones and dialogue were changed for the American market).

With the release of the 1981 film "For Your Eyes Only", Marvel Comics published a two-issue comic book adaptation of the film. When "Octopussy" was released in the cinemas in 1983, Marvel published an accompanying comic; Eclipse also produced a one-off comic for "Licence to Kill", although Timothy Dalton refused to allow his likeness to be used. New Bond stories were also drawn up and published from 1989 onwards through Marvel, Eclipse Comics and Dark Horse Comics.

In 1962 Eon Productions, the company of Canadian Harry Saltzman and American Albert R. "Cubby" Broccoli, released the first cinema adaptation of an Ian Fleming novel, "Dr. No", featuring Sean Connery as 007. Connery starred in a further four films before leaving the role after "You Only Live Twice", which was taken up by George Lazenby for "On Her Majesty's Secret Service". Lazenby left the role after just one appearance and Connery was tempted back for his last Eon-produced film "Diamonds Are Forever".

In 1973 Roger Moore was appointed to the role of 007 for "Live and Let Die" and played Bond a further six times over twelve years before being replaced by Timothy Dalton for two films. After a six-year hiatus, during which a legal wrangle threatened Eon's productions of the Bond films, Irish actor Pierce Brosnan was cast as Bond in "GoldenEye", released in 1995; he remained in the role for a total of four films, before leaving in 2002. In 2006, Daniel Craig was given the role of Bond for "Casino Royale", which rebooted the series. The twenty-third Eon produced film, "Skyfall", was released on 26 October 2012. The series has grossed almost $7 billion to date, making it the third-highest-grossing film series (behind "Harry Potter" and the films of the Marvel Cinematic Universe), and the single most successful adjusted for inflation.

In 1967 "Casino Royale" was adapted into a parody Bond film starring David Niven as Sir James Bond and Ursula Andress as Vesper Lynd. Niven had been Fleming's preference for the role of Bond. The result of a court case in the High Court in London in 1963 allowed Kevin McClory to produce a remake of "Thunderball" titled "Never Say Never Again" in 1983. The film, produced by Jack Schwartzman's Taliafilm production company and starring Sean Connery as Bond, was not part of the Eon series of Bond films. In 1997 the Sony Corporation acquired all or some of McClory's rights in an undisclosed deal, which were then subsequently acquired by MGM, whilst on 4 December 1997, MGM announced that the company had purchased the rights to "Never Say Never Again" from Taliafilm. As of 2015, Eon holds the full adaptation rights to all of Fleming's Bond novels.

The "James Bond Theme" was written by Monty Norman and was first orchestrated by the John Barry Orchestra for 1962's "Dr. No", although the actual authorship of the music has been a matter of controversy for many years. In 2001, Norman won £30,000 in libel damages from "The Sunday Times" newspaper, which suggested that Barry was entirely responsible for the composition. The theme, as written by Norman and arranged by Barry, was described by another Bond film composer, David Arnold, as "bebop-swing vibe coupled with that vicious, dark, distorted electric guitar, definitely an instrument of rock 'n' roll ... it represented everything about the character you would want: It was cocky, swaggering, confident, dark, dangerous, suggestive, sexy, unstoppable. And he did it in two minutes." Barry composed the scores for eleven Bond films and had an uncredited contribution to "Dr. No" with his arrangement of the Bond Theme.

A Bond film staple are the theme songs heard during their title sequences sung by well-known popular singers. Several of the songs produced for the films have been nominated for Academy Awards for Original Song, including Paul McCartney's "Live and Let Die", Carly Simon's "Nobody Does It Better", Sheena Easton's "For Your Eyes Only", Adele's "Skyfall", and Sam Smith's "Writing's on the Wall". Adele won the award at the 85th Academy Awards, and Smith won at the 88th Academy Awards. For the non-Eon produced "Casino Royale", Burt Bacharach's score included "The Look of Love", which was nominated for an Academy Award for Best Song.

In 1983 the first Bond video game, developed and published by Parker Brothers, was released for the Atari 2600, the Atari 5200, the Atari 800, the Commodore 64 and the ColecoVision. Since then, there have been numerous video games either based on the films or using original storylines. In 1997 the first-person shooter video game "GoldenEye 007" was developed by Rare for the Nintendo 64, based on the 1995 Pierce Brosnan film "GoldenEye". The game received very positive reviews, won the BAFTA Interactive Entertainment Award for UK Developer of the Year in 1998 and sold over eight million copies worldwide, grossing $250 million.

In 1999 Electronic Arts acquired the licence and released "Tomorrow Never Dies" on 16 December 1999. In October 2000, they released for the Nintendo 64 followed by "007 Racing" for the PlayStation on 21 November 2000. In 2003, the company released "", which included the likenesses and voices of Pierce Brosnan, Willem Dafoe, Heidi Klum, Judi Dench and John Cleese, amongst others. In November 2005, Electronic Arts released a video game adaptation of "", which involved Sean Connery's image and voice-over for Bond. In 2006 Electronic Arts announced a game based on then-upcoming film "Casino Royale": the game was cancelled because it would not be ready by the film's release in November of that year. With MGM losing revenue from lost licensing fees, the franchise was removed from EA to Activision. Activision subsequently released the "" game on 31 October 2008, based on the film of the same name.

A new version of "GoldenEye 007" featuring Daniel Craig was released for the Wii and a handheld version for the Nintendo DS in November 2010. A year later a new version was released for Xbox 360 and PlayStation 3 under the title "". In October 2012 "007 Legends" was released, which featured one mission from each of the Bond actors of the Eon Productions' series.

For the first five novels, Fleming armed Bond with a Beretta 418 until he received a letter from a thirty-one-year-old Bond enthusiast and gun expert, Geoffrey Boothroyd, criticising Fleming's choice of firearm for Bond, calling it "a lady's gun – and not a very nice lady at that!" Boothroyd suggested that Bond should swap his Beretta for a Walther PPK 7.65mm and this exchange of arms made it to "Dr. No". Boothroyd also gave Fleming advice on the Berns-Martin triple draw shoulder holster and a number of the weapons used by SMERSH and other villains. In thanks, Fleming gave the MI6 Armourer in his novels the name Major Boothroyd and, in "Dr. No", M introduces him to Bond as "the greatest small-arms expert in the world". Bond also used a variety of rifles, including the Savage Model 99 in "For Your Eyes Only" and a Winchester .308 target rifle in "The Living Daylights". Other handguns used by Bond in the Fleming books included the Colt Detective Special and a long-barrelled Colt .45 Army Special.

The first Bond film, "Dr. No", saw M ordering Bond to leave his Beretta behind and take up the Walther PPK, which the film Bond used in eighteen films. In "Tomorrow Never Dies" and the two subsequent films, Bond's main weapon was the Walther P99 semi-automatic pistol.
In the early Bond stories Fleming gave Bond a battleship-grey Bentley 4½ Litre with an Amherst Villiers supercharger. After Bond's car was written off by Hugo Drax in "Moonraker", Fleming gave Bond a Mark II Continental Bentley, which he used in the remaining books of the series. During "Goldfinger", Bond was issued with an Aston Martin DB Mark III with a homing device, which he used to track Goldfinger across France. Bond returned to his Bentley for the subsequent novels.

The Bond of the films has driven a number of cars, including the Aston Martin V8 Vantage, during the 1980s, the V12 Vanquish and DBS during the 2000s, as well as the Lotus Esprit; the BMW Z3, BMW 750iL and the BMW Z8. He has, however, also needed to drive a number of other vehicles, ranging from a Citroën 2CV to a Routemaster Bus, amongst others.

Bond's most famous car is the silver grey Aston Martin DB5, first seen in "Goldfinger"; it later featured in "Thunderball", "GoldenEye", "Tomorrow Never Dies", "Casino Royale", "Skyfall" and "Spectre". The films have used a number of different Aston Martins for filming and publicity, one of which was sold in January 2006 at an auction in the US for $2,090,000 to an unnamed European collector.

Fleming's novels and early screen adaptations presented minimal equipment such as the booby-trapped attaché case in "From Russia with Love", although this situation changed dramatically with the films. However, the effects of the two Eon-produced Bond films "Dr. No" and "From Russia with Love" had an effect on the novel "The Man with the Golden Gun", through the increased number of devices used in Fleming's final story.

For the film adaptations of Bond, the pre-mission briefing by Q Branch became one of the motifs that ran through the series. "Dr. No" provided no spy-related gadgets, but a Geiger counter was used; industrial designer Andy Davey observed that the first ever onscreen spy-gadget was the attaché case shown in "From Russia with Love", which he described as "a classic 007 product". The gadgets assumed a higher profile in the 1964 film "Goldfinger". The film's success encouraged further espionage equipment from Q Branch to be supplied to Bond, although the increased use of technology led to an accusation that Bond was over-reliant on equipment, particularly in the later films.
Davey noted that "Bond's gizmos follow the zeitgeist more closely than any other ... nuance in the films" as they moved from the potential representations of the future in the early films, through to the brand-name obsessions of the later films. It is also noticeable that, although Bond uses a number of pieces of equipment from Q Branch, including the Little Nellie autogyro, a jet pack and the exploding attaché case, the villains are also well-equipped with custom-made devices, including Scaramanga's golden gun, Rosa Klebb's poison-tipped shoes, Oddjob's steel-rimmed bowler hat and Blofeld's communication devices in his agents' vanity case.

Cinematically, Bond has been a major influence within the spy genre since the release of "Dr. No" in 1962, with 22 secret agent films released in 1966 alone attempting to capitalise on the Bond franchise's popularity and success. The first parody was the 1964 film "Carry On Spying", which shows the villain Dr. Crow being overcome by agents who included James Bind (Charles Hawtry) and Daphne Honeybutt (Barbara Windsor). One of the films that reacted against the portrayal of Bond was the Harry Palmer series, whose first film, "The Ipcress File" was released in 1965. The eponymous hero of the series was what academic Jeremy Packer called an "anti-Bond", or what Christoph Lindner calls "the thinking man's Bond". The Palmer series were produced by Harry Saltzman, who also used key crew members from the Bond series, including designer Ken Adam, editor Peter R. Hunt and composer John Barry. The four "Matt Helm" films starring Dean Martin (released between 1966 and 1969), the "Flint" series starring James Coburn (comprising two films, one each in 1966 and 1969), while "The Man from U.N.C.L.E." also moved onto the cinema screen, with eight films released: all were testaments to Bond's prominence in popular culture. More recently, the "Austin Powers" series by writer, producer and comedian Mike Myers, and other parodies such as the 2003 film "Johnny English", have also used elements from or parodied the Bond films.

Following the release of the film "Dr. No" in 1962, the line "Bond ... James Bond", became a catch phrase that entered the lexicon of Western popular culture: writers Cork and Scivally said of the introduction in "Dr. No" that the "signature introduction would become the most famous and loved film line ever". In 2001, it was voted as the "best-loved one-liner in cinema" by British cinema goers, and in 2005, it was honoured as the 22nd greatest quotation in cinema history by the American Film Institute as part of their 100 Years Series. The 2005 American Film Institute's '100 Years' series recognised the character of James Bond himself as the third greatest film hero. He was also placed at number 11 on a similar list by "Empire" and as the fifth greatest movie character of all time by "Premiere".

The 23 James Bond films produced by Eon Productions, which have grossed $4,910 million in box office returns alone, have made the series one of the highest-grossing ever. It is estimated that since "Dr. No", a quarter of the world's population have seen at least one Bond film. The UK Film Distributors' Association have stated that the importance of the Bond series of films to the British film industry cannot be overstated, as they "form the backbone of the industry".

Television also saw the effect of Bond films, with the NBC series "The Man from U.N.C.L.E.", which was described as the "first network television imitation" of Bond, largely because Fleming provided advice and ideas on the development of the series, even giving the main character the name Napoleon Solo. Other 1960s television series inspired by Bond include "I Spy", and "Get Smart".

A British cultural icon, by 2012, James Bond had become such a symbol of the United Kingdom that the character, played by Craig, appeared in the opening ceremony of the 2012 London Olympics as Queen Elizabeth II's escort.

Throughout the life of the film series, a number of tie-in products have been released. In 2018 a James Bond museum opened atop of Austrian Alps. The futuristic museum is constructed on the summit of Gaislachkogl Mountain in Sölden at 3,048 m above sea level.

The James Bond character and related media have triggered a number of criticisms and reactions across the political spectrum, and are still highly debated in popular culture studies. Some observers accuse the Bond novels and films of misogyny and sexism. Geographers have considered the role of exotic locations in the movies in the dynamics of the Cold War, with power struggles among blocs playing out in the peripheral areas. Other critics claim that the Bond films reflect imperial nostalgia. American conservative critics, particularly in the 1960s and 1970s, saw Bond as a nihilistic, hedonistic, and amoral character that challenged family values.




</doc>
<doc id="15606" url="https://en.wikipedia.org/wiki?curid=15606" title="Japanese language">
Japanese language

Little is known of the language's prehistory, or when it first appeared in Japan. Chinese documents from the 3rd century recorded a few Japanese words, but substantial texts did not appear until the 8th century. During the Heian period (794–1185), Chinese had considerable influence on the vocabulary and phonology of Old Japanese. Late Middle Japanese (1185–1600) included changes in features that brought it closer to the modern language, and the first appearance of European loanwords. The standard dialect moved from the Kansai region to the Edo (modern Tokyo) region in the Early Modern Japanese period (early 17th century–mid-19th century). Following the end in 1853 of Japan's self-imposed isolation, the flow of loanwords from European languages increased significantly. English loanwords, in particular, have become frequent, and Japanese words from English roots have proliferated.

Japanese is an agglutinative, mora-timed language with simple phonotactics, a pure vowel system, phonemic vowel and consonant length, and a lexically significant pitch-accent. Word order is normally subject–object–verb with particles marking the grammatical function of words, and sentence structure is topic–comment. Sentence-final particles are used to add emotional or emphatic impact, or make questions. Nouns have no grammatical number or gender, and there are no articles. Verbs are conjugated, primarily for tense and voice, but not person. Japanese equivalents of adjectives are also conjugated. Japanese has a complex system of honorifics with verb forms and vocabulary to indicate the relative status of the speaker, the listener, and persons mentioned.

Japanese has no genetic relationship with Chinese, but it makes extensive use of Chinese characters, or , in its writing system, and a large portion of its vocabulary is borrowed from Chinese. Along with "kanji", the Japanese writing system primarily uses two syllabic (or moraic) scripts, and . Latin script is used in a limited fashion, such as for imported acronyms, and the numeral system uses mostly Arabic numerals alongside traditional Chinese numerals.

A common ancestor of Japanese and Ryukyuan languages or dialects is thought to have been brought to Japan by settlers coming from either continental Asia or nearby Pacific islands sometime in the early- to mid-2nd century BC (the Yayoi period), replacing the languages of the original Jōmon inhabitants, including the ancestor of the modern Ainu language. Very little is known about the Japanese of this period. Because writing like the "Kanji" which later devolved into the writing systems "Hiragana" and "Katakana" had yet to be introduced from China, there is no direct evidence, and anything that can be discerned about this period of Japanese must be based on the reconstructions of Old Japanese.

Old Japanese is the oldest attested stage of the Japanese language. Through the spread of Buddhism, the Chinese writing system was imported to Japan. The earliest texts found in Japan are written in Classical Chinese, but they may have been meant to be read as Japanese by the kanbun method. Some of these Chinese texts show the influences of Japanese grammar, such as the word order (for example, placing the verb after the object). In these hybrid texts, Chinese characters are also occasionally used phonetically to represent Japanese particles. The earliest text, the "Kojiki", dates to the early 8th century, and was written entirely in Chinese characters. The end of Old Japanese coincides with the end of the Nara period in 794. Old Japanese uses the Man'yōgana system of writing, which uses "kanji" for their phonetic as well as semantic values. Based on the Man'yōgana system, Old Japanese can be reconstructed as having 88 distinct syllables. Texts written with Man'yōgana use two different "kanji" for each of the syllables now pronounced . (The "Kojiki" has 88, but all later texts have 87. The distinction between mo and mo apparently was lost immediately following its composition.) This set of syllables shrank to 67 in Early Middle Japanese, though some were added through Chinese influence.

Due to these extra syllables, it has been hypothesized that Old Japanese's vowel system was larger than that of Modern Japanese – it perhaps contained up to eight vowels. According to Shinkichi Hashimoto, the extra syllables in Man'yōgana derive from differences between the vowels of the syllables in question. These differences would indicate that Old Japanese had an eight-vowel system, in contrast to the five vowels of later Japanese. The vowel system would have to have shrunk some time between these texts and the invention of the "kana" ("hiragana" and "katakana") in the early 9th century. According to this view, the eight-vowel system of ancient Japanese would resemble that of the Uralic and Altaic language families. However, it is not fully certain that the alternation between syllables necessarily reflects a difference in the vowels rather than the consonants – at the moment, the only undisputed fact is that they are different syllables. A newer reconstruction of ancient Japanese shows strikingly similarities with Southeast-Asian languages, especially with Austronesian languages.

Old Japanese does not have , but rather (preserved in modern "fu", ), which has been reconstructed to an earlier *. Man'yōgana also has a symbol for , which merges with before the end of the period.

Several fossilizations of Old Japanese grammatical elements remain in the modern language – the genitive particle "tsu" (superseded by modern "no") is preserved in words such as "matsuge" ("eyelash", lit. "hair of the eye"); modern "mieru" ("to be visible") and "kikoeru" ("to be audible") retain what may have been a mediopassive suffix -"yu(ru)" ("kikoyu" → "kikoyuru" (the attributive form, which slowly replaced the plain form starting in the late Heian period) > "kikoeru" (as all shimo-nidan verbs in modern Japanese did)); and the genitive particle "ga" remains in intentionally archaic speech.

Early Middle Japanese is the Japanese of the Heian period, from 794 to 1185. Early Middle Japanese sees a significant amount of Chinese influence on the language's phonology – length distinctions become phonemic for both consonants and vowels, and series of both labialised (e.g. "kwa") and palatalised ("kya") consonants are added. Intervocalic merges with by the 11th century.
The end of Early Middle Japanese sees the beginning of a shift where the attributive form (Japanese "rentaikei") slowly replaces the uninflected form ("shūshikei") for those verb classes where the two were distinct.

Late Middle Japanese covers the years from 1185 to 1600, and is normally divided into two sections, roughly equivalent to the Kamakura period and the Muromachi period, respectively. The later forms of Late Middle Japanese are the first to be described by non-native sources, in this case the Jesuit and Franciscan missionaries; and thus there is better documentation of Late Middle Japanese phonology than for previous forms (for instance, the "Arte da Lingoa de Iapam"). Among other sound changes, the sequence merges to , in contrast with ; is reintroduced from Chinese; and merges with . Some forms rather more familiar to Modern Japanese speakers begin to appear – the continuative ending -"te" begins to reduce onto the verb (e.g. "yonde" for earlier "yomite"), the -k- in the final syllable of adjectives drops out ("shiroi" for earlier "shiroki"); and some forms exist where modern standard Japanese has retained the earlier form (e.g. "hayaku" > "hayau" > "hayɔɔ", where modern Japanese just has "hayaku", though the alternative form is preserved in the standard greeting "o-hayō gozaimasu" "good morning"; this ending is also seen in "o-medetō" "congratulations", from "medetaku").

Late Middle Japanese has the first loanwords from European languages – now-common words borrowed into Japanese in this period include "pan" ("bread") and "tabako" ("tobacco", now "cigarette"), both from Portuguese.

Early Modern Japanese, not to be confused with Modern Japanese, was the dialect used after the Meiji Restoration. Because the two languages are extremely similar, Early Modern Japanese is commonly referred to as Modern Japanese. Early Modern Japanese gradually evolved into Modern Japanese during the 19th century. Only after 1945, shortly after World War II, did Modern Japanese become the standard language, seeing use in most official communications. In this time period the Japanese in addition to their use of Katakana and Hiragana they also used traditional Chinese characters called "Han" which later developed in "Kanji" which is a form of writing used to express ideas in the Japanese and Chinese languages.

Modern Japanese is considered to begin with the Edo period, which lasted between 1603 and 1868. Since Old Japanese, the de facto standard Japanese had been the Kansai dialect, especially that of Kyoto. However, during the Edo period, Edo (now Tokyo) developed into the largest city in Japan, and the Edo-area dialect became standard Japanese. Since the end of Japan's self-imposed isolation in 1853, the flow of loanwords from European languages has increased significantly. The period since 1945 has seen a large number of words borrowed from other languagessuch as German, Portuguese and English. Many English loan words especially relate to technologyfor example, "pasokon" (short for "personal computer"), "intānetto" ("internet"), and "kamera" ("camera"). Due to the large quantity of English loanwords, modern Japanese has developed a distinction between and , and and , with the latter in each pair only found in loanwords.

Although Japanese is spoken almost exclusively in Japan, it has been spoken outside. Before and during World War II, through Japanese annexation of Taiwan and Korea, as well as partial occupation of China, the Philippines, and various Pacific islands, locals in those countries learned Japanese as the language of the empire. As a result, many elderly people in these countries can still speak Japanese.

Japanese emigrant communities (the largest of which are to be found in Brazil, with 1.4 million to 1.5 million Japanese immigrants and descendants, according to Brazilian IBGE data, more than the 1.2 million of the United States) sometimes employ Japanese as their primary language. Approximately 12% of Hawaii residents speak Japanese, with an estimated 12.6% of the population of Japanese ancestry in 2008. Japanese emigrants can also be found in Peru, Argentina, Australia (especially in the eastern states), Canada (especially in Vancouver where 1.4% of the population has Japanese ancestry), the United States (notably Hawaii, where 16.7% of the population has Japanese ancestry, and California), and the Philippines (particularly in Davao and Laguna).

Japanese has no official status, but is the "de facto" national language of Japan. There is a form of the language considered standard: , meaning "standard Japanese", or , "common language". The meanings of the two terms are almost the same. "Hyōjungo" or "kyōtsūgo" is a conception that forms the counterpart of dialect. This normative language was born after the from the language spoken in the higher-class areas of Tokyo (see Yamanote). "Hyōjungo" is taught in schools and used on television and even in official communications. It is the version of Japanese discussed in this article.

Formerly, standard was different from . The two systems have different rules of grammar and some variance in vocabulary. "Bungo" was the main method of writing Japanese until about 1900; since then "kōgo" gradually extended its influence and the two methods were both used in writing until the 1940s. "Bungo" still has some relevance for historians, literary scholars, and lawyers (many Japanese laws that survived World War II are still written in "bungo", although there are ongoing efforts to modernize their language). "Kōgo" is the dominant method of both speaking and writing Japanese today, although "bungo" grammar and vocabulary are occasionally used in modern Japanese for effect.

Dozens of dialects are spoken in Japan. The profusion is due to many factors, including the length of time the Japanese Archipelago has been inhabited, its mountainous island terrain, and Japan's long history of both external and internal isolation. Dialects typically differ in terms of pitch accent, inflectional morphology, vocabulary, and particle usage. Some even differ in vowel and consonant inventories, although this is uncommon.

The main distinction in Japanese accents is between and . Within each type are several subdivisions. Kyoto-Osaka-type dialects are in the central region, roughly formed by Kansai, Shikoku, and western Hokuriku regions.

Dialects from peripheral regions, such as Tōhoku or Kagoshima, may be unintelligible to speakers from the other parts of the country. There are some language islands in mountain villages or isolated islands such as Hachijō-jima island whose dialects are descended from the Eastern dialect of Old Japanese. Dialects of the Kansai region are spoken or known by many Japanese, and Osaka dialect in particular is associated with comedy (see Kansai dialect). Dialects of Tōhoku and North Kantō are associated with typical farmers.

The Ryūkyūan languages, spoken in Okinawa and the Amami Islands (politically part of Kagoshima), are distinct enough to be considered a separate branch of the Japonic family; not only is each language unintelligible to Japanese speakers, but most are unintelligible to those who speak other Ryūkyūan languages. However, in contrast to linguists, many ordinary Japanese people tend to consider the Ryūkyūan languages as dialects of Japanese. This is the result of the official language policy of the Japanese government, which has declared these languages to be dialects and prohibited their use in schools.

The imperial court also seems to have spoken an unusual variant of the Japanese of the time. Most likely being the spoken form of Classical Japanese language, a writing style that was prevalent during the Heian period, but began decline during the late Meiji period.

Modern Japanese has become prevalent nationwide (including the Ryūkyū islands) due to education, mass media, and an increase of mobility within Japan, as well as economic integration.

Japanese is a member of the Japonic languages family, which also includes the languages spoken throughout the Ryūkyū Islands. As these closely related languages are commonly treated as dialects of the same language, Japanese is often called a language isolate.

According to Martine Irma Robbeets, Japanese has been subject to more attempts to show its relation to other languages than any other language in the world. Since Japanese first gained the consideration of linguists in the late 19th century, attempts have been made to show its genealogical relation to languages or language families such as Ainu, Korean, Chinese, Tibeto-Burman, Ural-Altaic, Altaic, Uralic, Mon–Khmer, Malayo-Polynesian and Ryukyuan. At the fringe, some linguists have suggested a link to Indo-European languages, including Greek, and to Lepcha. As it stands, only the link to Ryukyuan has wide support, though linguist Kurakichi Shiratori maintained that Japanese was a language isolate.

Similarities between Korean and Japanese were noted by Arai Hakuseki in 1717, and the idea that the two might be related was first proposed in 1781 by Japanese scholar Teikan Fujii. The idea received little attention until William George Aston proposed it again in 1879. Japanese scholar Shōsaburō Kanazawa took it up in 1910, as did Shinpei Ogura in 1934. Shirō Hattori was nearly alone when he criticised these theories in 1959. Samuel Martin furthered the idea in 1966 with his "Lexical evidence relating Korean to Japanese", as did John Whitman with his dissertation on the subject in 1985. Despite this, definitive proof of the relation has yet to be provided. Historical linguists studying Japanese and Korean tend to accept the genealogical relation, while general linguists and historical linguists in Japan and Korea have remained skeptical.

Alexander Vovin suggests that, while typologically modern Korean and Japanese share similarities that sometimes allow word-to-word translations, studies of the pre-modern languages show greater differences. According to Vovin, this suggests linguistic convergence rather than divergence, which he believes is amongst the evidence of the languages not having a genealogical connection.

The largely discredited Altaic family, which would include languages from far eastern Europe to northeastern Asia, has had its supporters and detractors over its history. The most controversial aspect of the hypothesis is the proposed inclusion of Korean and Japanese, which even some proponents of Altaic have rejected. Philipp Franz von Siebold suggested the connection in 1832, but the inclusion first attracted significant attention in the early 1970s. Roy Andrew Miller published "Japanese and the Other Altaic Languages", and dedicated much of his later career to the subject. Sergei Starostin published a 1991 monograph which was another significant stepping stone in Japanese—Altaic research. A team of scholars made a database of Altaic etymologies available over the internet, from which the three-volume "Etymological Dictionary of the Altaic Languages" was published in 2003. Scholars such as Yevgeny Polivanov and Yoshizo Itabashi, on the other hand, have proposed a hybrid origin of Japanese, in which Austronesian and Altaic elements became mixed.

Skepticism over the Japanese relation to Altaic is widespread among Altaic's proponents, in part because of a large number of unsuccessful attempts to establish genealogical relationships with Japanese and other languages. Opinions are polarized, with many strongly convinced of the Altaic relation, and others strongly convinced of the lack of one. While some sources are undecided, often strong proponents of either view will not even acknowledge the claims of the other side.

Japanese shows in its proto-form strong similarities to Southeast Asian languages. A 2015 analysis using the Automated Similarity Judgment Program resulted in the Japonic languages being grouped with the Ainu and then with the Austroasiatic languages.

All Japanese vowels are pure—that is, there are no diphthongs, only monophthongs. The only unusual vowel is the high back vowel , which may be compressed rather than rounded and fronted. Japanese has five vowels, and vowel length is phonemic, with each having both a short and a long version. Elongated vowels are usually denoted with a line over the vowel (a macron) in rōmaji, a repeated vowel character in hiragana, or a chōonpu succeeding the vowel in katakana.

Some Japanese consonants have several allophones, which may give the impression of a larger inventory of sounds. However, some of these allophones have since become phonemic. For example, in the Japanese language up to and including the first half of the 20th century, the phonemic sequence was palatalized and realized phonetically as , approximately "chi" ; however, now and are distinct, as evidenced by words like "tī" "Western style tea" and "chii" "social status".

The "r" of the Japanese language is of particular interest, ranging between an apical central tap and a lateral approximant. The "g" is also notable; unless it starts a sentence, it may be pronounced , in the Kanto prestige dialect and in other eastern dialects.

The syllabic structure and the phonotactics are very simple: the only consonant clusters allowed within a syllable consist of one of a subset of the consonants plus . This type of cluster only occurs in onsets. However, consonant clusters across syllables are allowed as long as the two consonants are a nasal followed by a homorganic consonant. Consonant length (gemination) is also phonemic.

The phonology of Japanese also includes a pitch accent system, which is a system that helps differentiate words with identical Hiragana spelling or words in different Japanese dialects. An example of words with identical Hiragana would be the words ("chopsticks") and ("bridge"), both spelled (はし, "hashi") in Hiragana. The stresses differentiate the words.

Japanese word order is classified as subject–object–verb. Unlike many Indo-European languages, the only strict rule of word order is that the verb must be placed at the end of a sentence (possibly followed by sentence-end particles). This is because Japanese sentence elements are marked with particles that identify their grammatical functions.

The basic sentence structure is topic–comment. For example, "Kochira wa Tanaka-san desu" (). "kochira" ("this") is the topic of the sentence, indicated by the particle " wa". The verb "de aru" ("desu" is a contraction of its polite form "de arimasu") is a copula, commonly translated as "to be" or "it is" (though there are other verbs that can be translated as "to be"), though technically it holds no meaning and is used to give a sentence 'politeness'. As a phrase, "Tanaka-san desu" is the comment. This sentence literally translates to "As for this person, (it) is Mr./Ms. Tanaka." Thus Japanese, like many other Asian languages, is often called a topic-prominent language, which means it has a strong tendency to indicate the topic separately from the subject, and that the two do not always coincide. The sentence "Zō wa hana ga nagai " () literally means, "As for elephant(s), (the) nose(s) (is/are) long". The topic is "zō" "elephant", and the subject is "hana" "nose".

In Japanese, the subject or object of a sentence need not be stated if it is obvious from context. As a result of this grammatical permissiveness, there is a tendency to gravitate towards brevity; Japanese speakers tend to omit pronouns on the theory they are inferred from the previous sentence, and are therefore understood. In the context of the above example, "hana-ga nagai" would mean "[their] noses are long," while "nagai" by itself would mean "[they] are long." A single verb can be a complete sentence: "Yatta!" () "[I / we / they / etc] did [it]!". In addition, since adjectives can form the predicate in a Japanese sentence (below), a single adjective can be a complete sentence: "Urayamashii!" () "[I'm] jealous [of it]!".

While the language has some words that are typically translated as pronouns, these are not used as frequently as pronouns in some Indo-European languages, and function differently. In some cases Japanese relies on special verb forms and auxiliary verbs to indicate the direction of benefit of an action: "down" to indicate the out-group gives a benefit to the in-group; and "up" to indicate the in-group gives a benefit to the out-group. Here, the in-group includes the speaker and the out-group does not, and their boundary depends on context. For example, "oshiete moratta" () (literally, "explained" with a benefit from the out-group to the in-group) means "[he/she/they] explained [it] to [me/us]". Similarly, "oshiete ageta" () (literally, "explained" with a benefit from the in-group to the out-group) means "[I/we] explained [it] to [him/her/them]". Such beneficiary auxiliary verbs thus serve a function comparable to that of pronouns and prepositions in Indo-European languages to indicate the actor and the recipient of an action.

Japanese "pronouns" also function differently from most modern Indo-European pronouns (and more like nouns) in that they can take modifiers as any other noun may. For instance, one does not say in English:
But one "can" grammatically say essentially the same thing in Japanese:

This is partly because these words evolved from regular nouns, such as "kimi" "you" ( "lord"), "anata" "you" ( "that side, yonder"), and "boku" "I" ( "servant"). This is why some linguists do not classify Japanese "pronouns" as pronouns, but rather as referential nouns, much like Spanish "usted" (contracted from "vuestra merced", "your [(flattering majestic) plural] grace") or Portuguese "o senhor". Japanese personal pronouns are generally used only in situations requiring special emphasis as to who is doing what to whom.

The choice of words used as pronouns is correlated with the sex of the speaker and the social situation in which they are spoken: men and women alike in a formal situation generally refer to themselves as "watashi" ( "private") or "watakushi" (also ), while men in rougher or intimate conversation are much more likely to use the word "ore" ( "oneself", "myself") or "boku". Similarly, different words such as "anata", "kimi", and "omae" (, more formally "the one before me") may be used to refer to a listener depending on the listener's relative social position and the degree of familiarity between the speaker and the listener. When used in different social relationships, the same word may have positive (intimate or respectful) or negative (distant or disrespectful) connotations.

Japanese often use titles of the person referred to where pronouns would be used in English. For example, when speaking to one's teacher, it is appropriate to use "sensei" (, teacher), but inappropriate to use "anata". This is because "anata" is used to refer to people of equal or lower status, and one's teacher has higher status.

Japanese nouns have no grammatical number, gender or article aspect. The noun "hon" () may refer to a single book or several books; "hito" () can mean "person" or "people", and "ki" () can be "tree" or "trees". Where number is important, it can be indicated by providing a quantity (often with a counter word) or (rarely) by adding a suffix, or sometimes by duplication (e.g. , "hitobito", usually written with an iteration mark as ). Words for people are usually understood as singular. Thus "Tanaka-san" usually means "Mr./Ms. Tanaka". Words that refer to people and animals can be made to indicate a group of individuals through the addition of a collective suffix (a noun suffix that indicates a group), such as "-tachi", but this is not a true plural: the meaning is closer to the English phrase "and company". A group described as "Tanaka-san-tachi" may include people not named Tanaka. Some Japanese nouns are effectively plural, such as "hitobito" "people" and "wareware" "we/us", while the word "tomodachi" "friend" is considered singular, although plural in form.

Verbs are conjugated to show tenses, of which there are two: past and present (or non-past) which is used for the present and the future. For verbs that represent an ongoing process, the "-te iru" form indicates a continuous (or progressive) aspect, similar to the suffix "ing" in English. For others that represent a change of state, the "-te iru" form indicates a perfect aspect. For example, "kite iru" means "He has come (and is still here)", but "tabete iru" means "He is eating".

Questions (both with an interrogative pronoun and yes/no questions) have the same structure as affirmative sentences, but with intonation rising at the end. In the formal register, the question particle "-ka" is added. For example, "ii desu" () "It is OK" becomes "ii desu-ka" () "Is it OK?". In a more informal tone sometimes the particle "-no" () is added instead to show a personal interest of the speaker: "Dōshite konai-no?" "Why aren't (you) coming?". Some simple queries are formed simply by mentioning the topic with an interrogative intonation to call for the hearer's attention: "Kore wa?" "(What about) this?"; "O-namae wa?" () "(What's your) name?".

Negatives are formed by inflecting the verb. For example, "Pan o taberu" () "I will eat bread" or "I eat bread" becomes "Pan o tabenai" () "I will not eat bread" or "I do not eat bread". Plain negative forms are actually "i"-adjectives (see below) and inflect as such, e.g. "Pan o tabenakatta" () "I did not eat bread".

The so-called "-te" verb form is used for a variety of purposes: either progressive or perfect aspect (see above); combining verbs in a temporal sequence ("Asagohan o tabete sugu dekakeru" "I'll eat breakfast and leave at once"), simple commands, conditional statements and permissions ("Dekakete-mo ii?" "May I go out?"), etc.

The word "da" (plain), "desu" (polite) is the copula verb. It corresponds approximately to the English "be", but often takes on other roles, including a marker for tense, when the verb is conjugated into its past form "datta" (plain), "deshita" (polite). This comes into use because only "i"-adjectives and verbs can carry tense in Japanese. Two additional common verbs are used to indicate existence ("there is") or, in some contexts, property: "aru" (negative "nai") and "iru" (negative "inai"), for inanimate and animate things, respectively. For example, "Neko ga iru" "There's a cat", "Ii kangae-ga nai" "[I] haven't got a good idea".

The verb "to do" ("suru", polite form "shimasu") is often used to make verbs from nouns ("ryōri suru" "to cook", "benkyō suru" "to study", etc.) and has been productive in creating modern slang words. Japanese also has a huge number of compound verbs to express concepts that are described in English using a verb and an adverbial particle (e.g. "tobidasu" "to fly out, to flee," from "tobu" "to fly, to jump" + "dasu" "to put out, to emit").

There are three types of adjective (see Japanese adjectives):

Both "keiyōshi" and "keiyōdōshi" may predicate sentences. For example,
Both inflect, though they do not show the full range of conjugation found in true verbs.
The "rentaishi" in Modern Japanese are few in number, and unlike the other words, are limited to directly modifying nouns. They never predicate sentences. Examples include "ookina" "big", "kono" "this", "iwayuru" "so-called" and "taishita" "amazing".

Both "keiyōdōshi" and "keiyōshi" form adverbs, by following with "ni" in the case of "keiyōdōshi":
and by changing "i" to "ku" in the case of "keiyōshi":

The grammatical function of nouns is indicated by postpositions, also called particles. These include for example:
It is also used for the lative case, indicating a motion to a location.

Note: The subtle difference between wa and ga in Japanese cannot be derived from the English language as such, because the distinction between sentence topic and subject is not made there. While "wa" indicates the topic, which the rest of the sentence describes or acts upon, it carries the implication that the subject indicated by "wa" is not unique, or may be part of a larger group.
Absence of "wa" often means the subject is the focus of the sentence.

Japanese has an extensive grammatical system to express politeness and formality.

The Japanese language can express differing levels in social status. The differences in social position are determined by a variety of factors including job, age, experience, or even psychological state (e.g., a person asking a favour tends to do so politely). The person in the lower position is expected to use a polite form of speech, whereas the other person might use a plainer form. Strangers will also speak to each other politely. Japanese children rarely use polite speech until they are teens, at which point they are expected to begin speaking in a more adult manner. "See uchi-soto".

Whereas "teineigo" () (polite language) is commonly an inflectional system, "sonkeigo" () (respectful language) and "kenjōgo" () (humble language) often employ many special honorific and humble alternate verbs: "iku" "go" becomes "ikimasu" in polite form, but is replaced by "irassharu" in honorific speech and "ukagau" or "mairu" in humble speech.

The difference between honorific and humble speech is particularly pronounced in the Japanese language. Humble language is used to talk about oneself or one's own group (company, family) whilst honorific language is mostly used when describing the interlocutor and their group. For example, the "-san" suffix ("Mr" "Mrs." or "Miss") is an example of honorific language. It is not used to talk about oneself or when talking about someone from one's company to an external person, since the company is the speaker's in-group. When speaking directly to one's superior in one's company or when speaking with other employees within one's company about a superior, a Japanese person will use vocabulary and inflections of the honorific register to refer to the in-group superior and their speech and actions. When speaking to a person from another company (i.e., a member of an out-group), however, a Japanese person will use the plain or the humble register to refer to the speech and actions of their own in-group superiors. In short, the register used in Japanese to refer to the person, speech, or actions of any particular individual varies depending on the relationship (either in-group or out-group) between the speaker and listener, as well as depending on the relative status of the speaker, listener, and third-person referents.

Most nouns in the Japanese language may be made polite by the addition of "o-" or "go-" as a prefix. "o-" is generally used for words of native Japanese origin, whereas "go-" is affixed to words of Chinese derivation. In some cases, the prefix has become a fixed part of the word, and is included even in regular speech, such as "gohan" 'cooked rice; meal.' Such a construction often indicates deference to either the item's owner or to the object itself. For example, the word "tomodachi" 'friend,' would become "o-tomodachi" when referring to the friend of someone of higher status (though mothers often use this form to refer to their children's friends). On the other hand, a polite speaker may sometimes refer to "mizu" 'water' as "o-mizu" in order to show politeness.

Most Japanese people employ politeness to indicate a lack of familiarity. That is, they use polite forms for new acquaintances, but if a relationship becomes more intimate, they no longer use them. This occurs regardless of age, social class, or gender.

There are three main sources of words in the Japanese language, the "yamato kotoba" (大和言葉) or "wago" (和語), "kango" (漢語), and "gairaigo" (外来語).

The original language of Japan, or at least the original language of a certain population that was ancestral to a significant portion of the historical and present Japanese nation, was the so-called "yamato kotoba" ( or infrequently , i.e. "Yamato words"), which in scholarly contexts is sometimes referred to as "wago" ( or rarely , i.e. the "Wa language"). In addition to words from this original language, present-day Japanese includes a number of words that were either borrowed from Chinese or constructed from Chinese roots following Chinese patterns. These words, known as "kango" (), entered the language from the 5th century onwards via contact with Chinese culture. According to the "Shinsen Kokugo Jiten" (新選国語辞典) Japanese dictionary, "kango" comprise 49.1% of the total vocabulary, "wago" make up 33.8%, other foreign words or "gairaigo" () account for 8.8%, and the remaining 8.3% constitute hybridized words or "konshugo" () that draw elements from more than one language.

There are also a great number of words of mimetic origin in Japanese, with Japanese having a rich collection of sound symbolism, both onomatopoeia for physical sounds, and more abstract words. A small number of words have come into Japanese from the Ainu language. "Tonakai" (reindeer), "rakko" (sea otter) and "shishamo" (smelt, a type of fish) are well-known examples of words of Ainu origin.

Words of different origins occupy different registers in Japanese. Like Latin-derived words in English, "kango" words are typically perceived as somewhat formal or academic compared to equivalent Yamato words. Indeed, it is generally fair to say that an English word derived from Latin/French roots typically corresponds to a Sino-Japanese word in Japanese, whereas a simpler Anglo-Saxon word would best be translated by a Yamato equivalent.

Incorporating vocabulary from European languages, "gairaigo", began with borrowings from Portuguese in the 16th century, followed by words from Dutch during Japan's long isolation of the Edo period. With the Meiji Restoration and the reopening of Japan in the 19th century, borrowing occurred from German, French, and English. Today most borrowings are from English.

In the Meiji era, the Japanese also coined many neologisms using Chinese roots and morphology to translate European concepts; these are known as wasei kango (Japanese-made Chinese words). Many of these were then imported into Chinese, Korean, and Vietnamese via their kanji in the late 19th and early 20th centuries. For example, "seiji" 政治 ("politics"), and "kagaku" 化学 ("chemistry") are words derived from Chinese roots that were first created and used by the Japanese, and only later borrowed into Chinese and other East Asian languages. As a result, Japanese, Chinese, Korean, and Vietnamese share a large common corpus of vocabulary in the same way a large number of Greek- and Latin-derived words – both inherited or borrowed into European languages, or modern coinages from Greek or Latin roots – are shared among modern European languages – see classical compound.

In the past few decades, "wasei-eigo" ("made-in-Japan English") has become a prominent phenomenon. Words such as "wanpatān" (< "one" + "pattern", "to be in a rut", "to have a one-track mind") and "sukinshippu" (< "skin" + "-ship", "physical contact"), although coined by compounding English roots, are nonsensical in most non-Japanese contexts; exceptions exist in nearby languages such as Korean however, which often use words such as "skinship" and "rimokon" (remote control) in the same way as in Japanese.

The popularity of many Japanese cultural exports has made some native Japanese words familiar in English, including "futon, haiku, judo, kamikaze, karaoke, karate, ninja, origami, rickshaw" (from "jinrikisha"), "samurai, sayonara, Sudoku, sumo, sushi, tsunami, tycoon". See list of English words of Japanese origin for more.

Literacy was introduced to Japan in the form of the Chinese writing system, by way of Baekje before the 5th century. Using this language, the Japanese king Bu presented a petition to Emperor Shun of Liu Song in AD 478. After the ruin of Baekje, Japan invited scholars from China to learn more of the Chinese writing system. Japanese emperors gave an official rank to Chinese scholars (続守言/薩弘格/ 袁晋卿) and spread the use of Chinese characters from the 7th century to the 8th century.

At first, the Japanese wrote in Classical Chinese, with Japanese names represented by characters used for their meanings and not their sounds. Later, during the 7th century AD, the Chinese-sounding phoneme principle was used to write pure Japanese poetry and prose, but some Japanese words were still written with characters for their meaning and not the original Chinese sound. This is when the history of Japanese as a written language begins in its own right. By this time, the Japanese language was already very distinct from the Ryukyuan languages.

An example of this mixed style is the Kojiki, which was written in AD 712. They then started to use Chinese characters to write Japanese in a style known as "man'yōgana", a syllabic script which used Chinese characters for their sounds in order to transcribe the words of Japanese speech syllable by syllable.

Over time, a writing system evolved. Chinese characters (kanji) were used to write either words borrowed from Chinese, or Japanese words with the same or similar meanings. Chinese characters were also used to write grammatical elements, were simplified, and eventually became two syllabic scripts: hiragana and katakana which were developed based on Manyogana from Baekje. However this hypothesis "Manyogana from Baekje" is denied by other scholars.

Hiragana and Katakana were first simplified from Kanji, and Hiragana, emerging somewhere around the 9th century, was mainly used by women. Hiragana was seen as an informal language, whereas Katakana and Kanji were considered more formal and was typically used by men and in official settings. However, because of hiragana's easy of use, more and more people began using it. Eventually, by the 10th century, hiragana was used by everyone.

Modern Japanese is written in a mixture of three main systems: kanji, characters of Chinese origin used to represent both Chinese loanwords into Japanese and a number of native Japanese morphemes; and two syllabaries: hiragana and katakana. The Latin script (or romaji in Japanese) is used to a certain extent, such as for imported acronyms and to transcribe Japanese names and in other instances where non-Japanese speakers need to know how to pronounce a word (such as "ramen" at a restaurant). Arabic numerals are much more common than the kanji when used in counting, but kanji numerals are still used in compounds, such as "tōitsu" ("unification").

Historically, attempts to limit the number of kanji in use commenced in the mid-19th century, but did not become a matter of government intervention until after Japan's defeat in the Second World War. During the period of post-war occupation (and influenced by the views of some U.S. officials), various schemes including the complete abolition of kanji and exclusive use of rōmaji were considered. The "jōyō kanji" ("common use kanji", originally called "tōyō kanji" [kanji for general use]) scheme arose as a compromise solution.

Japanese students begin to learn kanji from their first year at elementary school. A guideline created by the Japanese Ministry of Education, the list of "kyōiku kanji" ("education kanji", a subset of "jōyō kanji"), specifies the 1,006 simple characters a child is to learn by the end of sixth grade. Children continue to study another 1,130 characters in junior high school, covering in total 2,136 "jōyō kanji". The official list of "jōyō kanji" was revised several times, but the total number of officially sanctioned characters remained largely unchanged.

As for kanji for personal names, the circumstances are somewhat complicated. "Jōyō kanji" and "jinmeiyō kanji" (an appendix of additional characters for names) are approved for registering personal names. Names containing unapproved characters are denied registration. However, as with the list of "jōyō kanji", criteria for inclusion were often arbitrary and led to many common and popular characters being disapproved for use. Under popular pressure and following a court decision holding the exclusion of common characters unlawful, the list of "jinmeiyō kanji" was substantially extended from 92 in 1951 (the year it was first decreed) to 983 in 2004. Furthermore, families whose names are not on these lists were permitted to continue using the older forms.

"Hiragana" are used for words without kanji representation, for words no longer written in kanji, and also following kanji to show conjugational endings. Because of the way verbs (and adjectives) in Japanese are conjugated, kanji alone cannot fully convey Japanese tense and mood, as kanji cannot be subject to variation when written without losing its meaning. For this reason, hiragana are suffixed to the ends of kanji to show verb and adjective conjugations. Hiragana used in this way are called okurigana. Hiragana can also be written in a superscript called furigana above or beside a kanji to show the proper reading. This is done to facilitate learning, as well as to clarify particularly old or obscure (or sometimes invented) readings.

"Katakana", like hiragana, are a syllabary; katakana are primarily used to write foreign words, plant and animal names, and for emphasis. For example, "Australia" has been adapted as "Ōsutoraria" (), and "supermarket" has been adapted and shortened into "sūpā" ().

Many major universities throughout the world provide Japanese language courses, and a number of secondary and even primary schools worldwide offer courses in the language. This is much changed from before World War II; in 1940, only 65 Americans not of Japanese descent were able to read, write and understand the language.

International interest in the Japanese language dates from the 19th century but has become more prevalent following Japan's economic bubble of the 1980s and the global popularity of Japanese popular culture (such as anime and video games) since the 1990s. Close to 4 million people studied the language worldwide in 2012: more than 1 million Chinese, 872,000 Indonesian, and 840,000 South Koreans studied Japanese in lower and higher educational institutions. In the three years from 2009 to 2012 the number of students studying Japanese in China increased by 26.5 percent/three years, and by 21.8 percent in Indonesia, but dropped 12.8 percent in South Korea.

In Japan, more than 90,000 foreign students studied at Japanese universities and Japanese language schools, including 77,000 Chinese and 15,000 South Koreans in 2003. In addition, local governments and some NPO groups provide free Japanese language classes for foreign residents, including Japanese Brazilians and foreigners married to Japanese nationals. In the United Kingdom, study of the Japanese language is supported by the British Association for Japanese Studies. In Ireland, Japanese is offered as a language in the Leaving Certificate in some schools.

The Japanese government provides standardized tests to measure spoken and written comprehension of Japanese for second language learners; the most prominent is the Japanese Language Proficiency Test (JLPT), which features five levels of exams (changed from four levels in 2010), ranging from elementary (N5) to advanced (N1). The JLPT is offered twice a year. The Japanese External Trade Organization JETRO organizes the "Business Japanese Proficiency Test" which tests the learner's ability to understand Japanese in a business setting. The Japan Kanji Aptitude Testing Foundation, which took over the BJT from JETRO in 2009, announced in August 2010 that the test would be discontinued in 2011 due to financial pressures on the Foundation. However, it has since issued a statement to the effect that the test will continue to be available as a result of support from the Japanese government.






</doc>
<doc id="15608" url="https://en.wikipedia.org/wiki?curid=15608" title="Johnny Got His Gun">
Johnny Got His Gun

Johnny Got His Gun is an anti-war novel written in 1938 by American novelist, and later blacklisted screenwriter, Dalton Trumbo, and published September 1939 by J. B. Lippincott. The novel won one of the early National Book Awards: the Most Original Book of 1939.

Joe Bonham, a young American soldier serving in World War I, awakens in a hospital bed after being caught in the blast of an exploding artillery shell. He gradually realizes that he has lost his arms, legs, and all of his face (including his eyes, ears, teeth, and tongue), but that his mind functions perfectly, leaving him a prisoner in his own body.

Joe attempts suicide by suffocation, but finds that he had been given a tracheotomy that he can neither remove nor control. At first Joe wishes to die, but later decides that he desires to be placed in a glass box and toured around the country in order to show others the true horrors of war. Joe successfully communicates these desires with military officials after months and months of banging his head on his pillow in Morse code. However, he realizes that the military will not grant his wishes, as it is "against regulations". It is implied that he will live the rest of his natural life in his condition.

As Joe drifts between reality and fantasy, he remembers his old life with his family and girlfriend, and reflects upon the myths and realities of war.

Joe Bonham is the main character. "The novel mainly consists of his reminiscences of childhood and his current struggle to remain sane and, finally, to communicate."

"As a caretaker, capable of great humanistic love, the regular day nurse stands apart from the terse medical establishment, represented by the Morse code man, yet is not capable of the perceptive sympathy of the new day nurse."

Joe's father, Bill Bonham, courted Joe's mother and raised a family with her in Colorado. "His character comes to stand for Joe's nostalgia for an older way of life." It is also said that Bill passes away (chapter 1) leaving his mother and his younger sisters alone (one aged 13 years, the other aged about 9 years).

Joe's mother, Marcia Bonham, was always close to Joe and Bill. She is referenced regularly in the book singing, cooking/baking and playing the piano often.

Kareen (who was aged 19 years at the time of Joe's departure) is mentioned throughout the book as Joe floats between reality and fantasy. She and Joe sleep together for the first time (chapter 3) the night before he leaves, with her father's approval.

Diane is only mentioned in chapter 4. In that chapter it is mentioned that she cheated on Joe with a boy named Glen Hogan. She also cheats on Joe with his best friend, Bill Harper (who told him that she cheated with Hogan).

Bill Harper warns Joe that Diane has cheated on him with Glen Hogan. Joe, who doesn't believe the news, hits Bill. Joe later finds out Bill was truthful and decides that he wants to renew their friendship. However, he finds Bill and Diane making out at her home and is hurt by both. The end of chapter 4 references how Bill was killed at Belleau Wood.

Joe meets Howie (chapter 4) after his troubles with Diane and Glen Hogan. It seems that Howie was never able to keep a girl in his life, and his girlfriend Onie also cheated on him with Glen Hogan. Joe and Howie decide not only to forget about their girlfriends but also about Glen Hogan. Joe and Howie join a group of Mexicans working on a railroad. However, once Howie receives an apologetic telegram from Onie, the boys decide to return home.

José worked at a bakery with Joe. He was given the job at the bakery through the local homeless shelter. José has many stories that set him apart from the other homeless workers, including the fact that he refused marriage to a wealthy woman. José wanted to work in Hollywood. When the opportunity presented itself to work for a picture company, José purposely gets fired because he feels his own personal honor will not allow him to quit on the boss that gave him his original opportunity.

The new day nurse was the first person to successfully communicate with Joe after his injuries. She moved her finger on his bare chest in the shape of the letter M until Joe signaled that he understood "M". She then spelled out "MERRY CHRISTMAS" and Joe signaled that he understood. The new day nurse then deduced that Joe's head-banging was in Morse Code and fetched someone who knew Morse Code.

The title is a play on the phrase "Johnny get your gun",
a rallying call that was commonly used to encourage young American men to enlist in the military in the late 19th and early 20th century. That phrase was popularized in the George M. Cohan song "Over There", which was widely recorded in the first year of American involvement in World War I; the versions by Al Jolson, Enrico Caruso, and Nora Bayes are believed to have sold the most copies on phonograph records at the time. "Johnny Get Your Gun" is also the name of a 1919 film directed by Donald Crisp.

Many of protagonist Joe Bonham's early memories are based on Dalton Trumbo's early life in Colorado and Los Angeles. The novel was inspired by an article he read about the Prince of Wales' visit to a Canadian veterans hospital to see a soldier who had lost all of his senses and his limbs (possibly Ethelbert “Curley” Christian, though he did not suffer severe facial wounds or lose any of his senses 1, 2, 3, and 4 in references). "Though the novel was a pacifist piece published in wartime, it was well reviewed and won an American Booksellers Award in 1940."

Serialized in the "Daily Worker" in March 1940, the book became "a rally point for the political left" which had opposed involvement in World War II during the period of the Molotov–Ribbentrop Pact. Shortly after the 1941 German invasion of the Soviet Union, Trumbo and his publishers decided to suspend reprinting the book until the end of the war. After receiving letters from right-wing isolationists requesting copies of the book, Trumbo contacted the FBI and turned these letters over to them. Trumbo regretted this decision, which he later called "foolish," after two FBI agents showed up at his home and it became clear that "their interest lay not in the letters but in me."

On March 9, 1940, a radio adaptation of "Johnny Got His Gun" was produced and directed by Arch Oboler, based on his script, and presented on the NBC Radio series, "Arch Oboler's Plays". James Cagney voiced Joe Bonham on that broadcast.

In 1971, Trumbo directed a film adaptation of the novel, starring Timothy Bottoms as Joe Bonham. In 1982, "Johnny Got His Gun" was adapted into a stage play by Bradley Rand Smith, which has since been performed worldwide. Its first off-Broadway run starred Jeff Daniels, who won an Obie Award for his performance.

The song "One", by American heavy metal band Metallica, is heavily based on the events of the book and condition of Joe. The music video for the song features several clips from the film adaptation.

In 2008, Benjamin McKenzie earned critical acclaim for his solo performance (as Joe Bonham) in the "live on stage, on film" version of the 1982 Off-Broadway play based on the novel, McKenzie's fourth starring role in a feature film.

In early 2009, the 1971 film made its U.S. DVD debut, produced by Shout! Factory. The DVD included the original, uncut film, plus a 2005 documentary ("Dalton Trumbo: Rebel In Hollywood"), new cast interviews, Metallica's music video "One", behind-the-scenes footage with commentary by stars Timothy Bottoms and Jules Brenner, the 1940 radio adaptation, and the original theatrical trailer.

In October 2010, a special educational DVD of the 2008 film version starring McKenzie became available free of charge to every high school library in the U.S.

The UK stage premiere of the Bradley Rand Smith version—directed by David Mercatali and starring Jack Holden—ran at the Southwark Playhouse from May 21, 2014 to June 14, 2014.



</doc>
<doc id="15611" url="https://en.wikipedia.org/wiki?curid=15611" title="Simon–Ehrlich wager">
Simon–Ehrlich wager

The Simon-Ehrlich Wager describes a 1980 scientific wager between business professor Julian L. Simon and biologist Paul Ehrlich, betting on a mutually agreed-upon measure of resource scarcity over the decade leading up to 1990. The widely-followed contest originated in the pages of "Social Science Quarterly", where Simon challenged Ehrlich to put his money where his mouth was. In response to Ehrlich's published claim that "If I were a gambler, I would take even money that England will not exist in the year 2000"—a proposition Simon regarded as too silly to bother with—Simon countered with "a public offer to stake US$10,000 ... on my belief that the cost of non-government-controlled raw materials (including grain and oil) will not rise in the long run."

Simon challenged Ehrlich to choose any raw material he wanted and a date more than a year away, and he would wager on the inflation-adjusted prices decreasing as opposed to increasing. Ehrlich chose copper, chromium, nickel, tin, and tungsten. The bet was formalized on September 29, 1980, with September 29, 1990 as the payoff date. Ehrlich lost the bet, as all five commodities that were bet on declined in price from 1980 through 1990, the wager period.

In 1968, Ehrlich published "The Population Bomb", which argued that mankind was facing a demographic catastrophe with the rate of population growth quickly outstripping growth in the supply of food and resources. Simon was highly skeptical of such claims, so proposed a wager, telling Ehrlich to select any raw material he wanted and select "any date more than a year away," and Simon would bet that the commodity's price on that date would be lower than what it was at the time of the wager.

Ehrlich and his colleagues picked five metals that they thought would undergo big price increases: chromium, copper, nickel, tin, and tungsten. Then, on paper, they bought $200 worth of each, for a total bet of $1,000, using the prices on September 29, 1980, as an index. They designated September 29, 1990, 10 years hence, as the payoff date. If the inflation-adjusted prices of the various metals rose in the interim, Simon would pay Ehrlich the combined difference. If the prices fell, Ehrlich et al. would pay Simon.

Between 1980 and 1990, the world's population grew by more than 800 million, the largest increase in one decade in all of history. But by September 1990, the price of each of Ehrlich's selected metals had fallen. Chromium, which had sold for $3.90 a pound in 1980, was down to $3.70 in 1990. Tin, which was $8.72 a pound in 1980, was down to $3.88 a decade later.

As a result, in October 1990, Paul Ehrlich mailed Julian Simon a check for $576.07 to settle the wager in Simon's favor.

Julian Simon won because the price of three of the five metals went down in nominal terms and all five of the metals fell in price in inflation-adjusted terms, with both tin and tungsten falling by more than half. In his book "Betrayal of Science and Reason", Ehrlich wrote that Simon "[asserted] that humanity would never run out of anything". Ehrlich added that he and fellow fellow scientists viewed renewable resources as more important indicators of the state of planet Earth, but that he decided to go along with the bet anyway. Afterward, Simon offered to raise the wager to $20,000 and to use any resources at any time that Ehrlich preferred. Ehrlich countered with a challenge to bet that temperatures would increase in the future. The two were unable to reach an agreement on the terms of a second wager before Simon died.

Ehrlich could have won if the bet had been for a different ten-year period. Ehrlich wrote that the five metals in question had increased in price between the years 1950 to 1975. Asset manager Jeremy Grantham wrote that if the Simon–Ehrlich wager had been for a longer period (from 1980 to 2011), then Simon would have lost on four of the five metals. He also noted that if the wager had been expanded to "all of the most important commodities," instead of just five metals, over that longer period of 1980 to 2011, then Simon would have lost "by a lot." Economist Mark J. Perry noted that for an even longer period of time, from 1934 to 2013, the inflation-adjusted price of the Dow Jones-AIG Commodity Index showed "an overall significant downward trend" and concluded that Simon was "more right than lucky". Economist Tim Worstall wrote that "The end result of all of this is that yes, it is true that Ehrlich could have, would have, won the bet depending upon the starting date. ... But the long term trend for metals at least is downwards."

The price of raw and other natural commodities such as oil, gold, and uranium have risen substantially in recent years, due to increased demand from China, India, and other industrializing countries. However, Simon has argued that this price increase is not necessarily contrary to his cornucopian theory. Ehrlich has dismissed the bet as a side issue and stated that the main worry is environmental problems like the ozone hole, acid rain, and global warming.

Understanding that Simon wanted to bet again, Ehrlich and climatologist Stephen Schneider counter-offered, challenging Simon to bet on 15 current trends, betting $1000 that each will get worse (as in the previous wager) over a ten-year future period.

The trends they bet would continue to worsen were:

Simon declined Ehrlich and Schneider's offer to bet, and used the following analogy to explain why he did so:

In 1996, Simon bet $1000 with David South, professor of the Auburn University School of Forestry, that the inflation-adjusted price of timber would decrease in the following five years. Simon paid out early on the bet in 1997 (before his death in 1998) based on his expectation that prices would remain above 1996 levels (which they did).

In 1999, when "The Economist" headlined an article entitled, "$5 a barrel oil soon?" and with oil trading in the $12/barrel range, David South offered $1000 to any economist who would bet with him that the price of oil would be greater than $12/barrel in 2010. No economist took him up on the offer. However, in October 2000, Zagros Madjd-Sadjadi, an economist with The University of the West Indies, bet $1000 with David South that the inflation-adjusted price of oil would decrease to an inflation-adjusted price of $25 by 2010 (down from what was then $30/barrel). Madjd-Sadjadi paid South an inflation-adjusted $1,242 in January 2010. The price of oil at the time was $81/barrel.





</doc>
<doc id="15612" url="https://en.wikipedia.org/wiki?curid=15612" title="John Tenniel">
John Tenniel

Sir John Tenniel (28 February 1820 – 25 February 1914) was an English illustrator, graphic humorist, and political cartoonist prominent in the second half of the 19th century. He was knighted for his artistic achievements in 1893. Tenniel is remembered especially as the principal political cartoonist for "Punch" magazine for over 50 years, and for his illustrations to Lewis Carroll's "Alice's Adventures in Wonderland" (1865) and "Through the Looking-Glass, and What Alice Found There" (1871).

Tenniel was born in Bayswater, West London, to John Baptist Tenniel, a fencing and dancing master of Huguenot descent, and Eliza Maria Tenniel. Tenniel had five siblings; two brothers and three sisters. One sister, Mary, was later to marry Thomas Goodwin Green, owner of the pottery that produced Cornishware. Tenniel was a quiet and introverted person, both as a boy and as an adult. He was content to remain firmly out of the limelight and seemed unaffected by competition or change. His biographer Rodney Engen wrote that Tenniel's "life and career was that of the supreme gentlemanly outside, living on the edge of respectability."

In 1840, Tenniel, while practising fencing with his father, received a serious eye wound from his father's foil, which had accidentally lost its protective tip. Over the years, Tenniel gradually lost sight in his right eye; he never told his father of the severity of the wound, as he did not wish to upset his father further.

In spite of his tendency towards high art, Tenniel was already known and appreciated as a humorist and his early companionship with Charles Keene fostered and developed his talent for scholarly caricature.

Tenniel became a student of the Royal Academy of Arts in 1842 by probation—he was admitted because he made several copies of classical sculptures to provide the necessary admission portfolio. So, it was here that Tenniel returned to his earlier independent education. While Tenniel's more formal training at the Royal Academy and at other institutions was beneficial in nurturing his artistic ambitions, it failed in Tenniel's mind because he disagreed with the school's teaching methods, resulting in Tenniel educating himself for his career. Tenniel studied classical sculptures through painting; however, Tenniel was frustrated that he was never taught how to draw. Tenniel would draw the classical statues at the London's Townley Gallery, copied illustrations from books of costumes and armor in the British museum, and drew the animals from the zoo in Regent's Park as well as the actors from the London theatres, which were drawn from the pits. It was in these studies that Tenniel learned to love detail; however, he became impatient with his work and was the happiest when he could draw from memory. Tenniel was blessed with a photographic memory, undermining his early training and seriously restricting his artistic ambitions.

Another "formal" means of training was Tenniel's participation in an artists group, free from the rules of the academy that stifled Tenniel. In the mid-1840s Tenniel joined the Artist's Society or Clipstone Street Life Academy, and it could be said here that Tenniel first emerged as a satirical draftsman.

Tenniel's first book illustration was for Samuel Carter Hall's "The Book of British Ballads", in 1842. While engaged with his first book illustrations, various contests were taking place in London, as a way in which the government could combat the growing Germanic Nazarenes style and promote a truly national English school of art. Tenniel planned to enter the 1845 House of Lords competition amongst artists to win the opportunity to design the mural decoration of the new Palace of Westminster. Despite missing the deadline, he submitted a cartoon, "An Allegory of Justice", to a competition for designs for the mural decoration of the new Palace of Westminster. For this he received a £200 premium and a commission to paint a fresco in the Upper Waiting Hall (or Hall of Poets) in the House of Lords.

As the influential result of his position as the chief cartoon artist for "Punch" (published 1841–1992, 1996–2002), John Tenniel, through satirical, often radical and at times vitriolic images of the world, for five decades was and remained Great Britain's steadfast social witness to the sweeping national changes in that nation's moment of political and social reform.
At Christmas 1850 he was invited by Mark Lemon to fill the position of joint cartoonist (with John Leech) on "Punch". He had been selected on the strength of his recent illustrations to Aesop's "Fables". He contributed his first drawing in the initial letter appearing on p. 224, vol. xix. His first cartoon was "Lord Jack the Giant Killer", which showed Lord John Russell assailing Cardinal Wiseman.

In 1861, Tenniel was offered John Leech's position at "Punch", as political cartoonist; however, Tenniel still maintained some sense of decorum and restraint into the heated social and political issues of the day.

Because his task was to construct the willful choices of his "Punch" editors, who probably took their cue from "The Times" and would have felt the suggestions of political tensions from Parliament as well, Tenniel's work, as was its design, could be scathing in effect. The restlessness of the Victorian period's issues of working class radicalism, labor, war, economy, and other national themes were the targets of "Punch", which in turn commanded the nature of Tenniel's subjects. Tenniel's cartoons published in the 1860s made popular the portrait of the Irishman as a sub-human being, wanton in his appetites and most resembling an orangutan in both facial features and posture. Many of Tenniel's political cartoons expressed strong hostility to Irish Nationalism, with Fenians and Land leagues depicted as monstrous, ape-like brutes, while "Hibernia"—the personification of Ireland—was depicted as a beautiful, helpless young girl threatened by these "monsters" and turning for protection to "her elder sister", the powerful armoured Britannia.

"An Unequal Match", his drawing published in "Punch" on 8 October 1881, depicted a police officer fighting a criminal with only a 'baton' for protection, trying to put a point across to the public that policing methods needed to be changed.

When examined separately from the book illustrations he did over time, Tenniel's work at "Punch" alone, expressing decades of editorial viewpoints, often controversial and socially sensitive, was created to echo the voices of the British public. Tenniel drew 2,165 cartoons for "Punch", a liberal and politically active publication that mirrored the Victorian public's mood for liberal social changes; thus Tenniel, in his cartoons, represented for years the conscience of the British majority.

In his career Tenniel contributed around 2,300 cartoons, innumerable minor drawings, many double-page cartoons for "Punch's Almanac" and other special numbers, and 250 designs for "Punch's Pocket-books". By 1866 he was "able to command ten to fifteen guineas for the reworking of a single "Punch" cartoon as a pencil sketch", alongside his "comfortable" "Punch" salary "of about £800 a year". According to the Bank of England inflation calculator, £800 in 1866 would buy goods and services worth over £85,000 in 2015 (with inflation averaged at 3.1% a year).

Despite the thousands of political cartoons and hundreds of illustrative works attributed to him, much of Tenniel's fame stems from his illustrations for "Alice". Tenniel drew ninety-two drawings for Lewis Carroll's "Alice's Adventures in Wonderland" (London: Macmillan, 1865) and "Through the Looking-Glass and What Alice Found There" (London: Macmillan, 1871).

Lewis Carroll originally illustrated "Wonderland" himself, but his artistic abilities were limited. Engraver Orlando Jewitt, who had worked for Carroll in 1859 and had reviewed Carroll's drawings for "Wonderland", suggested that he employ a professional illustrator. Carroll was a regular reader of "Punch" and was therefore familiar with Tenniel. In 1865 Tenniel, after long talks with Carroll, illustrated the first edition of "Alice's Adventures in Wonderland".

The first print run of 2,000 was sold in the United States, rather than England, because Tenniel objected to the print quality. A new edition was released in December 1865, carrying an 1866 date, and became an instant best-seller, increasing Tenniel's fame. His drawings for both books have become some of the most famous literary illustrations. After 1872, when the Carroll projects were finished, Tenniel largely abandoned literary illustration. Carroll did later approach Tenniel to undertake another project for him. To this Tenniel replied:
Tenniel's illustrations for the "Alice" books were engraved onto blocks of deal wood by the Brothers Dalziel. These engravings were then used as masters for making the electrotype copies for the actual printing of the books. The original wood blocks are held in the collection of the Bodleian Library in Oxford. They are not usually on public display, but were exhibited in 2003.

The style associated with the Nazarene movement of the nineteenth century influenced many subsequent artists including Tenniel. This style can be characterized as "shaded outlines" where the lines on the side of figures or objects are given extra thickness or are drawn as double lines in order to suggest shading or volume. Additionally, this style is extremely precise, with the artist making a hard clear outline along its figures, creating dignified figures and compositions, as well as a restraint in expression and paleness of tone. While Tenniel's early illustrations done in the Nazarene style were not well received, his encounter with the style pointed him in the right direction.

After the 1850s, Tenniel's style modernized to incorporate more detail in backgrounds and in figures. The inclusion of background details corrected the previously weak Germanic staging of his illustrations. Tenniel's more precisely-designed illustrations depicted specific moments of time, locale, and individual character instead of just generalized scenes.

In addition to a change in specificity of background, Tenniel developed a new interest in human types, expressions, and individualized representation, something that would carry over into Tenniel's illustrations of Wonderland. Referred to by many as theatricalism, this hallmark of Tenniel's style probably stemmed from his earlier interest in caricature. In Tenniel's first years on "Punch" he developed this caricaturist's interest in the uniqueness of persons and things, almost giving a human like personality to the objects in the environment. For example, in a comparison to one of John Everett Millais's illustration of a girl in a chair with Tenniel's illustration of Alice in a chair, one can see how where Millais's chair is just a prop, Tenniel's chair possesses a menacing and towering presence.

Another change in style was his shaded lines. These transformed from mechanical horizontal lines to vigorously hand-drawn hatching that greatly intensified darker areas.

Tenniel's "grotesqueness" was one of the main reasons why Lewis Carroll wanted Tenniel as his illustrator for the "Alice" books. The grotesque is an abnormality that imparts the disturbing sense that the real world may have ceased to be reliable. Tenniel's style was characteristically grotesque in his dark atmospheric compositions of exaggerated fantasy creatures that were carefully drawn in outline. Often though, the mechanism was to use animal heads on recognizable human bodies or vice versa, as Grandville had done with such effect in the pages of the Parisian satirical journal, "Charivari". In Tenniel's illustrations, the grotesque is found also in the merging of beings and things, deformities of and violence to the human body (as seen in the illustration when Alice drinks the potion and gets large), a proclivity to deal with the ordinary things of this world while exhibiting such phenomena. Most notably done in grotesque fashion is that of Tenniel's famous Jabberwock drawing in "Alice".

The "Alice" illustrations combine fantasy and reality. Scholars such as Morris say that Tenniel's stylistic change can be attributed to the late 1850s trend towards realism. For the grotesque to operate, "it is our world which has to be transformed and not some fantasy realm." In the illustrations we are constantly but subtly reminded of the real world, such as some of Tenniel's scenes being derived from a medieval town, the portico of Georgian town, or the checked jacket on the white rabbit. Additionally, Tenniel closely follows the text provided by Carroll so readers are ensured that what they are reading, they are seeing in his illustrations. These subtle points of realism help convince readers that all these seemingly grotesque inhabitants of Wonderland are simply themselves, are simply real, they are not performing.

One of the most unusual elements of the "Alice" books is the placement of Tenniel's illustrations on the pages. There was a physical relation of the illustrations to the text, intended to subtly mesh illustrations with certain points of the text. Carroll and Tenniel expressed this in various ways; one of them bracketing. Two relevant sentences would bracket an image, which might better define the moment that Tenniel was trying to illustrate. It is this precise bracketing of Tenniel's pictures by the text that adds to their "dramatic immediacy." However, other illustrations work with the texts in that they act as captions, though it is not as frequent as bracketing.

Another way in which the illustrations correspond with the text is by having broader and narrower illustrations. Broader illustrations are meant to be centered on the page, where as narrower illustrations are meant to be "let in" or run flush to the margin to be set alongside a narrowed column of the continuing text. Still, words run in parallel with the depiction of those things. For example, in this image, we see how when Alice says, "Oh, my poor little feet," it not only occurs at the foot of the page but is directly next to her feet in the illustration. Part of these narrower illustrations was the "L" shaped illustration, which was of great importance, being that these are where Tenniel did some of his most memorable work. The top or base of these illustrations would run the full width of the page but the other end would have some room on one side of the quadrant for the text.

A selected list:

Entirely by Tenniel

Tenniel's different collaborations:

An ultimate tribute came to an elderly Tenniel as he was knighted for his public service in 1893 by Queen Victoria. It was the first such honour ever bestowed on an illustrator or cartoonist, and his fellows saw his knighting as gratitude for "raising what had been a fairly lowly profession to an unprecedented level of respectability." With knighthood, Tenniel elevated the social status of the black and white illustrator, and sparked a new sense of recognition of his profession. When he retired in January 1901, Tenniel was honoured with a farewell banquet (12 June), at which AJ Balfour, then Leader of the House of Commons, presided. 

Tenniel died in February 25, 1914, just 3 days before his 94th birthday. He was 93 years old.

"Punch" historian M. H. Spielmann, who knew Tenniel, wrote that the political clout contained in his "Punch" cartoons was capable of "swaying parties and people, too... (the cartoons) exercised great influence" on the ideas of popular reform skirting throughout the British public. Two days after his death, "The Daily Graphic" recalled Tenniel: "He had an influence on the political feeling of this time which is hardly measurable ... While Tenniel was drawing them (his subjects), we always looked to the Punch cartoon to crystallize the national and international situation, and the popular feeling about it—and never looked in vain." This condition of social influence resulted from the weekly publishing over a fifty-year span of his political cartoons, whereby Tenniel's fame allowed for a want and need for his particular illustrative work, away from the newspaper. Tenniel became not only one of Victorian Britain's most published illustrators, but as a "Punch" cartoonist he became one of the "supreme social observers" of British society, and an integral component of a powerful journalistic force. Also in 1914, the "New-York Tribune" journalist George W. Smalley referred to John Tenniel as "one of the greatest intellectual forces of his time, (who) understood social laws and political energies."

Public exhibitions of Sir John Tenniel's work were held in 1895 and in 1900. Sir John Tenniel is also the author of one of the mosaics, "Leonardo da Vinci", in the South Court in the Victoria and Albert Museum; while his highly stippled watercolour drawings appeared from time to time in the exhibitions of the Royal Institute of Painters in Water Colours, of which he had been elected a member in 1874.

A Bayswater street, Tenniel Close, near his former studio, is named after him.




</doc>
<doc id="15613" url="https://en.wikipedia.org/wiki?curid=15613" title="Jazz">
Jazz

Jazz is a music genre that originated in the African-American communities of New Orleans, United States, in the late 19th and early 20th centuries, and developed from roots in blues and ragtime. Jazz is seen by many as "America's classical music". Since the 1920s Jazz Age, jazz has become recognized as a major form of musical expression. It then emerged in the form of independent traditional and popular musical styles, all linked by the common bonds of African-American and European-American musical parentage with a performance orientation. Jazz is characterized by swing and blue notes, call and response vocals, polyrhythms and improvisation. Jazz has roots in West African cultural and musical expression, and in African-American music traditions including blues and ragtime, as well as European military band music. Intellectuals around the world have hailed jazz as "one of America's original art forms".

As jazz spread around the world, it drew on different national, regional, and local musical cultures, which gave rise to many distinctive styles. New Orleans jazz began in the early 1910s, combining earlier brass-band marches, French quadrilles, biguine, ragtime and blues with collective polyphonic improvisation. In the 1930s, heavily arranged dance-oriented swing big bands, Kansas City jazz, a hard-swinging, bluesy, improvisational style and Gypsy jazz (a style that emphasized musette waltzes) were the prominent styles. Bebop emerged in the 1940s, shifting jazz from danceable popular music toward a more challenging "musician's music" which was played at faster tempos and used more chord-based improvisation. Cool jazz developed near the end of the 1940s, introducing calmer, smoother sounds and long, linear melodic lines.

The 1950s saw the emergence of free jazz, which explored playing without regular meter, beat and formal structures, and in the mid-1950s, hard bop emerged, which introduced influences from rhythm and blues, gospel, and blues, especially in the saxophone and piano playing. Modal jazz developed in the late 1950s, using the mode, or musical scale, as the basis of musical structure and improvisation. Jazz-rock fusion appeared in the late 1960s and early 1970s, combining jazz improvisation with rock music's rhythms, electric instruments, and highly amplified stage sound. In the early 1980s, a commercial form of jazz fusion called smooth jazz became successful, garnering significant radio airplay. Other styles and genres abound in the 2000s, such as Latin and Afro-Cuban jazz.

The origin of the word "jazz" has resulted in considerable research, and its history is well documented. It is believed to be related to "jasm", a slang term dating back to 1860 meaning "pep, energy". The earliest written record of the word is in a 1912 article in the "Los Angeles Times" in which a minor league baseball pitcher described a pitch which he called a "jazz ball" "because it wobbles and you simply can't do anything with it".

The use of the word in a musical context was documented as early as 1915 in the "Chicago Daily Tribune." Its first documented use in a musical context in New Orleans was in a November 14, 1916 "Times-Picayune" article about "jas bands". In an interview with NPR, musician Eubie Blake offered his recollections of the original slang connotations of the term, saying: "When Broadway picked it up, they called it 'J-A-Z-Z'. It wasn't called that. It was spelled 'J-A-S-S'. That was dirty, and if you knew what it was, you wouldn't say it in front of ladies." The American Dialect Society named it the Word of the Twentieth Century.

Jazz is difficult to define because it encompasses a wide range of music spanning a period of over 100 years, from ragtime to the rock-infused fusion. Attempts have been made to define jazz from the perspective of other musical traditions, such as European music history or African music. But critic Joachim-Ernst Berendt argues that its terms of reference and its definition should be broader, defining jazz as a "form of art music which originated in the United States through the confrontation of the Negro with European music" and arguing that it differs from European music in that jazz has a "special relationship to time defined as 'swing. Jazz involves "a spontaneity and vitality of musical production in which improvisation plays a role" and contains a "sonority and manner of phrasing which mirror the individuality of the performing jazz musician". In the opinion of Robert Christgau, "most of us would say that inventing meaning while letting loose is the essence and promise of jazz".
A broader definition that encompasses different eras of jazz has been proposed by Travis Jackson: "it is music that includes qualities such as swing, improvising, group interaction, developing an 'individual voice', and being open to different musical possibilities". Krin Gibbard argued that "jazz is a construct" which designates "a number of musics with enough in common to be understood as part of a coherent tradition". In contrast to commentators who have argued for excluding types of jazz, musicians are sometimes reluctant to define the music they play. Duke Ellington, one of jazz's most famous figures, said, "It's all music."

Although jazz is considered difficult to define, in part because it contains many subgenres, improvisation is one of its key elements. The centrality of improvisation is attributed to the influence of earlier forms of music such as blues, a form of folk music which arose in part from the work songs and field hollers of African-American slaves on plantations. These work songs were commonly structured around a repetitive call-and-response pattern, but early blues was also improvisational. Classical music performance is evaluated more by its fidelity to the musical score, with less attention given to interpretation, ornamentation, and accompaniment. The classical performer's goal is to play the composition as it was written. In contrast, jazz is often characterized by the product of interaction and collaboration, placing less value on the contribution of the composer, if there is one, and performer. The jazz performer interprets a tune in individual ways, never playing the same composition twice. Depending on the performer's mood, experience, and interaction with band members or audience members, the performer may change melodies, harmonies, and time signatures.

In early Dixieland, a.k.a. New Orleans jazz, performers took turns playing melodies and improvising countermelodies. In the swing era of the 1920s–'40s, big bands relied more on arrangements which were written or learned by ear and memorized. Soloists improvised within these arrangements. In the bebop era of the 1940s, big bands gave way to small groups and minimal arrangements in which the melody was stated briefly at the beginning and most of the song was improvised. Modal jazz abandoned chord progressions to allow musicians to improvise even more. In many forms of jazz, a soloist is supported by a rhythm section of one or more chordal instruments (piano, guitar), double bass, and drums. The rhythm section plays chords and rhythms that outline the song structure and complement the soloist. In avant-garde and free jazz, the separation of soloist and band is reduced, and there is license, or even a requirement, for the abandoning of chords, scales, and meters.

Since the emergence of bebop, forms of jazz that are commercially oriented or influenced by popular music have been criticized. According to Bruce Johnson, there has always been a "tension between jazz as a commercial music and an art form". Traditional jazz enthusiasts have dismissed bebop, free jazz, and jazz fusion as forms of debasement and betrayal. An alternative view is that jazz can absorb and transform diverse musical styles. By avoiding the creation of norms, jazz allows avant-garde styles to emerge.

For some African Americans, jazz has drawn attention to African-American contributions to culture and history. For others, jazz is a reminder of "an oppressive and racist society and restrictions on their artistic visions". Amiri Baraka argues that there is a "white jazz" genre that expresses whiteness. White jazz musicians appeared in the midwest and in other areas throughout the U.S. Papa Jack Laine, who ran the Reliance band in New Orleans in the 1910s, was called "the father of white jazz". The Original Dixieland Jazz Band, whose members were white, were the first jazz group to record, and Bix Beiderbecke was one of the most prominent jazz soloists of the 1920s. The Chicago School (or Chicago Style) was developed by white musicians such as Eddie Condon, Bud Freeman, Jimmy McPartland, and Dave Tough. Others from Chicago such as Benny Goodman and Gene Krupa became leading members of swing during the 1930s. Many bands included both black and white musicians. These musicians helped change attitudes toward race in the U.S.

Female jazz performers and composers have contributed throughout jazz history. Although Betty Carter, Ella Fitzgerald, Adelaide Hall, Billie Holiday, Abbey Lincoln, Anita O'Day, Dinah Washington, and Ethel Waters were recognized for their vocal talent, women received less recognition for their accomplishments as bandleaders, composers, and instrumentalists. This group includes pianist Lil Hardin Armstrong and songwriters Irene Higginbotham and Dorothy Fields. Women began playing instruments in jazz in the early 1920s, drawing particular recognition on piano. Popular musicians of the time were Lovie Austin, Sweet Emma Barrett, Jeanette Kimball, Billie Pierce, Mary Lou Williams

When male jazz musicians were drafted during World War II, many all-female bands took over. The International Sweethearts of Rhythm, which was founded in 1937, was a popular band that became the first all-female integrated band in the U.S. and the first to travel with the USO, touring Europe in 1945. Women were members of the big bands of Woody Herman and Gerald Wilson. From the 1950s onwards many women jazz instrumentalists became prominent, some sustaining lengthy careers. Over the decades, some of the most distinctive improvisers, composers and bandleaders in jazz have been women.

Jazz originated in the late 19th to early 20th century as interpretations of American and European classical music entwined with African and slave folk songs and the influences of West African culture. Its composition and style have changed many times throughout the years with each performer's personal interpretation and improvisation, which is also one of the greatest appeals of the genre.

 By the 18th century, slaves gathered socially at a special market, in an area which later became known as Congo Square, famous for its African dances.

By 1866, the Atlantic slave trade had brought nearly 400,000 Africans to North America. The slaves came largely from West Africa and the greater Congo River basin and brought strong musical traditions with them. The African traditions primarily use a single-line melody and call-and-response pattern, and the rhythms have a counter-metric structure and reflect African speech patterns.

An 1885 account says that they were making strange music (Creole) on an equally strange variety of 'instruments'—washboards, washtubs, jugs, boxes beaten with sticks or bones and a drum made by stretching skin over a flour-barrel.

Lavish festivals featuring African-based dances to drums were organized on Sundays at "Place Congo", or Congo Square, in New Orleans until 1843. There are historical accounts of other music and dance gatherings elsewhere in the southern United States. Robert Palmer said of percussive slave music:

Usually such music was associated with annual festivals, when the year's crop was harvested and several days were set aside for celebration. As late as 1861, a traveler in North Carolina saw dancers dressed in costumes that included horned headdresses and cow tails and heard music provided by a sheepskin-covered "gumbo box", apparently a frame drum; triangles and jawbones furnished the auxiliary percussion. There are quite a few [accounts] from the southeastern states and Louisiana dating from the period 1820–1850. Some of the earliest [Mississippi] Delta settlers came from the vicinity of New Orleans, where drumming was never actively discouraged for very long and homemade drums were used to accompany public dancing until the outbreak of the Civil War.

Another influence came from the harmonic style of hymns of the church, which black slaves had learned and incorporated into their own music as spirituals. The origins of the blues are undocumented, though they can be seen as the secular counterpart of the spirituals. However, as Gerhard Kubik points out, whereas the spirituals are homophonic, rural blues and early jazz "was largely based on concepts of heterophony."
During the early 19th century an increasing number of black musicians learned to play European instruments, particularly the violin, which they used to parody European dance music in their own cakewalk dances. In turn, European-American minstrel show performers in blackface popularized the music internationally, combining syncopation with European harmonic accompaniment. In the mid-1800s the white New Orleans composer Louis Moreau Gottschalk adapted slave rhythms and melodies from Cuba and other Caribbean islands into piano salon music. New Orleans was the main nexus between the Afro-Caribbean and African-American cultures.

The "Black Codes" outlawed drumming by slaves, which meant that African drumming traditions were not preserved in North America, unlike in Cuba, Haiti, and elsewhere in the Caribbean. African-based rhythmic patterns were retained in the United States in large part through "body rhythms" such as stomping, clapping, and patting juba dancing.

In the opinion of jazz historian Ernest Borneman, what preceded New Orleans jazz before 1890 was "Afro-Latin music", similar to what was played in the Caribbean at the time. A three-stroke pattern known in Cuban music as tresillo is a fundamental rhythmic figure heard in many different slave musics of the Caribbean, as well as the Afro-Caribbean folk dances performed in New Orleans Congo Square and Gottschalk's compositions (for example "Souvenirs From Havana" (1859)). Tresillo is the most basic and most prevalent duple-pulse rhythmic cell in sub-Saharan African music traditions and the music of the African Diaspora.

Tresillo is heard prominently in New Orleans second line music and in other forms of popular music from that city from the turn of the 20th century to present. "By and large the simpler African rhythmic patterns survived in jazz ... because they could be adapted more readily to European rhythmic conceptions," jazz historian Gunther Schuller observed. "Some survived, others were discarded as the Europeanization progressed."

In the post-Civil War period (after 1865), African Americans were able to obtain surplus military bass drums, snare drums and fifes, and an original African-American drum and fife music emerged, featuring tresillo and related syncopated rhythmic figures. This was a drumming tradition that was distinct from its Caribbean counterparts, expressing a uniquely African-American sensibility. "The snare and bass drummers played syncopated cross-rhythms," observed the writer Robert Palmer, speculating that "this tradition must have dated back to the latter half of the nineteenth century, and it could have not have developed in the first place if there hadn't been a reservoir of polyrhythmic sophistication in the culture it nurtured."

African-American music began incorporating Afro-Cuban rhythmic motifs in the 19th century when the habanera (Cuban contradanza) gained international popularity. Musicians from Havana and New Orleans would take the twice-daily ferry between both cities to perform, and the habanera quickly took root in the musically fertile Crescent City. John Storm Roberts states that the musical genre habanera "reached the U.S. twenty years before the first rag was published." For the more than quarter-century in which the cakewalk, ragtime, and proto-jazz were forming and developing, the habanera was a consistent part of African-American popular music.

Habaneras were widely available as sheet music and were the first written music which was rhythmically based on an African motif (1803). From the perspective of African-American music, the "habanera rhythm" (also known as "congo", "tango-congo", or "tango".) can be thought of as a combination of tresillo and the backbeat. The habanera was the first of many Cuban music genres which enjoyed periods of popularity in the United States and reinforced and inspired the use of tresillo-based rhythms in African-American music.
New Orleans native Louis Moreau Gottschalk's piano piece "Ojos Criollos (Danse Cubaine)" (1860) was influenced by the composer's studies in Cuba: the habanera rhythm is clearly heard in the left hand. In Gottschalk's symphonic work "A Night in the Tropics" (1859), the tresillo variant cinquillo appears extensively. The figure was later used by Scott Joplin and other ragtime composers.
Comparing the music of New Orleans with the music of Cuba, Wynton Marsalis observes that tresillo is the New Orleans "clave", a Spanish word meaning 'code' or 'key', as in the key to a puzzle, or mystery. Although technically the pattern is only half a clave, Marsalis makes the point that the single-celled figure is the guide-pattern of New Orleans music. Jelly Roll Morton called the rhythmic figure the "Spanish tinge" and considered it an essential ingredient of jazz.

The abolition of slavery in 1865 led to new opportunities for the education of freed African Americans. Although strict segregation limited employment opportunities for most blacks, many were able to find work in entertainment. Black musicians were able to provide entertainment in dances, minstrel shows, and in vaudeville, during which time many marching bands were formed. Black pianists played in bars, clubs, and brothels, as ragtime developed.

Ragtime appeared as sheet music, popularized by African-American musicians such as the entertainer Ernest Hogan, whose hit songs appeared in 1895. Two years later, Vess Ossman recorded a medley of these songs as a banjo solo known as "Rag Time Medley". Also in 1897, the white composer William H. Krell published his "Mississippi Rag" as the first written piano instrumental ragtime piece, and Tom Turpin published his "Harlem Rag", the first rag published by an African-American.

The classically trained pianist Scott Joplin produced his "Original Rags" in 1898 and, in 1899, had an international hit with "Maple Leaf Rag", a multi-strain ragtime march with four parts that feature recurring themes and a bass line with copious seventh chords. Its structure was the basis for many other rags, and the syncopations in the right hand, especially in the transition between the first and second strain, were novel at the time.

African-based rhythmic patterns such as tresillo and its variants, the habanera rhythm and cinquillo, are heard in the ragtime compositions of Joplin, Turpin, and others. Joplin's "Solace" (1909) is generally considered to be within the habanera genre: both of the pianist's hands play in a syncopated fashion, completely abandoning any sense of a march rhythm. Ned Sublette postulates that the tresillo/habanera rhythm "found its way into ragtime and the cakewalk," whilst Roberts suggests that "the habanera influence may have been part of what freed black music from ragtime's European bass."

Blues is the name given to both a musical form and a music genre, which originated in African-American communities of primarily the "Deep South" of the United States at the end of the 19th century from their spirituals, work songs, field hollers, shouts and chants and rhymed simple narrative ballads.

The African use of pentatonic scales contributed to the development of blue notes in blues and jazz. As Kubik explains:
Many of the rural blues of the Deep South are "stylistically" an extension and merger of basically two broad accompanied song-style traditions in the west central Sudanic belt:

W. C. Handy became intrigued by the folk blues of the Deep South whilst traveling through the Mississippi Delta. In this folk blues form, the singer would improvise freely within a limited melodic range, sounding like a field holler, and the guitar accompaniment was slapped rather than strummed, like a small drum which responded in syncopated accents, functioning as another "voice". Handy and his band members were formally trained African-American musicians who had not grown up with the blues, yet he was able to adapt the blues to a larger band instrument format and arrange them in a popular music form.

Handy wrote about his adopting of the blues:

The primitive southern Negro, as he sang, was sure to bear down on the third and seventh tone of the scale, slurring between major and minor. Whether in the cotton field of the Delta or on the Levee up St. Louis way, it was always the same. Till then, however, I had never heard this slur used by a more sophisticated Negro, or by any white man. I tried to convey this effect ... by introducing flat thirds and sevenths (now called blue notes) into my song, although its prevailing key was major ..., and I carried this device into my melody as well.

The publication of his "Memphis Blues" sheet music in 1912 introduced the 12-bar blues to the world (although Gunther Schuller argues that it is not really a blues, but "more like a cakewalk"). This composition, as well as his later "St. Louis Blues" and others, included the habanera rhythm, and would become jazz standards. Handy's music career began in the pre-jazz era and contributed to the codification of jazz through the publication of some of the first jazz sheet music.

The blues form which is ubiquitous in jazz is characterized by specific chord progressions, of which the twelve-bar blues progression is the most common. Basic blues progressionions are based on the I, IV and V chords (often called the "one", "four" and "five" chords). An important part of the sound are the microtonal blue notes which, for expressive purposes, are sung or played flattened (thus "between" the notes on a piano), or gradually "bent" (minor third to major third) in relation to the pitch of the major scale. The blue notes opened up an entirely new approach to Western harmony, ultimately leading to a high level of harmonic complexity in jazz.

The music of New Orleans had a profound effect on the creation of early jazz. The reason why jazz is mainly associated with New Orleans is due to the slaves being able to practice elements of their culture such as voodoo, and they were also allowed drums. Many early jazz performers played in venues throughout the city, such as the brothels and bars of the red-light district around Basin Street, known as "Storyville". In addition to dance bands, there were numerous marching bands who played at lavish funerals (later called jazz funerals), which were arranged by the African-American and European-American communities. The instruments used in marching bands and dance bands became the basic instruments of jazz: brass, reeds tuned in the European 12-tone scale, and drums. Small bands which mixed self-taught and well-educated African-American musicians, many of whom came from the funeral procession tradition of New Orleans, played a seminal role in the development and dissemination of early jazz. These bands travelled throughout Black communities in the Deep South and, from around 1914 onwards, Afro-Creole and African-American musicians played in vaudeville shows which took jazz to western and northern US cities.

In New Orleans, a white marching band leader named Papa Jack Laine integrated blacks and whites in his marching band. Laine was known as "the father of white jazz" because of the many top players who passed through his bands (including George Brunies, Sharkey Bonano and the future members of the Original Dixieland Jass Band). Laine was a good talent scout. During the early 1900s, jazz was mostly done in the African-American and mulatto communities, due to segregation laws. The red light district of Storyville, New Orleans was crucial in bringing jazz music to a wider audience via tourists who came to the port city. Many jazz musicians from the African-American communities were hired to perform live music in brothels and bars, including many early jazz pioneers such as Buddy Bolden and Jelly Roll Morton, in addition to those from New Orleans other communities such as Lorenzo Tio and Alcide Nunez. Louis Armstrong also got his start in Storyville and would later find success in Chicago (along with others from New Orleans) after the United States government shut down Storyville in 1917.

The cornetist Buddy Bolden led a band who are often mentioned as one of the prime originators of the style later to be called "jazz". He played in New Orleans around 1895–1906, before developing a mental illness; there are no recordings of him playing. Bolden's band is credited with creating the "big four", the first syncopated bass drum pattern to deviate from the standard on-the-beat march. As the example below shows, the second half of the big four pattern is the habanera rhythm.

Afro-Creole pianist Jelly Roll Morton began his career in Storyville. From 1904, he toured with vaudeville shows around southern cities, also playing in Chicago and New York City. In 1905, he composed his "Jelly Roll Blues", which on its publication in 1915 became the first jazz arrangement in print, introducing more musicians to the New Orleans style.

Morton considered the tresillo/habanera (which he called the Spanish tinge) to be an essential ingredient of jazz. In his own words: Now in one of my earliest tunes, "New Orleans Blues," you can notice the Spanish tinge. In fact, if you can't manage to put tinges of Spanish in your tunes, you will never be able to get the right seasoning, I call it, for jazz.

Morton was a crucial innovator in the evolution from the early jazz form known as ragtime to jazz piano, and could perform pieces in either style; in 1938, Morton made a series of recordings for the Library of Congress, in which he demonstrated the difference between the two styles. Morton's solos, however, were still close to ragtime, and were not merely improvisations over chord changes as in later jazz, but his use of the blues was of equal importance.

Morton loosened ragtime's rigid rhythmic feeling, decreasing its embellishments and employing a swing feeling. Swing is the most important and enduring African-based rhythmic technique used in jazz. An oft quoted definition of swing by Louis Armstrong is: "if you don't feel it, you'll never know it." "The New Harvard Dictionary of Music" states that swing is: "An intangible rhythmic momentum in jazz ... Swing defies analysis; claims to its presence may inspire arguments." The dictionary does nonetheless provide the useful description of triple subdivisions of the beat contrasted with duple subdivisions: swing superimposes six subdivisions of the beat over a basic pulse structure or four subdivisions. This aspect of swing is far more prevalent in African-American music than in Afro-Caribbean music. One aspect of swing, which is heard in more rhythmically complex Diaspora musics, places strokes in-between the triple and duple-pulse "grids".

New Orleans brass bands are a lasting influence, contributing horn players to the world of professional jazz with the distinct sound of the city whilst helping black children escape poverty. The leader of New Orleans' Camelia Brass Band, D'Jalma Ganier, taught Louis Armstrong to play trumpet; Armstrong would then popularize the New Orleans style of trumpet playing, and then expand it. Like Jelly Roll Morton, Armstrong is also credited with the abandonment of ragtime's stiffness in favor of swung notes. Armstrong, perhaps more than any other musician, codified the rhythmic technique of swing in jazz and broadened the jazz solo vocabulary.
The Original Dixieland Jass Band made the music's first recordings early in 1917, and their "Livery Stable Blues" became the earliest released jazz record. That year, numerous other bands made recordings featuring "jazz" in the title or band name, but most were ragtime or novelty records rather than jazz. In February 1918 during World War I, James Reese Europe's "Hellfighters" infantry band took ragtime to Europe, then on their return recorded Dixieland standards including "Darktown Strutters' Ball".

In the northeastern United States, a "hot" style of playing ragtime had developed, notably James Reese Europe's symphonic Clef Club orchestra in New York City, which played a benefit concert at Carnegie Hall in 1912. The Baltimore rag style of Eubie Blake influenced James P. Johnson's development of stride piano playing, in which the right hand plays the melody, while the left hand provides the rhythm and bassline.

In Ohio and elsewhere in the midwest the major influence was ragtime, until about 1919. Around 1912, when the four-string banjo and saxophone came in, musicians began to improvise the melody line, but the harmony and rhythm remained unchanged. A contemporary account states that blues could only be heard in jazz in the gut-bucket cabarets, which were generally looked down upon by the Black middle-class.

 From 1920 to 1933, Prohibition in the United States banned the sale of alcoholic drinks, resulting in illicit speakeasies which became lively venues of the "Jazz Age", hosting popular music including current dance songs, novelty songs, and show tunes. Jazz began to get a reputation as being immoral, and many members of the older generations saw it as threatening the old cultural values and promoting the new decadent values of the Roaring 20s. Henry van Dyke of Princeton University wrote, "... it is not music at all. It's merely an irritation of the nerves of hearing, a sensual teasing of the strings of physical passion." The media too began to denigrate jazz. "The New York Times" used stories and headlines to pick at jazz: Siberian villagers were said by the paper to have used jazz to scare off bears when, in fact, they had used pots and pans; another story claimed that the fatal heart attack of a celebrated conductor was caused by jazz.

In 1919, Kid Ory's Original Creole Jazz Band of musicians from New Orleans began playing in San Francisco and Los Angeles, where in 1922 they became the first black jazz band of New Orleans origin to make recordings. That year also saw the first recording by Bessie Smith, the most famous of the 1920s blues singers. Chicago meanwhile was developing the new "Hot Jazz", where King Oliver joined Bill Johnson. Bix Beiderbecke formed The Wolverines in 1924.

Despite its Southern black origins, there was a larger market for jazzy dance music played by white orchestras. In 1918, Paul Whiteman and his orchestra became a hit in San Francisco, California, signing with Victor Talking Machine Company in 1920 and becoming the top bandleader of the 1920s, giving "hot jazz" a white component, hiring white musicians including Bix Beiderbecke, Jimmy Dorsey, Tommy Dorsey, Frankie Trumbauer, and Joe Venuti. In 1924, Whiteman commissioned Gershwin's "Rhapsody in Blue", which was premiered by his orchestra and jazz began to be recognized as a notable musical form. Olin Downes, reviewing the concert in "The New York Times": "This composition shows extraordinary talent, as it shows a young composer with aims that go far beyond those of his ilk, struggling with a form of which he is far from being master... In spite of all this, he has expressed himself in a significant and, on the whole, highly original form... His first theme ... is no mere dance-tune ... it is an idea, or several ideas, correlated and combined in varying and contrasting rhythms that immediately intrigue the listener."

After Whiteman's band successfully toured Europe, huge hot jazz orchestras in theater pits caught on with other whites, including Fred Waring, Jean Goldkette, and Nathaniel Shilkret. According to Mario Dunkel, Whiteman's success was based on a "rhetoric of domestication" according to which he had elevated and rendered valuable (read "white") a previously inchoate (read "black") kind of music. Whiteman's success caused blacks to follow suit, including Earl Hines (who opened in The Grand Terrace Cafe in Chicago in 1928), Duke Ellington (who opened at the Cotton Club in Harlem in 1927), Lionel Hampton, Fletcher Henderson, Claude Hopkins, and Don Redman, with Henderson and Redman developing the "talking to one another" formula for "hot" Swing music.

In 1924, Louis Armstrong joined the Fletcher Henderson dance band for a year, as featured soloist. The original New Orleans style was polyphonic, with theme variation and simultaneous collective improvisation. Armstrong was a master of his hometown style, but by the time he joined Henderson's band, he was already a trailblazer in a new phase of jazz, with its emphasis on arrangements and soloists. Armstrong's solos went well beyond the theme-improvisation concept and extemporized on chords, rather than melodies. According to Schuller, by comparison, the solos by Armstrong's bandmates (including a young Coleman Hawkins), sounded "stiff, stodgy," with "jerky rhythms and a grey undistinguished tone quality." The following example shows a short excerpt of the straight melody of "Mandy, Make Up Your Mind" by George W. Meyer and Arthur Johnston (top), compared with Armstrong's solo improvisations (below) (recorded 1924). (The example approximates Armstrong's solo, as it doesn't convey his use of swing.)

Armstrong's solos were a significant factor in making jazz a true 20th-century language. After leaving Henderson's group, Armstrong formed his virtuosic Hot Five band, where he popularized scat singing.

Jelly Roll Morton recorded with the New Orleans Rhythm Kings in an early mixed-race collaboration, then in 1926 formed his Red Hot Peppers.

Also in the 1920s Skiffle, jazz played with homemade instruments such as washboard, jugs, musical saw, kazoos, etc. began to be recorded in Chicago, later merging with country music.

The 1930s belonged to popular swing big bands, in which some virtuoso soloists became as famous as the band leaders. Key figures in developing the "big" jazz band included bandleaders and arrangers Count Basie, Cab Calloway, Jimmy and Tommy Dorsey, Duke Ellington, Benny Goodman, Fletcher Henderson, Earl Hines, Harry James, Jimmie Lunceford, Glenn Miller and Artie Shaw. Although it was a collective sound, swing also offered individual musicians a chance to "solo" and improvise melodic, thematic solos which could at times be very complex "important" music.

Swing was also dance music. It was broadcast on the radio "live" nightly across America for many years, especially by Earl Hines and his Grand Terrace Cafe Orchestra broadcasting coast-to-coast from Chicago (well placed for "live" US time-zones).

Over time, social strictures regarding racial segregation began to relax in America: white bandleaders began to recruit black musicians and black bandleaders white ones. In the mid-1930s, Benny Goodman hired pianist Teddy Wilson, vibraphonist Lionel Hampton and guitarist Charlie Christian to join small groups. In the 1930s, Kansas City Jazz as exemplified by tenor saxophonist Lester Young marked the transition from big bands to the bebop influence of the 1940s. An early 1940s style known as "jumping the blues" or jump blues used small combos, uptempo music and blues chord progressions, drawing on boogie-woogie from the 1930s.

As only a limited number of American jazz records were released in Europe, European jazz traces many of its roots to American artists such as James Reese Europe, Paul Whiteman, and Lonnie Johnson, who visited Europe during and after World War I. It was their live performances which inspired European audiences' interest in jazz, as well as the interest in all things American (and therefore exotic) which accompanied the economic and political woes of Europe during this time. The beginnings of a distinct European style of jazz began to emerge in this interwar period.

British jazz began with a tour by the Original Dixieland Jazz Band in 1919. In 1926, Fred Elizalde and His Cambridge Undergraduates began broadcasting on the BBC. Thereafter jazz became an important element in many leading dance orchestras and jazz instrumentalists quickly became numerous.

This distinct style entered full swing in France with the Quintette du Hot Club de France, which began in 1934. Much of this French jazz was a combination of African-American jazz and the symphonic styles in which French musicians were well-trained; in this, it is easy to see the inspiration taken from Paul Whiteman since his style was also a fusion of the two. Belgian guitar virtuoso Django Reinhardt popularized gypsy jazz, a mix of 1930s American swing, French dance hall "musette" and Eastern European folk with a languid, seductive feel; the main instruments are steel stringed guitar, violin, and double bass, and solos pass from one player to another as the guitar and bass play the role of the rhythm section. Some music researchers hold that it was Philadelphia's Eddie Lang and Joe Venuti who pioneered the guitar-violin partnership typical of the genre, which was brought to France after they had been heard live or on Okeh Records in the late 1920s.

By the 1940s, Duke Ellington's music had transcended the bounds of swing, bridging jazz and art music in a natural synthesis. Ellington called his music "American Music" rather than jazz, and liked to describe those who impressed him as "beyond category." These included many of the musicians who were members of his orchestra, some of whom are considered among the best in jazz in their own right, but it was Ellington who melded them into one of the most popular jazz orchestras in the history of jazz. He often composed specifically for the style and skills of these individuals, such as "Jeep's Blues" for Johnny Hodges, "Concerto for Cootie" for Cootie Williams (which later became "Do Nothing Till You Hear from Me" with Bob Russell's lyrics), and "The Mooche" for Tricky Sam Nanton and Bubber Miley. He also recorded songs written by his bandsmen, such as Juan Tizol's "Caravan" and "Perdido", which brought the "Spanish Tinge" to big-band jazz. Several members of the orchestra remained with him for several decades. The band reached a creative peak in the early 1940s, when Ellington and a small hand-picked group of his composers and arrangers wrote for an orchestra of distinctive voices who displayed tremendous creativity.

 In the early 1940s, bebop-style performers began to shift jazz from danceable popular music toward a more challenging "musician's music." The most influential bebop musicians included saxophonist Charlie Parker, pianists Bud Powell and Thelonious Monk, trumpeters Dizzy Gillespie and Clifford Brown, and drummer Max Roach. Divorcing itself from dance music, bebop established itself more as an art form, thus lessening its potential popular and commercial appeal.

Composer Gunther Schuller wrote:

Dizzy Gillespie wrote:

Since bebop was meant to be listened to, not danced to, it could use faster tempos. Drumming shifted to a more elusive and explosive style, in which the ride cymbal was used to keep time while the snare and bass drum were used for accents. This led to a highly syncopated music with a linear rhythmic complexity.

Bebop musicians employed several harmonic devices which were not previously typical in jazz, engaging in a more abstracted form of chord-based improvisation. Bebop scales are traditional scales with an added chromatic passing note; bebop also uses "passing" chords, substitute chords, and altered chords. New forms of chromaticism and dissonance were introduced into jazz, and the dissonant tritone (or "flatted fifth") interval became the "most important interval of bebop" Chord progressions for bebop tunes were often taken directly from popular swing-era songs and reused with a new and more complex melody and/or reharmonized with more complex chord progressions to form new compositions, a practice which was already well-established in earlier jazz, but came to be central to the bebop style. Bebop made use of several relatively common chord progressions, such as blues (at base, I-IV-V, but often infused with ii-V motion) and 'rhythm changes' (I-VI-ii-V) - the chords to the 1930s pop standard "I Got Rhythm." Late bop also moved towards extended forms that represented a departure from pop and show tunes.

The harmonic development in bebop is often traced back to a transcendent moment experienced by Charlie Parker while performing "Cherokee" at Clark Monroe's Uptown House, New York, in early 1942:

Gerhard Kubik postulates that the harmonic development in bebop sprang from the blues and other African-related tonal sensibilities, rather than 20th-century Western art music as some have suggested:

Samuel Floyd states that blues were both the bedrock and propelling force of bebop, bringing about three main developments:

As Kubik explained:
While for an outside observer, the harmonic innovations in bebop would appear to be inspired by experiences in Western "serious" music, from Claude Debussy to Arnold Schoenberg, such a scheme cannot be sustained by the evidence from a cognitive approach. Claude Debussy did have some influence on jazz, for example, on Bix Beiderbecke's piano playing. And it is also true that Duke Ellington adopted and reinterpreted some harmonic devices in European contemporary music. West Coast jazz would run into such debts as would several forms of cool jazz, but bebop has hardly any such debts in the sense of direct borrowings. On the contrary, ideologically, bebop was a strong statement of rejection of any kind of eclecticism, propelled by a desire to activate something deeply buried in self. Bebop then revived tonal-harmonic ideas transmitted through the blues and reconstructed and expanded others in a basically non-Western harmonic approach. The ultimate significance of all this is that the experiments in jazz during the 1940s brought back to African-American music several structural principles and techniques rooted in African traditions

These divergences from the jazz mainstream of the time initially met with a divided, sometimes hostile, response among fans and fellow musicians, especially established swing players, who bristled at the new harmonic sounds. To hostile critics, bebop seemed to be filled with "racing, nervous phrases". But despite the initial friction, by the 1950s, bebop had become an accepted part of the jazz vocabulary.

The general consensus among musicians and musicologists is that the first original jazz piece to be overtly based in clave was "Tanga" (1943), composed by Cuban-born Mario Bauza and recorded by Machito and his Afro-Cubans in New York City. "Tanga" began as a spontaneous descarga (Cuban jam session), with jazz solos superimposed on top.

This was the birth of Afro-Cuban jazz. The use of clave brought the African "timeline", or "key pattern", into jazz. Music organized around key patterns convey a two-celled (binary) structure, which is a complex level of African cross-rhythm. Within the context of jazz, however, harmony is the primary referent, not rhythm. The harmonic progression can begin on either side of clave, and the harmonic "one" is always understood to be "one". If the progression begins on the "three-side" of clave, it is said to be in "3-2 clave". If the progression begins on the "two-side", its in "2-3 clave".

Bobby Sanabria mentions several innovations of Machito's Afro-Cubans, citing them as the first band: to wed big band jazz arranging techniques within an original composition, with jazz oriented soloists utilizing an authentic Afro-Cuban based rhythm section in a successful manner; to explore modal harmony (a concept explored much later by Miles Davis and Gil Evans) from a jazz arranging perspective; and to overtly explore the concept of clave counterpoint from an arranging standpoint (the ability to weave seamlessly from one side of the clave to the other without breaking its rhythmic integrity within the structure of a musical arrangement). They were also the first band in the United States to use the term "Afro-Cuban" as the band's moniker, thus identifying itself and acknowledging the West African roots of the musical form they were playing. It forced New York City's Latino and African-American communities to deal with their common West African musical roots in a direct way, whether they wanted to acknowledge it publicly or not.

Mario Bauzá introduced bebop innovator Dizzy Gillespie to Cuban conga drummer and composer Chano Pozo. Gillespie and Pozo's brief collaboration produced some of the most enduring Afro-Cuban jazz standards. "Manteca" (1947) is the first jazz standard to be rhythmically based on clave. According to Gillespie, Pozo composed the layered, contrapuntal guajeos (Afro-Cuban ostinatos) of the A section and the introduction, while Gillespie wrote the bridge. Gillespie recounted: "If I'd let it go like [Chano] wanted it, it would have been strictly Afro-Cuban all the way. There wouldn't have been a bridge. I thought I was writing an eight-bar bridge, but...I had to keep going and ended up writing a sixteen-bar bridge." The bridge gave "Manteca" a typical jazz harmonic structure, setting the piece apart from Bauza's modal "Tanga" of a few years earlier.

Gillespie's collaboration with Pozo brought specific African-based rhythms into bebop. While pushing the boundaries of harmonic improvisation, "cu-bop" also drew from African rhythm. Jazz arrangements with a Latin A section and a swung B section, with all choruses swung during solos, became common practice with many Latin tunes of the jazz standard repertoire. This approach can be heard on pre-1980 recordings of "Manteca", "A Night in Tunisia", "Tin Tin Deo", and "On Green Dolphin Street".

Cuban percussionist Mongo Santamaria first recorded his composition "Afro Blue" in 1959.
"Afro Blue" was the first jazz standard built upon a typical African three-against-two (3:2) cross-rhythm, or hemiola. The song begins with the bass repeatedly playing 6 cross-beats per each measure of 12/8, or 6 cross-beats per 4 main beats—6:4 (two cells of 3:2). The following example shows the original ostinato "Afro Blue" bass line; the slashed noteheads indicate the main beats (not bass notes), where you would normally tap your foot to keep time.
When John Coltrane covered "Afro Blue" in 1963, he inverted the metric hierarchy, interpreting the tune as a 3/4 jazz waltz with duple cross-beats superimposed (2:3). Originally a B pentatonic blues, Coltrane expanded the harmonic structure of "Afro Blue."

Perhaps the most respected Afro-cuban jazz combo of the late 1950s was vibraphonist Cal Tjader's band. Tjader had Mongo Santamaria, Armando Peraza, and Willie Bobo on his early recording dates.

In the late 1940s, there was a revival of Dixieland, harking back to the contrapuntal New Orleans style. This was driven in large part by record company reissues of jazz classics by the Oliver, Morton, and Armstrong bands of the 1930s. There were two types of musicians involved in the revival: the first group was made up of those who had begun their careers playing in the traditional style and were returning to it (or continuing what they had been playing all along), such as Bob Crosby's Bobcats, Max Kaminsky, Eddie Condon, and Wild Bill Davison. Most of these players were originally Midwesterners, although there were a small number of New Orleans musicians involved. The second group of revivalists consisted of younger musicians, such as those in the Lu Watters band, Conrad Janis, and Ward Kimball and his Firehouse Five Plus Two Jazz Band. By the late 1940s, Louis Armstrong's Allstars band became a leading ensemble. Through the 1950s and 1960s, Dixieland was one of the most commercially popular jazz styles in the US, Europe, and Japan, although critics paid little attention to it.

In 1944, jazz impresario Norman Granz organized the first Jazz at the Philharmonic concert in Los Angeles, which helped make a star of Nat "King" Cole and Les Paul. In 1946, he founded Clef Records, discovering Canadian jazz pianist Oscar Peterson in 1949, and merging Clef Records with his new label Verve Records in 1956, which advanced the career of Ella Fitzgerald et al.

By the end of the 1940s, the nervous energy and tension of bebop was replaced with a tendency toward calm and smoothness with the sounds of cool jazz, which favored long, linear melodic lines. It emerged in New York City and dominated jazz in the first half of the 1950s. The starting point was a collection of 1949 and 1950 singles by a nonet led by Miles Davis, released as the "Birth of the Cool" (1957). Later cool jazz recordings by musicians such as Chet Baker, Dave Brubeck, Bill Evans, Gil Evans, Stan Getz, the Modern Jazz Quartet, and Gerry Mulligan usually had a lighter sound that avoided the aggressive tempos and harmonic abstraction of bebop.

Cool jazz later became strongly identified with the West Coast jazz scene, as typified by singers Chet Baker, Mel Tormé, and Anita O'Day, but it also had a particular resonance in Europe, especially Scandinavia, where figures such as baritone saxophonist Lars Gullin and pianist Bengt Hallberg emerged. The theoretical underpinnings of cool jazz were laid out by the Chicago pianist Lennie Tristano, and its influence stretches into such later developments as bossa nova, modal jazz, and even free jazz.

Hard bop is an extension of bebop (or "bop") music which incorporates influences from rhythm and blues, gospel music and blues, especially in the saxophone and piano playing. Hard bop was developed in the mid-1950s, coalescing in 1953 and 1954; it developed partly in response to the vogue for cool jazz in the early 1950s and paralleled the rise of rhythm and blues. Miles Davis' 1954 performance of "Walkin'" at the first Newport Jazz Festival announced the style to the jazz world. The quintet Art Blakey and the Jazz Messengers, fronted by Blakey and featuring pianist Horace Silver and trumpeter Clifford Brown, were leaders in the hard bop movement along with Davis.

Modal jazz is a development which began in the later 1950s which takes the mode, or musical scale, as the basis of musical structure and improvisation. Previously, a solo was meant to fit into a given chord progression, but with modal jazz, the soloist creates a melody using one (or a small number of) modes. The emphasis is thus shifted from harmony to melody: "Historically, this caused a seismic shift among jazz musicians, away from thinking vertically (the chord), and towards a more horizontal approach (the scale)," explained pianist Mark Levine.

The modal theory stems from a work by George Russell. Miles Davis introduced the concept to the greater jazz world with "Kind of Blue" (1959), an exploration of the possibilities of modal jazz which would become the best selling jazz album of all time. In contrast to Davis' earlier work with hard bop and its complex chord progression and improvisation, "Kind of Blue" was composed as a series of modal sketches in which the musicians were given scales that defined the parameters of their improvisation and style.

"I didn't write out the music for "Kind of Blue", but brought in sketches for what everybody was supposed to play because I wanted a lot of spontaneity," recalled Davis. The track "So What" has only two chords: D-7 and E-7.

Other innovators in this style include Jackie McLean, and two of the musicians who had also played on "Kind of Blue": John Coltrane and Bill Evans.

By the 1950s, Afro-Cuban jazz had been using modes for at least a decade, as much of it borrowed from Cuban popular dance forms which are structured around multiple ostinatos with only a few chords. A case in point is Mario Bauza's "Tanga" (1943), the first Afro-Cuban jazz piece. Machito's Afro-Cubans recorded modal tunes in the 1940s, featuring jazz soloists such as Howard McGhee, Brew Moore, Charlie Parker, and Flip Phillips. However, there is no evidence that Davis or other mainstream jazz musicians were influenced by the use of modes in Afro-Cuban jazz, or other branches of Latin jazz.

Free jazz, and the related form of avant-garde jazz, broke through into an open space of "free tonality" in which meter, beat, and formal symmetry all disappeared, and a range of world music from India, Africa, and Arabia were melded into an intense, even religiously ecstatic or orgiastic style of playing. While loosely inspired by bebop, free jazz tunes gave players much more latitude; the loose harmony and tempo was deemed controversial when this approach was first developed. The bassist Charles Mingus is also frequently associated with the avant-garde in jazz, although his compositions draw from myriad styles and genres.

The first major stirrings came in the 1950s with the early work of Ornette Coleman (whose 1960 album "" coined the term) and Cecil Taylor. In the 1960s, exponents included Albert Ayler, Gato Barbieri, Carla Bley, Don Cherry, Larry Coryell, John Coltrane, Bill Dixon, Jimmy Giuffre, Steve Lacy, Michael Mantler, Sun Ra, Roswell Rudd, Pharoah Sanders, and John Tchicai. In developing his late style, Coltrane was especially influenced by the dissonance of Ayler's trio with bassist Gary Peacock and drummer Sunny Murray, a rhythm section honed with Cecil Taylor as leader. In November 1961, Coltrane played a gig at the Village Vanguard, which resulted in the classic "Chasin' the 'Trane", which "Down Beat" magazine panned as "anti-jazz". On his 1961 tour of France, he was booed, but persevered, signing with the new Impulse! Records in 1960 and turning it into "the house that Trane built", while championing many younger free jazz musicians, notably Archie Shepp, who often played with trumpeter Bill Dixon, who organized the 4-day "October Revolution in Jazz" in Manhattan in 1964, the first free jazz festival.

A series of recordings with the Classic Quartet in the first half of 1965 show Coltrane's playing becoming increasingly abstract, with greater incorporation of devices like multiphonics, utilization of overtones, and playing in the altissimo register, as well as a mutated return to Coltrane's sheets of sound. In the studio, he all but abandoned his soprano to concentrate on the tenor saxophone. In addition, the quartet responded to the leader by playing with increasing freedom. The group's evolution can be traced through the recordings "The John Coltrane Quartet Plays", "Living Space" and "Transition" (both June 1965), "New Thing at Newport" (July 1965), "Sun Ship" (August 1965), and "First Meditations" (September 1965).

In June 1965, Coltrane and 10 other musicians recorded "Ascension", a 40-minute-long piece without breaks that included adventurous solos by young avante-garde musicians as well as Coltrane, and was controversial primarily for the collective improvisation sections that separated the solos. Dave Liebman later called it "the torch that lit the free jazz thing.". After recording with the quartet over the next few months, Coltrane invited Pharoah Sanders to join the band in September 1965. While Coltrane used over-blowing frequently as an emotional exclamation-point, Sanders would opt to overblow his entire solo, resulting in a constant screaming and screeching in the altissimo range of the instrument.

Free jazz quickly found a foothold in Europe, in part because musicians such as Ayler, Taylor, Steve Lacy and Eric Dolphy spent extended periods there, and European musicians Michael Mantler, John Tchicai et al. traveled to the U.S. to experience American approaches firsthand. A distinctive European contemporary jazz (sometimes incorporating elements of free jazz but not limited to it) flourished because of the emergence of highly distinctive European or European-based musicians such as Peter Brötzmann, John Surman, Krzysztof Komeda, Zbigniew Namysłowski, Tomasz Stanko, Lars Gullin, Joe Harriott, Albert Mangelsdorff, Kenny Wheeler, Graham Collier, Michael Garrick and Mike Westbrook, who were anxious to develop new approaches reflecting their national and regional musical cultures and contexts. Since the 1960s, various creative centers of jazz have developed in Europe, such as the creative jazz scene in Amsterdam. Following the work of veteran drummer Han Bennink and pianist Misha Mengelberg, musicians started to explore free music by collectively improvising until a certain form (melody, rhythm, or even famous song) is found by the band. Jazz critic Kevin Whitehead documented the free jazz scene in Amsterdam and some of its main exponents such as the ICP (Instant Composers Pool) orchestra in his book "New Dutch Swing". Since the 1990s Keith Jarrett has been prominent in defending free jazz from criticism by traditionalists. British scholar Stuart Nicholson has been prominent in arguing that European contemporary jazz's identity is now substantially independent of American jazz and follows a different trajectory.

Latin jazz is the term used to describe jazz which employs Latin American rhythms and is generally understood to have a more specific meaning than simply jazz from Latin America. A more precise term might be Afro-Latin jazz, as the jazz subgenre typically employs rhythms that either have a direct analog in Africa or exhibit an African rhythmic influence beyond what is ordinarily heard in other jazz. The two main categories of Latin jazz are Afro-Cuban jazz and Brazilian jazz.

In the 1960s and 1970s, many jazz musicians had only a basic understanding of Cuban and Brazilian music, and jazz compositions which used Cuban or Brazilian elements were often referred to as "Latin tunes", with no distinction between a Cuban son montuno and a Brazilian bossa nova. Even as late as 2000, in Mark Gridley's "Jazz Styles: History and Analysis", a bossa nova bass line is referred to as a "Latin bass figure." It was not uncommon during the 1960s and 1970s to hear a conga playing a Cuban tumbao while the drumset and bass played a Brazilian bossa nova pattern. Many jazz standards such as "Manteca", "On Green Dolphin Street" and "Song for My Father" have a "Latin" A section and a swung B section. Typically, the band would only play an even-eighth "Latin" feel in the A section of the head and swing throughout all of the solos. Latin jazz specialists like Cal Tjader tended to be the exception. For example, on a 1959 live Tjader recording of "A Night in Tunisia", pianist Vince Guaraldi soloed through the entire form over an authentic mambo.

Afro-Cuban jazz often uses Afro-Cuban instruments such as congas, timbales, güiro, and claves, combined with piano, double bass, etc. Afro-Cuban jazz began with Machito's Afro-Cubans in the early 1940s, but took off and entered the mainstream in the late 1940s when bebop musicians such as Dizzy Gillespie and Billy Taylor began experimenting with Cuban rhythms. Mongo Santamaria and Cal Tjader further refined the genre in the late 1950s.

Although a great deal of Cuban-based Latin jazz is modal, Latin jazz is not always modal: it can be as harmonically expansive as post-bop jazz. For example, Tito Puente recorded an arrangement of "Giant Steps" done to an Afro-Cuban guaguancó. A Latin jazz piece may momentarily contract harmonically, as in the case of a percussion solo over a one or two-chord piano guajeo.

Guajeo is the name for the typical Afro-Cuban ostinato melodies which are commonly used motifs in Latin jazz compositions. They originated in the genre known as son. Guajeos provide a rhythmic and melodic framework that may be varied within certain parameters, whilst still maintaining a repetitive - and thus "danceable" - structure. Most guajeos are rhythmically based on clave (rhythm).

Guajeos are one of the most important elements of the vocabulary of Afro-Cuban descarga (jazz-inspired instrumental jams), providing a means of tension and resolution and a sense of forward momentum, within a relatively simple harmonic structure. The use of multiple, contrapuntal guajeos in Latin jazz facilitates simultaneous collective improvisation based on theme variation. In a way, this polyphonic texture is reminiscent of the original New Orleans style of jazz.

For most of its history, Afro-Cuban jazz had been a matter of superimposing jazz phrasing over Cuban rhythms. But by the end of the 1970s, a new generation of New York City musicians had emerged who were fluent in both salsa dance music and jazz, leading to a new level of integration of jazz and Cuban rhythms. This era of creativity and vitality is best represented by the Gonzalez brothers Jerry (congas and trumpet) and Andy (bass). During 1974–1976, they were members of one of Eddie Palmieri's most experimental salsa groups: salsa was the medium, but Palmieri was stretching the form in new ways. He incorporated parallel fourths, with McCoy Tyner-type vamps. The innovations of Palmieri, the Gonzalez brothers and others led to an Afro-Cuban jazz renaissance in New York City.

This occurred in parallel with developments in Cuba The first Cuban band of this new wave was Irakere. Their "Chékere-son" (1976) introduced a style of "Cubanized" bebop-flavored horn lines that departed from the more angular guajeo-based lines which were typical of Cuban popular music and Latin jazz up until that time. It was based on Charlie Parker's composition "Billie's Bounce", jumbled together in a way that fused clave and bebop horn lines. In spite of the ambivalence of some band members towards Irakere's Afro-Cuban folkloric / jazz fusion, their experiments forever changed Cuban jazz: their innovations are still heard in the high level of harmonic and rhythmic complexity in Cuban jazz and in the jazzy and complex contemporary form of popular dance music known as timba.

Brazilian jazz such as bossa nova is derived from samba, with influences from jazz and other 20th-century classical and popular music styles. Bossa is generally moderately paced, with melodies sung in Portuguese or English, whilst the related term jazz-samba describes an adaptation of street samba into jazz.

The bossa nova style was pioneered by Brazilians João Gilberto and Antônio Carlos Jobim and was made popular by Elizete Cardoso's recording of "Chega de Saudade" on the "Canção do Amor Demais" LP. Gilberto's initial releases, and the 1959 film "Black Orpheus", achieved significant popularity in Latin America; this spread to North America via visiting American jazz musicians. The resulting recordings by Charlie Byrd and Stan Getz cemented bossa nova's popularity and led to a worldwide boom, with 1963's "Getz/Gilberto", numerous recordings by famous jazz performers such as Ella Fitzgerald and Frank Sinatra, and the eventual entrenchment of the bossa nova style as a lasting influence in world music.

Brazilian percussionists such as Airto Moreira and Naná Vasconcelos also influenced jazz internationally by introducing Afro-Brazilian folkloric instruments and rhythms into a wide variety of jazz styles, thus attracting a greater audience to them.

Post-bop jazz is a form of small-combo jazz derived from earlier bop styles. The genre's origins lie in seminal work by John Coltrane, Miles Davis, Bill Evans, Charles Mingus, Wayne Shorter, and Herbie Hancock. Generally, the term post-bop is taken to mean jazz from the mid-sixties onwards that assimilates influences from hard bop, modal jazz, the avant-garde and free jazz, without necessarily being immediately identifiable as any of the above.

Much post-bop was recorded for Blue Note Records. Key albums include "Speak No Evil" by Shorter; "The Real McCoy" by McCoy Tyner; "Maiden Voyage" by Hancock; "Miles Smiles" by Davis; and "Search for the New Land" by Lee Morgan (an artist who is not typically associated with the post-bop genre). Most post-bop artists worked in other genres as well, with a particularly strong overlap with the earlier hard bop.

Soul jazz was a development of hard bop which incorporated strong influences from blues, gospel and rhythm and blues to create music for small groups, often the organ trio of Hammond organ, drummer and tenor saxophonist. Unlike hard bop, soul jazz generally emphasized repetitive grooves and melodic hooks, and improvisations were often less complex than in other jazz styles. It often had a steadier "funk" style groove, which was different from the swing rhythms typical of much hard bop.

Horace Silver had a large influence on the soul jazz style, with songs that used funky and often gospel-based piano vamps. Important soul jazz organists included Jimmy McGriff, Jimmy Smith and Johnny Hammond Smith, and influential tenor saxophone players included Eddie "Lockjaw" Davis and Stanley Turrentine.

There was a resurgence of interest in jazz and other forms of African-American cultural expression during the Black Arts Movement and Black nationalist period of the 1960s and 1970s. African themes became popular, and many new jazz compositions were given African-related titles: "Black Nile" (Wayne Shorter), "Blue Nile" (Alice Coltrane), "Obirin African" (Art Blakey), "Zambia" (Lee Morgan), "Appointment in Ghana" (Jackie McLean), "Marabi" (Cannonball Adderley), "Yoruba" (Hubert Laws), and many more. Pianist Randy Weston's music incorporated African elements, such as in the large-scale suite "Uhuru Africa" (with the participation of poet Langston Hughes) and "Highlife: Music From the New African Nations." Both Weston and saxophonist Stanley Turrentine covered the Nigerian Bobby Benson's piece "Niger Mambo", which features Afro-Caribbean and jazz elements within a West African Highlife style. Some musicians, including Pharoah Sanders, Hubert Laws, and Wayne Shorter, began using African instruments such as kalimbas, bells, beaded gourds and other instruments which were not traditional to jazz.

During this period, there was an increased use of the typical African 12/8 cross-rhythmic structure in jazz. Herbie Hancock's "Succotash" on "Inventions and Dimensions" (1963) is an open-ended modal 12/8 improvised jam, in which Hancock's pattern of attack-points, rather than the pattern of pitches, is the primary focus of his improvisations, accompanied by Paul Chambers on bass, percussionist Osvaldo Martinez playing a traditional Afro-Cuban chekeré part and Willie Bobo playing an Abakuá bell pattern on a snare drum with brushes.

The first jazz standard composed by a non-Latino to use an overt African 12/8 cross-rhythm was Wayne Shorter's "Footprints" (1967). On the version recorded on "Miles Smiles" by Miles Davis, the bass switches to a 4/4 tresillo figure at 2:20. "Footprints" is not, however, a Latin jazz tune: African rhythmic structures are accessed directly by Ron Carter (bass) and Tony Williams (drums) via the rhythmic sensibilities of swing. Throughout the piece, the four beats, whether sounded or not, are maintained as the temporal referent. In the example below, the main beats are indicated by slashed noteheads, which do not indicate bass notes.

The use of pentatonic scales was another trend associated with Africa. The use of pentatonic scales in Africa probably goes back thousands of years.

McCoy Tyner perfected the use of the pentatonic scale in his solos, and also used parallel fifths and fourths, which are common harmonies in West Africa.

The minor pentatonic scale is often used in blues improvisation, and like a blues scale, a minor pentatonic scale can be played over all of the chords in a blues. The following pentatonic lick was played over blues changes by Joe Henderson on Horace Silver's "African Queen" (1965).

Jazz pianist, theorist, and educator Mark Levine refers to the scale generated by beginning on the fifth step of a pentatonic scale as the "V pentatonic scale".

Levine points out that the V pentatonic scale works for all three chords of the standard II-V-I jazz progression. This is a very common progression, used in pieces such as Miles Davis' "Tune Up." The following example shows the V pentatonic scale over a II-V-I progression.

Accordingly, John Coltrane's "Giant Steps" (1960), with its 26 chords per 16 bars, can be played using only three pentatonic scales. Coltrane studied Nicolas Slonimsky's "Thesaurus of Scales and Melodic Patterns", which contains material that is virtually identical to portions of "Giant Steps". The harmonic complexity of "Giant Steps" is on the level of the most advanced 20th-century art music. Superimposing the pentatonic scale over "Giant Steps" is not merely a matter of harmonic simplification, but also a sort of "Africanizing" of the piece, which provides an alternate approach for soloing. Mark Levine observes that when mixed in with more conventional "playing the changes", pentatonic scales provide "structure and a feeling of increased space."

In the late 1960s and early 1970s, the hybrid form of jazz-rock fusion was developed by combining jazz improvisation with rock rhythms, electric instruments and the highly amplified stage sound of rock musicians such as Jimi Hendrix and Frank Zappa. Jazz fusion often uses mixed meters, odd time signatures, syncopation, complex chords, and harmonies.

According to AllMusic:
...until around 1967, the worlds of jazz and rock were nearly completely separate. [However, ...] as rock became more creative and its musicianship improved, and as some in the jazz world became bored with hard bop and did not want to play strictly avant-garde music, the two different idioms began to trade ideas and occasionally combine forces."

In 1969, Davis fully embraced the electric instrument approach to jazz with "In a Silent Way", which can be considered his first fusion album. Composed of two side-long suites edited heavily by producer Teo Macero, this quiet, static album would be equally influential to the development of ambient music.

As Davis recalls:
The music I was really listening to in 1968 was James Brown, the great guitar player Jimi Hendrix, and a new group who had just come out with a hit record, "Dance to the Music", Sly and the Family Stone... I wanted to make it more like rock. When we recorded "In a Silent Way" I just threw out all the chord sheets and told everyone to play off of that."

Two contributors to "In a Silent Way" also joined organist Larry Young to create one of the early acclaimed fusion albums: "Emergency!" by The Tony Williams Lifetime.

Davis' "Bitches Brew" (1970) album was his most successful of this era. Although inspired by rock and funk, Davis' fusion creations were original and brought about a new type of avant-garde, electronic, psychedelic-jazz, as far from pop music as any other Davis work.

Pianist Herbie Hancock (a Davis alumnus) released four albums in the short-lived (1970–1973) psychedelic-jazz subgenre: "Mwandishi" (1972), "Crossings" (1973), and "Sextant" (1973). The rhythmic background was a mix of rock, funk, and African-type textures.

Musicians who had previously worked with Davis formed the four most influential fusion groups: Weather Report and Mahavishnu Orchestra emerged in 1971 and were soon followed by Return to Forever and The Headhunters.

Weather Report's self-titled electronic and psychedelic "Weather Report" debut album caused a sensation in the jazz world on its arrival in 1971, thanks to the pedigree of the group's members (including percussionist Airto Moreira), and their unorthodox approach to music. The album featured a softer sound than would be the case in later years (predominantly using acoustic bass with Shorter exclusively playing soprano saxophone, and with no synthesizers involved), but is still considered a classic of early fusion. It built on the avant-garde experiments which Joe Zawinul and Shorter had pioneered with Miles Davis on "Bitches Brew", including an avoidance of head-and-chorus composition in favour of continuous rhythm and movement – but took the music further. To emphasise the group's rejection of standard methodology, the album opened with the inscrutable avant-garde atmospheric piece "Milky Way", which featured by Shorter's extremely muted saxophone inducing vibrations in Zawinul's piano strings while the latter pedalled the instrument. "Down Beat" described the album as "music beyond category", and awarded it Album of the Year in the magazine's polls that year.

Weather Report's subsequent releases were creative funk-jazz works.

Although some jazz purists protested against the blend of jazz and rock, many jazz innovators crossed over from the contemporary hard bop scene into fusion. As well as the electric instruments of rock (such as electric guitar, electric bass, electric piano and synthesizer keyboards), fusion also used the powerful amplification, "fuzz" pedals, wah-wah pedals and other effects that were used by 1970s-era rock bands. Notable performers of jazz fusion included Miles Davis, Eddie Harris, keyboardists Joe Zawinul, Chick Corea, and Herbie Hancock, vibraphonist Gary Burton, drummer Tony Williams (drummer), violinist Jean-Luc Ponty, guitarists Larry Coryell, Al Di Meola, John McLaughlin, and Frank Zappa, saxophonist Wayne Shorter and bassists Jaco Pastorius and Stanley Clarke. Jazz fusion was also popular in Japan, where the band Casiopea released over thirty fusion albums.

According to jazz writer Stuart Nicholson, "just as free jazz appeared on the verge of creating a whole new musical language in the 1960s ... jazz-rock briefly suggested the promise of doing the same" with albums such as Williams' "Emergency!" (1970) and Davis' "Agharta" (1975), which Nicholson said "suggested the potential of evolving into something that might eventually define itself as a wholly independent genre quite apart from the sound and conventions of anything that had gone before." This development was stifled by commercialism, Nicholson said, as the genre "mutated into a peculiar species of jazz-inflected pop music that eventually took up residence on FM radio" at the end of the 1970s.

By the mid-1970s, the sound known as jazz-funk had developed, characterized by a strong back beat (groove), electrified sounds and, often, the presence of electronic analog synthesizers. Jazz-funk also draws influences from traditional African music, Afro-Cuban rhythms and Jamaican reggae, notably Kingston bandleader Sonny Bradshaw. Another feature is the shift of emphasis from improvisation to composition: arrangements, melody and overall writing became important. The integration of funk, soul, and R&B music into jazz resulted in the creation of a genre whose spectrum is wide and ranges from strong jazz improvisation to soul, funk or disco with jazz arrangements, jazz riffs and jazz solos, and sometimes soul vocals.

Early examples are Herbie Hancock's Headhunters band and Miles Davis' "On the Corner" album, which, in 1972, began Davis' foray into jazz-funk and was, he claimed, an attempt at reconnecting with the young black audience which had largely forsaken jazz for rock and funk. While there is a discernible rock and funk influence in the timbres of the instruments employed, other tonal and rhythmic textures, such as the Indian tambora and tablas and Cuban congas and bongos, create a multi-layered soundscape. The album was a culmination of sorts of the "musique concrète" approach that Davis and producer Teo Macero had begun to explore in the late 1960s.

Jazz continued to expand and change, influenced by other types of music such as world music, avant garde classical music and rock and pop. Jazz musicians began to improvise on unusual instruments, such as the jazz harp (Alice Coltrane), the electrically amplified and wah-wah pedaled jazz violin (Jean-Luc Ponty) and the bagpipes (Rufus Harley). In 1966 jazz trumpeter Don Ellis and Indian sitar player Harihar Rao founded the Hindustani Jazz Sextet. In 1971, guitarist John McLaughlin's Mahavishnu Orchestra began playing a mix of rock and jazz infused with East Indian influences. In the 1970s the ECM record label began in Germany with artists including Keith Jarrett, Paul Bley, the Pat Metheny Group, Jan Garbarek, Ralph Towner, Kenny Wheeler, John Taylor, John Surman, and Eberhard Weber, establishing a new chamber music aesthetic which featured mainly acoustic instruments, occasionally incorporating elements of world music and folk.

In 1987, the United States House of Representatives and Senate passed a bill proposed by Democratic Representative John Conyers Jr. to define jazz as a unique form of American music, stating:

... that jazz is hereby designated as a rare and valuable national American treasure to which we should devote our attention, support and resources to make certain it is preserved, understood and promulgated.

It passed in the House of Representatives on September 23, 1987, and in the Senate on November 4, 1987.

The 1980s saw something of a reaction against the fusion and free jazz that had dominated the 1970s. Trumpeter Wynton Marsalis emerged early in the decade, and strove to create music within what he believed was the tradition, rejecting both fusion and free jazz and creating extensions of the small and large forms initially pioneered by artists such as Louis Armstrong and Duke Ellington, as well as the hard bop of the 1950s. It is debatable whether Marsalis' critical and commercial success was a cause or a symptom of the reaction against Fusion and Free Jazz and the resurgence of interest in the kind of jazz pioneered in the 1960s (particularly modal jazz and post-bop); nonetheless there were many other manifestations of a resurgence of traditionalism, even if fusion and free jazz were by no means abandoned and continued to develop and evolve.

For example, several musicians who had been prominent in the fusion genre during the 1970s began to record acoustic jazz once more, including Chick Corea and Herbie Hancock. Other musicians who had experimented with electronic instruments in the previous decade had abandoned them by the 1980s; for example, Bill Evans, Joe Henderson, and Stan Getz. Even the 1980s music of Miles Davis, although certainly still fusion, adopted a far more accessible and recognisably jazz-oriented approach than his abstract work of the mid-1970s, such as a return to a theme-and-solos approach.

The emergence of young jazz talent beginning to perform in older, established musicians' groups further impacted the resurgence of traditionalism in the jazz community. In the 1970s, the groups of Betty Carter and Art Blakey and the Jazz Messengers retained their conservative jazz approaches in the midst of fusion and jazz-rock, and in addition to difficulty booking their acts, struggled to find younger generations of personnel to authentically play traditional styles such as hard bop and bebop. In the late 1970s, however, a resurgence of younger jazz players in Blakey's band began to occur. This movement included musicians such as Valery Ponomarev and Bobby Watson, Dennis Irwin and James Williams.
In the 1980s, in addition to Wynton and Branford Marsalis, the emergence of pianists in the Jazz Messengers such as Donald Brown, Mulgrew Miller, and later, Benny Green, bassists such as Charles Fambrough, Lonnie Plaxico (and later, Peter Washington and Essiet Essiet) horn players such as Bill Pierce, Donald Harrison and later Javon Jackson and Terence Blanchard emerged as talented jazz musicians, all of whom made significant contributions in later 1990s and 2000s jazz music.

The young Jazz Messengers' contemporaries, including Roy Hargrove, Marcus Roberts, Wallace Roney and Mark Whitfield were also influenced by Wynton Marsalis's emphasis toward jazz tradition. These younger rising stars rejected avant-garde approaches and instead championed the acoustic jazz sound of Charlie Parker, Thelonious Monk and early recordings of the first Miles Davis quintet. This group of "Young Lions" sought to reaffirm jazz as a high art tradition comparable to the discipline of classical music.

In addition, Betty Carter's rotation of young musicians in her group foreshadowed many of New York's preeminent traditional jazz players later in their careers. Among these musicians were Jazz Messenger alumni Benny Green, Branford Marsalis and Ralph Peterson Jr., as well as Kenny Washington, Lewis Nash, Curtis Lundy, Cyrus Chestnut, Mark Shim, Craig Handy, Greg Hutchinson and Marc Cary, Taurus Mateen and Geri Allen.

Blue Note Records's O.T.B. ensemble featured a rotation of young jazz musicians such as Kenny Garrett, Steve Wilson, Kenny Davis, Renee Rosnes, Ralph Peterson Jr., Billy Drummond, and Robert Hurst.

A similar reaction took place against free jazz. According to Ted Gioia:

the very leaders of the avant garde started to signal a retreat from the core principles of Free Jazz. Anthony Braxton began recording standards over familiar chord changes. Cecil Taylor played duets in concert with Mary Lou Williams, and let her set out structured harmonies and familiar jazz vocabulary under his blistering keyboard attack. And the next generation of progressive players would be even more accommodating, moving inside and outside the changes without thinking twice. Musicians such as David Murray or Don Pullen may have felt the call of free-form jazz, but they never forgot all the other ways one could play African-American music for fun and profit.

Pianist Keith Jarrett—whose bands of the 1970s had played only original compositions with prominent free jazz elements—established his so-called 'Standards Trio' in 1983, which, although also occasionally exploring collective improvisation, has primarily performed and recorded jazz standards. Chick Corea similarly began exploring jazz standards in the 1980s, having neglected them for the 1970s.

In the early 1980s, a commercial form of jazz fusion called "pop fusion" or "smooth jazz" became successful, garnering significant radio airplay in "quiet storm" time slots at radio stations in urban markets across the U.S. This helped to establish or bolster the careers of vocalists including Al Jarreau, Anita Baker, Chaka Khan, and Sade, as well as saxophonists including Grover Washington Jr., Kenny G, Kirk Whalum, Boney James, and David Sanborn. In general, smooth jazz is downtempo (the most widely played tracks are of 90–105 beats per minute), and has a lead melody-playing instrument (saxophone, especially soprano and tenor, and legato electric guitar are popular).

In his "Newsweek" article "The Problem With Jazz Criticism", Stanley Crouch considers Miles Davis' playing of fusion to be a turning point that led to smooth jazz. Critic Aaron J. West has countered the often negative perceptions of smooth jazz, stating:
I challenge the prevalent marginalization and malignment of smooth jazz in the standard jazz narrative. Furthermore, I question the assumption that smooth jazz is an unfortunate and unwelcomed evolutionary outcome of the jazz-fusion era. Instead, I argue that smooth jazz is a long-lived musical style that merits multi-disciplinary analyses of its origins, critical dialogues, performance practice, and reception.

Acid jazz developed in the UK in the 1980s and 1990s, influenced by jazz-funk and electronic dance music. Acid jazz often contains various types of electronic composition (sometimes including Sampling (music) or a live DJ cutting and scratching), but it is just as likely to be played live by musicians, who often showcase jazz interpretation as part of their performance. Richard S. Ginell of AllMusic considers Roy Ayers "one of the prophets of acid jazz."

Nu jazz is influenced by jazz harmony and melodies, and there are usually no improvisational aspects. It can be very experimental in nature and can vary widely in sound and concept. It ranges from the combination of live instrumentation with the beats of jazz house (as exemplified by St Germain, Jazzanova, and Fila Brazillia) to more band-based improvised jazz with electronic elements (for example, The Cinematic Orchestra, Kobol and the Norwegian "future jazz" style pioneered by Bugge Wesseltoft, Jaga Jazzist, and Nils Petter Molvær).

Jazz rap developed in the late 1980s and early 1990s and incorporates jazz influences into hip hop. In 1988, Gang Starr released the debut single "Words I Manifest", which sampled Dizzy Gillespie's 1962 "Night in Tunisia", and Stetsasonic released "Talkin' All That Jazz", which sampled Lonnie Liston Smith. Gang Starr's debut LP "No More Mr. Nice Guy" (1989) and their 1990 track "Jazz Thing" sampled Charlie Parker and Ramsey Lewis. The groups which made up the Native Tongues Posse tended toward jazzy releases: these include the Jungle Brothers' debut "Straight Out the Jungle" (1988), and A Tribe Called Quest's "People's Instinctive Travels and the Paths of Rhythm" (1990) and "The Low End Theory" (1991). Rap duo Pete Rock & CL Smooth incorporated jazz influences on their 1992 debut "Mecca and the Soul Brother". Rapper Guru's Jazzmatazz series began in 1993 using jazz musicians during the studio recordings.

Alhough jazz rap had achieved little mainstream success, Miles Davis' final album "Doo-Bop" (released posthumously in 1992) was based on hip hop beats and collaborations with producer Easy Mo Bee. Davis' ex-bandmate Herbie Hancock also absorbed hip-hop influences in the mid-1990s, releasing the album "Dis Is Da Drum" in 1994.

The relaxation of orthodoxy which was concurrent with post-punk in London and New York City led to a new appreciation of jazz. In London, the Pop Group began to mix free jazz and dub reggae into their brand of punk rock. In New York, No Wave took direct inspiration from both free jazz and punk. Examples of this style include Lydia Lunch's "Queen of Siam", Gray, the work of James Chance and the Contortions (who mixed Soul with free jazz and punk) and the Lounge Lizards (the first group to call themselves "punk jazz").

John Zorn took note of the emphasis on speed and dissonance that was becoming prevalent in punk rock, and incorporated this into free jazz with the release of the "Spy vs. Spy" album in 1986, a collection of Ornette Coleman tunes done in the contemporary thrashcore style. In the same year, Sonny Sharrock, Peter Brötzmann, Bill Laswell, and Ronald Shannon Jackson recorded the first album under the name Last Exit, a similarly aggressive blend of thrash and free jazz. These developments are the origins of "jazzcore", the fusion of free jazz with hardcore punk.

The M-Base movement started in the 1980s, when a loose collective of young African-American musicians in New York which included Steve Coleman, Greg Osby, and Gary Thomas developed a complex but grooving sound.

In the 1990s, most M-Base participants turned to more conventional music, but Coleman, the most active participant, continued developing his music in accordance with the M-Base concept.

Coleman's audience decreased, but his music and concepts influenced many musicians, according to pianist Vijay Iver and critic Ben Ratlifff of "The New York Times".

M-Base changed from a movement of a loose collective of young musicians to a kind of informal Coleman "school", with a much advanced but already originally implied concept. Steve Coleman's music and M-Base concept gained recognition as "next logical step" after Charlie Parker, John Coltrane, and Ornette Coleman.

Since the 1990s, jazz has been characterized by a pluralism in which no one style dominates, but rather a wide range of styles and genres are popular. Individual performers often play in a variety of styles, sometimes in the same performance. Pianist Brad Mehldau and The Bad Plus have explored contemporary rock music within the context of the traditional jazz acoustic piano trio, recording instrumental jazz versions of songs by rock musicians. The Bad Plus have also incorporated elements of free jazz into their music. A firm avant-garde or free jazz stance has been maintained by some players, such as saxophonists Greg Osby and Charles Gayle, while others, such as James Carter, have incorporated free jazz elements into a more traditional framework.

Harry Connick Jr. began his career playing stride piano and the dixieland jazz of his home, New Orleans, beginning with his first recording when he was ten years old. Some of his earliest lessons were at the home of pianist Ellis Marsalis. Connick had success on the pop charts after recording the soundtrack to the movie "When Harry Met Sally", which sold over two million copies. Crossover success has also been achieved by Diana Krall, Norah Jones, Cassandra Wilson, Kurt Elling, and Jamie Cullum.

A number of players who usually perform in largely straight-ahead settings have emerged since the 1990s, including pianists Jason Moran and Vijay Iyer, guitarist Kurt Rosenwinkel, vibraphonist Stefon Harris, trumpeters Roy Hargrove and Terence Blanchard, saxophonists Chris Potter and Joshua Redman, clarinetist Ken Peplowski and bassist Christian McBride.

Although jazz-rock fusion reached the height of its popularity in the 1970s, the use of electronic instruments and rock-derived musical elements in jazz continued in the 1990s and 2000s. Musicians using this approach include Pat Metheny, John Abercrombie, John Scofield and the Swedish group e.s.t.

In 2001, Ken Burns's documentary "Jazz" was premiered on PBS, featuring Wynton Marsalis and other experts reviewing the entire history of American jazz to that time. It received some criticism, however, for its failure to reflect the many distinctive non-American traditions and styles in jazz that had developed, and its limited representation of US developments in the last quarter of the 20th century.

The mid-2010s have seen an increasing influence of R&B, hip-hop, and pop music on jazz. In 2015, Kendrick Lamar released his third studio album, "To Pimp a Butterfly". The album heavily featured prominent contemporary jazz artists such as Thundercat and redefined jazz rap with a larger focus on improvisation and live soloing rather than simply sampling. In that same year, saxophonist Kamasi Washington released his nearly three-hour long debut, "The Epic". Its hip-hop inspired beats and R&B vocal interludes was not only acclaimed by critics for being innovative in keeping jazz relevant, but also sparked a small resurgence in jazz on the internet.

Another internet-aided trend of 2010's jazz is that of extreme reharmonization, inspired by both virtuosic players known for their speed and rhythm such as Art Tatum, as well as players known for their ambitious voicings and chords such as Bill Evans. Supergroup Snarky Puppy has adopted this trend and has allowed for players like Cory Henry to shape the grooves and harmonies of modern jazz soloing. YouTube phenomenon Jacob Collier also gained recognition for his ability to play an incredibly large number of instruments and his ability to use microtones, advanced polyrhythms, and blend a spectrum of genres in his largely homemade production process.



</doc>
<doc id="15614" url="https://en.wikipedia.org/wiki?curid=15614" title="Jonathan Swift">
Jonathan Swift

Jonathan Swift (30 November 1667 – 19 October 1745) was an Anglo-Irish satirist, essayist, political pamphleteer (first for the Whigs, then for the Tories), poet and cleric who became Dean of St Patrick's Cathedral, Dublin.

Swift is remembered for works such as "A Tale of a Tub" (1704), "An Argument Against Abolishing Christianity" (1712), "Gulliver's Travels" (1726), and "A Modest Proposal" (1729). He is regarded by the "Encyclopædia Britannica" as the foremost prose satirist in the English language, and is less well known for his poetry. He originally published all of his works under pseudonyms – such as Lemuel Gulliver, Isaac Bickerstaff, M. B. Drapier – or anonymously. He was a master of two styles of satire, the Horatian and Juvenalian styles.

His deadpan, ironic writing style, particularly in "A Modest Proposal", has led to such satire being subsequently termed "Swiftian".

Jonathan Swift was born on 30 November 1667 in Dublin, Ireland. He was the second child and only son of Jonathan Swift (1640–1667) and his wife Abigail Erick (or Herrick) of Frisby on the Wreake. His father was a native of Goodrich, Herefordshire, but he accompanied his brothers to Ireland to seek their fortunes in law after their Royalist father's estate was brought to ruin during the English Civil War. His maternal grandfather, James Ericke, was the vicar of Thornton, England. In 1634 the vicar was convicted of Puritan practices. Some time thereafter, Ericke and his family, including his young daughter Abilgail, fled to Ireland.

Swift's father joined his older brother, Godwin, in the practice of law in Ireland. He died in Dublin about seven months before his namesake was born. He died of syphilis, which he said he got from dirty sheets when out of town. 

At the age of one, child Jonathan was taken by his wet nurse to her hometown of Whitehaven, Cumberland, England. He said that there he learned to read the Bible. His nurse returned him to his mother, still in Ireland, when he was three.

His mother returned to England after his birth, leaving him in the care of his Uncle Godwin, a close friend and confidant of Sir John Temple whose son later employed Swift as his secretary.
Swift's family had several interesting literary connections. His grandmother Elizabeth (Dryden) Swift was the niece of Sir Erasmus Dryden, grandfather of poet John Dryden. The same grandmother's aunt Katherine (Throckmorton) Dryden was a first cousin of Elizabeth, wife of Sir Walter Raleigh. His great-great grandmother Margaret (Godwin) Swift was the sister of Francis Godwin, author of "The Man in the Moone" which influenced parts of Swift's "Gulliver's Travels". His uncle Thomas Swift married a daughter of poet and playwright Sir William Davenant, a godson of William Shakespeare.

Swift's benefactor and uncle Godwin Swift (1628–1695) took primary responsibility for the young man, sending him with one of his cousins to Kilkenny College (also attended by philosopher George Berkeley). He arrived there at the age of six, where he was expected to have already learned the basic declensions in Latin. He hadn't and so started at a lower form. Swift graduated in 1682, when he was 15.

He attended Dublin University (Trinity College, Dublin) in 1682, financed by Godwin's son Willoughby. The four-year course followed a curriculum largely set in the Middle Ages for the priesthood. The lectures were dominated by Aristotelian logic and philosophy. The basic skill taught the students was debate and they were expected to be able to argue both sides of any argument or topic. Swift was an above-average student but not exceptional, and received his B.A. in 1686 "by special grace."

Swift was studying for his master's degree when political troubles in Ireland surrounding the Glorious Revolution forced him to leave for England in 1688, where his mother helped him get a position as secretary and personal assistant of Sir William Temple at Moor Park, Farnham. Temple was an English diplomat who arranged the Triple Alliance of 1668. He had retired from public service to his country estate to tend his gardens and write his memoirs. Gaining his employer's confidence, Swift "was often trusted with matters of great importance". Within three years of their acquaintance, Temple had introduced his secretary to William III and sent him to London to urge the King to consent to a bill for triennial Parliaments.

Swift took up his residence at Moor Park where he met Esther Johnson, then eight years old, the daughter of an impoverished widow who acted as companion to Temple's sister Lady Giffard. Swift was her tutor and mentor, giving her the nickname "Stella", and the two maintained a close but ambiguous relationship for the rest of Esther's life.

In 1690, Swift left Temple for Ireland because of his health but returned to Moor Park the following year. The illness consisted of fits of vertigo or giddiness, now known to be Ménière's disease, and it continued to plague him throughout his life. During this second stay with Temple, Swift received his M.A. from Hart Hall, Oxford, in 1692. Then he left Moor Park, apparently despairing of gaining a better position through Temple's patronage, to become an ordained priest in the Established Church of Ireland. He was appointed to the prebend of Kilroot in the Diocese of Connor in 1694, with his parish located at Kilroot, near Carrickfergus in County Antrim.

Swift appears to have been miserable in his new position, being isolated in a small, remote community far from the centres of power and influence. While at Kilroot, however, he may well have become romantically involved with Jane Waring, whom he called "Varina", the sister of an old college friend. A letter from him survives, offering to remain if she would marry him and promising to leave and never return to Ireland if she refused. She presumably refused, because Swift left his post and returned to England and Temple's service at Moor Park in 1696, and he remained there until Temple's death. There he was employed in helping to prepare Temple's memoirs and correspondence for publication. During this time, Swift wrote "The Battle of the Books", a satire responding to critics of Temple's "Essay upon Ancient and Modern Learning" (1690), though "Battle" was not published until 1704.

Temple died on 27 January 1699. Swift, normally a harsh judge of human nature, said that all that was good and amiable in mankind had died with Temple. He stayed on briefly in England to complete editing Temple's memoirs, and perhaps in the hope that recognition of his work might earn him a suitable position in England. Unfortunately, his work made enemies among some of Temple's family and friends, in particular Temple's formidable sister Lady Giffard, who objected to indiscretions included in the memoirs. Swift's next move was to approach King William directly, based on his imagined connection through Temple and a belief that he had been promised a position. This failed so miserably that he accepted the lesser post of secretary and chaplain to the Earl of Berkeley, one of the Lords Justice of Ireland. However, when he reached Ireland, he found that the secretaryship had already been given to another. He soon obtained the living of Laracor, Agher, and Rathbeggan, and the prebend of Dunlavin in St Patrick's Cathedral, Dublin.

Swift ministered to a congregation of about 15 at Laracor, which was just over four and half miles (7.5 km) from Summerhill, County Meath, and from Dublin. He had abundant leisure for cultivating his garden, making a canal after the Dutch fashion of Moor Park, planting willows, and rebuilding the vicarage. As chaplain to Lord Berkeley, he spent much of his time in Dublin and travelled to London frequently over the next ten years. In 1701, he anonymously published the political pamphlet "A Discourse on the Contests and Dissentions in Athens and Rome".

Swift had residence in Trim, County Meath, after 1700. He wrote many of his works during this time period. In February 1702, Swift received his Doctor of Divinity degree from Trinity College, Dublin. That spring he travelled to England and then returned to Ireland in October, accompanied by Esther Johnson—now 20—and his friend Rebecca Dingley, another member of William Temple's household. There is a great mystery and controversy over Swift's relationship with Esther Johnson, nicknamed "Stella". Many, notably his close friend Thomas Sheridan, believed that they were secretly married in 1716; others, like Swift's housekeeper Mrs Brent and Rebecca Dingley (who lived with Stella all through her years in Ireland) dismissed the story as absurd. Swift certainly did not wish her to marry anyone else: in 1704, when their mutual friend William Tisdall informed Swift that he intended to propose to Stella, Swift wrote to him to dissuade him from the idea. Although the tone of the letter was courteous, Swift privately expressed his disgust for Tisdall as an "interloper", and they were estranged for many years.

During his visits to England in these years, Swift published "A Tale of a Tub" and "The Battle of the Books" (1704) and began to gain a reputation as a writer. This led to close, lifelong friendships with Alexander Pope, John Gay, and John Arbuthnot, forming the core of the Martinus Scriblerus Club (founded in 1713).

Swift became increasingly active politically in these years. From 1707 to 1709 and again in 1710, Swift was in London unsuccessfully urging upon the Whig administration of Lord Godolphin the claims of the Irish clergy to the First-Fruits and Twentieths ("Queen Anne's Bounty"), which brought in about £2,500 a year, already granted to their brethren in England. He found the opposition Tory leadership more sympathetic to his cause, and, when they came to power in 1710, he was recruited to support their cause as editor of "The Examiner". In 1711, Swift published the political pamphlet "The Conduct of the Allies", attacking the Whig government for its inability to end the prolonged war with France. The incoming Tory government conducted secret (and illegal) negotiations with France, resulting in the Treaty of Utrecht (1713) ending the War of the Spanish Succession.

Swift was part of the inner circle of the Tory government, and often acted as mediator between Henry St John (Viscount Bolingbroke), the secretary of state for foreign affairs (1710–15), and Robert Harley (Earl of Oxford), lord treasurer and prime minister (1711–1714). Swift recorded his experiences and thoughts during this difficult time in a long series of letters to Esther Johnson, collected and published after his death as "A Journal to Stella". The animosity between the two Tory leaders eventually led to the dismissal of Harley in 1714. With the death of Queen Anne and accession of George I that year, the Whigs returned to power, and the Tory leaders were tried for treason for conducting secret negotiations with France.

Also during these years in London, Swift became acquainted with the Vanhomrigh family (Dutch merchants who had settled in Ireland, then moved to London) and became involved with one of the daughters, Esther. Swift furnished Esther with the nickname "Vanessa" (derived by adding "Essa", a pet form of Esther, to the "Van" of her surname, Vanhomrigh), and she features as one of the main characters in his poem "Cadenus and Vanessa". The poem and their correspondence suggest that Esther was infatuated with Swift, and that he may have reciprocated her affections, only to regret this and then try to break off the relationship. Esther followed Swift to Ireland in 1714, and settled at her old family home, Celbridge Abbey. Their uneasy relationship continued for some years; then there appears to have been a confrontation, possibly involving Esther Johnson. Esther Vanhomrigh died in 1723 at the age of 35, having destroyed the will she had made in Swift's favour. Another lady with whom he had a close but less intense relationship was Anne Long, a toast of the Kit-Cat Club.

Before the fall of the Tory government, Swift hoped that his services would be rewarded with a church appointment in England. However, Queen Anne appeared to have taken a dislike to Swift and thwarted these efforts. Her dislike has been attributed to "A Tale of a Tub", which she thought blasphemous, compounded by "The Windsor Prophecy", where Swift, with a surprising lack of tact, advised the Queen on which of her bedchamber ladies she should and should not trust. The best position his friends could secure for him was the Deanery of St Patrick's; this was not in the Queen's gift and Anne, who could be a bitter enemy, made it clear that Swift would not have received the preferment if she could have prevented it. With the return of the Whigs, Swift's best move was to leave England and he returned to Ireland in disappointment, a virtual exile, to live "like a rat in a hole".

Once in Ireland, however, Swift began to turn his pamphleteering skills in support of Irish causes, producing some of his most memorable works: "Proposal for Universal Use of Irish Manufacture" (1720), "Drapier's Letters" (1724), and "A Modest Proposal" (1729), earning him the status of an Irish patriot. This new role was unwelcome to the Government, which made clumsy attempts to silence him. His printer, Edward Waters, was convicted of seditious libel in 1720, but four years later a grand jury refused to find that the "Drapier's Letters" (which, though written under a pseudonym, were universally known to be Swift's work) were seditious. Swift responded with an attack on the Irish judiciary almost unparalleled in its ferocity, his principal target being the "vile and profligate villain" William Whitshed, Lord Chief Justice of Ireland.

Also during these years, he began writing his masterpiece, "Travels into Several Remote Nations of the World, in Four Parts, by Lemuel Gulliver, first a surgeon, and then a captain of several ships", better known as "Gulliver's Travels". Much of the material reflects his political experiences of the preceding decade. For instance, the episode in which the giant Gulliver puts out the Lilliputian palace fire by urinating on it can be seen as a metaphor for the Tories' illegal peace treaty; having done a good thing in an unfortunate manner. In 1726 he paid a long-deferred visit to London, taking with him the manuscript of "Gulliver's Travels". During his visit he stayed with his old friends Alexander Pope, John Arbuthnot and John Gay, who helped him arrange for the anonymous publication of his book. First published in November 1726, it was an immediate hit, with a total of three printings that year and another in early 1727. French, German, and Dutch translations appeared in 1727, and pirated copies were printed in Ireland.

Swift returned to England one more time in 1727 and stayed with Alexander Pope once again. The visit was cut short when Swift received word that Esther Johnson was dying, and rushed back home to be with her. On 28 January 1728, Esther Johnson died; Swift had prayed at her bedside, even composing prayers for her comfort. Swift could not bear to be present at the end, but on the night of her death he began to write his "The Death of Mrs Johnson". He was too ill to attend the funeral at St Patrick's. Many years later, a lock of hair, assumed to be Esther Johnson's, was found in his desk, wrapped in a paper bearing the words, "Only a woman's hair".
Death became a frequent feature of Swift's life from this point. In 1731 he wrote "Verses on the Death of Dr. Swift", his own obituary published in 1739. In 1732, his good friend and collaborator John Gay died. In 1735, John Arbuthnot, another friend from his days in London, died. In 1738 Swift began to show signs of illness, and in 1742 he may have suffered a stroke, losing the ability to speak and realising his worst fears of becoming mentally disabled. ("I shall be like that tree," he once said, "I shall die at the top.") He became increasingly quarrelsome, and long-standing friendships, like that with Thomas Sheridan, ended without sufficient cause. To protect him from unscrupulous hangers on, who had begun to prey on the great man, his closest companions had him declared of "unsound mind and memory". However, it was long believed by many that Swift was actually insane at this point. In his book "Literature and Western Man", author J. B. Priestley even cites the final chapters of "Gulliver's Travels" as proof of Swift's approaching "insanity". Bewley attributes his decline to 'terminal dementia'.

In part VIII of his series, "The Story of Civilization", Will Durant describes the final years of Swift's life as such:

"Definite symptoms of madness appeared in 1738. In 1741, guardians were appointed to take care of his affairs and watch lest in his outbursts of violence he should do himself harm. In 1742, he suffered great pain from the inflammation of his left eye, which swelled to the size of an egg; five attendants had to restrain him from tearing out his eye. He went a whole year without uttering a word."

In 1744, Alexander Pope died. Then on 19 October 1745, Swift, at nearly 80, died. After being laid out in public view for the people of Dublin to pay their last respects, he was buried in his own cathedral by Esther Johnson's side, in accordance with his wishes. The bulk of his fortune (£12,000) was left to found a hospital for the mentally ill, originally known as St Patrick’s Hospital for Imbeciles, which opened in 1757, and which still exists as a psychiatric hospital.

Jonathan Swift wrote his own epitaph:
W. B. Yeats poetically translated it from the Latin as:

Swift was a prolific writer, notable for his satires. The most recent collection of his prose works (Herbert Davis, ed. Basil Blackwell, 1965–) comprises fourteen volumes. A recent edition of his complete poetry (Pat Rodges, ed. Penguin, 1983) is 953 pages long. One edition of his correspondence (David Woolley, ed. P. Lang, 1999) fills three volumes.

Swift's first major prose work, "A Tale of a Tub", demonstrates many of the themes and stylistic techniques he would employ in his later work. It is at once wildly playful and funny while being pointed and harshly critical of its targets. In its main thread, the "Tale" recounts the exploits of three sons, representing the main threads of Christianity, who receive a bequest from their father of a coat each, with the added instructions to make no alterations whatsoever. However, the sons soon find that their coats have fallen out of current fashion, and begin to look for loopholes in their father's will that will let them make the needed alterations. As each finds his own means of getting around their father's admonition, they struggle with each other for power and dominance. Inserted into this story, in alternating chapters, the narrator includes a series of whimsical "digressions" on various subjects.

In 1690, Sir William Temple, Swift's patron, published "An Essay upon Ancient and Modern Learning" a defence of classical writing (see Quarrel of the Ancients and the Moderns), holding up the "Epistles of Phalaris" as an example. William Wotton responded to Temple with "Reflections upon Ancient and Modern Learning" (1694), showing that the "Epistles" were a later forgery. A response by the supporters of the Ancients was then made by Charles Boyle (later the 4th Earl of Orrery and father of Swift's first biographer). A further retort on the Modern side came from Richard Bentley, one of the pre-eminent scholars of the day, in his essay "Dissertation upon the Epistles of Phalaris" (1699). The final words on the topic belong to Swift in his "Battle of the Books" (1697, published 1704) in which he makes a humorous defence on behalf of Temple and the cause of the Ancients.

In 1708, a cobbler named John Partridge published a popular almanac of astrological predictions. Because Partridge falsely determined the deaths of several church officials, Swift attacked Partridge in "Predictions for the Ensuing Year" by Isaac Bickerstaff, a parody predicting that Partridge would die on 29 March. Swift followed up with a pamphlet issued on 30 March claiming that Partridge had in fact died, which was widely believed despite Partridge's statements to the contrary. According to other sources, Richard Steele uses the personae of Isaac Bickerstaff and was the one who wrote about the "death" of John Partridge and published it in "The Spectator," not Jonathan Swift.

The "Drapier's Letters" (1724) was a series of pamphlets against the monopoly granted by the English government to William Wood to mint copper coinage for Ireland. It was widely believed that Wood would need to flood Ireland with debased coinage in order to make a profit. In these "letters" Swift posed as a shop-keeper—a draper—to criticise the plan. Swift's writing was so effective in undermining opinion in the project that a reward was offered by the government to anyone disclosing the true identity of the author. Though hardly a secret (on returning to Dublin after one of his trips to England, Swift was greeted with a banner, "Welcome Home, Drapier") no one turned Swift in, although there was an unsuccessful attempt to prosecute the publisher Harding. Thanks to the general outcry against the coinage, Wood's patent was recinded in September 1725 and the coins were kept out of circulation. In "Verses on the Death of Dr. Swift" (1739) Swift recalled this as one of his best achievements.

"Gulliver's Travels", a large portion of which Swift wrote at Woodbrook House in County Laois, was published in 1726. It is regarded as his masterpiece. As with his other writings, the "Travels" was published under a pseudonym, the fictional Lemuel Gulliver, a ship's surgeon and later a sea captain. Some of the correspondence between printer Benj. Motte and Gulliver's also-fictional cousin negotiating the book's publication has survived. Though it has often been mistakenly thought of and published in bowdlerised form as a children's book, it is a great and sophisticated satire of human nature based on Swift's experience of his times. "Gulliver's Travels" is an anatomy of human nature, a sardonic looking-glass, often criticised for its apparent misanthropy. It asks its readers to refute it, to deny that it has adequately characterised human nature and society. Each of the four books—recounting four voyages to mostly fictional exotic lands—has a different theme, but all are attempts to deflate human pride. Critics hail the work as a satiric reflection on the shortcomings of Enlightenment thought.

In 1729, Swift published "A Modest Proposal for Preventing the Children of Poor People in Ireland Being a Burden on Their Parents or Country, and for Making Them Beneficial to the Publick", a satire in which the narrator, with intentionally grotesque arguments, recommends that Ireland's poor escape their poverty by selling their children as food to the rich: "I have been assured by a very knowing American of my acquaintance in London, that a young healthy child well nursed is at a year old a most delicious nourishing and wholesome food..." Following the satirical form, he introduces the reforms he is actually suggesting by deriding them:
Therefore let no man talk to me of other expedients...taxing our absentees...using [nothing] except what is of our own growth and manufacture...rejecting...foreign luxury...introducing a vein of parsimony, prudence and temperance...learning to love our country...quitting our animosities and factions...teaching landlords to have at least one degree of mercy towards their tenants...Therefore I repeat, let no man talk to me of these and the like expedients, 'till he hath at least some glympse of hope, that there will ever be some hearty and sincere attempt to put them into practice.






John Ruskin named him as one of the three people in history who were the most influential for him.

George Orwell named him as one of the writers he most admired, despite disagreeing with him on almost every moral and political issue. Modernist poet Edith Sitwell wrote a fictional biography of Swift, titled "I Live Under a Black Sun" and published in 1937.

Swift crater, a crater on Mars's moon Deimos, is named after Jonathan Swift, who predicted the existence of the moons of Mars.

In honour of Swift's long-time residence in Trim, there are several monuments in the town marking his legacy. Most notable is Swift's Street, named after him. Trim also holds a recurring festival in honour of Swift, called the 'Trim Swift Festival'.

Jake Arnott features him in his 2017 novel The Fatal Tree.

A 2017 analysis of library holdings data revealed that Swift is the most popular Irish author, and that "Gulliver’s Travels" is the most widely held work of Irish literature in libraries globally.



Online works


</doc>
<doc id="15616" url="https://en.wikipedia.org/wiki?curid=15616" title="Jello Biafra">
Jello Biafra

Eric Reed Boucher (born June 17, 1958), better known by his stage name Jello Biafra, is the former lead singer and songwriter for the San Francisco punk rock band Dead Kennedys. He is active as both a musician and spoken word artist.

Initially active from 1979 to 1986, Dead Kennedys were known for rapid-fire music topped with Biafra's sardonic lyrics and biting social commentary, delivered in his "unique quiver of a voice." When the band broke up in 1986, he took over the influential independent record label Alternative Tentacles, which he had co-founded in 1979 with Dead Kennedys bandmate East Bay Ray. In a 2000 lawsuit, upheld on appeal in 2003 by the California Supreme Court, Biafra was found liable for breach of contract, fraud and malice in withholding a decade's worth of royalties from his former bandmates and ordered to pay over $200,000 in compensation and punitive damages; the band subsequently reformed without Biafra. Although now focused primarily on spoken word performances, Biafra has continued as a musician in numerous collaborations.

Politically, Biafra is a member of the Green Party of the United States and actively supports various political causes. He ran for the party's Presidential nomination in the 2000 United States Presidential Election, finishing a distant second to Ralph Nader. He is a staunch believer in a free society, and utilizes shock value and advocates direct action and pranksterism in the name of political causes. Biafra is known to use absurdist media tactics, in the leftist tradition of the Yippies, to highlight issues of civil rights and social justice.

Eric Reed Boucher was born in Boulder, Colorado, the son of Virginia (née Parker), a librarian, and Stanley Wayne Boucher, a psychiatric social worker and poet. He had a sister, Julie J. Boucher, the Associate Director of the Library Research Service at the Colorado State Library (who died in a mountain-climbing accident on October 12, 1996). Biafra is 1/8th Jewish.

As a child, Boucher developed an interest in international politics that was encouraged by his parents. An avid news watcher, one of his earliest memories was of the John F. Kennedy assassination. Biafra says he has been a fan of rock music since first hearing it in 1965, when his parents accidentally tuned in to a rock radio station. Boucher was informed by his high school guidance counselor that he should be spending his high school years preparing to become a dental hygienist.

He began his career in music in January 1977 as a roadie for the punk rock band The Ravers (who later changed their name to The Nails), soon joining his friend John Greenway in a band called The Healers. The Healers became infamous locally for their mainly improvised lyrics and avant garde music. In the autumn of that year, he began attending the University of California, Santa Cruz.

In June 1978, he responded to an advertisement placed in a store by guitarist East Bay Ray, stating; "guitarist wants to form punk band", and together they formed the Dead Kennedys. He began performing with the band under the stage name Occupant, but soon began to use his current stage name, a combination of the brand name Jell-O and the short-lived African state Biafra. The band's lyrics were written by Biafra. The lyrics were mostly political in nature and displayed a sardonic, sometimes absurdist, sense of humor despite their serious subject matter. In the tradition of UK anarcho-punk bands like Crass, the Dead Kennedys were one of the first US punk bands to write politically themed songs. The lyrics Biafra wrote helped popularize the use of humorous lyrics in punk and other types of hard-core music. Biafra cites Joey Ramone as the inspiration for his use of humor in his songs (as well as being the musician who made him interested in punk rock), noting in particular songs by The Ramones such as "Beat on the Brat" and "Now I Wanna Sniff Some Glue".

Biafra initially attempted to compose music on guitar, but his lack of experience on the instrument and his own admission of being "a fumbler with my hands" led Dead Kennedys bassist Klaus Flouride to suggest that Biafra simply sing the parts he envisioned to the band.<ref name="re/search">V. Vale, "Incredibly Strange Music, Vol. 2", RE/Search Publications, 1995</ref> Biafra sang his riffs and melodies into a tape recorder, which he brought to the band's rehearsal and/or recording sessions. This later became a problem when the other members of the Dead Kennedys sued Biafra over royalties and publishing rights. By all accounts, including his own, Biafra is not a conventionally skilled musician, though he and his collaborators (Joey Shithead of D.O.A. in particular) attest that he is a skilled composer and his work, particularly with the Dead Kennedys, is highly respected by punk-oriented critics and fans.
Biafra's first popular song was the first single by the Dead Kennedys, "California Über Alles". The song, which spoofed California governor Jerry Brown, was the first of many political songs by the group and Biafra. The song's popularity resulted in its being covered by other musicians, such as The Disposable Heroes of Hiphoprisy (who rewrote the lyrics to parody Pete Wilson), John Linnell of They Might Be Giants and Six Feet Under on their "Graveyard Classics" album of cover versions. Not long after, the Dead Kennedys had a second and bigger hit with "Holiday in Cambodia" from their debut album "Fresh Fruit for Rotting Vegetables". "AllMusic" cites this song as "possibly the most successful single of the American hardcore scene" and Biafra counts it as his personal favorite Dead Kennedy's song. Minor hits from the album included "Kill the Poor" (about potential abuse of the then-new neutron bomb) and a satirical cover of Elvis Presley's "Viva Las Vegas".

The Dead Kennedys received some controversy in the spring of 1981 over the single "Too Drunk to Fuck". The song became a hit in Britain, and the BBC feared that it would manage to be a big enough hit to appear among the top 30 songs on the national charts, requiring a mention on "Top of the Pops". However, the single peaked at number 31 in the charts.

Later albums also contained memorable songs, but with less popularity than the earlier ones. The EP "In God We Trust, Inc." contained the song "Nazi Punks Fuck Off!" as well as "We've Got A Bigger Problem Now", a rewritten version of "California Über Alles" about Ronald Reagan. Punk musician and scholar Vic Bondi considers the latter song to be the song that "defined the lyrical agenda of much of hardcore music, and represented its break with punk". The band's most controversial album, "Frankenchrist", brought with it the song "MTV Get Off the Air", which accused MTV of promoting poor quality music and sedating the public. The album also contained a controversial poster by Swiss surrealist artist H. R. Giger entitled "Penis Landscape".

The Dead Kennedys toured widely during their career, starting in the late 1970s. They began playing at San Francisco's Mabuhay Gardens (their home base) and other Bay Area venues, later branching out to shows in southern Californian clubs (most notably the Whisky a Go Go), but eventually they moved to major clubs across the country, including CBGB in New York. Later, they played to larger audiences such as at the 1980 Bay Area Music Awards (where they played the notorious "Pull My Strings" for the only time), and headlined the 1983 Rock Against Reagan festival.

On May 7, 1994, punk rock fans who believed Biafra was a "sell out" attacked him at the 924 Gilman Street club in Berkeley, California. Biafra claims that he was attacked by a man nicknamed Cretin, who crashed into him while moshing. The crash injured Biafra's leg, causing an argument between the two men. During the argument, Cretin pushed Biafra to the floor and five or six friends of Cretin assaulted Biafra while he was down, yelling "Sellout rock star, kick him", and attempting to pull out his hair. Biafra was later hospitalized with serious injuries. The attack derailed Biafra's plans for both a Canadian spoken-word tour and an accompanying album, and the production of "Pure Chewing Satisfaction" was halted. However, Biafra returned to the Gilman club a few months after the incident to perform a spoken-word performance as an act of reconciliation with the club.

Biafra has been a prominent figure of the Californian punk scene and was one of the third generation members of the San Francisco punk community. Many later hardcore bands have cited the Dead Kennedys as a major influence. Hardcore punk author Steven Blush describes Biafra as hardcore's "biggest star" who was a "powerful presence whose political insurgence and rabid fandom made him the father figure of a burgeoning subculture [and an] inspirational force [who] could also be a real prick... Biafra was a visionary, incendiary [performer]."

After the Dead Kennedys disbanded, Biafra's new songs were recorded with other bands, and he released only spoken word albums as solo projects. These collaborations had less popularity than Biafra's earlier work. However, his song "That's Progress", originally recorded with D.O.A. for the album "Last Scream of the Missing Neighbors", received considerable exposure when it appeared on the album "Rock Against Bush, Vol. 1".

In April 1986, police officers raided his house in response to complaints by the Parents Music Resource Center (PMRC). In June 1986, L.A. deputy city attorney Michael Guarino, working under City Attorney James Hahn, brought Biafra to trial in Los Angeles for distributing "harmful material to minors" in the Dead Kennedys album "Frankenchrist". However, the dispute was about neither the music nor the lyrics from the album, but rather the print of the H. R. Giger poster "Landscape XX" ("Penis Landscape") included with the album. Biafra believes the trial was politically motivated; it was often reported that the PMRC took Biafra to court as a cost-effective way of sending a message out to other musicians with content considered offensive in their music.

Music author Reebee Garofalo argued that Biafra and Alternative Tentacles may have been targeted because the label was a "small, self-managed and self-supported company that could ill afford a protracted legal battle." Facing the possible sentence of a year in jail and a $2,000 fine, Biafra, Dirk Dirksen, and Suzanne Stefanac founded the No More Censorship Defense Fund, a benefit featuring several punk rock bands, to help pay for his legal fees, which neither he nor his record label could afford. The jury deadlocked 5 to 7 in favor of acquittal, prompting a mistrial; despite a motion to re-try the case, the judge ordered all charges dropped. The Dead Kennedys disbanded during the trial, in December 1986, due to the mounting legal costs; in the wake of their disbandment, Biafra made a career of his spoken word performances.

Biafra has a cameo role in the 1988 film "Tapeheads". He plays an FBI agent who arrests the two protagonists (played by Tim Robbins and John Cusack). While arresting them his character asks "Remember what we did to Jello Biafra?" lampooning the obscenity prosecution.

On March 25, 2005, Biafra appeared on the U.S. radio program "This American Life", "Episode 285: Know Your Enemy", which featured a phone call between Jello Biafra and Michael Guarino, the prosecutor in the "Frankenchrist" trial.

In October 1998, three former members of the Dead Kennedys sued Biafra for nonpayment of royalties. The other members of Dead Kennedys alleged that Biafra, in his capacity as the head of Alternative Tentacles records, discovered an accounting error amounting to some $75,000 in unpaid royalties over almost a decade. Rather than informing his bandmates of this mistake, the suit alleged, Biafra knowingly concealed the information until a whistleblower employee at the record label notified the band.

According to Biafra, the suit resulted from his refusal to allow one of the band's most well-known singles, "Holiday in Cambodia", to be used in a commercial for Levi's Dockers; Biafra opposes Levi's because of his claim that they use unfair business practices and sweatshop labor. Biafra maintained that he had never denied them royalties, and that he himself had not even received royalties for re-releases of their albums or "posthumous" live albums which had been licensed to other labels by the Decay Music partnership. Decay Music denied this charge and have posted what they say are his cashed royalty checks, written to his legal name of Eric Boucher. Biafra also complained about the songwriting credits in new reissues and archival live albums of songs, alleging that he was the sole composer of songs that were wrongly credited to the entire band.

In May 2000, a jury found Biafra and Alternative Tentacles "guilty of malice, oppression and fraud" by not promptly informing his former bandmates of the accounting error and instead withholding the information during subsequent discussions and contractual negotiations. Biafra was ordered to pay $200,000, including $20,000 in punitive damages. After an appeal by Biafra's lawyers, in June 2003, the California Court of Appeal unanimously upheld all the conditions of the 2000 verdict against Biafra and Alternative Tentacles. Furthermore, the plaintiffs were awarded the rights to most of Dead Kennedys recorded works—which accounted for about half the sales for Alternative Tentacles. Now in control of the Dead Kennedys name, Biafra's former bandmates went on tour with a new lead vocalist.

In the early 1980s, Biafra collaborated with musicians Christian Lunch and Adrian Borland (of The Sound) for the electropunk musical project The Witch Trials, releasing one self-titled EP in its lifetime.

In 1988, Biafra, with Al Jourgensen and Paul Barker of the band Ministry, and Jeff Ward, formed Lard. The band became yet another side project for Ministry, with Biafra providing vocals and lyrics. According to a March 2009 interview with Jourgensen, he and Biafra are working on a new Lard album, which is being recorded in Jourgensen's El Paso studio. While working on the film "Terminal City Ricochet" in 1989, Biafra did a song for the film's soundtrack with D.O.A.. As a result, Biafra worked with D.O.A. on the album "Last Scream of the Missing Neighbors". Biafra also worked with Nomeansno on the soundtrack, which led to their collaboration on the album "The Sky Is Falling and I Want My Mommy" the following year. Biafra also provided lyrics for the song "Biotech is Godzilla" for Sepultura's 1993 album "Chaos A.D.".

In 1999, Biafra and other members of the anti-globalization movement protested the WTO Meeting of 1999 in Seattle. Along with other prominent West Coast musicians, he formed the short-lived band the No WTO Combo to help promote the movement's cause. The band was originally scheduled to play during the protest, but the performance was canceled due to riots. The band performed a short set the following night at the Showbox in downtown Seattle (outside the designated area), along with the hiphop group Spearhead. No WTO Combo later released a CD of recordings from the concert, entitled "Live from the Battle in Seattle".

As of late 2005, Biafra was performing with the band The Melvins under the name "Jello Biafra and the Melvins", though fans sometimes refer to them as "The Jelvins." Together they have released two albums, and have been working on material for a third collaborative release, much of which was premiered live at two concerts at the Great American Music Hall in San Francisco during an event called Biafra Five-O, commemorating Biafra's 50th birthday, the 30th anniversary of the founding of the Dead Kennedys, and the beginning of legalized same-sex marriage in California. Biafra is also working with a new band known as Jello Biafra and the Guantanamo School of Medicine, which includes Ralph Spight of Victims Family on guitar and Billy Gould of Faith No More on bass. This group debuted during Biafra Five-O.

In 2011, Biafra appeared in a singular concert event with an all-star cast of Southern musicians including members from Cowboy Mouth, Dash Rip Rock, Mojo Nixon and Down entitled, "Jello Biafra and the New Orleans Raunch & Soul All Stars" who performed an array of classic Soul covers to a packed house at the 12-Bar in New Orleans, Louisiana. He would later reunite with many of the same musicians during the Carnival season 2014 to revisit many of these classics at Siberia, New Orleans. A live album from the 2011 performance, "Walk on Jindal's Splinters", and a companion single, "Fannie May"/"Just a Little Bit", were released in 2015.

In June 1979, Biafra co-founded the record label Alternative Tentacles, with which the Dead Kennedys released their first single, "California Über Alles". The label was created to allow the band to release albums without having to deal with pressure from major labels to change their music, although the major labels were not willing to sign the band due to their songs being deemed too controversial. After dealing with Cherry Red in the UK and IRS Records in the US for their first album "Fresh Fruit for Rotting Vegetables", the band released all later albums, and later pressings of "Fresh Fruit" on Alternative Tentacles. The exception being live albums released after the band's break-up, which the other band members compiled from recordings in the band partnership's vaults without Biafra's input or endorsement.. Biafra has been the owner of the company since its founding, though he does not receive a salary for his position; Biafra has referred to his position in the company as "absentee thoughtlord".

Biafra is an ardent collector of unusual vinyl records of all kinds, from 1950s and 1960s ethno-pop recordings by the likes of Les Baxter and Esquivel to vanity pressings that have circulated regionally, to German crooner Heino (for whom he would later participate in the documentary "Heino: Made In Germany"); he cites his always growing collection as one of his biggest musical influences. In 1993 he gave an interview to RE/Search Publications for their second "Incredibly Strange Music" book focusing primarily on these records, and later participated in a two-part episode of Fuse TV's program "Crate Diggers" on the same subject. His interest in such recordings, often categorized as outsider music, led to his discovery of the prolific (and schizophrenic) singer/songwriter/artist Wesley Willis, whom he signed to Alternative Tentacles in 1994, preceding Willis' major label deal with American Recordings. His collection grew so large that on October 1, 2005, Biafra donated a portion of his collection to an annual yard sale co-promoted by Alternative Tentacles and held at their warehouse in Emeryville, California.

In 2006, along with Alternative Tentacles employee and The Frisk lead singer Jesse Luscious, Biafra began co-hosting "The Alternative Tentacles Batcast", a downloadable podcast hosted by alternativetentacles.com. The show primarily focuses on interviews with artists and bands that are currently signed to the Alternative Tentacles label, although there are also occasional episodes where Biafra devoted the show to answering fan questions.

Biafra became a spoken word artist in January 1986 with a performance at University of California, Los Angeles. In his performance he combined humor with his political beliefs, much in the same way that he did with the lyrics to his songs. Despite his continued spoken word performances, he did not begin recording spoken word albums until after the disbanding of the Dead Kennedys.

His ninth spoken word album, "In the Grip of Official Treason", was released in October 2006.

Biafra was also featured in the British band Pitchshifter's song "As Seen on TV" reciting the words of dystopian futuristic radio advertisements.

Biafra was an anarchist in the 1980s, but has shifted away from his former anti-government views. In a 2012 interview, Biafra said "I'm very pro-tax as long as it goes for the right things. I don't mind paying more money as long as it's going to provide shelter for people sleeping in the street or getting the schools fixed back up, getting the infrastructure up to the standards of other countries, including a high speed rail system. I'm totally down with that."

In the autumn of 1979, Biafra ran for mayor of San Francisco, using the Jell-O ad campaign catchphrase, "There's always room for Jello", as his campaign slogan. Having entered the race before creating a campaign platform, Biafra later wrote his platform on a napkin while attending a Pere Ubu concert where Dead Kennedys drummer Ted told Biafra, "Biafra, you have such a big mouth that you should run for Mayor." As he campaigned, Biafra wore campaign T-shirts from his opponent Quentin Kopp's previous campaign and at one point vacuumed leaves off the front lawn of another opponent, current U.S. Senator Dianne Feinstein, to mock her publicity stunt of sweeping streets in downtown San Francisco for a few hours. He also made a whistlestop campaign tour along the BART line. Supporters committed equally odd actions; two well known signs held by supporters said "If he doesn't win I'll kill myself" and "What if he does win?"

In San Francisco any individual could legally run for mayor if a petition was signed by 1500 people or if $1500 was paid. Biafra paid $900 and got signatures over time and eventually became a legal candidate, meaning he received statements put in voters' pamphlets and equal news coverage.

His platform included unconventional points such as forcing businessmen to wear clown suits within city limits, erecting statues of Dan White, who assassinated Mayor George Moscone and City Supervisor Harvey Milk in 1978, around the city and allowing the parks department to sell eggs and tomatoes with which people could pelt the statues, hiring workers who'd lost their jobs due to a tax initiative to panhandle in wealthy neighborhoods (including Dianne Feinstein's), and a citywide ban on cars. Biafra has expressed irritation that these parts of his platform attained such notoriety, preferring instead to be remembered for serious proposals such as legalizing squatting in vacant, tax-delinquent buildings and requiring police officers to run for election by the people of the neighborhoods they patrol.

He finished third out of a field of ten, receiving 3.79% of the vote (6,591 votes); the election ended in a runoff that did not involve him (Feinstein was declared the winner).

In 2000, the New York State Green Party drafted Biafra as a candidate for the Green Party presidential nomination, and a few supporters were elected to the party's nominating convention in Denver, Colorado. Biafra chose death row inmate Mumia Abu-Jamal as his running mate. The party overwhelmingly chose Ralph Nader as the presidential candidate with 295 of the 319 delegate votes. Biafra received 10 votes.

Biafra, along with a camera crew (dubbed by Biafra as "The Camcorder Truth Jihad"), later reported for the Independent Media Center at the Republican and Democratic conventions.

After losing the 2000 nomination, Jello became highly active in Ralph Nader's presidential campaign, as well as in 2004 and 2008. During the 2008 campaign Jello played at rallies and answered questions for journalists in support of Ralph Nader. When gay rights activists accused Nader of costing Al Gore the 2000 election, Biafra reminded them that Tipper Gore's Parents Music Resource Center wanted warning stickers on albums with homosexual content.

After Barack Obama won the general election, Jello wrote an open letter making suggestions on how to run his term as president. Biafra criticized Obama during his term, stating that "Obama even won the award for best advertising campaign of 2008." Biafra dubbed Obama "Barackstar O'Bummer". Biafra refused to support Obama in 2012. Biafra has stated that he feels that Obama continued many of George W. Bush's policies, summarizing Obama's policies as containing "worse and worse laws against human rights and more and more illegal unconstitutional spying."

On September 18, 2015, it was announced that Jello would be supporting Bernie Sanders in his campaign for the 2016 presidential election. He has strongly criticised the political position of Donald Trump, saying "how can people be so fucking stupid" on hearing the election result, and later adding "The last person we want with their finger on the nuclear button is somebody connected to this extreme Christianist doomsday cult."

In the summer of 2011 Jello Biafra and his band were scheduled to play at the Barby Club in Tel Aviv. They came under heavy pressure by the pro-Palestinian Boycott, Divestment and Sanctions (BDS) campaign, and finally decided to cancel the gig – after a debate which according to Biafra "deeply tore at the fabric of our band [. . .] This whole controversy has been one of the most intense situations of my life – and I thrive on intense situations".
Biafra then decided to travel to Israel and the Palestinian Occupied Territories, at his own expense, and talk with Israeli and Palestinian activists as well as with fans disappointed at his cancellation. In the article stating his conclusions he wrote:
"I will not perform in Israel unless it is a pro-human rights, anti-occupation event, that does not violate the spirit of the boycott. Each musician, artist, etc. must decide this for themselves. I am staying away for now, but am also really creeped out by the attitudes of some of the hardliners and hope some day to find a way to contribute something positive here. I will not march or sign on with anyone who runs around calling people Zionazis and is more interested in making threats than making friends."

Biafra married Theresa Soder, a.k.a. Ninotchka, lead singer of San Francisco-area punk band the Situations, on October 31, 1981. The wedding was conducted by Flipper vocalist/bassist Bruce Loose, who became a Universal Life Church minister just to conduct the ceremony, which took place in a graveyard. The wedding reception, which members of Flipper, Black Flag, and D.O.A. attended, was held at director Joe Rees' Target Video studios. The marriage ended in 1986.

Biafra lives in San Francisco, California.

"For a more complete list, see the Jello Biafra discography."








</doc>
<doc id="15621" url="https://en.wikipedia.org/wiki?curid=15621" title="John Grierson">
John Grierson

John Grierson CBE (26 April 1898 – 19 February 1972) was a pioneering Scottish documentary maker, often considered the father of British and Canadian documentary film. In 1926, Grierson coined the term "documentary" in a review of Robert Flaherty's "Moana".

Grierson was born in the old schoolhouse in Deanston, near Doune, Scotland, to his father a schoolmaster Robert Morrison Grierson from Boddam, near Peterhead and mother Jane Anthony a teacher from Ayrshire. His mother a suffragette and ardent Labour Party activist, she often took the chair at Tom Johnston's election meetings. 

The family moved to Cambusbarron, Stirling in 1900, when the children were still young after Grierson's father was appointed headmaster of Cambusbarron school. When the family moved, John had three elder sisters Agnes, Janet and Margaret, and a younger brother called Anthony. John and Anthony were enrolled at Cambusbarron school in November 1903, his sister Margaret died in 1906; however, the family continued to grow as John gained three younger sisters in Dorothy, Ruby and Marion. 

From an early age, both parents steeped their son in liberal politics, humanistic ideals, and Calvinist moral and religious philosophies, particularly the notion that education was essential to individual freedom and that hard and meaningful work was the way to prove oneself worthy in the sight of God. John was enrolled in the High School at Stirling in September 1908, where he played football and rugby for the school. 

In July 1915, he left school with an overall subject mark of 82%; John had sat the bursary examination at Gilmorehill the month before as his parents wanted him to follow his elder sisters, Janet and Agnes in going to the University of Glasgow. The results for the bursary examination were not posted until October 1915; John applied to work at the munitions at Alexandria, the munitions building had been the original home of the Argyll Motor Company which had earlier in the twentieth century built the first complete motor car in Scotland. 

John Grierson was the second name on the bursary list and received the John Clark bursary which was tenable for four years. Grierson entered the University of Glasgow in 1916; however, he was unhappy with his efforts to help in World War I were only through his work at the munitions. John wanted to join the navy, his family on his father's side had long been lighthouse keepers, and John had many memories of visiting lighthouses and being beside the sea. John went to the Crystal Palace, London to train with the Royal Naval Volunteer Reserve, in his recruitment letter he had added an extra year so that he could attend. 

On 7 January 1916, John was sent to the wireless telegraphy station at Aultbea, Cromarty as an ordinary telegraphist but was promoted to telegraphist on 2 June 1916. On 23 January 1917, he became a telegraphist on the minesweeper H.M.S "Surf" and served there until 13 October 1917, the next day he joined H.M.S "Rightwhale" where he was promoted to leading telegraphist on 2 June 1918, and remained on the vessel until he was demobilised. John walked away from his time in the navy with a British War Medal and the Victory Medal. 

John returned to University in 1919, he joined the Fabian Society in 1919 and dissolved it in 1921. The New University Labour Club was initiated by John as well as the Critic's Club; he also had poetry published in the Glasgow University magazine from November 1920 until February 1923. Grierson received the Buchan Prize in the Ordinary Class of English Language in the academic year of 1919-20, he also received the prize and first-class certificate in the academic year of 1920-21 in the Ordinary Class of Moral Philosophy and graduated with a Master of Arts in English and Moral Philosophy in 1923. 

In 1923, he received a Rockefeller Research Fellowship to study in the United States at the University of Chicago, and later at Columbia and the University of Wisconsin–Madison. His research focus was the psychology of propaganda—the impact of the press, film, and other mass media on forming public opinion. Grierson was particularly interested in the popular appeal and influence of the "yellow" (tabloid) press, and the influence and role of these journals on the education of new American citizens from abroad.

In his review of Robert Flaherty's film "Moana" (1926) in the "New York Sun" (8 February 1926), Grierson wrote that it had 'documentary' value.
In his essay "First Principles of Documentary" (1932), Grierson argued that the principles of documentary were that cinema's potential for observing life could be exploited in a new art form; that the "original" actor and "original" scene are better guides than their fiction counterparts to interpreting the modern world; and that materials "thus taken from the raw" can be more real than the acted article. In this regard, Grierson's views align with the Soviet filmmaker Dziga Vertov's contempt for dramatic fiction as "bourgeois excess", though with considerably more subtlety. Grierson's definition of documentary as "creative treatment of actuality" has gained some acceptance, though it presents philosophical questions about documentaries containing stagings and reenactments.

Like many social critics of the time, Grierson was profoundly concerned about what he perceived to be clear threats to democracy. In the US, he encountered a marked tendency toward political reaction, anti-democratic sentiments, and political apathy. He read and agreed with the journalist and political philosopher Walter Lippmann's book "Public Opinion" which blamed the erosion of democracy in part on the fact that the political and social complexities of contemporary society made it difficult if not impossible for the public to comprehend and respond to issues vital to the maintenance of democratic society.

In Grierson's view, a way to counter these problems was to involve citizens in their government with the kind of engaging excitement generated by the popular press, which simplified and dramatized public affairs. It was during this time that Grierson developed a conviction that motion pictures could play a central role in promoting this process. (It has been suggested [by whom?] that some of Grierson's notions regarding the social and political uses of film were influenced by reading Lenin's writing about film as education and propaganda.)

Grierson's emerging view of film was as a form of social and political communication—a mechanism for social reform, education, and perhaps spiritual uplift. His view of Hollywood movie-making was considerably less sanguine:

Grierson's emerging and outspoken film philosophies caught the attention of New York film critics at the time. He was asked to write criticism for the "New York Sun". At the "Sun", Grierson wrote articles on film aesthetics and audience reception, and developed broad contacts in the film world. According to popular myth, in the course of this writing stint, Grierson coined the term "documentary" in writing about Robert J. Flaherty's film "Moana" (1926): "Of course "Moana", being a visual account of events in the daily life of a Polynesian youth and his family, has documentary value."

During this time, Grierson was also involved in scrutinizing the film industries of other countries. He may have been involved in arranging to bring Sergei Eisenstein's groundbreaking film "The Battleship Potemkin" (1925) to US audiences for the first time. Eisenstein's editing techniques and film theories, particularly the use of montage, would have a significant influence on Grierson's own work.

Grierson returned to Great Britain in 1927 armed with the sense that film could be enlisted to deal with the problems of the Great Depression, and to build national morale and national consensus. Filmmaking for Grierson was an exalted calling; the Filmmaker a patriot. In all of this, there was more than a little elitism, a stance reflected in Grierson's many dicta of the time: "The elect have their duty." "I look on cinema as a pulpit, and use it as a propagandist."

In the US Grierson had met pioneering documentary filmmaker Robert Flaherty. Grierson respected Flaherty immensely for his contributions to documentary form and his attempts to use the camera to bring alive the lives of everyday people and everyday events. Less commendable in Grierson's view was Flaherty's focus on exotic and faraway cultures. ("In the profounder kind of way", wrote Grierson of Flaherty, "we live and prosper each of us by denouncing the other"). In Grierson's view, the focus of film should be on the everyday drama of ordinary people. As Grierson wrote in his diaries: "Beware the ends of the earth and the exotic: the drama is on your doorstep wherever the slums; are, wherever there is malnutrition, wherever there is exploitation and cruelty." "'You keep your savages in the far place Bob; we are going after the savages of Birmingham,' I think I said to him pretty early on. And we did.")

On his return to England, Grierson was employed on a temporary basis as an Assistant Films Officer of the Empire Marketing Board (EMB), a governmental agency which had been established in 1926 to promote British world trade and British unity throughout the empire. One of the major functions of the EMB was publicity, which the Board accomplished through exhibits, posters, and publications and films. It was within the context of this State-funded organization that the "documentary" as we know it today got its start.

In late 1929 Grierson and his cameraman, Basil Emmott completed his first film, "Drifters", which he wrote, produced and directed. The film, which follows the heroic work of North Sea herring fishermen, was a radical departure from anything being made by the British film industry or Hollywood. A large part of its innovation lies in the fierce boldness in bringing the camera to rugged locations such as a small boat in the middle of a gale while leaving relatively less of the action staged. The choice of topic was chosen less from Grierson's curiosity than the fact that he discovered that the Financial Secretary had made the herring industry his hobbyhorse. It premiered in a private film club in London in November 1929 on a double-bill with Eisenstein's -then controversial- film "The Battleship Potemkin" (which was banned from general release in Britain until 1954) and received high praise from both its sponsors and the press. The film was shown from 9 December 1929, in the Stoll in Kingsway and then was later screened throughout Britain. 

After this success, Grierson moved away from film direction into a greater focus on production and administration within the EMB. He became a tireless organizer and recruiter for the EMB, enlisting a stable of energetic young filmmakers into the film unit between 1930 and 1933. Those enlisted included filmmakers Basil Wright, Edgar Anstey, Stuart Legg, Paul Rotha, Arthur Elton, Humphrey Jennings, Harry Watt, and Alberto Cavalcanti. This group formed the core of what was to become known as the British Documentary Film Movement. Robert Flaherty himself also worked briefly for the unit. In 1933 the EMB Film Unit was disbanded, a casualty of Depression-era economics.

Grierson's boss at the EMB moved to the General Post Office (GPO) as its first public relations officer, with the stipulation that he could bring the EMB film unit with him. Grierson's crew were charged with demonstrating how the Post Office facilitated modern communication and brought the nation together, a task aimed as much at GPO workers as the general public. During Grierson's administration, the GPO Film Unit produced a series of groundbreaking films, including "Night Mail" (dir. Basil Wright and Harry Watt, 1936) and "Coal Face" (dir. Alberto Cavalcanti, 1935). In 1934 he produced at the GPO Film Unit the award-winning "The Song of Ceylon" (dir. Basil Wright) which was sponsored jointly by the Ceylon Tea Propaganda Bureau and the EMB. 

In 1934, Grierson sailed on the "Isabella Greig" out of Granton to film "Granton Trawler" on Viking Bank which is between Shetland and the Norwegian coast. The footage from his voyage was handed over to Edgar Anstey, who pulled footage of when the camera had fallen over on the deck of the boat to create a storm scene. "Granton Trawler" was a favourite film of Grierson's, he saw it as a homage to the "Isabella Greig" that was sunk in 1941 by German bombs when it went out to fish and was never seen again. "The Private Life of Gannets" was also filmed on the "Isabella Greig;" the film was shot on Grassholm with Grierson shooting the slow-motion sequence of the gannets diving for fish which took only one afternoon to shoot near Bass Rock in the Firth of Forth. The Private Life of Gannets went on to pick up an Academy Award in 1937.

Grierson eventually grew restless with having to work within the bureaucratic and budgetary confines of government sponsorship. Grierson resigned from the G.P.O. on 30 June 1937, which gave him more time to pursue his passions and the freedom to speak his mind on issues around the world. In response, he sought out private industry sponsorship for film production. He was finally successful in getting the British gas industry to underwrite an annual film program. Perhaps the most significant works produced during this time were "Housing Problems" (dir. Arthur Elton, Edgar Anstey, John Taylor, and Grierson's sister Ruby Grierson, 1935). 

In 1938, Grierson was invited by the Canadian government to study the country's film production. Grierson sailed at the end of May in 1938 for Canada and arrived on 17 June. Grierson met with the Prime Minister, William Lyon Mackenzie King and also spoke with many important figures across Canada, they were all in agreement of the importance of film in reducing sectionalism and in promoting the relationship of Canada between home and abroad. The head of the Motion Picture Bureau for Canada, Frank Bagdley, did not appreciate Grierson's assessment and criticism of the films made by the Bureau which was that they focused too much on Canada as a place to holiday. Grierson delivered his report on government film propaganda and the weaknesses he had found in Canadian film production; his suggestion was to create a national coordinating body for the production of films. An abridged version of the report ran to 66 pages, which was prepared by August in London. Grierson returned to Britain but was invited back to Canada on 14 October 1938; he returned in the November.

In 1939, Canada created the National Film Commission, which would later become the National Film Board of Canada. The bill to create a National Film Board was drafted by Grierson; the bill was introduced in March 1939 and given Royal Assent on 2 May 1939. Grierson was appointed the first Commissioner of the National Film Board in October 1939. When Canada entered World War II in 1939, the NFB focused on the production of propaganda films, many of which Grierson directed. For example, captured footage of German war activity was incorporated in documentaries that were distributed to the then-neutral United States.

Grierson grieved the death of his sister Ruby in 1940; she was on "City of Benares" while it was evacuating children to Canada. "The City of Benares" was torpedoed, and of the 406 on board, only 248 survived. Grierson resigned from his position in January 1941, over his year as Commissioner at the National Film Board 40 films were made, the year before the Motion Picture Bureau had made only one and a half. Recommendations for the future running were made for the National Film Board, and Grierson was persuaded to stay for a further six months to oversee the changes.

During WW II, Grierson was a consultant to prime minister William Lyon Mackenzie King as a minister of the Wartime Information Board. He remained on the National Film Board and managed to complete his duties to Wartime Information Board as well through his deputies that aided him in the task. Grierson was asked to keep his dual role until January 1944, however, he resigned in 1943 as the job he had been asked to complete had been finished as far as he was concerned. Before he finished with the Wartime Information Bureau Grierson was also offered the role of chairman of the Canadian Broadcasting Corporation but turned it down as he believed that this would give him too much power.

On 26 February 1942, Grierson attended the Academy Awards and received the award on behalf of the National Film Board for "Churchill's Island". Grierson also presented the award for the best documentary, the first time that this award was given by the Academy. After the Dieppe Raid, there were reports that Canadians that had been taken as prisoners of war had been manacled under Hitler's orders. Grierson proposed that the Film Board show how the German prisoners of war were being treated in Canada through a film. Ham Wright directed the film showing the German sailors that had been captured; plating football, enjoying meals and looking healthy. Only one copy of the film was made, it was sent to the Swiss Red Cross who deliberately let it fall into German hands. Grierson was to learn at a later date that Hitler had indeed watched the film and ordered that the Canadian prisoners of war released from their manacles.

After the war, the National Film Board focused on producing documentaries that reflected the lives of Canadians. The National Film Board has become recognized around the world for producing quality films, some of which have won Academy Awards. The National Film Board had become one of the largest film studios and was respected around the world for what it had achieved; it had especially had influence in Czechoslovakia and China.

In December 1943 Grierson was elected by the Permanent Film Committee of the National Council for Canadian-Soviet Friendship to become honorary chairman. One of the tasks at the National Film Board that Grierson strongly pushed for the films being produced to be in French as well as English. He also pushed for a French unit in the National Film Board.

Grierson concentrated on documentary film production in New York after resigning his post following in August 1945; his resignation was to take effect in November 1945. In 1946 Grierson was asked to testify regarding communist spies in the National Film Board and the Wartime Information Board, rumours spread that he had been a leader of a spy ring during his offices with the Canadian government, a rumour he denied. Due to the rumours, the projects that Grierson had been trying to put together were not commissioned.

Grierson was appointed as a foreign adviser to the Commission on Freedom of the Press in December 1943, which had been set up by the University of Chicago. Grierson was able to make a large contribution to the committee which included Robert M. Hutchins, William E. Hocking, Harold D. Lasswell, Archibald McLeish and Charles Merriam. "A Free and Responsible Press" was published in 1947. 

Grierson was offered the position of head of information at UNESCO at the end of 1946; he attended the first General Conference of UNESCO from 26 November until 10 December in Paris. He had the idea for the "Unesco Courier" which was published in published in several languages across the world, first as a tabloid and later as a magazine. Grierson was invited to open the Edinburgh International Film Festival in 1947, from 31 August to 7 September. At the start of 1948 he resigned from his position as director for Mass Communications and Public Information, he left in April to return to Britain.

In February 1948, Grierson was appointed the controller of the Central Office of Information's film operations to co-ordinate the work of the Crown Film Unit and Films Division, and to take overall charge of the planning, production and distribution of government films. On 23 June 1948, he accepted an honorary degree, an LL.D from the University of Glasgow. He left in 1950 due to financial restrictions on the documentaries that he wished to make.

Grierson was appointed to the position of executive producer of Group 3 at the end of 1950; it was a film production enterprise that received loans of government money through the National Film Finance Corporation. They filmed at Southall Studios in West London but later moved to Beaconsfield Studios. Group 3 was to have continuous production from 1951 until 1955 when it stopped producing films, the organisation had made a loss of over £400,000 as production of the films usually ran over the time allocated, and there had also been difficulty getting the films shown in cinemas. 

During this time Grierson had been diagnosed with tuberculosis in May 1953, he spent a fortnight in hospital and then had a year of convalescing at his home, Tog Hill in Calstone. Grierson spent much of his time corresponding with the directors at Group 3, as well as commenting on scripts and story ideas. He had recovered enough to attend the Cannes Film Festival in April 1954, taking the production of "Man of Africa". At the Edinburgh Film Festival in the same year, a dinner was held in Grierson's honour to celebrate twenty-five years of documentary. 

Grierson joined the newly revived Films of Scotland Committee in 1955, also on the committee was Norman Wilson, Forsyth Hardy, George Singleton, C. A. Oakley and Neil Paterson. In 1956, Grierson was the president of the Venice Film Festival's jury; he was also jury president at the Cork Film Festival and the South American Film Festival in 1958. In 1957, Grierson received a special Canadian Film Award. Grierson wrote the script for, "Seawards the Great Ship," it was directed by Hilary Harris and awarded an Academy Award in 1961, a feat for the Films of Scotland Committee. 

The first programme of This Wonderful World was aired on 11 October 1957 in Scotland; it was on "The Culbin Sands" which focused on how the Forestry Commission had replanted six thousand acres of woodland along the mouth of Findhorn. In the seventeenth century wild sand had blown into the mouth and covered the land, the successful replanting of the forest was a great success for the Commission. This Wonderful World was shown weekly, other topics for episodes included Leonardo da Vinci, ballet, King Penguins and Norman McLaren's "Boogie Doodle". 

This Wonderful World began to be aired in England In February 1959, it ran for a further eight years and was in the Top Ten programmes for the week for the UK in 1960. In 1961, Grierson was appointed a Commander of the Order of the British Empire in the Queen's Birthday Honours. In 1962, he was a member of the jury for the Vancouver Film Festival, during his visit to Canada he also received the Royal Canadian Academy of Arts Medal for his contribution to the visual arts. In 1963, he was busy with This Wonderful World and the Films of Scotland Committee but still found time to attend the twenty-fifth anniversary of the National Film Board in Montreal. 

In 1965, Grierson was the patron of the Commonwealth Film Festival which took place in Cardiff in that year. In 1966, he was offered the role of Governor of the British Film Institute; however, he turned down the position. This Wonderful World changed the title to John Grierson Presents. 

In 1967, after returning from the Oberhausen Film Festival where he had been the President of Honour of the jury, Grierson suffered a bout of bronchitis which lasted eight days. His brother Anthony, who had trained to be a doctor was called and diagnosed Grierson with emphysema, his coughing fits were a cause for concern, and he was admitted to Manor Hospital. Grierson decided to give up smoking and drinking to benefit his health.

Grierson opened the new primary school at Cambusbarron on 10 October 1967, his sister Dorothy attended the day with him. The BBC expressed their wishes to make a programme about Grierson in the year of his seventieth birthday which he turned down three times In he year of his seventieth birthday, Grierson received many tributes from across the globe, he was made an honorary member of the Association of Cinematograph, Television and Allied Technicians which he pressed for the ceremony to be held in Glasgow. He also received the Golden Thistle Award for Outstanding Achievement in the Art of Cinema at the Edinburgh Film Festival. 

In January 1969, Grierson left for Canada to lecture at McGill University, enrollment for his classes grew to around seven hundred students, he also lectured at Carleton University once a fortnight. At Heriot-Watt University in Edinburgh on 8 July 1969, Grierson also received an Honorary Doctorate of Literature. A few days earlier on 4 July 1969, Grierson had opened the Scottish Fisheries Museum in Anstruther. 

Grierson was a member of the jury for the Canadian Film Awards in 1970. He spent a few months in 1971, travelling around India instilling the importance of having small production units throughout the country. He returned to the UK in December 1971 and was meant to travel back to India however it was delayed due to Indo-Pakistani War. Grierson went into hospital for a health check-up in January 1972, he was diagnosed with lung and liver cancer and was given months to live. During his time in hospital he spent time dictating letters to his wife Margaret and received visitors; however, he fell unconscious on 18 February and died on the 19th. He had been detailed in his wishes for his funeral and wished to be cremated, his urn was to be placed in the sea off the Old Head in Kinsale and his brother Anthony's ashes, who had died in August 1971, were to be placed at the same time. A small flotilla followed the "Able Seaman," and when the urns were lowered into the water, the fishing boats sounded their sirens. 

The Grierson Archive at the University of Stirling Archives was opened by Angus Macdonald in October 1977. 

Filmography as director:

Filmography as producer/creative contributor:




The Grierson Documentary Film Awards were established in 1972 to commemorate John Grierson and
are currently supervised by The Grierson Trust. The aim of the awards is to recognise "outstanding films that demonstrate integrity, originality and technical excellence, together with social or cultural significance".

Grierson Awards are presented annually in nine categories:

The Canadian Film Awards had presented a Grierson Award for "an outstanding contribution to Canadian cinema in the spirit of John Grierson."




 


</doc>
<doc id="15622" url="https://en.wikipedia.org/wiki?curid=15622" title="James Cameron">
James Cameron

James Francis Cameron (born August 16, 1954) is a Canadian filmmaker, philanthropist, and deep-sea explorer. After working in special effects, he found major success after directing and writing the science fiction action film "The Terminator" (1984). He then became a popular Hollywood director and was hired to write and direct "Aliens" (1986); three years later he followed up with "The Abyss" (1989). He found further critical acclaim for his use of special effects in "" (1991). After his film "True Lies" (1994), Cameron took on his biggest film at the time, "Titanic" (1997), which earned him Academy Awards for Best Picture, Best Director and Best Film Editing.

After "Titanic", Cameron began a project that took almost 10 years to make: his science-fiction epic "Avatar" (2009), which was in particular a landmark for 3D technology, and for which he received nominations for the same three Academy Awards. Despite "Avatar" being his only movie made to date in 3D, Cameron is the most successful 3D film-maker in terms of box-office revenue. In the time between making "Titanic" and "Avatar", Cameron spent several years creating many documentary films (specifically underwater documentaries) and co-developed the digital 3D Fusion Camera System. Described by a biographer as part scientist and part artist, Cameron has also contributed to underwater filming and remote vehicle technologies. On March 26, 2012, Cameron reached the bottom of the Mariana Trench, the deepest part of the ocean, in the "Deepsea Challenger" submersible. He is the first person to do this in a solo descent, and is only the third person to do so ever. In 2010, "Time" magazine listed Cameron among the 100 most influential people in the world.

In total, Cameron's directorial efforts have grossed approximately US$2 billion in North America and US$6 billion worldwide. Not adjusted for inflation, Cameron's "Titanic" and "Avatar" are the two highest-grossing films of all time at $2.19 billion and $2.78 billion respectively. Cameron also holds the distinction of having directed two of the four films in history to gross over $2 billion worldwide (the other two being "" and ""). Moreover, they were also the first two films to accomplish the feat. In March 2011, he was named Hollywood's top earner by "Vanity Fair", with estimated 2010 earnings of $257 million. In October 2013, a new species of frog "Pristimantis jamescameroni" from Venezuela was named after him in recognition of his efforts in environmental awareness, in addition to his public promotion of veganism.

Cameron was born in 1954 in Kapuskasing, Ontario, Canada, the son of Shirley (née Lowe), an artist and nurse, and Phillip Cameron, an electrical engineer. His paternal great-great-great-grandfather emigrated from Balquhidder, Scotland, in 1825.

Cameron grew up in Chippawa, Ontario, and attended Stamford Collegiate School in Niagara Falls, Ontario. His family moved to Brea, California in 1971, when Cameron was 17 years old. He dropped out of Sonora High School, then attended Brea Olinda High School to further his secondary education.

Cameron enrolled at Fullerton College, a two-year community college, in 1973 to study physics. He switched to English, then dropped out before the start of the fall 1974 semester. Next, he worked several jobs, including as a truck driver, writing when he had time. During this period he taught himself about special effects: "I'd go down to the USC library and pull any thesis that graduate students had written about optical printing, or front screen projection, or dye transfers, anything that related to film technology. That way I could sit down and read it, and if they'd let me photocopy it, I would. If not, I'd make notes."

Cameron quit his job as a truck driver to enter the film industry after seeing "Star Wars" in 1977. When Cameron read Syd Field's book "Screenplay", it occurred to him that integrating science and art was possible, and he wrote a 10-minute science-fiction script with two friends, titled "Xenogenesis". They raised money, rented camera, lenses, film stock and studio then shot it in 35 mm. They dismantled the camera to understand how to operate it and spent the first half-day of the shoot trying to figure out how to get it running.

He was the director, writer, producer, and production designer for "Xenogenesis" (1978). He then became an uncredited production assistant on "Rock and Roll High School" in 1979. While continuing to educate himself in filmmaking techniques, Cameron started working as a miniature model maker at Roger Corman Studios. Making rapidly produced, low-budget productions taught Cameron to work efficiently. He soon found employment as an art director in the sci-fi movie "Battle Beyond the Stars" (1980). He did special effects work design and direction on John Carpenter's "Escape from New York" (1981), acted as production designer on "Galaxy of Terror" (1981), and consulted on the design of "Android" (1982).

Cameron was hired as the special effects director for the sequel to "Piranha", entitled "" in 1981. The original director, Miller Drake, left the project due to creative differences with producer Ovidio Assonitis, who then gave Cameron his first job as director. The interior scenes were filmed in Rome, Italy, while the underwater sequences were shot at Grand Cayman Island.

The movie was to be produced in Jamaica. On location, production slowed due to numerous problems and adverse weather. James Cameron was fired after failing to get a close up of Carole Davis in her opening scene. Ovidio ordered Cameron to do the close-up the next day before he started on that day's shooting. Cameron spent the entire day sailing around the resort, reproducing the lighting but still failed to get the close-up. After he was fired, Ovidio invited Cameron to stay on location and assist in the shooting. Once in Rome, Ovidio took over the editing when Cameron was stricken with food poisoning. During his illness, Cameron had a nightmare about an invincible robot hitman sent from the future to kill him, giving him the idea for "The Terminator", which later catapulted his film career.

After completing a screenplay for "The Terminator", Cameron decided to sell it so that he could direct the movie. However, the production companies he contacted, while expressing interest in the project, were unwilling to let a largely inexperienced feature film director make the movie. Finally, Cameron found a company called Hemdale Pictures, which was willing to let him direct. Gale Anne Hurd, who had started her own production company, Pacific Western Productions, had previously worked with Cameron in Roger Corman's company and agreed to buy Cameron's screenplay for one dollar, on the condition that Cameron direct the film. Hurd was signed on as a producer, and Cameron finally got his first break as director. Orion Pictures distributed the film. Hurd and Cameron were married from 1985 to 1989.

For the role of the Terminator, Cameron envisioned a man who was not exceptionally muscular, who could "blend into" a crowd. Lance Henriksen, who had starred in "Piranha II: The Spawning", was considered for the title role, but when Arnold Schwarzenegger and Cameron first met over lunch to discuss Schwarzenegger's playing the role of Kyle Reese, both came to the conclusion that the cyborg villain would be the more compelling role for the Austrian bodybuilder; Henriksen got the smaller part of LAPD detective Hal Vukovich and the role of Kyle Reese went to Michael Biehn. In addition, Linda Hamilton first appeared in this film in her iconic role of Sarah Connor, and later married Cameron.

"The Terminator" was a box-office hit, breaking expectations by Orion Pictures executives that the film would be regarded as no more than a sci-fi film, and then only last a week in theaters. It was a low-budget film which cost $6.5 million to make, cutting expenses in such ways as recording the audio track in mono. However, "The Terminator" eventually earned over $78 million worldwide.

During the early 1980s, Cameron wrote three screenplays simultaneously: "The Terminator", "Aliens", and the first draft of "". While Cameron continued with "The Terminator" and "Aliens", Sylvester Stallone eventually took over the script of "", creating a final draft which differed radically from Cameron's initial vision.

Cameron next began the sequel to "Alien", the 1979 film by Ridley Scott. Cameron named the sequel "Aliens" and again cast Sigourney Weaver in the iconic role of Ellen Ripley. According to Cameron, the crew on "Aliens" was hostile to him, regarding him as a poor substitute for Ridley Scott. Cameron sought to show them "The Terminator" but the majority of the crew refused to watch it and remained skeptical of his direction throughout production. Despite this and other off-screen problems (such as clashing with an uncooperative camera man and having to replace one of the lead actors when Michael Biehn of "Terminator" took James Remar's place as Corporal Hicks), "Aliens" became a box-office success. It received Academy Award nominations for Best Actress for Weaver, Best Art Direction, Best Film Editing, Best Original Score, Best Sound, and won awards for Best Sound Editing and Best Visual Effects. In addition, the film and its lead actress made the cover of "TIME" magazine as a result of numerous and extensive scenes of women in combat; these were almost without precedent and expressed the feminist theme of the film very strongly.

Cameron's next project stemmed from an idea that had come up during a high school biology class. The story of oil-rig workers who discover otherworldly underwater creatures became the basis of Cameron's screenplay for "The Abyss", which cast Ed Harris, Mary Elizabeth Mastrantonio and Michael Biehn. Initially budgeted at $41 million United States (though the production ran considerably over budget), it was considered to be one of the most expensive films of its time and required cutting-edge effects technology. Because much of the filming took place underwater and the technology wasn't advanced enough to digitally create an underwater environment, Cameron chose to shoot much of the movie "reel-for-real", at depths of up to . For creation of the sets, the containment building of an unfinished nuclear power plant was converted, and two huge tanks were used. The main tank was filled with of water and the second with . The cast and crew resided there for much of the filming.

After the success of "The Terminator", there had been talking about a sequel to continue the story of Sarah Connor and her struggle against machines from the future. Although Cameron had come up with a core idea for the sequel, and Schwarzenegger expressed interest in continuing the story, there were still problems regarding who had the rights to the story, as well as the logistics of the special effects needed to make the sequel. Finally, in the late-1980s, Mario Kassar of Carolco Pictures secured the rights to the sequel, allowing Cameron to greenlight production of the film, now called "".

For the film, Linda Hamilton reprised her iconic role of Sarah Connor. In addition, Schwarzenegger also returned in his role as The Terminator, but this time as a protector. Unlike Schwarzenegger's character—the T-800 Terminator which is made of a metal endoskeleton—the new villain of the sequel, called the T-1000, is a more advanced Terminator made of liquid metal, and with polymorphic abilities. The T-1000 would also be much less bulky than the T-800. For the role, Cameron cast Robert Patrick, a sharp contrast to Schwarzenegger. Cameron explained, "I wanted someone who was extremely fast and agile. If the T-800 is a human Panzer tank, then the T-1000 is a Porsche."

Cameron had originally wanted to incorporate this advanced-model-Terminator into the first film, but the special effects at the time were not advanced enough. The ground-breaking effects used in "The Abyss" to digitally depict the water tentacle convinced Cameron that his liquid metal villain was now possible.

TriStar Pictures agreed to distribute the film, but required a locked release date, intended to be about one year after the start of shooting. The movie, co-written by Cameron and his longtime friend, William Wisher Jr., had to go from screenplay to finished film in just that amount of time. Like Cameron's previous film, it was one of the most expensive films of its era, with a budget of about $100 million. The biggest challenge of the movie was the special effects used in creating the T-1000. Nevertheless, the film was finished on time and released to theaters on July 3, 1991.

"Terminator 2", or "T2", as it was abbreviated, broke box-office records (including the opening weekend record for an R-rated film), earning over $200 million in the United States and Canada, and over $300 million in other territories, and became the highest-grossing film of that year. It won four Academy Awards: Best Makeup, Best Sound Mixing, Best Sound Effects Editing, and Best Visual Effects. It was also nominated for Best Cinematography and Best Film Editing, but lost both Awards to "JFK".

James Cameron announced a third "Terminator" film many times during the 1990s, but without coming out with any finished scripts. Kassar and Vajna purchased the rights to the Terminator franchise from a bankruptcy sale of Carolco's assets. "" was eventually made and released in July 2003 without Cameron's involvement. Jonathan Mostow directed the film and Schwarzenegger returned as the Terminator.

Cameron reunited with the main cast of "Terminator 2" to film "", an attraction at Universal Studios Florida, Universal Studios Hollywood, and Universal Studios Japan. It was released in 1996 and was a mini-sequel to "Terminator 2: Judgment Day". The show is in two parts: a prequel segment in which a spokesperson talks about Cyberdyne, and the main feature, in which the performers interact with a 3-D movie.

Before the release of "T2", Schwarzenegger came to Cameron with the idea of remaking the French comedy "La Totale!" Titled "True Lies", with filming beginning after "T2"s release, the story revolves around a secret-agent spy who leads a double life as a married man, whose wife believes he is a computer salesman. Schwarzenegger was cast as Harry Tasker, a spy charged with stopping a plan by a terrorist to use nuclear weapons against the United States. Jamie Lee Curtis and Eliza Dushku played the character's family, and Tom Arnold the sidekick.

Cameron's Lightstorm Entertainment signed on with Twentieth Century Fox for the production of "True Lies". Made on a budget of $115 million and released in 1994, the film earned $146 million in North America, and $232 million abroad. The film received an Academy Award nomination for Best Visual Effects.

An American science-fiction action thriller film directed by Kathryn Bigelow. It was co-written, produced, and co-edited by Cameron, her ex-husband, and co-written by Jay Cocks.

Cameron expressed interest in the 1912 sinking of the ship and decided to script and film his next project based on this event. The picture revolved around a fictional romance story between two young lovers from different social classes who meet on board. Before production began, he took dives to the bottom of the Atlantic and shot actual footage of the ship underwater, which he inserted into the final film. Much of the film's dialogue was also written during these dives.

Subsequently, Cameron cast Leonardo DiCaprio, Kate Winslet, Billy Zane, Kathy Bates, Frances Fisher, Gloria Stuart, Bernard Hill, Jonathan Hyde, Victor Garber, Danny Nucci, David Warner, Suzy Amis, and Bill Paxton as the film's principal cast. Cameron's budget for the film reached about $200 million, making it the most expensive movie ever made at the time. Before its release, the film was widely ridiculed for its expense and protracted production schedule.

Released to theaters on December 19, 1997, "Titanic" grossed less in its first weekend ($28.6 million) than in its second ($35.4 million), an increase of 23.8%. This is unheard of for a widely released film, which is a testament to the movie's appeal. This was especially noteworthy, considering that the film's running time of more than three hours limited the number of showings each theater could schedule. It held the No. 1 spot on the box-office charts for months, eventually grossing a total of $600.8 million in the United States and Canada and more than $1.84 billion worldwide. "Titanic" became the highest-grossing film of all time, both worldwide and in the United States and Canada, and was also the first film to gross more than $1 billion worldwide. It was the highest-grossing film from 1998 until 2010, when Cameron's 2009 film "Avatar" surpassed its gross.

The CG visuals surrounding the sinking and destruction of the ship were considered spectacular. Despite criticism during production of the film, it received a record-tying 14 Oscar nominations (tied with "All About Eve") at the 1998 Academy Awards. It won 11 Oscars (also tying the record for most Oscar wins with "Ben-Hur" and later ""), including: Best Picture, Best Director, Best Art Direction, Best Cinematography, Best Visual Effects, Best Film Editing, Best Costume Design, Best Sound Mixing, Best Sound Editing, Best Original Dramatic Score, Best Original Song. Upon receiving the Best Director Oscar, Cameron exclaimed, "I'm king of the world!", in reference to one of the main characters' lines from the film. After receiving the Best Picture Oscar along with Jon Landau, Cameron asked for a moment of silence for the 1,500 men, women, and children who died when the ship sank.

In March 2010, Cameron revealed that "Titanic" would be re-released in 3D in April 2012, to commemorate the 100th anniversary of the sinking of the real ship. On March 27, 2012, Cameron attended the world première with Kate Winslet at the Royal Albert Hall in London. Following the re-release, "Titanic's" domestic total was pushed to $658.6 million and more than $2.18 billion worldwide. It became the second film to gross more than $2 billion worldwide (the first being Avatar).

Cameron had initially next planned to do a film of the comic-book character Spider-Man, a project developed by Menahem Golan of Cannon Films. Columbia hired David Koepp to adapt Cameron's treatment into a screenplay, and Koepp's first draft is taken often word-for-word from Cameron's story, though later drafts were heavily rewritten by Koepp himself, Scott Rosenberg, and Alvin Sargent. Columbia preferred to credit David Koepp solely, and none of the scripts before or after his were ever examined by the Writers Guild of America, East to determine proper credit attribution. Cameron and other writers objected, but Columbia and the WGA prevailed. In its release in 2002, "Spider-Man" had its screenplay credited solely to Koepp.

Unable to make "Spider-Man", Cameron moved to television and created "Dark Angel", a superheroine-centered series influenced by cyberpunk, biopunk, contemporary superhero franchises, and third-wave feminism. Co-produced with Charles H. Eglee, "Dark Angel" starred Jessica Alba as Max Guevara, a genetically enhanced super-soldier created by a secretive organization. According to a website called "DarkAngelFan.com", Cameron's work was said to "bring empowered female warriors back to television screens [...] by mixing the sober feminism of his "Terminator" and "Aliens" characters with the sexed-up Girl Power of a Britney Spears concert." While a success in its first season, low ratings in the second led to its cancellation. Cameron himself directed the series finale, a two-hour episode wrapping up many of the series' loose ends.

In 1998 James and John David Cameron formed a digital media company, earthship.tv, which became Earthship Productions. The company produced live multimedia documentaries from the depths of the Atlantic and Pacific oceans. With Earthship Productions, John Cameron's recent projects have included undersea documentaries on the ("", 2002) and the ("Ghosts of the Abyss" (2003, in IMAX 3D) and "Tony Robinson's Titanic Adventure" (2005)). He was a producer on the 2002 film "Solaris", and narrated "The Exodus Decoded".

Cameron is an advocate for stereoscopic digital 3D films. In a 2003 interview about his IMAX 2D documentary "Ghosts of the Abyss", he mentioned that he is "going to do everything in 3D now". He has made similar statements in other interviews. "Ghosts of the Abyss" and "Aliens of the Deep" (also an IMAX documentary) were both shot in 3-D and released by Walt Disney Pictures and Walden Media, and Cameron did the same for his new project, "Avatar" for 20th Century Fox & Sony Pictures' Columbia Pictures. He intends to use the same technology for "The Dive", "Sanctum" and an adaptation of the manga series "Battle Angel Alita".

Cameron was the co-founder and CEO of Digital Domain, a visual-effects production and technology company.

In addition, he plans to create a 3-D project about the first trip to Mars. ("I've been very interested in the Humans to Mars movement—the 'Mars Underground'—and I've done a tremendous amount of personal research for a novel, a miniseries, and a 3-D film.") He is on the science team for the 2011 Mars Science Laboratory.

Cameron announced on February 26, 2007, that he, along with his director, Simcha Jacobovici, have documented the unearthing of the Talpiot Tomb, which is alleged to be the tomb of Jesus. Unearthed in 1980 by Israeli construction workers, the names on the tomb are claimed, in the documentary, to correlate with the names of Jesus and several individuals closely associated with him. The documentary, named "The Lost Tomb of Jesus", was broadcast on the Discovery Channel on March 4, 2007.

As a National Geographic explorer-in-residence, Cameron re-investigated the sinking of the "Titanic" with eight experts in 2012. The investigation was featured in the TV documentary special "Titanic: The Final Word with James Cameron", which premiered on April 8 on the National Geographic Channel. In the conclusion of the analysis, the consensus revised the CGI animation of the sinking conceived in 1995.

In June 2005, Cameron was announced to be working on a project tentatively titled "Project 880" (now known to be "Avatar") in parallel with another project, "" (an adaptation of the manga series Battle Angel Alita). Both movies were to be shot in 3D. By December, Cameron stated that he wanted to film "Battle Angel" first, followed by "Avatar". However, in February 2006, he switched goals for the two film projects and decided to film "Avatar" first. He mentioned that if both films were successful, he would be interested in seeing a trilogy being made for both. "Alita: Battle Angel" eventually began production in 2016 with Cameron writing and producing and Robert Rodriguez directing.

"Avatar" had an estimated budget of over $300 million and was released on December 18, 2009. This marked his first feature film since 1997's "Titanic". It is composed almost entirely of computer-generated animation, using a more-advanced version of the "performance capture" technique used by director Robert Zemeckis in "The Polar Express". James Cameron had written an 80-page scriptment for "Avatar" in 1995 and announced in 1996 that he would make the film after completing "Titanic". In December 2006, Cameron explained that the delay in producing the film since the 1990s had been to wait until the technology necessary to create his project was advanced enough, since at the time no studio would finance for the development of the visual effects. The film was originally scheduled to be released in May 2009 but was pushed back to December 2009 to allow more time for post-production on the complex CGI and to give more time for theatres worldwide to install 3D projectors. Cameron originally intended "Avatar" to be 3D-only.

"Avatar" broke several box office records during its initial theatrical run. It grossed $749.7 million in the United States and Canada and more than $2.74 billion worldwide, becoming the highest-grossing film of all time in the United States and Canada, surpassing Cameron's "Titanic". "Avatar" also became the first movie to ever earn more than $2 billion worldwide. Including revenue from the re-release of "Avatar" featuring extended footage, it grossed $760.5 million in the United States and Canada and more than $2.78 billion worldwide. It was nominated for nine Academy Awards, including Best Picture and Best Director, and won three for Best Art Direction, Best Cinematography and Best Visual Effects.

Avatar's success made Cameron the highest earner in Hollywood for 2010, netting him $257 million as reported by "Vanity Fair".

Disney announced in September 2011 that it would adapt James Cameron's film "Avatar" into Pandora–The World of Avatar, a themed area at Disney's Animal Kingdom in Lake Buena Vista, Florida.

Cameron served as the executive producer of "Sanctum", a film detailing the expedition of a team of underwater cave divers who find themselves trapped in a cave, their exit blocked and with no known way to reach the surface either in person or by radio contact.

In August 2013, Cameron announced his intention to film three sequels to "Avatar" simultaneously, to be released in December 2016, 2017, and 2018. However, on January 14, 2015, Cameron announced that the release dates for the three sequels were each delayed a year with the first sequel scheduled to be released in December 2017. In September 2017, the sequels to "Avatar" started production with the plan for the first to be released in December 2020, the next in 2021 and a further two in 2024 and 2025 respectively. "Deadline Hollywood" estimated that the budget for these would be over $1 billion in total.

"Avatar 2" and "Avatar 3" started preliminary shooting simultaneously in Manhattan Beach, California on August 15, 2017, followed by principal photography in New Zealand on September 25, 2017. The other sequels are expected to start shooting as soon as "Avatar 2" and "3"s filming wraps.

Although the last two sequels have been greenlit, Cameron stated in a November 26, 2017 interview: "Let’s face it, if "Avatar 2" and "3" don’t make enough money, there’s not going to be a "4" and "5"".

His original plans were to do "" next, but he changed his mind due to "Avatar"'s success; "My intention when I made "Avatar" was to do "Battle Angel" next. However, the positive feedback for "Avatar" and the support of the message of "Avatar", encouraged me to do more of those films." Cameron's Lightstorm Entertainment bought the film rights to the Taylor Stevens novel "The Informationist" in October 2012 with plans for Cameron to direct it. A screenwriter will be hired to adapt the novel while Cameron works on the "Avatar" sequels. Another project Cameron has announced is a personal commitment to shoot a film on the atomic bombings of Hiroshima and Nagasaki as told through the story of Tsutomu Yamaguchi, a man who survived both attacks. Cameron met with Yamaguchi just days before he died in 2010.

In January 2017, it was reported that Cameron would be returning to the "Terminator" franchise as producer and creative consultant for the next film installment, with Tim Miller signed on as director. In May 2017, Arnold Schwarzenegger confirmed that he will return in the film and that James Cameron will be involved. In January 2017 it was also announced that Cameron would be making a documentary about the history of science fiction. Cameron stated, "Without Jules Verne and H. G. Wells, there wouldn’t have been Ray Bradbury or Robert A. Heinlein, and without them, there wouldn’t be [George] Lucas, [Steven] Spielberg, Ridley Scott or me."

In the mid-1990s, Cameron announced that he would make a "Spider-Man" film. The project was cancelled and dropped by Cameron when Carolco Pictures filed for bankruptcy. His script was rewritten by David Koepp for the 2002 movie "Spider-Man", directed by Sam Raimi.

In 1996, James Cameron decided to produce the new installment in the "Planet of the Apes" franchise, but it was cancelled before the Tim Burton version was made.

Although Cameron has resided in the United States since 1971, he remains a Canadian citizen. Cameron applied for American citizenship in 2004, but withdrew his application after George W. Bush won the presidential election. Cameron divides his time between his home in California and his second home in New Zealand, a country he fell in love with when he was filming "Avatar". In 2016, Cameron partnered with Tourism New Zealand to produce a series of videos that expressed his love for the country.

Cameron calls himself "Converted Agnostic", and says "I've sworn off agnosticism, which I now call cowardly atheism". As a child he described the Lord's Prayer as being a "tribal chant".

He has been a close friend of fellow filmmaker Guillermo del Toro since they met during the production of Del Toro's 1993 film "Cronos". Around 1997, Del Toro's father, Federico del Toro Torres, was kidnapped in Guadalajara; immediately after learning of the kidnapping, Cameron took Del Toro and gave him over $1 million in cash he just withdrew to help paying the ransom. After the ransom was paid, Federico was released, having spent 72 days abducted. The culprits were never apprehended, and the money of both Cameron and Del Toro's family was never recovered; Del Toro and his family moved outside of Mexico after the event, in fear of a similar event happening again.

Cameron has been married five times to the following spouses: Sharon Williams (1978–1984), Gale Anne Hurd (1985–1989), director Kathryn Bigelow (1989–1991), Linda Hamilton (1997–1999, daughter Josephine born in 1993), and Suzy Amis (2000–present). Cameron had dated Hamilton since 1991. Eight months after the marriage, however, they separated, and within days of Cameron's Oscar victory with "Titanic," the couple announced their divorce. As part of the divorce settlement, Cameron was ordered to pay Hamilton $50 million. Hamilton later revealed that one reason for their divorce was that he had been dating Suzy Amis, an actress he cast as Lizzy Calvert in "Titanic." He married Amis in 2000, and they have one son and two daughters. Hurd was the producer of Cameron's "The Terminator", "Aliens", and "The Abyss", and the executive producer of "". Hamilton played the role of Sarah Connor in both "Terminator" films. Amis played the part of Lizzy Calvert, Rose's granddaughter, in "Titanic". Both Cameron ("Avatar") and Bigelow ("The Hurt Locker") were nominated for the Oscar, the Golden Globe, and the BAFTA Award for Best Director for films released in 2009. Cameron won the Golden Globe, while Bigelow won the Oscar and the BAFTA for Best Director, becoming the first woman to win either.

Cameron became an expert on deep-sea exploration in conjunction with his research and underwater filming for "The Abyss" (1989) and "Titanic" (1997). In June 2010, Cameron met in Washington with the EPA to discuss possible solutions to the 2010 Deepwater Horizon (BP) oil spill. Later that week at the All Things Digital Conference, he attracted some notoriety when he stated, "Over the last few weeks I've watched...and been thinking, 'Those morons don't know what they're doing'." Reportedly, Cameron had offered BP help to plug the oil well, but it declined. The oil spill was eventually stopped using techniques similar to those Cameron recommended.

On March 7, 2012, Cameron took the "Deepsea Challenger" submersible to the bottom of the New Britain Trench in a five-mile-deep solo dive. On March 26, 2012, Cameron reached the Challenger Deep, the deepest part of the Mariana Trench. He spent more than three hours exploring the ocean floor before returning to the surface. Cameron is the first person to accomplish the trip solo. He was preceded by unmanned dives in 1995 and 2009 and by Jacques Piccard and Don Walsh, who were the first men to reach the bottom of the Mariana Trench aboard the Bathyscaphe Trieste in 1960. Cameron has made a three-dimensional film of his dive. During his dive to the Challenger Deep, the data he collected resulted in interesting new finds in the field of marine biology, including new species of sea cucumber, squid worm, and giant single-celled amoeba, which are exciting finds due to the harshness of the environment.

In 2012, Cameron, his wife and his children adopted a vegan diet. Cameron explains that "By changing what you eat, you will change the entire contract between the human species and the natural world".

When asked what's the best thing an individual can do to fight climate change, Cameron said, "Stop eating animals."

Cameron and his wife are featured in "Eating You Alive", a 2016 American documentary. His "The Game Changers" (2017) showcases vegan athletes and other icons.

Cameron is a member of the NASA Advisory Council and is working on the project to put cameras on an upcoming human mission to Mars. Cameron has also given speeches and raised money for the Mars Society, a non-profit organization lobbying for the colonization of Mars.

In 2006 Cameron's wife co-founded MUSE School, in 2015 the school became the first K-12 vegan school in the United States.

Early in 2014, Cameron purchased the Beaufort Vineyard and Estate Winery in Courtenay, British Columbia, at a price of $2.7 million, as well as a number of other businesses in the area, including cattle ranching operations, to pursue his passion for sustainable agribusiness.

In April 2018, Cameron attended a private dinner with Saudi Arabia's Crown Prince Mohammad bin Salman.

In an interview in November 2017, Cameron revealed that he had a hostile altercation with Harvey Weinstein at the 70th Academy Awards in 1998 after Weinstein came to him trying to promote his company Miramax. Cameron, whose friend Guillermo del Toro was unhappy with the way Miramax had treated him on his film "Mimic", proceeded in return to "read him chapter and verse about how great I thought he was for the artist", which led to a dispute that almost resulted in an actual fight. Cameron recalled "[almost] hitting him with my Oscar", adding that "[a lot of people] would’ve preferred I had played through on that one", referring to the recent Harvey Weinstein sexual abuse allegations.

In June 2013, British artist Roger Dean filed a legal action at a court in New York against Cameron. Dean accused Cameron of "wilful and deliberate copying, dissemination and exploitation" of his original images, relating to Cameron's 2009 film "Avatar" and sought damages of $50m. Dean subsequently lost the case.

Cameron's directorial style has had significant influence within the Hollywood film industry. "Buffy the Vampire Slayer" and "Firefly" creator Joss Whedon stated that Cameron's approach to action scenes was influential to those in "The Avengers". Whedon also admired Cameron's ability of writing female characters such as Ellen Ripley. He also cited Cameron as "the leader and the teacher and the Yoda".<ref name="http://www.slashfilm.com/film-interview-joss-whedon-writer-director-the-avengers/"></ref> Michael Bay considers Cameron an idol and was convinced by him to use 3D in "".<ref name="http://www.hollywoodreporter.com/news/michael-bay-reveals-james-camerons-191774"></ref> Cameron's approach to 3D also inspired Baz Luhrmann to use it in "The Great Gatsby". Other directors that have drawn inspiration from Cameron include Peter Jackson, Neill Blomkamp and Quentin Tarantino.<ref name="https://web.archive.org/web/20071225035055/http://tbhl.theonering.net/peter/faq.html"></ref>

In 1999, Cameron was labeled selfish and cruel by one collaborator, author Orson Scott Card, who had been hired a decade earlier to work with Cameron on the novelization of "The Abyss". Card said the experience was "hell on wheels":
He was very nice to me, because I could afford to walk away. But he made everyone around him miserable, and his unkindness did nothing to improve the film in any way. Nor did it motivate people to work faster or better. And unless he changes his way of working with people, I hope he never directs anything of mine. In fact, now that this is in print, I can fairly guarantee that he will never direct anything of mine. Life is too short to collaborate with selfish, cruel people.
He later alluded to Cameron in his review of "Me and Orson Welles", where he described witnessing a famous director chew out an assistant for his own error.

After working with Cameron on "Titanic", Kate Winslet decided she would not work with Cameron again unless she earned "a lot of money". She said that Cameron was a nice man, but she found his temper difficult to deal with. In an editorial, the British newspaper "The Independent" said that Cameron "is a nightmare to work with. Studios have come to fear his habit of straying way over schedule and over budget. He is notorious on set for his uncompromising and dictatorial manner, as well as his flaming temper."

Sam Worthington, who worked with Cameron on "Avatar", stated on "The Jay Leno Show" that Cameron had very high expectations from everyone: he would use a nail gun to nail the film crew's cell phones to a wall above an exit door in retaliation for unwanted ringing during production. Other actors, such as Bill Paxton and Sigourney Weaver, have praised Cameron's perfectionism. Weaver said of Cameron: "He really does want us to risk our lives and limbs for the shot, but he doesn't mind risking his own." Michael Biehn has also praised Cameron, claiming "Jim is a really passionate person. He cares more about his movies than other directors care about their movies", but added "I’ve never seen him yell at anybody." However, Biehn did claim Cameron is "not real sensitive when it comes to actors and their trailers."

Composer James Horner refused to work with Cameron for a decade following their strained working relationship on 1986's "Aliens". They eventually settled their differences, and Horner went on to score both "Titanic" and "Avatar".

In 2014, Cameron was the keynote speaker at the first annual Fame and Philanthropy, a charity fundraiser which raised money for several high-profile celebrity charities. Cameron was one of several guest speakers at the event along with Charlize Theron and Halle Berry.

In a 2015 joint interview, Cameron collaborators Sigourney Weaver and Jamie Lee Curtis commented very positively on him. Curtis stated, "the truth is he can do every other job [than acting]. I'm talking about every single department, from art direction to props to wardrobe to cameras, he knows more than everyone doing the job." Weaver answered "There are very few geniuses in the world, let alone in our business, and he's certainly one of them." She also said, "he's misunderstood in the industry, somewhat. He is so generous to actors."

Cameron received the inaugural Ray Bradbury Award from the Science Fiction and Fantasy Writers of America in 1992 for "" ("Avatar" would be a finalist in 2010).

Cameron did not receive any major mainstream filmmaking awards prior to "Titanic". For "Titanic", he won several, including Academy Awards for Best Picture (shared with Jon Landau), Best Director and Best Film Editing (shared with Conrad Buff and Richard A. Harris). Cameron is one of the few filmmakers to win three Oscars in a single evening and Golden Globes for Best Motion Picture – Drama and Best Director.

In recognition of "a distinguished career as a Canadian filmmaker", Carleton University, Ottawa, awarded Cameron the honorary degree of Doctor of Fine Arts on June 13, 1998. Cameron accepted the degree in person and gave the Convocation Address.

He also received an honorary doctorate in 1998 from Brock University in St. Catharines, Ontario, for his accomplishments in the international film industry.

In 1998, Cameron attended convocation to receive an honorary doctorate of Laws from Ryerson University, Toronto. The university awards its highest honor to those who have made extraordinary contributions in Canada, or internationally.

In 1999, Cameron received the honorary Doctor of Fine Arts degree from California State University, Fullerton, where he had been a student in the 1970s. He received the degree at the university's annual Commencement exercises that year, where he gave the keynote speech.

In recognition of his contributions to underwater filming and remote vehicle technology, the University of Southampton awarded Cameron the honorary degree of Doctor of the University. Cameron did not attend the Engineering Sciences graduation ceremony in July 2004 where the degree was awarded but instead received it in person at the National Oceanography Centre.

On June 3, 2008, it was announced that he would be inducted into Canada's Walk of Fame. On December 18, 2009, the same day "Avatar" was released worldwide, Cameron received the 2,396th star on the Hollywood Walk of Fame. After the release of "Avatar", on February 28, 2010, Cameron was also honored with a Visual Effects Society (VES) Lifetime Achievement Award.

For "Avatar", Cameron won numerous awards as well, including: Golden Globes for Best Motion Picture – Drama (shared with Jon Landau) and Best Director. He was nominated for three Academy Awards: Best Picture, Best Director and Best Film Editing (shared with John Refoua and Stephen E. Rivkin). However, Cameron and "Avatar" lost to his former wife Kathryn Bigelow and her film, "The Hurt Locker".

On September 24, 2010, James Cameron was named Number 1 in The 2010 Guardian Film Power 100 list. In a list compiled by the British magazine "New Statesman" in September 2010, he was listed 30th in the list of "The World's 50 Most Influential Figures 2010".

The Science Fiction Hall of Fame inducted Cameron in June 2012.

Cameron has received numerous awards, mainly for "Titanic" and "Avatar".
Cameron has consistently worked with Bill Paxton, Michael Biehn, Lance Henriksen, Jenette Goldstein and Arnold Schwarzenegger.

Cameron's films have recurring themes and subtexts. These include the conflicts between humanity and technology, the dangers of corporate greed, strong female characters, and a strong romance subplot. In almost all films, the main characters usually get into dramatic crisis situations with significant threats to their own life or even the threat of an impending apocalypse. "The Abyss" dealt with deep sea exploration (shot in an unfinished nuclear reactor filled with water) and Cameron himself became an expert in the field of deep-sea wreckage exploration, exploring the wreckage of the "Titanic" and the "Bismarck". Cameron will return to this theme with "The Dive", shooting from a minisub.

Cameron has written the scripts to all the movies that he has directed. He has contributed to many more projects as a writer, director, and producer, or as a combination of the three.

Cameron's first film was the 1978 science fiction short film "Xenogenesis", which he directed, wrote and produced. Cameron's films have grossed a total of over $7 billion worldwide.

In addition to works of fiction, Cameron has directed and appeared in several documentaries including "Ghosts of the Abyss" and "Aliens of the Deep". He also contributed to a number of television series including "Dark Angel" and "Entourage".

Critical, public and commercial reception to films James Cameron has directed.




</doc>
<doc id="15624" url="https://en.wikipedia.org/wiki?curid=15624" title="Judaism">
Judaism

Judaism (originally from Hebrew , "Yehudah", "Judah"; via Latin and Greek) is the religion of the Jewish people. It is an ancient, monotheistic, Abrahamic religion with the Torah as its foundational text. It encompasses the religion, philosophy, and culture of the Jewish people. Judaism is considered by religious Jews to be the expression of the covenant that God established with the Children of Israel. Judaism encompasses a wide corpus of texts, practices, theological positions, and forms of organization. The Torah is part of the larger text known as the Tanakh or the Hebrew Bible, and supplemental oral tradition represented by later texts such as the Midrash and the Talmud. With between 14.5 and 17.4 million adherents worldwide, Judaism is the tenth largest religion in the world.

Within Judaism there are a variety of movements, most of which emerged from Rabbinic Judaism, which holds that God revealed his laws and commandments to Moses on Mount Sinai in the form of both the Written and Oral Torah. Historically, this assertion was challenged by various groups such as the Sadducees and Hellenistic Judaism during the Second Temple period; the Karaites and Sabbateans during the early and later medieval period; and among segments of the modern non-Orthodox denominations. Modern branches of Judaism such as Humanistic Judaism may be nontheistic. Today, the largest Jewish religious movements are Orthodox Judaism (Haredi Judaism and Modern Orthodox Judaism), Conservative Judaism, and Reform Judaism. Major sources of difference between these groups are their approaches to Jewish law, the authority of the Rabbinic tradition, and the significance of the State of Israel. Orthodox Judaism maintains that the Torah and Jewish law are divine in origin, eternal and unalterable, and that they should be strictly followed. Conservative and Reform Judaism are more liberal, with Conservative Judaism generally promoting a more traditionalist interpretation of Judaism's requirements than Reform Judaism. A typical Reform position is that Jewish law should be viewed as a set of general guidelines rather than as a set of restrictions and obligations whose observance is required of all Jews. Historically, special courts enforced Jewish law; today, these courts still exist but the practice of Judaism is mostly voluntary. Authority on theological and legal matters is not vested in any one person or organization, but in the sacred texts and the rabbis and scholars who interpret them.

The history of Judaism spans more than 3,000 years. Judaism has its roots as an organized religion in the Middle East during the Bronze Age. Judaism is considered one of the oldest monotheistic religions. The Hebrews and Israelites were already referred to as "Jews" in later books of the Tanakh such as the Book of Esther, with the term Jews replacing the title "Children of Israel". Judaism's texts, traditions and values strongly influenced later Abrahamic religions, including Christianity, Islam and the Baha'i Faith. Many aspects of Judaism have also directly or indirectly influenced secular Western ethics and civil law. Hebraism was just as important a factor in the ancient era development of Western civilization as Hellenism, and Judaism, as the background of Christianity, has considerably shaped Western ideals and morality since Early Christianity.

Jews are an ethnoreligious group including those born Jewish, in addition to converts to Judaism. In 2015, the world Jewish population was estimated at about 14.3 million, or roughly 0.2% of the total world population. About 43% of all Jews reside in Israel and another 43% reside in the United States and Canada, with most of the remainder living in Europe, and other minority groups spread throughout Latin America, Asia, Africa, and Australia.

Unlike other ancient Near Eastern gods, the Hebrew God is portrayed as unitary and solitary; consequently, the Hebrew God's principal relationships are not with other gods, but with the world, and more specifically, with the people he created. Judaism thus begins with ethical monotheism: the belief that God is one and is concerned with the actions of mankind. According to the Tanakh (Hebrew Bible), God promised Abraham to make of his offspring a great nation. Many generations later, he commanded the nation of Israel to love and worship only one God; that is, the Jewish nation is to reciprocate God's concern for the world. He also commanded the Jewish people to love one another; that is, Jews are to imitate God's love for people. These commandments are but two of a large corpus of commandments and laws that constitute this covenant, which is the substance of Judaism.

Thus, although there is an esoteric tradition in Judaism (Kabbalah), Rabbinic scholar Max Kadushin has characterized normative Judaism as "normal mysticism", because it involves everyday personal experiences of God through ways or modes that are common to all Jews. This is played out through the observance of the Halakha (Jewish law) and given verbal expression in the Birkat Ha-Mizvot, the short blessings that are spoken every time a positive commandment is to be fulfilled.
Whereas Jewish philosophers often debate whether God is immanent or transcendent, and whether people have free will or their lives are determined, Halakha is a system through which any Jew acts to bring God into the world.

Ethical monotheism is central in all sacred or normative texts of Judaism. However, monotheism has not always been followed in practice. The Jewish Bible (Tanakh) records and repeatedly condemns the widespread worship of other gods in ancient Israel. In the Greco-Roman era, many different interpretations of monotheism existed in Judaism, including the interpretations that gave rise to Christianity.

Moreover, some have argued that Judaism is a non-creedal religion that does not require one to believe in God. For some, observance of Jewish law is more important than belief in God "per se". In modern times, some liberal Jewish movements do not accept the existence of a personified deity active in history. The debate about whether one can speak of authentic or normative Judaism is not only a debate among religious Jews but also among historians.

Scholars throughout Jewish history have proposed numerous formulations of Judaism's core tenets, all of which have met with criticism. The most popular formulation is Maimonides' thirteen principles of faith, developed in the 12th century. According to Maimonides, any Jew who rejects even one of these principles would be considered an apostate and a heretic. Jewish scholars have held points of view diverging in various ways from Maimonides' principles.

In Maimonides' time, his list of tenets was criticized by Hasdai Crescas and Joseph Albo. Albo and the Raavad argued that Maimonides' principles contained too many items that, while true, were not fundamentals of the faith.

Along these lines, the ancient historian Josephus emphasized practices and observances rather than religious beliefs, associating apostasy with a failure to observe Jewish law and maintaining that the requirements for conversion to Judaism included circumcision and adherence to traditional customs. Maimonides' principles were largely ignored over the next few centuries. Later, two poetic restatements of these principles (""Ani Ma'amin"" and ""Yigdal"") became integrated into many Jewish liturgies, leading to their eventual near-universal acceptance.

In modern times, Judaism lacks a centralized authority that would dictate an exact religious dogma. Because of this, many different variations on the basic beliefs are considered within the scope of Judaism. Even so, all Jewish religious movements are, to a greater or lesser extent, based on the principles of the Hebrew Bible and various commentaries such as the Talmud and Midrash. Judaism also universally recognizes the Biblical Covenant between God and the Patriarch Abraham as well as the additional aspects of the Covenant revealed to Moses, who is considered Judaism's greatest prophet. In the Mishnah, a core text of Rabbinic Judaism, acceptance of the Divine origins of this covenant is considered an essential aspect of Judaism and those who reject the Covenant forfeit their share in the World to Come.

Establishing the core tenets of Judaism in the modern era is even more difficult, given the number and diversity of the contemporary Jewish denominations. Even if to restrict the problem to the most influential intellectual trends of the nineteenth and twentieth century, the matter remains complicated. Thus for instance, Joseph Soloveitchik's (associated with the Modern Orthodox movement) answer to modernity is constituted upon the identification of Judaism with following the halakha whereas its ultimate goal is to bring the holiness down to the world. Mordecai Kaplan, the founder of the Reconstructionist Judaism, abandons the idea of religion for the sake of identifying Judaism with civilization and by means of the latter term and secular translation of the core ideas, he tries to embrace as many Jewish denominations as possible. In turn, Solomon Schechter's Conservative Judaism was identical with the tradition understood as the interpretation of Torah, in itself being the history of the constant updates and adjustment of the Law performed by means of the creative interpretation. Finally, David Philipson draws the outlines of the Reform movement in Judaism by opposing it to the strict and traditional rabbinical approach and thus comes to the conclusions similar to that of the Conservative movement.

The following is a basic, structured list of the central works of Jewish practice and thought.

Many traditional Jewish texts are available online in various Torah databases (electronic versions of the Traditional Jewish Bookshelf). Many of these have advanced search options available.

The basis of Jewish law and tradition (halakha) is the Torah (also known as the Pentateuch or the Five Books of Moses). According to rabbinic tradition, there are 613 commandments in the Torah. Some of these laws are directed only to men or to women, some only to the ancient priestly groups, the Kohanim and Leviyim (members of the tribe of Levi), some only to farmers within the Land of Israel. Many laws were only applicable when the Temple in Jerusalem existed, and only 369 of these commandments are still applicable today.

While there have been Jewish groups whose beliefs were based on the written text of the Torah alone (e.g., the Sadducees, and the Karaites), most Jews believe in the oral law. These oral traditions were transmitted by the Pharisee school of thought of ancient Judaism and were later recorded in written form and expanded upon by the rabbis.

According to Rabbinical Jewish tradition, God gave both the Written Law (the Torah) and the Oral law to Moses on Mount Sinai. The Oral law is the oral tradition as relayed by God to Moses and from him, transmitted and taught to the sages (rabbinic leaders) of each subsequent generation.

For centuries, the Torah appeared only as a written text transmitted in parallel with the oral tradition. Fearing that the oral teachings might be forgotten, Rabbi Judah haNasi undertook the mission of consolidating the various opinions into one body of law which became known as the "Mishnah".

The Mishnah consists of 63 tractates codifying Jewish law, which are the basis of the "Talmud." According to Abraham ben David, the "Mishnah" was compiled by Rabbi Judah haNasi after the destruction of Jerusalem, in anno mundi 3949, which corresponds to 189 CE.

Over the next four centuries, the Mishnah underwent discussion and debate in both of the world's major Jewish communities (in Israel and Babylonia). The commentaries from each of these communities were eventually compiled into the two Talmuds, the Jerusalem Talmud ("Talmud Yerushalmi") and the Babylonian Talmud ("Talmud Bavli"). These have been further expounded by commentaries of various Torah scholars during the ages.

In the text of the Torah, many words are left undefined and many procedures are mentioned without explanation or instructions. Such phenomena are sometimes offered to validate the viewpoint that the Written Law has always been transmitted with a parallel oral tradition, illustrating the assumption that the reader is already familiar with the details from other, i.e., oral, sources.

Halakha, the rabbinic Jewish way of life, then, is based on a combined reading of the Torah, and the oral tradition—the Mishnah, the halakhic Midrash, the Talmud and its commentaries. The Halakha has developed slowly, through a precedent-based system. The literature of questions to rabbis, and their considered answers, is referred to as responsa (in Hebrew, "Sheelot U-Teshuvot".) Over time, as practices develop, codes of Jewish law are written that are based on the responsa; the most important code, the Shulchan Aruch, largely determines Orthodox religious practice today.

Jewish philosophy refers to the conjunction between serious study of philosophy and Jewish theology. Major Jewish philosophers include Solomon ibn Gabirol, Saadia Gaon, Judah Halevi, Maimonides, and Gersonides. Major changes occurred in response to the Enlightenment (late 18th to early 19th century) leading to the post-Enlightenment Jewish philosophers. Modern Jewish philosophy consists of both Orthodox and non-Orthodox oriented philosophy. Notable among Orthodox Jewish philosophers are Eliyahu Eliezer Dessler, Joseph B. Soloveitchik, and Yitzchok Hutner. Well-known non-Orthodox Jewish philosophers include Martin Buber, Franz Rosenzweig, Mordecai Kaplan, Abraham Joshua Heschel, Will Herberg, and Emmanuel Lévinas.

Orthodox and many other Jews do not believe that the revealed Torah consists solely of its written contents, but of its interpretations as well. The study of Torah (in its widest sense, to include both poetry, narrative, and law, and both the Hebrew Bible and the Talmud) is in Judaism itself a sacred act of central importance. For the sages of the Mishnah and Talmud, and for their successors today, the study of Torah was therefore not merely a means to learn the contents of God's revelation, but an end in itself. According to the Talmud,
In Judaism, "the study of Torah can be a means of experiencing God". Reflecting on the contribution of the Amoraim and Tanaim to contemporary Judaism, Professor Jacob Neusner observed:
To study the Written Torah and the Oral Torah in light of each other is thus also to study "how" to study the word of God.

In the study of Torah, the sages formulated and followed various logical and hermeneutical principles. According to David Stern, all Rabbinic hermeneutics rest on two basic axioms:
These two principles make possible a great variety of interpretations. According to the Talmud,
Observant Jews thus view the Torah as dynamic, because it contains within it a host of interpretations

According to Rabbinic tradition, all valid interpretations of the written Torah were revealed to Moses at Sinai in oral form, and handed down from teacher to pupil (The oral revelation is in effect coextensive with the Talmud itself). When different rabbis forwarded conflicting interpretations, they sometimes appealed to hermeneutic principles to legitimize their arguments; some rabbis claim that these principles were themselves revealed by God to Moses at Sinai.

Thus, Hillel called attention to seven commonly used hermeneutical principles in the interpretation of laws (baraita at the beginning of Sifra); R. Ishmael, thirteen (baraita at the beginning of Sifra; this collection is largely an amplification of that of Hillel). Eliezer b. Jose ha-Gelili listed 32, largely used for the exegesis of narrative elements of Torah. All the hermeneutic rules scattered through the Talmudim and Midrashim have been collected by Malbim in "Ayyelet ha-Shachar", the introduction to his commentary on the Sifra. Nevertheless, R. Ishmael's 13 principles are perhaps the ones most widely known; they constitute an important, and one of Judaism's earliest, contributions to logic, hermeneutics, and jurisprudence. Judah Hadassi incorporated Ishmael's principles into Karaite Judaism in the 12th century. Today R. Ishmael's 13 principles are incorporated into the Jewish prayer book to be read by observant Jews on a daily basis.

The term "Judaism" derives from "Iudaismus", a Latinized form of the Ancient Greek Ioudaismos (Ἰουδαϊσμός) (from the verb , "to side with or imitate the [Judeans]"), and it was ultimately inspired by the Hebrew יהודה, "Yehudah", "Judah"; in Hebrew: יַהֲדוּת, "Yahadut". The term "Ἰουδαϊσμός" first appears in the Hellenistic Greek book of 2 Maccabees in the 2nd century BCE. In the context of the age and period it meant "seeking or forming part of a cultural entity" and it resembled its antonym "hellenismos", a word that signified a people's submission to Hellenic (Greek) cultural norms. The conflict between "iudaismos" and "hellenismos" lay behind the Maccabean revolt and hence the invention of the term "iudaismos".

Shaye J. D. Cohen writes in his book "The Beginnings of Jewishness":

The earliest instance in Europe where the term was used to mean "the profession or practice of the Jewish religion; the religious system or polity of the Jews" is Robert Fabyan's "The newe cronycles of Englande and of Fraunce a 1513". "Judaism" as a direct translation of the Latin "Iudaismus" first occurred in a 1611 English translation of the apocrypha (Deuterocanon in Catholic and Eastern Orthodoxy), 2 Macc. ii. 21: "Those that behaved themselves manfully to their honour for Iudaisme."

According to Daniel Boyarin, the underlying distinction between religion and ethnicity is foreign to Judaism itself, and is one form of the dualism between spirit and flesh that has its origin in Platonic philosophy and that permeated Hellenistic Judaism. Consequently, in his view, Judaism does not fit easily into conventional Western categories, such as religion, ethnicity, or culture. Boyarin suggests that this in part reflects the fact that much of Judaism's more than 3,000-year history predates the rise of Western culture and occurred outside the West (that is, Europe, particularly medieval and modern Europe). During this time, Jews experienced slavery, anarchic and theocratic self-government, conquest, occupation, and exile. In the Diaspora, they were in contact with, and influenced by, ancient Egyptian, Babylonian, Persian, and Hellenic cultures, as well as modern movements such as the Enlightenment (see Haskalah) and the rise of nationalism, which would bear fruit in the form of a Jewish state in their ancient homeland, the Land of Israel. They also saw an elite population convert to Judaism (the Khazars), only to disappear as the centers of power in the lands once occupied by that elite fell to the people of Rus and then the Mongols. Thus, Boyarin has argued that "Jewishness disrupts the very categories of identity, because it is not national, not genealogical, not religious, but all of these, in dialectical tension."

In contrast to this point of view, practices such as Humanistic Judaism reject the religious aspects of Judaism, while retaining certain cultural traditions.

According to Rabbinic Judaism, a Jew is anyone who was either born of a Jewish mother or who converted to Judaism in accordance with Jewish Law. Reconstructionist Judaism and the larger denominations of worldwide Progressive Judaism (also known as Liberal or Reform Judaism) accept the child as Jewish if one of the parents is Jewish, if the parents raise the child with a Jewish identity, but not the smaller regional branches. All mainstream forms of Judaism today are open to sincere converts, although conversion has traditionally been discouraged since the time of the Talmud. The conversion process is evaluated by an authority, and the convert is examined on his or her sincerity and knowledge. Converts are called "ben Abraham" or "bat Abraham", (son or daughter of Abraham). Conversions have on occasion been overturned. In 2008, Israel's highest religious court invalidated the conversion of 40,000 Jews, mostly from Russian immigrant families, even though they had been approved by an Orthodox rabbi.

Rabbinical Judaism maintains that a Jew, whether by birth or conversion, is a Jew forever. Thus a Jew who claims to be an atheist or converts to another religion is still considered by traditional Judaism to be Jewish. According to some sources, the Reform movement has maintained that a Jew who has converted to another religion is no longer a Jew, and the Israeli Government has also taken that stance after Supreme Court cases and statutes. However, the Reform movement has indicated that this is not so cut and dried, and different situations call for consideration and differing actions. For example, Jews who have converted under duress may be permitted to return to Judaism "without any action on their part but their desire to rejoin the Jewish community" and "A proselyte who has become an apostate remains, nevertheless, a Jew".

Karaite Judaism believes that Jewish identity can only be transmitted by patrilineal descent. Although a minority of modern Karaites believe that Jewish identity requires that both parents be Jewish, and not only the father. They argue that only patrilineal descent can transmit Jewish identity on the grounds that all descent in the Torah went according to the male line.

The question of what determines Jewish identity in the State of Israel was given new impetus when, in the 1950s, David Ben-Gurion requested opinions on "mihu Yehudi" ("Who is a Jew") from Jewish religious authorities and intellectuals worldwide in order to settle citizenship questions. This is still not settled, and occasionally resurfaces in Israeli politics.

Historical definitions of Jewish identity have traditionally been based on "halakhic" definitions of matrilineal descent, and halakhic conversions. Historical definitions of who is a Jew date back to the codification of the Oral Torah into the Babylonian Talmud, around 200 CE. Interpretations of sections of the Tanakh, such as Deuteronomy 7:1–5, by Jewish sages, are used as a warning against intermarriage between Jews and Canaanites because "[the non-Jewish husband] will cause your child to turn away from Me and they will worship the gods (i.e., idols) of others." says that the son in a marriage between a Hebrew woman and an Egyptian man is "of the community of Israel." This is complemented by , where Israelites returning from Babylon vow to put aside their gentile wives and their children. A popular theory is that the rape of Jewish women in captivity brought about the law of Jewish identity being inherited through the maternal line, although scholars challenge this theory citing the Talmudic establishment of the law from the pre-exile period. Since the anti-religious "Haskalah" movement of the late 18th and 19th centuries, "halakhic" interpretations of Jewish identity have been challenged.

The total number of Jews worldwide is difficult to assess because the definition of "who is a Jew" is problematic; not all Jews identify themselves as Jewish, and some who identify as Jewish are not considered so by other Jews. According to the "Jewish Year Book" (1901), the global Jewish population in 1900 was around 11 million. The latest available data is from the World Jewish Population Survey of 2002 and the Jewish Year Calendar (2005). In 2002, according to the Jewish Population Survey, there were 13.3 million Jews around the world. The Jewish Year Calendar cites 14.6 million. Jewish population growth is currently near zero percent, with 0.3% growth from 2000 to 2001.

Rabbinic Judaism (or in some Christian traditions, Rabbinism) (Hebrew: "Yahadut Rabanit" – יהדות רבנית) has been the mainstream form of Judaism since the 6th century CE, after the codification of the Talmud. It is characterised by the belief that the Written Torah (Written Law) cannot be correctly interpreted without reference to the Oral Torah and the voluminous literature specifying what behavior is sanctioned by the Law.

The Jewish Enlightenment of the late 18th century resulted in the division of Ashkenazi (Western) Jewry into religious movements or denominations, especially in North America and Anglophone countries. The main denominations today outside Israel (where the situation is rather different) are Orthodox, Conservative, and Reform.

While traditions and customs (see also "Sephardic law and customs") vary between discrete communities, it can be said that Sephardi and Mizrahi Jewish communities do not generally adhere to the "movement" framework popular in and among Ashkenazi Jewry. Historically, Sephardi and Mizrahi communities have eschewed denominations in favour of a "big tent" approach. This is particularly the case in contemporary Israel, which is home to the largest communities of Sephardi and Mizrahi Jews in the world. (However, individual Sephardi and Mizrahi Jews may be members of or attend synagogues that do adhere to one Ashkenazi-inflected movement or another.) 

Sephardi and Mizrahi observance of Judaism tends toward the conservative, and prayer rites are reflective of this, with the text of each rite being largely unchanged since their respective inception. Observant Sephardim may follow the teachings of a particular rabbi or school of thought; for example, the Sephardic Chief Rabbi of Israel.

Most Jewish Israelis classify themselves as "secular" ("hiloni"), "traditional" ("masorti"), "religious" ("dati") or "Haredi". The term "secular" is more popular as a self-description among Israeli families of western (European) origin, whose Jewish identity may be a very powerful force in their lives, but who see it as largely independent of traditional religious belief and practice. This portion of the population largely ignores organized religious life, be it of the official Israeli rabbinate (Orthodox) or of the liberal movements common to diaspora Judaism (Reform, Conservative).

The term "traditional" ("masorti") is most common as a self-description among Israeli families of "eastern" origin (i.e., the Middle East, Central Asia, and North Africa). This term, as commonly used, has nothing to do with the Conservative Judaism, which also names itself "Masorti" outside North America. There is a great deal of ambiguity in the ways "secular" and "traditional" are used in Israel: they often overlap, and they cover an extremely wide range in terms of worldview and practical religious observance. The term "Orthodox" is not popular in Israeli discourse, although the percentage of Jews who come under that category is far greater than in the diaspora. What would be called "Orthodox" in the diaspora includes what is commonly called "dati" (religious) or "haredi" (ultra-Orthodox) in Israel. The former term includes what is called "Religious Zionism" or the "National Religious" community, as well as what has become known over the past decade or so as "haredi-leumi" (nationalist "haredi"), or "Hardal", which combines a largely "haredi" lifestyle with nationalist ideology. (Some people, in Yiddish, also refer to observant Orthodox Jews as "frum", as opposed to "frei" (more liberal Jews)).

"Haredi" applies to a populace that can be roughly divided into three separate groups along both ethnic and ideological lines: (1) "Lithuanian" (non-hasidic) "haredim" of Ashkenazic origin; (2) Hasidic "haredim" of Ashkenazic origin; and (3) Sephardic "haredim".

Karaite Judaism defines itself as the remnants of the non-Rabbinic Jewish sects of the Second Temple period, such as the Sadducees. The Karaites ("Scripturalists") accept only the Hebrew Bible and what they view as the Peshat ("simple" meaning); they do not accept non-biblical writings as authoritative. Some European Karaites do not see themselves as part of the Jewish community at all, although most do.

The Samaritans, a very small community located entirely around Mount Gerizim in the Nablus/Shechem region of the West Bank and in Holon, near Tel Aviv in Israel, regard themselves as the descendants of the Israelites of the Iron Age kingdom of Israel. Their religious practices are based on the literal text of the written Torah (Five Books of Moses), which they view as the only authoritative scripture (with a special regard also for the Samaritan Book of Joshua).

"See also: Haymanot; Beta Israel."

Haymanot (meaning "religion" in Ge'ez and Amharic) refers the Judaism practiced by Ethiopian Jews. This version of Judaism differs substantially from Rabbinic, Karaite, and Samaritan Judaisms, Ethiopian Jews having diverged from their coreligionists earlier. Sacred scriptures (the Orit) are written in Ge'ez, not Hebrew, and dietary laws are based strictly on the text of the Orit, without explication from ancillary commentaries. Holidays also differ, with some Rabbinic holidays not observed in Ethiopian Jewish communities, and some additional holidays, like Sigd.

Jewish ethics may be guided by halakhic traditions, by other moral principles, or by central Jewish virtues. Jewish ethical practice is typically understood to be marked by values such as justice, truth, peace, loving-kindness (chesed), compassion, humility, and self-respect. Specific Jewish ethical practices include practices of charity (tzedakah) and refraining from negative speech (lashon hara). Proper ethical practices regarding sexuality and many other issues are subjects of dispute among Jews.

Traditionally, Jews recite prayers three times daily, Shacharit, Mincha, and Ma'ariv with a fourth prayer, Mussaf added on Shabbat and holidays. At the heart of each service is the "Amidah" or "Shemoneh Esrei". Another key prayer in many services is the declaration of faith, the "Shema Yisrael" (or "Shema"). The "Shema" is the recitation of a verse from the Torah (Deuteronomy 6:4): "Shema Yisrael Adonai Eloheinu Adonai Echad"—"Hear, O Israel! The Lord is our God! The Lord is One!"

Most of the prayers in a traditional Jewish service can be recited in solitary prayer, although communal prayer is preferred. Communal prayer requires a quorum of ten adult Jews, called a "minyan". In nearly all Orthodox and a few Conservative circles, only male Jews are counted toward a "minyan"; most Conservative Jews and members of other Jewish denominations count female Jews as well.

In addition to prayer services, observant traditional Jews recite prayers and benedictions throughout the day when performing various acts. Prayers are recited upon waking up in the morning, before eating or drinking different foods, after eating a meal, and so on.

The approach to prayer varies among the Jewish denominations. Differences can include the texts of prayers, the frequency of prayer, the number of prayers recited at various religious events, the use of musical instruments and choral music, and whether prayers are recited in the traditional liturgical languages or the vernacular. In general, Orthodox and Conservative congregations adhere most closely to tradition, and Reform and Reconstructionist synagogues are more likely to incorporate translations and contemporary writings in their services. Also, in most Conservative synagogues, and all Reform and Reconstructionist congregations, women participate in prayer services on an equal basis with men, including roles traditionally filled only by men, such as reading from the Torah. In addition, many Reform temples use musical accompaniment such as organs and mixed choirs.

A "kippah" (Hebrew: כִּפָּה, plural "kippot"; Yiddish: יאַרמלקע, "yarmulke") is a slightly rounded brimless skullcap worn by many Jews while praying, eating, reciting blessings, or studying Jewish religious texts, and at all times by some Jewish men. In Orthodox communities, only men wear kippot; in non-Orthodox communities, some women also wear kippot. "Kippot" range in size from a small round beanie that covers only the back of the head to a large, snug cap that covers the whole crown.

"Tzitzit" (Hebrew: צִיציִת) (Ashkenazi pronunciation: "tzitzis") are special knotted "fringes" or "tassels" found on the four corners of the "tallit" (Hebrew: טַלִּית) (Ashkenazi pronunciation: "tallis"), or prayer shawl. The "tallit" is worn by Jewish men and some Jewish women during the prayer service. Customs vary regarding when a Jew begins wearing a tallit. In the Sephardi community, boys wear a tallit from bar mitzvah age. In some Ashkenazi communities, it is customary to wear one only after marriage. A "tallit katan" (small tallit) is a fringed garment worn under the clothing throughout the day. In some Orthodox circles, the fringes are allowed to hang freely outside the clothing.

Tefillin (Hebrew: תְפִלִּין), known in English as phylacteries (from the Greek word φυλακτήριον, meaning "safeguard" or "amulet"), are two square leather boxes containing biblical verses, attached to the forehead and wound around the left arm by leather straps. They are worn during weekday morning prayer by observant Jewish men and some Jewish women.

A "kittel" (Yiddish: קיטל), a white knee-length overgarment, is worn by prayer leaders and some observant traditional Jews on the High Holidays. It is traditional for the head of the household to wear a kittel at the Passover seder in some communities, and some grooms wear one under the wedding canopy. Jewish males are buried in a "tallit" and sometimes also a "kittel" which are part of the "tachrichim" (burial garments).

Jewish holidays are special days in the Jewish calendar, which celebrate moments in Jewish history, as well as central themes in the relationship between God and the world, such as creation, revelation, and redemption.

"Shabbat", the weekly day of rest lasting from shortly before sundown on Friday night to nightfall on Saturday night, commemorates God's day of rest after six days of creation. It plays a pivotal role in Jewish practice and is governed by a large corpus of religious law. At sundown on Friday, the woman of the house welcomes the Shabbat by lighting two or more candles and reciting a blessing. The evening meal begins with the Kiddush, a blessing recited aloud over a cup of wine, and the Mohtzi, a blessing recited over the bread. It is customary to have challah, two braided loaves of bread, on the table. During Shabbat, Jews are forbidden to engage in any activity that falls under 39 categories of "melakhah", translated literally as "work". In fact the activities banned on the Sabbath are not "work" in the usual sense: They include such actions as lighting a fire, writing, using money and carrying in the public domain. The prohibition of lighting a fire has been extended in the modern era to driving a car, which involves burning fuel and using electricity.

Jewish holy days ("chaggim"), celebrate landmark events in Jewish history, such as the Exodus from Egypt and the giving of the Torah, and sometimes mark the change of seasons and transitions in the agricultural cycle. The three major festivals, Sukkot, Passover and Shavuot, are called "regalim" (derived from the Hebrew word "regel", or foot). On the three regalim, it was customary for the Israelites to make pilgrimages to Jerusalem to offer sacrifices in the Temple.

The High Holidays ("Yamim Noraim" or "Days of Awe") revolve around judgment and forgiveness.

Purim (Hebrew: "Pûrîm" "lots") is a joyous Jewish holiday that commemorates the deliverance of the Persian Jews from the plot of the evil Haman, who sought to exterminate them, as recorded in the biblical Book of Esther. It is characterized by public recitation of the Book of Esther, mutual gifts of food and drink, charity to the poor, and a celebratory meal (Esther 9:22). Other customs include drinking wine, eating special pastries called hamantashen, dressing up in masks and costumes, and organizing carnivals and parties.

Purim has celebrated annually on the 14th of the Hebrew month of Adar, which occurs in February or March of the Gregorian calendar.

Hanukkah (, "dedication") also known as the Festival of Lights, is an eight-day Jewish holiday that starts on the 25th day of Kislev (Hebrew calendar). The festival is observed in Jewish homes by the kindling of lights on each of the festival's eight nights, one on the first night, two on the second night and so on.

The holiday was called Hanukkah (meaning "dedication") because it marks the re-dedication of the Temple after its desecration by Antiochus IV Epiphanes. Spiritually, Hanukkah commemorates the "Miracle of the Oil". According to the Talmud, at the re-dedication of the Temple in Jerusalem following the victory of the Maccabees over the Seleucid Empire, there was only enough consecrated oil to fuel the eternal flame in the Temple for one day. Miraculously, the oil burned for eight days – which was the length of time it took to press, prepare and consecrate new oil.

Hanukkah is not mentioned in the Bible and was never considered a major holiday in Judaism, but it has become much more visible and widely celebrated in modern times, mainly because it falls around the same time as Christmas and has national Jewish overtones that have been emphasized since the establishment of the State of Israel.

Tisha B'Av ( or , "the Ninth of Av") is a day of mourning and fasting commemorating the destruction of the First and Second Temples, and in later times, the expulsion of the Jews from Spain.

There are three more minor Jewish fast days that commemorate various stages of the destruction of the Temples. They are the 17th Tamuz, the 10th of Tevet and Tzom Gedaliah (the 3rd of Tishrei).

The modern holidays of Yom Ha-shoah (Holocaust Remembrance Day), Yom Hazikaron (Israeli Memorial Day) and Yom Ha'atzmaut (Israeli Independence Day) commemorate the horrors of the Holocaust, the fallen soldiers of Israel and victims of terrorism, and Israeli independence, respectively.

There are some who prefer to commemorate those who were killed in the Holocaust on the 10th of Tevet.

The core of festival and Shabbat prayer services is the public reading of the Torah, along with connected readings from the other books of the Tanakh, called Haftarah. Over the course of a year, the whole Torah is read, with the cycle starting over in the autumn, on Simchat Torah.

Synagogues are Jewish houses of prayer and study. They usually contain separate rooms for prayer (the main sanctuary), smaller rooms for study, and often an area for community or educational use. There is no set blueprint for synagogues and the architectural shapes and interior designs of synagogues vary greatly. The Reform movement mostly refer to their synagogues as temples. Some traditional features of a synagogue are:

In addition to synagogues, other buildings of significance in Judaism include yeshivas, or institutions of Jewish learning, and mikvahs, which are ritual baths.

The Jewish dietary laws are known as "kashrut". Food prepared in accordance with them is termed kosher, and food that is not kosher is also known as "treifah" or "treif". People who observe these laws are colloquially said to be "keeping kosher".

Many of the laws apply to animal-based foods. For example, in order to be considered kosher, mammals must have split hooves and chew their cud. The pig is arguably the most well-known example of a non-kosher animal. Although it has split hooves, it does not chew its cud. For seafood to be kosher, the animal must have fins and scales. Certain types of seafood, such as shellfish, crustaceans, and eels, are therefore considered non-kosher. Concerning birds, a list of non-kosher species is given in the Torah. The exact translations of many of the species have not survived, and some non-kosher birds' identities are no longer certain. However, traditions exist about the "kashrut" status of a few birds. For example, both chickens and turkeys are permitted in most communities. Other types of animals, such as amphibians, reptiles, and most insects, are prohibited altogether.

In addition to the requirement that the species be considered kosher, meat and poultry (but not fish) must come from a healthy animal slaughtered in a process known as "shechitah". Without the proper slaughtering practices even an otherwise kosher animal will be rendered "treif". The slaughtering process is intended to be quick and relatively painless to the animal. Forbidden parts of animals include the blood, some fats, and the area in and around the sciatic nerve.

Jewish law also forbids the consumption of meat and dairy products together. The waiting period between eating meat and eating dairy varies by the order in which they are consumed and by community, and can extend for up to six hours. Based on the Biblical injunction against cooking a kid in its mother's milk, this rule is mostly derived from the Oral Torah, the Talmud and Rabbinic law. Chicken and other kosher birds are considered the same as meat under the laws of "kashrut", but the prohibition is Rabbinic, not Biblical.

The use of dishes, serving utensils, and ovens may make food "treif" that would otherwise be kosher. Utensils that have been used to prepare non-kosher food, or dishes that have held meat and are now used for dairy products, render the food "treif" under certain conditions.

Furthermore, all Orthodox and some Conservative authorities forbid the consumption of processed grape products made by non-Jews, due to ancient pagan practices of using wine in rituals. Some Conservative authorities permit wine and grape juice made without rabbinic supervision.

The Torah does not give specific reasons for most of the laws of "kashrut". However, a number of explanations have been offered, including maintaining ritual purity, teaching impulse control, encouraging obedience to God, improving health, reducing cruelty to animals and preserving the distinctness of the Jewish community. The various categories of dietary laws may have developed for different reasons, and some may exist for multiple reasons. For example, people are forbidden from consuming the blood of birds and mammals because, according to the Torah, this is where animal souls are contained. In contrast, the Torah forbids Israelites from eating non-kosher species because "they are unclean". The Kabbalah describes sparks of holiness that are released by the act of eating kosher foods, but are too tightly bound in non-kosher foods to be released by eating.

Survival concerns supersede all the laws of "kashrut", as they do for most halakhot.

The Tanakh describes circumstances in which a person who is "tahor" or ritually pure may become "tamei" or ritually impure. Some of these circumstances are contact with human corpses or graves, seminal flux, vaginal flux, menstruation, and contact with people who have become impure from any of these. In Rabbinic Judaism, Kohanim, members of the hereditary caste that served as priests in the time of the Temple, are mostly restricted from entering grave sites and touching dead bodies. During the Temple period, such priests (Kohanim) were required to eat their bread offering (Terumah) in a state of ritual purity, which laws eventually led to more rigid laws being enacted, such as hand-washing which became a requisite of all Jews before consuming ordinary bread.

An important subcategory of the ritual purity laws relates to the segregation of menstruating women. These laws are also known as "niddah", literally "separation", or family purity. Vital aspects of halakha for traditionally observant Jews, they are not usually followed by Jews in liberal denominations.

Especially in Orthodox Judaism, the Biblical laws are augmented by Rabbinical injunctions. For example, the Torah mandates that a woman in her normal menstrual period must abstain from sexual intercourse for seven days. A woman whose menstruation is prolonged must continue to abstain for seven more days after bleeding has stopped. The Rabbis conflated ordinary "niddah" with this extended menstrual period, known in the Torah as "zavah", and mandated that a woman may not have sexual intercourse with her husband from the time she begins her menstrual flow until seven days after it ends. In addition, Rabbinical law forbids the husband from touching or sharing a bed with his wife during this period. Afterwards, purification can occur in a ritual bath called a mikveh.

Traditional Ethiopian Jews keep menstruating women in separate huts and, similar to Karaite practice, do not allow menstruating women into their temples because of a temple's special sanctity. Emigration to Israel and the influence of other Jewish denominations have led to Ethiopian Jews adopting more normative Jewish practices.

Life-cycle events, or rites of passage, occur throughout a Jew's life that serves to strengthen Jewish identity and bind him/her to the entire community.

The role of the priesthood in Judaism has significantly diminished since the destruction of the Second Temple in 70 CE when priests attended to the Temple and sacrifices. The priesthood is an inherited position, and although priests no longer have any but ceremonial duties, they are still honored in many Jewish communities. Many Orthodox Jewish communities believe that they will be needed again for a future Third Temple and need to remain in readiness for future duty.

From the time of the Mishnah and Talmud to the present, Judaism has required specialists or authorities for the practice of very few rituals or ceremonies. A Jew can fulfill most requirements for prayer by himself. Some activities—reading the Torah and "haftarah" (a supplementary portion from the Prophets or Writings), the prayer for mourners, the blessings for bridegroom and bride, the complete grace after meals—require a "minyan", the presence of ten Jews.

The most common professional clergy in a synagogue are:

Jewish prayer services do involve two specified roles, which are sometimes, but not always, filled by a rabbi or hazzan in many congregations. In other congregations these roles are filled on an ad-hoc basis by members of the congregation who lead portions of services on a rotating basis:

Many congregations, especially larger ones, also rely on a:

The three preceding positions are usually voluntary and considered an honor. Since the Enlightenment large synagogues have often adopted the practice of hiring rabbis and hazzans to act as "shatz" and "baal kriyah", and this is still typically the case in many Conservative and Reform congregations. However, in most Orthodox synagogues these positions are filled by laypeople on a rotating or ad-hoc basis. Although most congregations hire one or more Rabbis, the use of a professional hazzan is generally declining in American congregations, and the use of professionals for other offices is rarer still.


At its core, the Tanakh is an account of the Israelites' relationship with God from their earliest history until the building of the Second Temple (c. 535 BCE). Abraham is hailed as the first Hebrew and the father of the Jewish people. As a reward for his act of faith in one God, he was promised that Isaac, his second son, would inherit the Land of Israel (then called Canaan). Later, the descendants of Isaac's son Jacob were enslaved in Egypt, and God commanded Moses to lead the Exodus from Egypt. At Mount Sinai, they received the Torah—the five books of Moses. These books, together with Nevi'im and Ketuvim are known as "Torah Shebikhtav" as opposed to the Oral Torah, which refers to the Mishnah and the Talmud. Eventually, God led them to the land of Israel where the tabernacle was planted in the city of Shiloh for over 300 years to rally the nation against attacking enemies. As time went on, the spiritual level of the nation declined to the point that God allowed the Philistines to capture the tabernacle. The people of Israel then told Samuel the prophet that they needed to be governed by a permanent king, and Samuel appointed Saul to be their King. When the people pressured Saul into going against a command conveyed to him by Samuel, God told Samuel to appoint David in his stead.

Once King David was established, he told the prophet Nathan that he would like to build a permanent temple, and as a reward for his actions, God promised David that he would allow his son, Solomon, to build the First Temple and the throne would never depart from his children.

Rabbinic tradition holds that the details and interpretation of the law, which are called the "Oral Torah" or "oral law", were originally an unwritten tradition based upon what God told Moses on Mount Sinai. However, as the persecutions of the Jews increased and the details were in danger of being forgotten, these oral laws were recorded by Rabbi Judah HaNasi (Judah the Prince) in the Mishnah, redacted "circa" 200 CE. The Talmud was a compilation of both the Mishnah and the Gemara, rabbinic commentaries redacted over the next three centuries. The Gemara originated in two major centers of Jewish scholarship, Palestine and Babylonia. Correspondingly, two bodies of analysis developed, and two works of Talmud were created. The older compilation is called the Jerusalem Talmud. It was compiled sometime during the 4th century in Palestine. The Babylonian Talmud was compiled from discussions in the houses of study by the scholars Ravina I, Ravina II, and Rav Ashi by 500 CE, although it continued to be edited later.

Some critical scholars oppose the view that the sacred texts, including the Hebrew Bible, were divinely inspired. Many of these scholars accept the general principles of the documentary hypothesis and suggest that the Torah consists of inconsistent texts edited together in a way that calls attention to divergent accounts. Many suggest that during the First Temple period, the people of Israel believed that each nation had its own god, but that their god was superior to other gods. Some suggest that strict monotheism developed during the Babylonian Exile, perhaps in reaction to Zoroastrian dualism. In this view, it was only by the Hellenic period that most Jews came to believe that their god was the only god and that the notion of a clearly bounded Jewish nation identical with the Jewish religion formed.

John Day argues that the origins of biblical Yahweh, El, Asherah, and Ba'al, may be rooted in earlier Canaanite religion, which was centered on a pantheon of gods much like the Greek pantheon.

According to the Hebrew Bible, the United Monarchy was established under Saul and continued under King David and Solomon with its capital in Jerusalem. After Solomon's reign, the nation split into two kingdoms, the Kingdom of Israel (in the north) and the Kingdom of Judah (in the south). The Kingdom of Israel was conquered by the Assyrian ruler Sargon II in the late 8th century BCE with many people from the capital Samaria being taken captive to Media and the Khabur River valley. The Kingdom of Judah continued as an independent state until it was conquered by a Babylonian army in the early 6th century BCE, destroying the First Temple that was at the center of ancient Jewish worship. The Judean elite was exiled to Babylonia and this is regarded as the first Jewish Diaspora. Later many of them returned to their homeland after the subsequent conquest of Babylonia by the Persians seventy years later, a period known as the Babylonian Captivity. A new Second Temple was constructed, and old religious practices were resumed.

During the early years of the Second Temple, the highest religious authority was a council known as the Great Assembly, led by Ezra of the Book of Ezra. Among other accomplishments of the Great Assembly, the last books of the Bible were written at this time and the canon sealed.

Hellenistic Judaism spread to Ptolemaic Egypt from the 3rd century BCE. After the Great Revolt (66–73 CE), the Romans destroyed the Temple. Hadrian built a pagan idol on the Temple grounds and prohibited circumcision; these acts of ethnocide provoked the Bar Kokhba revolt 132–136 CE after which the Romans banned the study of the Torah and the celebration of Jewish holidays, and forcibly removed virtually all Jews from Judea. In 200 CE, however, Jews were granted Roman citizenship and Judaism was recognized as a "religio licita" ("legitimate religion") until the rise of Gnosticism and Early Christianity in the fourth century.

Following the destruction of Jerusalem and the expulsion of the Jews, Jewish worship stopped being centrally organized around the Temple, prayer took the place of sacrifice, and worship was rebuilt around the community (represented by a minimum of ten adult men) and the establishment of the authority of rabbis who acted as teachers and leaders of individual communities (see Jewish diaspora).

Around the 1st century CE, there were several small Jewish sects: the Pharisees, Sadducees, Zealots, Essenes, and Christians. After the destruction of the Second Temple in 70 CE, these sects vanished. Christianity survived, but by breaking with Judaism and becoming a separate religion; the Pharisees survived but in the form of Rabbinic Judaism (today, known simply as "Judaism"). The Sadducees rejected the divine inspiration of the Prophets and the Writings, relying only on the Torah as divinely inspired. Consequently, a number of other core tenets of the Pharisees' belief system (which became the basis for modern Judaism), were also dismissed by the Sadducees. (The Samaritans practiced a similar religion, which is traditionally considered separate from Judaism.)

Like the Sadducees who relied only on the Torah, some Jews in the 8th and 9th centuries rejected the authority and divine inspiration of the oral law as recorded in the Mishnah (and developed by later rabbis in the two Talmuds), relying instead only upon the Tanakh. These included the Isunians, the Yudganites, the Malikites, and others. They soon developed oral traditions of their own, which differed from the rabbinic traditions, and eventually formed the Karaite sect. Karaites exist in small numbers today, mostly living in Israel. Rabbinical and Karaite Jews each hold that the others are Jews, but that the other faith is erroneous.

Over a long time, Jews formed distinct ethnic groups in several different geographic areas—amongst others, the Ashkenazi Jews (of central and Eastern Europe), the Sephardi Jews (of Spain, Portugal, and North Africa), the Beta Israel of Ethiopia, and the Yemenite Jews from the southern tip of the Arabian Peninsula. Many of these groups have developed differences in their prayers, traditions and accepted canons; however, these distinctions are mainly the result of their being formed at some cultural distance from normative (rabbinic) Judaism, rather than based on any doctrinal dispute.

Antisemitism arose during the Middle Ages, in the form of persecutions, pogroms, forced conversions, expulsions, social restrictions and ghettoization.

This was different in quality from the repressions of Jews which had occurred in ancient times. Ancient repressions were politically motivated and Jews were treated the same as members of other ethnic groups. With the rise of the Churches, the main motive for attacks on Jews changed from politics to religion and the religious motive for such attacks was specifically derived from Christian views about Jews and Judaism. During the Middle Ages, Jewish people who lived under Muslim rule generally experienced tolerance and integration, but there were occasional outbreaks of violence like Almohad's persecutions.

Hasidic Judaism was founded by Yisroel ben Eliezer (1700–1760), also known as the "Ba'al Shem Tov" (or "Besht"). It originated in a time of persecution of the Jewish people when European Jews had turned inward to Talmud study; many felt that most expressions of Jewish life had become too "academic", and that they no longer had any emphasis on spirituality or joy. Its adherents favored small and informal gatherings called Shtiebel, which, in contrast to a traditional synagogue, could be used both as a place of worship and for celebrations involving dancing, eating, and socializing. Ba'al Shem Tov's disciples attracted many followers; they themselves established numerous Hasidic sects across Europe. Unlike other religions, which typically expanded through word of mouth or by use of print, Hasidism spread largely owing to Tzadiks, who used their influence to encourage others to follow the movement. Hasidism appealed to many Europeans because it was easy to learn, did not require full immediate commitment, and presented a compelling spectacle. Hasidic Judaism eventually became the way of life for many Jews in Eastern Europe. Waves of Jewish immigration in the 1880s carried it to the United States. The movement itself claims to be nothing new, but a "refreshment" of original Judaism. As some have put it: ""they merely re-emphasized that which the generations had lost"". Nevertheless, early on there was a serious schism between Hasidic and non-Hasidic Jews. European Jews who rejected the Hasidic movement were dubbed by the Hasidim as Misnagdim, (lit. "opponents"). Some of the reasons for the rejection of Hasidic Judaism were the exuberance of Hasidic worship, its deviation from tradition in ascribing infallibility and miracles to their leaders, and the concern that it might become a messianic sect. Over time differences between the Hasidim and their opponents have slowly diminished and both groups are now considered part of Haredi Judaism.

In the late 18th century CE, Europe was swept by a group of intellectual, social and political movements known as the Enlightenment. The Enlightenment led to reductions in the European laws that prohibited Jews to interact with the wider secular world, thus allowing Jews access to secular education and experience. A parallel Jewish movement, Haskalah or the "Jewish Enlightenment", began, especially in Central Europe and Western Europe, in response to both the Enlightenment and these new freedoms. It placed an emphasis on integration with secular society and a pursuit of non-religious knowledge through reason. With the promise of political emancipation, many Jews saw no reason to continue to observe Jewish law and increasing numbers of Jews assimilated into Christian Europe. Modern religious movements of Judaism all formed in reaction to this trend.

In Central Europe, followed by Great Britain and the United States, Reform (or Liberal) Judaism developed, relaxing legal obligations (especially those that limited Jewish relations with non-Jews), emulating Protestant decorum in prayer, and emphasizing the ethical values of Judaism's Prophetic tradition. Modern Orthodox Judaism developed in reaction to Reform Judaism, by leaders who argued that Jews could participate in public life as citizens equal to Christians while maintaining the observance of Jewish law. Meanwhile, in the United States, wealthy Reform Jews helped European scholars, who were Orthodox in practice but critical (and skeptical) in their study of the Bible and Talmud, to establish a seminary to train rabbis for immigrants from Eastern Europe. These left-wing Orthodox rabbis were joined by right-wing Reform rabbis who felt that Jewish law should not be entirely abandoned, to form the Conservative movement. Orthodox Jews who opposed the Haskalah formed Haredi Orthodox Judaism. After massive movements of Jews following The Holocaust and the creation of the state of Israel, these movements have competed for followers from among traditional Jews in or from other countries.

Countries such as the United States, Israel, Canada, United Kingdom, Argentina and South Africa contain large Jewish populations. Jewish religious practice varies widely through all levels of observance. According to the 2001 edition of the National Jewish Population Survey, in the United States' Jewish community—the world's second largest—4.3 million Jews out of 5.1 million had some sort of connection to the religion. Of that population of connected Jews, 80% participated in some sort of Jewish religious observance, but only 48% belonged to a congregation, and fewer than 16% attend regularly.

Birth rates for American Jews have dropped from 2.0 to 1.7. (Replacement rate is 2.1.) Intermarriage rates range from 40–50% in the US, and only about a third of children of intermarried couples are raised as Jews. Due to intermarriage and low birth rates, the Jewish population in the US shrank from 5.5 million in 1990 to 5.1 million in 2001. This is indicative of the general population trends among the Jewish community in the Diaspora, but a focus on total population obscures growth trends in some denominations and communities, such as Haredi Judaism. The Baal teshuva movement is a movement of Jews who have "returned" to religion or become more observant.

Christianity was originally a sect of Second Temple Judaism, but the two religions diverged in the first century. The differences between Christianity and Judaism originally centered on whether Jesus was the Jewish Messiah but eventually became irreconcilable. Major differences between the two faiths include the nature of the Messiah, of atonement and sin, the status of God's commandments to Israel, and perhaps most significantly of the nature of God himself. Due to these differences, Judaism traditionally regards Christianity as Shituf or worship of the God of Israel which is not monotheistic. Christianity has traditionally regarded Judaism as obsolete with the invention of Christianity and Jews as a people replaced by the Church, though a Christian belief in dual-covenant theology emerged as a phenomenon following Christian reflection on how their theology influenced the Nazi Holocaust.

Since the time of the Middle Ages, the Catholic Church upheld the "Constitution pro Judæis" (Formal Statement on the Jews), which stated 

Until their emancipation in the late 18th and the 19th century, Jews in Christian lands were subject to humiliating legal restrictions and limitations. They included provisions requiring Jews to wear specific and identifying clothing such as the Jewish hat and the yellow badge, restricting Jews to certain cities and towns or in certain parts of towns (ghettos), and forbidding Jews to enter certain trades (for example selling new clothes in medieval Sweden). Disabilities also included special taxes levied on Jews, exclusion from public life, restraints on the performance of religious ceremonies, and linguistic censorship. Some countries went even further and completely expelled Jews, for example, England in 1290 (Jews were readmitted in 1655) and Spain in 1492 (readmitted in 1868). The first Jewish settlers in North America arrived in the Dutch colony of New Amsterdam in 1654; they were forbidden to hold public office, open a retail shop, or establish a synagogue. When the colony was seized by the British in 1664 Jewish rights remained unchanged, but by 1671 Asser Levy was the first Jew to serve on a jury in North America.
In 1791, Revolutionary France was the first country to abolish disabilities altogether, followed by Prussia in 1848. Emancipation of the Jews in the United Kingdom was achieved in 1858 after an almost 30-year struggle championed by Isaac Lyon Goldsmid with the ability of Jews to sit in parliament with the passing of the Jews Relief Act 1858. The newly united German Empire in 1871 abolished Jewish disabilities in Germany, which were reinstated in the Nuremberg Laws in 1935.

Jewish life in Christian lands was marked by frequent blood libels, expulsions, forced conversions and massacres. An underlying source of prejudice against Jews in Europe was religious. Christian rhetoric and antipathy towards Jews developed in the early years of Christianity and was reinforced by ever increasing anti-Jewish measures over the ensuing centuries. The action taken by Christians against Jews included acts of violence, and murder culminating in the Holocaust. These attitudes were reinforced in Christian preaching, art and popular teaching for two millennia, containing contempt for Jews, as well as statutes which were designed to humiliate and stigmatise Jews. The Nazi Party was known for its persecution of Christian Churches; many of them, such as the Protestant Confessing Church and the Catholic Church, as well as Quakers and Jehovah's Witnesses, aided and rescued Jews who were being targeted by the antireligious régime.

The attitude of Christians and Christian Churches toward the Jewish people and Judaism, have been changed mostly positive since World War II. Pope John Paul II and the Catholic Church have "upheld the Church's acceptance of the continuing and permanent election of the Jewish people" as well as a reaffirmation of the covenant between God and the Jews. In December 2015, the Vatican released a 10,000-word document that, among other things, stated that Catholics should work with Jews to fight antisemitism.

Both Judaism and Islam arose from the patriarch Abraham, and they are therefore considered Abrahamic religions. In both Jewish and Muslim tradition, the Jewish and Arab peoples are descended from the two sons of Abraham—Isaac and Ishmael, respectively. While both religions are monotheistic and share many commonalities, they differ based on the fact that Jews do not consider Jesus or Muhammad to be prophets. The religions' adherents have interacted with each other since the 7th century when Islam originated and spread in the Arabian peninsula. Indeed, the years 712 to 1066 CE under the Ummayad and the Abbasid rulers have been called the Golden age of Jewish culture in Spain. Non-Muslim monotheists living in these countries, including Jews, were known as dhimmis. Dhimmis were allowed to practice their own religions and administer their own internal affairs, but they were subject to certain restrictions that were not imposed on Muslims. For example, they had to pay the jizya, a per capita tax imposed on free adult non-Muslim males, and they were also forbidden to bear arms or testify in court cases involving Muslims. Many of the laws regarding dhimmis were highly symbolic. For example, dhimmis in some countries were required to wear distinctive clothing, a practice not found in either the Qur'an or the hadiths but invented in early medieval Baghdad and inconsistently enforced. Jews in Muslim countries were not entirely free from persecution—for example, many were killed, exiled or forcibly converted in the 12th century, in Persia, and by the rulers of the Almohad dynasty in North Africa and Al-Andalus, as well as by the Zaydi imams of Yemen in the 17th century (see: Mawza Exile). At times, Jews were also restricted in their choice of residence—in Morocco, for example, Jews were confined to walled quarters (mellahs) beginning in the 15th century and increasingly since the early 19th century.

In the mid-20th century, Jews were expelled from nearly all of the Arab countries. Most have chosen to live in Israel. Today, antisemitic themes including Holocaust denial have become commonplace in the propaganda of Islamic movements such as Hizbullah and Hamas, in the pronouncements of various agencies of the Islamic Republic of Iran, and even in the newspapers and other publications of Refah Partisi.

There are some movements that combine elements of Judaism with those of other religions. The most well-known of these is Messianic Judaism, a religious movement, which arose in the 1960s, that incorporates elements of Judaism with the tenets of Christianity. The movement generally states that Jesus is the Jewish Messiah, that he is one of the Three Divine Persons, and that salvation is only achieved through acceptance of Jesus as one's savior. Some members of the movement argue that Messianic Judaism is a sect of Judaism. Jewish organizations of every denomination reject this, stating that Messianic Judaism is a Christian sect, because it teaches creeds which are identical to those of Pauline Christianity.

Other examples of syncretism include Semitic neopaganism, a loosely organized sect which incorporates pagan or Wiccan beliefs with some Jewish religious practices; Jewish Buddhists, another loosely organized group that incorporates elements of Asian spirituality in their faith; and some Renewal Jews who borrow freely and openly from Buddhism, Sufism, Native American religions, and other faiths.

The Kabbalah Centre, which employs teachers from multiple religions, is a New Age movement that claims to popularize the kabbalah, part of the Jewish esoteric tradition.



Jews in Islamic countries:










See also Torah database for links to more Judaism e-texts.


Text study projects at . In many instances, the Hebrew versions of these projects are more fully developed than the English.


</doc>
<doc id="15626" url="https://en.wikipedia.org/wiki?curid=15626" title="John Stuart Mill">
John Stuart Mill

John Stuart Mill (20 May 1806 – 8 May 1873), usually cited as J. S. Mill, was a British philosopher, political economist, and civil servant. One of the most influential thinkers in the history of liberalism, he contributed widely to social theory, political theory, and political economy. Dubbed "the most influential English-speaking philosopher of the nineteenth century", Mill's conception of liberty justified the freedom of the individual in opposition to unlimited state and social control.

Mill was a proponent of utilitarianism, an ethical theory developed by his predecessor Jeremy Bentham. He contributed to the investigation of scientific methodology, though his knowledge of the topic was based on the writings of others, notably William Whewell, John Herschel, and Auguste Comte, and research carried out for Mill by Alexander Bain. Mill engaged in written debate with Whewell.

A member of the Liberal Party, he was also the first Member of Parliament to call for women's suffrage.

John Stuart Mill was born at 13 Rodney Street in Pentonville, Middlesex, the eldest son of the Scottish philosopher, historian and economist James Mill, and Harriet Burrow. John Stuart was educated by his father, with the advice and assistance of Jeremy Bentham and Francis Place. He was given an extremely rigorous upbringing, and was deliberately shielded from association with children his own age other than his siblings. His father, a follower of Bentham and an adherent of associationism, had as his explicit aim to create a genius intellect that would carry on the cause of
utilitarianism and its implementation after he and Bentham had died.

Mill was a notably precocious child. He describes his education in his autobiography. At the age of three he was taught Greek. By the age of eight, he had read "Aesop's Fables", Xenophon's "Anabasis", and the whole of Herodotus, and was acquainted with Lucian, Diogenes Laërtius, Isocrates and six dialogues of Plato. He had also read a great deal of history in English and had been taught arithmetic, physics and astronomy.

At the age of eight, Mill began studying Latin, the works of Euclid, and algebra, and was appointed schoolmaster to the younger children of the family. His main reading was still history, but he went through all the commonly taught Latin and Greek authors and by the age of ten could read Plato and Demosthenes with ease. His father also thought that it was important for Mill to study and compose poetry. One of Mill's earliest poetic compositions was a continuation of the Iliad. In his spare time he also enjoyed reading about natural sciences and popular novels, such as "Don Quixote" and "Robinson Crusoe".

His father's work, "The History of British India" was published in 1818; immediately thereafter, at about the age of twelve, Mill began a thorough study of the scholastic logic, at the same time reading Aristotle's logical treatises in the original language. In the following year he was introduced to political economy and studied Adam Smith and David Ricardo with his father, ultimately completing their classical economic view of factors of production. Mill's "comptes rendus" of his daily economy lessons helped his father in writing "Elements of Political Economy" in 1821, a textbook to promote the ideas of Ricardian economics; however, the book lacked popular support. Ricardo, who was a close friend of his father, used to invite the young Mill to his house for a walk in order to talk about political economy.

At the age of fourteen, Mill stayed a year in France with the family of Sir Samuel Bentham, brother of Jeremy Bentham. The mountain scenery he saw led to a lifelong taste for mountain landscapes. The lively and friendly way of life of the French also left a deep impression on him. In Montpellier, he attended the winter courses on chemistry, zoology, logic of the "Faculté des Sciences", as well as taking a course in higher mathematics. While coming and going from France, he stayed in Paris for a few days in the house of the renowned economist Jean-Baptiste Say, a friend of Mill's father. There he met many leaders of the Liberal party, as well as other notable Parisians, including Henri Saint-Simon.

Mill went through months of sadness and pondered suicide at twenty years of age. According to the opening paragraphs of Chapter V of his autobiography, he had asked himself whether the creation of a just society, his life's objective, would actually make him happy. His heart answered "no", and unsurprisingly he lost the happiness of striving towards this objective. Eventually, the poetry of William Wordsworth showed him that beauty generates compassion for others and stimulates joy. With renewed joy he continued to work towards a just society, but with more relish for the journey. He considered this one of the most pivotal shifts in his thinking. In fact, many of the differences between him and his father stemmed from this expanded source of joy.

Mill had been engaged in a pen-friendship with Auguste Comte, the founder of positivism and sociology, since Mill first contacted Comte in November 1841. Comte's "sociologie" was more an early philosophy of science than we perhaps know it today, and the "positive" philosophy aided in Mill's broad rejection of Benthamism.

As a nonconformist who refused to subscribe to the Thirty-Nine Articles of the Church of England, Mill was not eligible to study at the University of Oxford or the University of Cambridge. Instead he followed his father to work for the East India Company, and attended University College, London, to hear the lectures of John Austin, the first Professor of Jurisprudence. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1856.

Mill's career as a colonial administrator at the British East India Company spanned from when he was 17 years old in 1823 until 1858, when the Company was abolished in favor of direct rule by the British crown over India. In 1836, he was promoted to the Company's Political Department, where he was responsible for correspondence pertaining to the Company's relations with the princely states, and in 1856, was finally promoted to the position of Examiner of Indian Correspondence. In "On Liberty", "A Few Words on Non-Intervention", and other works, Mill defended British imperialism by arguing that a fundamental distinction existed between civilized and barbarous peoples. Mill viewed countries such as India and China as having once been progressive, but that were now stagnant and barbarous, thus legitimizing British rule as benevolent despotism, "provided the end is [the barbarians'] improvement." When the crown proposed to take direct control over the colonies in India, he was tasked with defending Company rule, penning "Memorandum on the Improvements in the Administration of India during the Last Thirty Years" among other petitions. He was offered a seat on the Council of India, the body created to advise the new Secretary of State for India, but declined, citing his disapproval of the new system of rule.

In 1851, Mill married Harriet Taylor after 21 years of intimate friendship. Taylor was married when they met, and their relationship was close but generally believed to be chaste during the years before her first husband died. Brilliant in her own right, Taylor was a significant influence on Mill's work and ideas during both friendship and marriage. His relationship with Harriet Taylor reinforced Mill's advocacy of women's rights. He cites her influence in his final revision of "On Liberty", which was published shortly after her death. Taylor died in 1858 after developing severe lung congestion, after only seven years of marriage to Mill.

Between the years 1865 and 1868 Mill served as Lord Rector of the University of St. Andrews. During the same period, 1865–68, he was a Member of Parliament for City and Westminster. He was sitting for the Liberal Party. During his time as an MP, Mill advocated easing the burdens on Ireland. In 1866, Mill became the first person in the history of Parliament to call for women to be given the right to vote, vigorously defending this position in subsequent debate. Mill became a strong advocate of such social reforms as labour unions and farm cooperatives. In "Considerations on Representative Government", Mill called for various reforms of Parliament and voting, especially proportional representation, the single transferable vote, and the extension of suffrage. In April 1868, Mill favoured in a Commons debate the retention of capital punishment for such crimes as aggravated murder; he termed its abolition "an effeminacy in the general mind of the country."

He was godfather to the philosopher Bertrand Russell.

In his views on religion, Mill was an agnostic.

Mill died in 1873 of erysipelas in Avignon, France, where his body was buried alongside his wife's.

Mill joined the debate over scientific method which followed on from John Herschel's 1830 publication of "A Preliminary Discourse on the study of Natural Philosophy", which incorporated inductive reasoning from the known to the unknown, discovering general laws in specific facts and verifying these laws empirically. William Whewell expanded on this in his 1837 "History of the Inductive Sciences, from the Earliest to the Present Time" followed in 1840 by "The Philosophy of the Inductive Sciences, Founded Upon their History", presenting induction as the mind superimposing concepts on facts. Laws were self-evident truths, which could be known without need for empirical verification. Mill countered this in 1843 in "A System of Logic, Ratiocinative and Inductive, Being a Connected View of the Principles of Evidence, and the Methods of Scientific Investigation." In Mill's Methods of induction, like Herschel's, laws were discovered through observation and induction, and required empirical verification.

Mill's "On Liberty" addresses the nature and limits of the power that can be legitimately exercised by society over the individual. However Mill is clear that his concern for liberty does not extend to all individuals and all societies. He states that "Despotism is a legitimate mode of government in dealing with barbarians".

Mill states that it is acceptable to harm oneself as long as the person doing so is not harming others. He also argues that individuals should be prevented from doing lasting, serious harm to themselves or their property by the harm principle. Because no one exists in isolation, harm done to oneself may also harm others, and destroying property deprives the community as well as oneself. Mill excuses those who are "incapable of self-government" from this principle, such as young children or those living in "backward states of society".

Though this principle seems clear, there are a number of complications. For example, Mill explicitly states that "harms" may include acts of omission as well as acts of commission. Thus, failing to rescue a drowning child counts as a harmful act, as does failing to pay taxes, or failing to appear as a witness in court. All such harmful omissions may be regulated, according to Mill. By contrast, it does not count as harming someone if – without force or fraud – the affected individual consents to assume the risk: thus one may permissibly offer unsafe employment to others, provided there is no deception involved. (Mill does, however, recognise one limit to consent: society should not permit people to sell themselves into slavery). In these and other cases, it is important to bear in mind that the arguments in "On Liberty" are grounded on the principle of Utility, and not on appeals to natural rights.

The question of what counts as a self-regarding action and what actions, whether of omission or commission, constitute harmful actions subject to regulation, continues to exercise interpreters of Mill. It is important to emphasise that Mill did not consider giving offence to constitute "harm"; an action could not be restricted because it violated the conventions or morals of a given society.

"On Liberty" involves an impassioned defense of free speech. Mill argues that free discourse is a necessary condition for intellectual and social progress. We can never be sure, he contends, that a silenced opinion does not contain some element of the truth. He also argues that allowing people to air false opinions is productive for two reasons. First, individuals are more likely to abandon erroneous beliefs if they are engaged in an open exchange of ideas. Second, by forcing other individuals to re-examine and re-affirm their beliefs in the process of debate, these beliefs are kept from declining into mere dogma. It is not enough for Mill that one simply has an unexamined belief that happens to be true; one must understand why the belief in question is the true one. Along those same lines Mill wrote, "unmeasured vituperation, employed on the side of prevailing opinion, really does deter people from expressing contrary opinions, and from listening to those who express them."

Mill believed that "the struggle between Liberty and Authority is the most conspicuous feature in the portions of history". For him, liberty in antiquity was a "contest… between subjects, or some classes of subjects, and the government." Mill defined "social liberty" as protection from "the tyranny of political rulers". He introduced a number of different concepts of the form tyranny can take, referred to as social tyranny, and tyranny of the majority.

Social liberty for Mill meant putting limits on the ruler's power so that he would not be able to use his power on his own wishes and make decisions which could harm society; in other words, people should have the right to have a say in the government's decisions. He said that social liberty was "the nature and limits of the power which can be legitimately exercised by society over the individual". It was attempted in two ways: first, by obtaining recognition of certain immunities, called political liberties or rights; second, by establishment of a system of "constitutional checks".

However, in Mill's view, limiting the power of government was not enough. He stated, "Society can and does execute its own mandates: and if it issues wrong mandates instead of right, or any mandates at all in things with which it ought not to meddle, it practices a social tyranny more formidable than many kinds of political oppression, since, though not usually upheld by such extreme penalties, it leaves fewer means of escape, penetrating much more deeply into the details of life, and enslaving the soul itself."

John Stuart Mill's view on liberty, which was influenced by Joseph Priestley and Josiah Warren, is that the individual ought to be free to do as she/he wishes unless she/he harms others. Individuals are rational enough to make decisions about their well being. Government should interfere when it is for the protection of society. Mill explained:

The sole end for which mankind are warranted, individually or collectively, in interfering with the liberty of action of any of their number, is self-protection. That the only purpose for which power can be rightfully exercised over any member of a civilized community, against his will, is to prevent harm to others. His own good, either physical or moral, is not sufficient warrant. He cannot rightfully be compelled to do or forbear because it will be better for him to do so, because it will make him happier, because, in the opinion of others, to do so would be wise, or even right...The only part of the conduct of anyone, for which he is amenable to society, is that which concerns others. In the part which merely concerns him, his independence is, of right, absolute. Over himself, over his own body and mind, the individual is sovereign.

An influential advocate of freedom of speech, Mill objected to censorship. He says:

I choose, by preference the cases which are least favourable to me – In which the argument opposing freedom of opinion, both on truth and that of utility, is considered the strongest. Let the opinions impugned be the belief of God and in a future state, or any of the commonly received doctrines of morality... But I must be permitted to observe that it is not the feeling sure of a doctrine (be it what it may) which I call an assumption of infallibility. It is the undertaking to decide that question "for others", without allowing them to hear what can be said on the contrary side. And I denounce and reprobate this pretension not the less if it is put forth on the side of my most solemn convictions. However, positive anyone's persuasion may be, not only of the faculty but of the pernicious consequences, but (to adopt expressions which I altogether condemn) the immorality and impiety of opinion. – yet if, in pursuance of that private judgement, though backed by the public judgement of his country or contemporaries, he prevents the opinion from being heard in its defence, he assumes infallibility. And so far from the assumption being less objectionable or less dangerous because the opinion is called immoral or impious, this is the case of all others in which it is most fatal.

Mill outlines the benefits of 'searching for and discovering the truth' as a way to further knowledge. He argued that even if an opinion is false, the truth can be better understood by refuting the error. And as most opinions are neither completely true nor completely false, he points out that allowing free expression allows the airing of competing views as a way to preserve partial truth in various opinions. Worried about minority views being suppressed, Mill also argued in support of freedom of speech on political grounds, stating that it is a critical component for a representative government to have in order to empower debate over public policy. Mill also eloquently argued that freedom of expression allows for personal growth and self-realization. He said that freedom of speech was a vital way to develop talents and realise a person's potential and creativity. He repeatedly said that eccentricity was preferable to uniformity and stagnation.

The belief that the freedom of speech will advance the society was formed with trust of the public’s ability to filter. If any argument is really wrong or harmful, the public will judge it as wrong or harmful, and then those arguments cannot be sustained and will be excluded. Mill argued that even any arguments which are used in justifying murder or rebellion against the government shouldn’t be politically suppressed or socially persecuted. According to him, if rebellion is really necessary, people should rebel; if murder is truly proper, it should be allowed. But, the way to express those arguments should be a public speech or writing, not in a way that causes actual harm to others. This is the harm principle.

At the beginning of the twentieth century, Associate Justice Oliver Wendell Holmes Jr. made the standard of "clear and present danger" based on Mill's idea. In the majority opinion, Holmes writes:

Shouting out "Fire!" in a dark theatre, which makes people panic and gets them injured, is a case of that. But if the situation allows people to reason by themselves and decide to accept it or not, any argument or theology should not be blocked.

Nowadays, Mill's argument is generally accepted by many democratic countries, and they have laws about the harm principle. For example, in American law some exceptions limit free speech such as obscenity, defamation, breach of peace, and "fighting words".

Mill, an employee for the British East India Company from 1823 to 1858, argued in support of what he called a 'benevolent despotism' with regard to the colonies. Mill argued that "To suppose that the same international customs, and the same rules of international morality, can obtain between one civilized nation and another, and between civilized nations and barbarians, is a grave error...To characterize any conduct whatever towards a barbarous people as a violation of the law of nations, only shows that he who so speaks has never considered the subject."

In 1850, Mill sent an anonymous letter (which came to be known under the title "The Negro Question"), in rebuttal to Thomas Carlyle's anonymous letter to "Fraser's Magazine for Town and Country" in which Carlyle argued for slavery. Mill supported abolition in the United States.

In Mill's essay from 1869, "The Subjection of Women", he expressed his opposition to slavery:

This absolutely extreme case of the law of force, condemned by those who can tolerate almost every other form of arbitrary power, and which, of all others, presents features the most revolting to the feeling of all who look at it from an impartial position, was the law of civilized and Christian England within the memory of persons now living: and in one half of Angle-Saxon America three or four years ago, not only did slavery exist, but the slave trade, and the breeding of slaves expressly for it, was a general practice between slave states. Yet not only was there a greater strength of sentiment against it, but, in England at least, a less amount either of feeling or of interest in favour of it, than of any other of the customary abuses of force: for its motive was the love of gain, unmixed and undisguised: and those who profited by it were a very small numerical fraction of the country, while the natural feeling of all who were not personally interested in it, was unmitigated abhorrence.

Mill's view of history was that right up until his time "the whole of the female" and "the great majority of the male sex" were simply "slaves". He countered arguments to the contrary, arguing that relations between sexes simply amounted to "the legal subordination of one sex to the other – [which] is wrong itself, and now one of the chief hindrances to human improvement; and that it ought to be replaced by a principle of perfect equality." With this, Mill can be considered among the earliest male proponents of gender equality. His book "The Subjection of Women" (1861, published 1869) is one of the earliest written on this subject by a male author. In "The Subjection of Women" Mill attempts to make a case for perfect equality. He talks about the role of women in marriage and how it needed to be changed. There, Mill comments on three major facets of women's lives that he felt are hindering them: society and gender construction, education, and marriage. He argued that the oppression of women was one of the few remaining relics from ancient times, a set of prejudices that severely impeded the progress of humanity.

The canonical statement of Mill's utilitarianism can be found in "Utilitarianism". This philosophy has a long tradition, although Mill's account is primarily influenced by Jeremy Bentham and Mill's father James Mill.

Jeremy Bentham's famous formulation of utilitarianism is known as the "greatest-happiness principle". It holds that one must always act so as to produce the greatest aggregate happiness among all sentient beings, within reason. In a similar vein, Mill’s method of determining the best utility is that a moral agent, when given the choice between two or more actions, ought to choose the action that contributes most to (maximizes) the total happiness in the world. Happiness in this context is understood as the production of pleasure or privation of pain. Given that determining the action that produces the most utility is not always so clear cut, Mill suggests that the utilitarian moral agent, when attempting to rank the utility of different actions, should refer to the general experience of persons. That is, if people generally experience more happiness following action X than they do action Y, the utilitarian should conclude that action X produces more utility than, and is thus favorable to, action Y.

Utilitarianism is built upon the basis of consequentialism, that is, the means are justified based solely off the result of said actions. The overarching goal of Utilitarianism – the ideal consequence – is to achieve the “greatest good for the greatest number as the end result of human action”. Mill states in his writings on Utilitarianism that “happiness is the sole end of human action." This statement brought about a bit of controversy, which is why Mill took it a step further, explaining how the very nature of humans wanting happiness, and who “take it to be reasonable under free consideration”, demands that happiness is indeed desirable. In other words, free will leads everyone to make actions inclined on their own happiness, unless reasoned that it would improve the happiness of others, in which case, the greatest utility is still being achieved. To that extent, the Utilitarianism that Mill is describing is a default lifestyle that he believes is what people who have not studied a specific opposing field of ethics would naturally and subconsciously utilize when faced with decision. Utilitarianism is thought of by some of its activists to be a more developed and overarching ethical theory of Kant’s belief in good will however, and not just some default cognitive process of humans. Where Kant would argue that reason can only be used properly by good will, Mill would say that the only way to universally create fair laws and systems would be to step back to the consequences, whereby Kant’s ethical theories become based around the ultimate good – utility. By this logic the only valid way to discern what is proper reason would be to view the consequences of any action and weigh the good and the bad, even if on the surface, the ethical reasoning seems to indicate a different train of thought.

Mill's major contribution to utilitarianism is his argument for the qualitative separation of pleasures. Bentham treats all forms of happiness as equal, whereas Mill argues that intellectual and moral pleasures (higher pleasures) are superior to more physical forms of pleasure (lower pleasures). Mill distinguishes between happiness and contentment, claiming that the former is of higher value than the latter, a belief wittily encapsulated in the statement that "it is better to be a human being dissatisfied than a pig satisfied; better to be Socrates dissatisfied than a fool satisfied. And if the fool, or the pig, are of a different opinion, it is because they only know their own side of the question."

Mill defines the difference between higher and lower forms of pleasure with the principle that those who have experienced both tend to prefer one over the other. This is, perhaps, in direct contrast with Bentham's statement that "Quantity of pleasure being equal, push-pin is as good as poetry", that, if a simple child's game like hopscotch causes more pleasure to more people than a night at the opera house, it is more imperative upon a society to devote more resources to propagating hopscotch than running opera houses. Mill's argument is that the "simple pleasures" tend to be preferred by people who have no experience with high art, and are therefore not in a proper position to judge. Mill also argues that people who, for example, are noble or practice philosophy, benefit society more than those who engage in individualist practices for pleasure, which are lower forms of happiness. It is not the agent's own greatest happiness that matters "but the greatest amount of happiness altogether".

Mill separated his explanation of Utilitarianism into five different sections; General Remarks, What Utilitarianism Is, Of the Ultimate Sanction of the Principle of Utility, Of What Sort of Proof the Principle of Utility is Susceptible, and Of the Connection between Justice and Utility. In the General Remarks portion of his essay he speaks how next to no progress has been made when it comes to judging what is right and what is wrong of morality and if there is such a thing as moral instinct (which he argues that there may not be). However he agrees that in general "Our moral faculty, according to all those of its interpreters who are entitled to the name of thinkers, supplies us only with the general principles of moral judgments". In the second chapter of his essay he focuses no longer on background information but Utilitarianism itself. He quotes Utilitarianism as "The greatest happiness principle" And defines this theory by saying that pleasure and no pain are the only inherently good things in the world and expands on it by saying that "actions are right in proportion as they tend to promote happiness, wrong as they tend to produce the reverse of happiness. By happiness is intended pleasure, and the absence of pain; by unhappiness, pain, and the privation of pleasure." He views it not as an animalistic concept because he sees seeking out pleasure as a way of using our higher facilities. He also says in this chapter that the happiness principle is based not exclusively on the individual but mainly on the community.

In his next chapter he focuses in more on the specifics of Utilitarianism when he writes about the sanctions of oneself. He states that a person possesses two sanctions; the internal sanction and the external sanction. According to Mill, the internal sanction is "a feeling in our own mind; a pain, more or less intense, attendant on violation of duty, which in properly cultivated moral natures rises, in the more serious cases, into shrinking from it as an impossibility." Shorthand, he basically just explains that your internal sanction is your conscience. The external sanction he says is "the hope of favour and the fear of displeasure, from our fellow creatures or from the Ruler of the Universe". This states that the external sanction is almost a form of fear of God himself. The sanctions are mentioned because according to Mill the internal sanction is what grasps onto the concept of Utilitarianism and is what make people want to accept Utilitarianism.

In Mill's fourth chapter he speaks of what proofs of Utility are affected. He starts this chapter off by saying that all of his claims cannot be backed up by reasoning. He claims that the only proof that something is brings one pleasure is if someone finds it pleasurable. Next he talks about how morality is the basic way to achieve happiness. He also discusses in this chapter that Utilitarianism is beneficial for virtue. He says that "it maintains not only that virtue is to be desired, but that it is to be desired disinterestedly, for itself." In his final chapter Mill looks and the connection between Utilitarianism and justice. He contemplates the question of whether justice is something distinct from Utility or not. He reasons this question in several different ways and finally comes to the conclusion that in certain cases justice is essential for Utility, but in others social duty is far more important than justice. Mill believes that "justice must give way to some other moral principle, but that what is just in ordinary cases is, by reason of that other principle, not just in the particular case."

The qualitative account of happiness that Mill advocates thus sheds light on his account presented in "On Liberty". As Mill suggests in that text, utility is to be conceived in relation to humanity "as a progressive being", which includes the development and exercise of rational capacities as we strive to achieve a "higher mode of existence". The rejection of censorship and paternalism is intended to provide the necessary social conditions for the achievement of knowledge and the greatest ability for the greatest number to develop and exercise their deliberative and rational capacities.

Mill redefines the definition of happiness as; "the ultimate end, for the sake of which all other things are desirable (whether we are considering our own good or that of other people) is an existence as free as possible from pain and as rich as possible in enjoyments". He firmly believed that moral rules and obligations could be referenced to promoting happiness, which connects to having a noble character. While John Stuart Mill is not a standard act or rule utilitarian, he is a minimizing utilitarian, which "affirms that it would be "desirable" to maximize happiness for the greatest number, but not that we are not morally "required" to do so".

Mill’s thesis distinguishes between higher and lower pleasures. He frequently discusses the importance of acknowledgement of higher pleasures. "To suppose that life has (as they express it) no higher end than pleasure- no better and nobler object of desire and pursuit they designate as utterly mean and groveling; as a doctrine worthy only of swine". When he says higher pleasures, he means the pleasures that access higher abilities and capacities in humans such as intellectual prosperity, whereas lower pleasures would mean bodily or temporary pleasures. "But it must be admitted that when utilitarian writers have said that mental pleasures are better than bodily ones they have mainly based this on mental pleasures being more permanent, safer, less costly and so on – i.e. from their circumstantial advantages rather than from their intrinsic nature". All of this factors into John Mill’s own definition of utilitarianism, and shows why it differs from other definitions.

Mill's early economic philosophy was one of free markets. However, he accepted interventions in the economy, such as a tax on alcohol, if there were sufficient utilitarian grounds. He also accepted the principle of legislative intervention for the purpose of animal welfare. Mill originally believed that "equality of taxation" meant "equality of sacrifice" and that progressive taxation penalised those who worked harder and saved more and was therefore "a mild form of robbery".

Given an equal tax rate regardless of income, Mill agreed that inheritance should be taxed. A utilitarian society would agree that everyone should be equal one way or another. Therefore, receiving inheritance would put one ahead of society unless taxed on the inheritance.
Those who donate should consider and choose carefully where their money goes – some charities are more deserving than others. Considering public charities boards such as a government will disburse the money equally. However, a private charity board like a church would disburse the monies fairly to those who are in more need than others.

Later he altered his views toward a more socialist bent, adding chapters to his Principles of Political Economy in defence of a socialist outlook, and defending some socialist causes. Within this revised work he also made the radical proposal that the whole wage system be abolished in favour of a co-operative wage system. Nonetheless, some of his views on the idea of flat taxation remained, albeit altered in the third edition of the "Principles of Political Economy" to reflect a concern for differentiating restrictions on "unearned" incomes, which he favoured, and those on "earned" incomes, which he did not favour.

Mill's "Principles", first published in 1848, was one of the most widely read of all books on economics in the period. As Adam Smith's "Wealth of Nations" had during an earlier period, Mill's "Principles" dominated economics teaching. In the case of Oxford University it was the standard text until 1919, when it was replaced by Marshall's "Principles of Economics".

Mill promoted economic democracy instead of capitalism, in the manner of substituting capitalist businesses with worker cooperatives. He says:

The form of association, however, which if mankind continue to improve, must be expected in the end to predominate, is not that which can exist between a capitalist as chief, and work-people without a voice in the management, but the association of the labourers themselves on terms of equality, collectively owning the capital with which they carry on their operations, and working under managers elected and removable by themselves.

Mill's major work on political democracy, "Considerations on Representative Government", defends two fundamental principles: extensive participation by citizens and enlightened competence of rulers. The two values are obviously in tension, and some readers have concluded that he is an elitist democrat, while others count him as an earlier participatory democrat. In one section he appears to defend plural voting, in which more competent citizens are given extra votes (a view he later repudiated). But in chapter 3 he presents what is still one of the most eloquent cases for the value of participation by all citizens. He believed that the incompetence of the masses could eventually be overcome if they were given a chance to take part in politics, especially at the local level.

Mill is one of the few political philosophers ever to serve in government as an elected official. In his three years in Parliament, he was more willing to compromise than the "radical" principles expressed in his writing would lead one to expect.

Mill demonstrated an early insight into the value of the natural world – in particular in Book IV, chapter VI of "Principles of Political Economy": "Of the Stationary State" in which Mill recognised wealth beyond the material, and argued that the logical conclusion of unlimited growth was destruction of the environment and a reduced quality of life. He concluded that a stationary state could be preferable to unending economic growth:

I cannot, therefore, regard the stationary states of capital and wealth with the unaffected aversion so generally manifested towards it by political economists of the old school.

If the earth must lose that great portion of its pleasantness which it owes to things that the unlimited increase of wealth and population would extirpate from it, for the mere purpose of enabling it to support a larger, but not a better or a happier population, I sincerely hope, for the sake of posterity, that they will be content to be stationary, long before necessity compel them to it.

Mill regarded economic development as a function of land, labour and capital. While land and labour are the two original factors of production, capital is "a stock, previously accumulated of the products of former labour." Increase in wealth is possible only if land and capital help to increase production faster than the labour force. It is productive labour that is productive of wealth and capital accumulation. "The rate of capital accumulation is the function of the proportion of the labour force employed productively. Profits earned by employing unproductive labours are merely transfers of income; unproductive labour does not generate wealth or income". It is productive labourers who do productive consumption. Productive consumption is that "which maintains and increase the productive capacity of the community." It implies that productive consumption is an input necessary to maintain productive labourers.

Mill supported the Malthusian theory of population. By population he meant the number of the working class only. He was therefore concerned about the growth in number of labourers who worked for hire. He believed that population control was essential for improving the condition of the working class so that they might enjoy the fruits of the technological progress and capital accumulation. Mill advocated birth control. In 1823 Mill and a friend were arrested while distributing pamphlets on birth control by Francis Place to women in working class areas.

According to Mill, supply is very elastic in response to wages. Wages generally exceed the minimum subsistence level, and are paid out of capital. Hence, wages are limited by existing capital for paying wages. Thus, wage per worker can be derived by dividing the total circulating capital by the size of the working population. Wages can increase by an increase in the capital used in paying wages, or by decrease in the number of workers. If wages rise, supply of labour will rise. Competition among workers not only brings down wages, but also keeps some workers out of employment. This is based on Mill's notion that "demand for commodities is not demand for labourers". It means that income invested as advances of wages to labour creates employment, and not income spent on consumer goods. An increase in consumption causes a decline in investment. So increased investment leads to increases in the wage fund and to economic progress.

In 1869, Mill recanted his support of the Wage-Fund Doctrine due to recognition that capital is not necessarily fixed in that it can be supplemented through "income of the employer which might otherwise go into saving or be spent on consumption." Walker also states in "The Wages Question" that the limits on capital and the growth in population "were accidental, not essential" to the formation of the doctrine. The limitation on the growth of industrial capacity placed a limit on the number of workers who could be accommodated more than the limit on capital. Furthermore, English agriculture "had reached the condition of diminishing returns."; therefore, each additional worker was not providing more output than he needed for himself for survival. Given the improvements in technology and productivity that followed 1848, the original reasons that gave rise to the doctrine were seen to be unusual and not the basis for a universal law.

According to Mill, the rate of capital accumulation depends on: (1) "the amount of fund from which saving can be made" or "the size of the net produce of the industry", and (2) the "disposition to save". Capital is the result of savings, and the savings come from the "abstinence from present consumption for the sake of future goods". Although capital is the result of saving, it is nevertheless consumed. This means saving is spending. Since saving depends on the net produce of the industry, it grows with profits and rent which go into making the net produce. On the other hand, the disposition to save depends on (1) the rate of profit and (2) the desire to save, or what Mill called "effective desire of accumulation". However, profit also depends on the cost of labour, and the rate of profit is the ratio of profits to wages. When profits rise or wages fall, the rate of profits increases, which in turn increases the rate of capital accumulation. Similarly, it is the desire to save which tends to increase the rate of capital accumulation.

According to Mill, the ultimate tendency in an economy is for the rate of profit to decline due to diminishing returns in agriculture and increase in population at a Malthusian rate 

<poem>
John Stuart Mill,
By a mighty effort of will,
Overcame his natural bonhomie
And wrote "Principles of Political Economy".
</poem>








</doc>
<doc id="15627" url="https://en.wikipedia.org/wiki?curid=15627" title="Junk science">
Junk science

The expression junk science is used to describe scientific data, research, or analysis considered by the person using the phrase to be spurious or fraudulent. The concept is often invoked in political and legal contexts where facts and scientific results have a great amount of weight in making a determination. It usually conveys a pejorative connotation that the research has been untowardly driven by political, ideological, financial, or otherwise unscientific motives.

The concept was popularized in the 1990s in relation to expert testimony in civil litigation. More recently, invoking the concept has been a tactic to criticize research on the harmful environmental or public health effects of corporate activities, and occasionally in response to such criticism. The term has been used by proponents of both sides of such political debates. Author Dan Agin in his book "Junk Science" harshly criticized those who deny the basic premise of global warming, while former Fox News commentator Steven Milloy has extensively denounced research linking the fossil fuel industry to climate change, on his website "junkscience.com".

In some contexts, junk science is counterposed to the "sound science" or "solid science" that favors one's own point of view. This dichotomy has been particularly promoted by Steven Milloy and the Advancement of Sound Science Center, and is somewhat different from pseudoscience and fringe science.

The phrase "junk science" appears to have been in use prior to 1985. A 1985 United States Department of Justice report by the Tort Policy Working Group noted:
The use of such invalid scientific evidence (commonly referred to as 'junk science') has resulted in findings of causation which simply cannot be justified or understood from the standpoint of the current state of credible scientific or medical knowledge.

In 1989, the climate scientist Jerry Mahlman (Director of the Geophysical Fluid Dynamics Laboratory) characterized the theory that global warming was due to solar variation (presented in "Scientific Perspectives on the Greenhouse Problem" by Frederick Seitz et al.) as "noisy junk science."

Peter W. Huber popularized the term with respect to litigation in his 1991 book "Galileo's Revenge: Junk Science in the Courtroom." The book has been cited in over 100 legal textbooks and references; as a consequence, some sources cite Huber as the first to coin the term. By 1997, the term had entered the legal lexicon as seen in an opinion by Supreme Court of the United States Justice John Paul Stevens: 
An example of 'junk science' that should be excluded under the Daubert standard as too unreliable would be the testimony of a phrenologist who would purport to prove a defendant's future dangerousness based on the contours of the defendant's skull. Lower courts have subsequently set guidelines for identifying junk science, such as the 2005 opinion of United States Court of Appeals for the Seventh Circuit Judge Easterbrook:
Positive reports about magnetic water treatment are not replicable; this plus the lack of a physical explanation for any effects are hallmarks of junk science.

As the subtitle of Huber's book, "Junk Science in the Courtroom", suggests, his emphasis was on the use or misuse of expert testimony in civil litigation. One prominent example cited in the book was litigation over casual contact in the spread of AIDS. A California school district sought to prevent a young boy with AIDS, Ryan Thomas, from attending kindergarten. The school district produced an expert witness, Steven Armentrout, who testified that a possibility existed that AIDS could be transmitted to schoolmates through yet undiscovered "vectors." However, five experts testified on behalf of Thomas that AIDS is not transmitted through casual contact, and the court affirmed the "solid science" (as Mr. Huber called it) and rejected Armentrout's argument.

In 1999, Paul Ehrlich and others advocated public policies to improve the dissemination of valid environmental scientific knowledge and discourage junk science: 
The Intergovernmental Panel on Climate Change reports offer an antidote to junk science by articulating the current consensus on the prospects for climate change, by outlining the extent of the uncertainties, and by describing the potential benefits and costs of policies to address climate change.

In a 2003 study about changes in environmental activism regarding the Crown of the Continent Ecosystem, Pedynowski noted that junk science can undermine the credibility of science over a much broader scale because misrepresentation by special interests casts doubt on more defensible claims and undermines the credibility of all research.

In his 2006 book "Junk Science", Dan Agin emphasized two main causes of junk science: fraud, and ignorance. In the first case, Agin discussed falsified results in the development of organic transistors: 
As far as understanding junk science is concerned, the important aspect is that both Bell Laboratories and the international physics community were fooled until someone noticed that noise records published by Jan Hendrik Schön in several papers were identical—which means physically impossible.

In the second case, he cites an example that demonstrates ignorance of statistical principles in the lay press: 
Since no such proof is possible [that genetically modified food is harmless], the article in The New York Times was what is called a "bad rap" against the U.S. Department of Agriculture—a bad rap based on a junk-science belief that it's possible to prove a null hypothesis.

Agin asks the reader to step back from the rhetoric, as "how things are labeled does not make a science junk science." In its place, he offers that junk science is ultimately motivated by the desire to hide undesirable truths from the public.

John Stauber and Sheldon Rampton of "PR Watch" say the concept of junk science has come to be invoked in attempts to dismiss scientific findings that stand in the way of short-term corporate profits. In their book "Trust Us, We're Experts" (2001), they write that industries have launched multimillion-dollar campaigns to position certain theories as junk science in the popular mind, often failing to employ the scientific method themselves. For example, the tobacco industry has described research demonstrating the harmful effects of smoking and second-hand smoke as junk science, through the vehicle of various astroturf groups.

Theories more favorable to corporate activities are portrayed in words as "sound science." Past examples where "sound science" was used include the research into the toxicity of Alar, which was heavily criticized by antiregulatory advocates, and Herbert Needleman's research into low dose lead poisoning. Needleman was accused of fraud and personally attacked.

Fox News commentator Steven Milloy often invokes the concept of junk science to attack the results of credible scientific research on topics like global warming, ozone depletion, and passive smoking. The credibility of Milloy's website junkscience.com was questioned by Paul D. Thacker, a writer for "The New Republic", in the wake of evidence that Milloy had received funding from Philip Morris, RJR Tobacco, and Exxon Mobil. Thacker also noted that Milloy was receiving almost $100,000 a year in consulting fees from Philip Morris while he criticized the evidence regarding the hazards of second-hand smoke as junk science. Following the publication of this article, the Cato Institute, which had hosted the junkscience.com site, ceased its association with the site and removed Milloy from its list of adjunct scholars.

Tobacco industry documents reveal that Philip Morris executives conceived of the "Whitecoat Project" in the 1980s as a response to emerging scientific data on the harmfulness of second-hand smoke. The goal of the Whitecoat Project, as conceived by Philip Morris and other tobacco companies, was to use ostensibly independent "scientific consultants" to spread doubt in the public mind about scientific data through invoking concepts like junk science. According to epidemiologist David Michaels, Assistant Secretary of Energy for Environment, Safety, and Health in the Clinton Administration, the tobacco industry invented the "sound science" movement in the 1980s as part of their campaign against the regulation of second-hand smoke.

David Michaels has argued that, since the U.S. Supreme Court ruling in "Daubert v. Merrell Dow Pharmaceuticals, Inc.", lay judges have become "gatekeepers" of scientific testimony and, as a result, respected scientists have sometimes been unable to provide testimony so that corporate defendants are "increasingly emboldened" to accuse adversaries of practicing junk science.

In 1995, the Union of Concerned Scientists launched the Sound Science Initiative, a national network of scientists committed to debunking junk science through media outreach, lobbying, and developing joint strategies to participate in town meetings or public hearings. In its newsletter on Science and Technology in Congress, the American Association for the Advancement of Science also recognized the need for increased understanding between scientists and lawmakers: "Although most individuals would agree that sound science is preferable to junk science, fewer recognize what makes a scientific study 'good' or 'bad'." The American Dietetic Association, criticizing marketing claims made for food products, has created a list of "Ten Red Flags of Junk Science."

Individual scientists have also invoked the concept.





</doc>
<doc id="15628" url="https://en.wikipedia.org/wiki?curid=15628" title="Java (disambiguation)">
Java (disambiguation)

Java is an island of Indonesia.

Java may also refer to:








</doc>
<doc id="15630" url="https://en.wikipedia.org/wiki?curid=15630" title="James Cook">
James Cook

Captain James Cook (7 November 172814 February 1779) was a British explorer, navigator, cartographer, and captain in the Royal Navy. Cook made detailed maps of Newfoundland prior to making three voyages to the Pacific Ocean, during which he achieved the first recorded European contact with the eastern coastline of Australia and the Hawaiian Islands, and the first recorded circumnavigation of New Zealand.

Cook joined the British merchant navy as a teenager and joined the Royal Navy in 1755. He saw action in the Seven Years' War, and subsequently surveyed and mapped much of the entrance to the Saint Lawrence River during the siege of Quebec. This helped bring Cook to the attention of the Admiralty and Royal Society. This notice came at a crucial moment in both Cook's career and the direction of British overseas exploration, and led to his commission in 1766 as commander of for the first of three Pacific voyages.

In three voyages Cook sailed thousands of miles across largely uncharted areas of the globe. He mapped lands from New Zealand to Hawaii in the Pacific Ocean in greater detail and on a scale not previously achieved. As he progressed on his voyages of discovery he surveyed and named features, and recorded islands and coastlines on European maps for the first time. He displayed a combination of seamanship, superior surveying and cartographic skills, physical courage and an ability to lead men in adverse conditions.

Cook was attacked and killed in 1779 during his third exploratory voyage in the Pacific while attempting to kidnap Kalaniʻōpuʻu, a Hawaiian chief, in order to reclaim a cutter stolen from one of his ships. He left a legacy of scientific and geographical knowledge which was to influence his successors well into the 20th century, and numerous memorials worldwide have been dedicated to him.

James Cook was born on 7 November 1728 (N.S.) in the village of Marton in Yorkshire and baptised on 14 November (N.S.) in the parish church of St Cuthbert, where his name can be seen in the church register. He was the second of eight children of James Cook, a Scottish farm labourer from Ednam in Roxburghshire, and his locally born wife, Grace Pace, from Thornaby-on-Tees. In 1736, his family moved to Airey Holme farm at Great Ayton, where his father's employer, Thomas Skottowe, paid for him to attend the local school. In 1741, after five years' schooling, he began work for his father, who had been promoted to farm manager. For leisure, he would climb a nearby hill, Roseberry Topping, enjoying the opportunity for solitude. Cooks' Cottage, his parents' last home, which he is likely to have visited, is now in Melbourne, Australia, having been moved from England and reassembled, brick by brick, in 1934.

In 1745, when he was 16, Cook moved to the fishing village of Staithes, to be apprenticed as a shop boy to grocer and haberdasher William Sanderson. Historians have speculated that this is where Cook first felt the lure of the sea while gazing out of the shop window.

After 18 months, not proving suitable for shop work, Cook travelled to the nearby port town of Whitby to be introduced to friends of Sanderson's, John and Henry Walker. The Walkers, who were Quakers, were prominent local ship-owners in the coal trade. Their house is now the Captain Cook Memorial Museum. Cook was taken on as a merchant navy apprentice in their small fleet of vessels, plying coal along the English coast. His first assignment was aboard the collier "Freelove", and he spent several years on this and various other coasters, sailing between the Tyne and London. As part of his apprenticeship, Cook applied himself to the study of algebra, geometry, trigonometry, navigation and astronomy—all skills he would need one day to command his own ship.

His three-year apprenticeship completed, Cook began working on trading ships in the Baltic Sea. After passing his examinations in 1752, he soon progressed through the merchant navy ranks, starting with his promotion in that year to mate aboard the collier brig "Friendship". In 1755, within a month of being offered command of this vessel, he volunteered for service in the Royal Navy, when Britain was re-arming for what was to become the Seven Years' War. Despite the need to start back at the bottom of the naval hierarchy, Cook realised his career would advance more quickly in military service and entered the Navy at Wapping on 17 June 1755.

Cook married Elizabeth Batts (1742–1835), the daughter of Samuel Batts, keeper of the Bell Inn, Wapping and one of his mentors, on 21 December 1762 at St Margaret's Church, Barking, Essex. The couple had six children: James (1763–94), Nathaniel (1764–80, lost aboard which foundered with all hands in a hurricane in the West Indies), Elizabeth (1767–71), Joseph (1768–68), George (1772–72) and Hugh (1776–93), the last of whom died of scarlet fever while a student at Christ's College, Cambridge. When not at sea, Cook lived in the East End of London. He attended St Paul's Church, Shadwell, where his son James was baptised. Cook has no direct descendants—all his children died before having children of their own.

Cook's first posting was with , serving as able seaman and master's mate under Captain Joseph Hamar for his first year aboard, and Captain Hugh Palliser thereafter. In October and November 1755 he took part in "Eagle"'s capture of one French warship and the sinking of another, following which he was promoted to boatswain in addition to his other duties. His first temporary command was in March 1756 when he was briefly master of "Cruizer", a small cutter attached to "Eagle" while on patrol.

In June 1757 Cook formally passed his master's examinations at Trinity House, Deptford, qualifying him to navigate and handle a ship of the King's fleet. He then joined the frigate as master under Captain Robert Craig.

During the Seven Years' War, Cook served in North America as master aboard the fourth-rate Navy vessel . With others in "Pembroke"s crew, he took part in the major amphibious assault that captured the Fortress of Louisbourg from the French in 1758, and in the siege of Quebec City in 1759. Throughout his service he demonstrated a talent for surveying and cartography, and was responsible for mapping much of the entrance to the Saint Lawrence River during the siege, thus allowing General Wolfe to make his famous stealth attack during the 1759 Battle of the Plains of Abraham.

Cook's surveying ability was also put to use in mapping the jagged coast of Newfoundland in the 1760s, aboard . He surveyed the north-west stretch in 1763 and 1764, the south coast between the Burin Peninsula and Cape Ray in 1765 and 1766, and the west coast in 1767. At this time Cook employed local pilots to point out the "rocks and hidden dangers" along the south and west coasts. During the 1765 season, four pilots were engaged at a daily pay of 4 shillings each: John Beck for the coast west of "Great St Lawrence", Morgan Snook for Fortune Bay, John Dawson for Connaigre and Hermitage Bay, and John Peck for the "Bay of Despair".

His five seasons in Newfoundland produced the first large-scale and accurate maps of the island's coasts and were the first scientific, large scale, hydrographic surveys to use precise triangulation to establish land outlines. They also gave Cook his mastery of practical surveying, achieved under often adverse conditions, and brought him to the attention of the Admiralty and Royal Society at a crucial moment both in his career and in the direction of British overseas discovery. Cook's map would be used into the 20th century—copies of it being referenced by those sailing Newfoundland's waters for 200 years.

Following on from his exertions in Newfoundland, it was at this time that Cook wrote that he intended to go not only "farther than any man has been before me, but as far as I think it is possible for a man to go".

In 1766, the Admiralty engaged Cook to command a scientific voyage to the Pacific Ocean. The purpose of the voyage was to observe and record the transit of Venus across the Sun for the benefit of a Royal Society inquiry into a means of determining longitude. Cook, at the age of 39, was promoted to lieutenant to grant him sufficient status to take the command. For its part the Royal Society agreed that Cook would receive a one hundred guinea gratuity in addition to his Naval pay.

The expedition sailed aboard , departing England on 26 August 1768. Cook and his crew rounded Cape Horn and continued westward across the Pacific to arrive at Tahiti on 13 April 1769, where the observations of the Venus Transit were made. However, the result of the observations was not as conclusive or accurate as had been hoped. Once the observations were completed, Cook opened the sealed orders which were additional instructions from the Admiralty for the second part of his voyage: to search the south Pacific for signs of the postulated rich southern continent of "Terra Australis".
Cook then sailed to New Zealand and mapped the complete coastline, making only some minor errors. He then voyaged west, reaching the south-eastern coast of Australia on 19 April 1770, and in doing so his expedition became the first recorded Europeans to have encountered its eastern coastline.

On 23 April he made his first recorded direct observation of indigenous Australians at Brush Island near Bawley Point, noting in his journal: "...and were so near the Shore as to distinguish several people upon the Sea beach they appear'd to be of a very dark or black Colour but whether this was the real colour of their skins or the C[l]othes they might have on I know not." On 29 April Cook and crew made their first landfall on the mainland of the continent at a place now known as the Kurnell Peninsula. Cook originally christened the area as "Stingray Bay", but later he crossed this out and named it "Botany Bay" after the unique specimens retrieved by the botanists Joseph Banks and Daniel Solander. It is here that James Cook made first contact with an aboriginal tribe known as the Gweagal.

After his departure from Botany Bay he continued northwards. He stopped at Bustard Bay (now known as Seventeen Seventy or 1770) at 8 o'clock on 23 May 1770. On 24 May, Cook and Banks and others went ashore. Continuing north, on 11 June a mishap occurred when HMS "Endeavour" ran aground on a shoal of the Great Barrier Reef, and then "nursed into a river mouth on 18 June 1770". The ship was badly damaged and his voyage was delayed almost seven weeks while repairs were carried out on the beach (near the docks of modern Cooktown, Queensland, at the mouth of the Endeavour River). The voyage then continued, sailing through Torres Strait and on 22 August Cook landed on Possession Island, where he claimed the entire coastline that he had just explored as British territory. He returned to England via Batavia (modern Jakarta, Indonesia), where many in his crew succumbed to malaria, and then the Cape of Good Hope, arriving at the island of Saint Helena on 12 July 1771.

Cook's journals were published upon his return, and he became something of a hero among the scientific community. Among the general public, however, the aristocratic botanist Joseph Banks was a greater hero. Banks even attempted to take command of Cook's second voyage, but removed himself from the voyage before it began, and Johann Reinhold Forster and his son Georg Forster were taken on as scientists for the voyage. Cook's son George was born five days before he left for his second voyage.
Shortly after his return from the first voyage, Cook was promoted in August 1771, to the rank of commander. In 1772 he was commissioned to lead another scientific expedition on behalf of the Royal Society, to search for the hypothetical Terra Australis. On his first voyage, Cook had demonstrated by circumnavigating New Zealand that it was not attached to a larger landmass to the south. Although he charted almost the entire eastern coastline of Australia, showing it to be continental in size, the Terra Australis was believed to lie further south. Despite this evidence to the contrary, Alexander Dalrymple and others of the Royal Society still believed that a massive southern continent should exist.

Cook commanded on this voyage, while Tobias Furneaux commanded its companion ship, . Cook's expedition circumnavigated the globe at an extreme southern latitude, becoming one of the first to cross the Antarctic Circle (17 January 1773). In the Antarctic fog, "Resolution" and "Adventure" became separated. Furneaux made his way to New Zealand, where he lost some of his men during an encounter with Māori, and eventually sailed back to Britain, while Cook continued to explore the Antarctic, reaching 71°10'S on 31 January 1774.

Cook almost encountered the mainland of Antarctica, but turned towards Tahiti to resupply his ship. He then resumed his southward course in a second fruitless attempt to find the supposed continent. On this leg of the voyage he brought a young Tahitian named Omai, who proved to be somewhat less knowledgeable about the Pacific than Tupaia had been on the first voyage. On his return voyage to New Zealand in 1774, Cook landed at the Friendly Islands, Easter Island, Norfolk Island, New Caledonia, and Vanuatu.

Before returning to England, Cook made a final sweep across the South Atlantic from Cape Horn and surveyed, mapped and took possession for Britain of South Georgia, which had been explored by the English merchant Anthony de la Roché in 1675. Cook also discovered and named Clerke Rocks and the South Sandwich Islands ("Sandwich Land"). He then turned north to South Africa, and from there continued back to England. His reports upon his return home put to rest the popular myth of Terra Australis.

Cook's second voyage marked a successful employment of Larcum Kendall's K1 copy of John Harrison's H4 marine chronometer, which enabled Cook to calculate his longitudinal position with much greater accuracy. Cook's log was full of praise for this time-piece which he used to make charts of the southern Pacific Ocean that were so remarkably accurate that copies of them were still in use in the mid-20th century.

Upon his return, Cook was promoted to the rank of post-captain and given an honorary retirement from the Royal Navy, with a posting as an officer of the Greenwich Hospital. He reluctantly accepted, insisting that he be allowed to quit the post if an opportunity for active duty should arise. His fame extended beyond the Admiralty; he was made a Fellow of the Royal Society, and awarded the Copley Gold Medal for completing his second voyage without losing a man to scurvy. Nathaniel Dance-Holland painted his portrait; he dined with James Boswell; he was described in the House of Lords as "the first navigator in Europe". But he could not be kept away from the sea. A third voyage was planned and Cook volunteered to find the Northwest Passage. He travelled to the Pacific and hoped to travel east to the Atlantic, while a simultaneous voyage travelled the opposite route.

On his last voyage, Cook again commanded HMS "Resolution", while Captain Charles Clerke commanded . The voyage was ostensibly planned to return the Pacific Islander, Omai to Tahiti, or so the public were led to believe. The trip's principal goal was to locate a Northwest Passage around the American continent. After dropping Omai at Tahiti, Cook travelled north and in 1778 became the first European to begin formal contact with the Hawaiian Islands. After his initial landfall in January 1778 at Waimea harbour, Kauai, Cook named the archipelago the "Sandwich Islands" after the fourth Earl of Sandwich—the acting First Lord of the Admiralty.

From the Sandwich Islands Cook sailed north and then north-east to explore the west coast of North America north of the Spanish settlements in Alta California. He made landfall on the Oregon coast at approximately 44°30′ north latitude, naming his landing point Cape Foulweather. Bad weather forced his ships south to about 43° north before they could begin their exploration of the coast northward. He unknowingly sailed past the Strait of Juan de Fuca, and soon after entered Nootka Sound on Vancouver Island. He anchored near the First Nations village of Yuquot. Cook's two ships remained in Nootka Sound from 29 March to 26 April 1778, in what Cook called Ship Cove, now Resolution Cove, at the south end of Bligh Island, about east across Nootka Sound from Yuquot, lay a Nuu-chah-nulth village (whose chief Cook did not identify but may have been Maquinna). Relations between Cook's crew and the people of Yuquot were cordial if sometimes strained. In trading, the people of Yuquot demanded much more valuable items than the usual trinkets that had worked in Hawaii. Metal objects were much desired, but the lead, pewter, and tin traded at first soon fell into disrepute. The most valuable items which the British received in trade were sea otter pelts. During the stay, the Yuquot "hosts" essentially controlled the trade with the British vessels; the natives usually visited the British vessels at Resolution Cove instead of the British visiting the village of Yuquot at Friendly Cove.

After leaving Nootka Sound, Cook explored and mapped the coast all the way to the Bering Strait, on the way identifying what came to be known as Cook Inlet in Alaska. In a single visit, Cook charted the majority of the North American north-west coastline on world maps for the first time, determined the extent of Alaska, and closed the gaps in Russian (from the West) and Spanish (from the South) exploratory probes of the Northern limits of the Pacific.

By the second week of August 1778 Cook was through the Bering Strait, sailing into the Chukchi Sea. He headed north-east up the coast of Alaska until he was blocked by sea ice. His furthest north was 70 degrees 44 minutes. Cook then sailed west to the Siberian coast, and then south-east down the Siberian coast back to the Bering Strait. By early September 1778 he was back in the Bering Sea to begin the trip to the Sandwich (Hawaiian) Islands. He became increasingly frustrated on this voyage, and perhaps began to suffer from a stomach ailment; it has been speculated that this led to irrational behaviour towards his crew, such as forcing them to eat walrus meat, which they had pronounced inedible.

Cook returned to Hawaii in 1779. After sailing around the archipelago for some eight weeks, he made landfall at Kealakekua Bay, on 'Hawaii Island', largest island in the Hawaiian Archipelago. Cook's arrival coincided with the "Makahiki", a Hawaiian harvest festival of worship for the Polynesian god Lono. Coincidentally the form of Cook's ship, HMS "Resolution", or more particularly the mast formation, sails and rigging, resembled certain significant artefacts that formed part of the season of worship. Similarly, Cook's clockwise route around the island of Hawaii before making landfall resembled the processions that took place in a clockwise direction around the island during the Lono festivals. It has been argued (most extensively by Marshall Sahlins) that such coincidences were the reasons for Cook's (and to a limited extent, his crew's) initial deification by some Hawaiians who treated Cook as an incarnation of Lono. Though this view was first suggested by members of Cook's expedition, the idea that any Hawaiians understood Cook to be Lono, and the evidence presented in support of it, were challenged in 1992.

After a month's stay, Cook attempted to resume his exploration of the Northern Pacific. Shortly after leaving Hawaii Island, however, the "Resolution"'s foremast broke, so the ships returned to Kealakekua Bay for repairs.

Tensions rose, and a number of quarrels broke out between the Europeans and Hawaiians at Kealakekua Bay. An unknown group of Hawaiians took one of Cook's small boats. The evening when the cutter was taken, the people had become "insolent" even with threats to fire upon them. Cook was forced into a wild goose chase that ended with his return to the ship frustrated. He attempted to kidnap and ransom the King of Hawaiʻi, Kalaniʻōpuʻu.

That following day, 14 February 1779, Cook marched through the village to retrieve the King. Cook took the King (aliʻi nui) by his own hand and led him willingly away. One of Kalaniʻōpuʻu's favorite wives, Kanekapolei and two chiefs approached the group as they were heading to boats. They pleaded with the king not to go until he stopped and sat where he stood. An old kahuna (priest), chanting rapidly while holding out a coconut, attempted to distract Cook and his men as a large crowd began to form at the shore. The king began to understand that Cook was his enemy. As Cook turned his back to help launch the boats, he was struck on the head by the villagers and then stabbed to death as he fell on his face in the surf. He was first struck on the head with a club by a chief named Kalaimanokahoʻowaha or Kanaʻina (namesake of Charles Kana'ina) and then stabbed by one of the king's attendants, Nuaa. The Hawaiians carried his body away towards the back of the town, still visible to the ship through their spyglass. Four marines, Corporal James Thomas, Private Theophilus Hinks, Private Thomas Fatchett and Private John Allen, were also killed and two others were wounded in the confrontation.

The esteem which the islanders nevertheless held for Cook caused them to retain his body. Following their practice of the time, they prepared his body with funerary rituals usually reserved for the chiefs and highest elders of the society. The body was disembowelled, baked to facilitate removal of the flesh, and the bones were carefully cleaned for preservation as religious icons in a fashion somewhat reminiscent of the treatment of European saints in the Middle Ages. Some of Cook's remains, thus preserved, were eventually returned to his crew for a formal burial at sea.

Clerke assumed leadership of the expedition, and made a final attempt to pass through the Bering Strait. He died of tuberculosis on 22 August 1779 and John Gore, a veteran of Cook's first voyage, took command of "Resolution" and of the expedition. James King replaced Gore in command of "Discovery". The expedition returned home, reaching England in October 1780. After their arrival in England, King completed Cook's account of the voyage.

David Samwell, who sailed with Cook on "Resolution", wrote of him: "He was a modest man, and rather bashful; of an agreeable lively conversation, sensible and intelligent. In temper he was somewhat hasty, but of a disposition the most friendly, benevolent and humane. His person was above six feet high: and, though a good looking man, he was plain both in dress and appearance. His face was full of expression: his nose extremely well shaped: his eyes which were small and of a brown cast, were quick and piercing; his eyebrows prominent, which gave his countenance altogether an air of austerity."

The Australian Museum acquired its "Cook Collection" in 1894 from the Government of New South Wales. At that time the collection consisted of 115 artefacts collected on Cook's three voyages throughout the Pacific Ocean, during the period 1768–80, along with documents and memorabilia related to these voyages. Many of the ethnographic artefacts were collected at a time of first contact between Pacific Peoples and Europeans. In 1935 most of the documents and memorabilia were transferred to the Mitchell Library in the State Library of New South Wales. The provenance of the collection shows that the objects remained in the hands of Cook's widow Elizabeth Cook, and her descendants, until 1886. In this year John Mackrell, the great-nephew of Isaac Smith, Elizabeth Cook's cousin, organised the display of this collection at the request of the NSW Government at the Colonial and Indian Exhibition in London. In 1887 the London-based Agent-General for the New South Wales Government, Saul Samuel, bought John Mackrell's items and also acquired items belonging to the other relatives Reverend Canon Frederick Bennett, Mrs Thomas Langton, H. M. C. Alexander, and William Adams. The collection remained with the Colonial Secretary of NSW until 1894, when it was transferred to the Australian Museum.

Cook's 12 years sailing around the Pacific Ocean contributed much to European knowledge of the area. Several islands such as the Sandwich Islands (Hawaii) were encountered for the first time by Europeans, and his more accurate navigational charting of large areas of the Pacific was a major achievement.

To create accurate maps, latitude and longitude must be accurately determined. Navigators had been able to work out latitude accurately for centuries by measuring the angle of the sun or a star above the horizon with an instrument such as a backstaff or quadrant. Longitude was more difficult to measure accurately because it requires precise knowledge of the time difference between points on the surface of the earth. The Earth turns a full 360 degrees relative to the sun each day. Thus longitude corresponds to time: 15 degrees every hour, or 1 degree every 4 minutes.

Cook gathered accurate longitude measurements during his first voyage due to his navigational skills, the help of astronomer Charles Green and by using the newly published Nautical Almanac tables, via the lunar distance method—measuring the angular distance from the moon to either the sun during daytime or one of eight bright stars during night-time to determine the time at the Royal Observatory, Greenwich, and comparing that to his local time determined via the altitude of the sun, moon, or stars. On his second voyage Cook used the K1 chronometer made by Larcum Kendall, which was the shape of a large pocket watch, in diameter. It was a copy of the H4 clock made by John Harrison, which proved to be the first to keep accurate time at sea when used on the ship "Deptford's" journey to Jamaica, 1761–62.

Cook succeeded in circumnavigating the world on his first voyage without losing a single man to scurvy, an unusual accomplishment at the time. He tested several preventive measures but the most important was frequent replenishment of fresh food. It was for presenting a paper on this aspect of the voyage to the Royal Society that he was presented with the Copley Medal in 1776. Ever the observer, Cook was the first European to have extensive contact with various people of the Pacific. He correctly postulated a link among all the Pacific peoples, despite their being separated by great ocean stretches (see Malayo-Polynesian languages). Cook theorised that Polynesians originated from Asia, which scientist Bryan Sykes later verified. In New Zealand the coming of Cook is often used to signify the onset of colonisation.

Cook carried several scientists on his voyages; they made significant observations and discoveries. Two botanists, Joseph Banks and Swede Daniel Solander, were on the first voyage. The two collected over 3,000 plant species. Banks subsequently strongly promoted British settlement of Australia.

Artists also sailed on Cook's first voyage. Sydney Parkinson was heavily involved in documenting the botanists' findings, completing 264 drawings before his death near the end of the voyage. They were of immense scientific value to British botanists. Cook's second expedition included William Hodges, who produced notable landscape paintings of Tahiti, Easter Island, and other locations.

Several officers who served under Cook went on to distinctive accomplishments. William Bligh, Cook's sailing master, was given command of in 1787 to sail to Tahiti and return with breadfruit. Bligh is most known for the mutiny of his crew which resulted in his being set adrift in 1789. He later became governor of New South Wales, where he was the subject of another mutiny—the Rum Rebellion was the only successful armed takeover of an Australian government. George Vancouver, one of Cook's midshipmen, led a voyage of exploration to the Pacific Coast of North America from 1791 to 1794. In honour of his former commander, Vancouver's ship was named . George Dixon, who sailed under Cook on his third expedition, later commanded his own. A lieutenant under Cook, Henry Roberts, spent many years after that voyage preparing the detailed charts that went into Cook's posthumous Atlas, published around 1784.

Cook's contributions to knowledge were internationally recognised during his lifetime. In 1779, while the American colonies were fighting Britain for their independence, Benjamin Franklin wrote to captains of colonial warships at sea, recommending that if they came into contact with Cook's vessel, they were to "not consider her an enemy, nor suffer any plunder to be made of the effects contained in her, nor obstruct her immediate return to England by detaining her or sending her into any other part of Europe or to America; but that you treat the said Captain Cook and his people with all civility and kindness ... as common friends to mankind." Unknown to Franklin, Cook had met his death a month before this safe conduct "passport" was written.

Cook's voyages were involved in another unusual first. The first recorded circumnavigation of the world by an animal was by Cook's goat, who made that memorable journey twice; the first time on HMS "Dolphin", under Samuel Wallis, and then aboard "Endeavour". When they returned to England, Cook had the goat presented with a silver collar engraved with lines from Samuel Johnson: "Perpetui, ambita bis terra, praemia lactis Haec habet altrici Capra secunda Jovis." ("In fame scarce second to the nurse of Jove,/ This Goat, who twice the world had traversed round,/Deserving both her master's care and love,/Ease and perpetual pasture now has found.") She was put to pasture on Cook's farm outside London, and was reportedly admitted to the privileges of the Royal Naval Hospital at Greenwich. Cook's journal recorded the date of the goat's death: 28 March 1772.

A US coin, the 1928 Hawaiian Sesquicentennial half dollar carries Cook's image. Minted for the 150th anniversary of his discovery of the islands, its low mintage (10,008) has made this example of Early United States commemorative coins both scarce and expensive. The site where he was killed in Hawaii was marked in 1874 by a white obelisk set on of chained-off beach. This land, although in Hawaii, was deeded to the United Kingdom. A nearby town is named Captain Cook, Hawaii; several Hawaiian businesses also carry his name. The Apollo 15 Command/Service Module "Endeavour" was named after Cook's ship, , as was the . Another shuttle, "Discovery", was named after Cook's .

The first institution of higher education in North Queensland, Australia was named after him, with James Cook University opening in Townsville in 1970. In Australian rhyming slang the expression ""Captain Cook"" means ""look"". Numerous institutions, landmarks and place names reflect the importance of Cook's contributions, including the Cook Islands, the Cook Strait, Cook Inlet, and the Cook crater on the Moon. Aoraki/Mount Cook, the highest summit in New Zealand, is named for him. Another Mount Cook is on the border between the US state of Alaska and the Canadian Yukon Territory, and is designated Boundary Peak 182 as one of the official Boundary Peaks of the Hay–Herbert Treaty. A life-size statue of Cook upon a column stands in Hyde Park located in the centre of Sydney. A large aquatic monument is planned for Cook's landing place at Botany Bay, Sydney.

One of the earliest monuments to Cook in the United Kingdom is located at The Vache, erected in 1780 by Admiral Hugh Palliser, a contemporary of Cook and one-time owner of the estate. A huge obelisk was built in 1827 as a monument to Cook on Easby Moor overlooking his boyhood village of Great Ayton, along with a smaller monument at the former location of Cook's cottage. There is also a monument to Cook in the church of St Andrew the Great, St Andrew's Street, Cambridge, where his sons Hugh, a student at Christ's College, and James were buried. Cook's widow Elizabeth was also buried in the church and in her will left money for the memorial's upkeep. The 250th anniversary of Cook's birth was marked at the site of his birthplace in Marton, by the opening of the Captain Cook Birthplace Museum, located within Stewart Park (1978). A granite vase just to the south of the museum marks the approximate spot where he was born. Tributes also abound in post-industrial Middlesbrough, including a primary school, shopping square and the "Bottle 'O Notes", a public artwork by Claes Oldenburg, that was erected in the town's Central Gardens in 1993. Also named after Cook is the James Cook University Hospital, a major teaching hospital which opened in 2003 with a railway station serving it called James Cook opening in 2014.
The Royal Research Ship RRS "James Cook" was built in 2006 to replace the RRS "Charles Darwin" in the UK's Royal Research Fleet, and Stepney Historical Trust placed a plaque on Free Trade Wharf in the Highway, Shadwell to commemorate his life in the East End of London. In 2002 Cook was placed at number 12 in the BBC's poll of the 100 Greatest Britons.








</doc>
<doc id="15632" url="https://en.wikipedia.org/wiki?curid=15632" title="John Baskerville">
John Baskerville

John Baskerville (baptised 28 January 1706 – 8 January 1775) was an English businessman, in areas including japanning and papier-mâché, but he is best remembered as a printer and type designer.

Baskerville was born in the village of Wolverley, near Kidderminster in Worcestershire and baptised on 28 January at Wolverley church. At the time of his birth this was considered the year 1706; it would now be considered early 1707. Baskerville established an early career teaching handwriting and is known to have offered his services cutting gravestones (a demonstration slab by him survives in the Library of Birmingham) before making a considerable fortune from the manufacture of lacquerwork items (japanning).

He practised as a printer in Birmingham, England. Baskerville was a member of the Royal Society of Arts, and an associate of some of the members of the Lunar Society. He directed his punchcutter, John Handy, in the design of many typefaces of broadly similar appearance. In 1757, Baskerville published a remarkable quarto edition of Virgil on wove paper, using his own type. It took three years to complete, but it made such an impact that he was appointed printer to the University of Cambridge the following year.
John Baskerville printed works for the University of Cambridge in 1758 and, although an atheist, printed a splendid folio Bible in 1763. His typefaces were greatly admired by Benjamin Franklin, a fellow printer. Baskerville's work was criticised by jealous competitors and soon fell out of favour, but since the 1920s many new fonts have been released by Linotype, Monotype, and other type foundries – revivals of his work and mostly called 'Baskerville'. Emigre released a popular revival of this typeface in 1996 called Mrs Eaves, named for Baskerville's wife, Sarah Eaves. Baskerville's most notable typeface Baskerville represents the peak of transitional type face and bridges the gap between Old Style and Modern type design.

Baskerville also was responsible for significant innovations in printing, paper and ink production. He developed a technique which produced a smoother whiter paper which showcased his strong black type. Baskerville also pioneered a completely new style of typography adding wide margins and leading between each line.

Baskerville died in January 1775 at his home, "Easy Hill". He requested that his body be placed

However, in 1821 a canal was built through the land and his body was placed on show by the landowner until Baskerville's family and friends arranged to have it moved to the crypt of Christ Church, Birmingham. Christ Church was demolished in 1897 so his remains were then moved, with other bodies from the crypt, to consecrated catacombs at Warstone Lane Cemetery. In 1963 a petition was presented to Bimingham City Council requesting that he be reburied in unconsecrated ground according to his wishes. 

Baskerville House was built on the grounds of "Easy Hill". In 1947, BBC radio broadcast a radio play about his burial, named "Hic Jacet: or The Corpse in the Crescent" by Neville Brendon Watts. The original recording was not preserved but a performance was staged by students at the Birmingham School of Acting in 2013 at the Typographic Hub Centre of Birmingham City University.

A Portland stone sculpture of the Baskerville typeface, "Industry and Genius", in his honour stands in front of Baskerville House in Centenary Square, Birmingham. It was created by local artist David Patten.

Some examples of volumes published by Baskerville.





</doc>
<doc id="15640" url="https://en.wikipedia.org/wiki?curid=15640" title="John Young">
John Young

John Young may refer to:










</doc>
<doc id="15641" url="https://en.wikipedia.org/wiki?curid=15641" title="Joseph Stalin">
Joseph Stalin

Joseph Stalin (born Ioseb Besarionis dze Jughashvili; 18 December 1878 – 5 March 1953) was a Soviet revolutionary and politician. He ruled the Soviet Union from the mid-1920s until his death in 1953, holding the titles of General Secretary of the Communist Party of the Soviet Union from 1922 to 1952 and the nation's Premier from 1941 to 1953. Initially presiding over an oligarchic one-party state that governed by consensus, he became the "de facto" dictator of the Soviet Union by the 1930s. Ideologically committed to the Leninist interpretation of Marxism, Stalin helped to formalise these ideas as Marxism–Leninism while his own policies became known as Stalinism.
Stalin was born in the Georgian town of Gori, the son of a shoemaker. He began his revolutionary career in his youth by joining the Marxist Russian Social Democratic Labour Party. There, he edited the party's newspaper, "Pravda", and raised funds for Vladimir Lenin's Bolshevik faction via robberies, kidnappings, and protection rackets. Repeatedly arrested, he underwent several internal exiles. After the Bolsheviks seized power in Russia during the 1917 October Revolution, Stalin joined the party's governing Politburo where he was instrumental in overseeing the Soviet Union's establishment in 1922. Despite Lenin's opposition, he assumed leadership over the country shortly after the former's death in 1924. During Stalin's rule, "Socialism in One Country" became a central tenet of the party's dogma, and Lenin's New Economic Policy was replaced with a centralized command economy. Under the Five-Year Plan system, the country underwent collectivisation and rapid industrialization but also experienced significant disruptions in food production that contributed to the famine of 1932–33. To eradicate those regarded as "enemies of the working class", Stalin instituted the "Great Purge" in which over a million were imprisoned and at least 700,000 were executed from 1934 to 1939.
Stalin's government promoted Marxism–Leninism abroad through the Communist International and supported anti-fascist movements throughout Europe during the 1930s, particularly in the Spanish Civil War. In 1939 it signed a non-aggression pact with Nazi Germany, resulting in their joint invasion of Poland. Germany ended the pact by invading the Soviet Union in 1941. Despite initial setbacks, the Soviet Red Army halted the German incursion and captured Berlin in 1945, ending World War II in Europe. The Soviets annexed the Baltic states and helped establish Soviet-aligned governments throughout most of Central and Eastern Europe and in China and North Korea. The Soviet Union and the United States emerged from the war as the two world superpowers. Tensions escalated into a Cold War between the Soviet-backed Eastern Bloc and U.S.-backed Western Bloc. Stalin led his country through its post-war reconstruction, during which it developed a nuclear weapon in 1949. In these years, the country experienced another major famine and a period of antisemitism peaking in the 1952–53 Doctors' plot. Stalin died in 1953 and was eventually succeeded by Nikita Khrushchev, who denounced his predecessor and initiated a de-Stalinisation process throughout Soviet society.
Widely considered one of the 20th century's most significant figures, Stalin was the subject of a pervasive personality cult within the international Marxist–Leninist movement, for whom Stalin was a champion of socialism and the working class. Since the 1991 dissolution of the Soviet Union, Stalin has retained popularity in Russia and Georgia as a victorious wartime leader who established the Soviet Union as a major world power. Conversely, his totalitarian government has been widely condemned for overseeing mass repressions, ethnic cleansing, hundreds of thousands of executions, and famines which caused the deaths of millions.

Stalin was born in the Georgian town of Gori on . He was the son of Besarion Jughashvili and Ekaterine "Keke" Geladze, They were ethnically Georgian and Stalin grew up speaking the Georgian language. Gori was then part of the Russian Empire, and was home to a population of 20,000, the majority of whom were Georgian but with Armenian, Russian, and Jewish minorities. Stalin was baptised on 29 December. He was nicknamed "Soso", a diminutive of "Ioseb".
Besarion was a shoemaker and owned his own workshop; it was initially a financial success, but later fell into decline due to Besarion's alcoholism. The family found themselves living in poverty, moving through nine different rented rooms in ten years. Besarion beat his wife and son. To escape the abusive relationship, Keke took Stalin and moved into the house of a family friend, Father Christopher Charkviani. She worked as a house cleaner and launderer for local families sympathetic to her plight. Keke was determined to send her son to school, something that none of the family had previously achieved. In late 1888, aged 10 Stalin enrolled at the Gori Church School. This was normally reserved for the children of clergy, although Charkviani ensured that the boy received a place. Stalin excelled academically, displaying talent in painting and drama classes, writing his own poetry, and singing as a choirboy. He got into many fights, and a childhood friend later noted that Stalin "was the best but also the naughtiest pupil" in the class. Stalin faced several severe health problems; in 1884, he contracted smallpox and was left with facial pock scars. Aged 12, he was seriously injured after being hit by a phaeton, which was the likely cause of a lifelong disability to his left arm.

At his teachers' recommendation, Stalin proceeded to the Spiritual Seminary in Tiflis. He enrolled at the school in August 1894, enabled by a scholarship that allowed him to study at a reduced rate. Here he joined 600 trainee priests who boarded at the seminary. Stalin was again academically successful and gained high grades. He continued writing poetry; five of his poems were published under the pseudonym of "Soselo" in Ilia Chavchavadze's newspaper "Iveria" ('Georgia'). Thematically, they dealt with topics like nature, land, and patriotism. According to Stalin's biographer Simon Sebag Montefiore, they became "minor Georgian classics", and were included in various anthologies of Georgian poetry over the coming years. As he grew older, Stalin lost interest in his studies; his grades dropped, and he was repeatedly confined to a cell for his rebellious behaviour. Teachers complained that he declared himself an atheist, chatted in class and refused to doff his hat to monks.

Stalin joined a forbidden book club active at the school; he was particularly influenced by Nikolay Chernyshevsky's 1863 pro-revolutionary novel "What Is To Be Done?". Another influential text was Alexander Kazbegi's "The Patricide", with Stalin adopting the nickname "Koba" from that of the book's bandit protagonist. He also read "", the 1867 book by German sociological theorist Karl Marx. Stalin devoted himself to Marx's socio-political theory, Marxism, which was then on the rise in Georgia, one of various forms of socialism opposed to the empire's governing Tsarist authorities. At night, he attended secret workers' meetings, and was introduced to Silibistro "Silva" Jibladze, the Marxist founder of Mesame Dasi ('Third Group'), a Georgian socialist group. In April 1899, Stalin left the seminary and never returned, although the school encouraged him to come back.

In October 1899, Stalin began work as a meteorologist at a Tiflis observatory, a position that allowed him to read while on duty. Stalin gave classes in socialist theory and attracted a group of young men around him. He co-organised a secret mass meeting of workers for May Day 1900, at which he successfully encouraged many of the men to take strike action. By this point, the empire's secret police—the Okhrana—were aware of Stalin's activities within Tiflis' revolutionary milieu. They attempted to arrest him in March 1901, but he escaped and went into hiding, living off the donations of friends and sympathisers. Remaining underground, he helped to plan a demonstration for May Day 1901, in which 3,000 marchers clashed with the authorities. He continued to evade arrest by using aliases and sleeping in different apartments. In November 1901, he was elected to the Tiflis Committee of the Russian Social Democratic Labour Party (RSDLP), a Marxist party founded in 1898.

That month, Stalin travelled to the port city of Batumi. His militant rhetoric proved divisive among the city's Marxists, some of whom suspected that he might be an "agent provocateur" secretly working for the government. He found employment at the Rothschild refinery storehouse, where he co-organised two workers' strikes. After several strike leaders were arrested, he co-organised a mass public demonstration that led to the storming of the prison; troops fired upon the demonstrators, 13 of whom were killed. Stalin organised a second mass demonstration on the day of their funeral, before being arrested in April 1902. He was initially held at Batumi Prison, and later moved to the more secure Kutaisi Prison. In mid-1903, Stalin was sentenced to three years of exile in eastern Siberia.

Stalin left Batumi in October, arriving at the small Siberian town of Novaya Uda in late November. There, he lived in a two-room peasant's house, sleeping in the building's larder. Stalin made two escape attempts; on the first he made it to Balagansk before returning due to frostbite. His second attempt was successful and he made it to Tiflis. There, he co-edited a Georgian Marxist newspaper, "Proletariatis Brdzola" ("Proletarian Struggle"), with Philip Makharadze. He called for the Georgian Marxist movement to split off from its Russian counterpart, resulting in several RSDLP members claiming that his views were contrary to the ethos of Marxist internationalism and calling for his expulsion from the party; he soon recanted his views. During his exile, the RSDLP had split between Vladimir Lenin's Bolsheviks and Julius Martov's Mensheviks. Stalin detested many of the Mensheviks in Georgia and aligned himself with the Bolsheviks. Although Stalin established a Bolshevik stronghold in the mining town of Chiatura, Bolshevism remained a minority force in the Menshevik-dominated Georgian revolutionary scene.

In January 1905, government troops massacred protesters in Saint Petersburg. Unrest soon spread across the Russian Empire in what came to be known as the Revolution of 1905. Georgia was one of the regions particularly affected. In February, Stalin was in Baku when ethnic violence broke out between Armenians and Azeris; at least 2,000 were killed. Stalin publicly lambasted the "pogroms against Jews and Armenians" as being part of Tsar Nicholas II's attempts to "buttress his despicable throne". He formed a Bolshevik Battle Squad which he used to try and keep Baku's warring ethnic factions apart, also using the unrest to steal printing equipment. Amid the growing violence throughout Georgia, Stalin formed further Battle Squads, with the Mensheviks doing the same. Stalin's Squads disarmed local police and troops, raided government arsenals, and raised funds through protection rackets on large local businesses and mines. They launched attacks on the government's Cossack troops and pro-Tsarist Black Hundreds, co-ordinating some of their operations with the Menshevik militia.

In November 1905, the Georgian Bolsheviks elected Stalin as one of their delegates to a Bolshevik conference in Saint Petersburg. On arrival, he met Lenin's wife Nadezhda Krupskaya, who informed him that the venue had been moved to Tampere in the Grand Duchy of Finland. At the conference Stalin met Lenin for the first time. Although Stalin held Lenin in deep respect, he was vocal in his disagreement with Lenin's view that the Bolsheviks should field candidates for the forthcoming election to the State Duma; Stalin saw the parliamentary process as a waste of time. 

In April 1906, Stalin attended the RSDLP Fourth Congress in Stockholm; this was his first trip outside the Russian Empire. At the conference, the RSDLP—then led by its Menshevik majority—agreed that it would not raise funds using armed robbery. Lenin and Stalin disagreed with this decision, and later privately discussed how they could continue the robberies for the Bolshevik cause.
Stalin married Kato Svanidze in a church ceremony at Senaki in July 1906. In March 1907 she bore a son, Yakov. By that year—according to the historian Robert Service—Stalin had established himself as "Georgia's leading Bolshevik". He attended the Fifth RSDLP Congress, held in London in May–June 1907. After returning to Tiflis, Stalin organized the robbing of a large delivery of money to the Imperial Bank in June 1907. His gang ambushed the armed convoy in Yerevan Square with gunfire and home-made bombs. Around 40 people were killed, but all of his gang escaped alive.
After the heist, Stalin settled in Baku with his wife and son. There, Mensheviks confronted Stalin about the robbery and voted to expel him from the RSDLP, but he took no notice of them. 

In Baku, Stalin secured Bolshevik domination of the local RSDLP branch, and edited two Bolshevik newspapers, "Bakinsky Proletary" and "Gudok" ("Whistle"). In August 1907, he attended the Seventh Congress of the Second International—an international socialist organisation—in Stuttgart, Germany. In November 1907, his wife died of typhus, and he left his son with her family in Tiflis. In Baku he had reassembled his gang, the Outfit, which continued to attack Black Hundreds and raised finances by running protection rackets, counterfeiting currency, and carrying out robberies. They also kidnapped the children of several wealthy figures in order to extract ransom money. In early 1908, he travelled to the Swiss city of Geneva to meet with Lenin and the prominent Russian Marxist Georgi Plekhanov, although the latter exasperated him.

In March 1908, Stalin was arrested and interred in Bailov Prison where he led the imprisoned Bolsheviks, organised discussion groups, and ordered the killing of suspected informants. He was eventually sentenced to two years exile in the village of Solvychegodsk, Vologda Province, arriving there in February 1909. In June, he escaped the village and made it to Kotlas disguised as a woman and from there to Saint Petersburg. In March 1910, he was arrested again, and sent back to Solvychegodsk. There he had affairs with at least two women; his landlady, Maria Kuzakova, later gave birth to his second son, Konstantin. In June 1911, Stalin was given permission to move to Vologda, where he stayed for two months, having a relationship with Pelageya Onufrieva. He proceeded to Saint Petersburg, where he was arrested in September 1911, and sentenced to a further three-year exile in Vologda.

While Stalin was in exile, the first Bolshevik Central Committee had been elected at the Prague Conference, after which Lenin and Grigory Zinoviev invited Stalin to join it. Still in Vologda, Stalin agreed, remaining a Central Committee member for the rest of his life. Lenin believed that Stalin, as a Georgian, would be useful in helping to secure support for the Bolsheviks from the Empire's minority ethnicities. In February 1912, Stalin escaped to Saint Petersburg, tasked with converting the Bolshevik weekly newspaper, "Zvezda" ("Star") into a daily, "Pravda" ("Truth"). The new newspaper was launched in April 1912, although Stalin's role as editor was kept secret. 

In May 1912, he was arrested again and imprisoned in the Shpalerhy Prison, before being sentenced to three years exile in Siberia. In July, he arrived at the Siberian village of Narym, where he shared a room with fellow Bolshevik Yakov Sverdlov. After two months, Stalin and Sverdlov escaped back to Saint Petersburg.
During a brief period back in Tiflis, Stalin and the Outfit planned the ambush of a mail coach, during which most of the group—although not Stalin—were apprehended by the authorities. Stalin returned to Saint Petersburg, where he continued editing and writing articles for "Pravda". 

After the October 1912 Duma elections resulted in six Bolsheviks and six Mensheviks being elected, Stalin wrote articles calling for reconciliation between the two Marxist factions, for which he was criticised by Lenin. In late 1912, he twice crossed into the Austro-Hungarian Empire to visit Lenin in Kraków, eventually bowing to Lenin's opposition to reunification with the Mensheviks.
In January 1913 Stalin travelled to Vienna, there focusing his attention on the 'national question' of how the Bolsheviks should deal with the Russian Empire's national and ethnic minorities. Lenin wanted to attract these groups to the Bolshevik cause by offering them the right of secession from the Russian state, but at the same time he hoped that they would remain part of a future Bolshevik-governed Russia. Stalin's finished article was titled "Marxism and the National Question"; Lenin was very happy with it. According to Montefiore, this was "Stalin's most famous work". The article was published under the pseudonym of "K. Stalin", a name he had been using since 1912. This name derived from the Russian language word for steel ("stal"), and has been translated as "Man of Steel"; it may have also been intended to imitate Lenin's pseudonym. Stalin retained this name for the rest of his life, possibly because it had been used on the article which established his reputation among the Bolsheviks.

In February 1913, Stalin was arrested while back in Saint Petersburg. He was sentenced to four years exile in Turukhansk, a remote part of Siberia from which escape was particularly difficult. In August, he arrived in the village of Monastyrskoe, although after four weeks was relocated to the hamlet of Kostino. In March 1914, concerned over a potential escape attempt, the authorities moved Stalin to the hamlet of Kureika on the edge of the Arctic Circle. In the hamlet, Stalin had a relationship with Lidia Pereprygia, who was thirteen at the time and thus a year under the legal age of consent in Tsarist Russia. Circa December 1914, Pereprygia gave birth to Stalin's child, although the infant soon died. She gave birth to another of his children, Alexander, circa April 1917. In Kureika, Stalin lived closely with the indigenous Tunguses and Ostyak, and spent much of his time fishing.

While Stalin was in exile, Russia entered the First World War, and in October 1916 Stalin and other exiled Bolsheviks were conscripted into the Russian Army, leaving for Monastyrskoe. They arrived in Krasnoyarsk in February 1917, where a medical examiner ruled Stalin unfit for military service due to his crippled arm. Stalin was required to serve four more months on his exile, and he successfully requested that he serve it in nearby Achinsk. Stalin was in the city when the February Revolution took place; uprisings broke out in Petrograd—as Saint Petersburg had been renamed—and Tsar Nicholas II abdicated to escape being violently overthrown. The Russian Empire became a "de facto" republic, headed by a Provisional Government dominated by liberals. In a celebratory mood, Stalin travelled by train to Petrograd in March. There, Stalin and fellow Bolshevik Lev Kamenev assumed control of "Pravda", and Stalin was appointed the Bolshevik representative to the Executive Committee of the Petrograd Soviet, an influential council of the city's workers. In April, Stalin came third in the Bolshevik elections for the party's Central Committee; Lenin came first and Zinoviev came second. This reflected his senior standing in the party at the time.

Stalin helped to organise the July Days uprising, an armed display of strength by Bolshevik supporters. After the armed demonstration was suppressed, the Provisional Government initiated a crackdown on the Bolsheviks, raiding "Pravda". During this raid, Stalin smuggled Lenin out of the newspaper's office and took charge of the Bolshevik leader's safety, moving him between Petrograd safe houses before smuggling him to Razliv. In Lenin's absence, Stalin continued editing "Pravda" and served as acting leader of the Bolsheviks, overseeing the party's Sixth Congress, which was held covertly. Lenin began calling for the Bolsheviks to seize power by toppling the Provisional Government in a coup. Stalin and fellow senior Bolshevik Leon Trotsky both endorsed Lenin's plan of action, but it was initially opposed by Kamenev and other party members. Lenin returned to Petrograd and at a meeting of the Central Committee on 10 October, he secured a majority in favour of a coup.

On 24 October, police raided the Bolshevik newspaper offices, smashing machinery and presses; Stalin salvaged some of this equipment in order to continue his activities. In the early hours of 25 October, Stalin joined Lenin in a Central Committee meeting in the Smolny Institute, from where the Bolshevik coup—the October Revolution—was directed. Bolshevik militia seized Petrograd's electric power station, main post office, state bank, telephone exchange, and several bridges. A Bolshevik-controlled ship, the "Aurora", opened fire on the Winter Palace; the Provisional Government's assembled delegates surrendered and were arrested by the Bolsheviks. Although he had been tasked with briefing the Bolshevik delegates of the Second Congress of Soviets about the developing situation, Stalin's role in the coup had not been publicly visible. Trotsky and other later Bolshevik opponents of Stalin used this as evidence that his role in the coup had been insignificant, although later historians reject this. According to the historian Oleg Khlevniuk, Stalin "filled an important role [in the October Revolution]... as a senior Bolshevik, member of the party's Central Committee, and editor of its main newspaper"; the historian Stephen Kotkin similarly noted that Stalin had been "in the thick of events" in the build-up to the coup.

On 26 October, Lenin formed a new government, the Council of People's Commissars ("Sovnarkom"), which he led as Chairman. Stalin backed Lenin's decision not to form a coalition with the Mensheviks and Socialist Revolutionary Party, although they did form a coalition government with the Left Socialist Revolutionaries. Stalin became part of an informal foursome leading the government, alongside Lenin, Trotsky, and Sverdlov; of these, Sverdlov was regularly absent, and died in March 1919. Stalin's office was based near to Lenin's in the Smolny Institute, and he and Trotsky were the only individuals allowed access to Lenin's study without an appointment. Although not so publicly well known as Lenin or Trotsky, Stalin's importance among the Bolsheviks grew. He co-signed Lenin's decrees shutting down hostile newspapers, and with Sverdlov chaired the sessions of the committee drafting a constitution for the new Russian Soviet Federative Socialist Republic. He strongly supported Lenin's formation of the Cheka security service and the subsequent Red Terror that it initiated; noting that state violence had proved an effective tool for capitalist powers, he believed that it would prove the same for the Soviet government. Unlike senior Bolsheviks like Kamenev and Nikolai Bukharin, Stalin never expressed concern about the rapid growth and expansion of the Cheka and Terror.

Having dropped his editorship of "Pravda", Stalin was appointed the People's Commissar for Nationalities. He took Nadezhda Alliluyeva as his secretary, and at some point married her, although the wedding date is unknown. In November 1917, he signed the Decree on Nationality, according ethnic and national minorities living in Russia the right of secession and self-determination. The decree's purpose was primarily strategic; the Bolsheviks wanted to gain favour among ethnic minorities but hoped that the latter would not actually desire independence. That month, he travelled to Helsinki to talk with the Finnish Social-Democrats, granting Finland's request for independence in December. His department allocated funds for the establishment of presses and schools in the languages of various ethnic minorities. Socialist Revolutionaries accused Stalin's talk of federalism and national self-determination as a front for Sovnarkom's centralising and imperialist policies.

Due to the ongoing First World War, in which Russia was fighting the Central Powers of Germany and Austria-Hungary, Lenin's government relocated from Petrograd to Moscow in March 1918. There, they based themselves in the Kremlin; it was here that Stalin, Trotsky, Sverdlov, and Lenin lived. Stalin supported Lenin's desire to sign an armistice with the Central Powers regardless of the cost in territory. Stalin thought it necessary because he was unconvinced that Europe itself was on the verge of proletarian revolution, a view that irked Lenin. Lenin eventually convinced the other senior Bolsheviks of his viewpoint, resulting in the signing of the Treaty of Brest-Litovsk in March 1918. The treaty gave vast areas of land and resources to the Central Powers and angered many in Russia; the Left Socialist Revolutionaries withdrew from the coalition government over the issue. The governing RSDLP party was soon renamed, becoming the Russian Communist Party.

After the Bolsheviks seized power, both right and left-wing armies rallied against them, generating the Russian Civil War. To secure access to the dwindling food supply, in May 1918 Sovnarkom sent Stalin to Tsaritsyn to take charge of food procurement in southern Russia. Eager to prove himself as a commander, once there he took control of regional military operations. He befriended two military figures, Kliment Voroshilov and Semyon Budyonny, who would form the nucleus of his military and political support base. Believing that victory was assured by numerical superiority, he sent large numbers of Red Army troops into battle against the region's anti-Bolshevik White armies, resulting in heavy losses; Lenin was concerned by this costly tactic. In Tsaritsyn, Stalin commanded the local Cheka branch to execute suspected counter-revolutionaries, sometimes without trial, and—in contravention of government orders—purged the military and food collection agencies of middle-class specialists, some of whom he also executed. His use of state violence and terror was at a greater scale than most Bolshevik leaders approved of; for instance, he ordered several villages to be torched to ensure compliance with his food procurement program.

In December 1918, Stalin was sent to Perm to lead an inquiry into how Alexander Kolchak's White forces had been able to decimate Red troops based there. He returned to Moscow between January and March 1919, before being assigned to the Western Front at Petrograd. When the Red Third Regiment defected, he ordered the public execution of captured defectors. In September he was returned to the Southern Front. During the war, he proved his worth to the Central Committee, displaying decisiveness, determination, and a willingness to take on responsibility in conflict situations. At the same time, he disregarded orders and repeatedly threatened to resign when affronted. In November 1919, the government awarded him the Order of the Red Banner for his wartime service.

The Bolsheviks had won the civil war by late 1919. Sovnarkom turned its attention to spreading proletarian revolution abroad, to this end forming the Communist International in March 1919; Stalin attended its inaugural ceremony. Although Stalin did not share Lenin's belief that the European proletariat were on the verge of revolution, he acknowledged that as long as it stood alone, Soviet Russia remained vulnerable. In December 1918, he had drawn up decrees recognising Marxist-governed Soviet republics in Estonia, Lithuania, and Latvia; during the civil war these Marxist governments had been overthrown and the Baltic countries became fully independent of Russia, an act which he regarded as illegitimate. In February 1920, Stalin was appointed to head the Workers' and Peasants' Inspectorate; that same month he was also transferred to the Caucasian Front.

Following earlier clashes between Polish and Russian troops, the Polish–Soviet War broke out in early 1920, with the Poles invadind Ukraine and taking Kiev. Stalin was moved to Ukraine, on the Southwest Front. The Red Army forced the Polish troops back into Poland. Lenin believed that the Polish proletariat would rise up to support the Russians against Józef Piłsudski's Polish government. Stalin had cautioned against this; he believed that nationalism would lead the Polish working-classes to support their government's war effort. He also believed that the Red Army was ill-prepared to conduct an offensive war and that it would give White Armies a chance to resurface in Crimea, potentially reigniting the civil war. Stalin lost the argument, after which he accepted Lenin's decision and supported it. Along the Southwest Front, he became determined to conquer Lwów; in focusing on this goal he disobeyed orders to transfer his troops to assist Mikhail Tukhachevsky's forces. In August, the Poles repulsed the Russian advance and Stalin returned to Moscow. A Polish-Soviet peace treaty was signed; Stalin saw this as a failure for which he blamed Trotsky. In turn, Trotsky accused Stalin of "strategic mistakes" in his handling of the war at the 9th Bolshevik Conference. Stalin felt resentful and under-appreciated; in September he demanded demission from the military, which was granted.

The Soviet government sought to bring neighbouring states under its domination; in February 1921 it invaded the Menshevik-governed Democratic Republic of Georgia, while in April 1921, Stalin ordered the Red Army to march into the Turkestan Autonomous Soviet Socialist Republic to reassert Russian state control. As People's Commissar for Nationalities, Stalin believed that each national and ethnic group should have the right to self-expression, facilitating this through "autonomous republics" within the Russian state in which they could oversee various regional affairs. In taking this view, some Marxists accused him of bending too much to bourgeois nationalism, while others accused him of remaining too Russocentric by seeking to retain these nations within the Russian state.

Stalin's native Caucasus posed a particular problem due to its highly multi-cultural mix. Stalin opposed the idea of separate Georgian, Armenian, and Azerbaijani autonomous republics, arguing that these would likely oppress ethnic minorities within their respective territories; instead he called for the formation of a Transcaucasian Socialist Federative Soviet Republic. The Georgian Communist Party opposed the idea, resulting in the Georgian Affair. In mid-1921, Stalin returned to the southern Caucasus, there calling on Georgian Communists to avoid the chauvinistic Georgian nationalism which he believed marginalised the Abkhazian, Ossetian, and Adjarian minorities living in Georgia. On this trip, Stalin met with his son Yakov, and brought him back to Moscow with him; Nadya had given birth to another of Stalin's sons, Vasily, in March 1921.

After the civil war, workers' strikes and peasant uprisings broke out across Russia, largely in opposition to Sovnarkom's food requisitioning project; as an antidote, Lenin introduced market-oriented reforms: the New Economic Policy (NEP). There was also internal turmoil in the Communist Party, as Trotsky led a faction calling for the abolition of trade unions; Lenin opposed this and Stalin helped to rally opposition to Trotsky's position. Stalin also agreed to supervise the Department of Agitation and Propaganda in the Central Committee Secretariat. At the 11th Party Congress in 1922, Lenin nominated Stalin as the party's new General Secretary. Although concerns were expressed that adopting this new post on top of his others would overstretch his workload and give him too much power, Stalin was appointed to the position. For Lenin, it was advantageous to have a key ally in this crucial post.

In May 1922, a massive stroke left Lenin partially paralysed. Residing at his Gorki dacha, Lenin's main connection to Sovnarkom was through Stalin, who was a regular visitor. Lenin twice asked Stalin to procure poison so that he could commit suicide, but Stalin never did so. Despite this comradeship, Lenin disliked what he referred to as Stalin's "Asiatic" manner, and told his sister Maria that Stalin was "not intelligent". Lenin and Stalin argued on the issue of foreign trade; Lenin believed that the Soviet state should have a monopoly on foreign trade, but Stalin supported Grigori Sokolnikov's view that doing so was impractical at that stage. Another disagreement came over the Georgian Affair, with Lenin backing the Georgian Central Committee's desire for a Georgian Soviet Republic over Stalin's idea of a Transcaucasian one.

They also disagreed on the nature of the Soviet state. Lenin called for the country to be renamed the "Union of Soviet Republics of Europe and Asia", reflecting his desire for expansion across the two continents. Stalin believed that this would encourage independence sentiment among non-Russians, instead arguing that ethnic minorities would be content as "autonomous republics" within the Russian Soviet Federative Socialist Republic. Lenin accused Stalin of "Great Russian chauvinism"; Stalin accused Lenin of "national liberalism". A compromise was reached, in which the country would be renamed the "Union of Soviet Socialist Republics" (USSR). The USSR's formation was ratified in December 1922; although officially a federal system, all major decisions were taken by the governing Politburo of the Communist Party of the Soviet Union in Moscow. Their differences were not just based on policy but also became personal; Lenin was particularly angered when Stalin was rude to his wife Krupskaya during a telephone conversation. In the final years of his life, Lenin dictated increasingly disparaging notes on Stalin that became his testament. He criticized Stalin's rude manners and excessive power, suggesting that Stalin should be removed from the position of General Secretary.

Lenin died in January 1924. Stalin took charge of the funeral and was one of its pallbearers; against the wishes of Lenin's widow, the Politburo embalmed his corpse and placed it within a mausoleum in Moscow's Red Square. It was incorporated into a growing personality cult devoted to Lenin, with Petrograd being renamed "Leningrad" that year. To bolster his image as a devoted Leninist, Stalin was eager to present himself as a theorist, giving nine lectures at Sverdlov University on the "Foundations of Leninism"; it was later published as a concise overview of Lenin's ideas. At the following 13th Party Congress, Lenin's Testament was read out to senior figures. Embarrassed by its contents, Stalin offered his resignation as General Secretary; this act of humility saved him and he was retained in the position. 

As General Secretary, Stalin had had a free hand in making appointments to his own staff, implanting his loyalists throughout the party and administration. He also ensured that these loyalists were dispersed across the USSR's various regions. He favoured new Communist Party members, many from worker and peasant backgrounds, to the Old Bolsheviks who tended to be university educated. Stalin had much contact with young party functionaries, and the desire for promotion led many provincial figures to seek to impress Stalin and gain his favour. Stalin also developed close relations with the three men at the heart of the secret police (first the Cheka and then its replacement, the State Political Directorate): Felix Dzerzhinsky, Genrikh Yagoda, and Vyacheslav Menzhinsky. In his private life, he was dividing his time between his Kremlin apartment and a dacha he had obtained at Zubalova. His wife had given birth to a daughter, Svetlana, in February 1926.

In the wake of Lenin's death, various protagonists emerged in the struggle to become his successor: alongside Stalin was Trotsky, Zinoviev, Kamenev, Bukharin, Alexei Rykov and Mikhail Tomsky. Stalin saw Trotsky—whom he personally despised—as the main obstacle to his rise to dominance within the Communist Party, and while Lenin had been ill he had forged an anti-Trotsky alliance with Kamenev and Zinoviev. Although Zinoviev had expressed concern about Stalin's growing authority, he rallied behind him at the 13th Congress as a counterweight to Trotsky, who now led a party faction known as the Left Opposition. The Left Opposition believed that too many concessions to capitalism had been made with the NEP; Stalin was deemed a "rightist" in the party for his support of the policy. Stalin built up a retinue of his supporters in the Central Committee, while the Left Opposition were gradually removed from their positions of influence. He was supported in this by Bukharin, who like Stalin believed that implementing the Left Opposition's proposals would plunge the Soviet Union into instability.

In late 1924, Stalin also removed Kamenev and Zinoviev's supporters from key positions. In 1925, Kamenev and Zinoviev moved into open opposition of Stalin and Bukharin. They attacked one another at the 14th Party Congress, where Stalin accused Kamenev and Zinoviev of reintroducing factionalism—and thus instability—into the party. In mid-1926, Kamenev and Zinoviev joined with Trotsky's supporters to form the United Opposition against Stalin; in October they agreed to stop factional activity under threat of expulsion, and later publicly recanted their views under Stalin's command. The factionalist arguments continued, with Stalin threatening to resign in both December 1926 and December 1927. In October 1927, Zinoviev and Trotsky were removed from the Central Committee; the latter was exiled to Kazakhstan and later deported from the country in 1929. Some of those United Opposition members who were repentant were later rehabilitated and allowed to return to government. Stalin had established himself as the party's supreme leader, although was not the head of government, a task he entrusted to key ally Vyacheslav Molotov. Other important supporters on the Politburo were Voroshilov, Lazar Kaganovich, and Sergo Ordzhonikidze, with Stalin ensuring that his allies ran the various state institutions. According to Montefiore, at this point "Stalin was the leader of the oligarchs but he was far from a dictator".

Stalin's growing influence was reflected in the decision to name various locations after him; in June 1924 the Ukrainian mining town of Yuzovka became Stalino, and in April 1925, Tsaritsyn was renamed Stalingrad on the order of Mikhail Kalinin and Avel Enukidze.
In 1926, Stalin published "On Questions of Leninism". It was in this book that he introduced the concept of "Socialism in One Country", which he claimed was an orthodox Leninist perspective. It nevertheless clashed with established Bolshevik views that socialism could not be established in one country but could only be achieved globally through the process of world revolution. Some scholars have argued that Stalin was, in fact, advancing the world revolution because "as first servant of the state, [Stalin] was also first servant of world revolution."
In 1927, there was some argument in the party over the USSR's policy regarding the situation in China. Stalin had called for the Communist Party of China, led by Mao Zedong, to ally itself with Chiang Kai-shek's Kuomintang (KMT) nationalists, viewing a Communist-Kuomintang alliance as the best bulwark against Japanese imperial expansionism in eastern Asia. Instead, the KMT repressed the Communists and a civil war broke out between the two sides.

By the latter half of the 1920s, the Soviet Union still lagged behind the industrial development of Western countries, and Stalin's government feared military attack from Japan, France, or the United Kingdom. Many Communists, including in Komsomol, OGPU, and the Red Army, were eager to be rid of the NEP and its market-oriented approach, desiring a push towards socialism. These Communists had concerns about those who had financially profited from the policy: affluent peasants known as 'kulaks' and the small business owners or 'Nepmen'. There had also been a shortfall of grain supplies; 1927 produced only 70% of grain produced in 1926. At this point, Stalin turned against the NEP, putting him on a course to the "left" even of Trotsky or Zinoviev.

In early 1928 Stalin travelled to Novosibirsk, there claiming that kulaks were hoarding their grain. He ordered that the kulaks be arrested and their grain confiscated, with Stalin bringing much of the area's grain back to Moscow with him in February. At his command, grain procurement squads surfaced across Western Siberia and the Urals, with violence breaking out between these squads and the peasantry. Stalin announced that both kulaks and the "middle peasants" must be coerced into releasing their harvest. Bukharin and several other members of the Central Committee were angry that they had not been consulted about this measure, which they deemed rash. In January 1930, the Politburo approved a measure to liquidate the existence of the kulaks as a class; accused kulaks were rounded up and exiled either elsewhere in their own regions, to other parts of the country, or to concentration camps. Large numbers died during the journey. By July 1930, over 320,000 households had been affected by the de-kulakisation policy. According to Stalin biographer Dmitri Volkogonov, de-kulakisation was "the first mass terror applied by Stalin in his own country".

In 1929, the Politburo announced the mass collectivisation of agriculture, establishing both "kolkhozy" collective farms and "sovkhoz" state farms. Stalin stipulated that kulaks would be barred from joining these collectives. Although officially voluntary, many peasants joined the collectives out of fear they would face the fate of the kulaks; others joined amid intimidation and violence from party loyalists.
By 1932, about 62% of households involved in agriculture were part of collectives, and by 1936 this had risen to 90%. Many of the peasants who had been collectivised resented the loss of their private farmland, and productivity slumped. Famine broke out in many areas, with the Politburo frequently ordering the distribution of emergency food relief to these regions. Armed peasant uprisings against dekulakisation and collectivisation broke out in Ukraine, northern Caucasus, southern Russia, and central Asia, reaching their apex in March 1930; these were repressed by the Red Army. Stalin responded to the uprisings with an article insisting that collectivisation was voluntary and blaming any violence and other excesses on local officials. Although he and Stalin had been close for many years, Bukharin expressed concerns about these policies; he regarded them as a return to Lenin's old "war communism" policy and believed that it would fail. By mid-1928 he was unable to rally sufficient support in the party to oppose the reforms. In November 1929 Stalin removed him from the Politburo.

Officially, the Soviet Union had replaced the irrationality and wastefulness of a market economy with a planned economy organised along a long-term, precise, and scientific framework; in reality, Soviet economics were based on "ad hoc" commandments issued from the centre, often to make short-term targets. In 1928, the first five-year plan was launched, its main focus on boosting heavy industry; it was finished a year ahead of schedule, in 1932.
The USSR underwent a massive economic transformation. New mines were opened, new cities like Magnitogorsk constructed, and work on the White Sea-Baltic Canal begun. Millions of peasants moved to the cities and became proletariat, although urban house building could not keep up with the demand. Large debts were accrued while purchasing foreign-made machinery. Many of the major construction projects, including the White Sea-Baltic Canal and the Moscow Metro, were constructed largely through forced labour. The last elements of workers' control over industry were removed, with factory managers increasing their authority and receiving privileges and perks; Stalin defended wage disparity by pointing to Marx's argument that it was necessary during the lower stages of socialism. To promote the intensification of labour, a series of medals and awards as well as the Stakhanovite movement were introduced. 
Stalin's message was that socialism was being established in the USSR while capitalism was crumbling amid the Wall Street crash. His speeches and articles reflected his utopian vision of the Soviet Union rising to unparalleled heights of human development, creating a "new Soviet person". 

In 1928, Stalin declared that class war between the proletariat and their enemies would intensify as socialism developed. He warned of a "danger from the right", including in the Communist Party itself. The first major show trial in the USSR was the Shakhty Trial of 1928, in which several middle-class "industrial specialists" were convicted of sabotage. From 1929 to 1930, further show trials were held to intimidate opposition: these included the Industrial Party Trial, Menshevik Trial, and Metro-Vickers Trial. Aware that the ethnic Russian majority may have concerns about being ruled by a Georgian, he promoted ethnic Russians throughout the state hierarchy and made the Russian language compulsory throughout schools and offices, albeit to be used in tandem with local languages in areas with non-Russian majorities. Nationalist sentiment among ethnic minorities was suppressed. Conservative social policies were promoted to enhance social discipline and boost population growth; this included a focus on strong family units and motherhood, the re-criminalisation of homosexuality, restrictions placed on abortion and divorce, and the abolition of the "Zhenotdel" women's department.

Stalin desired a "cultural revolution", entailing both the creation of a culture for the "masses" and the wider dissemination of previously elite culture. He oversaw the proliferation of schools, newspapers, and libraries, as well as the advancement of literacy and numeracy. "Socialist realism" was promoted throughout the arts, while Stalin personally wooed prominent writers, namely Maxim Gorky, Mikhail Sholokhov, and Aleksey Nikolayevich Tolstoy. He also expressed patronage for scientists whose research fitted within his preconceived interpretation of Marxism; he for instance endorsed the research of agrobiologist Trofim Lysenko despite the fact that it was rejected by the majority of Lysenko's scientific peers as pseudo-scientific. The government's anti-religious campaign was re-intensified, with increased funding given to the League of Militant Atheists. Christian, Muslim, Jewish, and Buddhist clergy faced persecution. Many religious buildings were demolished, most notably Moscow's Cathedral of Christ the Saviour, destroyed in 1931 to make way for the (never completed) Palace of the Soviets. Religion retained an influence over much of the population; in the 1937 census, 57% of respondents identified as religious.

Throughout the 1920s and beyond, Stalin placed a high priority on foreign policy. He personally met with a range of Western visitors, including George Bernard Shaw and H. G. Wells, both of whom were impressed with him. Through the Communist International, Stalin's government exerted a strong influence over Marxist parties elsewhere in the world; initially, Stalin left the running of the organisation largely to Bukharin. At its 6th Congress in July 1928, Stalin informed delegates that the main threat to socialism came not from the right but from non-Marxist socialists and social democrats, whom he called "social fascists"; Stalin recognised that in many countries, the social democrats were the Marxist-Leninists' main rivals for working-class support. This preoccupation with opposing rival leftists concerned Bukharin, who regarded the growth of fascism and the far right across Europe as a far greater threat. After Bukharin's departure, Stalin placed the Communist International under the administration of Dmitry Manuilsky and Osip Piatnitsky.

Stalin faced problems in his family life. In 1929, his son Yakov unsuccessfully attempted suicide; his failure earned Stalin's contempt. His relationship with Nadya was also strained amid their arguments and her mental health problems. In November 1932, after a group dinner in the Kremlin in which Stalin flirted with other women, Nadya shot herself.
Publicly, it was claimed that Nadya died of appendicitis; Stalin also concealed the real cause of death from his children. Stalin's friends noted that he underwent a significant change following her suicide, becoming emotionally harder.

Within the Soviet Union, there was widespread civic disgruntlement against Stalin's government. Social unrest, previously restricted largely to the countryside, was increasingly evident in urban areas, prompting Stalin to ease on some of his economic policies in 1932. In May 1932, he introduced a system of kolkhoz markets where peasants could trade their surplus produce. At the same time, penal sanctions became more severe; at Stalin's instigation, in August 1932 a measure was introduced meaning that the theft of even a handful of grain could be a capital offense. The second five-year plan had its production quotas reduced from that of the first, with the main emphasis now being on improving living conditions. It therefore emphasised the expansion of housing space and the production of consumer goods. Like its predecessor, this Plan was repeatedly amended to meet changing situations; there was for instance an increasing emphasis placed on armament production after Adolf Hitler became German Chancellor in 1933.

Such policies nevertheless failed to stop the famine which peaked in the winter of 1932–33. Between five and seven million people died; many resorted to cannibalising the dead to survive. Worst affected were Ukraine and the North Caucuses, although the famine also impacted Kazakhstan and several Russian provinces. The 1932 harvest had been a poor one, and had followed several years in which lower productivity had resulted in a gradual decline in output. Stalin blamed the famine on hostile elements and wreckers within the peasantry; his government provided small amounts of food to famine-struck rural areas, although this was wholly insufficient to deal with the levels of starvation. Grain exports, which were a major means of Soviet payment for machinery, declined heavily. Stalin would not acknowledge that his policies had contributed to the famine, the existence of which was denied to foreign observers. 

In 1935–36, Stalin oversaw a new constitution; its dramatic liberal features were designed as propaganda weapons, for all power rested in the hands of Stalin and his Politburo. He declared that "socialism, which is the first phase of communism, has basically been achieved in this country". In 1938, "The History of the Communist Party of the Soviet Union (Bolsheviks)", colloquially known as the "Short Course", was released; Conquest later referred to it as the "central text of Stalinism". A number of authorised Stalin biographies were also published, although Stalin generally wanted to be portrayed as the embodiment of the Communist Party rather than have his life story explored. During the later 1930s, Stalin placed "a few limits on the worship of his own greatness". By 1938, Stalin's inner circle had gained a degree of stability, containing the personalities who would remain there until Stalin's death.

Seeking improved international relations, in 1934 the Soviet Union secured membership of the League of Nations, of which it had previously been excluded. Stalin initiated confidential communications with Hitler in October 1933, shortly after the latter came to power in Germany. Stalin admired Hitler, particularly his manoeuvres to remove rivals within the Nazi Party in the Night of the Long Knives. Stalin nevertheless recognised the threat posed by fascism and sought to establish better links with the liberal democracies of Western Europe; in May 1935, the Soviets signed a treaty of mutual assistance with France and Czechoslovakia. At the Communist International's 7th Congress, held in July–August 1935, the Soviet government encouraged Marxist-Leninists to unite with other leftists as part of a popular front against fascism. In turn, the anti-communist governments of Germany, Fascist Italy and Japan signed the Anti-Comintern Pact of 1936. When the Spanish Civil War broke out the same year, the Soviets sent 648 aircraft and 407 tanks to the left-wing Republican faction; these were accompanied by 3000 Soviet troops and 42,000 members of the International Brigades set up by the Communist International. Stalin took a strong personal involvement in the Spanish situation. Germany and Italy backed the Nationalist faction, which was ultimately victorious in March 1939. Stalin also gave aid to the Chinese after the outbreak of the Second Sino-Japanese War in July 1937, as the KMT and the Communists formed Stalin's desired United Front.

Regarding state repressions, Stalin often provided conflicting signals. In May 1933, he ordered the release of many criminals convicted of minor offenses from the overcrowded prisons and ordered the security services not to enact further mass arrests and deportations. In September 1934, he ordered the Politburo to establish a commission to investigate any false imprisonments; that same month he called for the execution of workers at the Stalin Metallurgical Factory accused of spying for Japan. This mixed approach began to change in December 1934, when the prominent party member Sergey Kirov was murdered. After Kirov's murder, Stalin became increasingly attentive of the possibility of murder and subsequently improved his own personal security, including being heavily guarded at all times and rarely going out in public.

Kirov's killing was followed by an intensification of state repression; Stalin issued a decree establishing NKVD troikas which could mete out rulings without involving the courts. Just as the de-kulakisation policy had sought to rid rural areas of anti-government forces, so Stalin sought to do the same in the cities and towns. In 1935, the NKVD was ordered to expel suspected counter-revolutionaries, particularly those who had been aristocrats, landlords, or businesspeople before the October Revolution. In the early months of 1935, over 11,000 people were expelled from Leningrad, to live in isolated rural areas. In 1936, Nikolai Yezhov became head of the NKVD and oversaw this intensification. Stalin instigated this intensification of repression, which was rooted in his own psychological compulsions and the logic of the system he had created, one which prioritised security above other considerations.

Stalin orchestrated the arrest of many former opponents in the Communist Party as well as sitting members of the Central Committee: denounced as Western-backed mercenaries, many were imprisoned or exiled internally. The first Moscow Trial took place in August 1936; Kamenev and Zinoviev were among those accused of plotting assassinations, found guilty in a show trial, and executed. The second Moscow Show Trial took place in January 1937, and the third in March 1938, in which Bukharin and Rykov were accused of involvement in the alleged Trotskyite-Zinovievite terrorist plot and sentenced to death. By late 1937, all remnants of collective leadership were gone from the Politburo, which was controlled entirely by Stalin.
There were mass expulsions from the party, with Stalin commanding foreign communist parties to also purge anti-Stalinist elements.

During the 1930s and 1940s, NKVD groups assassinated defectors and opponents abroad; in August 1940, Trotsky was assassinated in Mexico, eliminating the last of Stalin's opponents among the former Party leadership. In May, this was followed by the arrest of most members of the military Supreme Command and mass arrests throughout the military, often on fabricated charges. These purges replaced most of the party's old guard with younger officials who did not remember a time before Stalin's leadership and who were regarded as more personally loyal to him. Party functionaries readily carried out their commands and sought to ingratiate themselves with Stalin to avoid becoming the victim of the purge. Such functionaries often carried out a greater number of arrests and executions than their quotas set by Stalin's central government.

Repressions further intensified in December 1936 and remained at a high level until November 1938, a period known as the Great Purge. By the latter part of 1937, the purges had moved beyond the party and were affecting the wider population. In July 1937, the Politburo ordered a purge of "anti-Soviet elements" in society, affecting Bolsheviks who had opposed Stalin, former Mensheviks and Socialist Revolutionaries, priests, former soldiers in the White Army, and common criminals. That month, Stalin and Yezhov signed Order No. 00447, listing 268,950 people for arrest, of whom 75,950 were executed. He also initiated "national operations", the ethnic cleansing of non-Soviet ethnic groups—among them Poles, Germans, Latvians, Finns, Greeks, Koreans, and Chinese—through internal or external exile. During these years, approximately 1.6 million people were arrested, 700,000 were shot, and an unknown number died under NKVD torture.

Stalin initiated all of the key decisions during the Terror, personally directing many of its operations and taking an interest in the details of their implementation. His motives in doing so have been much debated by historians. His personal writings from the period were—according to Khlevniuk—"unusually convoluted and incoherent", filled with claims about conspiracies and enemies encircling him. He was particularly concerned at the success that right-wing forces had in overthrowing the leftist Spanish government, worried that domestic anti-Stalinist elements would become a fifth column in the event of a future war with Japan and Germany. The Great Terror ended when Yezhov was removed as the head of the NKVD, to be replaced by Lavrentiy Beria, a man totally devoted to Stalin. Yezhov was arrested in April 1939 and executed in 1940. The Terror had damaged the Soviet Union's reputation abroad, particularly among previously sympathetic leftists, and as the Terror wound down, so Stalin sought to deflect responsibility away from himself. He later claimed that the "excesses" and "violations of law" during the Terror were Yezhov's fault.

As a Marxist–Leninist, Stalin expected an inevitable Second World War between competing capitalist powers; as Nazi Germany annexed Austria and then part of Czechoslovakia in 1938, Stalin recognised that this war was looming. He sought to maintain Soviet neutrality in the conflict, hoping that a German war against France and the UK would leave the Soviets a dominant force in Europe. Militarily, the Soviets also faced a threat from the east, with Soviet troops clashing with the expansionist Japanese in the latter part of the 1930s. Stalin initiated a military build-up, with the Red Army more than doubling between January 1939 and June 1941, although in its haste to expand many of its officers were poorly trained. Between 1940 and 1941 he also launched a purge of the military, leaving it with a severe shortage of trained officers at every level when the war ultimately broke out.

As Britain and France seemed unwilling to commit to an alliance with the Soviet Union, Stalin saw a better deal with the Germans. In May 1939, Germany began negotiations with the Soviets, proposing that Eastern Europe be divided between the two powers. Stalin saw this as an opportunity both for territorial expansion and temporary peace with Germany. In August 1939, the Soviet Union signed a non-aggression pact with Germany, negotiated by Soviet foreign minister Vyacheslav Molotov and German foreign minister Joachim von Ribbentrop. A week later, Germany invaded Poland, sparking the UK and France to declare war on it. On 17 September, the Red Army entered eastern Poland, officially to restore order amid the collapse of the Polish state; this explanation was also designed so as not to anger the UK and France.

Stalin suggested a territorial exchange with Germany, giving them the ethnic Polish-dominated areas of Lublin Province and part of Warsaw Province, and in return receiving Lithuania; Stalin had desired the reintegration of the three Baltic states into the Soviet Union. This was agreed in 28 September. A German–Soviet Frontier Treaty was signed shortly after, in Stalin's presence. The two nations continued trading, undermining the British blockade of Germany.

The Soviets further demanded parts of eastern Finland, but the Finnish government refused. The Soviets invaded Finland in November 1939, yet despite numerical inferiority, the Finns kept the Red Army at bay. International opinion backed Finland, with the Soviets being expelled from the League of Nations. Embarrassed by their inability to defeat the Finns, the Soviets signed an interim peace treaty, in which they received territorial concessions from Finland. In June 1940, the Red Army entered the Baltic states, which were forcibly merged into the Soviet Union in August. Bessarabia and northern Bukovina, parts of Romania, were also annexed into the Soviet Union. The Soviet authorities sought to forestall any dissent in these new East European territories with mass repressions. One of the most noted instances was the Katyn massacre of April and May 1940, in which around 22,000 members of the Polish armed forces, police, and intelligentsia were executed.

The speed of the German victory over and occupation of France in mid-1940 took Stalin by surprise. He increasingly focused on appeasement with the Germans to delay any conflict with them. After the Tripartite Pact was signed by Axis Powers Germany, Japan and Italy, in October 1940, Stalin approached Germany with the suggestion that it too join the Axis alliance. To demonstrate peaceful intentions toward Germany, in April 1941 the Soviets signed a neutrality pact with Japan.
On 6 May, Stalin replaced Molotov as Premier of the Soviet Union. Although "de facto" head of government for a decade and a half, Stalin concluded that relations with Germany had deteriorated to such an extent that he needed to deal with the problem as "de jure" head of government as well.

In June 1941, Germany invaded the Soviet Union, initiating the war on the Eastern Front. Despite having prior warning, Stalin was taken by surprise. He formed a State Committee of Defence, which he headed as Supreme Commander, as well as a military Supreme Command (Stavka), with Georgy Zhukov appointed its Chief of Staff. The German tactic of "blitzkrieg" was initially highly effective; the Soviet air force in the western borderlands was destroyed within two days. The German Wehrmacht pushed deep into Soviet territory; soon, Ukraine, Belorussia, and the Baltic states were under German occupation. Soviet refugees flooded into Moscow and Leningrad to escape the Wehrmacht, although there were other Soviet citizens—namely those who were neither ethnically Russian nor Jewish—who welcomed the German Army as liberators; they soon found that the Nazis regarded them as "Untermensch", fit only for economic exploitation. By July, Germany's Luftwaffe was bombing Moscow, and by October the Wehrmacht was amassing for a full assault on the capital. Plans were made for the Soviet government to evacuate to Kuibyshev, although Stalin decided to remain in Moscow, believing that his flight would damage troop morale. The German advance on Moscow was halted by the arrival of winter.

Against the advice of Zhukov and other generals, Stalin emphasised attack over defence. In June 1941, he ordered a scorched earth policy of destroying infrastructure and food supplies before the Germans could seize them, also commanding the NKVD to kill around 100,000 political prisoners in areas the Wehrmacht approached. He purged the military command; several high-ranking figures were demoted or reassigned and others were arrested and executed. With Order No. 270, Stalin commanded soldiers risking capture to commit suicide or fight to the death, stating that any who were captured were traitors; among those taken as a prisoner of war by the German Army was Stalin's son Yakov, who died in their custody. Stalin issued Order No. 227 in July 1942, which directed that those retreating would be placed in "penal battalions" used as cannon fodder on the front lines. Amid the fighting, both the German and Soviet armies disregarded the law of war set forth in the Geneva Conventions; the Soviets heavily publicised Nazi massacres of communists, Jews, and Romani. Stalin exploited Nazi anti-Semitism, and in April 1942 he sponsored the Jewish Anti-Fascist Committee (JAC) to garner Jewish and foreign support for the Soviet war effort.

The Soviets allied with the United Kingdom and United States; although the US joined the war against Germany in 1941, little direct American assistance reached the Soviets until late 1942. Responding to the invasion, the Soviets intensified their industrial enterprises in central Russia, focusing almost entirely on production for the military. They achieved high levels of industrial productivity, outstripping that of Germany. During the war, Stalin was more tolerant of the Russian Orthodox Church, allowing it to resume some of its activities and meeting with Patriarch Sergius in September 1943. He also permitted a wider range of cultural expression, notably permitting formerly suppressed writers and artists like Anna Akhmatova and Dmitri Shostakovich to disperse their work more widely. The Internationale was dropped as the country's national anthem, to be replaced with a more patriotic song. The government increasingly promoted Pan-Slavist sentiment, while encouraging increased criticism of cosmopolitanism, particularly the idea of "rootless cosmopolitanism", an approach with particular repercussions for Soviet Jews. Comintern was dissolved in 1943, and Stalin encouraged foreign Marxist–Leninist parties to emphasise nationalism over internationalism to broaden their domestic appeal.

In April 1942 Stalin overrode Stavka by ordering the Soviets' first serious counter-attack, an attempt to seize German-held Kharkov in eastern Ukraine. This attack proved unsuccessful. That year, Hitler shifted his primary goal from an immediate victory in the East, to the more long-term goal of securing the southern Soviet Union to conquer oil fields vital to a long-term German war effort. While Red Army generals saw evidence that Hitler would shift efforts south, Stalin considered this to be a flanking campaign in efforts to take Moscow. In June 1942, the German Army attacked Stalingrad; Stalin ordered the Red Army to hold the city at all costs. This resulted in the protracted Battle of Stalingrad. In December 1942 he placed Konstantin Rokossovski in charge of holding the city. In February 1943, the German troops attacking Stalingrad surrendered. The Soviet victory there marked a major turning point in the war; in commemoration, Stalin declared himself Marshal of the Soviet Union.

By November 1942, the Soviets had begun to repulse the important German strategic southern campaign and, although there were 2.5 million Soviet casualties in that effort, it permitted the Soviets to take the offensive for most of the rest of the war on the Eastern Front. Germany attempted an encirclement attack at Kursk, which was successfully repulsed by the Soviets. By the end of 1943, the Soviets occupied half of the territory taken by the Germans from 1941 to 1942. Soviet military industrial output also had increased substantially from late 1941 to early 1943 after Stalin had moved factories well to the East of the front, safe from German invasion and air attack.

In Allied countries, Stalin was increasingly depicted in a positive light over the course of the war. In 1941, the London Philharmonic Orchestra performed a concert to celebrate his birthday, and in 1942, "Time" magazine named him "Man of the Year". When Stalin learned that people in Western countries affectionately called him "Uncle Joe" he was initially offended, regarding it as undignified. There remained mutual suspicions between Stalin, British Prime Minister Winston Churchill, and U.S. President Franklin D. Roosevelt, who were together known as the "Big Three". Churchill flew to Moscow to visit Stalin in August 1942 and again in October 1944. Stalin scarcely left Moscow throughout the war, with Roosevelt and Churchill frustrated with his reluctance to travel to meet them.

In November 1943, Stalin met with Churchill and Roosevelt in Tehran, a location of Stalin's choosing. There, Stalin and Roosevelt got on well, with both desiring the post-war dismantling of the British Empire. At Tehran, the trio agreed that to prevent Germany rising to military prowess yet again, the German state should be broken up. Roosevelt and Churchill also agreed to Stalin's demand that the German city of Konigsberg be declared Soviet territory. Stalin was impatient for the UK and US to open up a Western Front to take the pressure off of the East; they eventually did so in mid-1944. Stalin insisted that, after the war, the Soviet Union should incorporate the portions of Poland it occupied pursuant to the Molotov–Ribbentrop Pact with Germany, which Churchill opposed. Discussing the fate of the Balkans, later in 1944 Churchill agreed to Stalin's suggestion that after the war, Bulgaria, Romania, Hungary, and Yugoslavia would come under the Soviet sphere of influence while Greece would come under that of the West.

In 1944, the Soviet Union made significant advances across Eastern Europe toward Germany, including Operation Bagration, a massive offensive in the Byelorussian SSR against the German Army Group Centre. In 1944 the German armies were pushed out of the Baltic states, which were then re-annexed into the Soviet Union. As the Red Army reconquered the Caucasus and Crimea, various ethnic groups living in the region—the Kalmyks, Chechens, Ingushi, Karachai, Balkars, and Crimean Tatars—were accused of having collaborated with the Germans. Using the idea of collective responsibility as a basis, Stalin's government abolished their autonomous republics and between late 1943 and 1944 deported the majority of their populations to Central Asia and Siberia. Over one million people were deported as a result of the policy.

In February 1945, the three leaders met at the Yalta Conference. Roosevelt and Churchill conceded to Stalin's demand that Germany pay the Soviet Union 20 billion dollars in reparations, and that his country be permitted to annex Sakhalin and the Kurile Islands in exchange for entering the war against Japan. An agreement was also made that a post-war Polish government should be a coalition consisting of both communist and conservative elements. Privately, Stalin sought to ensure that Poland would come fully under Soviet influence. The Red Army withheld assistance to Polish resistance fighters battling the Germans in the Warsaw Uprising, with Stalin believing that any victorious Polish militants could interfere with his aspirations to dominate Poland through a future Marxist government. Although concealing his desires from the other Allied leaders, Stalin placed great emphasis on capturing Berlin first, believing that this would enable him to bring more of Europe under long-term Soviet control. Churchill was concerned that this was the case, and unsuccessfully tried to convince the U.S. that the Western Allies should pursue the same goal.

In April 1945, the Red Army seized Berlin, Hitler committed suicide, and Germany surrendered unconditionally. Stalin was annoyed that Hitler was dead, having wanted to capture him alive. He ordered his intelligence agencies to secretly bring Hitler's remains to Moscow, seeking to prevent any physical remains becoming a relic for Nazi sympathisers. As the Red Army had conquered German territory, they discovered the extermination camps that the Nazi administration had run. Many Soviet soldiers engaged in looting, pillaging, and rape, both in Germany and parts of Eastern Europe. Stalin refused to punish the offenders. After receiving a complaint about this from Yugoslav communist Milovan Djilas, Stalin asked how after experiencing the traumas of war a soldier could "react normally? And what is so awful in his having fun with a woman, after such horrors?"

With Germany defeated, Stalin switched his focus to the ongoing war with Japan, transferring half a million troops to the far east. Stalin was aware that the United States had developed nuclear weaponry, with which it intended to subdue the Japanese, and was steadfast in entering the war before he could be denied the territories promised to him. On 8 August, in between the atomic bombings of Hiroshima and Nagasaki, the Soviet army invaded Japanese occupied Manchuria and defeated the Kwantung Army. These events led to the Japanese surrender and the complete end of World War II. Soviet forces continued to expand until they occupied all their territorial concessions, but the U.S. rebuffed Stalin's desire for the Red Army to take a role in the Allied occupation of Japan.

Stalin attended the Potsdam Conference in July–August 1945, alongside his new British and U.S. counterparts, Prime Minister Clement Attlee and President Harry Truman. At the beginning of the conference, Stalin repeated previous promises to Churchill that he would refrain from a "Sovietization" of Eastern Europe. Stalin pushed for reparations from Germany without regard to the base minimum supply for German citizens' survival, which worried Truman and Churchill who thought that Germany would become a financial burden for Western powers.
He also pushed for "war booty", which would permit the Soviet Union to directly seize property from conquered nations without quantitative or qualitative limitation, and a clause was added permitting this to occur with some limitations. Germany was divided into four zones: Soviet, U.S., British, and French, with Berlin itself—located within the Soviet area—also subdivided thusly.

After the war, Stalin was—according to Service—at the "apex of his career". Within the Soviet Union he was widely regarded as the embodiment of victory and patriotism. His armies controlled Central and Eastern Europe up to the River Elbe.
In June 1945, Stalin adopted the title of Generalissimus, and stood atop Lenin's Mausoleum to watch a celebratory parade led by Zhukov through Red Square. At a banquet held for army commanders, he described the Russian people as "the outstanding nation" and "leading force" within the Soviet Union, the first time that he had unequivocally endorsed the Russians over other Soviet nationalities. In 1946, the state published Stalin's "Collected Works". In 1947, it brought out a second edition of his official biography, which eulogised him to a greater extent than its predecessor. He was quoted in "Pravda" on a daily basis and pictures of him remained pervasive on the walls of workplaces and homes.

Despite his strengthened international position, Stalin was cautious about internal dissent and desire for change among the population. He was also concerned about his returning armies, who had been exposed to a wide range of consumer goods in Germany, much of which they had looted and brought back with them. In this he recalled the 1825 Decembrist Revolt by Russian soldiers returning from having defeated France in the Napoleonic Wars. He ensured that returning Soviet prisoners of war went through "filtration" camps as they arrived in the Soviet Union, in which 2,775,700 were interrogated to determine if they were traitors. About half were then imprisoned in labour camps. In the Baltic states, where there was much opposition to Soviet rule, de-kulakisation and de-clericalisation programs were initiated, resulting in 142,000 deportations between 1945 and 1949. The Gulag system of labour camps was expanded further; by January 1953, 3% of the Soviet population were imprisoned or in internal exile, with 2.8 million in "special settlements" in isolated areas and another 2.5 million in camps, penal colonies, and prisons.

The NKVD were ordered to catalogue the scale of destruction during the war. It was established that 1,710 Soviet towns and 70,000 villages had been destroyed. They recorded that between 26 and 27 million Soviet citizens had been killed, with millions more being wounded, malnourished, or orphaned. In the war's aftermath, some of Stalin's associates suggested modifications to government policy. Post-war Soviet society was more tolerant than its pre-war phase in various respects. Stalin allowed the Russian Orthodox Church to retain the churches it had opened during the war. Academia and the arts were also allowed greater freedom than they had prior to 1941. Recognising the need for drastic steps to be taken to combat inflation and promote economic regeneration, in December 1947 Stalin's government devalued the ruble and abolished the ration-book system. Capital punishment was abolished in 1947 but reinstalled in 1950.

Stalin's health was deteriorating, and heart problems forced a two-month vacation in the latter part of 1945.
He grew increasingly concerned that senior political and military figures might try to oust him; he prevented any of them from becoming powerful enough to rival him and had their apartments bugged with listening devices. He demoted Molotov, and increasingly favoured Beria and Malenkov for key positions. In 1949, he brought Nikita Khrushchev from Ukraine to Moscow, appointing him a Central Committee secretary and the head of the city's party branch. In the Leningrad Affair, the city's leadership was purged amid accusations of treachery; executions of many of the accused took place in 1950.

In the post-war period there were often food shortages in Soviet cities, and the USSR experienced a major famine from 1946 to 1947. Sparked by a drought and ensuing bad harvest in 1946, it was exacerbated by government policy towards food procurement, including the state's decision to build up stocks and export food internationally rather than distributing it to famine hit areas. Current estimates indicate that between 1 million and 1.5 million people died from malnutrition or disease as a result. While agricultural production stagnated, Stalin focused on a series of major infrastructure projects, including the construction of hydroelectric plants, canals, and railway lines running to the polar north. Much of this was constructed by prison labour.

In the aftermath of the Second World War, the British Empire declined, leaving the U.S. and USSR as the dominant world powers. Tensions among these former Allies grew, resulting in the Cold War. Although Stalin publicly described the British and U.S. governments as aggressive, he thought it unlikely that a war with them would be imminent, believing that several decades of peace was likely. He nevertheless secretly intensified Soviet research into nuclear weaponry, intent on creating an atom bomb. Still, Stalin foresaw the undesirability of a nuclear conflict, saying in 1949 that "atomic weapons can hardly be used without spelling the end of the world." He personally took a keen interest in the development of the weapon. In August 1949, the bomb was successfully tested in the deserts outside Semipalatinsk in Kazakhstan. Stalin also initiated a new military build-up; the Soviet army was expanded from 2.9 million soldiers, as it stood in 1949, to 5.8 million by 1953.

The US began pushing its interests on every continent, acquiring air force bases in Africa and Asia and ensuring pro-U.S. regimes took power across Latin America. It launched the Marshall Plan in June 1947, with which it sought to undermine Soviet hegemony in eastern Europe. The US also offered financial assistance as part of the Marshall Plan on the condition that they opened their markets to trade, aware that the Soviets would never agree.
The Allies demanded that Stalin withdraw the Red Army from northern Iran, which he did in April 1947.
Stalin also tried to maximise Soviet influence on the world stage, unsuccessfully pushing for Libya—recently liberated from Italian occupation—to become a Soviet protectorate. He sent Molotov as his representative to San Francisco to take part in negotiations to form the United Nations, insisting that the Soviets have a place on the Security Council. In April 1949, the Western powers established the North Atlantic Treaty Organisation (NATO), an international military alliance of capitalist countries. Within Western countries, Stalin was increasingly portrayed as the "most evil dictator alive" and compared to Hitler.

In 1948, Stalin edited and rewrote sections of "Falsifiers of History", published as a series of "Pravda" articles in February 1948 and then in book form. Written in response to public revelations of the 1939 Soviet alliance with Germany, it focused on blaming Western powers for the war. He erroneously claimed that the initial German advance in the early part of the war was not a result of Soviet military weakness, but rather a deliberate Soviet strategic retreat. In 1949, celebrations took place to mark Stalin's seventieth birthday (albeit not the correct year) at which Stalin attended an event in the Bolshoi Theatre alongside Marxist-Leninist leaders from across Europe and Asia.

After the war, Stalin sought to retain Soviet dominance across Eastern Europe while expanding its influence in Asia.
Cautious regarding the responses from the Western Allies, Stalin avoided immediately installing Communist Party governments across Eastern Europe, instead initially ensuring that Marxist-Leninists were placed in coalition ministries. In contrast to his approach to the Baltic states, he rejected the proposal of merging these states into the Soviet Union, rather recognising them as independent nation-states.
He was faced with the problem that there were few Marxists left in Eastern Europe, with most having been killed by the Nazis. He demanded that war reparations be paid by Germany and its Axis allies Hungary, Romania, and the Slovak Republic.
Aware that these countries had been pushed toward socialism through invasion rather than by proletarian revolution, Stalin referred to them not as "dictatorships of the proletariat" but as "people's democracies", suggesting that in these countries there was a pro-socialist alliance combining the proletariat, peasantry, and lower middle-class.

Churchill observed that an "Iron Curtain" had been drawn across Europe, separating the east from the west. In September 1947, a meeting of East European communist leaders was held in Szklarska Poręba, Poland, from which was formed Cominform to co-ordinate the Communist Parties across Eastern Europe and also in France and Italy. Stalin did not personally attend the meeting, sending Zhdanov in his place. Various East European communists also visited Stalin in Moscow. There, he offered advice on their ideas; for instance he cautioned against the Yugoslav idea for a Balkan federation incorporating Bulgaria and Albania. Stalin had a particularly strained relationship with Yugoslav leader Josip Broz Tito due to the latter's continued calls for Balkan federation and for Soviet aid for the communist forces in the ongoing Greek Civil War. In March 1948, Stalin launched an anti-Tito campaign, accusing the Yugoslav communists of adventurism and deviating from Marxist–Leninist doctrine. At the second Cominform conference, held in Bucharest in June 1948, East European communist leaders all denounced Tito's government, accusing them of being fascists and agents of Western capitalism. Stalin ordered several assassination attempts on Tito's life and contemplated invading Yugoslavia.

Stalin suggested that a unified, but demilitarised, German state be established, hoping that it would either come under Soviet influence or remain neutral. When the US and UK remained opposed to this, Stalin sought to force their hand by blockading Berlin in June 1948. He gambled that the others would not risk war, but they airlifted supplies into West Berlin until May 1949, when Stalin relented and ended the blockade. In September 1949 the Western powers transformed Western Germany into an independent Federal Republic of Germany; in response the Soviets formed East Germany into the German Democratic Republic in October. In accordance with their earlier agreements, the Western powers expected Poland to become an independent state with free democratic elections. In Poland, the Soviets merged various socialist parties into the Polish United Workers' Party, and vote rigging was used to ensure that it secured office. The 1947 Hungarian elections were also rigged, with the Hungarian Working People's Party taking control. In Czechoslovakia, where the communists did have a level of popular support, they were elected the largest party in 1946.
Across Eastern Europe, the Soviet model was enforced, with a termination of political pluralism, agricultural collectivisation, and investment in heavy industry. It was aimed for economic autarky within the Eastern Bloc. Monarchies were removed from power in Romania and Bulgaria.

In October 1949, Mao took power in China. With this accomplished, Marxist governments now controlled a third of the world's land mass. Privately, Stalin revealed that he had underestimated the Chinese Communists and their ability to win the civil war, instead encouraging them to make another peace with the KMT. In December 1949, Mao visited Stalin. Initially Stalin refused to repeal the Sino-Soviet Treaty of 1945, which significantly benefited the Soviet Union over China, although in January 1950 he relented and agreed to sign a new treaty between the two countries. Stalin was concerned that Mao might follow Tito's example by pursuing a course independent of Soviet influence, and made it known that if displeased he would withdraw assistance from China; the Chinese desperately needed said assistance after decades of civil war.

After the Second World War, the Soviet Union and the United States divided up the Korean Peninsula, formerly a Japanese colonial possession, along the 38th parallel, setting up a communist government in the north and a pro-Western government in the south. North Korean leader Kim Il-Sung visited Stalin in March 1949 and again in March 1950; he wanted to invade the south and although Stalin was initially reluctant to provide support, he eventually agreed by May 1950. The North Korean Army launched the Korean War by invading the south in June 1950, making swift gains and capturing Seoul. Both Stalin and Mao believed that a swift victory would ensue. The U.S. went to the UN Security Council—which the Soviets were boycotting over its refusal to recognise Mao's government—and secured military support for the South Koreans. U.S. led forces pushed the North Koreans back. Stalin wanted to avoid direct Soviet conflict with the U.S., convincing the Chinese to aid the North.

The Soviet Union was one of the first nations to extend diplomatic recognition to the newly created state of Israel in 1948. When the Israeli ambassador Golda Meir arrived in the USSR, Stalin was angered by the Jewish crowds who gathered to greet her. He was further angered by Israel's growing alliance with the U.S. After Stalin fell out with Israel, he launched an anti-Jewish campaign within the Soviet Union and the Eastern Bloc. In November 1948, he abolished the JAC, and show trials took place for some of its members. The Soviet press engaged in attacks on Zionism, Jewish culture, and "rootless cosmopolitanism", with growing levels of anti-Semitism being expressed across Soviet society. Stalin's increasing tolerance of anti-Semitism may have stemmed from his increasing Russian nationalism or from the recognition that anti-Semitism had proved a useful mobilising tool for Hitler and that he could do the same; he may have increasingly viewed the Jewish people as a "counter-revolutionary" nation whose members were loyal to the U.S. There were rumours, although they have never been substantiated, that Stalin was planning on deporting all Soviet Jews to the Jewish Autonomous Region in Birobidzhan, eastern Siberia.

In his later years, Stalin was in poor health. He took increasingly long holidays; in 1950 and again in 1951 he spent almost five months vacationing at his Abkhazian dacha. Stalin nevertheless mistrusted his doctors; in January 1952 he had one imprisoned after they suggested that he should retire to improve his health. In September 1952, several Kremlin doctors were arrested for allegedly plotting to kill senior politicians in what came to be known as the Doctors' Plot; the majority of the accused were Jewish. He instructed the arrested doctors to be tortured to ensure confession. In November, the Slánský trial took place in Czechoslovakia as 13 senior Communist Party figures, 11 of them Jewish, were accused and convicted of being part of a vast Zionist-American conspiracy to subvert Eastern Bloc governments. That same month, a much publicised trial of accused Jewish industrial wreckers took place in Ukraine. In 1951, he initiated the Mingrelian affair, a purge of the Georgian branch of the Communist Party which resulted in over 11,000 deportations.

From 1946 until his death, Stalin only gave three public speeches, two of which lasted only a few minutes. The amount of written material that he produced also declined. In 1950, Stalin issued the article "Marxism and Problems of Linguistics", which reflected his interest in questions of Russian nationhood.
In 1952, Stalin's last book, "The Economic Problems of Socialism in the USSR", was published. It sought to provide a guide to leading the country for after his death. In October 1952, Stalin gave an hour and a half speech at the Central Committee plenum. There, he emphasised what he regarded as leadership qualities necessary in the future and highlighted the weaknesses of various potential successors, particularly Molotov and Mikoyan. In 1952, he also eliminated the Politburo and replaced it with a larger version which he called the Presidium.

On 1 March 1953, Stalin's staff found him semi-conscious on the bedroom floor of his Volynskoe dacha. He had suffered a cerebral hemorrhage. He was moved onto a couch and remained there for three days. He was hand-fed using a spoon, given various medicines and injections, and leeches were applied to him. Svetlana and Vasily were called to the dacha on 2 March; the latter was drunk and angrily shouted at the doctors, resulting in him being sent home. Stalin died on 5 March 1953. According to Svetlana, it had been "a difficult and terrible death". An autopsy revealed that he had died of a cerebral haemorrhage and that he also suffered from severe damage to his cerebral arteries due to atherosclerosis. It is possible that Stalin was murdered. Beria has been suspected of murder, although no firm evidence has ever appeared. 

Stalin's death was announced on 6 March. The body was embalmed, and then placed on display in Moscow's House of Unions for three days. Crowds were such that a crush killed around 100 people. The funeral involved the body being laid to rest in Lenin's Mausoleum in Red Square on 9 March; hundreds of thousands attended. That month featured a surge in arrests for "anti-Soviet agitation" as those celebrating Stalin's death came to police attention. The Chinese government instituted a period of official mourning for Stalin's death.

Stalin left no anointed successor nor a framework within which a transfer of power could take place. The Central Committee met on the day of his death, with Malenkov, Beria, and Khruschev emerging as the party's key figures. The system of collective leadership was restored, and measures introduced to prevent any one member attaining autocratic domination again. The collective leadership included the following eight senior members of the Presidium of the Central Committee of the Communist Party of the Soviet Union listed according to the order of precedence presented formally on 5 March 1953: Georgy Malenkov, Lavrentiy Beria, Vyacheslav Molotov, Kliment Voroshilov, Nikita Khrushchev, Nikolai Bulganin, Lazar Kaganovich and Anastas Mikoyan. Reforms to the Soviet system were immediately implemented. Economic reform scaled back the mass construction projects, placed a new emphasis on house building, and eased the levels of taxation on the peasantry to stimulate production. The new leaders sought rapprochement with Yugoslavia and a less hostile relationship with the U.S., pursuing a negotiated end to the Korean War in July 1953. The doctors who had been imprisoned were released and the anti-Semitic purges ceased. A mass amnesty for those imprisoned for non-political crimes was issued, halving the country's inmate population, while the state security and Gulag systems were reformed, with torture being banned in April 1953.

Stalin claimed to have embraced Marxism at the age of fifteen, and it served as the guiding philosophy throughout his adult life; according to Kotkin, Stalin held "zealous Marxist convictions", while Montefiore suggested that Marxism held a "quasi-religious" value for Stalin. Although he never became a Georgian nationalist, during his early life elements from Georgian nationalist thought blended with Marxism in his outlook. The historian Alfred J. Rieber noted that he had been raised in "a society where rebellion was deeply rooted in folklore and popular rituals". Stalin believed in the need to adapt Marxism to changing circumstances; in 1917, he declared that "there is dogmatic Marxism and there is creative Marxism. I stand on the ground of the latter". Volkogonov believed that Stalin's Marxism was shaped by his "dogmatic turn of mind", suggesting that this had been instilled in the Soviet leader during his education in religious institutions. According to scholar Robert Service, Stalin's "few innovations in ideology were crude, dubious developments of Marxism". Some of these derived from political expediency rather than any sincere intellectual commitment; Stalin would often turn to ideology "post hoc" to justify his decisions. Stalin referred to himself as a "praktik", meaning that he was more of a practical revolutionary than a theoretician.

As a Marxist, Stalin believed in an inevitable class war between the world's proletariat and bourgeoise. He believed that the working classes would prove successful in this struggle and would establish a dictatorship of the proletariat, regarding the Soviet Union as an example of such a state. He also believed that this proletarian state would need to introduce repressive measures to ensure the full crushing of the propertied classes, and thus the class war would intensify with the advance of socialism. The new state would then be able to ensure that all citizens had access to work, food, shelter, healthcare, and education, with the wastefulness of capitalism eliminated by a new, standardised economic system. According to Sandle, Stalin was "committed to the creation of a society that was industrialized, collectivized, centrally planned and technologically advanced."

Stalin adhered to the Leninist variant of Marxism. He claimed to be a loyal Leninist, although was—according to Service—"not a blindly obedient Leninist". Stalin respected Lenin, but not uncritically, and spoke out when he believed that Lenin was wrong. During the period of his revolutionary activity, Stalin regarded some of Lenin's views and actions as being the self-indulgent activities of a spoiled émigré, deeming them counterproductive for those Bolshevik activists based within the Russian Empire itself. After the October Revolution, they continued to have differences. Whereas Lenin believed that all countries across Europe and Asia would readily unite as a single state following proletariat revolution, Stalin argued that national pride would prevent this, and that different socialist states would have to be formed; in his view, a country like Germany would not readily submit to being part of a Russian-dominated federal state. Stalin biographer Oleg Khlevniuk nevertheless believed that the pair developed a "strong bond" over the years, and after Lenin's death, Stalin relied heavily on Lenin's writings—far more so than those of Marx and Engels—to guide him in the affairs of state. Stalin adopted the Leninist view on the need for a revolutionary vanguard who could lead the proletariat rather than being led by them. Leading this vanguard, he believed that the Soviet peoples needed a strong, central figure—akin to a Tsar—whom they could rally around. In his words, "the people need a Tsar, whom they can worship and for whom they can live and work". He read about, and admired, two Tsars in particular: Ivan the Terrible and Peter the Great.

Stalinism was a development of Leninism, and while Stalin avoided using the term "Marxism-Leninism-Stalinism", he allowed others to do so. Following Lenin's death, Stalin contributed to the theoretical debates within the Communist Party, namely by developing the idea of "Socialism in One Country". This concept was intricately linked to factional struggles within the party, particularly against Trotsky. He first developed the idea in December 1924 and elaborated upon in his writings of 1925–26. Stalin's doctrine held that socialism could be completed in Russia but that its final victory there could not be guaranteed because of the threat from capitalist intervention. For this reason, he retained the Leninist view that world revolution was still a necessity to ensure the ultimate victory of socialism. Although retaining the Marxist belief that the state would wither away as socialism transformed into pure communism, he believed that the Soviet state would remain until the final defeat of international capitalism. This concept synthesised Marxist and Leninist ideas with nationalist ideals, and served to discredit Trotsky—who promoted the idea of "permanent revolution"—by presenting the latter as a defeatist with little faith in Russian workers' abilities to construct socialism.

Stalin viewed nations as contingent entities which were formed by capitalism and could merge into others. Ultimately he believed that all nations would merge into a single, global human community, and regarded all nations as inherently equal. Stalin argued that the Jews possessed a "national character" but were not a "nation" and were thus unassimilable. He argued that Jewish nationalism, particularly Zionism, was hostile to socialism. In his work, he stated that "the right of secession" should be offered to the ethnic-minorities of the Russian Empire, but that they should not be encouraged to take that option. He was of the view that if they became fully autonomous, then they would end up being controlled by the most reactionary elements of their community; as an example he cited the largely illiterate Tatars, whom he claimed would end up dominated by their mullahs. Khlevniuk therefore argued that Stalin reconciled Marxism with imperialism.
According to Service, Stalin's Marxism was imbued with a great deal of Russian nationalism. According to Montefiore, Stalin's embrace of the Russian nation was pragmatic, as the Russians were the core of the population of the USSR; it was not a rejection of his Georgian origins. Stalin's push for Soviet westward expansion into eastern Europe resulted in accusations of Russian imperialism.

In adulthood, Stalin measured tall. To give the impression that he was taller, he wore stacked shoes, and stood on a small wooden platform during parades. His mustached face was pock-marked from smallpox during childhood. He was born with a webbed left foot, and his left arm had been permanently injured in childhood which left it shorter than his right and lacking in flexibility, which was probably the result of being hit, at the age of 12, by a horse-drawn carriage.

During his youth, Stalin usually wore a red satin shirt, grey coat, and red fedora, or alternatively a traditional Georgian "chokha" and white hood. At the time he grew his hair long and often had a beard. His cultivation of a scruffy appearance deliberately sought to reject middle-class aesthetic values. From mid-1918 until his death he took to wearing military-style clothing, in particular long black boots and light-coloured collarless tunics, and also carried a gun. He had few material demands and lived plainly, with simple and inexpensive clothing and furniture; his interest was in power rather than wealth. He was a lifelong smoker, who smoked both a pipe and cigarettes.

Stalin was ethnically Georgian, and had grown up speaking the Georgian language, only learning Russian when aged eight or nine. Stalin remained proud of his Georgian identity and culture, and throughout his life, he retained his Georgian accent when speaking Russian. According to Montefiore, his adoption of Russian culture has been exaggerated, and he was profoundly Georgian in his lifestyle and personality, spending much of his final years in his homeland. Montefiore was of the view that "after 1917, he became quadri-national: Georgian by nationality, Russian by loyalty, internationalist by ideology, Soviet by citizenship." Service stated that Stalin "would never be Russian", could not credibly pass as one and contrary to what has been previously suggested, he never really tried to be one. Stalin was described as "Asiatic" by his colleagues, and told a Japanese journalist, "I am not a European man, but an Asian, a Russified Georgian". He first adopted the pseudonym "Stalin" in 1912; being based on the Russian word for "steel" it has often been translated as "Man of Steel". Prior nicknames included "Koba", "Soselo", "Ivanov" and many others.

Stalin had a soft voice, and when speaking Russian he did so slowly, carefully choosing his phrasing. Although he avoided doing so in public, in private Stalin used coarse language. Described as a poor orator, according to Volkogonov, Stalin's speaking style was "simple and clear, without flights of fancy, catchy phrases or platform histrionics". He rarely spoke before large audiences, and preferred to express himself in written form. His writing style was similar, being characterised by its simplicity, clarity, and conciseness.

Trotsky and several other Soviet figures promoted the idea that Stalin was a mediocrity. This idea gained widespread acceptance outside the Soviet Union but was misleading. According to Montefiore, "it is clear from hostile and friendly witnesses alike that Stalin was always exceptional, even from childhood". Stalin had a complex mind, with a great deal of self-control. He rarely raised his voice in anger, although as his health declined in later life he became increasingly unpredictable and bad tempered. A hard worker, he displayed a keen desire to learn, and had an excellent memory. When in power, he scrutinised many details of Soviet life, from film scripts to architectural plans and military hardware. According to Volkogonov, "Stalin's private life and working life were one and the same"; he did not take days off from political activities.

Stalin was a capable actor who could play many different roles to different audiences, and was adept at deception, often lying or deceiving others as to his true motives and aims. He was a good organiser, with a strategic mind, and judged others according to their inner strength, practicality, and cleverness. He could be rude and insulting, traits in his personality that he publicly acknowledged. Despite his short temper and tough-talking attitude, he could be very charming; when relaxed, he cracked jokes and mimicked others. Montefiore suggested that it was his charm which represented "the foundation of Stalin's power in the Party". Several historians have seen it appropriate to follow Lazar Kaganovich's description of there being "several Stalins" as a means of understanding his multi-faceted personality.

Stalin was ruthless, temperamentally cruel, and had a propensity for violence excessive even among the Bolsheviks. He lacked compassion, something which Volkogonov suggested might have been accentuated by his many years spent in prison and exile, although he was capable of acts of kindness to strangers, even amid the Great Terror. He never personally attended any torture sessions or executions. Service stated that Stalin "derived deep satisfaction" from degrading and humiliating people, and that he "delighted" in keeping even close associates in a state of "unrelieved fear". He was capable of self-righteous indignation, and was both resentful, and vengeful, holding onto grievances against others for many years. He was also suspicious and conspiratorial, prone to believing that people were plotting against him and that there were vast international conspiracies behind acts of dissent. Montefiore thought that Stalin's brutality marked him out as a "natural extremist"; Service suggested that he had a paranoid or sociopathic personality disorder, with this "dangerously damaged" personality supplying "the high-octane fuel for the journey to the Great Terror". Other historians have argued that Stalin's brutality should be seen not as a result of any personality traits, but through his unflinching commitment to the survival of his socialist state and the cause of international socialism. By the period of "glasnost" and "perestroika", Soviet psychologists were openly debating whether Stalin had been insane.

Stalin admired artistic talent, and protected several Soviet writers, such as Mikhail Bulgakov, even when their work was regarded as harmful to his regime. He enjoyed listening to music, and owned around 2,700 albums. During the 1930s and 1940s, he frequently attended performances at the Bolshoi Theatre. His taste in music and theatre was conservative, favouring classical drama, opera, and ballet over what he dismissed as experimental "formalism". He similarly favoured classical forms in the visual arts, disliking avant-garde styles like cubism and futurism. He was a voracious reader, with a library of over 20,000 books. Little of this was fiction, although he knew passages from the work of Alexander Pushkin and Nikolay Nekrasov by heart and could also recite Walt Whitman. He favoured historical studies, keeping up with debates in the study of Russian, Mesopotamian, ancient Roman, and Byzantine history. An autodidact, he claimed to read as many as 500 pages a day, with Montefiore regarding him as an intellectual.

Stalin typically awoke at around 11am, and worked late into the evening. His main meal was lunch, which took place between 3 and 5pm, while dinner was held no earlier than 9pm. He often chose to dine with other Politburo members and their wives who lived in the Kremlin. He spent much time in the cinemas installed in the Kremlin and his dachas, where he enjoyed watching films with other officials late at night; he had a particular fondness for the Western genre, although his favourite film was the 1938 film "Volga Volga". Stalin enjoyed alcoholic beverages, and at dinner parties and other social events would encourage those around him to join in, hoping that in a drunken state they would reveal secrets. He enjoyed practical jokes, for instance by putting a tomato on the seat of Politburo members and waiting for them to sit on it, and encouraged singing at social events. As an infant, Stalin had displayed a love of flowers, and later in life he became a keen gardener. His dacha in the Moscow suburb of Volynskoe was surrounded by a 50-acre park, with Stalin devoting much attention to its agricultural activities. Stalin also enjoyed billiards and was an accomplished player.

Stalin disliked travel, and refused to travel by plane. As leader of the USSR, he rarely left Moscow, unless to go to his dacha or on holiday. His choice of favoured holiday house changed over the years, although he holidayed in southern parts of the USSR every year from 1925 to 1936 and again from 1945 to 1951. Along with other senior figures, he had a dacha at Zubalova, 35 km outside Moscow, although he ceased using it after Nadya's 1932 suicide. After 1932, he favoured Abkhazia as a holiday destination, being a friend of its leader, Nestor Lakoba. In 1934, his new Kuntsevo Dacha was built; 9 km from the Kremlin, it became his primary residence. In 1935 he began using a new dacha provided for him by Lakoba at Novy Afon; in 1936, he had the Kholodnaya Rechka dacha built on the Abkhazian coast, designed by Miron Merzhanov. Before World War II he added the Lipki estate and Semyonovskaya and had at least four dachas in the south by 1937, including one near Sochi. A luxury villa near Gagri was given to him by Beria. In Abkhazia he maintained a mountain retreat. After the war he added dachas at Novy Afon, near Sukhumi, in the Valdai Hills, and at Lake Mitsa. Another estate was near Zelyony Myss on the Black Sea. All these dachas, estates, and palaces were staffed, well-furnished and equipped, kept safe by security forces, and were mainly used privately, rarely for diplomatic purposes.

Although Stalin publicly condemned anti-Semitism, he was repeatedly accused of being anti-Semitic. People who knew him, such as Khrushchev, suggested that he had long harbored negative sentiments toward Jews, and anti-Semitic trends in the Kremlin's policies were further fueled by Stalin's struggle against Trotsky. After Stalin's death, Khrushchev made the claim that Stalin hinted that he should incite anti-Semitism in the Ukraine, allegedly telling him that "the good workers at the factory should be given clubs so they can beat the hell out of those Jews." In 1946, Stalin allegedly said privately that "every Jew is a potential spy." Conquest stated that although Stalin had Jewish associates, he promoted anti-Semitism. Service cautioned that there was "no irrefutable evidence" of anti-Semitism in Stalin's published work, although his private statements and public actions were "undeniably reminiscent of crude antagonism towards Jews"; he added that throughout Stalin's lifetime, the Georgian "would be the friend, associate or leader of countless individual Jews". According to Beria, Stalin had affairs with several Jewish women.

Friendship was important to Stalin, and he used it to gain and maintain power. Kotkin observed that Stalin "generally gravitated to people like himself: parvenu intelligentsia of humble background". He gave nicknames to his favourites, for instance referring to Yezhov as "my blackberry". Stalin was sociable and enjoyed a joke. According to Montefiore, Stalin's friendships "meandered between love, admiration, and venomous jealousy". While head of the Soviet Union he remained in contact with many of his old friends in Georgia, sending them letters and gifts of money.

Stalin was attracted to women and there are no reports of any homosexual tendencies; according to Montefiore, in his early life Stalin "rarely seems to have been without a girlfriend". He was sexually promiscuous, although rarely talked about his sex life. Montefiore noted that Stalin's favoured types were "young, malleable teenagers or buxom peasant women", who would be supportive and unchallenging toward him. According to Service, Stalin "regarded women as a resource for sexual gratification and domestic comfort". Stalin married twice and had several offspring. He married his first wife, Ekaterina Svanidze, in 1906. According to Montefiore, theirs was "a true love match"; Volkogonov suggested that she was "probably the one human being he had really loved". They had a son, Yakov, who often frustrated and annoyed Stalin. Yakov had a daughter, Galina, before fighting for the Red Army in the Second World War. He was captured by the German Army and then committed suicide.

Stalin's second wife was Nadezhda Alliluyeva; theirs was not an easy relationship, and they often rowed. They had two biological children—a son, Vasiliy, and a daughter, Svetlana—and adopted another son, Artyom Sergeev, in 1921. During his marriage to Nadezhda, Stalin had affairs with many other women, most of whom were fellow revolutionaries or their wives. Nadezdha suspected that this was the case, and committed suicide in 1932. Stalin regarded Vasily as spoiled and often chastised his behaviour; as Stalin's son, Vasily nevertheless was swiftly promoted through the ranks of the Red Army and allowed a lavish lifestyle. Conversely, Stalin had an affectionate relationship with Svetlana during her childhood, and was also very fond of Artyom. In later life, he disapproved of Svetlana's various suitors and husbands, putting a strain on his relationship with her. After the Second World War he made little time for his children and his family played a decreasingly important role in his life. After Stalin's death, Svetlana changed her surname from Stalin to Allilueva, and defected to the U.S.

After Nadezdha's death, Stalin became increasingly close to his sister-in-law Zhenya Alliluyeva; Montefiore believed that they were probably lovers. There are unproven rumours that from 1934 onward he had a relationship with his housekeeper Valentina Istomina. Stalin had at least two illegitimate children, although he never recognised these as being his. One of these, Constantin Kuzakova, later taught philosophy at the Leningrad Military Mechanical Institute, but never met his father. The other, Alexander, was the son of Lidia Pereprygia; he was raised as the son of a peasant fisherman and the Soviet authorities made him swear never to reveal that Stalin was his biological father.

The historian Robert Conquest stated that Stalin, "perhaps more than any other [person,] determined the course of the twentieth century". Service regarded him as "one of the twentieth century's outstanding politicians"; Volkogonov also deemed him "an exceptional politician". Montefiore labelled Stalin as "that rare combination: both 'intellectual' and killer", a man who was "the ultimate politician" and "the most elusive and fascinating of the twentieth-century titans". According to historian Kevin McDermott, interpretations of Stalin range from "the sycophantic and adulatory to the vitriolic and condemnatory". For most Westerners and anti-communist Russians, he is viewed overwhelmingly negatively as a mass murderer; for significant numbers of Russians and Georgians, he is regarded as a great statesman and state-builder. Stalin has also been described as a terrorist for his revolutionary activities in Georgia.

Stalin strengthened and stabilised the Soviet Union, transforming it into a "great power" in under three decades. Service suggested that without Stalin's leadership the Soviet Union might have collapsed long before 1991. By the time of his death, the country had been transformed into a world power and industrial colossus, with a literate population. According to Service, Stalin's USSR "could claim impressive achievements" in terms of urbanisation, military strength, education, and Soviet pride. Although millions of Soviet citizens despised him, support for him was nevertheless widespread throughout Soviet society.

Stalin's Soviet Union has been characterised as totalitarian. Various biographers have described him as a dictator, an autocrat, or accused him of practicing Caesarism. Montefiore argued that while Stalin initially ruled as part of a Communist Party oligarchy, in 1934 the Soviet government transformed from this oligarchy into a personal dictatorship, with Stalin only becoming "absolute dictator" between March and June 1937, when senior military and NKVD figures were eliminated. According to Kotkin, Stalin "built a personal dictatorship within the Bolshevik dictatorship". In both the Soviet Union and elsewhere he came to be portrayed as an "Oriental despot". The biographer Dmitri Volkogonov characterised him as "one of the most powerful figures in human history", while McDermott stated that Stalin had "concentrated unprecedented political authority in his hands", and Service noted that by the late 1930s, Stalin "had come closer to personal despotism than almost any monarch in history".

McDermott nevertheless cautioned about "over-simplistic stereotypes"—promoted in the fiction of writers like Aleksandr Solzhenitsyn, Vasily Grossman, and Anatoly Rybakov—that portrayed Stalin as an omnipotent and omnipresent tyrant who controlled every aspect of Soviet life through repression and totalitarianism. Service similarly warned of the portrayal of Stalin as an "unimpeded despot", noting that "powerful though he was, his powers were not limitless", and his rule depended on his willingness to conserve the Soviet structure he had inherited. Khlevniuk noted that at various points, particularly when Stalin was old and frail, there were "periodic manifestations" in which the party oligarchy threatened his autocratic control. Stalin denied to foreign visitors that he was a dictator, stating that those who labelled him such did not understand the Soviet governance structure.

A vast literature devoted to Stalin has been produced.
During Stalin's lifetime, his approved biographies were largely hagiographic in content. Stalin ensured that these works gave very little attention to his early life, particularly because he did not wish to emphasise his Georgian origins in a state numerically dominated by Russians. A large number of Stalin biographies have been published since his death. Until the 1980s, these relied largely on the same sources of information as each other. Under the administration of Mikhail Gorbachev a number of previously classified files on Stalin's life were made available to historians; during Gorbachev's "glasnost" period, Stalin became "one of the most urgent and vital issues on the public agenda" in the Soviet Union. After the dissolution of the Union in 1991, the rest of the archives were opened to historians, resulting in much new information about Stalin coming to light, and producing a flood of new research.

Leninists remain divided in their views on Stalin. Some view him as the authentic successor to Lenin, who continued and developed his legacy, while others believe that Stalin betrayed Lenin's ideas by deviating from them. The socio-economic nature of Stalin's Soviet Union has also been much debated, varyingly being labelled a form of state socialism, state capitalism, bureaucratic collectivism, or a totally unique mode of production. Volkogonov, who was a socialist, noted that Stalin's example "supplied ample ammunition and weighty arguments" to those who opposed socialism, suggesting that the Soviet leader had damaged "the enormous appeal of socialism generated by the October Revolution".

According to Service, Stalin was "one of the most notorious figures in history", one who ordered "the systematic killing of people on a massive scale". Khlevniuk stated that Stalin's actions "upended or utterly destroyed literally millions upon millions of lives", suggesting that at least 60 million people faced some form of repression or discrimination under Stalin's regime. Official records show that 800,000 were shot in the Soviet Union between 1930 and 1952, although a larger number died during torture or as a result of poor conditions in labour camps. Many more died as a result of famines and starvation; particularly during the 1932–33 famine.

Cold War era estimates for the total number of deaths under Stalin have been as high as 60 million but more recent estimates have been far lower. In his 2008 edition of "The Great Terror", Conquest stated that "at least 15 million people" were killed by "the whole range of Soviet regime's terrors", although acknowledged that exact numbers will never be known. The historian and archival researcher Stephen G. Wheatcroft attributes roughly 3 million deaths to the Stalinist regime, including those from criminal negligence but excluding famine deaths, which he and historian R. W. Davies estimate to be around 5.5 to 6.5 million. The American historian Timothy D. Snyder asserts that while the Nazi regime killed 11–12 million non-combatants, Stalin's was responsible for about 6–9 million, negating claims that Stalin killed more than Hitler.

Historians continue to debate whether or not the 1932–33 Ukrainian famine—known in Ukraine as the Holodomor—should be called a genocide. Popular among Ukrainian nationalists is the idea that Stalin consciously organised the famine to suppress nationalist desires among the Ukrainian people. Twenty-six countries have officially recognized the famine under the legal definition of genocide. In 2006, the Ukrainian Parliament declared it to be genocide, and in 2010 a Ukrainian court posthumously convicted Stalin, Lazar Kaganovich, Stanislav Kosior and other Soviet leaders of genocide. This interpretation has been rejected by more recent historical studies. These have articulated the view that—while Stalin's policies contributed significantly to the high mortality rate—there is no evidence that Stalin or the Soviet government consciously engineered the famine. The idea that this was a targeted attack on the Ukrainians is complicated by the widespread suffering that also affected other Soviet peoples in this period, including the Russians. Despite this lack of clear intentionality on Stalin's part, the historian Norman Naimark noted that although there may not be sufficient "evidence to convict him in an international court of justice as a genocidaire[...] that does not mean that the event itself cannot be judged as genocide".

The process of de-Stalinization in the Soviet Union took place shortly after his death. Malenkov denounced the Stalin personality cult, while "Pravda" restrained its praise of him and began to criticise his personality cult. Stalingrad was renamed Volgograd. In 1956, Khruschev gave his "Secret Speech", titled "On the Cult of Personality and Its Consequences", to a closed session of the Party's 20th Congress. There, Khrushchev denounced Stalin for both his mass repression and his personality cult. He repeated these denunciations at the 22nd Party Congress in October 1962. In October 1961, Stalin's body was removed from the mausoleum and buried in the Kremlin Wall Necropolis next to the Kremlin walls, the location marked only by a simple bust.

Khrushchev's de-Stalinisation process in Soviet society ended when he was replaced as leader by Leonid Brezhnev in 1964; the latter introduced a level of re-Stalinisation within the Soviet Union. In 1969 and again in 1979, plans were proposed for a full rehabilitation of Stalin's legacy; both were defeated by complaints both domestically and from foreign Communist parties. Gorbachev saw the total denunciation of Stalin as being necessary for the regeneration of Soviet society. After the fall of the Soviet Union in 1991, the first President of the new Russian Federation, Boris Yeltsin, also retained Gorbachev's denunciation of Stalin but added to it a denunciation of Lenin. His successor, Vladimir Putin, did not seek to rehabilitate Stalin but placed an emphasis on celebrating Soviet achievement under Stalin's leadership rather than the repressions.

Amid the social and economic turmoil of the post-Soviet period, many Russians viewed Stalin as having overseen an era of order, predictability, and pride. He remains a revered figure among many Russian nationalists, who feel nostalgic about the Soviet victory over Nazi Germany in World War II, and he is regularly invoked approvingly within both Russia's far-left and far-right. In the 2008 "Name of Russia" television show, Stalin was voted as the third most notable personality in Russian history. A 2017 poll revealed that Stalin's popularity reached a 16-year high among the Russian population, with 46% expressing a favourable view of him. At the same time, there was a growth in pro-Stalinist literature in Russia, much of which relies upon the misrepresentation or fabrication of source material. In this literature, Stalin's repressions are regarded either as a necessary measure to defeat "enemies of the people" or the result of lower-level officials acting without Stalin's knowledge. In October 2017, Russian President Vladimir Putin has commented on Stalinist repressions while opening the Wall of Grief memorial in Moscow, saying "This terrible past can not be erased from the national memory, and it cannot be justified by anything, not even by the so-called highest interests of the welfare of the people."

The only part of the former Soviet Union where admiration for Stalin has remained consistently widespread is Georgia. Many Georgians resent criticism of Stalin, the most famous figure from their nation's modern history; a 2013 survey by Tbilisi University found 45% of Georgians expressing "a positive attitude" to him. In a 2012 opinion survey commissioned by the Carnegie Endowment, 38% of Armenians agreed with the statement, "Our people will always have need of a leader like Stalin, who will come and restore order."

In early 2010 a new monument in honor of Stalin was erected in Zaporizhia, Ukraine; in December unknown persons cut off its head and in 2011 it was destroyed in an explosion. In a 2016 Kiev International Institute of Sociology poll, 38% of respondents had a negative attitude to Stalin, 26% a neutral one and 17% a positive (19% refused to answer).





</doc>
<doc id="15642" url="https://en.wikipedia.org/wiki?curid=15642" title="January">
January

January is the first month of the year in the Julian and Gregorian calendars and the first of seven months to have a length of 31 days. The first day of the month is known as New Year's Day. It is, on average, the coldest month of the year within most of the Northern Hemisphere (where it is the second month of winter) and the warmest month of the year within most of the Southern Hemisphere (where it is the second month of summer). In the Southern hemisphere, January is the seasonal equivalent of July in the Northern hemisphere and vice versa.

Ancient Roman observances during this month include Cervula, and Juvenalia; celebrated January 1, as well as one of three Agonalia, celebrated January 9, and Carmentalia, celebrated January 11. These dates do not correspond to the modern Gregorian calendar.

January (in Latin, "Ianuarius") is named after the Latin word for door ("ianua"), since January is the door to the year and an opening to new beginnings. The month is conventionally thought of as being named after Janus, the god of beginnings and transitions in Roman mythology, but according to ancient Roman farmers' almanacs Juno was the tutelary deity of the month.

Traditionally, the original Roman calendar consisted of 10 months totaling 304 days, winter being considered a month-less period. Around 713 BC, the semi-mythical successor of Romulus, King Numa Pompilius, is supposed to have added the months of January and February, so that the calendar covered a standard lunar year (354 days). Although March was originally the first month in the old Roman calendar, January became the first month of the calendar year either under Numa or under the Decemvirs about 450 BC (Roman writers differ). In contrast, each specific calendar year was identified by the names of the two consuls, who entered office on May 1 or March 15 until 153 BC, from when they entered office on January 1.

Various Christian feast dates were used for the New Year in Europe during the Middle Ages, including March 25 (Feast of the Annunciation) and December 25. However, medieval calendars were still displayed in the Roman fashion with twelve columns from January to December. Beginning in the 16th century, European countries began officially making January 1 the start of the New Year once again—sometimes called "Circumcision Style" because this was the date of the Feast of the Circumcision, being the seventh day after December 25.

Historical names for January include its original Roman designation, Ianuarius, the Saxon term "Wulf-monath" (meaning "wolf month") and Charlemagne's designation Wintarmanoth ("winter / cold month"). In Slovene, it is traditionally called "prosinec". The name, associated with millet bread and the act of asking for something, was first written in 1466 in the Škofja Loka manuscript.

According to Theodor Mommsen, 1 January became the first day of the year in 600 AUC of the Roman calendar (153 BC), due to disasters in the Lusitanian War. A Lusitanian chief called Punicus invaded the Roman territory, defeated two Roman governors, and killed their troops. The Romans resolved to send a consul to Hispania, and in order to accelerate the dispatch of aid, "they even made the new consuls enter into office two months and a half before the legal time" (March 15).


"This list does not necessarily imply either official status or general observance."<br>


"This list does not necessarily imply either official status or general observance."

"All Baha'i, Islamic, and Jewish observances begin at sundown prior to the date listed, and end at sundown on the date in question."

"This list does not necessarily imply either official status or general observance."

First Monday: January 1
First Sunday of the year, unless the Sunday falls on January 1, 6, or 7, then January 2: January 2
January 2 unless that day is a Sunday, in which case January 3: January 2) 
First Friday: January 5 
Sunday following January 6: January 7 
Monday after January 6: January 8 
Second Monday: January 8
Friday before third Monday: January 12
Second Saturday: January 13 
Third full week of January: January 15–19
Third Monday: January 15
Wednesday of the third full week of January: January 17
Friday between January 19–25: January 19
Third Friday: January 19
Third Sunday: January 21
11th Sunday before Pascha (Eastern Christianity): January 21
Sunday closest to January 22: January 21
Last week of January: January 22–26
Fourth Monday: January 22
63 days before Pascha (Eastern Christianity): January 27
Last Saturday: January 27
9th Sunday before Easter in Western Christianity: January 28
10th Sunday before Pascha in Eastern Christianity: January 28 
Last Sunday: January 28
January 30 or the nearest Sunday: January 28
Monday Closest to January 29: January 29
Last Monday in January: January 29
Second Monday before Clean Monday in Eastern Christianity: January 29–31



</doc>
<doc id="15644" url="https://en.wikipedia.org/wiki?curid=15644" title="Johnny Unitas">
Johnny Unitas

John Constantine Unitas (; May 7, 1933 – September 11, 2002), nicknamed "Johnny U" and "The Golden Arm", was an American football player in the National Football League (NFL). He spent the majority of his career playing for the Baltimore Colts. He was a record-setting quarterback, and the NFL's most valuable player in 1959, 1964, and 1967. For 52 years he held the record for most consecutive games with a touchdown pass (which he set between 1956 and 1960), until quarterback Drew Brees broke his long standing record on October 7, 2012. Unitas was the prototype of the modern era marquee quarterback with a strong passing game, media fanfare, and widespread popularity. He has been consistently listed as one of the greatest NFL players of all time.

John Constantine Unitas was born in Pittsburgh in 1933 to Francis J. Unitas and Helen Superfisky, who both were of Lithuanian descent; he grew up in the Mount Washington neighborhood. When Unitas was five years old, his father died of cardiovascular renal disease complicated by pneumonia, leaving the young boy to be raised by his mother, who worked two jobs to support the family. His surname was a result of a phonetic transliteration of a common Lithuanian last name "Jonaitis". Attending St. Justin's High School in Pittsburgh, Unitas played halfback and quarterback. After high school, he looked for an opportunity to play college football.

The University of Louisville offered him the chance and Unitas left home to play for the Cardinals.

In his younger years, Unitas dreamed about being part of the Fighting Irish of Notre Dame but when he tried out for the team, coach Frank Leahy simply said that he was just too skinny and he would "get murdered" if he was put on the field.

In his four-year career as a Louisville Cardinal, Unitas completed 245 passes for 3,139 yards and 27 touchdowns. Reportedly, the 6-foot-1 Unitas weighed 145 pounds on his first day of practice at Louisville. Unitas's first start was in the fifth game of the 1951 season against St. Bonaventure, where he threw 11 consecutive passes and three touchdowns to give the Cardinals a 21–19 lead. Louisville ended up losing the game 22–21 on a disputed field goal, but found a regular. Unitas completed 12 of 19 passes for 240 yards and four touchdowns in a 35–28 victory over Houston. The team finished the season 5–5 overall and 4–1 with Unitas as the starting quarterback. As a freshman, Unitas completed 46 of 99 passes for 602 yards and nine touchdowns (44).

By the 1952 season, the university decided to de-emphasize sports. The new president at Louisville, Dr. Philip Grant Davidson, reduced the amount of athletic aid, and tightened academic standards for athletes. As a result, 15 returning players could not meet the new standards and lost their scholarships. But Unitas maintained his scholarship by taking on a new elective: square dancing. In 1952, coach Frank Camp switched the team to two-way football. Unitas not only played safety or linebacker on defense and quarterback on offense but returned kicks and punts on special teams. The Cards won their first game against Wayne State, and then Florida State in the second game. Unitas completed 16 of 21 passes for 198 yards and three touchdowns. It was said that Unitas put on such a show at the Florida State game that he threw a pass under his legs for 15 yards. The rest of the season was a struggle for the Cards, who finished 3–5. Unitas completed 106 of 198 passes for 1,540 yards and 12 touchdowns in his sophomore year.

The team won their first game in 1953, against Murray State, and lost the rest for a record of 1–7. One of the most memorable games of the season came in a 59–6 loss against Tennessee. Unitas completed 9 of 19 passes for 73 yards, rushed 9 times for 52 yards, returned 6 kickoffs for eighty-five yards, 1 punt for three yards, and had 86 percent of the team's tackles. The only touchdown the team scored was in the fourth quarter when Unitas made a fake pitch to the running back and ran the ball 23 yards for a touchdown. Unitas was hurt later in the fourth quarter while trying to run the ball. On his way off the field, he received a standing ovation. When he got to the locker room he was so worn that his jersey and shoulder pads had to be cut off because he could not lift his arms. Louisville ended the season with a 20–13 loss to Eastern Kentucky. In his junior year, Unitas completed 49 of 95 passes for 470 yards and three touchdowns.

Unitas was elected captain for the 1954 season, but due to an early injury did not see much playing time. His first start was the third game of the season, against Florida State. Of the 34-man team, 21 were freshmen. The 1954 Cardinals went 3–6, with their last win at home against Morehead State. Unitas was slowed by so many injuries his senior year his 527 passing yards ended second to Jim Houser's 560.

After his collegiate career, the Pittsburgh Steelers of the NFL drafted Unitas in the ninth round. However, Unitas was released before the season began as the odd man out among four quarterbacks trying to fill three spots. Steelers' head coach Walt Kiesling had made up his mind about Unitas; he thought he was not smart enough to quarterback an NFL team, and Unitas was not given any snaps in practice with the Steelers. Among those edging out Unitas was Ted Marchibroda, future longtime NFL head coach. Out of pro football, Unitas—by this time married—worked in construction in Pittsburgh to support his family. On the weekends, he played quarterback, safety and punter on a local semi-professional team called the Bloomfield Rams for $6 a game.

In 1956, Unitas joined the Baltimore Colts of the NFL under legendary coach Weeb Ewbank, after being asked at the last minute to join Bloomfield Rams lineman Jim Deglau, a Croatian steel worker with a life much like Unitas', at the latter's scheduled Colts tryout. The pair borrowed money from friends to pay for the gas to make the trip. Deglau later told a reporter after Unitas' death, "[His] uncle told him not to come. [He] was worried that if he came down and the Colts passed on him, it would look bad (to other NFL teams)." The Colts signed Unitas, much to the chagrin of the Cleveland Browns, who had hoped to claim the rejected Steeler quarterback.

Unitas made his NFL debut with an inauspicious "mop-up" appearance against Detroit, going 0-2 with one interception. Two weeks later, starting quarterback George Shaw suffered a broken leg against the Chicago Bears. In his first serious action, Unitas' initial pass was intercepted and returned for a touchdown. Then he botched a hand-off on his next play, a fumble recovered by the Bears. Unitas rebounded quickly from that 58–27 loss, leading the Colts to an upset of Green Bay and their first win over Cleveland. He threw nine touchdown passes that year, including one in the season finale that started his record 47-game streak. His 55.6-percent completion mark was a rookie record.

In 1957, his first season as the Colts full-time starter at quarterback, Unitas finished first in the NFL in passing yards (2,550) and touchdown passes (24) as he helped lead the Colts to a 7–5 record, the first winning record in franchise history. At season's end, Unitas received the Jim Thorpe Trophy as the NFL's Most Valuable Player by the Newspaper Enterprise Association (NEA).

Unitas continued his prowess in 1958 passing for 2,007 yards and 19 touchdowns as the Colts won the Western Conference title. The Colts won the NFL championship under Unitas' leadership on December 28, 1958, by defeating the New York Giants 23–17 in sudden death overtime on a touchdown by fullback Alan Ameche. It was the first overtime game in NFL history, and is often referred to as the "greatest game ever played". The game, nationally televised by NBC, has been credited for sparking the rise in popularity of professional football during the 1960s.

In 1959, Unitas was named the NFL's MVP by the Associated Press (AP) for the first time, as well as United Press International's player of the year, after leading the NFL in passing yards (2,899), touchdown passes (32), and completions (193). Unitas then led the Colts to a repeat championship, beating the Giants again 31–16 in the title game.

As the 1960s began, the Colts' fortunes (and win totals) declined. Injuries to key players such as Alan Ameche, Raymond Berry, and Lenny Moore were a contributing factor. Unitas' streak of 47 straight games with at least one touchdown pass ended against the Los Angeles Rams in week 11 of the 1960 season. In spite of this, Unitas topped the 3000 yard passing mark for the first time and led the league in touchdown passes for the fourth consecutive season.

After three middle-of-the-pack seasons, Colts owner Carroll Rosenbloom fired Weeb Ewbank and replaced him with Don Shula, who at the time was the youngest head coach in NFL history (33 years of age when he was hired). The Colts finished 8–6 in Shula's first season at the helm, good enough for only third place in the NFL's Western Conference but they did end the season on a strong note by winning their final three games. The season was very successful for Unitas personally as he led the NFL in passing yards with a career-best total of 3,481 and also led in completions with 237.

The 1964 season would see the Colts return to the top of the Western Conference. After dropping their season opener to the Minnesota Vikings, the Colts ran off 10 straight victories to finish with a 12–2 record. The season was one of Unitas' best as he finished with 2,824 yards passing, a league-best 9.26 yards per pass attempt, 19 touchdown passes and only 6 interceptions. He was named the NFL's Most Valuable Player by the AP and UPI for a second time. However, the season would end on a disappointing note for the Colts as they were upset by the Cleveland Browns in the 1964 NFL Championship Game, losing 27–0.

Unitas resumed his torrid passing in 1965, as he threw for 2,530 yards, 23 touchdowns and finished with a league-high and career best 97.1 passer rating. But he was lost for the balance of the season due to a knee injury in a week 12 loss to the Bears. More postseason heartbreak would follow in 1965. The Colts and Packers finished in a tie for first place in the Western Conference and a one-game playoff was played in Green Bay to decide who would be the conference representative in the 1965 NFL Championship Game. The Colts lost in overtime 13–10 due in large part to a game-tying field goal by Don Chandler that many say was incorrectly ruled good. Backup quarterback Gary Cuozzo also suffered a season-ending injury the following week and it would be running back Tom Matte who filled in as the emergency QB for the regular-season finale and the playoff loss to the Packers.

Unitas, healthy once more, threw for 2748 yards and 22 touchdowns in 1966 in a return to Pro Bowl form. However, he posted a league-high 24 interceptions.

After once again finishing 2nd in the Western Conference in 1966, the Colts rebounded to finish 11–1–2 in 1967 tying the Los Angeles Rams for the NFL's best record. In winning his third MVP award from the AP and UPI in 1967 (and his second from the NEA), Unitas had a league-high 58.5 completion percentage and passed for 3,428 yards and 20 touchdowns. He openly complained about having tennis elbow and he threw eight interceptions and only three touchdown passes in the final five games. Once again, the season ended in heartbreak for the Colts, as they were shut out of the newly instituted four team NFL playoff after losing the divisional tiebreaker to the Rams, a 34–10 rout in the regular season finale.

In the final game of the 1968 preseason, the muscles in Unitas' arm were torn when he was hit by a member of the Dallas Cowboys defense. Unitas wrote in his autobiography that he felt his arm was initially injured by the use of the "night ball" that the NFL was testing for better TV visibility during night games. In a post-game interview the previous year, he noted having constant pain in his elbow for several years prior. He would spend most of the season sitting on the bench. The Colts still marched to a league-best 13–1 record behind backup quarterback and ultimate 1968 NFL MVP Earl Morrall. Although he was injured through most of the season, Unitas came off the bench to play in Super Bowl III, the famous game where Joe Namath guaranteed a New York Jets win despite conventional wisdom. Unitas' insertion was a desperation move in an attempt to retrieve dominance of the NFL over the upstart AFL. Although the Colts won an NFL Championship in 1968, they lost the Super Bowl to the AFL Champion New York Jets, thus becoming the first ever NFL champions that were not also deemed world champions. Unitas helped put together the Colts' only score, a touchdown late in the game. Unitas also drove the Colts into scoring position following the touchdown and successful onside kick, but for reasons that to this day are unknown, head coach Don Shula eschewed a field goal attempt, which (if successful) would have cut the Jets' lead to 16-10. Despite not playing until late in the third quarter, Unitas still finished the game with more passing yards than the team's starter, Earl Morrall.

After an off-season of rehabilitation on his elbow, Unitas rebounded in 1969, passing for 2342 yards and twelve touchdowns with 20 interceptions. But the Colts finished with a disappointing 8-5-1 record, and missed the playoffs.

In 1970, the NFL and AFL had merged into one league, and the Colts moved to the new American Football Conference, along with the Cleveland Browns and the Pittsburgh Steelers. Unitas threw for 2213 yards and 14 touchdowns while leading the Colts to an 11-2-1 season. In their first rematch with the Jets, Unitas and Namath threw a combined nine interceptions in a 29-22 Colts win. Namath threw 62 passes and broke his hand on the final play of the game, ending his season.

Unitas threw for 390 yards, three touchdowns, and no interceptions in AFC playoff victories over the Cincinnati Bengals and the Oakland Raiders. In Super Bowl V against the Dallas Cowboys, he was knocked out of the game with a rib injury in the second quarter, soon after throwing a 75-yard touchdown pass (setting a then-Super Bowl record) to John Mackey. However, he had also tossed two interceptions before his departure from the game. Earl Morrall came in to lead the team to a last second, 16-13 victory.

In 1971, Unitas split playing time with Morrall, throwing only three touchdown passes. He started both playoff games, a win over the Cleveland Browns that sent the Colts to the AFC Championship game against the Miami Dolphins, which they lost by a score of 21–0. Unitas threw three interceptions in the game, one of which was returned for a touchdown by safety Dick Anderson.

The 1972 season saw the Colts declining into mediocrity. After losing the season opener, Unitas was involved in the second and final regular season head-to-head meeting with "Broadway" Joe Namath. The first was in 1970 (won by the Colts, 29–22). The last meeting was a memorable one, which took place on September 24, 1972, at Memorial Stadium. Unitas threw for 376 yards and three touchdowns, but Namath upstaged him again, bombing the Colts for 496 yards and six touchdowns in a 44–34 Jets victory – their first over Baltimore since the 1970 merger. After losing four of their first five games, the Colts fired head coach Don McCafferty, and benched Unitas.

One of the more memorable moments in football history came on Unitas' last game in a Colts uniform at Memorial Stadium, in a game against the Buffalo Bills. Unitas was not the starter for this game, but the Colts were blowing the Bills out by a score of 28–0 behind Marty Domres; Unitas entered the game due to the fans chanting, "We want Unitas!!!", and a plan devised by head coach John Sandusky to convince Unitas that the starting quarterback was injured. Unitas came onto the field, and threw two passes, one of which was a long touchdown to wide receiver Eddie Hinton which would be his last pass as a Colt. The Colts won the game by the score of 35–7.

Unitas was traded to the San Diego Chargers in 1973 after posting a 5-9 record in 1972 with Baltimore, but he was far past his prime. He replaced former Chargers quarterback John Hadl. He was replaced in Baltimore by Marty Domres, who had been acquired from the San Diego Chargers in August, 1972. Domres was ultimately replaced by LSU's Bert Jones, drafted with the number two overall pick in 1973. Unitas started the season with a 38-0 loss to the Washington Redskins. He threw for just 55 yards and 3 interceptions, and was sacked 8 times. His final victory as a starter came against the Buffalo Bills in week two. Unitas was 10-18 for 175 yards, two touchdown passes, and no interceptions in a 34-7 Chargers rout . Unitas was clearly not the same player he was years ago and many were questioning his role as a starter after a loss to the Bengals in week three. Two weeks later, he threw two first-half interceptions, passed for only 19 yards, and went 2-for-9 against the Pittsburgh Steelers. He was then replaced by rookie quarterback, future Hall-of-Famer Dan Fouts. After having posted a 1-3 record as a starter, Unitas retired in the preseason of 1974.

Unitas finished his 18 NFL seasons with 2,830 completions in 5,186 attempts for 40,239 yards and 290 touchdowns, with 253 interceptions. He also rushed for 1,777 yards and 13 touchdowns. Plagued by arm trouble in his later seasons, he threw more interceptions (64) than touchdowns (38) in 1968–1973. After averaging 215.8 yards per game in his first twelve seasons, his production fell to 124.4 in his final six. His passer rating plummeted from 82.9 to 60.4 for the same periods. Even so, Unitas set many passing records during his career. He was the first quarterback to throw for more than 40,000 yards, despite playing during an era when NFL teams played shorter seasons of 12 or 14 games (as opposed to today's 16-game seasons) and prior to modern passing-friendly rules implemented in 1978. His 32 touchdown passes in 1959 were a record at the time, making Unitas the first QB to hit the 30 touchdown mark in a season. His 47-game consecutive touchdown streak between 1956 and 1960 was a record considered by many to be unbreakable. The streak stood for 52 years before being broken by New Orleans Saints quarterback Drew Brees in a game against the San Diego Chargers on October 7, 2012.

After his playing days were finished, Unitas settled in Baltimore where he raised his family while also pursuing a career in broadcasting, doing color commentary for NFL games on CBS in the 1970s. He was elected to the Pro Football Hall of Fame in 1979. After Robert Irsay moved the Colts franchise to Indianapolis in 1984, a move reviled to this day in Baltimore as "Bob Irsay's Midnight Ride," Unitas was so outraged that he cut all ties to the relocated team (though his No. 19 jersey is still retired by the Colts), declaring himself strictly a "Baltimore" Colt for the remainder of his life. Some other prominent old-time Colts followed his lead, although many attended the 1975 team's reunion at Lucas Oil Stadium in Indianapolis in 2009. A total of 39 Colts players from that 1975 team attended said reunion in Indianapolis, including Bert Jones and Lydell Mitchell. Unitas asked the Pro Football Hall of Fame on numerous occasions (including on Roy Firestone's "Up Close") to remove his display unless it was listed as belonging to the Baltimore Colts. The Hall of Fame has never complied with the request. Unitas donated his Colts memorabilia to the Babe Ruth Museum in Baltimore; they are now on display in the Sports Legends Museum at Camden Yards.

Unitas was inducted into the American Football Association's Semi Pro Football Hall of Fame in 1987.

Unitas actively lobbied for another NFL team to come to Baltimore. After the Cleveland Browns moved to Baltimore in 1996 and changed their name to the Ravens, Unitas and some of the other old-time Colts attended the Ravens' first game ever against the Raiders on Opening Day at Memorial Stadium. Unitas was frequently seen on the Ravens' sidelines at home games (most prominently in 1998 when the now-Indianapolis Colts played the Ravens) and received a thunderous ovation every time he was pictured on each of the huge widescreens at M&T Bank Stadium. He was often seen on the 30-yard line on the Ravens side. When the NFL celebrated its first 50 years, Unitas was voted the league's best player. Retired Bears quarterback Sid Luckman said of Unitas, "He was better than me, better than Sammy Baugh, better than anyone."

Unitas lived most of the final years of his life severely hobbled. Due to an elbow injury suffered during his playing career, he had only very limited use of his right hand, and could not perform any physical activity more strenuous than golf due to his artificial knees.

"Source":


At the age of 21 on November 20, 1954, Unitas married his high school sweetheart Dorothy Hoelle; they lived in Towson and had five children before divorcing. Unitas' second wife was Sandra Lemon, whom he married on June 26, 1972; they had three children, lived in Baldwin, and remained married until Unitas' death on September 11, 2002.

On August 24, 2013, Unitas was posthumously inducted into the National Lithuanian American Hall of Fame.

On September 11, 2002, Unitas died after suffering a heart attack while working out at the Kernan Physical Therapy Center (now The University of Maryland Rehabilitation & Orthopaedic Institute) in Baltimore. After his death, many fans of the Baltimore Ravens petitioned the renaming of the Ravens' home stadium (owned by the State of Maryland) after Unitas. These requests were unsuccessful since the lucrative naming rights had already been leased by the Ravens to Buffalo-based M&T Bank. However, a statue of Unitas was erected as the centerpiece of the plaza in front of the stadium named in Unitas' honor. Large banners depicting the NFL Hall of Famer in his Baltimore Colts heyday flank the entrance to the stadium. Towson University, where Unitas was a major fund-raiser and which his children attended, named its football and lacrosse complex Johnny Unitas Stadium in recognition of both his football career and service to the university.

Toward the end of his life, Unitas brought media attention to the many permanent physical disabilities that he and his fellow players suffered during their careers before heavy padding and other safety features became popular. Unitas himself lost almost total use of his right hand, with the middle finger and thumb noticeably disfigured from being repeatedly broken during games.

Unitas is buried at Dulaney Valley Memorial Gardens in Timonium, Maryland.






</doc>
<doc id="15645" url="https://en.wikipedia.org/wiki?curid=15645" title="John Jacob Astor">
John Jacob Astor

John Jacob Astor (July 17, 1763 – March 29, 1848) (born Johann Jakob Astor) was a German–American businessman, merchant, real estate mogul and investor who mainly made his fortune in fur trade and by investing in real estate in or around New York City.

Born in Germany, Astor immigrated to England as a teenager and worked as a musical instrument manufacturer. He moved to the United States after the American Revolutionary War. He entered the fur trade and built a monopoly, managing a business empire that extended to the Great Lakes region and Canada, and later expanded into the American West and Pacific coast. Seeing the decline of demand, he got out of the fur trade in 1830, diversifying by investing in New York City real estate and later becoming a famed patron of the arts.

He was the first prominent member of the Astor family and the first multi-millionaire in the United States.

Johann Jakob Astor was born in Walldorf near Heidelberg in Electoral Palatinate. He was the youngest son of Johann Jacob Astor and Maria Magdalena vom Berg. His three elder brothers were George, Henry, and Melchior. Astor's father was a butcher; Johann first worked in his father's shop and as a dairy salesman. In 1779, at the age of 16, he moved to London to join his brother George in working for an uncle's piano and flute manufactory, Astor & Broadwood. While there, he learned English and anglicized his name.

In 1783 or March 1784, Astor immigrated to New York City, just following the end of the American Revolution. There, he rented a room from Sarah Cox Todd, a widow, and began a flirtation with his landlady's daughter, also named Sarah Cox Todd, whom he would marry in 1785. His intent was to join his brother Henry, who had established a butcher shop there, but a chance meeting with a fur trader on his voyage inspired him to join the North American fur trade as well. After working at his brother's shop for a time, he began to purchase raw hides from Native Americans, prepare them himself, and then resell them in London and elsewhere at great profit. He opened his own fur goods shop in New York in the late 1780s and also served as the New York agent of his uncle's musical instrument business.

Astor took advantage of the Jay Treaty between Great Britain and the United States in 1794, which opened new markets in Canada and the Great Lakes region. In London, Astor at once made a contract with the North West Company, who from Montreal rivaled the trade interests of the Hudson's Bay Company, then based in London. 
Astor imported furs from Montreal to New York and shipped them to Europe. By 1800, he had amassed almost a quarter of a million dollars, and had become one of the leading figures in the fur trade. His agents worked throughout the western areas and were ruthless in competition. In 1800, following the example of the "Empress of China", the first American trading vessel to China, Astor traded furs, teas, and sandalwood with Canton in China, and greatly benefited from it.

The U.S. Embargo Act in 1807, however, disrupted Astor's import/export business because it closed off trade with Canada. With the permission of President Thomas Jefferson, Astor established the American Fur Company on April 6, 1808. He later formed subsidiaries: the Pacific Fur Company, and the Southwest Fur Company (in which Canadians had a part), in order to control fur trading in the Great Lakes areas and Columbia River region. His Columbia River trading post at Fort Astoria (established in April 1811) was the first United States community on the Pacific coast. He financed the overland Astor Expedition in 1810–12 to reach the outpost. Members of the expedition were to discover South Pass, through which hundreds of thousands of settlers on the Oregon, Mormon, and California trails passed through the Rocky Mountains.

Astor's fur trading ventures were disrupted during the War of 1812, when the British captured his trading posts. In 1816, he joined the opium-smuggling trade. His American Fur Company purchased ten tons of Turkish opium, then shipped the contraband item to Canton on the packet ship "Macedonian". Astor later left the China opium trade and sold solely to the United Kingdom.

Astor's business rebounded in 1817 after the U.S. Congress passed a protectionist law that barred foreign fur traders from U.S. territories. The American Fur Company came to dominate trading in the area around the Great Lakes. John Jacob Astor had a townhouse at 233 Broadway in Manhattan and a country estate, Hellgate in Northern New York City. In 1822, Astor established the Robert Stuart House on Mackinac Island as headquarters for the reorganized American Fur Company, making the island a metropolis of the fur trade. A lengthy description based on documents, diaries, etc. was given by Washington Irving in his travelogue "Astoria". Astor's commercial connections extended over the entire globe, and his ships were found in every sea. And he and Sarah moved to a townhouse on Prince Street in Manhattan, New York.

Astor began buying land in New York in 1799 and acquired sizable holdings along the waterfront. After the start of the 19th century, flush with China trade profits, he became more systematic, ambitious, and calculating by investing in New York real estate. In 1803, he bought a 70-acre farm that ran west of Broadway to the Hudson River between 42nd and 46th streets. That same year, and the following year, he bought considerable holdings from the disgraced Aaron Burr.
In the 1830s, Astor foresaw that the next big boom would be the build-up of New York, which would soon emerge as one of the world's greatest cities. Astor withdrew from the American Fur Company, as well as all his other ventures, and used the money to buy and develop large tracts of Manhattan real estate. Astor correctly predicted New York's rapid growth northward on Manhattan Island, and he purchased more and more land beyond the then-existing city limits. Astor rarely built on his land, but leased it to others for rent and their use. After retiring from his business, Astor spent the rest of his life as a patron of culture. He supported the ornithologist John James Audubon in his studies, art work, and travels, and the presidential campaign of Henry Clay.

On September 19, 1785, Astor married Sarah Cox Todd (1762–1842), the daughter of Scottish immigrants Adam Todd and Sarah Cox. Although she brought him a dowry of only $300, she possessed a frugal mind and a business judgment that he declared better than that of most merchants. She assisted him in the practical details of his business, and managed Astor's affairs when he was away from New York.

They had eight children:

Astor was a Freemason, and served as Master of Holland Lodge #8, New York City in 1788. Later he served as Grand Treasurer for the Grand Lodge of New York.

At the time of his death in 1848, Astor was the wealthiest person in the United States, leaving an estate estimated to be worth at least $20 million. His estimated net worth would have been equivalent to more than $500 million in 2016 U.S. dollars, making him the fifth-richest person in American history.

In his will, Astor bequeathed $400,000 to build the Astor Library for the New York public, which was later consolidated with other libraries to form the New York Public Library. He also left $50,000 for a poorhouse and orphanage in his German hometown of Walldorf. The "Astorhaus" is now operated as a museum honoring Astor and serves as a renowned fest hall for marriages. Astor donated gifts totaling $20,000 to the German Society of the City of New York, during his term as president, from 1837 until 1841. 

Astor left the bulk of his fortune to his second son William because his eldest son, John Jr., was sickly and mentally unstable. Astor left enough money to care for John Jr. for the rest of his life. Astor is buried in Trinity Church Cemetery in Manhattan, New York. Many members of his family had joined its congregation, but Astor remained a member of the local German Reformed congregation to his death. Herman Melville used Astor as a symbol of men who made the earliest fortunes in New York in his novella, "Bartleby, the Scrivener".

The pair of marble lions that sit by the entrance of the New York Public Library at Fifth Avenue and 42nd Street were originally named Leo Astor and Leo Lenox, after Astor and James Lenox, who founded the library from his own collection. Next, they were called Lord Astor and Lady Lenox (both lions are males). Mayor Fiorello La Guardia renamed them "Patience" and "Fortitude" during the Great Depression.

In 1908, when the association football club FC Astoria Walldorf was formed in Astor's birthplace in Germany, the group added "Astoria" to its name in his, and the family's, honor.





 


</doc>
<doc id="15651" url="https://en.wikipedia.org/wiki?curid=15651" title="Julian calendar">
Julian calendar

The Julian calendar, proposed by Julius Caesar in 46 BC (708 AUC), was a reform of the Roman calendar. It took effect on 1 January 45 BC (AUC 709), by edict. It was the predominant calendar in the Roman world, most of Europe, and in European settlements in the Americas and elsewhere, until it was refined and gradually replaced by the Gregorian calendar, promulgated in 1582 by Pope Gregory XIII. The Julian calendar gains against the mean tropical year at the rate of one day in 128 years. For the Gregorian calendar, the figure is one day in 3,030 years. The difference in the average length of the year between Julian (365.25 days) and Gregorian (365.2425 days) is 0.002%, being 10.8 minutes longer.

The Julian calendar has two types of year: "normal" years of 365 days and "leap" years of 366 days. There is a simple cycle of three "normal" years followed by a leap year and this pattern repeats forever without exception. The Julian year is, therefore, on average 365.25 days long. Consequently the Julian year drifts over time with respect to the tropical (solar) year. Although Greek astronomers had known, at least since Hipparchus, a century before the Julian reform, that the tropical year was slightly shorter than 365.25 days, the calendar did not compensate for this difference. As a result, the calendar year gains about three days every four centuries compared to observed equinox times and the seasons. This discrepancy was corrected by the Gregorian reform of 1582. The Gregorian calendar has the same months and month lengths as the Julian calendar, but, in the Gregorian calendar, year numbers evenly divisible by 100 are not leap years, except that those evenly divisible by 400 remain leap years. This means since 16 February "Julian" (1 March 1900 "Gregorian") and until 15 February "Julian" (28 "February" 2100 "Gregorian") "Julian" is 13 days behind "Gregorian" and the disparity will widen.

The Julian calendar has been replaced as the civil calendar by the Gregorian calendar in all countries which officially used it. An analog, the Alexandrian calendar, is the basis for the Ethiopian calendar, which is the civil calendar of Ethiopia. Egypt converted on 20 December 1874/1 January 1875. Turkey switched (for fiscal purposes) on 16 February/1 March 1917. Russia changed on 1/14 February 1918. Greece made the change for civil purposes on 16 February/1 March 1923, but the next national day (25 March)—a religious holiday—took place as if on the old calendar. Most Christian denominations in the west and areas evangelised by western churches have made the same change for their liturgical calendars.

Most branches of the Eastern Orthodox Church use the Julian calendar for calculating the date of Easter, upon which the timing of all the other moveable feasts depends. Some such churches have adopted the Revised Julian calendar for the observance of fixed feasts, while such Orthodox churches retain the Julian calendar for all purposes. The Julian calendar is still used by the Berbers of the Maghreb in the form of the Berber calendar, and on Mount Athos.

During the changeover between calendars and for some time afterwards, dual dating was used in documents and gave the date according to both systems. In contemporary as well as modern texts that describe events during the period of change, it is customary to clarify to which calendar a given date refers by using an O.S. or N.S. suffix (denoting Old Style, Julian or New Style, Gregorian).

The ordinary year in the previous Roman calendar consisted of 12 months, for a total of 355 days. In addition, a 27- or 28-day intercalary month, the Mensis Intercalaris, was sometimes inserted between February and March. This intercalary month was formed by inserting 22 or 23 days after the first 23 days of February; the last five days of February, which counted down toward the start of March, became the last five days of Intercalaris. The net effect was to add 22 or 23 days to the year, forming an intercalary year of 377 or 378 days. Some say the "mensis intercalaris" always had 27 days and began on either the first or the second day after the Terminalia (23 February).

According to the later writers Censorinus and Macrobius, the ideal intercalary cycle consisted of ordinary years of 355 days alternating with intercalary years, alternately 377 and 378 days long. In this system, the average Roman year would have had days over four years, giving it an average drift of one day per year relative to any solstice or equinox. Macrobius describes a further refinement whereby, in one 8-year period within a 24-year cycle, there were only three intercalary years, each of 377 days (thus 11 intercalary years out of 24). This refinement averages the length of the year to 365.25 days over 24 years.

In practice, intercalations did not occur systematically according to any of these ideal systems, but were determined by the pontifices. So far as can be determined from the historical evidence, they were much less regular than these ideal schemes suggest. They usually occurred every second or third year, but were sometimes omitted for much longer, and occasionally occurred in two consecutive years.

If managed correctly this system could have allowed the Roman year to stay roughly aligned to a tropical year. However, since the pontifices were often politicians, and because a Roman magistrate's term of office corresponded with a calendar year, this power was prone to abuse: a pontifex could lengthen a year in which he or one of his political allies was in office, or refuse to lengthen one in which his opponents were in power.

If too many intercalations were omitted, as happened after the Second Punic War and during the Civil Wars, the calendar would drift out of alignment with the tropical year. Moreover, because intercalations were often determined quite late, the average Roman citizen often did not know the date, particularly if he were some distance from the city. For these reasons, the last years of the pre-Julian calendar were later known as "years of confusion". The problems became particularly acute during the years of Julius Caesar's pontificate before the reform, 63–46 BC, when there were only five intercalary months (instead of eight), none of which were during the five Roman years before 46 BC.

Caesar's reform was intended to solve this problem permanently, by creating a calendar that remained aligned to the sun without any human intervention. This proved useful very soon after the new calendar came into effect. Varro used it in 37 BC to fix calendar dates for the start of the four seasons, which would have been impossible only 8 years earlier. A century later, when Pliny dated the winter solstice to 25 December because the sun entered the 8th degree of Capricorn on that date, this stability had become an ordinary fact of life.

Although the approximation of days for the tropical year had been known for a long time ancient solar calendars had used less precise periods, resulting in gradual misalignment of the calendar with the seasons.

The octaeteris, a cycle of 8 lunar years popularised by Cleostratus (and also commonly attributed to Eudoxus) which was used in some early Greek calendars, notably in Athens, is 1.53 days longer than eight Julian years. The length of nineteen years in the cycle of Meton was 6,940 days, six hours longer than the mean Julian year. The mean Julian year was the basis of the 76-year cycle devised by Callippus (a student under Eudoxus) to improve the Metonic cycle.

In Persia (Iran) after the reform in the Persian calendar by introduction of the Persian Zoroastrian (i. e. Young Avestan) calendar in 503 BC and afterwards, the first day of the year (1 Farvardin=Nowruz) slipped against the vernal equinox at the rate of approximately one day every four years.

Likewise in the Egyptian calendar, a fixed year of 365 days was in use, drifting by one day against the sun in four years. An unsuccessful attempt to add an extra day every fourth year was made in 238 BC (Decree of Canopus). Caesar probably experienced this "wandering" or "vague" calendar in that country. He landed in the Nile delta in October 48 BC and soon became embroiled in the Ptolemaic dynastic war, especially after Cleopatra managed to be "introduced" to him in Alexandria.

Caesar imposed a peace, and a banquet was held to celebrate the event. Lucan depicted Caesar talking to a wise man called Acoreus during the feast, stating his intention to create a calendar more perfect than that of Eudoxus (Eudoxus was popularly credited with having determined the length of the year to be days). But the war soon resumed and Caesar was attacked by the Egyptian army for several months until he achieved victory. He then enjoyed a long cruise on the Nile with Cleopatra before leaving the country in June 47 BC.

Caesar returned to Rome in 46 BC and, according to Plutarch, called in the best philosophers and mathematicians of his time to solve the problem of the calendar. Pliny says that Caesar was aided in his reform by the astronomer Sosigenes of Alexandria who is generally considered the principal designer of the reform. Sosigenes may also have been the author of the astronomical almanac published by Caesar to facilitate the reform. Eventually, it was decided to establish a calendar that would be a combination between the old Roman months, the fixed length of the Egyptian calendar, and the days of the Greek astronomy. According to Macrobius, Caesar was assisted in this by a certain Marcus Flavius.

The first step of the reform was to realign the start of the calendar year (1 January) to the tropical year by making 46 BC (708 AUC) 445 days long, compensating for the intercalations which had been missed during Caesar's pontificate. This year had already been extended from 355 to 378 days by the insertion of a regular intercalary month in February. When Caesar decreed the reform, probably shortly after his return from the African campaign in late Quintilis (July), he added 67 more days by inserting two extraordinary intercalary months between November and December.

These months are called "Intercalaris Prior" and "Intercalaris Posterior" in letters of Cicero written at the time; there is no basis for the statement sometimes seen that they were called "Undecimber" and "Duodecimber", terms that arose in the 18th century over a millennium after the Roman Empire's collapse. Their individual lengths are unknown, as is the position of the Nones and Ides within them.

Because 46 BC was the last of a series of irregular years, this extra-long year was, and is, referred to as the "last year of confusion". The new calendar began operation after the realignment had been completed, in 45 BC.

The Julian months were formed by adding ten days to a regular pre-Julian Roman year of 355 days, creating a regular Julian year of 365 days. Two extra days were added to January, Sextilis (August) and December, and one extra day was added to April, June, September and November. February was not changed in ordinary years, and so continued to be the traditional 28 days. Thus, the ordinary (i.e., non-leap year) lengths of all of the months were set by the Julian calendar to the same values they still hold today. (See Sacrobosco's theory on month lengths below for stories purporting otherwise.)

The Julian reform did not change the method used to account days of the month in the pre-Julian calendar, based on the Kalends, Nones and Ides, nor did it change the positions of these three dates within the months. Macrobius states that the extra days were added immediately before the last day of each month to avoid disturbing the position of the established religious ceremonies relative to the Nones and Ides of the month. However, since Roman dates after the Ides of the month counted down toward the start of the next month, the extra days had the effect of raising the initial value of the count of the day following the Ides in the lengthened months. Thus, in January, Sextilis and December the 14th day of the month became a.d. XIX Kal. instead of a.d. XVII Kal., while in April, June, September and November it became a.d. XVIII Kal.

Romans of the time born after the Ides of a month responded differently to the effect of this change on their birthdays. Mark Antony kept his birthday on 14 January, which changed its date from a.d. XVII Kal. Feb to a.d. XIX Kal. Feb, a date that had previously not existed. Livia kept the date of her birthday unchanged at a.d. III Kal. Feb., which moved it from 28 to 30 January, a day that had previously not existed. Augustus kept his on 23 September, but both the old date (a.d. VIII Kal. Oct.) and the new (a.d. IX Kal. Oct.) were celebrated in some places.

The inserted days were all initially characterised as "dies fasti" (F – see Roman calendar). The character of a few festival days was changed. In the early Julio-Claudian period a large number of festivals were decreed to celebrate events of dynastic importance, which caused the character of the associated dates to be changed to NP. However, this practice was discontinued around the reign of Claudius, and the practice of characterising days fell into disuse around the end of the first century AD: the Antonine jurist Gaius speaks of "dies nefasti" as a thing of the past.

The old intercalary month was abolished. The new leap day was dated as "ante diem bis sextum Kalendas Martias" ('the sixth doubled day before the Kalends of March'), usually abbreviated as "a.d. bis VI Kal. Mart."; hence it is called in English the bissextile day. The year in which it occurred was termed "annus bissextus", in English the bissextile year.

There is debate about the exact position of the bissextile day in the early Julian calendar. The earliest direct evidence is a statement of the 2nd century jurist Celsus, who states that there were two halves of a 48-hour day, and that the intercalated day was the "posterior" half. An inscription from AD 168 states that "a.d. V Kal. Mart." was the day after the bissextile day. The 19th century chronologist Ideler argued that Celsus used the term "posterior" in a technical fashion to refer to the earlier of the two days, which requires the inscription to refer to the whole 48-hour day as the bissextile. Some later historians share this view. Others, following Mommsen, take the view that Celsus was using the ordinary Latin (and English) meaning of "posterior". A third view is that neither half of the 48-hour "bis sextum" was originally formally designated as intercalated, but that the need to do so arose as the concept of a 48-hour day became obsolete.

There is no doubt that the bissextile day eventually became the earlier of the two days for most purposes. In 238 Censorinus stated that it was inserted after the Terminalia (23 February) and was followed by the last five days of February, i.e., a.d. VI, V, IV, III and prid. Kal. Mart. (which would be 24 to 28 February in a common year and the 25th to 29th in a leap year). Hence he regarded the bissextum as the first half of the doubled day. All later writers, including Macrobius about 430, Bede in 725, and other medieval computists (calculators of Easter) followed this rule, as does the liturgical calendar of the Roman Catholic Church. However, Celsus' definition continued to be used for legal purposes. It was incorporated into Justinian's Digest, and in the English statute "De anno et die bissextili" of 1236, which was not formally repealed until 1879.

The effect of the bissextile day on the nundinal cycle is not discussed in the sources. According to Dio Cassius, a leap day was inserted in 41 BC to ensure that the first market day of 40 BC did not fall on 1 January, which implies that the old 8-day cycle was not immediately affected by the Julian reform. However, he also reports that in AD 44, and on some previous occasions, the market day was changed to avoid a conflict with a religious festival. This may indicate that a single nundinal letter was assigned to both halves of the 48-hour bissextile day by this time, so that the Regifugium and the market day might fall on the same date but on different days. In any case, the 8-day nundinal cycle began to be displaced by the 7-day week in the first century AD, and dominical letters began to appear alongside nundinal letters in the fasti.

During the late Middle Ages days in the month came to be numbered in consecutive day order. Consequently, the leap day was considered to be the last day in February in leap years, i.e., 29 February, which is its current position.

The Julian reform set the lengths of the months to their modern values. However, a 13th-century scholar, Sacrobosco, proposed a different explanation for the lengths of Julian months which is still widely repeated but is certainly wrong.

According to Sacrobosco, the month lengths for ordinary years in the Roman Republican calendar, from January to December, were:

Sacrobosco then thought that Julius Caesar added one day to every month except February, a total of 11 more days to regular months, giving the ordinary Julian year of 365 days. A single leap day could now be added to this extra short February:

He then said Augustus changed this, by taking one day from February to add it to Sextilis, and then modifying the alternation of the following months, to:

so that the length of "Augustus" (August) would not be shorter than (and therefore inferior to) the length of "Iulius" (July), giving us the irregular month lengths which are still in use.

There is abundant evidence disproving this theory. First, a wall painting of a Roman calendar predating the Julian reform has survived, which confirms the literary accounts that the months were already irregular before Julius Caesar reformed them, with an ordinary year of 355 days, not 354, with month lengths arranged as:

Also, the Julian reform did not change the dates of the Nones and Ides. In particular, the Ides were late (on the 15th rather than 13th) in March, May, July and October, showing that these months always had 31 days in the Roman calendar, whereas Sacrobosco's theory requires that March, May and July were originally 30 days long and that the length of October was changed from 29 to 30 days by Caesar and to 31 days by Augustus. Further, Sacrobosco's theory is explicitly contradicted by the 3rd and 5th century authors Censorinus and Macrobius, and it is inconsistent with seasonal lengths given by Varro, writing in 37 BC, before Sextilis was renamed for Augustus in 8 BC, with the 31-day Sextilis given by an Egyptian papyrus from 24 BC, and with the 28-day February shown in the "Fasti Caeretani", which is dated before 12 BC.

Caesar's reform only applied to the Roman calendar. However, in the following decades many of the local civic and provincial calendars of the empire and neighbouring client kingdoms were aligned to the Julian calendar by transforming them into calendars with years of 365 days with an extra day intercalated every four years. The reformed calendars typically retained many features of the unreformed calendars. In many cases, the New Year was not on 1 January, the leap day was not on the bissextile day, the old month names were retained, the lengths of the reformed months did not match the lengths of Julian months, and, even if they did, their first days did not match the first day of the corresponding Julian month. Nevertheless, since the reformed calendars had fixed relationships to each other and to the Julian calendar, the process of converting dates between them became quite straightforward, through the use of conversion tables known as "hemerologia". Several of the reformed calendars are only known through surviving hemerologia.

The three most important of these calendars are the Alexandrian calendar, the Asian calendar and the Syro-Macedonian calendar. Other reformed calendars are known from Cappadocia, Cyprus and the cities of Syria and Palestine. Most reformed calendars were adopted under Augustus, though the calendar of Nabatea was reformed after the kingdom became the Roman province of Arabia in AD 106. There is no evidence that local calendars were aligned to the Julian calendar in the western empire. Unreformed calendars continued to be used in Gaul, Greece, Macedon, the Balkans and parts of Palestine, most notably in Judea.

The Alexandrian calendar adapted the Egyptian calendar by adding a 6th epagomenal day as the last day of the year in every fourth year, falling on 29 August preceding a Julian bissextile day. It was otherwise identical to the Egyptian calendar. The first leap day was in 22 BC, and they occurred every four years from the beginning, even though Roman leap days occurred every three years at this time (see Leap year error). This calendar influenced the structure of several other reformed calendars, such as those of the cities of Gaza and Ascalon in Palestine, Salamis in Cyprus, and the province of Arabia. It was adopted by the Coptic church and remains in use both as the liturgical calendar of the Coptic church and as the civil calendar of Ethiopia.

The Asian calendar was an adaptation of the Macedonian calendar used in the province of Asia and, with minor variations, in nearby cities and provinces. It is known in detail through the survival of decrees promulgating it issued in 8 BC by the proconsul Paullus Fabius Maximus. It renamed the first month Dios as "Kaisar", and arranged the months such that each month started on the ninth day before the kalends of the corresponding Roman month; thus the year began on 23 September, Augustus' birthday. Since Greek months typically had 29 or 30 days, the extra day of 31-day months was named "Sebaste"—the emperor's day—and was the first day of these months. The leap day was a second Sebaste day in the month of Xandikos, i.e., 24 February. This calendar remained in use at least until the middle of the fifth century AD.

The Syro-Macedonian calendar was an adaptation of the Macedonian calendar used in Antioch and other parts of Syria. The months were exactly aligned to the Julian calendar, but they retained their Macedonian names and the year began in Dios = November until the fifth century, when the start of the year was moved to Gorpiaios = September.

These reformed calendars generally remained in use until the fifth or sixth century. Around that time most of them were replaced as civil calendars by the Julian calendar, but with a year starting in September to reflect the year of the indiction cycle.

The Julian calendar spread beyond the borders of the Roman Empire through its use as the Christian liturgical calendar. When a people or a country was converted to Christianity, they generally also adopted the Christian calendar of the church responsible for conversion. Thus, Christian Nubia and Ethiopia adopted the Alexandrian calendar, while Christian Europe adopted the Julian calendar, in either the Catholic or Orthodox variant. Starting in the 16th century, European settlements in the Americas and elsewhere likewise inherited the Julian calendar of the mother country, until they adopted the Gregorian reform. The last country to adopt the Julian calendar was the Ottoman Empire, which used it for financial purposes for some time under the name Rumi calendar and dropped the "escape years" which tied it to Muslim chronology in 1840.

Although the new calendar was much simpler than the pre-Julian calendar, the pontifices initially added a leap day every three years, instead of every four. There are accounts of this in Solinus, Pliny, Ammianus, Suetonius, and Censorinus.

Macrobius gives the following account of the introduction of the Julian calendar:

Caesar’s regulation of the civil year to accord with his revised measurement was proclaimed publicly by edict, and the arrangement might have continued to stand had not the correction itself of the calendar led the priests to introduce a new error of their own; for they proceeded to insert the intercalary day, which represented the four quarter-days, at the beginning of each fourth year instead of at its end, although the intercalation ought to have been made at the end of each fourth year and before the beginning of the fifth.

This error continued for thirty-six years by which time twelve intercalary days had been inserted instead of the number actually due, namely nine. But when this error was at length recognised, it too was corrected, by an order of Augustus, that twelve years should be allowed to pass without an intercalary day, since the sequence of twelve such years would account for the three days which, in the course of thirty-six years, had been introduced by the premature actions of the priests.

So, according to Macrobius,

Some people have had different ideas as to how the leap years went. The above scheme is that of Scaliger in the table below. He established that the Augustan reform was instituted in 8 BC. The table shows for each reconstruction the implied proleptic Julian date for the first day of Caesar's reformed calendar (Kal. Ian. AUC 709) and the first Julian date on which the Roman calendar date matches the Julian calendar after the completion of Augustus' reform.

Alexander Jones claims that the correct Julian calendar was in use in Egypt in 24 BC, implying that the first day of the reform in both Egypt and Rome, , was the Julian date 1 January if 45 BC was a leap year and 2 January if it was not. This necessitates fourteen leap days up to and including AD 8 if 45 BC was a leap year and thirteen if it was not.

Pierre Brind'Amour argued that "only one day was intercalated between 1/1/45 and 1/1/40 (disregarding a momentary 'fiddling' in December of 41 to avoid the nundinum falling on Kal. Ian."

In 1999, a papyrus was discovered which gives the dates of astronomical phenomena in 24 BC in both the Egyptian and Roman calendars. From , Egypt had two calendars: the old Egyptian in which every year had 365 days and the new Alexandrian in which every fourth year had 366 days. Up to the date in both calendars was the same. The dates in the Alexandrian and Julian calendars are in one-to-one correspondence except for the period from 29 August in the year preceding a Julian leap year to the following 24 February. From a comparison of the astronomical data with the Egyptian and Roman dates, Alexander Jones concluded that the Egyptian astronomers (as opposed to travellers from Rome) used the correct Julian calendar.

An inscription has been discovered which orders a new calendar to be used in Asia to replace the previous Greek lunar calendar. According to one translation
Intercalation shall commence on the day after 14 Peritius [a.d. IX Kal. Feb, which would have been 15 Peritius] as it is currently constituted in the third year following promulgation of the decree. Xanthicus shall have 32 days in this intercalary year.
This is historically correct. It was decreed by the proconsul that the first day of the year in the new calendar shall be Augustus' birthday, a.d. IX Kal. Oct. Every month begins on the ninth day before the kalends. The date of introduction, the day after 14 Peritius, was 1 Dystrus, the next month. The month after that was Xanthicus. Thus Xanthicus began on a.d. IX Kal. Mart., and normally contained 31 days. In leap year, however, it contained an extra "Sebaste day", the Roman leap day, and thus had 32 days. From the lunar nature of the old calendar we can fix the starting date of the new one as 24 January, in the Julian calendar, which was a leap year. Thus from inception the dates of the reformed Asian calendar are in one-to-one correspondence with the Julian.

Another translation of this inscription is
Intercalation shall commence on the day after the fourteenth day in the current month of Peritius [a.d. IX Kal. Feb], occurring every third year. Xanthicus shall have 32 days in this intercalary year.
This would move the starting date back three years to 8 BC, and from the lunar synchronism back to 26 January (Julian). But since the corresponding Roman date in the inscription is 24 January, this must be according to the incorrect calendar which in 8 BC Augustus had ordered to be corrected by the omission of leap days. As the authors of the previous paper point out, with the correct four-year cycle being used in Egypt and the three-year cycle abolished in Rome it is unlikely that Augustus would have ordered the three-year cycle to be introduced in Asia.

The Julian reform did not immediately cause the names of any months to be changed. The old intercalary month was abolished and replaced with a single intercalary day at the same point (i.e., five days before the end of February). January continued to be the first month of the year.

The Romans later renamed months after Julius Caesar and Augustus, renaming Quintilis as "Iulius" (July) in 44 BC and Sextilis as "Augustus" (August) in 8 BC. Quintilis was renamed to honour Caesar because it was the month of his birth. According to a "senatus consultum" quoted by Macrobius, Sextilis was renamed to honour Augustus because several of the most significant events in his rise to power, culminating in the fall of Alexandria, occurred in that month.

Other months were renamed by other emperors, but apparently none of the later changes survived their deaths. In AD 37, Caligula renamed September as "Germanicus" after his father; in AD 65, Nero renamed April as "Neroneus", May as "Claudius" and June as "Germanicus"; and in AD 84 Domitian renamed September as "Germanicus" and October as "Domitianus". Commodus was unique in renaming all twelve months after his own adopted names (January to December): "Amazonius", "Invictus", "Felix", "Pius", "Lucius", "Aelius", "Aurelius", "Commodus", "Augustus", "Herculeus", "Romanus", and "Exsuperatorius". The emperor Tacitus is said to have ordered that September, the month of his birth and accession, be renamed after him, but the story is doubtful since he did not become emperor before November 275. Similar honorific month names were implemented in many of the provincial calendars that were aligned to the Julian calendar.

Other name changes were proposed but were never implemented. Tiberius rejected a senatorial proposal to rename September as "Tiberius" and October as "Livius", after his mother Livia. Antoninus Pius rejected a senatorial decree renaming September as "Antoninus" and November as "Faustina", after his empress.

Much more lasting than the ephemeral month names of the post-Augustan Roman emperors were the Old High German names introduced by Charlemagne. According to his biographer, Charlemagne renamed all of the months agriculturally into German. These names were used until the 15th century, over 700 years after his rule, and continued, with some modifications, to see some use as "traditional" month names until the late 18th century. The names (January to December) were: "Wintarmanoth" ("winter month"), "Hornung", "Lentzinmanoth" ("spring month", "Lent month"), "Ostarmanoth" ("Easter month"), "Wonnemanoth" ("joy-month", a corruption of "Winnimanoth" "pasture-month"), "Brachmanoth" ("fallow-month"), "Heuuimanoth" ("hay month"), "Aranmanoth" ("reaping month"), "Witumanoth" ("wood month"), "Windumemanoth" ("vintage month"), "Herbistmanoth" ("harvest month"), and "Heilagmanoth" ("holy month").

The calendar month names used in western and northern Europe, in Byzantium, and by the Berbers, were derived from the Latin names. However, in eastern Europe older seasonal month names continued to be used into the 19th century, and in some cases are still in use, in many languages, including: Belarusian, Bulgarian, Croatian, Czech, Finnish, Georgian, Lithuanian, Macedonian, Polish, Romanian, Slovene, Ukrainian. When the Ottoman Empire adopted the Julian calendar, in the form of the Rumi calendar, the month names reflected Ottoman tradition.

The principal method used by the Romans to identify a year for dating purposes was to name it after the two consuls who took office in it, the eponymous period in question being the consular year. Beginning in 153 BC, consuls began to take office on 1 January, thus synchronizing the commencement of the consular and calendar years. The calendar year has begun in January and ended in December since about 450 BC according to Ovid or since about 713 BC according to Macrobius and Plutarch (see Roman calendar). Julius Caesar did not change the beginning of either the consular year or the calendar year. In addition to consular years, the Romans sometimes used the regnal year of the emperor, and by the late 4th century documents were also being dated according to the 15-year cycle of the indiction. In 537, Justinian required that henceforth the date must include the name of the emperor and his regnal year, in addition to the indiction and the consul, while also allowing the use of local eras.

In 309 and 310, and from time to time thereafter, no consuls were appointed. When this happened, the consular date was given a count of years since the last consul (so-called "post-consular" dating). After 541, only the reigning emperor held the consulate, typically for only one year in his reign, and so post-consular dating became the norm. Similar post-consular dates were also known in the west in the early 6th century. The system of consular dating, long obsolete, was formally abolished in the law code of Leo VI, issued in 888.

Only rarely did the Romans number the year from the founding of the city (of Rome), "ab urbe condita" (AUC). This method was used by Roman historians to determine the number of years from one event to another, not to date a year. Different historians had several different dates for the founding. The "Fasti Capitolini", an inscription containing an official list of the consuls which was published by Augustus, used an epoch of 752 BC. The epoch used by Varro, 753 BC, has been adopted by modern historians. Indeed, Renaissance editors often added it to the manuscripts that they published, giving the false impression that the Romans numbered their years. Most modern historians tacitly assume that it began on the day the consuls took office, and ancient documents such as the "Fasti Capitolini" which use other AUC systems do so in the same way. However, Censorinus, writing in the 3rd century AD, states that, in his time, the AUC year began with the Parilia, celebrated on 21 April, which was regarded as the actual anniversary of the foundation of Rome.

Many local eras, such as the Era of Actium and the Spanish Era, were adopted for the Julian calendar or its local equivalent in the provinces and cities of the Roman Empire. Some of these were used for a considerable time. Perhaps the best known is the Era of Martyrs, sometimes also called "Anno Diocletiani" (after Diocletian), which was associated with the Alexandrian calendar and often used by the Alexandrian Christians to number their Easters during the 4th and 5th centuries, and continues to be used by the Coptic and Ethiopian churches.

In the eastern Mediterranean, the efforts of Christian chronographers such as Annianus of Alexandria to date the Biblical creation of the world led to the introduction of Anno Mundi eras based on this event. The most important of these was the Etos Kosmou, used throughout the Byzantine world from the 10th century and in Russia until 1700. In the west, the kingdoms succeeding the empire initially used indictions and regnal years, alone or in combination. The chronicler Prosper of Aquitaine, in the fifth century, used an era dated from the Passion of Christ, but this era was not widely adopted. Dionysius Exiguus proposed the system of Anno Domini in 525. This era gradually spread through the western Christian world, once the system was adopted by Bede in the eighth century.

The Julian calendar was also used in some Muslim countries. The Rumi calendar, the Julian calendar used in the later years of the Ottoman Empire, adopted an era derived from the lunar AH year equivalent to AD 1840, i.e., the effective Rumi epoch was AD 585. In recent years, some users of the Berber calendar have adopted an era starting in 950 BC, the approximate date that the Libyan pharaoh Sheshonq I came to power in Egypt.

The Roman calendar began the year on 1 January, and this remained the start of the year after the Julian reform. However, even after local calendars were aligned to the Julian calendar, they started the new year on different dates. The Alexandrian calendar in Egypt started on 29 August (30 August after an Alexandrian leap year). Several local provincial calendars were aligned to start on the birthday of Augustus, 23 September. The indiction caused the Byzantine year, which used the Julian calendar, to begin on 1 September; this date is still used in the Eastern Orthodox Church for the beginning of the liturgical year. When the Julian calendar was adopted in AD 988 by Vladimir I of Kiev, the year was numbered Anno Mundi 6496, beginning on 1 March, six months after the start of the Byzantine Anno Mundi year with the same number. In 1492 (AM 7000), Ivan III, according to church tradition, realigned the start of the year to 1 September, so that AM 7000 only lasted for six months in Russia, from 1 March to 31 August 1492.

During the Middle Ages 1 January retained the name "New Year's Day" (or an equivalent name) in all western European countries (affiliated with the Roman Catholic Church), since the medieval calendar continued to display the months from January to December (in twelve columns containing 28 to 31 days each), just as the Romans had. However, most of those countries began their numbered year on 25 December (the Nativity of Jesus), 25 March (the Incarnation of Jesus), or even Easter, as in France (see the Liturgical year article for more details).

In Anglo-Saxon England, the year most commonly began on 25 December, which, as (approximately) the winter solstice, had marked the start of the year in pagan times, though 25 March (the equinox) is occasionally documented in the 11th century. Sometimes the start of the year was reckoned as 24 September, the start of the so-called "western indiction" introduced by Bede. These practices changed after the Norman conquest. From 1087 to 1155 the English year began on 1 January, and from 1155 to 1751 began on 25 March. In 1752 it was moved back to 1 January. (See Calendar (New Style) Act 1750).

Even before 1752, 1 January was sometimes treated as the start of the new year – for example by Pepys – while the "year starting 25th March was called the Civil or Legal Year". To reduce misunderstandings on the date, it was not uncommon for a date between 1 January and 24 March to be written as "1661/62". This was to explain to the reader that the year was 1661 counting from March and 1662 counting from January as the start of the year. (For more detail, see Dual dating).

Most western European countries shifted the first day of their numbered year to 1 January while they were still using the Julian calendar, "before" they adopted the Gregorian calendar, many during the 16th century. The following table shows the years in which various countries adopted 1 January as the start of the year. Eastern European countries, with populations showing allegiance to the Orthodox Church, began the year on 1 September from about 988. The Rumi calendar used in the Ottoman Empire began the civil year on 1 March until 1918.

The Julian calendar was in general use in Europe and northern Africa until 1582, when Pope Gregory XIII promulgated the Gregorian calendar. Reform was required because too many leap days are added with respect to the astronomical seasons on the Julian scheme. On average, the astronomical solstices and the equinoxes advance by 10.8 minutes per year against the Julian year. As a result, the calculated date of Easter gradually moved out of alignment with the March equinox.

While Hipparchus and presumably Sosigenes were aware of the discrepancy, although not of its correct value, it was evidently felt to be of little importance at the time of the Julian reform. However, it accumulated significantly over time: the Julian calendar gained a day about every 134 years. By 1582, it was ten days out of alignment from where it supposedly had been in 325 during the Council of Nicaea.

The Gregorian calendar was soon adopted by most Catholic countries (e.g., Spain, Portugal, Poland, most of Italy). Protestant countries followed later, and some countries of eastern Europe even later. In the British Empire (including the American colonies), Wednesday was followed by Thursday . For 12 years from 1700 Sweden used a modified Julian calendar, and adopted the Gregorian calendar in 1753.

Since the Julian and Gregorian calendars were long used simultaneously, although in different places, calendar dates in the transition period are often ambiguous, unless it is specified which calendar was being used. In some circumstances, double dates might be used, one in each calendar. The notation "Old Style" (O.S.) is sometimes used to indicate a date in the Julian calendar, as opposed to "New Style" (N.S.), which either represents the Julian date with the start of the year as 1 January or a full mapping onto the Gregorian calendar. This notation is used to clarify dates from countries which continued to use the Julian calendar after the Gregorian reform, such as Great Britain, which did not switch to the reformed calendar until 1752, or Russia, which did not switch until 1918. This is why the Russian Revolution of 7 November 1917 N.S. is known as the October Revolution, because it began on 25 October OS.

Throughout the long transition period, the Julian calendar has continued to diverge from the Gregorian. This has happened in whole-day steps, as leap days which were dropped in certain centennial years in the Gregorian calendar continued to be present in the Julian calendar. Thus, in the year 1700 the difference increased to 11 days; in 1800, 12; and in 1900, 13. Since 2000 was a leap year according to both the Julian and Gregorian calendars, the difference of 13 days did not change in that year: (Gregorian) fell on (Julian). This difference will persist until the last day of February 2100 (Gregorian), since 2100 is "not" a Gregorian leap year, but "is" a Julian leap year. Monday (Gregorian) falls on Monday (Julian).

Although most Eastern Orthodox countries (most of them in eastern or southeastern Europe) had adopted the Gregorian calendar by 1924, their national churches had not. The "Revised Julian calendar" was endorsed by a synod in Constantinople in May 1923, consisting of a solar part which was and will be identical to the Gregorian calendar until the year 2800, and a lunar part which calculated Easter astronomically at Jerusalem. All Orthodox churches refused to accept the lunar part, so almost all Orthodox churches continue to celebrate Easter according to the Julian calendar (with the exception of the Estonian Orthodox Church
and the Finnish Orthodox Church).

The solar part of the Revised Julian calendar was accepted by only some Orthodox churches. Those that did accept it, with hope for improved dialogue and negotiations with the western denominations, were the Ecumenical Patriarchate of Constantinople, the Patriarchates of Alexandria, Antioch, the Orthodox Churches of Greece, Cyprus, Romania, Poland (from 1924 to 2014; it is still permitted to use the Revised Julian calendar in parishes that want it), Bulgaria (in 1963), and the Orthodox Church in America (although some OCA parishes are permitted to use the Julian calendar). Thus these churches celebrate the Nativity on the same day that western Christians do, 25 December Gregorian until 2799.

The Orthodox Churches of Jerusalem, Russia, Serbia, Montenegro, Poland (from 15 June 2014), Macedonia, Georgia, Ukraine, and the Greek Old Calendarists and other groups continue to use the Julian calendar, thus they celebrate the Nativity on 25 December Julian (which is 7 January Gregorian until 2100). The Russian Orthodox Church has some parishes in the west which celebrate the Nativity on 25 December Gregorian until 2799.

Parishes of the Orthodox Church in America Bulgarian Diocese, both before and after the 1976 transfer of that diocese from the Russian Orthodox Church Outside Russia to the Orthodox Church in America, were permitted to use this date. Some Old Calendarist groups which stand in opposition to the state churches of their homelands will use the Great Feast of the Theophany (6 January Julian/19 January Gregorian) as a day for religious processions and the Great Blessing of Waters, to publicise their cause.

The Oriental Orthodox Churches generally use the local calendar of their homelands. However, when calculating the Nativity Feast, most observe the Julian calendar. This was traditionally for the sake of unity throughout Christendom. In the west, some Oriental Orthodox Churches either use the Gregorian calendar or are permitted to observe the Nativity according to it. The Armenian Apostolic Orthodox Church celebrates the Nativity as part of the Feast of Theophany according to its traditional calendar.





</doc>
