<doc id="18761" url="https://en.wikipedia.org/wiki?curid=18761" title="Load (album)">
Load (album)

Load is the sixth studio album by the American heavy metal band Metallica, released on June 4, 1996 by Elektra Records in the United States and by Vertigo Records internationally. The album showed more of a hard rock side of Metallica than the band's typical thrash metal style, which alienated much of the band's fanbase. It also featured influences from genres such as Southern rock, blues rock, country rock and alternative rock. Drummer Lars Ulrich said about "Load" more exploratory nature, "This album and what we're doing with it – that, to me, is what Metallica are all about: exploring different things. The minute you stop exploring, then just sit down and fucking die". At 79 minutes, it is Metallica's longest studio album.

"Load" debuted and spent four consecutive weeks at number one on the US "Billboard" 200 chart. "Load" sold 680,000 units in its first week, making it the biggest opening week for Metallica as well as the biggest debut of 1996. It was certified 5× platinum by the Recording Industry Association of America (RIAA) for shipping five million copies in the United States. Four singles—"Until It Sleeps", "Hero of the Day", "Mama Said", and "King Nothing"—were released as part of the marketing campaign for the album.

"Load", released approximately five years after the commercially successful album "Metallica", saw the band shifting toward hard rock and away from their thrash metal roots. As on previous releases, the album's fourteen songs began as rough demos created by principal songwriters James Hetfield and Lars Ulrich in Ulrich's basement recording studio, "The Dungeon". In early 1995, the band took over thirty demos into The Plant Studios, where they would work for approximately one year. Metallica worked with producer Bob Rock, who had been at the helm during the recording process for "Metallica".

The songwriting dispenses almost entirely with the thrash metal style that characterized the band's sound in the 1980s. In place of staccato riffs, Hetfield and lead guitarist Kirk Hammett experimented with blues rock-based tones and styles. Additionally, Ulrich adopted a minimalist approach to his drum recording, abandoning the speed and complex double bass drumming patterns of previous albums, and using simpler techniques and playing styles. Hetfield displayed a lyrical evolution, writing what many said were his most personal and introspective lyrics. "Until It Sleeps", the album's lead single, addressed his mother's unsuccessful battle with cancer, and "Mama Said" also explores his relationship with her. All of this marked a departure from the political and social overtones of "...And Justice for All" and "Master of Puppets".

At 79 minutes, "Load" is Metallica's longest studio album. With the CD length at 78:59, initial pressings of the album were affixed with stickers boasting of its long playtime, simply reading "78:59". "The Outlaw Torn" had to be shortened by about one minute to fit on the album; the full version of the track was released on the single "The Memory Remains" as "The Outlaw Torn (Unencumbered by Manufacturing Restrictions Version)", with a running time of 10:48. An explanation on the single's back cover stated:

"Load" was Metallica's first album on which all tracks were down-tuned to E♭ tuning. Hammett states:

The band had recorded songs on earlier albums in tunings lower than E; "The God That Failed" on "Metallica" which was in E♭, and the same album's "Sad but True" and "The Thing That Should Not Be" from "Master of Puppets" were in D tuning. The Australian CD release of "Load" includes a bonus interview CD that is unavailable elsewhere. 10 songs from the album have been played live including "King Nothing", "Until It Sleeps", "Ain't My Bitch", "Bleeding Me", "Wasting My Hate", "Hero of the Day", "The Outlaw Torn", "2 X 4", "Poor Twisted Me", "Mama Said". Songs that have not been played live in their entirety are "The House Jack Built", "Cure", "Thorn Within", and "Ronnie".

The cover of "Load" is an original artwork titled "Semen and Blood III". It is one of three photographic studies that Andres Serrano created in 1990 by mingling bovine blood and his own semen between two sheets of Plexiglas. The liner notes simply state "cover art by Andres Serrano" rather than listing the title of the work. In a 2009 interview with "Classic Rock", Hetfield expressed his dislike of the album cover and its inspiration:

"Load" also marked the first appearance of a new Metallica logo that rounded off the stabbing edges of the band's earlier logo, greatly simplifying its appearance. The M from the original logo was used to make a shuriken-like symbol known as the "ninja star", which was used as an alternate logo on this and future albums, and on related artwork. The album featured an expansive booklet containing photographs by Anton Corbijn. These photographs depict the band in various dress, including white A-shirts with suspenders, Cuban suits, and gothic. In the aforementioned 2009 interview, James Hetfield said:

"Load" received positive to mixed reviews from critics. "Rolling Stone" said, "The foursome dams the bombast and chugs half-speed ahead, settling into a wholly magnetizing groove that bridges old-school biker rock and the doomier side of post-grunge '90s rock." "Q" enthused, "These boys set up their tents in the darkest place of all, in the naked horror of their own heads... Metallica make existential metal and they've never needed the props... Metallica are still awesome... What is new is streamlined attack, the focus and, yes, the tunes."

"Melody Maker" expressed reservations about "Load"'s heaviness compared to its predecessors: "A Metallica album is traditionally an exhausting event. It should rock you to exhaustion, leave you brutalised and drained. This one is no exception. It is, however, the first Metallica album to make me wonder at any point, 'What the fuck was that?' It's as if the jackboot grinding the human face were to take occasional breaks for a pedicure." AllMusic considered "Load" repetitive, uninteresting and poorly executed.

"Some of that stuff was pretty cool," remarked Lars Ulrich of the album and its sequel. "With "Load", it was disappointing that some people's reaction to the music was biased by how they dealt with the pictures – the hair and all that crap "[see Artwork, above]". People have come up to me years afterwards and said, 'I never gave the record a fair chance because I couldn't get beyond Jason Newsted wearing eyeliner.' But 'The Outlaw Torn', some of that shit is pretty fucking awesome."

In 2018, James Hetfield said that looking back now he considers the Load era as the "U2 era" of the band and as a compromise to Hammett and Ulrich who were both heavily into Oasis and Blur at the time. Hetfield also stated that neither he or former bassist Jason Newsted were fully on board with the direction taken by Ulrich and Hammett and that both Load and the follow up album didn't turn out as good as he hoped. Asked if deceased bass player Cliff Burton would have approved, Hetfield said Burton would have opposed the direction taken on Load as in a lot of aspects it was following trends and that wasn't Cliff. 
Credits are adapted from the album's liner notes.

Metallica

Production


</doc>
<doc id="18763" url="https://en.wikipedia.org/wiki?curid=18763" title="Garage Inc.">
Garage Inc.

Garage Inc. is a compilation album of cover songs by American heavy metal band Metallica. It was released on November 23, 1998 through Elektra Records. Over 2.5 million copies have been sold in the U.S. as certified by the RIAA. It includes cover songs, B-side covers, and "", which had gone out of print since its original release in 1987. The title is a combination of "Garage Days Revisited" and their song "Damage, Inc.", from "Master of Puppets", and the album's graphical cover draws heavily from the 1987 EP. The album features songs by artists that have influenced Metallica, including many bands from the new wave of British heavy metal movement, hardcore punk bands and popular songs. As of August 2013, the album has sold more than 6 million copies worldwide.

The day after Metallica finished the North American leg of the Poor Re-Touring Me Tour in San Diego's Coors Amphitheatre, they hit the studio to start recording a new album of cover versions. As Lars Ulrich explained, the band wanted to do something different after "three pretty serious albums in a row, starting with the Black album and then "Load" and "ReLoad"", and the process would be easier by working with covers, especially as the band had a tradition of taking other people's songs and "turn them into something very Metallica, different from what the original artist did". Given the band had recorded many covers that were spread across various releases, such as B-sides of their singles and the 1987 EP "", the band would "put them all in a nice little packaging for easy listening" along with the newly recorded cover versions, chosen through a group decision. Only one of the eleven songs in the "New Recordings '98" disk was not done in the three-week sessions, a version of Lynyrd Skynyrd's "Tuesday's Gone" the band recorded for a radio broadcast along with friends such as Les Claypool, John Popper and Gary Rossington.

The cover for "Garage Inc." had a Ross Halfin photograph of Metallica dressed as mechanics. The band wanted the booklet to hold a detailed account of the contents of the project, and designer Andy Airfix was allowed to search through Ulrich's catalogue of Metallica memorabilia in San Francisco to create a 32-page booklet. Airfix also did the back cover, where the front of "Garage Days Re-Revisited" was modified with headshots of Metallica in 1998 and the track list written on tracing paper.


These tracks were recorded in September–October 1998 for the "Garage Inc." album.


These tracks are a collection of B-sides from artists Metallica were inspired by, throughout the early years of the band.


Metallica

Guest musicians on "Tuesday's Gone"
Technical personnel

In the U.S., "Garage Inc." sold 426,500 units in the first week of release, making Metallica's fourth straight debut surpassing 400,000 copies. Still, the million-plus record breaking debut of Garth Brooks' "Double Live" made "Garage Inc." land only at second on the "Billboard 200". As of 2003 the album has sold over 2.5 million copies and has been certified 5× Platinum.



</doc>
<doc id="18764" url="https://en.wikipedia.org/wiki?curid=18764" title="S&amp;M (album)">
S&amp;M (album)

S&M (an abbreviation of "Symphony and Metallica") is a live album by American heavy metal band Metallica, with The San Francisco Symphony conducted by Michael Kamen. It was recorded on April 21–22, 1999 at The Berkeley Community Theatre. This is the final Metallica album to feature Jason Newsted as Metallica's bassist.

"S&M" contains performances of Metallica songs with additional symphonic accompaniment, which was composed by Michael Kamen, who also conducted the orchestra during the concert. The idea to combine heavy metal with an epic classical approach, as James Hetfield has stated repeatedly, was an idea of Cliff Burton. Burton’s love of classical music, especially of Johann Sebastian Bach, can be traced back to many instrumental parts and melodic characteristics in Metallica’s songwriting including songs from "Ride The Lightning" and "Master of Puppets". The other inspiration was Deep Purple's 1969 "Concerto for Group and Orchestra". Purple revived their hybrid musical performance in 1999's "In Concert with The London Symphony Orchestra" after being notified that Metallica were doing theirs earlier that year.

In addition to songs from previous albums spanning "Ride the Lightning" through "ReLoad", there are two new compositions: "No Leaf Clover" and "−Human". "The Ecstasy of Gold" by Ennio Morricone, Metallica's entrance music, was played live by the orchestra. "No Leaf Clover" has since been performed by Metallica in concert, using a recording of the orchestral prelude.

Several other songs, including "Wasting My Hate", "The Unforgiven", "Low Man's Lyric", "Fade to Black", The Unforgiven II, "Through the Never", "Harvester of Sorrow", and even more obscure and lesser-played tracks such as "Ronnie" and "Mama Said" were considered for selection, but were eventually dropped as it was decided by both Metallica and Kamen that they were not well-suited for symphonic accompaniment. Unexpectedly, the nearly ten-minute "...And Justice For All" was also considered for the setlist but, due to the gap in time since the band last played it and with little time to prepare, the idea was abandoned. On the "S&M" DVD documentary, Metallica and Kamen can be seen and heard discussing the orchestration for "So What?", though it's obvious both parties were enjoying a good joke and had no intentions of performing the song.

Changes were made to the lyrics of some songs, most notably the removal of the second verse and chorus of "The Thing That Should Not Be" and playing the third verse in its place.

The "S" in the stylized "S&M" on the album cover is a backwards treble clef, while the "M" is taken from Metallica's logo.

The drum kit Ulrich used on the album currently resides in a Guitar Center in San Francisco.


"S&M" sold 300,000 units in the first week of release, and went on to sell a total of 2.5 million copies. As of 2003, the album had been certified 5× platinum. As of August 2013 the album had sold more than 8 million copies worldwide.

The album was included in the book "1001 Albums You Must Hear Before You Die".

This album was also ranked 48th on NME's list of 50 Greatest Live Albums.

With Kamen's death in 2003, Metallica has not revisited the S&M concept in any further performances or recording work.

Metallica also filmed and released the concert in DVD and VHS with direction by Wayne Isham. The VHS set has only the concert video, while the double DVD set has 5.1 sound (also: 2.0 band+orchestra, 2.0 band-only and 2.0 orchestra-only), 41 minute documentary about the concert, and two "No Leaf Clover" music videos: "Slice & Dice" version and the "Maestro Edit". The DVD also contains four songs with multi-angles where each band member can be viewed individually: "Of Wolf and Man", "Fuel", "Sad But True", and "Enter Sandman".


</doc>
<doc id="18787" url="https://en.wikipedia.org/wiki?curid=18787" title="Metallica">
Metallica

Metallica is an American heavy metal band. The band was formed in 1981 in Los Angeles, California by drummer Lars Ulrich and vocalist/guitarist James Hetfield, and has been based in San Francisco, California for most of its career. The group's fast tempos, instrumentals and aggressive musicianship made them one of the founding "big four" bands of thrash metal, alongside Megadeth, Anthrax and Slayer. Metallica's current lineup comprises founding members Hetfield and Ulrich, longtime lead guitarist Kirk Hammett and bassist Robert Trujillo. Guitarist Dave Mustaine (who went on to form Megadeth) and bassists Ron McGovney, Cliff Burton and Jason Newsted are former members of the band.

Metallica earned a growing fan base in the underground music community and won critical acclaim with its first five albums. The band's third album, "Master of Puppets" (1986), was described as one of the heaviest and most influential thrash metal albums; its eponymous fifth album, "Metallica" (1991), the band's first to root predominantly in heavy metal, appealed to a more mainstream audience, achieving substantial commercial success and selling over 16 million copies in the United States to date, making it the best-selling album of the SoundScan era. After experimenting with different genres and directions in subsequent releases, the band returned to its thrash metal roots with the release of its ninth album, "Death Magnetic" (2008), which drew similar praise to that of the band's earlier albums.

In 2000, Metallica led the case against the peer-to-peer file sharing service Napster, in which the band and several other artists filed lawsuits against the service for sharing their copyright-protected material without consent; after reaching a settlement, Napster became a pay-to-use service in 2003. Metallica was the subject of the acclaimed 2004 documentary film "Some Kind of Monster", which documented the troubled production of the band's eighth album, "St. Anger" (2003), and the internal struggles within the band at the time. In 2009, Metallica was inducted into the Rock and Roll Hall of Fame. The band wrote the screenplay for and starred in the 2013 IMAX concert film "", in which the band performed live against a fictional thriller storyline.

Metallica has released ten studio albums, four live albums, a cover album, five extended plays, 37 singles and 39 music videos. The band has won nine Grammy Awards from 23 nominations, and its last six studio albums (beginning with "Metallica") have consecutively debuted at number one on the "Billboard" 200. Metallica ranks as one of the most commercially successful bands of all time, having sold over 125 million albums worldwide as of 2018. Metallica has been listed as one of the greatest artists of all time by magazines such as "Rolling Stone", which ranked them at no. 61 on its "100 Greatest Artists of All Time" list. As of 2017, Metallica is the third best-selling music artist since Nielsen SoundScan began tracking sales in 1991, selling a total of 58 million albums in the United States.

Metallica was formed in Los Angeles, California, in late 1981 when Danish-born drummer Lars Ulrich placed an advertisement in a Los Angeles newspaper, "The Recycler", which read, "Drummer looking for other metal musicians to jam with Tygers of Pan Tang, Diamond Head and Iron Maiden." Guitarists James Hetfield and Hugh Tanner of Leather Charm answered the advertisement. Although he had not formed a band, Ulrich asked Metal Blade Records founder Brian Slagel if he could record a song for the label's upcoming compilation album, "Metal Massacre". Slagel accepted, and Ulrich recruited Hetfield to sing and play rhythm guitar. The band was officially formed on October 28, 1981, five months after Ulrich and Hetfield first met.

Ulrich talked to his friend Ron Quintana, who was brainstorming names for a fanzine. Quintana had proposed the names MetalMania and Metallica. Ulrich named his band Metallica. A second advertisement was placed in "The Recycler" for a position as lead guitarist. Dave Mustaine answered; Ulrich and Hetfield recruited him after seeing his expensive guitar equipment. In early 1982, Metallica recorded its first original song, "Hit the Lights", for the "Metal Massacre I" compilation. Hetfield played bass on the song, and Lloyd Grant was credited with a guitar solo. "Metal Massacre I" was released on June 14, 1982; early pressings listed the band incorrectly as "Mettallica". Although angered by the error, Metallica created enough "buzz" with the song, and the band played its first live performance on March 14, 1982, at Radio City in Anaheim, California, with newly recruited bassist Ron McGovney. The band's first taste of live success came early; they were chosen to open for British heavy metal band Saxon at one gig of their 1982 US tour. This was Metallica's second gig. Metallica recorded its first demo, "Power Metal", whose name was inspired by Quintana's early business cards in early 1982.

The term "thrash metal" was coined by "Kerrang!" journalist Malcolm Dome in reference to Anthrax's song "Metal Thrashing Mad" in "Kerrang!" issue 62, published on February 23, 1984. Prior to this, James Hetfield referred to Metallica's sound as "power metal". In late 1982, Ulrich and Hetfield attended a show at the West Hollywood nightclub Whisky a Go Go, which featured bassist Cliff Burton in a band named Trauma. The two were "blown away" by Burton's use of a wah-wah pedal and asked him to join Metallica. Hetfield and Mustaine wanted McGovney to leave because they thought he "didn't contribute anything, he just followed". Although Burton initially declined the offer, by the end of the year, he had accepted on the condition the band move to El Cerrito in the San Francisco Bay Area. Metallica's first live performance with Burton was at the nightclub The Stone in March 1983, and the first recording to feature Burton was the "Megaforce" demo (1983).

Metallica was ready to record their debut album, but when Metal Blade was unable to cover the cost, the band began looking for other options. Concert promoter Johny "Z" Zazula, who had heard the demo "No Life 'til Leather" (1982), offered to broker a record deal between Metallica and New York City-based record labels. After those record labels showed no interest, Zazula borrowed enough money to cover the recording budget and signed Metallica to his own label, Megaforce Records.

In May 1983, Metallica traveled to Rochester, New York to record its debut album, "Metal Up Your Ass", which was produced by Paul Curcio. The other members decided to eject Mustaine from the band because of his drug and alcohol abuse, and violent behavior just before the recording sessions on April 11, 1983. Exodus guitarist Kirk Hammett replaced Mustaine the same afternoon.
Mustaine, who went on to found Megadeth, has expressed his dislike for Hammett in interviews, saying Hammett "stole" his job. Mustaine was "pissed off" because he believes Hammett became popular by playing guitar leads that Mustaine himself had written. In a 1985 interview with "Metal Forces", Mustaine said, "it's real funny how Kirk Hammett ripped off every lead break I'd played on that "No Life 'til Leather" tape and got voted No. 1 guitarist in your magazine". On Megadeth's debut album "Killing Is My Business... and Business Is Good!" (1985), Mustaine included the song "Mechanix", which Metallica had previously reworked and retitled "The Four Horsemen" on "Kill 'Em All". Mustaine said he did this to "straighten Metallica up" because Metallica referred to Mustaine as a drunk and said he could not play guitar. Metallica's first live performance with Hammett was on April 16, 1983, at a nightclub in Dover, New Jersey called The Showplace; the support act was Anthrax's original line-up, which included Dan Lilker and Neil Turbin. This was the first time the two bands performed live together.

Because of conflicts with its record label and the distributors' refusal to release an album titled "Metal Up Your Ass", the album was renamed "Kill 'Em All". It was released on Megaforce Records in the U.S. and on Music for Nations in Europe, and peaked at number 155 on the "Billboard" 200 in 1986. Although the album was not initially a financial success, it earned Metallica a growing fan base in the underground metal scene. To support the release, Metallica embarked on the Kill 'Em All for One tour with Raven. In February 1984, Metallica supported Venom on the Seven Dates of Hell tour, during which the bands performed in front of 7,000 people at the Aardschok Festival in Zwolle, Netherlands.

Metallica recorded its second studio album, "Ride the Lightning", at Sweet Silence Studios in Copenhagen, Denmark. It was released in August 1984 and reached number 100 on the "Billboard" 200. A French printing press mistakenly printed green covers for the album, which are now considered collectors' items. Mustaine received writing credit for "Ride the Lightning" and "The Call of Ktulu".

Elektra Records A&R director Michael Alago, and co-founder of Q-Prime Management Cliff Burnstein, attended a Metallica concert in September 1984. They were impressed with the performance, signed Metallica to Elektra, and made the band as a client of Q-Prime Management. Metallica's growing success was such that the band's British label Music for Nations released "Creeping Death" as a limited edition single, which sold 40,000 copies as an import in the U.S. Two of the three songs on the recordcover versions of Diamond Head's "Am I Evil?" and Blitzkrieg's "Blitzkrieg"appeared on the 1988 Elektra reissue of "Kill 'Em All". Metallica embarked on its first major European tour with Tank to an average crowd of 1,300. Returning to the U.S., it embarked upon a tour co-headlining with W.A.S.P. and supported by Armored Saint. Metallica played its largest show at the Monsters of Rock festival at Donington Park, England, on August 17, 1985, with Bon Jovi and Ratt, playing to 70,000 people. At a show in Oakland, California, at the Day on the Green festival, the band played to a crowd of 60,000.

Metallica's third studio album, "Master of Puppets", was recorded at Sweet Silence Studios and was released in March 1986. The album reached number 29 on the "Billboard" 200 and spent 72 weeks on the chart. It was the band's first album to be certified gold on November 4, 1986, and was certified six times platinum in 2003. Steve Huey of AllMusic considered the album "the band's greatest achievement". Following the release of the album, Metallica supported Ozzy Osbourne on a U.S. tour. Hetfield broke his wrist while skateboarding; he continued with the tour, performing vocals, with guitar technician John Marshall playing rhythm guitar.

On September 27, 1986, during the European leg of Metallica's Damage, Inc. Tour, members drew cards to determine which bunks on the tour bus they would sleep in. Burton won and chose to sleep in Hammett's bunk. At around sunrise near Dörarp, Sweden, the bus driver lost control and skidded, which caused the bus to overturn several times. Ulrich, Hammett, and Hetfield sustained no serious injuries; however, bassist Burton was pinned under the bus and died. Hetfield said:
I saw the bus lying right on him. I saw his legs sticking out. I freaked. The bus driver, I recall, was trying to yank the blanket out from under him to use for other people. I just went, 'Don't fucking do that!' I already wanted to kill the [bus driver]. I don't know if he was drunk or if he hit some ice. All I knew was, he was driving and Cliff wasn't alive anymore.
Burton's death left Metallica's future in doubt. The three remaining members decided Burton would want them to carry on, and with the Burton family's blessings the band sought a replacement. Roughly 40 people, including Hammett's childhood friend, Les Claypool of Primus, Troy Gregory of Prong, and Jason Newsted, formerly of Flotsam and Jetsam, auditioned for the band. Newsted learned Metallica's entire set list; after the audition Metallica invited him to Tommy's Joynt in San Francisco. Hetfield, Ulrich, and Hammett decided on Newsted as Burton's replacement; Newsted's first live performance with Metallica was at the Country Club in Reseda, California. The members initiated Newsted by tricking him into eating a ball of wasabi.

After Newsted joined Metallica, the band left its El Cerrito practice spacea suburban house formerly rented by sound engineer Mark Whitaker dubbed "the Metalli-mansion"and relocated to the adjacent cities of Berkeley and Albany before eventually settling in the Marin County city of San Rafael, north of San Francisco.

Metallica finished its tour in the early months of 1987. In March 1987, Hetfield again broke his wrist while skateboarding, forcing the band to cancel an appearance on "Saturday Night Live". In August 1987, an all-covers extended play (EP) titled "" was released. The EP was recorded in an effort to use the band's newly constructed recording studio, test Newsted's talents, and to relieve grief and stress following the death of Burton. A video titled "Cliff 'Em All" commemorating Burton's three years in Metallica was released in 1987; the video included bass solos, home videos, and pictures.

Metallica's first studio album since Burton's death, "...And Justice for All", was released in 1988. The album was a commercial success, reaching number six on the "Billboard" 200, and was the band's first album to enter the top 10. The album was certified platinum nine weeks after its release. There were complaints about the production; Steve Huey of AllMusic said Ulrich's drums were clicking more than thudding, and the guitars "buzz thinly". To promote the album, Metallica embarked on a tour called Damaged Justice.
In 1989, Metallica received its first Grammy Award nomination for "...And Justice for All" in the new Best Hard Rock/Metal Performance Vocal or Instrument category. Metallica was the favorite to win but the award was given to Jethro Tull for the album "Crest of a Knave". The award was controversial with fans and the press; Metallica was standing off-stage waiting to receive the award after performing the song "One". Jethro Tull had been advised by its manager not to attend the ceremony because he was expecting Metallica to win. The award was named in "Entertainment Weekly" "Grammy's 10 Biggest Upsets".

Following the release of "...And Justice for All", Metallica released its debut music video for the song "One", which the band performed in an abandoned warehouse. The footage was remixed with the film "Johnny Got His Gun". Rather than organize an ongoing licensing deal, Metallica purchased the rights to the film. The remixed video was submitted to MTV with an alternative, performance-only version that was held back in case MTV banned the remixed version. MTV accepted the remixed version; the video was viewers' first exposure to Metallica. In 1999, it was voted number 38 in MTV's "Top 100 Videos of All Time" countdown; it was featured in the network's 25th Anniversary edition of "ADD Video", which showcased the most popular videos on MTV in the last 25 years.

In October 1990, Metallica entered One on One Recording's studio in North Hollywood to record its next album. Bob Rock, who had worked with Aerosmith, The Cult, Bon Jovi, and Mötley Crüe, was hired as the producer. "Metallica"also known as "The Black Album"was remixed three times, cost , and ended three marriages. Although the release was delayed until 1991, "Metallica" debuted at number one in ten countries, selling 650,000 units in the U.S. during its first week. The album brought Metallica mainstream attention; it has been certified 16 times platinum in the U.S., which makes it the 25th-best-selling album in the country. The making of "Metallica" and the following tour was documented in "A Year and a Half in the Life of Metallica". The tour in support of the album, called the Wherever We May Roam Tour, lasted 14 months and included dates in the U.S., Japan, and the UK. In April 1992, Metallica appeared at The Freddie Mercury Tribute Concert and performed a three-song set. Hetfield later performed "Stone Cold Crazy" with the remaining members of Queen and Tony Iommi.

On August 8, 1992, during the co-headlining Guns N' Roses/Metallica Stadium Tour, Hetfield suffered second and third degree burns to his arms, face, hands, and legs. There had been some confusion with the new pyrotechnics setup, which resulted in Hetfield walking into a flame during "Fade to Black". Newsted said Hetfield's skin was "bubbling like on "The Toxic Avenger"". Metallica returned to the stage 17 days later with guitar technician and Metal Church member John Marshall replacing Hetfield on guitar for the remainder of the tour, although Hetfield was able to sing. Later in 1993, Metallica went on the Nowhere Else to Roam Tour, playing five shows in Mexico City. "", the band's first box set, was released in November 1993. The collection contained three live CDs, three home videos, and a book filled with riders and letters.

After almost three years of touring to promote the album "Metallica", including a headlining performance at Woodstock '94, Metallica returned to the studio to write and record its sixth studio album. The band went on a brief hiatus in the summer of 1995 and played three outdoor shows that included headlining at Donington Park, where it was supported by Slayer, Skid Row, Slash's Snakepit, Therapy?, and Corrosion of Conformity. The short tour was titled Escape from the Studio '95. The band spent about a year writing and recording new songs, resulting in the release of "Load" in 1996. "Load" debuted at number one on the "Billboard" 200 and ARIA Charts; it was the band's second number one album. The cover art of "Load", called "Blood and Semen III", was created by Andres Serrano, who pressed a mixture of his own semen and blood between sheets of plexiglass. The release marked a change in the band's musical direction and a new image; band members' hair was cut. Metallica headlined the alternative rock festival Lollapalooza festival in mid-1996.
During early production of the album, the band had recorded enough material to fill a double album. It was decided that half of the songs were to be released; the band would continue to work on the remaining songs and release them the following year. This resulted in the follow-up album, "Reload". The cover art was again created by Serrano, this time using a mixture of blood and urine. "Reload" debuted at number one on the "Billboard" 200 and reached number two on the Top Canadian Album chart. Hetfield said in the 2004 documentary film "Some Kind of Monster" that the band initially thought some of the songs on these albums were of average quality; these were "polished and reworked" until judged to be releasable. To promote "Reload", Metallica performed "Fuel" and "The Memory Remains" with Marianne Faithfull on NBC's "Saturday Night Live" in December 1997.

In 1998, Metallica compiled a double album of cover songs titled "Garage Inc." The first disc contained newly recorded covers of songs by Diamond Head, Killing Joke, the Misfits, Thin Lizzy, Mercyful Fate, Black Sabbath, and others. The second disc featured the original version of "The $5.98 E.P.: Garage Days Re-Revisited", which had become a scarce collectors' item. The album entered the "Billboard" 200 at number two.

On April 21 and 22, 1999, Metallica recorded two performances with the San Francisco Symphony conducted by Michael Kamen, who had previously worked with producer Rock on "Nothing Else Matters". Kamen approached Metallica in 1991 with the idea of pairing the band's music with a symphony orchestra. Kamen and his staff of over 100 composed additional orchestral material for Metallica songs. Metallica wrote two new Kamen-scored songs for the event, "No Leaf Clover" and "-Human". The audio recording and concert footage were released in 1999 as the album and concert film "S&M". It entered the "Billboard" 200 at number two, and the Australian ARIA charts and Top Internet Albums chart at number one.

In 2000, Metallica discovered that a demo of its song "I Disappear", which was supposed to be released in combination with the , was receiving radio airplay. Tracing the source of the leak, the band found the file on the Napster peer-to-peer file-sharing network, and also found that the band's entire catalogue was freely available. Legal action was initiated against Napster; Metallica filed a lawsuit at the U.S. District Court, Central District of California, alleging that Napster violated three areas of the law: copyright infringement, unlawful use of digital audio interface device, and the Racketeer Influenced and Corrupt Organizations Act (RICO).
Ulrich provided a statement to the Senate Judiciary Committee regarding copyright infringement on July 11, 2000. Federal Judge Marilyn Hall Patel ordered the site to place a filter on the program within 72 hours or be shut down. A settlement between Metallica and Napster was reached when German media conglomerate Bertelsmann BMG showed interest in purchasing the rights to Napster for $94 million. Under the terms of settlement, Napster agreed to block users who shared music by artists who do not want their music shared. On June 3, 2002, Napster filed for Chapter 11 protection under U.S. bankruptcy laws. On September 3, 2002, an American bankruptcy judge blocked the sale of Napster to Bertelsmann and forced Napster to liquidate its assets according to Chapter 7 of the U.S. bankruptcy laws.

At the 2000 MTV Video Music Awards, Ulrich appeared with host Marlon Wayans in a skit that criticized the idea of using Napster to share music. Marlon played a college student listening to Metallica's "I Disappear". Ulrich walked in and asked for an explanation. Ulrich responded to Wayans' excuse that using Napster was just "sharing" by saying that Wayans' idea of sharing was "borrowing things that were not yours without asking". He called in the Metallica road crew, who proceeded to confiscate all of Wayans' belongings, leaving him almost naked in an empty room. Napster creator Shawn Fanning responded later in the ceremony by presenting an award wearing a Metallica shirt, saying, "I borrowed this shirt from a friend. Maybe, if I like it, I'll buy one of my own." Ulrich was later booed on stage at the award show when he introduced the final musical act, Blink-182.

Newsted left Metallica on January 17, 2001, as plans were being made to enter the recording studio. He said he left the band for "private and personal reasons, and the physical damage I have done to myself over the years while playing the music that I love". During a "Playboy" interview with Metallica, Newsted said he wanted to release an album with his side project, Echobrain. Hetfield was opposed to the idea and said, "When someone does a side project, it takes away from the strength of Metallica", and that a side project is "like cheating on your wife in a way". Newsted said Hetfield had recorded vocals for a song used in the film "", and appeared on two Corrosion of Conformity albums. Hetfield replied, "My name isn't on those records. And I'm not out trying to sell them", and pondered questions such as, "Where would it end? Does he start touring with it? Does he sell shirts? Is it his band?"
In April 2001, filmmakers Joe Berlinger and Bruce Sinofsky began following Metallica to document the recording process of the band's next studio album. Over two years they recorded more than 1,000 hours of footage. On July 19, 2001, before preparations to enter the recording studio, Hetfield entered rehab to treat his "alcoholism and other addictions". All recording plans were put on hold and the band's future was in doubt. Hetfield left rehab on December 4, 2001, and the band returned to the recording studio on April 12, 2002. Hetfield was required to limit his work to four hours a day between noon and 4 pm, and to spend the rest of his time with his family. The footage recorded by Berlinger and Sinofsky was compiled into the documentary "Some Kind of Monster", which premiered at the Sundance Film Festival in January 2004. In the documentary, Newsted said his former bandmates' decision to hire a therapist to help solve their problems which he felt they could have solved on their own was "really fucking lame and weak".

For the duration of the recording period, producer Bob Rock played bass on the album and in several live shows at which Metallica performed during that time. Once the record was completed in early 2003, the band started to hold auditions for Newsted's permanent replacement. Bassists Pepper Keenan, Jeordie White, Scott Reeder, Eric Avery, Danny Lohner, and Chris Wyseamong othersauditioned for the role. After three months of auditions, Robert Trujillo, formerly of Suicidal Tendencies and Ozzy Osbourne's band, was chosen as the new bassist. As Metallica moved on, Newsted joined Canadian thrash metal band Voivod in 2002, and was Trujillo's replacement in Osbourne's band during the 2003 Ozzfest tour, which included Voivod.
In June 2003, Metallica's eighth studio album, "St. Anger", debuted at number one on the "Billboard" 200, and drew mixed reactions from critics. Ulrich's "steely" sounding snare drum and the absence of guitar solos received particular criticism. Kevin Forest Moreau of "Shakingthrough.net" said, "the guitars stumble in a monotone of mid-level, processed rattle; the drums don't propel as much as struggle to disguise an all-too-turgid pace; and the rage is both unfocused and leavened with too much narcissistic navel-gazing". Brent DiCrescenzo of "Pitchfork" described it as "an utter mess". However, "Blender" magazine called it the "grimiest and grimmest of the band's Bob Rock productions", and "New York Magazine" called it "utterly raw and rocking". The title track, "St. Anger", won the Grammy Award for Best Metal Performance in 2004; it was used as the official theme song for WWE's "SummerSlam 2003".

Before the band's set at the 2004 Download Festival, Ulrich was rushed to the hospital after having an anxiety seizure and was unable to perform. Hetfield searched for last-minute volunteers to replace Ulrich. Slayer drummer Dave Lombardo and Slipknot drummer Joey Jordison volunteered. Lombardo performed "Battery" and "The Four Horsemen", Ulrich's drum technician Flemming Larsen performed "Fade to Black", and Jordison performed the remainder of the set. Having toured for two years in support of "St. Anger" on the Summer Sanitarium Tour 2003 and the Madly in Anger with the World Tour, with multi-platinum rock band Godsmack in support, Metallica took a break from performing and spent most of 2005 with friends and family. The band opened for The Rolling Stones at AT&T Park in San Francisco on November 13 and 15, 2005.

In December 2006, Metallica released a DVD titled "The Videos 1989–2004", which sold 28,000 copies in its first week and entered the "Billboard" Top Videos chart at number three. Metallica recorded a guitar-based interpretation of Ennio Morricone's "The Ecstasy of Gold" for a tribute album titled "We All Love Ennio Morricone", which was released in February 2007. The track received a Grammy nomination at the 50th Grammy Awards for the category "Best Rock Instrumental Performance". A recording of "The Ecstasy of Gold" has been played to introduce Metallica's performances since the 1980s. Earlier that year, Metallica announced on its official website that after 15 years, long-time producer Bob Rock would not be producing the band's next studio album. Instead, the band chose to work with producer Rick Rubin. Metallica scheduled the release of "Death Magnetic" as September 12, 2008, and the band filmed a music video for the album's first single, "The Day That Never Comes".
On September 2, 2008, a record store in France began selling copies of "Death Magnetic" nearly two weeks before its scheduled worldwide release date, which resulted in the album being made available on peer-to-peer clients. This prompted the band's UK distributor Vertigo Records to officially release the album on September 10, 2008. Rumors of Metallica or Warner Bros. taking legal action against the French retailer were unconfirmed, though drummer Lars Ulrich responded to the leak by saying, "...We're ten days from release. I mean, from here, we're golden. If this thing leaks all over the world today or tomorrow, happy days. Happy days. Trust me", and, "By 2008 standards, that's a victory. If you'd told me six months ago that our record wouldn't leak until 10 days out, I would have signed up for that."

"Death Magnetic" debuted at number one in the U.S. selling 490,000 units; Metallica became the first band to have five consecutive studio albums debut at number one in the history of the "Billboard" 200. A week after its release, "Death Magnetic" remained at number one on the "Billboard" 200 and the European album chart; it also became the fastest selling album of 2008 in Australia.

"Death Magnetic" remained at number one on the "Billboard" 200 album chart for three consecutive weeks. Metallica was one of two artists whose albumthe other being Jack Johnson's album "Sleep Through the Static"remained on the "Billboard" 200 for three consecutive weeks at number one in 2008. "Death Magnetic" also remained at number one on "Billboard"'s Hard Rock, Modern Rock/Alternative and Rock album charts for five consecutive weeks. The album reached number one in 32 countries outside the U.S., including the UK, Canada, and Australia. In November 2008, Metallica's record deal with Warner Bros. ended and the band considered releasing its next album through the internet.

On January 14, 2009, it was announced that Metallica would be inducted into the Rock and Roll Hall of Fame on April 4, 2009, and that former bassist Jason Newstedwho left the band in 2001would perform with the band at the ceremony. Initially, it was announced that the matter had been discussed and that bassist Trujillo had agreed not to play because he "wanted to see the Black Album band". However, during the band's set of "Master of Puppets" and "Enter Sandman", both Trujillo and Newsted were on stage. Ray Burton, father of the late Cliff Burton, accepted the honor on his behalf. Although he was not to be inducted with them, Metallica invited Dave Mustaine to take part in the induction ceremony. Mustaine declined because of his touring commitments in Europe.

Metallica, Slayer, Megadeth, and Anthrax performed on the same bill for the first time on June 16, 2010, at Warsaw Babice Airport, Warsaw, as a part of the Sonisphere Festival series. The show in Sofia, Bulgaria, on June 22, 2010, was broadcast via satellite to cinemas. The bands also played concerts in Bucharest on June 26, 2010, and Istanbul on June 27, 2010. On June 28, 2010, "Death Magnetic" was certified double platinum by the RIAA.
Metallica's World Magnetic Tour ended in Melbourne on November 21, 2010. The band had been touring for over two years in support of "Death Magnetic". To accompany the final tour dates in Australia and New Zealand, a live, limited edition EP of past performances in Australia called "Six Feet Down Under" was released. The EP was followed by "Six Feet Down Under (Part II)", which was released on November 12, 2010. Part 2 contains a further eight songs recorded during the first two Oceanic Legs of the World Magnetic Tour.

On November 26, 2010, Metallica released a live EP titled "Live at Grimey's", which was recorded in June 2008 at Grimey's Record Store, just before the band's appearance at Bonnaroo Music Festival that year.

In a June 2009 interview with Italy's Rock TV, Ulrich said Metallica was planning to continue touring until August 2010, and that there were no plans for a tenth album. He said he was sure the band would collaborate with producer Rick Rubin again. According to Blabbermouth.net, the band was considering recording its next album in the second half of 2011. In November 2010, during an interview with The Pulse of Radio, Ulrich said Metallica would return to writing in 2011. Ulrich said, "There's a bunch of balls in the air for 2011, but I think the main one is we really want to get back to writing again. We haven't really written since, what, '06, '07, and we want to get back to kind of just being creative again. Right now we are going to just chill out and then probably start up again in, I'd say, March or April, and start probably putting the creative cap back on and start writing some songs."

In an interview at the April 2011 Big Four concert, Robert Trujillo said Metallica will work with Rick Rubin again as producer for the new album and were "really excited to write some new music. There's no shortage of riffage in Metallica world right now." He added, "The first album with Rick was also the first album for me, so in a lot of ways, you're kind of testing the water. Now that we're comfortable with Rick and his incredible engineer, Greg Fidelman, who worked with Slayer, actually, on this last recordit's my heroit's a great team. And it's only gonna better; I really believe that. So I'm super-excited." In June 2011, Rubin said Metallica had begun writing its new album. On November 9, 2010, Metallica announced it would be headlining the Rock in Rio festival in Rio de Janeiro on September 25, 2011.

On December 13, 2010, the band announced it would again play as part of the "big four" during the Sonisphere Festival at Knebworth House, Hertfordshire, on July 8, 2011. It was the first time all of the "big four" members played on the same stage in the UK. On December 17, 2010, Another "big four" Sonisphere performance that would take place in France on July 9 was announced. On January 25, 2011, another "big four" performance on April 23, 2011, at the Empire Polo Club in Indio, California, was announced. It was the first time all of the "big four" members played on the same stage in the U.S. On February 17, 2011, a show in Gelsenkirchen, Germany, on July 2, 2011, was announced. On February 22, a "big four" show in Milan on July 6, 2011, was announced. On March 2, 2011, another "big four" concert, which took place in Gothenburg on July 3, 2011, was announced. The final "big four" concert was in New York City, at Yankee Stadium, on September 14, 2011.

On June 15, 2011, Metallica announced that recording sessions with singer-songwriter Lou Reed had concluded. The album, which was titled "Lulu", was recorded over several months and comprised ten songs based on Frank Wedekind's "Lulu" plays "Earth Spirit" and "Pandora's Box". The album was released on October 31, 2011. The recording of the album was problematic at times; Lars Ulrich later said Lou Reed challenged him to a "street fight". On October 16, 2011, Robert Trujillo confirmed that the band was back in the studio and writing new material. He said, "The writing process for the new Metallica album has begun. We've been in the studio with Rick Rubin, working on a couple of things, and we're going to be recording during the most of next year."
Metallica was due to make its first appearance in India at the "India Rocks" concert, supporting the 2011 Indian Grand Prix. However, the concert was canceled when the venue was proven to be unsafe. Fans raided the stage during the event and the organizers were later arrested for fraud. Metallica made its Indian debut in Bangalore on October 30, 2011. On November 10, it was announced that Metallica would headline the main stage on Saturday June 9, 2012, at the Download Festival at Donington Park and that the band would play "The Black Album" in its entirety. Metallica celebrated its 30th anniversary by playing four shows at the Fillmore in San Francisco in December 2011. The shows were exclusive to Met Club members and tickets were charged at $6 each or $19.81 for all four nights. The shows consisted of songs from the band's career and featured guest appearances by artists who had either helped or had influenced Metallica. These shows were notable because Lloyd Grant, Dave Mustaine, Jason Newsted, Glenn Danzig, Ozzy Osbourne, Jerry Cantrell, Apocalyptica, members of Diamond Head, and King Diamond joined Metallica on stage for all appropriate songs. In December 2011, Metallica began releasing songs that were written for "Death Magnetic" but were not included on the album online. On December 13, 2011, the band released "Beyond Magnetic", a digital EP release exclusively on iTunes. It was released on CD in January 2012.

On February 7, 2012, Metallica announced that it would start a new music festival called Orion Music + More, which took place on June 23 and 24, 2012, in Atlantic City. Metallica also confirmed that it would headline the festival on both days and would perform two of its most critically acclaimed albums in their entirety: "The Black Album" on one night, and "Ride the Lightning" on the other. In a July 2012 interview with Canadian radio station 99.3 The Fox, Ulrich said Metallica would not release its new album until at least early 2014. In November 2012, Metallica left Warner Bros. Records and launched an independent record label, Blackened Recordings, which will produce the band's future releases. The band has acquired the rights to all of its studio albums, which will be reissued through the new label. Blackened releases will be licensed through Warner subsidiary Rhino Entertainment in North America and internationally through Universal Music. On September 20, 2012, Metallica announced via its official website that a new DVD containing footage of shows it performed in Quebec in 2009 would be released that December; fans would get the chance to vote for two setlists that would appear on the DVD. The film, titled "Quebec Magnetic", was released in the U.S. on December 10, 2012.

In an interview with "Classic Rock" on January 8, 2013, Ulrich said regarding the band's upcoming album, "What we're doing now certainly sounds like a continuation [of "Death Magnetic"]". He also said, "I love Rick [Rubin]. We all love Rick. We're in touch with Rick constantly. We'll see where it goes. It would stun me if the record came out in 2013." Also in 2013, the band starred in a 3D concert film titled "", which was directed by Antal Nimród and was released in IMAX theaters on September 27. In an interview dated July 22, 2013, Ulrich told "Ultimate Guitar", "2014 will be all about making a new Metallica record"; he said the album will most likely be released during 2015. Kirk Hammett and Robert Trujillo later confirmed the band's intention to enter the studio. At the second Orion Music + More festival held in Detroit, the band played under the name "Dehaan"a reference to actor Dane DeHaan, who starred in "Metallica: Through the Never". The band performed its debut album "Kill 'Em All" in its entirety, celebrating the 30th anniversary of its release. On December 8, 2013, the band played a show called "Freeze 'Em All" in Antarctica, becoming the first band to play on all seven continents. The performance was filmed and released as a live album the same month.
At the 56th Annual Grammy Awards in January 2014, Metallica performed "One" with Chinese pianist Lang Lang. In March 2014, Metallica began a tour called "Metallica By Request", in which fans request songs for the band to perform. A new song, titled "Lords of Summer" was written for the concerts and released as a "first take" demo in June 2014. In June 2014, the band headlined the Glastonbury Festival in an attempt to attract new fans. Ulrich said, "We have one shot, you never know if you'll be invited back". In November 2014, Metallica performed at the closing ceremony of BlizzCon 2014. In January 2015, Metallica announced a "Metallica Night" with the San Jose Sharks, which featured a Q&A session with the band and a charity auction benefiting the San Francisco Bay Chapter of the Sierra Club, but no performances. They were announced to headline Lollapalooza in March 2015, returning to perform there for the first time in 20 years. On May 2, 2015, Metallica performed their third annual Metallica Day at AT&T Park. Metallica were also announced to play at X Games for the first time at X Games Austin 2015 in Austin, Texas. On June 14, 2015, Hetfield and Hammett performed The Star-Spangled Banner live via electric guitars prior to game 5 of the NBA Finals between the Cleveland Cavaliers and Golden State Warriors at Oracle Arena in Oakland, California. In late October, the band unveiled a new website with an introduction from Ulrich containing footage from the studio of the band working on new material. On November 2, Metallica were announced to play "The Night Before" Super Bowl 50 at AT&T Park. Metallica announced they would be opening the U.S. Bank Stadium on August 20, 2016, with Avenged Sevenfold and Volbeat as support.
In April 2016, during the week leading up to Record Store Day, for which the band was its ambassador for 2016, Ulrich told "Billboard" that the band's expanded role within the music industry had played a part in the amount of time that it had taken to write and record the album. "The way we do things now is very different than the way we did things back in the days of "Kill 'Em All" and "Ride the Lightning". Nowadays we like to do so many different things." Ulrich was also optimistic that production of the album had almost reached its completion. "Unless something radical happens it would be difficult for me to believe that it won't come out in 2016".

On August 18, 2016, the band announced via their website that their tenth studio album, "Hardwired... to Self-Destruct", would be released worldwide on November 18, 2016, via their independent label, Blackened Recordings. They also unveiled the track listing, album artwork, and released a music video for the album's first single, "Hardwired". The album was released as scheduled and debuted at number one on the "Billboard" 200.

Metallica announced they would be touring the US in summer of 2017 for the WorldWired Tour. The stadium tour also includes Avenged Sevenfold, Volbeat and Gojira as supporting acts. On August 7, 2017, Metallica was invited by the San Francisco Giants again for the fifth annual "Metallica Night" with Hammett and Hetfield performing the national anthem. In January 2018, the band announced that they would be reissuing "The $5.98 E.P.: Garage Days Re-Revisited" on April 13 for Record Store Day, and the sixth annual "Metallica Night" was also announced a few weeks later, this time in April, with all proceeds going to the All Within My Hands Foundation, which the band created in late 2017. In February 2018, the band announced a second set of North American tour dates, most of which for cities that they had not visited in up to thirty years.

Metallica was influenced by early heavy metal and hard rock bands and artists Black Sabbath, Deep Purple, Kiss, Led Zeppelin, Queen, Ted Nugent, AC/DC, Rush, Aerosmith, Judas Priest, Scorpions and by new wave of British heavy metal (NWOBHM) bands Venom, Motörhead, Saxon, Diamond Head, Blitzkrieg, and Iron Maiden, and early punk rock bands Ramones, Sex Pistols, and the Misfits also influenced Metallica's style as did post-punk band Killing Joke. The band's early releases contained fast tempos, harmonized leads, and nine-minute instrumental tracks. Steve Huey of AllMusic said "Ride the Lightning" featured "extended, progressive epics; tight, concise groove-rockers". Huey said Metallica expanded its compositional technique and range of expression to take on a more aggressive approach in following releases, and lyrics dealt with personal and socially conscious issues. Religious and military leaders, rage, insanity, monsters, and drugsamong other themeswere explored on "Master of Puppets".

In 1991, Huey said Metallica with new producer Bob Rock simplified and streamlined its music for a more commercial approach to appeal to mainstream audiences. Robert Palmer of "Rolling Stone" said the band abandoned its aggressive, fast tempos to expand its music and expressive range. The change in direction proved commercially successful; "Metallica" was the band's first album to peak at number one on the "Billboard" 200. Metallica noticed changes to the rock scene created by the grunge movement of the early 1990s. In "Load"—an album that has been described as having "an almost alternative rock" approach—the band changed musical direction and focused on non-metal influences. Metallica's new lyrical approach moved away from drugs and monsters, and focused on anger, loss, and retribution. Some fans and critics were not pleased with this change, which included haircuts, the cover art of "Load", and headlining the Lollapalooza festival of 1996. David Fricke of "Rolling Stone" described the move as "goodbye to the moldy stricture and dead-end Puritanism of no-frills thrash", and called "Load" the heaviest record of 1996. With the release of "ReLoad" in 1997, the band displayed blues and early hard rock influences, incorporating more rhythm and harmony in song structures.

"St. Anger" marked another large change in the band's sound. Guitar solos were excluded from the album, leaving a "raw and unpolished sound". The band used drop C tuning; Ulrich's snare drum received particular criticism. "New York Magazine"s Ethan Brown said it "reverberates with a thwong". The album's lyrics deal with Hetfield's drug rehabilitation and include references to the devil, anti-drug themes, claustrophobia, impending doom, and religious hypocrisy. At the advice of producer Rick Rubin, for its ninth studio album "Death Magnetic", the band returned to standard tuning and guitar solos. As a return to Metallica's thrash roots, "Death Magnetic" was a riff-oriented album featuring intense guitar solos and subtle lyrics dealing with suicide and redemption.

Metallica has become one of the most influential heavy metal bands of all time, and is credited as one of the "big four" of thrash metal, along with Slayer, Anthrax, and Megadeth. The band has sold more than 110 million records worldwide, including an RIAA-certified 66 million and Nielsen SoundScan-reported 58,000,000 in the US, making Metallica one of the most commercially successful bands of all time. The writers of "The Rolling Stone Encyclopedia of Rock & Roll" said Metallica gave heavy metal "a much-needed charge". Stephen Thomas Erlewine and Greg Prato of Allmusic said Metallica "expanded the limits of thrash, using speed and volume not for their own sake, but to enhance their intricately structured compositions", and called the band "easily the best, most influential heavy metal band of the '80s, responsible for bringing the music back to Earth".

Jonathan Davis of Korn said he respects Metallica as his favorite band; he said, "I love that they've done things their own way and they've persevered over the years and they're still relevant to this day. I think they're one of the greatest bands ever." Godsmack drummer Shannon Larkin said Metallica has been the biggest influence on the band, stating, "they really changed my life when I was 16 years oldI'd never heard anything that heavy". Vocalist and guitarist Robb Flynn of Machine Head said that when creating the band's 2007 album, "The Blackening", "What we mean is an album that has the power, influence and epic grandeur of that album "Master of Puppets"and the staying powera timeless record like that". Trivium guitarists Corey Beaulieu and Matt Heafy said that when they heard Metallica they wanted to start playing guitar. M. Shadows of Avenged Sevenfold said touring with Metallica was the band's career highlight, and said, "Selling tons of records and playing huge shows will never compare to meeting your idols "Metallica"". God Forbid guitarists Doc and Dallas Coyle were inspired by Metallica as they grew up, and the band's bassist John Outcalt admires Burton as a "rocker". Ill Niño drummer Dave Chavarri said he finds early Metallica releases are "heavy, raw, rebellious. It said, 'fuck you'", and Adema drummer Kris Kohls said the band is influenced by Metallica.

On April 4, 2009, Metallica were inducted into the Rock And Roll Hall Of Fame. They entered the Rock and Roll Hall of Fame the second year they were eligible and first year they were nominated. Metallica's induction into the Hall included its current lineup, James Hetfield, Kirk Hammett, Robert Trujillo, and Lars Ulrich, and former members Jason Newsted and Cliff Burton.

MTV ranked Metallica the third "Greatest Heavy Metal Band in History". Metallica was ranked 42nd on VH1's "100 Greatest Artists Of All Time", was listed fifth on VH1's "100 Greatest Artists of Hard Rock", and the band was number one on VH1's "20 Greatest Metal Bands" list. "Rolling Stone" placed the band 61st on its list of "The 100 Greatest Artists of All Time"; its albums "Master of Puppets" and "Metallica" were ranked at numbers 167 and 252 respectively on the magazine's list of "The 500 Greatest Albums of All Time". "Master of Puppets" was named in "Q Magazine" "50 Heaviest Albums of All Time", and was ranked number one on IGN's "Top 25 Metal Albums", and number one on Metal-rules.com's "Top 100 Heavy Metal Albums" list. "Enter Sandman" was ranked number 399 on "Rolling Stone" "500 Greatest Songs of All Time".

"Kerrang!" released a tribute album titled "Master of Puppets: Remastered" with the April 8, 2006, edition of the magazine to celebrate the 20th anniversary of "Master of Puppets". The album featured cover versions of Metallica songs by Machine Head, Bullet for My Valentine, Chimaira, Mastodon, Mendeed, and Triviumall of which are influenced by Metallica. At least 15 Metallica tribute albums have been released. On September 10, 2006, Metallica guest starred on "The Simpsons" eighteenth-season premiere, "The Mook, the Chef, the Wife and Her Homer". Hammett's and Hetfield's voices were used in three episodes of the animated television series "Metalocalypse". Finnish cello metal band Apocalyptica released a tribute album titled "Plays Metallica by Four Cellos", which features eight Metallica songs played on cellos. A parody band named Beatallica plays music using a combination of The Beatles and Metallica songs. Beatallica faced legal troubles when Sony, which owns The Beatles' catalog, issued a cease and desist order, claiming "substantial and irreparable injury" and ordering the group to pay damages. Ulrich, a fan of Beatallica, asked Metallica's lawyer Peter Paterno to help settle the case.

On March 7, 1999, Metallica was inducted into the San Francisco Walk of Fame. The mayor of San Francisco, Willie Brown, proclaimed the day "Official Metallica Day". The band was awarded the MTV Icon award in 2003, and a concert paying tribute to the band with artists performing its songs was held. Performances included Sum 41 and a medley of "For Whom the Bell Tolls", "Enter Sandman", and "Master of Puppets". Staind covered "Nothing Else Matters", Avril Lavigne played "Fuel", hip-hop artist Snoop Dogg performed "Sad but True", Korn played "One", and Limp Bizkit performed "Welcome Home (Sanitarium)".

The "Guitar Hero" video game series included several of Metallica's songs. "One" was used in "Guitar Hero III". The album "Death Magnetic" was later released as purchasable, downloadable content for the game. "Trapped Under Ice" was featured in the sequel, "Guitar Hero World Tour". In 2009, Metallica collaborated with the game's developers to make "", which included a number of Metallica's songs. Harmonix' video game series "Rock Band" included "Enter Sandman"; "Ride the Lightning", "Blackened", and "...And Justice for All" were released as downloadable tracks. In 2013, due to expiring content licenses, "Ride the Lightning", "Blackened", and "...And Justice for All" are no longer available for download.

Current members
Former members

Session/touring musicians
Timeline
Grammy Awards



The 1988 re-issue of "Kill 'Em All" on Elektra Records also charted on the Billboard 200, peaking at number 120.



</doc>
<doc id="18816" url="https://en.wikipedia.org/wiki?curid=18816" title="Mural">
Mural

A mural is any piece of artwork painted or applied directly on a wall, ceiling or other permanent surface. A distinguishing characteristic of mural painting is that the architectural elements of the given space are harmoniously incorporated into the picture.

Some wall paintings are painted on large canvases, which are then attached to the wall (e.g., with marouflage). Whether these works can be accurately called "murals" is a subject of some controversy in the art world, but the technique has been in common use since the late 19th century.

Murals of sorts date to Upper Paleolithic times such as the paintings in the Chauvet Cave in Ardèche department of southern France (around 30,000 BC). Many ancient murals have been found within ancient Egyptian tombs (around 3150 BC), the Minoan palaces (Middle period III of the Neopalatial period, 1700–1600 BC) and in Pompeii (around 100 BC – AD 79).

During the Middle Ages murals were usually executed on dry plaster (secco). The huge collection of Kerala mural painting dating from the 14th century are examples of fresco secco. In Italy, circa 1300, the technique of painting of frescos on wet plaster was reintroduced and led to a significant increase in the quality of mural painting.

In modern times, the term became more well-known with the Mexican muralism art movement (Diego Rivera, David Siqueiros and José Orozco). There are many different styles and techniques. The best-known is probably "fresco", which uses water-soluble paints with a damp lime wash, a rapid use of the resulting mixture over a large surface, and often in parts (but with a sense of the whole). The colors lighten as they dry. The "marouflage" method has also been used for millennia.

Murals today are painted in a variety of ways, using oil or water-based media. The styles can vary from abstract to "trompe-l'œil" (a French term for "fool" or "trick the eye"). Initiated by the works of mural artists like Graham Rust or Rainer Maria Latzke in the 1980s, trompe-l'oeil painting has experienced a renaissance in private and public buildings in Europe.
Today, the beauty of a wall mural has become much more widely available with a technique whereby a painting or photographic image is transferred to poster paper or canvas which is then pasted to a wall surface "(see wallpaper, Frescography)" to give the effect of either a hand-painted mural or realistic scene.

In the history of mural several methods have been used:

"A fresco" painting, from the Italian word "affresco" which derives from the adjective "fresco" ("fresh"), describes a method in which the paint is applied on plaster on walls or ceilings. The "buon fresco" technique consists of painting in pigment mixed with water on a thin layer of wet, fresh, lime mortar or plaster. The pigment is then absorbed by the wet plaster; after a number of hours, the plaster dries and reacts with the air: it is this chemical reaction which fixes the pigment particles in the plaster. After this the painting stays for a long time up to centuries in fresh and brilliant colors.

"Fresco-secco" painting is done on dry plaster ("secco" is "dry" in Italian). The pigments thus require a binding medium, such as egg (tempera), glue or oil to attach the pigment to the wall.

"Mezzo-fresco" is painted on nearly-dry plaster, and was defined by the sixteenth-century author Ignazio Pozzo as "firm enough not to take a thumb-print" so that the pigment only penetrates slightly into the plaster. By the end of the sixteenth century this had largely displaced the "buon fresco" method, and was used by painters such as Gianbattista Tiepolo or Michelangelo. This technique had, in reduced form, the advantages of "a secco" work.

In Greco-Roman times, mostly encaustic colors applied in a cold state were used.

Tempera painting is one of the oldest known methods in mural painting. In tempera, the pigments are bound in an albuminous medium such as egg yolk or egg white diluted in water.

In 16th-century Europe, oil painting on canvas arose as an easier method for mural painting. The advantage was that the artwork could be completed in the artist's studio and later transported to its destination and there attached to the wall or ceiling. Oil paint may be a less satisfactory medium for murals because of its lack of brilliance in colour. Also the pigments are yellowed by the binder or are more easily affected by atmospheric conditions. The canvas itself is more subject to rapid deterioration than a plaster ground.
Different muralists tend to become experts in their preferred medium and application, whether that be oil paints, emulsion or acrylic paints applied by brush, roller or airbrush/aerosols. Clients will often ask for a particular style and the artist may adjust to the appropriate technique.

A consultation usually leads to a detailed design and layout of the proposed mural with a price quote that the client approves before the muralist starts on the work. The area to be painted can be gridded to match the design allowing the image to be scaled accurately step by step. In some cases the design is projected straight onto the wall and traced with pencil before painting begins. Some muralists will paint directly without any prior sketching, preferring the spontaneous technique.

Once completed the mural can be given coats of varnish or protective acrylic glaze to protect the work from UV rays and surface damage.

In modern, quick form of muralling, young enthusiasts also use POP clay mixed with glue or bond to give desired models on a canvas board. The canvas is later set aside to let the clay dry. Once dried, the canvas and the shape can be painted with your choice of colors and later coated with varnish.

As an alternative to a hand-painted or airbrushed mural, digitally printed murals can also be applied to surfaces. Already existing murals can be photographed and then be reproduced in near-to-original quality.

The disadvantages of pre-fabricated murals and decals are that they are often mass-produced and lack the allure and exclusivity of an original artwork. They are often not fitted to the individual wall sizes of the client and their personal ideas or wishes can not be added to the mural as it progresses. The Frescography technique, a digital manufacturing method (CAM) invented by Rainer Maria Latzke addresses some of the personalisation and size restrictions.

Digital techniques are commonly used in advertisements. A "wallscape" is a large advertisement on or attached to the outside wall of a building. Wallscapes can be painted directly on the wall as a mural, or printed on vinyl and securely attached to the wall in the manner of a billboard. Although not strictly classed as murals, large scale printed media are often referred to as such. Advertising murals were traditionally painted onto buildings and shops by sign-writers, later as large scale poster billboards.

Murals are important in that they bring art into the public sphere. Due to the size, cost, and work involved in creating a mural, muralists must often be commissioned by a sponsor. Often it is the local government or a business, but many murals have been paid for with grants of patronage. For artists, their work gets a wide audience who otherwise might not set foot in an art gallery. A city benefits by the beauty of a work of art.

Murals can be a relatively effective tool of social emancipation or achieving a political goal. Murals have sometimes been created against the law, or have been commissioned by local bars and coffeeshops. Often, the visual effects are an enticement to attract public attention to social issues.
State-sponsored public art expressions, particularly murals, are often used by totalitarian regimes as a tool of propaganda. However, despite the propagandist character of that works, some of them still have an artistic value.

Murals can have a dramatic impact whether consciously or subconsciously on the attitudes of passers by, when they are added to areas where people live and work. It can also be argued that the presence of large, public murals can add aesthetic improvement to the daily lives of residents or that of employees at a corporate venue.

Other world-famous murals can be found in Mexico, New York City, Philadelphia, Belfast, Derry, Los Angeles, Nicaragua, Cuba and in India. They have functioned as an important means of communication for members of socially, ethnically and racially divided communities in times of conflict. They also proved to be an effective tool in establishing a dialogue and hence solving the cleavage in the long run.
The Indian state Kerala has exclusive murals. These Kerala mural painting are on walls of Hindu temples. They can be dated from 9th century AD.

The San Bartolo murals of the Maya civilization in Guatemala, are the oldest example of this art in Mesoamerica and are dated at 300 BC.

Many rural towns have begun using murals to create tourist attractions in order to boost economic income. Colquitt, Georgia is one such town. Colquitt was chosen to host the 2010 Global Mural Conference. The town has more than twelve murals completed, and will host the Conference along with Dothan, Alabama, and Blakely, Georgia. In the summer of 2010, Colquitt will begin work on their Icon Mural.

The Mexican mural movement in the 1930s brought a new prominence to murals as a social and political tool. Diego Rivera, José Orozco and David Siqueiros were the most famous artists of the movement. Between 1932 and 1940, Rivera also painted murals in San Francisco, Detroit, and New York City. In 1933, he completed a famous series of twenty-seven fresco panels entitled "Detroit Industry" on the walls of an inner court at the Detroit Institute of Arts. During the McCarthyism of the 1950s, a was placed in the courtyard defending the artistic merit of the murals while attacking his politics as "detestable."

In 1948, the Colombian government hosted the IX Pan-American Conference to establish the Marshall plan for the Americas. The director of the OEA and the Colombian government commissioned master Santiago Martinez Delgado, to paint a mural in the Colombian congress building to commemorate the event. Martinez decided to make it about the Cúcuta Congress, and painted Bolívar in front of Santander, making liberals upset; so, due to the murder of Jorge Elieser Gaitan the mobs of el bogotazo tried to burn the capitol, but the Colombian Army stopped them. Years later, in the 1980s, with liberals in charge of the Congress, they passed a resolution to turn the whole chamber in the Elliptic Room 90 degrees to put the main mural on the side and commissioned Alejandro Obregon to paint a non-partisan mural in the surrealist style.

Northern Ireland contains some of the most famous political murals in the world. Almost 2,000 murals have been documented in Northern Ireland since the 1970s. In recent times, many murals are non-sectarian, concerning political and social issues such as racism and environmentalism, and many are completely apolitical, depicting children at play and scenes from everyday life. (See Northern Irish murals.)

A not political, but social related mural covers a wall in an old building, once a prison, at the top of a cliff in Bardiyah, in Libya. It was painted and signed by the artist in April 1942, weeks before his death on the first day of the First Battle of El Alamein. Known as the Bardia Mural, it was created by English artist, private John Frederick Brill.
In 1961 East Germany began to erect a wall between East and West Berlin, which became famous as the Berlin Wall. While on the East Berlin side painting was not allowed, artists painted on the Western side of the Wall from the 80s until the fall of the Wall in 1989.

Many unknown and known artists such as Thierry Noir and Keith Haring painted on the Wall, the "World's longest canvas". The sometimes detailed artwork were often painted over within hours or days. On the Western side the Wall was not protected, so everybody could paint on the Wall. After the fall of the Berlin Wall in 1989, the Eastern side of the Wall became also a popular "canvas" for many mural and graffiti artists.
Orgosolo, in Sardinia, is a most important center of murals politics.

It is also common for mural graffiti be used as a memoir. In the book "Somebody Told Me," Rick Bragg writes about a series of communities, mainly located in New York, that have walls dedicated to the people who died. These memorials, both written word and mural style, provide the deceased to be present in the communities in which they lived. Bragg states that the "murals have woven themselves in the fabric of the neighborhoods, and the city." These memorials remind people of the deaths caused by inner city violence.

Many people like to express their individuality by commissioning an artist to paint a mural in their home. This is not an activity exclusively for owners of large houses. A mural artist is only limited by the fee and therefore the time spent on the painting; dictating the level of detail; a simple mural can be added to the smallest of walls.

Private commissions can be for dining rooms, bathrooms, living rooms or, as is often the case- children's bedrooms. A child's room can be transformed into the 'fantasy world' of a forest or racing track, encouraging imaginative play and an awareness of art.

The current trend for feature walls has increased commissions for muralists in the UK. A large hand-painted mural can be designed on a specific theme, incorporate personal images and elements and may be altered during the course of painting it. The personal interaction between client and muralist is often a unique experience for an individual not usually involved in the arts.

In the 1980s, illusionary wall painting experienced a renaissance in private homes. The reason for this revival in interior design could, in some cases be attributed to the reduction in living space for the individual. Faux architectural features as well as natural scenery and views can have the effect of 'opening out' the walls. Densely built up areas of housing may also contribute to people's feelings of being cut off from nature in its free form. A mural commission of this sort may be an attempt by some people to re-establish a balance with nature.

Commissions of murals in schools, hospitals and retirement homes can achieve a pleasing and welcoming atmosphere in these caring institutions. Murals in other public buildings, such as public houses are also common.

Recently, graffiti and street art have played a key role in contemporary wall painting. Such graffiti/street artists as Keith Haring, Shepard Fairey, Above, Mint&Serf, Futura 2000, Os Gemeos, and Faile among others have successfully transcended their street art aesthetic beyond the walls of urban landscape and onto walls of private and corporate clients. As graffiti/street art became more mainstream in the late 1990s, youth oriented brands such as Nike and Red Bull, with Wieden Kennedy, have turned to graffiti/street artists to decorate walls of their respective offices. This trend continued through 2000's with graffiti/street art gaining more recognition from art institutions worldwide.

Many home owners choose to display the traditional art and culture of their society or events from their history in their homes. Ethnic murals have become an important form of interior decoration. Warli painting murals are becoming a preferred mode of wall decor in India. Warli painting is an ancient Indian art form in which the tribal people used to depict different phases of their life on the walls of their mud houses.

Tile murals are murals made out of stone, ceramic, porcelain, glass and or metal tiles that are installed within, or added onto the surface of an existing wall. They are also inlaid into floors. Mural tiles are painted, glazed, sublimation printed (as described below) or more traditionally cut or broken into pieces. Unlike the traditional painted murals described above, tile murals are always made with the use of tiles.

Mosaic murals are made by combining small 1/4" to 2" size pieces of colorful stone, ceramic, or glass tiles which are then laid out to create a picture. Modern day technology has allowed commercial mosaic mural makers to use computer programs to separate photographs into colors that are automatically cut and glued onto sheets of mesh creating precise murals fast and in large quantities.

The azulejo (, ) refers to a typical form of Portuguese or Spanish painted, tin-glazed, ceramic tilework. They have become a typical aspect of Portuguese culture, manifesting without interruption during five centuries, the consecutive trends in art.

Azulejos can be found inside and outside churches, palaces, ordinary houses and even railway stations or subway stations.

They were not only used as an ornamental art form, but also had a specific functional capacity like temperature control in homes. Many azulejos chronicle major historical and cultural aspects of Portuguese history.

Custom-printed tile murals can be produced using digital images for kitchen splashbacks, wall displays, and flooring. Digital photos and artwork can be resized and printed to accommodate the desired size for the area to be decorated. Custom tile printing uses a variety of techniques including dye sublimation and ceramic-type laser toners. The latter technique can yield fade-resistant custom tiles which are suitable for long term exterior exposure.





</doc>
<doc id="18819" url="https://en.wikipedia.org/wiki?curid=18819" title="Microeconomics">
Microeconomics

Microeconomics (from Greek prefix "mikro-" meaning "small") is a branch of economics that studies the behavior of individuals and firms in making decisions regarding the allocation of scarce resources and the interactions among these individuals and firms.

One goal of microeconomics is to analyze the market mechanisms that establish relative prices among goods and services and allocate limited resources among alternative uses. Microeconomics shows conditions under which free markets lead to desirable allocations. It also analyzes market failure, where markets fail to produce efficient results.

Microeconomics stands in contrast to macroeconomics, which involves "the sum total of economic activity, dealing with the issues of growth, inflation, and unemployment and with national policies relating to these issues". Microeconomics also deals with the effects of economic policies (such as changing taxation levels) on the aforementioned aspects of the economy. Particularly in the wake of the Lucas critique, much of modern macroeconomic theory has been built upon microfoundations—i.e. based upon basic assumptions about micro-level behavior.

Microeconomic theory typically begins with the study of a single rational and utility maximizing individual. To economists, rationality means an individual possesses stable preferences that are both complete and transitive. 

The technical assumption that preference relations are continuous is needed to ensure the existence of a utility function. Although microeconomic theory can continue without this assumption, it would make comparative statics impossible since there is no guarantee that the resulting utility function would be differentiable.

Microeconomic theory progresses by defining a competitive budget set which is a subset of the consumption set. It is at this point that economists make The technical assumption that preferences are locally non-satiated. Without the assumption of LNS (local non-satiation) there is no guarantee that a rational individual would maximize utility. With the necessary tools and assumptions in place the utility maximization problem (UMP) is developed.

The utility maximization problem is the heart of consumer theory. The utility maximization problem attempts to explain the action axiom by imposing rationality axioms on consumer preferences and then mathematically modeling and analyzing the consequences. The utility maximization problem serves not only as the mathematical foundation of consumer theory but as a metaphysical explanation of it as well. That is, the utility maximization problem is used by economists to not only explain "what" or "how" individuals make choices but "why" individuals make choices as well.

The utility maximization problem is a constrained optimization problem in which an individual seeks to maximize utility subject to a budget constraint. Economists use the extreme value theorem to guarantee that a solution to the utility maximization problem exists. That is, since the budget constraint is both bounded and closed, a solution to the utility maximization problem exists. Economists call the solution to the utility maximization problem a Walrasian demand function or correspondence.

The utility maximization problem has so far been developed by taking consumer tastes (i.e. consumer utility) as the primitive. However, an alternative way to develop microeconomic theory is by taking consumer choice as the primitive. This model of microeconomic theory is referred to as revealed preference theory.

The theory of supply and demand usually assumes that markets are perfectly competitive. This implies that there are many buyers and sellers in the market and none of them have the capacity to significantly influence prices of goods and services. In many real-life transactions, the assumption fails because some individual buyers or sellers have the ability to influence prices. Quite often, a sophisticated analysis is required to understand the demand-supply equation of a good model. However, the theory works well in situations meeting these assumptions.

Mainstream economics does not assume "a priori" that markets are preferable to other forms of social organization. In fact, much analysis is devoted to cases where market failures lead to resource allocation that is suboptimal and creates deadweight loss. A classic example of suboptimal resource allocation is that of a public good. In such cases, economists may attempt to find policies that avoid waste, either directly by government control, indirectly by regulation that induces market participants to act in a manner consistent with optimal welfare, or by creating "missing markets" to enable efficient trading where none had previously existed.

This is studied in the field of collective action and public choice theory. "Optimal welfare" usually takes on a Paretian norm, which is a mathematical application of the Kaldor–Hicks method. This can diverge from the Utilitarian goal of maximizing utility because it does not consider the distribution of goods between people. Market failure in positive economics (microeconomics) is limited in implications without mixing the belief of the economist and their theory.

The demand for various commodities by individuals is generally thought of as the outcome of a utility-maximizing process, with each individual trying to maximize their own utility under a budget constraint and a given consumption set.

The study of microeconomics involves several "key" areas:

Supply and demand is an economic model of price determination in a perfectly competitive market. It concludes that in a perfectly competitive market with no externalities, per unit taxes, or price controls, the unit price for a particular good is the price at which the quantity demanded by consumers equals the quantity supplied by producers. This price results in a stable economic equilibrium.

Elasticity is the measurement of how responsive an economic variable is to a change in another variable. Elasticity can be quantified as the ratio of the change in one variable to the change in another variable, when the later variable has a causal influence on the former. It is a tool for measuring the responsiveness of a variable, or of the function that determines it, to changes in causative variables in unitless ways. Frequently used elasticities include price elasticity of demand, price elasticity of supply, income elasticity of demand, elasticity of substitution or constant elasticity of substitution between factors of production and elasticity of intertemporal substitution.

Consumer demand theory relates preferences for the consumption of both goods and services to the consumption expenditures; ultimately, this relationship between preferences and consumption expenditures is used to relate preferences to consumer demand curves. The link between personal preferences, consumption and the demand curve is one of the most closely studied relations in economics. It is a way of analyzing how consumers may achieve equilibrium between preferences and expenditures by maximizing utility subject to consumer budget constraints.

Production theory is the study of production, or the economic process of converting inputs into outputs. Production uses resources to create a good or service that is suitable for use, gift-giving in a gift economy, or exchange in a market economy. This can include manufacturing, storing, shipping, and packaging. Some economists define production broadly as all economic activity other than consumption. They see every commercial activity other than the final purchase as some form of production.

The cost-of-production theory of value states that the price of an object or condition is determined by the sum of the cost of the resources that went into making it. The cost can comprise any of the factors of production: labour, capital, land. Technology can be viewed either as a form of fixed capital (e.g.
plant) or circulating capital (e.g.
intermediate goods).

The economic idea of opportunity cost is closely related to the idea of time constraints. You can do only one thing at a time, which means that, inevitably, you’re always giving up other things.

The opportunity cost of any activity is the value of the next-best alternative thing you may have done instead. Opportunity cost depends only on the value of the next-best alternative. It doesn’t matter whether you have 5 alternatives or 5,000.

Opportunity costs can tell you when "not" to do something as well as when to do something. For example, you may like waffles, but you like chocolate even more. If someone offers you only waffles, you’re going to take it. But if you’re offered waffles or chocolate, you’re going to take the chocolate. The opportunity cost of eating waffles is sacrificing the chance to eat chocolate. Because the cost of not eating the chocolate is higher than the benefits of eating the waffles, it makes no sense to choose waffles. Of course, if you choose chocolate, you’re still faced with the opportunity cost of giving up having waffles. But you’re willing to do that because the waffle's opportunity cost is lower than the benefits of the chocolate. Opportunity costs are unavoidable constraints on behaviour because you have to decide what’s best and give up the next-best alternative.

The market structure can have several types of interacting market systems. 
Different forms of markets are a feature of capitalism, and advocates of socialism often criticize markets and aim to substitute markets with economic planning to varying degrees. Competition is the regulatory mechanism of the market system.

Some examples of markets:

Perfect competition is a situation in which numerous small firms producing identical products compete against each other in a given industry. Perfect competition leads to firms producing the socially optimal output level at the minimum possible cost per unit. Firms in perfect competition are "price takers" (they do not have enough market power to profitably increase the price of their goods or services). A good example would be that of digital marketplaces, such as eBay, on which many different sellers sell similar products to many different buyers.

In economic theory, imperfect competition is a type of market structure showing some but not all features of competitive markets.

Monopolistic competition is a situation in which many firms with slightly different products compete. Production costs are above what may be achieved by perfectly competitive firms, but society benefits from the product differentiation. Examples of industries with market structures similar to monopolistic competition include restaurants, cereal, clothing, shoes, and service industries in large cities.

A monopoly (from Greek "monos" μόνος (alone or single) + "polein" πωλεῖν (to sell)) is a market structure in which a market or industry is dominated by a single supplier of a particular good or service. Because monopolies have no competition they tend to sell goods and services at a higher price and produce below the socially optimal output level. Although not all monopolies are a bad thing, especially in industries where multiple firms would result in more problems than benefits (i.e. natural monopolies).


An oligopoly is a market structure in which a market or industry is dominated by a small number of firms (oligopolists). Oligopolies can create the incentive for firms to engage in collusion and form cartels that reduce competition leading to higher prices for consumers and less overall market output. Alternatively, oligopolies can be fiercely competitive and engage in flamboyant advertising campaigns.


A monopsony is a market where there is only one buyer and many sellers.

An oligopsony is a market where there are a few buyers and many sellers.

Game theory is a major method used in mathematical economics and business for modeling competing behaviors of interacting agents. The term "game" here implies the study of any strategic interaction between people. Applications include a wide array of economic phenomena and approaches, such as auctions, bargaining, mergers & acquisitions pricing, fair division, duopolies, oligopolies, social network formation, agent-based computational economics, general equilibrium, mechanism design, and voting systems, and across such broad areas as experimental economics, behavioral economics, information economics, industrial organization, and political economy.

Labor economics seeks to understand the functioning and dynamics of the markets for wage labor. Labor markets function through the interaction of workers and employers. Labor economics looks at the suppliers of labor services (workers), the demands of labor services (employers), and attempts to understand the resulting pattern of wages, employment, and income. In economics, labor is a measure of the work done by human beings. It is conventionally contrasted with such other factors of production as land and capital. There are theories which have developed a concept called human capital (referring to the skills that workers possess, not necessarily their actual work), although there are also counter posing macro-economic system theories that think human capital is a contradiction in terms.

Welfare economics is a branch of economics that uses microeconomics techniques to evaluate well-being from allocation of productive factors as to desirability and economic efficiency within an economy, often relative to competitive general equilibrium. It analyzes "social welfare", however measured, in terms of economic activities of the individuals that compose the theoretical society considered. Accordingly, individuals, with associated economic activities, are the basic units for aggregating to social welfare, whether of a group, a community, or a society, and there is no "social welfare" apart from the "welfare" associated with its individual units.

Information economics or the economics of information is a branch of microeconomic theory that studies how information and information systems affect an economy and economic decisions. Information has special characteristics. It is easy to create but hard to trust. It is easy to spread but hard to control. It influences many decisions. These special characteristics (as compared with other types of goods) complicate many standard economic theories.

Applied microeconomics includes a range of specialized areas of study, many of which draw on methods from other fields. Industrial organization examines topics such as the entry and exit of firms, innovation, and the role of trademarks. Labor economics examines wages, employment, and labor market dynamics. Financial economics examines topics such as the structure of optimal portfolios, the rate of return to capital, econometric analysis of security returns, and corporate financial behavior. Public economics examines the design of government tax and expenditure policies and economic effects of these policies (e.g., social insurance programs). Political economy examines the role of political institutions in determining policy outcomes. Health economics examines the organization of health care systems, including the role of the health care workforce and health insurance programs. Education economics examines the organization of education provision and its implication for efficiency and equity, including the effects of education on productivity. Urban economics, which examines the challenges faced by cities, such as sprawl, air and water pollution, traffic congestion, and poverty, draws on the fields of urban geography and sociology. Law and economics applies microeconomic principles to the selection and enforcement of competing legal regimes and their relative efficiencies. Economic history examines the evolution of the economy and economic institutions, using methods and techniques from the fields of economics, history, geography, sociology, psychology, and political science.

The difference between microeconomics and macroeconomics was introduced in 1933 by the Norwegian economist Ragnar Frisch (Nobel Prize 1969).




</doc>
<doc id="18820" url="https://en.wikipedia.org/wiki?curid=18820" title="Macroeconomics">
Macroeconomics

Macroeconomics (from the Greek prefix "makro-" meaning "large" and economics) is a branch of economics dealing with the performance, structure, behavior, and decision-making of an economy as a whole. This includes regional, national, and global economies. 

Macroeconomists study aggregated indicators such as GDP, unemployment rates, national income, price indices, and the interrelations among the different sectors of the economy to better understand how the whole economy functions. 

Macroeconomists develop models that explain the relationship between such factors as national income, output, consumption, unemployment, inflation, savings, investment, international trade and international finance.

While macroeconomics is a broad field of study, there are two areas of research that are emblematic of the discipline: the attempt to understand the causes and consequences of short-run fluctuations in national income (the business cycle), and the attempt to understand the determinants of long-run economic growth (increases in national income). Macroeconomic models and their forecasts are used by governments to assist in the development and evaluation of economic policy.

Macroeconomics and microeconomics, a pair of terms coined by Ragnar Frisch, are the two most general fields in economics. In contrast to macroeconomics, microeconomics is the branch of economics that studies the behavior of individuals and firms in making decisions and the interactions among these individuals and firms in narrowly-defined markets.

Macroeconomics encompasses a variety of concepts and variables, but there are three central topics for macroeconomic research. Macroeconomic theories usually relate the phenomena of output, unemployment, and inflation. Outside of macroeconomic theory, these topics are also important to all economic agents including workers, consumers, and producers.

National output is the total amount of everything a country produces in a given period of time. Everything that is produced and sold generates an equal amount of income. Therefore, output and income are usually considered equivalent and the two terms are often used interchangeably. Output can be measured as total income, or it can be viewed from the production side and measured as the total value of final goods and services or the sum of all value added in the economy.

Macroeconomic output is usually measured by gross domestic product (GDP) or one of the other national accounts. Economists interested in long-run increases in output study economic growth. Advances in technology, accumulation of machinery and other capital, and better education and human capital are all factors that lead to increased economic output over time. However, output does not always increase consistently over time. Business cycles can cause short-term drops in output called recessions. Economists look for macroeconomic policies that prevent economies from slipping into recessions and that lead to faster long-term growth.

The amount of unemployment in an economy is measured by the unemployment rate, i.e. the percentage of workers without jobs in the labor force. The unemployment rate in the labor force only includes workers actively looking for jobs. People who are retired, pursuing education, or discouraged from seeking work by a lack of job prospects are excluded.

Unemployment can be generally broken down into several types that are related to different causes.

A general price increase across the entire economy is called inflation. When prices decrease, there is deflation. Economists measure these changes in prices with price indexes. Inflation can occur when an economy becomes overheated and grows too quickly. Similarly, a declining economy can lead to deflation.

Central bankers, who manage a country's money supply, try to avoid changes in price level by using monetary policy. Raising interest rates or reducing the supply of money in an economy will reduce inflation. Inflation can lead to increased uncertainty and other negative consequences. Deflation can lower economic output. Central bankers try to stabilize prices to protect economies from the negative consequences of price changes.

Changes in price level may be the result of several factors. The quantity theory of money holds that changes in price level are directly related to changes in the money supply. Most economists believe that this relationship explains long-run changes in the price level. Short-run fluctuations may also be related to monetary factors, but changes in aggregate demand and aggregate supply can also influence price level. For example, a decrease in demand due to a recession can lead to lower price levels and deflation. A negative supply shock, such as an oil crisis, lowers aggregate supply and can cause inflation.

The AD-AS model has become the standard textbook model for explaining the macroeconomy. This model shows the price level and level of real output given the equilibrium in aggregate demand and aggregate supply. The aggregate demand curve's downward slope means that more output is demanded at lower price levels. The downward slope is the result of three effects: the Pigou or real balance effect, which states that as real prices fall, real wealth increases, resulting in higher consumer demand of goods; the Keynes or interest rate effect, which states that as prices fall, the demand for money decreases, causing interest rates to decline and borrowing for investment and consumption to increase; and the net export effect, which states that as prices rise, domestic goods become comparatively more expensive to foreign consumers, leading to a decline in exports.

In the conventional Keynesian use of the AS-AD model, the aggregate supply curve is horizontal at low levels of output and becomes inelastic near the point of potential output, which corresponds with full employment. Since the economy cannot produce beyond the potential output, any AD expansion will lead to higher price levels instead of higher output.

The AD–AS diagram can model a variety of macroeconomic phenomena, including inflation. Changes in the non-price level factors or determinants cause changes in aggregate demand and shifts of the entire aggregate demand (AD) curve. When demand for goods exceeds supply there is an inflationary gap where demand-pull inflation occurs and the AD curve shifts upward to a higher price level. When the economy faces higher costs, cost-push inflation occurs and the AS curve shifts upward to higher price levels. The AS–AD diagram is also widely used as a pedagogical tool to model the effects of various macroeconomic policies.

The IS–LM model represents all the combinations of interest rates and output that ensure the equilibrium in the goods and money markets. The goods market is represented by the equilibrium in investment and saving (IS), and the money market is represented by the equilibrium between the money supply and liquidity preference. The IS curve consists of the points where investment, given the interest rate, is equal to savings, given output.

The IS curve is downward sloping because output and interest rate have an inverse relationship in the goods market: as output increases, more money is saved, which means interest rates must be lower to spur enough investment to match savings. The LM curve is upward sloping because interest rate and output have a positive relationship in the money market: as output increases, the demand for money increases, resulting in a rise in interest rate.

The IS/LM model is often used to demonstrate the effects of monetary and fiscal policy. Textbooks frequently use the IS/LM model, but it does not feature the complexities of most modern macroeconomic models. Nevertheless, these models still feature similar relationships to those in IS/LM.

The neoclassical growth model of Robert Solow has become a common textbook model for explaining economic growth in the long-run. The model begins with a production function where national output is the product of two inputs: capital and labor. The Solow model assumes that labor and capital are used at constant rates without the fluctuations in unemployment and capital utilization commonly seen in business cycles.

An increase in output, or economic growth, can only occur because of an increase in the capital stock, a larger population, or technological advancements that lead to higher productivity (total factor productivity). An increase in the savings rate leads to a temporary increase as the economy creates more capital, which adds to output. However, eventually the depreciation rate will limit the expansion of capital: savings will be used up replacing depreciated capital, and no savings will remain to pay for an additional expansion in capital. Solow's model suggests that economic growth in terms of output per capita depends solely on technological advances that enhance productivity.

In the 1980s and 1990s endogenous growth theory arose to challenge neoclassical growth theory. This group of models explains economic growth through other factors, such as increasing returns to scale for capital and learning-by-doing, that are endogenously determined instead of the exogenous technological improvement used to explain growth in Solow's model.

Macroeconomic policy is usually implemented through two sets of tools: fiscal and monetary policy. Both forms of policy are used to stabilize the economy, which can mean boosting the economy to the level of GDP consistent with full employment. Macroeconomic policy focuses on limiting the effects of the business cycle to achieve the economic goals of price stability, full employment, and growth. 

Central banks implement monetary policy by controlling the money supply through several mechanisms. Typically, central banks take action by issuing money to buy bonds (or other assets), which boosts the supply of money and lowers interest rates, or, in the case of contractionary monetary policy, banks sell bonds and take money out of circulation. Usually policy is not implemented by directly targeting the supply of money.

Central banks continuously shift the money supply to maintain a targeted fixed interest rate. Some of them allow the interest rate to fluctuate and focus on targeting inflation rates instead. Central banks generally try to achieve high output without letting loose monetary policy that create large amounts of inflation.

Conventional monetary policy can be ineffective in situations such as a liquidity trap. When interest rates and inflation are near zero, the central bank cannot loosen monetary policy through conventional means.

Central banks can use unconventional monetary policy such as quantitative easing to help increase output. Instead of buying government bonds, central banks can implement quantitative easing by buying not only government bonds, but also other assets such as corporate bonds, stocks, and other securities. This allows lower interest rates for a broader class of assets beyond government bonds. In another example of unconventional monetary policy, the United States Federal Reserve recently made an attempt at such a policy with Operation Twist. Unable to lower current interest rates, the Federal Reserve lowered long-term interest rates by buying long-term bonds and selling short-term bonds to create a flat yield curve.

Fiscal policy is the use of government's revenue and expenditure as instruments to influence the economy. Examples of such tools are expenditure, taxes, debt.

For example, if the economy is producing less than potential output, government spending can be used to employ idle resources and boost output. Government spending does not have to make up for the entire output gap. There is a multiplier effect that boosts the impact of government spending. For instance, when the government pays for a bridge, the project not only adds the value of the bridge to output, but also allows the bridge workers to increase their consumption and investment, which helps to close the output gap.

The effects of fiscal policy can be limited by crowding out. When the government takes on spending projects, it limits the amount of resources available for the private sector to use. Crowding out occurs when government spending simply replaces private sector output instead of adding additional output to the economy. Crowding out also occurs when government spending raises interest rates, which limits investment. Defenders of fiscal stimulus argue that crowding out is not a concern when the economy is depressed, plenty of resources are left idle, and interest rates are low.

Fiscal policy can be implemented through automatic stabilizers. Automatic stabilizers do not suffer from the policy lags of discretionary fiscal policy. Automatic stabilizers use conventional fiscal mechanisms but take effect as soon as the economy takes a downturn: spending on unemployment benefits automatically increases when unemployment rises and, in a progressive income tax system, the effective tax rate automatically falls when incomes decline.

Economists usually favor monetary over fiscal policy because it has two major advantages. First, monetary policy is generally implemented by independent central banks instead of the political institutions that control fiscal policy. Independent central banks are less likely to make decisions based on political motives. Second, monetary policy suffers shorter inside lags and outside lags than fiscal policy. Central banks can quickly make and implement decisions while discretionary fiscal policy may take time to pass and even longer to carry out.

Macroeconomics descended from the once divided fields of business cycle theory and monetary theory. The quantity theory of money was particularly influential prior to World War II. It took many forms, including the version based on the work of Irving Fisher:

In the typical view of the quantity theory, money velocity (V) and the quantity of goods produced (Q) would be constant, so any increase in money supply (M) would lead to a direct increase in price level (P). The quantity theory of money was a central part of the classical theory of the economy that prevailed in the early twentieth century.

Ludwig Von Mises's work "Theory of Money and Credit", published in 1912, was one of the first books from the Austrian School to deal with macroeconomic topics.

Macroeconomics, at least in its modern form, began with the publication of John Maynard Keynes's "General Theory of Employment, Interest and Money". When the Great Depression struck, classical economists had difficulty explaining how goods could go unsold and workers could be left unemployed. In classical theory, prices and wages would drop until the market cleared, and all goods and labor were sold. Keynes offered a new theory of economics that explained why markets might not clear, which would evolve (later in the 20th century) into a group of macroeconomic schools of thought known as Keynesian economics – also called Keynesianism or Keynesian theory.

In Keynes's theory, the quantity theory broke down because people and businesses tend to hold on to their cash in tough economic times – a phenomenon he described in terms of liquidity preferences. Keynes also explained how the multiplier effect would magnify a small decrease in consumption or investment and cause declines throughout the economy. Keynes also noted the role uncertainty and animal spirits can play in the economy.

The generation following Keynes combined the macroeconomics of the "General Theory" with neoclassical microeconomics to create the neoclassical synthesis. By the 1950s, most economists had accepted the synthesis view of the macroeconomy. Economists like Paul Samuelson, Franco Modigliani, James Tobin, and Robert Solow developed formal Keynesian models and contributed formal theories of consumption, investment, and money demand that fleshed out the Keynesian framework.

Milton Friedman updated the quantity theory of money to include a role for money demand. He argued that the role of money in the economy was sufficient to explain the Great Depression, and that aggregate demand oriented explanations were not necessary. Friedman also argued that monetary policy was more effective than fiscal policy; however, Friedman doubted the government's ability to "fine-tune" the economy with monetary policy. He generally favored a policy of steady growth in money supply instead of frequent intervention.

Friedman also challenged the Phillips curve relationship between inflation and unemployment. Friedman and Edmund Phelps (who was not a monetarist) proposed an "augmented" version of the Phillips curve that excluded the possibility of a stable, long-run tradeoff between inflation and unemployment. When the oil shocks of the 1970s created a high unemployment and high inflation, Friedman and Phelps were vindicated. Monetarism was particularly influential in the early 1980s. Monetarism fell out of favor when central banks found it difficult to target money supply instead of interest rates as monetarists recommended. Monetarism also became politically unpopular when the central banks created recessions in order to slow inflation.

New classical macroeconomics further challenged the Keynesian school. A central development in new classical thought came when Robert Lucas introduced rational expectations to macroeconomics. Prior to Lucas, economists had generally used adaptive expectations where agents were assumed to look at the recent past to make expectations about the future. Under rational expectations, agents are assumed to be more sophisticated. A consumer will not simply assume a 2% inflation rate just because that has been the average the past few years; she will look at current monetary policy and economic conditions to make an informed forecast. When new classical economists introduced rational expectations into their models, they showed that monetary policy could only have a limited impact.

Lucas also made an influential critique of Keynesian empirical models. He argued that forecasting models based on empirical relationships would keep producing the same predictions even as the underlying model generating the data changed. He advocated models based on fundamental economic theory that would, in principle, be structurally accurate as economies changed. Following Lucas's critique, new classical economists, led by Edward C. Prescott and Finn E. Kydland, created real business cycle (RBC) models of the macroeconomy.

RBC models were created by combining fundamental equations from neo-classical microeconomics. In order to generate macroeconomic fluctuations, RBC models explained recessions and unemployment with changes in technology instead of changes in the markets for goods or money. Critics of RBC models argue that money clearly plays an important role in the economy, and the idea that technological regress can explain recent recessions is implausible. However, technological shocks are only the more prominent of a myriad of possible shocks to the system that can be modeled. Despite questions about the theory behind RBC models, they have clearly been influential in economic methodology.

New Keynesian economists responded to the new classical school by adopting rational expectations and focusing on developing micro-founded models that are immune to the Lucas critique. Stanley Fischer and John B. Taylor produced early work in this area by showing that monetary policy could be effective even in models with rational expectations when contracts locked in wages for workers. Other new Keynesian economists, including Olivier Blanchard, Julio Rotemberg, Greg Mankiw, David Romer, and Michael Woodford, expanded on this work and demonstrated other cases where inflexible prices and wages led to monetary and fiscal policy having real effects.

Like classical models, new classical models had assumed that prices would be able to adjust perfectly and monetary policy would only lead to price changes. New Keynesian models investigated sources of sticky prices and wages due to imperfect competition, which would not adjust, allowing monetary policy to impact quantities instead of prices.

By the late 1990s economists had reached a rough consensus. The nominal rigidity of new Keynesian theory was combined with rational expectations and the RBC methodology to produce dynamic stochastic general equilibrium (DSGE) models. The fusion of elements from different schools of thought has been dubbed the new neoclassical synthesis. These models are now used by many central banks and are a core part of contemporary macroeconomics.

New Keynesian economics, which developed partly in response to new classical economics, strives to provide microeconomic foundations to Keynesian economics by showing how imperfect markets can justify demand management.




</doc>
<doc id="18823" url="https://en.wikipedia.org/wiki?curid=18823" title="Mary Pickford">
Mary Pickford

Gladys Louise Smith (April 8, 1892 – May 29, 1979), known professionally as Mary Pickford, was a Canadian-born film actress and producer. She was a co-founder of both the Pickford-Fairbanks Studio (along with Douglas Fairbanks) and, later, the United Artists film studio (with Fairbanks, Charlie Chaplin and D.W. Griffith), and one of the original 36 founders of the Academy of Motion Picture Arts and Sciences who present the yearly "Oscar" award ceremony.

Pickford was known in her prime as "America's Sweetheart" and the "girl with the curls". She was one of the Canadian pioneers in early Hollywood and a significant figure in the development of film acting. Pickford was one of the earliest stars to be billed under her own name, and was one of the most popular actresses of the 1910s and 1920s, earning the nickname "Queen of the Movies". She is credited as having defined the ingénue archetype in cinema. 

She was awarded the second ever Academy Award for Best Actress for her first sound-film role in "Coquette" (1929) and also received an honorary Academy Award in 1976. In consideration of her contributions to American cinema, the American Film Institute ranked Pickford as 24th in its 1999 list of greatest female stars of classic Hollywood Cinema.

Mary Pickford was born Gladys Louise Smith in 1892 (although she would later claim 1893 or 1894 as her year of birth) at 211 University Avenue, Toronto, Ontario. Her father, John Charles Smith, was the son of English Methodist immigrants, and worked a variety of odd jobs. Her mother, Charlotte Hennessey, was of Irish Catholic descent and worked for a time as a seamstress. She had two younger siblings, Charlotte, called "Lottie" (born 1893), and John Charles, called "Jack" (born 1896), who also became actors. To please her husband's relatives, Pickford's mother baptized her children as Methodists, the religion of their father. John Charles Smith was an alcoholic; he abandoned the family and died on February 11, 1898, from a fatal blood clot caused by a workplace accident when he was a purser with Niagara Steamship.

When Gladys was age four, her household was under infectious quarantine, a public health measure. Their devoutly Catholic maternal grandmother (Catherine Faeley Hennessey) asked a visiting Roman Catholic priest to baptize the children. Pickford was at this time baptized as Gladys Marie Smith.

Charlotte Smith began taking in boarders after being widowed. One of these was a theatrical stage manager. At his suggestion, Gladys (age 7) was given two small roles, one as a boy and the other as a girl, in a stock company production of "The Silver King" at Toronto's Princess Theatre. She subsequently acted in many melodramas with Toronto's Valentine Company, finally playing the major child role in their version of "The Silver King". She capped her short career in Toronto with the starring role of Little Eva in their production of "Uncle Tom's Cabin" adapted from the 1852 novel.

By the early 1900s, theatre had become a family enterprise. Gladys, her mother and two younger siblings toured the United States by rail, performing in third-rate companies and plays. After six impoverished years, Pickford allowed one more summer to land a leading role on Broadway, planning to quit acting if she failed. In 1906 Gladys, Lottie and Jack Smith supported singer Chauncey Olcott on Broadway in "Edmund Burke". Gladys finally landed a supporting role in a 1907 Broadway play, "The Warrens of Virginia". The play was written by William C. deMille, whose brother, Cecil, appeared in the cast. David Belasco, the producer of the play, insisted that Gladys Smith assume the stage name Mary Pickford. After completing the Broadway run and touring the play, however, Pickford was again out of work.

On April 19, 1909, the Biograph Company director D. W. Griffith screen-tested her at the company's New York studio for a role in the nickelodeon film, "Pippa Passes". The role went to someone else but Griffith was immediately taken with Pickford. She quickly grasped that movie acting was simpler than the stylized stage acting of the day. Most Biograph actors earned $5 a day but, after Pickford's single day in the studio, Griffith agreed to pay her $10 a day against a guarantee of $40 a week.

Pickford, like all actors at Biograph, played both bit parts and leading roles, including mothers, ingenues, charwomen, spitfires, slaves, Native Americans, spurned women, and a prostitute. As Pickford said of her success at Biograph:I played scrubwomen and secretaries and women of all nationalities ... I decided that if I could get into as many pictures as possible, I'd become known, and there would be a demand for my work. She appeared in 51 films in 1909 – almost one a week. While at Biograph, she suggested to Florence La Badie to "try pictures", invited her to the studio and later introduced her to D. W. Griffith, who launched La Badie's career.

In January 1910, Pickford traveled with a Biograph crew to Los Angeles. Many other film companies wintered on the West Coast, escaping the weak light and short days that hampered winter shooting in the East. Pickford added to her 1909 Biographs ("Sweet and Twenty", "They Would Elope," and "To Save Her Soul", to name a few) with films made in California.

Actors were not listed in the credits in Griffith's company. Audiences noticed and identified Pickford within weeks of her first film appearance. Exhibitors in turn capitalized on her popularity by advertising on sandwich boards that a film featuring "The Girl with the Golden Curls", "Blondilocks", or "The Biograph Girl" was inside.

Pickford left Biograph in December 1910. The following year, she starred in films at Carl Laemmle's Independent Moving Pictures Company (IMP). IMP was absorbed into Universal Pictures in 1912, along with Majestic. Unhappy with their creative standards, Pickford returned to work with Griffith in 1912. Some of her best performances were in his films, such as "Friends", "The Mender of Nets", "Just Like a Woman", and "The Female of the Species". That year Pickford also introduced Dorothy and Lillian Gish (both friends from her days in touring melodrama) to Griffith. Both became major silent stars, in comedy and tragedy, respectively. Pickford made her last Biograph picture, "The New York Hat," in late 1912.

She returned to Broadway in the David Belasco production of "A Good Little Devil" (1912). This was a major turning point in her career. Pickford, who had always hoped to conquer the Broadway stage, discovered how deeply she missed film acting. In 1913, she decided to work exclusively in film. The previous year, Adolph Zukor had formed Famous Players in Famous Plays. It was later known as Famous Players-Lasky and then Paramount Pictures, one of the first American feature film companies.
Pickford left the stage to join Zukor's roster of stars. Zukor believed film's potential lay in recording theatrical players in replicas of their most famous stage roles and productions. Zukor first filmed Pickford in a silent version of "A Good Little Devil". The film, produced in 1913, showed the play's Broadway actors reciting every line of dialogue, resulting in a stiff film that Pickford later called "one of the worst [features] I ever made ... it was deadly". Zukor agreed; he held the film back from distribution for a year.

Pickford's work in material written for the camera by that time had attracted a strong following. Comedy-dramas, such as "In the Bishop's Carriage" (1913), "Caprice" (1913), and especially "Hearts Adrift" (1914), made her irresistible to moviegoers. "Hearts Adrift" was so popular that Pickford asked for the first of her many publicized pay raises based on the profits and reviews. The film marked the first time Pickford’s name was featured above the title on movie marquees. "Tess of the Storm Country" was released five weeks later. Biographer Kevin Brownlow observed that the film "sent her career into orbit and made her the most popular actress in America, if not the world".

Her appeal was summed up two years later by the February 1916 issue of "Photoplay" as "luminous tenderness in a steel band of gutter ferocity". Only Charlie Chaplin, who slightly surpassed Pickford's popularity in 1916, had a similarly spellbinding pull with critics and the audience. Each enjoyed a level of fame far exceeding that of other actors. Throughout the 1910s and 1920s, Pickford was believed to be the most famous woman in the world, or, as a silent-film journalist described her, "the best known woman who has ever lived, the woman who was known to more people and loved by more people than any other woman that has been in all history".

 Pickford starred in 52 features throughout her career. On June 24, 1916, Pickford signed a new contract with Zukor that granted her full authority over production of the films in which she starred, and a record-breaking salary of $10,000 a week. In addition, Pickford's compensation was half of a film's profits, with a guarantee of $1,040,000 (US$ in 2018). Occasionally, she played a child, in films such as "The Poor Little Rich Girl" (1917), "Rebecca of Sunnybrook Farm" (1917), "Daddy-Long-Legs" (1919) and "Pollyanna" (1920). Pickford's fans were devoted to these "little girl" roles, but they were not typical of her career.

In August 1918, Pickford's contract expired and, when refusing Zukor's terms for a renewal, she was offered $250,000 to leave the motion picture business. She declined, and went to First National Pictures, which agreed to her terms. In 1919, Pickford, along with D.W. Griffith, Charlie Chaplin, and Douglas Fairbanks, formed the independent film production company United Artists. Through United Artists, Pickford continued to produce and perform in her own movies; she could also distribute them as she chose. In 1920, Pickford's film "Pollyanna" grossed around $1,100,000. The following year, Pickford's film "Little Lord Fauntleroy" was also a success, and in 1923, "Rosita" grossed over $1,000,000 as well. During this period, she also made "Little Annie Rooney" (1925), another film in which Pickford played a child, "Sparrows" (1926), which blended the Dickensian with newly minted German expressionist style, and "My Best Girl" (1927), a romantic comedy featuring her future husband Buddy Rogers.

The arrival of sound was her undoing. Pickford underestimated the value of adding sound to movies, claiming that "adding sound to movies would be like putting lipstick on the Venus de Milo".

She played a reckless socialite in "Coquette" (1929), a role for which her famous ringlets were cut into a 1920s' bob. Pickford had already cut her hair in the wake of her mother's death in 1928. Fans were shocked at the transformation. Pickford's hair had become a symbol of female virtue, and when she cut it, the act made front-page news in "The New York Times" and other papers. "Coquette" was a success and won her an Academy Award for Best Actress, although this was highly controversial. The public failed to respond to her in the more sophisticated roles. Like most movie stars of the silent era, Pickford found her career fading as talkies became more popular among audiences.

Her next film, "The Taming of The Shrew", made with husband Douglas Fairbanks, was not well received at the box office. Established Hollywood actors were panicked by the impending arrival of the talkies. On March 29, 1928, "The Dodge Brothers Hour" was broadcast from Pickford's bungalow, featuring Fairbanks, Chaplin, Norma Talmadge, Gloria Swanson, John Barrymore, D.W. Griffith, and Dolores del Rio, among others. They spoke on the radio show to prove that they could meet the challenge of talking movies.

A transition in the roles Pickford selected came when she was in her late 30s, no longer able to play the children, teenage spitfires, and feisty young women so adored by her fans, and was not suited for the glamorous and vampish heroines of early sound. In 1933, she underwent a Technicolor screen test for an animated/live action film version of "Alice in Wonderland", but Walt Disney discarded the project when Paramount released its own version of the book. Only one Technicolor still of her screen test still exists. She retired from acting in 1933; her last acting film was released in 1934. She continued to produce for others, however, including "Sleep, My Love" (1948; with Claudette Colbert) and "Love Happy" (1949), with the Marx Brothers).

Pickford used her stature in the movie industry to promote a variety of causes. Although her image depicted fragility and innocence, Pickford proved to be a worthy businesswoman who took control of her career in a cutthroat industry.

During World War I, she promoted the sale of Liberty Bonds, making an intensive series of fund-raising speeches that kicked off in Washington, D.C., where she sold bonds alongside Charlie Chaplin, Douglas Fairbanks, Theda Bara, and Marie Dressler. Five days later she spoke on Wall Street to an estimated 50,000 people. Though Canadian-born, she was a powerful symbol of Americana, kissing the American flag for cameras and auctioning one of her world-famous curls for $15,000. In a single speech in Chicago she sold an estimated five million dollars' worth of bonds. She was christened the U.S. Navy's official "Little Sister"; the Army named two cannons after her and made her an honorary colonel.

At the end of World War I, Pickford conceived of the Motion Picture Relief Fund, an organization to help financially needy actors. Leftover funds from her work selling Liberty Bonds were put toward its creation, and in 1921, the Motion Picture Relief Fund (MPRF) was officially incorporated, with Joseph Schenck voted its first president and Pickford its vice president. In 1932, Pickford spearheaded the "Payroll Pledge Program", a payroll-deduction plan for studio workers who gave one half of one percent of their earnings to the MPRF. As a result, in 1940, the Fund was able to purchase land and build the Motion Picture Country House and Hospital, in Woodland Hills, California.

An astute businesswoman, Pickford became her own producer within three years of her start in features. According to her Foundation, "she oversaw every aspect of the making of her films, from hiring talent and crew to overseeing the script, the shooting, the editing, to the final release and promotion of each project". She demanded (and received) these powers in 1916, when she was under contract to Zukor's Famous Players In Famous Plays (later Paramount). Zukor acquiesced to her refusal to participate in block-booking, the widespread practice of forcing an exhibitor to show a bad film of the studio's choosing to also be able to show a Pickford film. In 1916, Pickford's films were distributed, singly, through a special distribution unit called Artcraft. The Mary Pickford Corporation was briefly Pickford's motion-picture production company.
In 1919, she increased her power by co-founding United Artists (UA) with Charlie Chaplin, D. W. Griffith, and her soon-to-be husband, Douglas Fairbanks. Before UA's creation, Hollywood studios were vertically integrated, not only producing films but forming chains of theaters. Distributors (also part of the studios) arranged for company productions to be shown in the company's movie venues. Filmmakers relied on the studios for bookings; in return they put up with what many considered creative interference.

United Artists broke from this tradition. It was solely a distribution company, offering independent film producers access to its own screens as well as the rental of temporarily unbooked cinemas owned by other companies. Pickford and Fairbanks produced and shot their films after 1920 at the jointly owned Pickford-Fairbanks studio on Santa Monica Boulevard. The producers who signed with UA were true independents, producing, creating and controlling their work to an unprecedented degree. As a co-founder, as well as the producer and star of her own films, Pickford became the most powerful woman who has ever worked in Hollywood. By 1930, Pickford's acting career had largely faded. After retiring three years later, however, she continued to produce films for United Artists. She and Chaplin remained partners in the company for decades. Chaplin left the company in 1955, and Pickford followed suit in 1956, selling her remaining shares for three million dollars.

Pickford was married three times. She married Owen Moore, an Irish-born silent film actor, on January 7, 1911. It is rumored she became pregnant by Moore in the early 1910s and had a miscarriage or an abortion. Some accounts suggest this resulted in her later inability to have children. The couple had numerous marital problems, notably Moore's alcoholism, insecurity about living in the shadow of Pickford's fame, and bouts of domestic violence. The couple lived together on-and-off for several years.

Pickford became secretly involved in a relationship with Douglas Fairbanks. They toured the U.S. together in 1918 to promote Liberty Bond sales for the World War I effort. Around this time, Pickford also suffered from the flu during the 1918 flu pandemic. Pickford divorced Moore on March 2, 1920, after she agreed to his $100,000 demand for a settlement. She married Fairbanks just days later on March 28, 1920. They went to Europe for their honeymoon; fans in London and in Paris caused riots trying to get to the famous couple. The couple's triumphant return to Hollywood was witnessed by vast crowds who turned out to hail them at railway stations across the United States.

"The Mark of Zorro" (1920) and a series of other swashbucklers gave the popular Fairbanks a more romantic, heroic image. Pickford continued to epitomize the virtuous but fiery girl next door. Even at private parties, people instinctively stood up when Pickford entered a room; she and her husband were often referred to as "Hollywood royalty". Their international reputations were broad. Foreign heads of state and dignitaries who visited the White House often asked if they could also visit Pickfair, the couple's mansion in Beverly Hills.

Dinners at Pickfair included a number of notable guests. Charlie Chaplin, Fairbanks' best friend, was often present. Other guests included George Bernard Shaw, Albert Einstein, Elinor Glyn, Helen Keller, H. G. Wells, Lord Mountbatten, Fritz Kreisler, Amelia Earhart, F. Scott Fitzgerald, Noël Coward, Max Reinhardt, Baron Nishi, Vladimir Nemirovich-Danchenko, Sir Arthur Conan Doyle, Austen Chamberlain, Sir Harry Lauder, and Meher Baba, among others. The public nature of Pickford's second marriage strained it to the breaking point. Both she and Fairbanks had little time off from producing and acting in their films. They were also constantly on display as America's unofficial ambassadors to the world, leading parades, cutting ribbons, and making speeches. When their film careers both began to flounder at the end of the silent era, Fairbanks' restless nature prompted him to overseas travel (something which Pickford did not enjoy). When Fairbanks' romance with Sylvia, Lady Ashley became public in the early 1930s, he and Pickford separated. They divorced January 10, 1936. Fairbanks' son by his first wife, Douglas Fairbanks Jr., claimed his father and Pickford long regretted their inability to reconcile.

On June 24, 1937, Pickford married her third and last husband, actor and band leader Buddy Rogers. They adopted two children: Roxanne (born 1944, adopted 1944) and Ronald Charles (born 1937, adopted 1943, a.k.a. Ronnie Pickford Rogers). As a PBS "American Experience" documentary noted, Pickford's relationship with her children was tense. She criticized their physical imperfections, including Ronnie's small stature and Roxanne's crooked teeth. Both children later said their mother was too self-absorbed to provide real maternal love. In 2003, Ronnie recalled that "Things didn't work out that much, you know. But I'll never forget her. I think that she was a good woman."

After retiring from the screen, Pickford became an alcoholic, as her father had been. Her mother Charlotte died of breast cancer in March 1928. Her siblings, Lottie and Jack, both died of alcohol-related causes. These deaths, her divorce from Fairbanks, and the end of silent films left Pickford deeply depressed. Her relationship with her children, Roxanne and Ronald, was turbulent at best. Pickford withdrew and gradually became a recluse, remaining almost entirely at Pickfair and allowing visits only from Lillian Gish, her stepson Douglas Fairbanks, Jr., and few other people. She appeared in court in 1959, in a matter pertaining to her co-ownership of North Carolina TV station WSJS-TV. The court date coincided with the date of her 67th birthday; under oath, when asked to give her age, Pickford replied: "I'm 21, going on 20."

In the mid-1960s, Pickford often received visitors only by telephone, speaking to them from her bedroom. Buddy Rogers often gave guests tours of Pickfair, including views of a genuine western bar Pickford had bought for Douglas Fairbanks, and a portrait of Pickford in the drawing room. A print of this image now hangs in the Library of Congress. In addition to her Oscar as best actress for "Coquette" (1929), Mary Pickford received an Academy Honorary Award in 1976 for lifetime achievement. The Academy sent a TV crew to her house to record her short statement of thanks – offering the public a very rare glimpse into Pickfair Manor.

Pickford had ceased to be a British subject when she became an American citizen upon her marriage to Fairbanks in 1920. Thus, she never acquired Canadian citizenship when it was first created in 1947. Toward the end of her life, Pickford made arrangements with the Department of Citizenship to acquire Canadian citizenship because she wished to "die as a Canadian". Her request was approved and she became a dual Canadian-American citizen.

On May 29, 1979, Pickford died at a Santa Monica, California, hospital of complications from a cerebral hemorrhage she had suffered the week before. She was interred in the Garden of Memory of the Forest Lawn Memorial Park cemetery in Glendale, California.







</doc>
<doc id="18824" url="https://en.wikipedia.org/wiki?curid=18824" title="Mack Sennett">
Mack Sennett

Mack Sennett (born Michael Sinnott; January 17, 1880 – November 5, 1960) was an American film director and producer, known as the King of Comedy.

Born in Canada, he started in films in the Biograph company of New York, he opened Keystone Studios in Edendale, California in 1912. It was the first fully enclosed film stage, and Sennett became famous as the originator of slapstick routines such as pie-throwing and car-chases, as seen in the Keystone Cops films. He also produced short features that displayed his Bathing Beauties, many of whom went on to develop successful acting careers.
Sennett’s work in sound-movies was less successful and he was bankrupted in 1933. He was presented with an honorary Academy Award for his contribution to film comedy.

Born Michael Sinnott in Richmond Ste-Bibiane Parish, Quebec, Canada, he was the son of Irish Catholic John Sinnott and Catherine Foy, married 1879 in Tingwick St-Patrice Parish, Québec. The newlyweds moved the same year to Richmond, where John Sinnott was hired as a laborer. By 1883, when Michael's brother George was born, John Sinnott was working in Richmond as an innkeeper; he worked as an innkeeper for many years afterward. John Sinnott and Catherine Foy had all their children and raised their family in Richmond, then a small Eastern Townships village. At that time, Michael's grandparents were living in Danville, Québec. Michael Sinnott moved to Connecticut when he was 17 years old.

He lived for a while in Northampton, Massachusetts, where, according to his autobiography, Sennett first got the idea to become an opera singer after seeing a vaudeville show. He claimed that the most respected lawyer in town, Northampton mayor (and future President of the United States) Calvin Coolidge, as well as Sennett's own mother, tried to talk him out of his musical ambitions.
In New York City, Sennett became an actor, singer, dancer, clown, set designer, and director for Biograph. A major distinction in his acting career, often overlooked, is the fact that Sennett played Sherlock Holmes 11 times, albeit as a parody, between 1911 and 1913.

With financial backing from Adam Kessel and Charles O. Bauman of the New York Motion Picture Company, Michael "Mack" Sennett founded Keystone Studios in Edendale, California in 1912 (which is now a part of Echo Park). The original main building which was the first totally enclosed film stage and studio ever constructed, is still there today. Many important actors cemented their film careers with Sennett, including Marie Dressler, Mabel Normand, Charles Chaplin, Harry Langdon, Roscoe Arbuckle, Harold Lloyd, Raymond Griffith, Gloria Swanson, Ford Sterling, Andy Clyde, Chester Conklin, Polly Moran, Louise Fazenda, The Keystone Cops, Bing Crosby, and W. C. Fields.

Mack Sennett's slapstick comedies were noted for their wild car chases and custard pie warfare, especially in the "Keystone Cops "series. Additionally, Sennett's first female comedian was Mabel Normand, who became a major star under his direction and with whom he embarked on a tumultuous romantic relationship. Sennett also developed the "Kid Comedies", a forerunner of the "Our Gang" films, and in a short time, his name became synonymous with screen comedy which were called "flickers" at the time. In 1915, Keystone Studios became an autonomous production unit of the ambitious Triangle Film Corporation, as Sennett joined forces with D. W. Griffith and Thomas Ince, both powerful figures in the film industry.

Also beginning in 1915, Sennett assembled a bevy of women known as the Sennett Bathing Beauties to appear in provocative bathing costumes in comedy short subjects, in promotional material, and in promotional events such as Venice Beach beauty contests.

Two of those often named as Bathing Beauties do not belong on the list: Mabel Normand and Gloria Swanson. Normand was a featured player, and her 1912 8-minute film "The Water Nymph" may have been the direct inspiration for the Bathing Beauties. Although Gloria Swanson worked for Sennett in 1916 and was photographed in a bathing suit, she was also a star and "vehemently denied" being one of the bathing beauties.

Not individually featured or named, many of these young women ascended to significant careers of their own. They included Juanita Hansen, Claire Anderson, Marie Prevost, Phyllis Haver, and Carole Lombard. In the 1920s, Sennett's Bathing Beauties remained popular enough to provoke imitators such as the Christie Studios' Bathing Beauties (counting Raquel Torres and Laura La Plante as alumnae) and Fox Film Corporation's "Sunshine Girls" (counting Janet Gaynor as an alumna). The Sennett Bathing Beauties continued to appear through 1928.

In 1917, Sennett gave up the Keystone trademark and organized his own company, Mack Sennett Comedies Corporation. (Sennett's bosses retained the Keystone trademark and produced a cheap series of comedy shorts that were "Keystones" in name only: they were unsuccessful, and Sennett had no connection with them.) Sennett went on to produce more ambitious comedy short films and a few feature-length films.
During the 1920s, his short subjects were in much demand, featuring stars such as Billy Bevan, Andy Clyde, Harry Gribbon, Vernon Dent, Alice Day, Ralph Graves, Charlie Murray, and Harry Langdon. He produced several features with his brightest stars such as Ben Turpin and Mabel Normand.

Many of Sennett's films of the early 1920s were inherited by Warner Bros. Studio. Warner Bros. merged with the original distributor, First National, and added music and commentary to several of these short subjects. Unfortunately, many of the films of this period were destroyed due to inadequate storage. As a result, many of Sennett's films from his most productive and creative period no longer exist.

In the mid-1920s, Sennett moved to Pathé Exchange distribution. Pathé had a huge market share, but made bad corporate decisions, such as attempting to sell too many comedies at once (including those of Sennett's main competitor, Hal Roach). In 1927, Paramount and MGM, which were Hollywood's two top studios at the time, took note of the profits being made by smaller companies such as Pathé Exchange and Educational Pictures. So, Paramount and MGM decided to resume the production and distribution of short subjects. Hal Roach signed with MGM, but Mack Sennett remained with Pathé Exchange even during hard times, which were brought on by the competition. Hundreds of other independent exhibitors and movie houses of this period had switched from Pathe′ to the new MGM or Paramount films and short subjects.

Sennett made a reasonably smooth transition to sound films, releasing them through Earle Hammons's Educational Pictures. Sennett occasionally experimented with color. Plus, he was the first to get a talkie short subject on the market in 1928. In 1932, he was nominated for the Academy Award for Live Action Short Film in the comedy division for producing "The Loud Mouth" (with Matt McHugh, in the sports-heckler role later taken in Columbia Pictures remakes by Charley Chase and Shemp Howard). Sennett also won an Academy Award in the novelty division for his film "Wrestling Swordfish" also in 1932. On March 25, 1932, he became a United States citizen.

Sennett often clung to outmoded techniques, making his early-1930s films seem dated and quaint. This doomed his attempt to re-enter the feature-film market with "Hypnotized" (starring blackface comedians Moran and Mack, "The Two Black Crows"). However, Sennett enjoyed great success with short comedies starring Bing Crosby, which were more than likely instrumental in Sennett's product being picked up by a major studio, Paramount Pictures. W. C. Fields conceived and starred in four famous Sennett-Paramount comedies. Fields himself recalled that he "made seven comedies for the Irishman", his original deal called for one film and an option for six more, but ultimately only four were made.

Sennett's studio did not survive the Great Depression. His partnership with Paramount lasted only one year and he was forced into bankruptcy in November 1933.

On January 12, 1934, Sennett was injured in an automobile accident that killed blackface performer Charles Mack in Mesa, Arizona.

His last work, in 1935, was as a producer-director for Educational Pictures, in which he directed Buster Keaton in "The Timid Young Man" and Joan Davis in "Way Up Thar". (The 1935 Vitaphone short subject "Keystone Hotel" is not a Sennett production, although it featured several alumni from the Mack Sennett Studios. Actually, Sennett was not involved in the making of this film.)

Mack Sennett went into semiretirement at the age of 55, having produced more than 1,000 silent films and several dozen talkies during a 25-year career. His studio property was purchased by Mascot Pictures (later part of Republic Pictures), and many of his former staffers found work at Columbia Pictures.

In March 1938, Sennett was presented with an honorary Academy Award: "for his lasting contribution to the comedy technique of the screen, the basic principles of which are as important today as when they were first put into practice, the Academy presents a Special Award to that master of fun, discoverer of stars, sympathetic, kindly, understanding comedy genius - Mack Sennett."

Rumors abounded that Sennett would be returning to film production (a 1938 publicity release indicated that he would be working with Stan Laurel of Laurel and Hardy), but apart from Sennett reissuing a couple of his Bing Crosby two-reelers to theaters, nothing happened. Sennett did appear in front of the camera, however, in "Hollywood Cavalcade" (1939), itself a thinly disguised version of the Mack Sennett-Mabel Normand romance. In 1949, he provided film footage for and also appeared in the first full-length comedy compilation called "Down Memory Lane" (1949), which was written and narrated by Steve Allen. Sennett was profiled in the television series "This is Your Life" in 1954. and made a cameo appearance (for $1,000) in "Abbott and Costello Meet the Keystone Kops" (1955). His last contribution worth noting was to the NBC radio program "Biography in Sound" relating memories of working with W.C. Fields, which was broadcast February 28, 1956.

Mack Sennett died on November 5, 1960, in Woodland Hills, California, aged 80. He was interred in the Holy Cross Cemetery in Culver City, California.

For his contribution to the motion picture industry, Sennett honored with a star on the Hollywood Walk of Fame at 6712 Hollywood Boulevard. He was also inducted into Canada's Walk of Fame in 2014.

A line in a Henry Kuttner science-fiction short story "Piggy Bank" reads, "Within seconds the scene resembled a Mack Sennett pie-throwing comedy."

In "A Story of Water", a 1961 short film by Jean-Luc Godard and François Truffaut, the directors dedicate the film to Mack Sennett.

Henry Mancini's score for the 1963 film "The Pink Panther", the original entry in the series, contains a segment called "Shades of Sennett". It is played on a silent film era style "barrel house" piano, and accompanies a climactic scene in which the incompetent police detective Inspector Clouseau is involved in a multi-vehicle chase with the antagonists.

In 1974, Michael Stewart and Jerry Herman wrote the musical "Mack & Mabel", chronicling the romance between Sennett and Mabel Normand.

Sennett also was a leading character in "The Biograph Girl", a 1980 musical about the silent-film era.

Peter Lovesey's 1983 novel "Keystone" is a whodunnit set in the Keystone Studios and involving (among others), Mack Sennett, Mabel Normand, Roscoe Arbuckle, and the Keystone Cops.

Dan Aykroyd portrayed Mack Sennett in the 1992 movie "Chaplin". Marisa Tomei played Mabel Normand and Robert Downey, Jr. starred as Charlie Chaplin.

Joseph Beattie and Andrea Deck portrayed Mack Sennett and Mabel Normand, respectively, in episode eight of series two of ITV's "Mr. Selfridge".





</doc>
<doc id="18825" url="https://en.wikipedia.org/wiki?curid=18825" title="Motion Picture Patents Company">
Motion Picture Patents Company

The Motion Picture Patents Company (MPPC, also known as the Edison Trust), founded in December 1908 and terminated seven years later in 1915 after conflicts within the industry, was a trust of all the major USA film companies and local foreign-branches (Edison, Biograph, Vitagraph, Essanay, Selig Polyscope, Lubin Manufacturing, Kalem Company, Star Film Paris, American Pathé), the leading film distributor (George Kleine) and the biggest supplier of raw film stock, Eastman Kodak. The MPPC ended the domination of foreign films on USA screens, standardized the manner in which films were distributed and exhibited within the USA, and improved the quality of USA motion pictures by internal competition. But it also discouraged its members' entry into feature film production, and the use of outside financing, both to its members' eventual detriment.

The MPPC was preceded by the Edison licensing system, in effect in 1907–1908, on which the MPPC was modeled. During the 1890s, Thomas Edison owned most of the major USA patents relating to motion picture cameras. The Edison Manufacturing Company's patent lawsuits against each of its domestic competitors crippled the USA's film industry, reducing production mainly to two companies: Edison and Biograph, which used a different camera design. This left Edison's other rivals with little recourse but to import French and British films.

Since 1902, Edison had also been notifying distributors and exhibitors that if they did not use Edison machines and films exclusively, they would be subject to litigation for supporting filmmaking that infringed Edison's patents. Exhausted by the lawsuits, Edison's competitors — Essanay, Kalem, Pathé Frères, Selig, and Vitagraph — approached him in 1907 to negotiate a licensing agreement, which Lubin was also invited to join. The one notable filmmaker excluded from the licensing agreement was Biograph, which Edison hoped to squeeze out of the market. No further applicants could become licensees. The purpose of the licensing agreement, according to an Edison lawyer, was to "preserve the business of present manufacturers and not to throw the field open to all competitors."

Biograph retaliated for being frozen out of the trust agreement by purchasing the patent to the Latham film loop, a key feature of virtually all motion picture cameras then in use. Edison sued to gain control of the patent; however, after a federal court upheld the validity of the patent in 1907, Edison began negotiation with Biograph in May 1908 to reorganize the Edison licensing system. The resulting trust pooled 16 motion picture patents. Ten were considered of minor importance; the remaining key six pertained one each to films, cameras, and the Latham loop, and three to projectors.

The MPPC eliminated the outright sale of films to distributors and exhibitors, replacing it with rentals, which allowed quality control over prints that had formerly been exhibited long past their prime. The trust also established a uniform rental rate for all licensed films, thereby removing price as a factor for the exhibitor in film selection, in favor of selection made on quality, which in turn encouraged the upgrading of production values.

However, the MPPC also established a monopoly on all aspects of filmmaking. Eastman Kodak, which owned the patent on raw film stock, was a member of the trust and thus agreed to sell stock only to other members. Likewise, the trust's control of patents on motion picture cameras ensured that only MPPC studios were able to film, and the projector patents allowed the trust to make licensing agreements with distributors and theaters – and thus determine who screened their films and where.

The patents owned by the MPPC allowed them to use federal law enforcement officials to enforce their licensing agreements and to prevent unauthorized use of their cameras, films, projectors, and other equipment. In some cases, however, the MPPC made use of hired thugs and mob connections to violently disrupt productions that were not licensed by the trust.

The MPPC also strictly regulated the production content of their films, primarily as a means of cost control. Films were initially limited to one reel in length (13–17 minutes), although competition by independent and foreign producers by 1912 led to the introduction of two-reelers, and by 1913, three- and four-reelers.

Many independent filmmakers, who controlled from one-quarter to one-third of the domestic marketplace, responded to the creation of the MPPC by moving their operations to Hollywood, whose distance from Edison's home base of New Jersey made it more difficult for the MPPC to enforce its patents. The Ninth Circuit Court of Appeals, which is headquartered in San Francisco, California, and covers the area, was averse to enforcing patent claims. Southern California was also chosen because of its beautiful year-round weather and varied countryside; its topography, semi-arid climate and widespread irrigation gave its landscapes the ability to offer motion picture shooting scenes set in deserts, jungles and great mountains. Hollywood had one additional advantage: if a non-licensed studio was sued, it was only a hundred miles to "run for the border" and get out of the US to Mexico, where the trust's patents were not in effect and thus equipment could not be seized.

The reasons for the MPPC's decline are manifold. The first blow came in 1911, when Eastman Kodak modified its exclusive contract with the MPPC to allow Kodak, which led the industry in quality and price, to sell its raw film stock to unlicensed independents. The number of theaters exhibiting independent films grew by 33 percent within twelve months, to half of all houses.

Another reason was the MPPC's overestimation of the efficiency of controlling the motion picture industry through patent litigation and the exclusion of independents from licensing. The slow process of using detectives to investigate patent infringements, and of obtaining injunctions against the infringers, was outpaced by the dynamic rise of new companies in diverse locations.

Despite the rise in popularity of feature films in 1912–1913 from independent producers and foreign imports, the MPPC was very reluctant to make the changes necessary to distribute such longer films. Edison, Biograph, Essanay, and Vitagraph did not release their first features until 1914, after dozens, if not hundreds, of feature films had been released by independents.

Patent royalties to the MPPC ended in September 1913 with the expiration of the last of the patents filed in the mid-1890s at the dawn of commercial film production and exhibition. Thus the MPPC lost the ability to control the American film industry through patent licensing, and had to rely instead on its subsidiary, the General Film Company, formed in 1910, which monopolized film distribution in USA.

The outbreak of World War I in 1914 cut off most of the European market, which played a much more significant part of the revenue and profit for MPPC members than for the independents, which concentrated on Westerns produced for a primarily USA market.

The end came with a federal court decision in "United States v. Motion Picture Patents Co." on October 1, 1915, which ruled that the MPPC's acts went "far beyond what was necessary to protect the use of patents or the monopoly which went with them" and was therefore an illegal restraint of trade under the Sherman Antitrust Act. An appellate court dismissed the MPPC's appeal, and officially terminated the company in 1918.




</doc>
<doc id="18826" url="https://en.wikipedia.org/wiki?curid=18826" title="MD5">
MD5

The MD5 message-digest algorithm is a widely used hash function producing a 128-bit hash value. Although MD5 was initially designed to be used as a cryptographic hash function, it has been found to suffer from extensive vulnerabilities. It can still be used as a checksum to verify data integrity, but only against unintentional corruption.
One basic requirement of any cryptographic hash function is that it should be computationally infeasible to find two non-identical messages which hash to the same value. MD5 fails this requirement catastrophically; such collisions can be found in seconds on an ordinary home computer.

The weaknesses of MD5 have been exploited in the field, most infamously by the Flame malware in 2012. The CMU Software Engineering Institute considers MD5 essentially "cryptographically broken and unsuitable for further use".

MD5 was designed by Ronald Rivest in 1991 to replace an earlier hash function MD4, and was specified in 1992 as RFC 1321.

MD5 is one in a series of message digest algorithms designed by Professor Ronald Rivest of MIT (Rivest, 1992). When analytic work indicated that MD5's predecessor MD4 was likely to be insecure, Rivest designed MD5 in 1991 as a secure replacement. (Hans Dobbertin did indeed later find weaknesses in MD4.)

In 1993, Den Boer and Bosselaers gave an early, although limited, result of finding a "pseudo-collision" of the MD5 compression function; that is, two different initialization vectors that produce an identical digest.

In 1996, Dobbertin announced a collision of the compression function of MD5 (Dobbertin, 1996). While this was not an attack on the full MD5 hash function, it was close enough for cryptographers to recommend switching to a replacement, such as SHA-1 or RIPEMD-160.

The size of the hash value (128 bits) is small enough to contemplate a birthday attack. MD5CRK was a distributed project started in March 2004 with the aim of demonstrating that MD5 is practically insecure by finding a collision using a birthday attack.

MD5CRK ended shortly after 17 August 2004, when collisions for the full MD5 were announced by Xiaoyun Wang, Dengguo Feng, Xuejia Lai, and Hongbo Yu. Their analytical attack was reported to take only one hour on an IBM p690 cluster.

On 1 March 2005, Arjen Lenstra, Xiaoyun Wang, and Benne de Weger demonstrated construction of two X.509 certificates with different public keys and the same MD5 hash value, a demonstrably practical collision. The construction included private keys for both public keys. A few days later, Vlastimil Klima described an improved algorithm, able to construct MD5 collisions in a few hours on a single notebook computer. On 18 March 2006, Klima published an algorithm that could find a collision within one minute on a single notebook computer, using a method he calls tunneling.

Various MD5-related RFC errata have been published. 
In 2009, the United States Cyber Command used an MD5 hash value of their mission statement as a part of their official emblem.

On 24 December 2010, Tao Xie and Dengguo Feng announced the first published single-block (512-bit) MD5 collision. (Previous collision discoveries had relied on multi-block attacks.) For "security reasons", Xie and Feng did not disclose the new attack method. They issued a challenge to the cryptographic community, offering a US$10,000 reward to the first finder of a different 64-byte collision before 1 January 2013. Marc Stevens responded to the challenge and published colliding single-block messages as well as the construction algorithm and sources.

In 2011 an informational RFC 6151 was approved to update the security considerations in MD5 and HMAC-MD5.

The security of the MD5 hash function is severely compromised. A collision attack exists that can find collisions within seconds on a computer with a 2.6 GHz Pentium 4 processor (complexity of 2). Further, there is also a chosen-prefix collision attack that can produce a collision for two inputs with specified prefixes within hours, using off-the-shelf computing hardware (complexity 2).
The ability to find collisions has been greatly aided by the use of off-the-shelf GPUs. On an NVIDIA GeForce 8400GS graphics processor, 16–18 million hashes per second can be computed. An NVIDIA GeForce 8800 Ultra can calculate more than 200 million hashes per second.

These hash and collision attacks have been demonstrated in the public in various situations, including colliding document files and digital certificates. As of 2015, MD5 was demonstrated to be still quite widely used, most notably by security research and antivirus companies.

In 1996, a flaw was found in the design of MD5. While it was not deemed a fatal weakness at the time, cryptographers began recommending the use of other algorithms, such as SHA-1, which has since been found to be vulnerable as well.
In 2004 it was shown that MD5 is not collision-resistant. As such, MD5 is not suitable for applications like SSL certificates or digital signatures that rely on this property for digital security. Also in 2004 more serious flaws were discovered in MD5, making further use of the algorithm for security purposes questionable; specifically, a group of researchers described how to create a pair of files that share the same MD5 checksum. Further advances were made in breaking MD5 in 2005, 2006, and 2007. In December 2008, a group of researchers used this technique to fake SSL certificate validity.

As of 2010, the CMU Software Engineering Institute considers MD5 "cryptographically broken and unsuitable for further use", and most U.S. government applications now require the SHA-2 family of hash functions. In 2012, the Flame malware exploited the weaknesses in MD5 to fake a Microsoft digital signature.

In 1996, collisions were found in the compression function of MD5, and Hans Dobbertin wrote in the RSA Laboratories technical newsletter, "The presented attack does not yet threaten practical applications of MD5, but it comes rather close ... in the future MD5 should no longer be implemented ... where a collision-resistant hash function is required."

In 2005, researchers were able to create pairs of PostScript documents and X.509 certificates with the same hash. Later that year, MD5's designer Ron Rivest wrote that "md5 and sha1 are both clearly broken (in terms of collision-resistance)".

On 30 December 2008, a group of researchers announced at the 25th Chaos Communication Congress how they had used MD5 collisions to create an intermediate certificate authority certificate that appeared to be legitimate when checked by its MD5 hash. The researchers used a cluster of Sony PlayStation 3 units at the EPFL in Lausanne, Switzerland to change a normal SSL certificate issued by RapidSSL into a working CA certificate for that issuer, which could then be used to create other certificates that would appear to be legitimate and issued by RapidSSL. VeriSign, the issuers of RapidSSL certificates, said they stopped issuing new certificates using MD5 as their checksum algorithm for RapidSSL once the vulnerability was announced. Although Verisign declined to revoke existing certificates signed using MD5, their response was considered adequate by the authors of the exploit (Alexander Sotirov, Marc Stevens, Jacob Appelbaum, Arjen Lenstra, David Molnar, Dag Arne Osvik, and Benne de Weger). Bruce Schneier wrote of the attack that "we already knew that MD5 is a broken hash function" and that "no one should be using MD5 anymore". The SSL researchers wrote, "Our desired impact is that Certification Authorities will stop using MD5 in issuing new certificates. We also hope that use of MD5 in other applications will be reconsidered as well."

In 2012, according to Microsoft, the authors of the Flame malware used an MD5 collision to forge a Windows code-signing certificate.

MD5 uses the Merkle–Damgård construction, so if two prefixes with the same hash can be constructed, a common suffix can be added to both to make the collision more likely to be accepted as valid data by the application using it. Furthermore, current collision-finding techniques allow to specify an arbitrary "prefix": an attacker can create two colliding files that both begin with the same content. All the attacker needs to generate two colliding files is a template file with a 128-byte block of data, aligned on a 64-byte boundary that can be changed freely by the collision-finding algorithm. An example MD5 collision, with the two messages differing in 6 bits, is:

Both produce the MD5 hash 79054025255fb1a26e4bc422aef54eb4.
The difference between the two samples is that the leading bit in each nibble has been flipped. For example, the 20th byte (offset 0x13) in the top sample, 0x87, is 10000111 in binary. The leading bit in the byte (also the leading bit in the first nibble) is flipped to make 00000111, which is 0x07, as shown in the lower sample.

Later it was also found to be possible to construct collisions between two files with separately chosen prefixes. This technique was used in the creation of the rogue CA certificate in 2008. A new variant of parallelized collision searching using MPI was proposed by Anton Kuznetsov in 2014, which allowed to find a collision in 11 hours on a computing cluster.

In April 2009, a preimage attack against MD5 was published that breaks MD5's preimage resistance. This attack is only theoretical, with a computational complexity of 2 for full preimage.

MD5 digests have been widely used in the software world to provide some assurance that a transferred file has arrived intact. For example, file servers often provide a pre-computed MD5 (known as md5sum) checksum for the files, so that a user can compare the checksum of the downloaded file to it. Most unix-based operating systems include MD5 sum utilities in their distribution packages; Windows users may use the included PowerShell function "Get-FileHash", install a Microsoft utility, or use third-party applications. Android ROMs also use this type of checksum.

As it is easy to generate MD5 collisions, it is possible for the person who created the file to create a second file with the same checksum, so this technique cannot protect against some forms of malicious tampering. In some cases, the checksum cannot be trusted (for example, if it was obtained over the same channel as the downloaded file), in which case MD5 can only provide error-checking functionality: it will recognize a corrupt or incomplete download, which becomes more likely when downloading larger files.

Historically, MD5 has been used to store a one-way hash of a password, often with key stretching. NIST does not include MD5 in their list of recommended hashes for password storage.

MD5 is also used in the field of electronic discovery, in order to provide a unique identifier for each document that is exchanged during the legal discovery process. This method can be used to replace the Bates stamp numbering system that has been used for decades during the exchange of paper documents. As above, this usage should be discouraged due to the ease of collision attacks.

MD5 processes a variable-length message into a fixed-length output of 128 bits. The input message is broken up into chunks of 512-bit blocks (sixteen 32-bit words); the message is padded so that its length is divisible by 512. The padding works as follows: first a single bit, 1, is appended to the end of the message. This is followed by as many zeros as are required to bring the length of the message up to 64 bits fewer than a multiple of 512. The remaining bits are filled up with 64 bits representing the length of the original message, modulo 2.

The main MD5 algorithm operates on a 128-bit state, divided into four 32-bit words, denoted , , , and . These are initialized to certain fixed constants. The main algorithm then uses each 512-bit message block in turn to modify the state. The processing of a message block consists of four similar stages, termed "rounds"; each round is composed of 16 similar operations based on a non-linear function , modular addition, and left rotation. Figure 1 illustrates one operation within a round. There are four possible functions; a different one is used in each round:

formula_2 denote the XOR, AND, OR and NOT operations respectively.

The MD5 hash is calculated according to this algorithm. All values are in little-endian.

"Note: Instead of the formulation from the original RFC 1321 shown, the following may be used for improved efficiency (useful if assembly language is being used – otherwise, the compiler will generally optimize the above code. Since each computation is dependent on another in these formulations, this is often slower than the above method where the nand/and can be parallelised):"

The 128-bit (16-byte) MD5 hashes (also termed "message digests") are typically represented as a sequence of 32 hexadecimal digits. The following demonstrates a 43-byte ASCII input and the corresponding MD5 hash:

Even a small change in the message will (with overwhelming probability) result in a mostly different hash, due to the avalanche effect. For example, adding a period to the end of the sentence:

The hash of the zero-length string is:

The MD5 algorithm is specified for messages consisting of any number of bits; it is not limited to multiples of eight bit (octets, bytes). Some MD5 implementations such as md5sum might be limited to octets, or they might not support "streaming" for messages of an initially undetermined length.





</doc>
<doc id="18830" url="https://en.wikipedia.org/wiki?curid=18830" title="Magic: The Gathering">
Magic: The Gathering

Magic: The Gathering is a both a trading card and digital collectible card game created by Richard Garfield. Released in 1993 by Wizards of the Coast, "Magic" was the first trading card game created and it continues to thrive, with approximately twenty million players , and over twenty billion Magic cards produced in the period of 2008 to 2016 alone.

"Magic" can be played by two or more players in various formats, which fall into two categories: constructed and limited. Limited formats involve players building a deck spontaneously out of a pool of random cards with a minimum deck size of 40 cards. In constructed, players created decks from cards they own, usually 60 cards with no more than 4 of any given card. "Magic" is played in person with printed cards, or using a deck of virtual cards through the Internet-based "", or on a smartphone or tablet, or through .
Each game represents a battle between wizards known as "planeswalkers", who employ spells, artifacts, and creatures depicted on individual "Magic" cards to defeat their opponents. Although the original concept of the game drew heavily from the motifs of traditional fantasy role-playing games such as "Dungeons & Dragons", the gameplay of "Magic" bears little similarity to pencil-and-paper adventure games, while having substantially more cards and more complex rules than many other card games.

New cards are released on a regular basis through . An organized tournament system played at an international level and a worldwide community of has developed, as well as a substantial secondary market for "Magic" cards. Certain "Magic" cards can be valuable due to their rarity and utility in game play, with prices ranging from a few cents to thousands of dollars.

Richard Garfield was a doctoral candidate in combinatorial mathematics at University of Pennsylvania when he first started to design the game. During his free time he worked with local volunteer playtesters to help refine the game. He had been brought on as an adjunct professor at Whitman College in 1991 when Peter Adkison (then CEO of Wizards of the Coast games company) first met with Garfield to discuss Garfield's new game "RoboRally". Adkison saw the game as very promising, but decided that Wizards of the Coast lacked the resources to produce it at that point. He did like Garfield's ideas and mentioned that he was looking for a portable game that could be played in the downtime that frequently occurs at gaming conventions. Garfield returned and presented the general outline of the concept of a trading card game. It was based on Garfield's game "Five Magics" from 1982. Adkison immediately saw the potential of this idea and agreed to produce it. "Magic: The Gathering" underwent a general release on August 5, 1993.

While the game was simply called "Magic" through most of playtesting, when the game had to be officially named a lawyer informed them that the name "Magic" was too generic to be trademarked. "Mana Clash" was instead chosen to be the name used in the first solicitation of the game, however, everybody involved with the game continued to refer to it as "Magic". After further consultation with the lawyer, it was decided to rename the game "Magic: The Gathering", thus enabling the name to be trademarked.

A patent was granted to Wizards of the Coast in 1997 for "a novel method of game play and game components that in one embodiment are in the form of trading cards" that includes claims covering games whose rules include many of "Magic"'s elements in combination, including concepts such as changing orientation of a game component to indicate use (referred to in the "Magic" and "" rules as "tapping") and constructing a deck by selecting cards from a larger pool. The patent has aroused criticism from some observers, who believe some of its claims to be invalid. In 2003, the patent was an element of a larger legal dispute between Wizards of the Coast and Nintendo, regarding trade secrets related to Nintendo's "Pokémon Trading Card Game". The legal action was settled out of court, and its terms were not disclosed.

"Magic" was an immediate success for Wizards of the Coast. Early on they were even reluctant to advertise the game because they were unable to keep pace with existing demand. Initially "Magic" attracted many "Dungeons & Dragons" players, but the following included all types of other people as well. The success of the game quickly led to the creation of similar games by other companies as well as Wizards of the Coast themselves. Companion Games produced the Galactic Empires CCG (the first science fiction trading card game), which allowed players to pay for and design their own promotional cards, while TSR created the "Spellfire" game, which eventually included five editions in six languages, plus twelve expansion sets. Wizards of the Coast produced "" (now called "Vampire: The Eternal Struggle"), a game about modern-day vampires. Other similar games included trading card games based on "Star Trek" and "Star Wars". "Magic" is often cited as an example of a 1990s collecting fad, though the game's makers were able to overcome the bubble traditionally associated with collecting fads.

The success of the initial edition prompted a reissue later in 1993, along with expansions to the game. "" was released as the first in December 1993. New expansions and revisions of the base game ("Core Sets") have since been released on a regular basis, amounting to four releases a year. By the end of 1994, the game had printed over a billion cards. Until the release of "" in 1996, expansions were released on an irregular basis. Beginning in 2009 one revision of the core set and a set of three related expansions called a "block" were released every year. This system was revised in 2015, with the Core Set being eliminated and blocks now consisting of two sets, released biannually. While the essence of the game has always stayed the same, the rules of "Magic" have undergone three major revisions with the release of the "" in 1994, in 1999, and "Magic 2010" in July 2009. With the release of the "" in 2003, "Magic" also received a major visual redesign.

In 1996, Wizards of the Coast established the "", a circuit of tournaments where players can compete for sizeable cash prizes over the course of a single weekend-long tournament. In 2009 the top prize at a single tournament was US$40,000. Sanctioned through the DCI, the tournaments added an element of prestige to the game by virtue of the cash payouts and media coverage from within the community. For a brief period of time, ESPN2 televised the tournaments.

By April 1997, cards had been sold. In 1999, Wizards of The Coast was acquired by Hasbro for $325 million, making "Magic" a Hasbro game.

While unofficial methods of online play existed previously, "" ("MTGO" or "Modo"), an official online version of the game, was released in 2002. A new, updated version of "Magic Online" was released in April 2008.

In January 2014, Hasbro announced a franchise film deal with 20th Century Fox for "Magic: The Gathering", saying that they wanted "to launch a massive franchise on the scale of "Harry Potter" and "The Lord of the Rings"." Simon Kinberg was to serve as a producer for the project. In June 2014, Fox hired screenwriter Bryan Cogman to write the script for the film. As of 2017, no film has entered production.

In February 2018, Wizards noted that between the years of 2008 and 2016 they had printed over 20 billion "Magic: the Gathering" cards.

A 2004 article in "USA Today" suggested that playing "Magic" might help improve the social and mental skills of some of the players. The article interviewed players' parents who believe that the game, similar to sports, teaches children how to more gracefully win and lose. "Magic" also contains a great amount of strategy and vocabulary that children may not be exposed to on a regular basis. Parents also claimed that playing "Magic" helped keep their children out of trouble, such as using illegal drugs or joining criminal gangs. On the other hand, the article also briefly mentions that "Magic" can be highly addictive, leading to parents worried about their children's "Magic" obsession. In addition, until 2007, some of the better players had opportunities to compete for a small number of scholarships.

Jordan Weisman, an American game designer and entrepreneur, commented, "I love games that challenge and change our definition of adventure gaming, and "Magic: The Gathering" is definitely one of a very short list of titles that has accomplished that elusive goal. By combining the collecting and trading elements of baseball cards with the fantasy play dynamics of roleplaying games, "Magic" created a whole new genre of product that changed our industry forever."


In addition several individuals including Richard Garfield and Donato Giancola won personal awards for their contributions to "Magic".

A game of "Magic" involves two or more players who are engaged in a battle acting as powerful wizards called planeswalkers. Each player has their own deck, either one previously constructed or made from a limited pool of cards for the event. A player starts the game with twenty "life points" and loses the game when their life total is reduced to zero. A player can also lose if they must draw from an empty deck. In addition, some cards specify other ways to win or lose the game. Garfield has stated that two major influences in his creation of "Magic: the Gathering" were the games "Cosmic Encounter", which first used the concept that normal rules could sometimes be overridden, and "Dungeons & Dragons". The "Golden Rule of "Magic"" states that "Whenever a card's text directly contradicts the rules, the card takes precedence." The "Comprehensive Rules", a detailed rulebook, exists to clarify conflicts.

Players begin the game by shuffling their decks and then drawing seven cards. Players draw one card at the beginning of each of their turns, except the first player on their first turn unless there are more than 2 players. Players alternate turns. The two basic kinds of cards are "spells" and "lands". Lands provide "mana", or magical energy, which is used as magical fuel when the player attempts to cast spells. Players may only play one land per turn. More powerful spells cost more mana, so as the game progresses more mana becomes available, and the quantity and relative power of the spells played tends to increase. Spells come in several varieties: "sorceries" and "instants" have a single, one-time effect before they go to the "graveyard" (discard pile); "enchantments" and "artifacts" are "permanents" that remain in play after being cast to provide a lasting magical effect; "creature" spells (also a type of permanent) summon creatures that can attack and damage an opponent. The set "Lorwyn" introduced the new "planeswalker" card type, which represents powerful allies who fight with their own magic abilities.

In most Constructed tournament formats, decks are required to be a minimum of sixty cards, with no upper limit. Players may use no more than four copies of any named card, with the exception of "basic lands", which act as a standard resource in "Magic", and some specific cards that state otherwise. For example, the card Relentless Rats states that a deck may contain any number of itself. Certain formats such as may limit the number of iterations of a single card players may have in their decks. These are colloquially known as singleton formats.

In most Constructed formats, there exists a list of individual cards which have been "restricted" (the card is limited to a single copy per deck) or "banned" (the card is no longer legal for tournament play). These limitations are usually for balance of power reasons, but have been occasionally made because of gameplay mechanics.

In "" tournament formats, a small number of cards are opened for play from booster packs or tournament packs, and a minimum deck size of forty cards is enforced. The most popular limited format is draft, in which players open a booster pack, choose a card from it, and pass it to the player seated next to them. This continues until all the cards have been picked, and then a new pack is opened. Three packs are opened altogether, and the direction of passing alternates left-right-left.

Deck building requires strategy as players must choose among thousands of cards which they want to play. This requires players to evaluate the power of their cards, as well as the possible synergies between them, and their possible interactions with the cards they expect to play against (this "metagame" can vary in different locations or time periods). The choice of cards is usually narrowed by the player deciding which colors they want to include in the deck. This decision is a key part of creating a deck. In general, reducing the number of colors used increases the consistency of play and the probability of drawing the lands needed to cast one's spells, at the expense of restricting the range of tactics available to the player.

Most spells come in one of five colors. The colors can be seen on the back of the cards, in a pentagonal design, called the "Color Wheel" or "Color Pie". Clockwise from the top, they are: white (W), blue (U), black (B), red (R), and green (G). To play a spell of a given color, at least one mana of that color is required. This mana is normally generated by a basic land: plains for white, island for blue, swamp for black, mountain for red, and forest for green. The balances and distinctions among the five colors form one of the defining aspects of the game. Each color has strengths and weaknesses based on the "style" of magic it represents.

The colors adjacent to each other on the pentagon are "allied" and often have similar, complementary abilities. For example, Blue has a relatively large number of flying creatures, as do White and Black, which are next to it. The two non-adjacent colors to a particular color are "enemy" colors, and are thematically opposed. For instance, Red tends to be very aggressive, while White and Blue are often more defensive in nature. The Research and Development (R&D) team at Wizards of the Coast aims to balance power and abilities among the five colors by using the "Color Pie" to differentiate the strengths and weaknesses of each. This guideline lays out the capabilities, themes, and mechanics of each color and allows for every color to have its own distinct attributes and gameplay. The Color Pie is used to ensure new cards are thematically in the correct color and do not infringe on the territory of other colors.

"Magic", like many other games, combines chance and skill. One frequent complaint about the game involves the notion that there is too much luck involved, especially concerning possessing too many or too few lands. Early in the game especially, too many or too few lands could ruin a player's chance at victory without the player having made a mistake. This in-game statistical variance can be minimized by proper deck construction, as an appropriate land count can reduce mana problems. In "Duels of the Planeswalkers 2012", the land count is automatically adjusted to 40% of the total deck size.

A "mulligan" rule was introduced into the game, first informally in casual play and then in the official game rules. The most current mulligan rule allows players to shuffle an unsatisfactory opening hand back into the deck at the start of the game, draw a new hand with one fewer card, and repeat until satisfied, after which any player who has fewer than seven cards may look at the top card of his or her deck and either return it or put it at the bottom of the deck. In multiplayer, a player may take one mulligan without penalty, while subsequent mulligans will still cost one card (a rule known as "Partial Paris mulligan"). The original mulligan allowed a player a single redraw of seven new cards if that player's initial hand contained seven or zero lands. A variation of this rule called a "forced mulligan" is still used in some casual play circles and in multiplayer formats on "Magic Online", and allows a single "free" redraw of seven new cards if a player's initial hand contains seven, six, one or zero lands.

Confessing his love for games combining both luck and skill, "Magic" creator Richard Garfield admitted its influence in his design of "Magic". In addressing the complaint about luck influencing a game, Garfield points out that new and casual players tend to appreciate luck as a leveling field, in which a random effect increases their chances of winning. Meanwhile, a player with higher skills appreciates a game with less chance, as the higher degree of control increases their chances of winning. According to Garfield, "Magic" has and would likely continue decreasing its degree of luck as the game matured. The "Mulligan rule", as well as card design, past vs. present, are good examples of this trend. He feels that this is a universal trend for maturing games. Garfield explained using chess as an example, that unlike modern chess, in predecessors, players would use dice to determine which chess piece to move.

The original set of rules prescribed that all games were to be played for ante. Garfield was partly inspired by the game of marbles and wanted folks to play with the cards rather than collect them. For Magic, each player removed a card at random from the deck they wished to play with and the two cards would be set aside as the ante. At the end of the match, the winner would take and keep both cards. Early sets included a few cards with rules designed to interact with this gambling aspect, allowing replacements of cards up for ante, adding more cards to the ante, or even permanently trading cards in play.

The ante concept became controversial because many regions had restrictions on games of chance. The rule was later made optional because of these restrictions and because of players' reluctance to possibly lose a card that they owned. The gambling rule is forbidden at sanctioned events and is now mostly a relic of the past, though it still sees occasional usage in friendly games as well as the format. The last card to mention ante was printed in the 1995 expansion set "".

"Magic" tournaments regularly occur in gaming stores and other venues. Larger tournaments with hundreds of competitors from around the globe sponsored by Wizards of the Coast are arranged many times every year, with substantial cash prizes for the top finishers. A number of websites report on tournament news, give complete lists for the most currently popular decks, and feature articles on current issues of debate about the game. The DCI, which is owned and operated by Wizards of the Coast, is the organizing body for sanctioned "Magic" events. The two major categories of tournament play are "Constructed" and "Limited".

In "Constructed" tournaments, each player arrives with a pre-built deck, which must have a minimum of sixty cards and follow other deck construction rules. The deck may also have up to a fifteen card sideboard, which allows players to modify their deck. Normally the first player to win two games is the winner of the match.

Different formats of Constructed "Magic" exist, each allowing different cards. The DCI maintains a "Banned and Restricted List" for each format; players may not use banned cards at all, and restricted cards are limited to one copy per deck. The DCI bans cards that it determines are damaging the health of a format; it seeks to use this remedy as infrequently as possible, and only a handful of cards have been banned in recent years.

In "Limited" tournaments, players construct decks using booster packs plus any additional basic lands of their choice. The decks in Limited tournaments must be a minimum of forty cards. All unused cards function as the sideboard, which, as in "Constructed" formats, can be freely exchanged between games of a match, as long as the deck continues to adhere to the forty card minimum. The rule that a player may use only four copies of any given card does not apply.
Players often create their own formats based on any number of criteria. Sometimes these can be based on limiting the financial value of a deck, mixing and matching different blocks or sets, or taking an existing format and modifying the DCI Banned List. Commander (formerly Elder Dragon Highlander) was one such format, before being officially supported by wizards. One of the most popular player created formats for Limited is Cube Drafting. Similar in structure to Draft, players will instead use a collection of pre-selected cards instead of random boosters to draft from. Since 2014 player created formats are allowed as Friday Night Magic events, so long as they follow basic Magic Tournament Rules (no fake cards, no gambling etc.)

The DCI maintains a set of rules for being able to sanction tournaments, as well as runs its own circuit. Local shops often offer "Friday Night Magic" tournaments as a stepping-stone to more competitive play. The DCI runs the as a series of major tournaments to attract interest. The right to compete in a Pro Tour has to be earned by either winning a Pro Tour Qualifier Tournament or being successful in a previous tournament on a similar level. A Pro Tour is usually structured into two days of individual competition played in the Swiss format. On the final day, the top eight players compete with each other in an elimination format to select the winner.

At the end of the competition in a Pro Tour, players are awarded depending on their finishing place. If the player finishes high enough, they will also be awarded prize money. Frequent winners of these events have made names for themselves in the "Magic" community, such as Gabriel Nassif, Kai Budde and Jon Finkel. As a promotional tool, the DCI launched the in 2005 to honor selected players.

At the end of the year the is held. The World Championship functions like a Pro Tour, except that competitors have to present their skill in three different formats (usually Standard, booster draft and a second constructed format) rather than one. Another difference is that invitation to the World Championship can be gained not through Pro Tour Qualifiers, but via the national championship of a country. Most countries send their top four players of the tournament as representatives, though nations with minor "Magic" playing communities may send just one player. The World Championship also has a team-based competition, where the national teams compete with each other.

At the beginning of the World Championship, new members are inducted into the Hall of Fame. The tournament also concludes the current season of tournament play and at the end of the event, the player who earned the most Pro Points during the year is awarded the title "". The player who earned the most Pro Points and did not compete in any previous season is awarded the title "".

Invitation to a Pro Tour, Pro Points and prize money can also be earned in lesser tournaments called that are open to the general public and are held more frequently throughout the year. Grand Prix events are usually the largest "Magic" tournaments, sometimes drawing more than 2,000 players. The largest "Magic" tournament ever held was Grand Prix: Las Vegas in June 2013 with a total of 4,500 players.

"Magic: The Gathering" cards are produced in much the same way as normal playing cards. Each "Magic" card, approximately 63 × 88 mm in size (2.5 by 3.5 inches), has a face which displays the card's name and rules text as well as an illustration appropriate to the card's concept. 16,203 unique cards have been produced for the game ,
many of them with variant editions, artwork, or layouts, and 600–1000 new ones are added each year. The first "Magic" cards were printed exclusively in English, but current sets are also printed in Simplified Chinese, Traditional Chinese, French, German, Italian, Japanese, Korean, Portuguese, Russian, and Spanish.

The overwhelming majority of "Magic" cards are issued and marketed in the form of sets. For the majority of its history there were two types: the Core Set and the themed expansion sets. Under Wizards of the Coast's current production and marketing scheme, a new set is released quarterly. Various products are released with each set to appeal to different segments of the "Magic" playing community:
"Shards of Alara" also debuted mythic rares (red-orange), which replace one in eight rare cards on average. There are also premium versions of every card with holographic foil, randomly inserted into some boosters in place of a common, which replace about one in seventy cards.

Expansion sets are currently released in a two-set block, starting with a large set and ending with a smaller one three months later. Prior to 2016, expansion sets were released in a three-set block (again, beginning with a larger set followed by two smaller sets). These sets consist almost exclusively of newly designed cards. Contrasting with the wide-ranging Core Set, each expansion is focused around a subset of mechanics and ties into a set storyline. Expansions also dedicate several cards to a handful of particular, often newly introduced, game mechanics.

The Core Sets began to be released annually (previously biennially) in July 2009 coinciding with the name change from to "Magic 2010". This shift also introduced new, never before printed cards into the core set, something that previously had never been done. However, core sets were discontinued following the release of "Magic Origins", on July 17, 2015, at the same time that two-set blocks were introduced. Wizards of Coast announced on June 12, 2017 that they plan on revamping and reintroducing a revamped core set for the year 2018.

In addition to the quarterly set releases, "Magic" cards are released in other products as well, such as the "Planechase" and "" spin-off games. These combine reprinted "Magic" cards with new, oversize cards with new functionality. "Magic" cards are also printed specifically for collectors, such as the "From the Vault" and "Premium Deck Series" sets, which contain exclusively premium foil cards.

In 2003, starting with the "" Core Set, the game went through its biggest visual change since its creation—a new card frame layout was developed to allow more rules text and larger art on the cards, while reducing the thick, colored border to a minimum. The new frame design aimed to improve contrast and readability using black type instead of the previous white, a new font, and partitioned areas for the name, card type, and power and toughness.

For the first few years of its production, "Magic: The Gathering" featured a small number of cards with names or artwork with demonic or occultist themes, in 1995 the company elected to remove such references from the game. In 2002, believing that the depiction of demons was becoming less controversial and that the game had established itself sufficiently, Wizards of the Coast reversed this policy and resumed printing cards with "demon" in their names.

"Magic: The Gathering" video games, comics, and books have been produced under licensing or directly by Wizards of the Coast.

In September 2011, Hasbro and IDW Publishing accorded to make a four-issue mini-series about "Magic: The Gathering" with a new story but heavily based on MTG elements and with a new Planeswalker called "Dack Fayden", which story is mainly developed in the planes of Ravnica and Innistrad. The ongoing series started in February 2012.

In 2015 Wizards of the Coast and Hasbro published "Magic: The Gathering – Arena of the Planeswalkers". Arena of the Planeswalkers is a tactical boardgame where the players maneuver miniatures over a customizable board game, and the ruleset and terrain is based on Heroscape, but with an addition of spell cards and summoning. The original master set includes miniatures that represent the five Planeswalkers Gideon, Jace, Liliana, Chandra, and Nissa as well as select creatures from the Magic: The Gathering universe. They later released an expansion "Battle for Zendikar" featuring multi-color Planeswalkers Kiora and Ob Nixilis and a colorless Eldrazi Ruiner, and a second master set Shadows Over Innistrad which has 4 new Planeswalkers and also includes the addition of cryptoliths.

While comics and books have mostly been supplements to develop a background story for the game, several video games have been produced which lean in varying degree on the original game. For the first computer games Wizards of the Coast had sold licenses to Acclaim and MicroProse roughly at the same time. While MicroProse's "" received favorable reviews, Acclaim's "Magic: The Gathering: BattleMage" was mostly dismissed with negative reaction.

With "" or MTGO for short, Wizards developed and released a computer version of the game themselves that allows players to compete online against other players using the original "Magic" cards and rules. Players purchase digital cards, and are able to play online against each other using their digital collections. Magic: The Gathering Online is the closest to paper magic of the digital alternatives.

A stripped down version of MTGO is "" which was developed by Stainless Games and released for the Xbox 360 in June 2009. The game was ported to Windows in June of the next year. Six months after the PC release of "Duels of the Planeswalkers", the game was ported to the PlayStation 3 platform. The game was the most-played Xbox Live title for two weeks after its release. Stainless continued to release yearly updates to this, culminating in "Magic Duels", a free-to-play title released in 2015.

Hiberium and D3 Publisher licensed "Magic: the Gathering" for its mobile game, "", combining deck building with match-3-style casual gaming. This was released in December 2015 and continues to be updated with new card sets from the physical game.

Cryptic Studios and Perfect World Entertainment have announced plans to create a "Magic: The Gathering" massively multiplayer online role-playing game, to be released for personal computers and consoles.

In January 2014, 20th Century Fox acquired the rights to produce a "Magic: The Gathering" film with Simon Kinberg as producer and TSG Entertainment (its co-financing partner), Hasbro Studios and subsidiary company Allspark Pictures as co-financers, after Universal Pictures allegedly dropped the film from their schedule (Both Universal and Hasbro had been developing the original "Magic: The Gathering" film since 2009).

In April, 2016, "Enter the Battlefield", a documentary about life on the Magic Pro Tour was released. The film was written by Greg Collins, Nathan Holt, and Shawn Kornhauser.

In 1998, PGI Limited created "", which was a parody of "Magic: The Gathering". Wizards of the Coast, which owned the rights to "Magic: The Gathering", took active steps to hinder the distribution of the game and successfully shut out PGI Limited from attending GenCon in July 1998. In an attempt to avoid breaching copyright and Richard Garfield's patent, each starter deck of "Havic" had printed on the back side, "This is a Parody", and on the bottom of the rule card was printed, ""Do not have each player": construct their own library of predetermined number of game components by examining and selecting [the] game components from [a] reservoir of game components or you may infringe on U.S. Patent No. 5,662,332 to Garfield."

Three official parody expansions of "Magic" exist: "", "", and "". Most of the cards in these sets feature silver borders and humorous themes. The silver-bordered cards are not legal for play in DCI-sanctioned tournaments.

There is an active secondary market in individual cards among players and game shops. Many physical and online stores sell single cards or "playsets" of four of a card. Common cards rarely sell for more than a few cents and are usually sold in bulk. Uncommon cards and weak rare cards typically sell from 10¢ up to $1. The more expensive cards in standard tournament play are typically priced between $1 to $25, although many commonly played cards in the modern and legacy formats sell for $60 to $180. Foil versions of rare and mythic rare cards are typically priced at about twice as much as the regular versions. Some of the more sought-after rare and mythic rare cards can have foil versions that cost up to three or four times more than the non-foil versions.

A few of the oldest cards, due to smaller printings and limited distribution, are highly valued and rare. This is partly due to the "Reserved List", a list of cards from the sets "Alpha" to "Urza's Destiny" (1994–1999) that Wizards has promised never to reprint.
The most expensive card that was in regular print (as opposed to being a promotional or special printing) is "Black Lotus". In 2013, a "Pristine 9.5 grade" Beckett Grading Services graded Alpha Black Lotus was bought by an anonymous buyer, for a record $27,302.

The secondary market started with comic book stores, and hobby shops displaying and selling cards, with the cards' values determined somewhat arbitrarily by the employees of the store. With the expansion of the internet, prices of cards were determined by the amount of tournament deck lists a given card would appear in. If a card was played in a tournament more frequently, the cost of the card would be higher (in addition to the market availability of the card). When eBay, Amazon, and other large online markets started to gain popularity, the "Magic" secondary market evolved substantially. Buying and selling "Magic" cards online became a source of income for people who learned how to manipulate the market. Today, the secondary market is so large and complex, it has become an area of study for consumer research, and some people make a career out of market manipulation, creating mathematical models to analyze the growth of cards' worth, and predict the market value of both individual cards, and entire sets of cards.

As of late 2013, Wizards of the Coast has expressed concern over the increasing number of counterfeit cards in the secondary market. Wizards of the Coast has since made an effort to counteract the rise of counterfeits by introducing a new holofoil stamp on all rare and mythic rare cards as of Magic 2015.

Each card has an illustration to represent the flavor of the card, often reflecting the setting of the expansion for which it was designed. Much of "Magic"<nowiki>'</nowiki>s early artwork was commissioned with little specific direction or concern for visual cohesion. One infamous example was the printing of the creature Whippoorwill without the "flying" ability even though its art showed a bird in flight. The art direction team later decided to impose a few constraints so that the artistic vision more closely aligned with the design and development of the cards. Each block of cards now has its own style guide with sketches and descriptions of the various races and places featured in the setting.

A few early sets experimented with alternate art for cards. However, Wizards came to believe that this impeded easy recognition of a card and that having multiple versions caused confusion when identifying a card at a glance. Consequently, alternate art is now only used sparingly and mostly for promotional cards. When older cards are reprinted in new sets, however, Wizards of the Coast has guaranteed that they will be printed with new art to make the older cards more collectible.

As "Magic" has expanded across the globe, its artwork has had to change for its international audience. Artwork has been edited or given alternate art to comply with the governmental standards. For example, the portrayal of skeletons and most undead in artwork was prohibited by the Chinese government until 2008.

The way "Magic" storylines are conceived and deployed has changed considerably over the years. The main premise of "Magic" is that countless possible worlds (planes) exist in the , and only unique and rare beings called Planeswalkers are capable of traversing the Multiverse. This allows the game to frequently change worlds so as to renew its mechanical inspiration, while maintaining planeswalkers as recurrent, common elements across worlds. An intricate storyline underlies the cards released in each expansion and is shown in the art and flavor text of the cards, as well as in novels and anthologies published by Wizards of the Coast (and formerly by Harper Prism). Important storyline characters, objects and locations often appear as cards in "Magic" sets, usually as "Legendary" creatures, artifacts, and lands, or as "Planeswalker" cards.

The original "Magic: The Gathering Limited Edition" has no overarching storyline, and the cards only have unconnected bits of lore and trivia to give the cards some individual depth. In the early expansion sets until "" there is usually no real story arc either. Instead, some of these sets are inspired from mythologies of various cultures. This is most apparent in , that takes some of the One Thousand and One Nights characters and makes them into "Magic" cards. Norse mythological influences can be seen worked into and African influences into . However, not all of the early sets can be linked as directly to earth mythology. touches on an independent storyline about two warring brothers, Urza and Mishra. is the exception in that period. For this set, a back story was first conceived and the cards in the set were designed afterwards to fit the storyline.

Beginning with the "Weatherlight" expansion there was a shift in the way "Magic" storylines were used. For the blocks "Weatherlight" through "", the story was laid out in a character driven story, following the events of the Weatherlight ship and its crew. With help of the planeswalking capabilities of the Weatherlight, the protagonists travel through the multiverse to fight Yawgmoth and his army of Phyrexians. "" through "" are an unconnected storyline set 100 years later on Dominaria where multiple factions battle for control of the Mirari, a powerful magical artifact left by Karn. After "", "Magic" storylines have mostly panned away from Dominaria. New planes were created to set the scene for new storylines. In contrast to the previous character driven stories, these releases focused on thematic worlds. This was the model from through , a world split into five magically and culturally distinct "shards" but later reunited.

After Alara, "Magic" visited , a world used as a prison to entrap a race of interplanar parasitic monsters called the Eldrazi, which were inspired by H. P. Lovecraft's Old Ones. Beginning with "Zendikar" the world-centric storytelling was complemented by an overlying story layer. Planeswalker cards had been introduced in "Lorwyn" and these Planeswalker characters were used to give the overarching storyline a sense of continuity, despite the constant change of setting. The block following Zendikar, "Scars of Mirrodin", revisited the plane of Mirrodin, where the Mirran natives battled against an invading Phyrexian corruption unwittilingly left by Karn (again interconnecting various storylines). To further integrate the storyline into the gameplay, certain events for the second set, "Mirrodin Besieged", encouraged players to affiliate themselves with either the Mirran or Phyrexian faction. Much of the recent focus has been on both integrating the play experience with the story line and on making mechanics and individual cards which represent pivotal points in the story.

On "Innistrad", a plane inspired heavily by gothic horror, its guardian angel has gone missing. Darkness has started to consume the plane, and the players must discover that the Helvault, a magical prison, has been holding the archangel Avacyn as well as demons. Thalia, a cathar of the Church of Avacyn, broke open the Helvault and released Avacyn as well as all of the demons. In the "Return to Ravnica" block, players were encouraged to affiliate themselves with a guild and take control of the city of Ravnica by completing the maze discovered by Niv-Mizzet.

"Theros" was a plane inspired by Greek mythology, containing many references to Greek mythological figures such as Prometheus and the pantheon of gods. "Tarkir" would have been a plane where dragons had long since died, controlled by five clans ruled by khans. Through time travel, the result of the struggle between the ancient clans and the dragons was reversed and the dragons now reign over each of the five clans, which are both similar and different to their alternate-timeline predecessors.

"Battle for Zendikar" was a return to the plane of Zendikar, which had been ravaged by the Eldrazi horrors. This marks a change in "Magic"s storytelling, where each block's story is shown from the perspective of a group of planeswalkers called the Gatewatch. "Shadows Over Innistrad" is a return to Innistrad, where Avacyn has been corrupted. The next set, "Eldritch Moon", focuses on the fact that Emrakul, the most powerful Eldrazi titan who had been missing from the "Battle for Zendikar" storyline, is now on Innistrad. Together, the Gatewatch must find a way to save the plane from Emrakul's influence. This story also focuses on cosmic horror instead of the traditional gothic horror of old Innistrad.

"Kaladesh" has the Gatewatch go to Chandra Nalaar's home world, the titular plane of Kaladesh, where she finds her mother (presumed dead) and almost kills Tezzeret. Tezzeret later kidnaps Rashmi, winner of the famous Inventor's Fair, and begins a dastardly plot to control the ruling Consulate. With the Consulate imprisoning inventors and confiscating their devices following the Fair, tensions between the populace and the government reach a boiling point, as depicted in "Aether Revolt". The block focuses on a Steampunk aesthetic, with the steam replaced by the powerful material aether.

Amonkhet has the Gatewatch set out to destroy the evil dragon planeswalker Nicol Bolas after learning of his dominion over the titular desert plane Amonkhet. In the desert, they find a city (Naktamun) teeming with food, water, and life, ruled by five gods, with Bolas seemingly absent altogether. The people of Naktamun train their entire lives to die in ritual combat, hoping to experience pure bliss in the afterlife when the God-Pharaoh (Bolas) returns to Amonkhet. In "Hour of Devastation", Bolas returns as prophesied, only to raze Naktamun and reveal the true purpose of the training and combat: to create an army of physically-perfect and combat-adept mummies to serve as an unquestionably loyal army. The block's setting is based ancient Egypt, with themes of social hierarchy and contrast between life and death.

There are several examples of academic, peer-reviewed research concerning different aspects of "Magic: The Gathering". One study examined how players use their imaginations when playing. This research studied hobby players and showed how players sought to create and participate in an epic fantasy narrative. Another example used online auctions for "Magic" cards to test revenue outcomes for various auction types. A final example uses probability to examine "Magic" card-collecting strategies. Using a specific set of cards in a specialized manner has shown "Magic: The Gathering" to be Turing complete.



</doc>
<doc id="18831" url="https://en.wikipedia.org/wiki?curid=18831" title="Mathematics">
Mathematics

Mathematics (from Greek μάθημα "máthēma", "knowledge, study, learning") is the study of such topics as quantity, structure, space, and change. It has no generally accepted definition.

Mathematicians seek and use patterns to formulate new conjectures; they resolve the truth or falsity of conjectures by mathematical proof. When mathematical structures are good models of real phenomena, then mathematical reasoning can provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity from as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry.

Rigorous arguments first appeared in Greek mathematics, most notably in Euclid's "Elements". Since the pioneering work of Giuseppe Peano (1858–1932), David Hilbert (1862–1943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day.

Galileo Galilei (1564–1642) said, "The universe cannot be read until we have learned the language and become familiar with the characters in which it is written. It is written in mathematical language, and the letters are triangles, circles and other geometrical figures, without which means it is humanly impossible to comprehend a single word. Without these, one is wandering about in a dark labyrinth." Carl Friedrich Gauss (1777–1855) referred to mathematics as "the Queen of the Sciences". Benjamin Peirce (1809–1880) called mathematics "the science that draws necessary conclusions". David Hilbert said of mathematics: "We are not speaking here of arbitrariness in any sense. Mathematics is not like a game whose tasks are determined by arbitrarily stipulated rules. Rather, it is a conceptual system possessing internal necessity that can only be so and by no means otherwise." Albert Einstein (1879–1955) stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."

Mathematics is essential in many fields, including natural science, engineering, medicine, finance and the social sciences. Applied mathematics has led to entirely new mathematical disciplines, such as statistics and game theory. Mathematicians engage in pure mathematics, or mathematics for its own sake, without having any application in mind. Practical applications for what began as pure mathematics are often discovered.

The history of mathematics can be seen as an ever-increasing series of abstractions. The first abstraction, which is shared by many animals, was probably that of numbers: the realization that a collection of two apples and a collection of two oranges (for example) have something in common, namely quantity of their members.

As evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also recognized how to count abstract quantities, like time – days, seasons, years.

Evidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra and geometry for taxation and other financial calculations, for building and construction, and for astronomy. The earliest uses of mathematics were in trading, land measurement, painting and weaving patterns and the recording of time.

In Babylonian mathematics, elementary arithmetic (addition, subtraction, multiplication and division) first appears in the archaeological record. Numeracy pre-dated writing and numeral systems have been many and diverse, with the first known written numerals created by Egyptians in Middle Kingdom texts such as the Rhind Mathematical Papyrus.

Between 600 and 300 BC the Ancient Greeks began a systematic study of mathematics in its own right with Greek mathematics.

During the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics: most of them include the contributions from Persian mathematicians such as Al-Khwarismi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī.

Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made today. According to Mikhail B. Sevryuk, in the January 2006 issue of the "Bulletin of the American Mathematical Society", "The number of papers and books included in the "Mathematical Reviews" database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs."

The word "mathematics" comes from Ancient Greek μάθημα ("máthēma"), meaning "that which is learnt", "what one gets to know", hence also "study" and "science". The word for "mathematics" came to have the narrower and more technical meaning "mathematical study" even in Classical times. Its adjective is ("mathēmatikós"), meaning "related to learning" or "studious", which likewise further came to mean "mathematical". In particular, ("mathēmatikḗ tékhnē"), , meant "the mathematical art".

Similarly, one of the two main schools of thought in Pythagoreanism was known as the "mathēmatikoi" (μαθηματικοί)—which at the time meant "teachers" rather than "mathematicians" in the modern sense.

In Latin, and in English until around 1700, the term "mathematics" more commonly meant "astrology" (or sometimes "astronomy") rather than "mathematics"; the meaning gradually changed to its present one from about 1500 to 1800. This has resulted in several mistranslations. For example, Saint Augustine's warning that Christians should beware of "mathematici", meaning astrologers, is sometimes mistranslated as a condemnation of mathematicians.

The apparent plural form in English, like the French plural form (and the less commonly used singular derivative ), goes back to the Latin neuter plural (Cicero), based on the Greek plural ("ta mathēmatiká"), used by Aristotle (384–322 BC), and meaning roughly "all things mathematical"; although it is plausible that English borrowed only the adjective "mathematic(al)" and formed the noun "mathematics" anew, after the pattern of "physics" and "metaphysics", which were inherited from Greek. In English, the noun "mathematics" takes a singular verb. It is often shortened to "maths" or, in North America, "math".

Aristotle defined mathematics as "the science of quantity", and this definition prevailed until the 18th century. Starting in the 19th century, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions. Some of these definitions emphasize the deductive character of much of mathematics, some emphasize its abstractness, some emphasize certain topics within mathematics. Today, no consensus on the definition of mathematics prevails, even among professionals. There is not even consensus on whether mathematics is an art or a science. A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable. Some just say, "Mathematics is what mathematicians do."

Three leading types of definition of mathematics are called logicist, intuitionist, and formalist, each reflecting a different philosophical school of thought. All have severe problems, none has widespread acceptance, and no reconciliation seems possible.

An early definition of mathematics in terms of logic was Benjamin Peirce's "the science that draws necessary conclusions" (1870). In the "Principia Mathematica", Bertrand Russell and Alfred North Whitehead advanced the philosophical program known as logicism, and attempted to prove that all mathematical concepts, statements, and principles can be defined and proved entirely in terms of symbolic logic. A logicist definition of mathematics is Russell's "All Mathematics is Symbolic Logic" (1903).

Intuitionist definitions, developing from the philosophy of mathematician L.E.J. Brouwer, identify mathematics with certain mental phenomena. An example of an intuitionist definition is "Mathematics is the mental activity which consists in carrying out constructs one after the other." A peculiarity of intuitionism is that it rejects some mathematical ideas considered valid according to other definitions. In particular, while other philosophies of mathematics allow objects that can be proved to exist even though they cannot be constructed, intuitionism allows only mathematical objects that one can actually construct.

Formalist definitions identify mathematics with its symbols and the rules for operating on them. Haskell Curry defined mathematics simply as "the science of formal systems". A formal system is a set of symbols, or "tokens", and some "rules" telling how the tokens may be combined into "formulas". In formal systems, the word "axiom" has a special meaning, different from the ordinary meaning of "a self-evident truth". In formal systems, an axiom is a combination of tokens that is included in a given formal system without needing to be derived using the rules of the system.

The German mathematician Carl Friedrich Gauss referred to mathematics as "the Queen of the Sciences". More recently, Marcus du Sautoy has called mathematics "the Queen of Science ... the main driving force behind scientific discovery". In the original Latin "Regina Scientiarum", as well as in German "Königin der Wissenschaften", the word corresponding to "science" means a "field of knowledge", and this was the original meaning of "science" in English, also; mathematics is in this sense a field of knowledge. The specialization restricting the meaning of "science" to "natural science" follows the rise of Baconian science, which contrasted "natural science" to scholasticism, the Aristotelean method of inquiring from first principles. The role of empirical experimentation and observation is negligible in mathematics, compared to natural sciences such as biology, chemistry, or physics. Albert Einstein stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."

Many philosophers believe that mathematics is not experimentally falsifiable, and thus not a science according to the definition of Karl Popper. However, in the 1930s Gödel's incompleteness theorems convinced many mathematicians that mathematics cannot be reduced to logic alone, and Karl Popper concluded that "most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently." Other thinkers, notably Imre Lakatos, have applied a version of falsificationism to mathematics itself.

An alternative view is that certain scientific fields (such as theoretical physics) are mathematics with axioms that are intended to correspond to reality. Mathematics shares much in common with many fields in the physical sciences, notably the exploration of the logical consequences of assumptions. Intuition and experimentation also play a role in the formulation of conjectures in both mathematics and the (other) sciences. Experimental mathematics continues to grow in importance within mathematics, and computation and simulation are playing an increasing role in both the sciences and mathematics.

The opinions of mathematicians on this matter are varied. Many mathematicians feel that to call their area a science is to downplay the importance of its aesthetic side, and its history in the traditional seven liberal arts; others feel that to ignore its connection to the sciences is to turn a blind eye to the fact that the interface between mathematics and its applications in science and engineering has driven much development in mathematics. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematics is "created" (as in art) or "discovered" (as in science). It is common to see universities divided into sections that include a division of "Science and Mathematics", indicating that the fields are seen as being allied but that they do not coincide. In practice, mathematicians are typically grouped with scientists at the gross level but separated at finer levels. This is one of many issues considered in the philosophy of mathematics.

Mathematics arises from many different kinds of problems. At first these were found in commerce, land measurement, architecture and later astronomy; today, all sciences suggest problems studied by mathematicians, and many problems arise within mathematics itself. For example, the physicist Richard Feynman invented the path integral formulation of quantum mechanics using a combination of mathematical reasoning and physical insight, and today's string theory, a still-developing scientific theory which attempts to unify the four fundamental forces of nature, continues to inspire new mathematics.

Some mathematics is relevant only in the area that inspired it, and is applied to solve further problems in that area. But often mathematics inspired by one area proves useful in many areas, and joins the general stock of mathematical concepts. A distinction is often made between pure mathematics and applied mathematics. However pure mathematics topics often turn out to have applications, e.g. number theory in cryptography. This remarkable fact, that even the "purest" mathematics often turns out to have practical applications, is what Eugene Wigner has called "the unreasonable effectiveness of mathematics". As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: there are now hundreds of specialized areas in mathematics and the latest Mathematics Subject Classification runs to 46 pages. Several areas of applied mathematics have merged with related traditions outside of mathematics and become disciplines in their own right, including statistics, operations research, and computer science.

For those who are mathematically inclined, there is often a definite aesthetic aspect to much of mathematics. Many mathematicians talk about the "elegance" of mathematics, its intrinsic aesthetics and inner beauty. Simplicity and generality are valued. There is beauty in a simple and elegant proof, such as Euclid's proof that there are infinitely many prime numbers, and in an elegant numerical method that speeds calculation, such as the fast Fourier transform. G.H. Hardy in "A Mathematician's Apology" expressed the belief that these aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He identified criteria such as significance, unexpectedness, inevitability, and economy as factors that contribute to a mathematical aesthetic. Mathematicians often strive to find proofs that are particularly elegant, proofs from "The Book" of God according to Paul Erdős. The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions.

Most of the mathematical notation in use today was not invented until the 16th century. Before that, mathematics was written out in words, limiting mathematical discovery. Euler (1707–1783) was responsible for many of the notations in use today. Modern notation makes mathematics much easier for the professional, but beginners often find it daunting. According to Barbara Oakley, this can be attributed to the fact that mathematical ideas are both more "abstract" and more "encrypted" than those of natural language. Unlike natural language, where people can often equate a word (such as "cow") with the physical object it corresponds to, mathematical symbols are abstract, lacking any physical analog. Mathematical symbols are also more highly encrypted than regular words, meaning a single symbol can encode a number of different operations or ideas.

Mathematical language can be difficult to understand for beginners because even common terms, such as "or" and "only", have a more precise meaning than they have in everyday speech, and other terms such as "open" and "field" refer to specific mathematical ideas, not covered by their laymen's meanings. Mathematical language also includes many technical terms such as "homeomorphism" and "integrable" that have no meaning outside of mathematics. Additionally, shorthand phrases such as "iff" for "if and only if" belong to mathematical jargon. There is a reason for special notation and technical vocabulary: mathematics requires more precision than everyday speech. Mathematicians refer to this precision of language and logic as "rigor".

Mathematical proof is fundamentally a matter of rigor. Mathematicians want their theorems to follow from axioms by means of systematic reasoning. This is to avoid mistaken "theorems", based on fallible intuitions, of which many instances have occurred in the history of the subject. The level of rigor expected in mathematics has varied over time: the Greeks expected detailed arguments, but at the time of Isaac Newton the methods employed were less rigorous. Problems inherent in the definitions used by Newton would lead to a resurgence of careful analysis and formal proof in the 19th century. Misunderstanding the rigor is a cause for some of the common misconceptions of mathematics. Today, mathematicians continue to argue among themselves about computer-assisted proofs. Since large computations are hard to verify, such proofs may not be sufficiently rigorous.

Axioms in traditional thought were "self-evident truths", but that conception is problematic. At a formal level, an axiom is just a string of symbols, which has an intrinsic meaning only in the context of all derivable formulas of an axiomatic system. It was the goal of Hilbert's program to put all of mathematics on a firm axiomatic basis, but according to Gödel's incompleteness theorem every (sufficiently powerful) axiomatic system has undecidable formulas; and so a final axiomatization of mathematics is impossible. Nonetheless mathematics is often imagined to be (as far as its formal content) nothing but set theory in some axiomatization, in the sense that every mathematical statement or proof could be cast into formulas within set theory.

Mathematics can, broadly speaking, be subdivided into the study of quantity, structure, space, and change (i.e. arithmetic, algebra, geometry, and analysis). In addition to these main concerns, there are also subdivisions dedicated to exploring links from the heart of mathematics to other fields: to logic, to set theory (foundations), to the empirical mathematics of the various sciences (applied mathematics), and more recently to the rigorous study of uncertainty. While some areas might seem unrelated, the Langlands program has found connections between areas previously thought unconnected, such as Galois groups, Riemann surfaces and number theory.

In order to clarify the foundations of mathematics, the fields of mathematical logic and set theory were developed. Mathematical logic includes the mathematical study of logic and the applications of formal logic to other areas of mathematics; set theory is the branch of mathematics that studies sets or collections of objects. Category theory, which deals in an abstract way with mathematical structures and relationships between them, is still in development. The phrase "crisis of foundations" describes the search for a rigorous foundation for mathematics that took place from approximately 1900 to 1930. Some disagreement about the foundations of mathematics continues to the present day. The crisis of foundations was stimulated by a number of controversies at the time, including the controversy over Cantor's set theory and the Brouwer–Hilbert controversy.

Mathematical logic is concerned with setting mathematics within a rigorous axiomatic framework, and studying the implications of such a framework. As such, it is home to Gödel's incompleteness theorems which (informally) imply that any effective formal system that contains basic arithmetic, if "sound" (meaning that all theorems that can be proved are true), is necessarily "incomplete" (meaning that there are true theorems which cannot be proved "in that system"). Whatever finite collection of number-theoretical axioms is taken as a foundation, Gödel showed how to construct a formal statement that is a true number-theoretical fact, but which does not follow from those axioms. Therefore, no formal system is a complete axiomatization of full number theory. Modern logic is divided into recursion theory, model theory, and proof theory, and is closely linked to theoretical computer science, as well as to category theory. In the context of recursion theory, the impossibility of a full axiomatization of number theory can also be formally demonstrated as a consequence of the MRDP theorem.

Theoretical computer science includes computability theory, computational complexity theory, and information theory. Computability theory examines the limitations of various theoretical models of the computer, including the most well-known model – the Turing machine. Complexity theory is the study of tractability by computer; some problems, although theoretically solvable by computer, are so expensive in terms of time or space that solving them is likely to remain practically unfeasible, even with the rapid advancement of computer hardware. A famous problem is the "" problem, one of the Millennium Prize Problems. Finally, information theory is concerned with the amount of data that can be stored on a given medium, and hence deals with concepts such as compression and entropy.

The study of quantity starts with numbers, first the familiar natural numbers and integers ("whole numbers") and arithmetical operations on them, which are characterized in arithmetic. The deeper properties of integers are studied in number theory, from which come such popular results as Fermat's Last Theorem. The twin prime conjecture and Goldbach's conjecture are two unsolved problems in number theory.

As the number system is further developed, the integers are recognized as a subset of the rational numbers ("fractions"). These, in turn, are contained within the real numbers, which are used to represent continuous quantities. Real numbers are generalized to complex numbers. These are the first steps of a hierarchy of numbers that goes on to include quaternions and octonions. Consideration of the natural numbers also leads to the transfinite numbers, which formalize the concept of "infinity". According to the fundamental theorem of algebra all solutions of equations in one unknown with complex coefficients are complex numbers, regardless of degree. Another area of study is the size of sets, which is described with the cardinal numbers. These include the aleph numbers, which allow meaningful comparison of the size of infinitely large sets.

Many mathematical objects, such as sets of numbers and functions, exhibit internal structure as a consequence of operations or relations that are defined on the set. Mathematics then studies properties of those sets that can be expressed in terms of that structure; for instance number theory studies properties of the set of integers that can be expressed in terms of arithmetic operations. Moreover, it frequently happens that different such structured sets (or structures) exhibit similar properties, which makes it possible, by a further step of abstraction, to state axioms for a class of structures, and then study at once the whole class of structures satisfying these axioms. Thus one can study groups, rings, fields and other abstract systems; together such studies (for structures defined by algebraic operations) constitute the domain of abstract algebra.

By its great generality, abstract algebra can often be applied to seemingly unrelated problems; for instance a number of ancient problems concerning compass and straightedge constructions were finally solved using Galois theory, which involves field theory and group theory. Another example of an algebraic theory is linear algebra, which is the general study of vector spaces, whose elements called vectors have both quantity and direction, and can be used to model (relations between) points in space. This is one example of the phenomenon that the originally unrelated areas of geometry and algebra have very strong interactions in modern mathematics. Combinatorics studies ways of enumerating the number of objects that fit a given structure.

The study of space originates with geometry – in particular, Euclidean geometry, which combines space and numbers, and encompasses the well-known Pythagorean theorem. Trigonometry is the branch of mathematics that deals with relationships between the sides and the angles of triangles and with the trigonometric functions. The modern study of space generalizes these ideas to include higher-dimensional geometry, non-Euclidean geometries (which play a central role in general relativity) and topology. Quantity and space both play a role in analytic geometry, differential geometry, and algebraic geometry. Convex and discrete geometry were developed to solve problems in number theory and functional analysis but now are pursued with an eye on applications in optimization and computer science. Within differential geometry are the concepts of fiber bundles and calculus on manifolds, in particular, vector and tensor calculus. Within algebraic geometry is the description of geometric objects as solution sets of polynomial equations, combining the concepts of quantity and space, and also the study of topological groups, which combine structure and space. Lie groups are used to study space, structure, and change. Topology in all its many ramifications may have been the greatest growth area in 20th-century mathematics; it includes point-set topology, set-theoretic topology, algebraic topology and differential topology. In particular, instances of modern-day topology are metrizability theory, axiomatic set theory, homotopy theory, and Morse theory. Topology also includes the now solved Poincaré conjecture, and the still unsolved areas of the Hodge conjecture. Other results in geometry and topology, including the four color theorem and Kepler conjecture, have been proved only with the help of computers.

Understanding and describing change is a common theme in the natural sciences, and calculus was developed as a powerful tool to investigate it. Functions arise here, as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.

Applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Thus, "applied mathematics" is a mathematical science with specialized knowledge. The term "applied mathematics" also describes the professional specialty in which mathematicians work on practical problems; as a profession focused on practical problems, "applied mathematics" focuses on the "formulation, study, and use of mathematical models" in science, engineering, and other areas of mathematical practice.

In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics, where mathematics is developed primarily for its own sake. Thus, the activity of applied mathematics is vitally connected with research in pure mathematics.

Applied mathematics has significant overlap with the discipline of statistics, whose theory is formulated mathematically, especially with probability theory. Statisticians (working as part of a research project) "create data that makes sense" with random sampling and with randomized experiments; the design of a statistical sample or experiment specifies the analysis of the data (before the data be available). When reconsidering data from experiments and samples or when analyzing data from observational studies, statisticians "make sense of the data" using the art of modelling and the theory of inference – with model selection and estimation; the estimated models and consequential predictions should be tested on new data.

Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints: For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence. Because of its use of optimization, the mathematical theory of statistics shares concerns with other decision sciences, such as operations research, control theory, and mathematical economics.

Computational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis includes the study of approximation and discretization broadly with special concern for rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic matrix and graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.

Arguably the most prestigious award in mathematics is the Fields Medal, established in 1936 and awarded every four years (except around World War II) to as many as four individuals. The Fields Medal is often considered a mathematical equivalent to the Nobel Prize.

The Wolf Prize in Mathematics, instituted in 1978, recognizes lifetime achievement, and another major international award, the Abel Prize, was instituted in 2003. The Chern Medal was introduced in 2010 to recognize lifetime achievement. These accolades are awarded in recognition of a particular body of work, which may be innovational, or provide a solution to an outstanding problem in an established field.

A famous list of 23 open problems, called "Hilbert's problems", was compiled in 1900 by German mathematician David Hilbert. This list achieved great celebrity among mathematicians, and at least nine of the problems have now been solved. A new list of seven important problems, titled the "Millennium Prize Problems", was published in 2000. A solution to each of these problems carries a $1 million reward, only one of which (the Riemann hypothesis) is duplicated in Hilbert's problems.




</doc>
<doc id="18835" url="https://en.wikipedia.org/wiki?curid=18835" title="Manhattan (disambiguation)">
Manhattan (disambiguation)

Manhattan is a borough of New York City.

Manhattan may also refer to:











</doc>
<doc id="18836" url="https://en.wikipedia.org/wiki?curid=18836" title="Middle Ages">
Middle Ages

In the history of Europe, the Middle Ages (or Medieval Period) lasted from the 5th to the 15th century. It began with the fall of the Western Roman Empire and merged into the Renaissance and the Age of Discovery. The Middle Ages is the middle period of the three traditional divisions of Western history: classical antiquity, the medieval period, and the modern period. The medieval period is itself subdivided into the Early, High, and Late Middle Ages.

Population decline, counterurbanisation, invasion, and movement of peoples, which had begun in Late Antiquity, continued in the Early Middle Ages. The large-scale movements of the Migration Period, including various Germanic peoples, formed new kingdoms in what remained of the Western Roman Empire. In the 7th century, North Africa and the Middle East—once part of the Byzantine Empire—came under the rule of the Umayyad Caliphate, an Islamic empire, after conquest by Muhammad's successors. Although there were substantial changes in society and political structures, the break with classical antiquity was not complete. The still-sizeable Byzantine Empire, Rome's direct continuation, survived in the Eastern Mediterranean and remained a major power. The empire's law code, the "Corpus Juris Civilis" or "Code of Justinian", was rediscovered in Northern Italy in 1070 and became widely admired later in the Middle Ages. In the West, most kingdoms incorporated the few extant Roman institutions. Monasteries were founded as campaigns to Christianise pagan Europe continued. The Franks, under the Carolingian dynasty, briefly established the Carolingian Empire during the later 8th and early 9th century. It covered much of Western Europe but later succumbed to the pressures of internal civil wars combined with external invasions: Vikings from the north, Magyars from the east, and Saracens from the south.

During the High Middle Ages, which began after 1000, the population of Europe increased greatly as technological and agricultural innovations allowed trade to flourish and the Medieval Warm Period climate change allowed crop yields to increase. Manorialism, the organisation of peasants into villages that owed rent and labour services to the nobles, and feudalism, the political structure whereby knights and lower-status nobles owed military service to their overlords in return for the right to rent from lands and manors, were two of the ways society was organised in the High Middle Ages. The Crusades, first preached in 1095, were military attempts by Western European Christians to regain control of the Holy Land from Muslims. Kings became the heads of centralised nation-states, reducing crime and violence but making the ideal of a unified Christendom more distant. Intellectual life was marked by scholasticism, a philosophy that emphasised joining faith to reason, and by the founding of universities. The theology of Thomas Aquinas, the paintings of Giotto, the poetry of Dante and Chaucer, the travels of Marco Polo, and the Gothic architecture of cathedrals such as Chartres are among the outstanding achievements toward the end of this period and into the Late Middle Ages.

The Late Middle Ages was marked by difficulties and calamities including famine, plague, and war, which significantly diminished the population of Europe; between 1347 and 1350, the Black Death killed about a third of Europeans. Controversy, heresy, and the Western Schism within the Catholic Church paralleled the interstate conflict, civil strife, and peasant revolts that occurred in the kingdoms. Cultural and technological developments transformed European society, concluding the Late Middle Ages and beginning the early modern period.

The Middle Ages is one of the three major periods in the most enduring scheme for analysing European history: classical civilisation, or Antiquity; the Middle Ages; and the Modern Period.

Medieval writers divided history into periods such as the "Six Ages" or the "Four Empires", and considered their time to be the last before the end of the world. When referring to their own times, they spoke of them as being "modern". In the 1330s, the humanist and poet Petrarch referred to pre-Christian times as "antiqua" (or "ancient") and to the Christian period as "nova" (or "new"). Leonardo Bruni was the first historian to use tripartite periodisation in his "History of the Florentine People" (1442). Bruni and later historians argued that Italy had recovered since Petrarch's time, and therefore added a third period to Petrarch's two. The "Middle Ages" first appears in Latin in 1469 as "media tempestas" or "middle season". In early usage, there were many variants, including "medium aevum", or "middle age", first recorded in 1604, and "media saecula", or "middle ages", first recorded in 1625. The alternative term "medieval" (or occasionally "mediaeval" or "mediæval") derives from "medium aevum". Tripartite periodisation became standard after the German 17th-century historian Christoph Cellarius divided history into three periods: Ancient, Medieval, and Modern.

The most commonly given starting point for the Middle Ages is around 500, with the date of 476 first used by Bruni. Later starting dates are sometimes used in the outer parts of Europe. For Europe as a whole, 1500 is often considered to be the end of the Middle Ages, but there is no universally agreed upon end date. Depending on the context, events such as Christopher Columbus's first voyage to the Americas in 1492, the conquest of Constantinople by the Turks in 1453, or the Protestant Reformation in 1517 are sometimes used. English historians often use the Battle of Bosworth Field in 1485 to mark the end of the period. For Spain, dates commonly used are the death of King Ferdinand II in 1516, the death of Queen Isabella I of Castile in 1504, or the conquest of Granada in 1492. Historians from Romance-speaking countries tend to divide the Middle Ages into two parts: an earlier "High" and later "Low" period. English-speaking historians, following their German counterparts, generally subdivide the Middle Ages into three intervals: "Early", "High", and "Late". In the 19th century, the entire Middle Ages were often referred to as the "Dark Ages", but with the adoption of these subdivisions, use of this term was restricted to the Early Middle Ages, at least among historians.

The Roman Empire reached its greatest territorial extent during the 2nd century AD; the following two centuries witnessed the slow decline of Roman control over its outlying territories. Economic issues, including inflation, and external pressure on the frontiers combined to create the Crisis of the Third Century, with emperors coming to the throne only to be rapidly replaced by new usurpers. Military expenses increased steadily during the 3rd century, mainly in response to the war with the Sasanian Empire, which revived in the middle of the 3rd century. The army doubled in size, and cavalry and smaller units replaced the Roman legion as the main tactical unit. The need for revenue led to increased taxes and a decline in numbers of the curial, or landowning, class, and decreasing numbers of them willing to shoulder the burdens of holding office in their native towns. More bureaucrats were needed in the central administration to deal with the needs of the army, which led to complaints from civilians that there were more tax-collectors in the empire than tax-payers.

The Emperor Diocletian (r. 284–305) split the empire into separately administered eastern and western halves in 286; the empire was not considered divided by its inhabitants or rulers, as legal and administrative promulgations in one division were considered valid in the other. In 330, after a period of civil war, Constantine the Great (r. 306–337) refounded the city of Byzantium as the newly renamed eastern capital, Constantinople. Diocletian's reforms strengthened the governmental bureaucracy, reformed taxation, and strengthened the army, which bought the empire time but did not resolve the problems it was facing: excessive taxation, a declining birthrate, and pressures on its frontiers, among others. Civil war between rival emperors became common in the middle of the 4th century, diverting soldiers from the empire's frontier forces and allowing invaders to encroach. For much of the 4th century, Roman society stabilised in a new form that differed from the earlier classical period, with a widening gulf between the rich and poor, and a decline in the vitality of the smaller towns. Another change was the Christianisation, or conversion of the empire to Christianity, a gradual process that lasted from the 2nd to the 5th centuries.

In 376, the Goths, fleeing from the Huns, received permission from Emperor Valens (r. 364–378) to settle in the Roman province of Thracia in the Balkans. The settlement did not go smoothly, and when Roman officials mishandled the situation, the Goths began to raid and plunder. Valens, attempting to put down the disorder, was killed fighting the Goths at the Battle of Adrianople on 9 August 378. As well as the threat from such tribal confederacies from the north, internal divisions within the empire, especially within the Christian Church, caused problems. In 400, the Visigoths invaded the Western Roman Empire and, although briefly forced back from Italy, in 410 sacked the city of Rome. In 406 the Alans, Vandals, and Suevi crossed into Gaul; over the next three years they spread across Gaul and in 409 crossed the Pyrenees Mountains into modern-day Spain. The Migration Period began, when various peoples, initially largely Germanic peoples, moved across Europe. The Franks, Alemanni, and the Burgundians all ended up in northern Gaul while the Angles, Saxons, and Jutes settled in Britain, and the Vandals went on to cross the strait of Gibraltar after which they conquered the province of Africa. In the 430s the Huns began invading the empire; their king Attila (r. 434–453) led invasions into the Balkans in 442 and 447, Gaul in 451, and Italy in 452. The Hunnic threat remained until Attila's death in 453, when the Hunnic confederation he led fell apart. These invasions by the tribes completely changed the political and demographic nature of what had been the Western Roman Empire.

By the end of the 5th century the western section of the empire was divided into smaller political units, ruled by the tribes that had invaded in the early part of the century. The deposition of the last emperor of the west, Romulus Augustulus, in 476 has traditionally marked the end of the Western Roman Empire. By 493 the Italian peninsula was conquered by the Ostrogoths. The Eastern Roman Empire, often referred to as the Byzantine Empire after the fall of its western counterpart, had little ability to assert control over the lost western territories. The Byzantine emperors maintained a claim over the territory, but while none of the new kings in the west dared to elevate himself to the position of emperor of the west, Byzantine control of most of the Western Empire could not be sustained; the reconquest of the Mediterranean periphery and the Italian Peninsula (Gothic War) in the reign of Justinian (r. 527–565) was the sole, and temporary, exception.

The political structure of Western Europe changed with the end of the united Roman Empire. Although the movements of peoples during this period are usually described as "invasions", they were not just military expeditions but migrations of entire peoples into the empire. Such movements were aided by the refusal of the Western Roman elites to support the army or pay the taxes that would have allowed the military to suppress the migration. The emperors of the 5th century were often controlled by military strongmen such as Stilicho (d. 408), Aetius (d. 454), Aspar (d. 471), Ricimer (d. 472), or Gundobad (d. 516), who were partly or fully of non-Roman background. When the line of Western emperors ceased, many of the kings who replaced them were from the same background. Intermarriage between the new kings and the Roman elites was common. This led to a fusion of Roman culture with the customs of the invading tribes, including the popular assemblies that allowed free male tribal members more say in political matters than was common in the Roman state. Material artefacts left by the Romans and the invaders are often similar, and tribal items were often modelled on Roman objects. Much of the scholarly and written culture of the new kingdoms was also based on Roman intellectual traditions. An important difference was the gradual loss of tax revenue by the new polities. Many of the new political entities no longer supported their armies through taxes, instead relying on granting them land or rents. This meant there was less need for large tax revenues and so the taxation systems decayed. Warfare was common between and within the kingdoms. Slavery declined as the supply weakened, and society became more rural.
Between the 5th and 8th centuries, new peoples and individuals filled the political void left by Roman centralised government. The Ostrogoths, a Gothic tribe, settled in Roman Italy in the late fifth century under Theoderic the Great (d. 526) and set up a kingdom marked by its co-operation between the Italians and the Ostrogoths, at least until the last years of Theodoric's reign. The Burgundians settled in Gaul, and after an earlier realm was destroyed by the Huns in 436 formed a new kingdom in the 440s. Between today's Geneva and Lyon, it grew to become the realm of Burgundy in the late 5th and early 6th centuries. Elsewhere in Gaul, the Franks and Celtic Britons set up small polities. Francia was centred in northern Gaul, and the first king of whom much is known is Childeric I (d. 481). His grave was discovered in 1653 and is remarkable for its grave goods, which included weapons and a large quantity of gold.

Under Childeric's son Clovis I (r. 509–511), the founder of the Merovingian dynasty, the Frankish kingdom expanded and converted to Christianity. The Britons, related to the natives of Britannia – modern-day Great Britain – settled in what is now Brittany. Other monarchies were established by the Visigothic Kingdom in the Iberian Peninsula, the Suebi in northwestern Iberia, and the Vandal Kingdom in North Africa. In the sixth century, the Lombards settled in Northern Italy, replacing the Ostrogothic kingdom with a grouping of duchies that occasionally selected a king to rule over them all. By the late sixth century, this arrangement had been replaced by a permanent monarchy, the Kingdom of the Lombards.

The invasions brought new ethnic groups to Europe, although some regions received a larger influx of new peoples than others. In Gaul for instance, the invaders settled much more extensively in the north-east than in the south-west. Slavs settled in Central and Eastern Europe and the Balkan Peninsula. The settlement of peoples was accompanied by changes in languages. Latin, the literary language of the Western Roman Empire, was gradually replaced by vernacular languages which evolved from Latin, but were distinct from it, collectively known as Romance languages. These changes from Latin to the new languages took many centuries. Greek remained the language of the Byzantine Empire, but the migrations of the Slavs added Slavic languages to Eastern Europe.

As Western Europe witnessed the formation of new kingdoms, the Eastern Roman Empire remained intact and experienced an economic revival that lasted into the early 7th century. There were fewer invasions of the eastern section of the empire; most occurred in the Balkans. Peace with the Sasanian Empire, the traditional enemy of Rome, lasted throughout most of the 5th century. The Eastern Empire was marked by closer relations between the political state and Christian Church, with doctrinal matters assuming an importance in Eastern politics that they did not have in Western Europe. Legal developments included the codification of Roman law; the first effort—the "Codex Theodosianus"—was completed in 438. Under Emperor Justinian (r. 527–565), another compilation took place—the "Corpus Juris Civilis". Justinian also oversaw the construction of the Hagia Sophia in Constantinople and the reconquest of North Africa from the Vandals and Italy from the Ostrogoths, under Belisarius (d. 565). The conquest of Italy was not complete, as a deadly outbreak of plague in 542 led to the rest of Justinian's reign concentrating on defensive measures rather than further conquests.

At the Emperor's death, the Byzantines had control of most of Italy, North Africa, and a small foothold in southern Spain. Justinian's reconquests have been criticised by historians for overextending his realm and setting the stage for the early Muslim conquests, but many of the difficulties faced by Justinian's successors were due not just to over-taxation to pay for his wars but to the essentially civilian nature of the empire, which made raising troops difficult.

In the Eastern Empire the slow infiltration of the Balkans by the Slavs added a further difficulty for Justinian's successors. It began gradually, but by the late 540s Slavic tribes were in Thrace and Illyrium, and had defeated an imperial army near Adrianople in 551. In the 560s the Avars began to expand from their base on the north bank of the Danube; by the end of the 6th-century, they were the dominant power in Central Europe and routinely able to force the Eastern emperors to pay tribute. They remained a strong power until 796.

An additional problem to face the empire came as a result of the involvement of Emperor Maurice (r. 582–602) in Persian politics when he intervened in a succession dispute. This led to a period of peace, but when Maurice was overthrown, the Persians invaded and during the reign of Emperor Heraclius (r. 610–641) controlled large chunks of the empire, including Egypt, Syria, and Anatolia until Heraclius' successful counterattack. In 628 the empire secured a peace treaty and recovered all of its lost territories.

In Western Europe, some of the older Roman elite families died out while others became more involved with ecclesiastical than secular affairs. Values attached to Latin scholarship and education mostly disappeared, and while literacy remained important, it became a practical skill rather than a sign of elite status. In the 4th century, Jerome (d. 420) dreamed that God rebuked him for spending more time reading Cicero than the Bible. By the 6th century, Gregory of Tours (d. 594) had a similar dream, but instead of being chastised for reading Cicero, he was chastised for learning shorthand. By the late 6th century, the principal means of religious instruction in the Church had become music and art rather than the book. Most intellectual efforts went towards imitating classical scholarship, but some original works were created, along with now-lost oral compositions. The writings of Sidonius Apollinaris (d. 489), Cassiodorus (d. c. 585), and Boethius (d. c. 525) were typical of the age.

Changes also took place among laymen, as aristocratic culture focused on great feasts held in halls rather than on literary pursuits. Clothing for the elites was richly embellished with jewels and gold. Lords and kings supported entourages of fighters who formed the backbone of the military forces. Family ties within the elites were important, as were the virtues of loyalty, courage, and honour. These ties led to the prevalence of the feud in aristocratic society, examples of which included those related by Gregory of Tours that took place in Merovingian Gaul. Most feuds seem to have ended quickly with the payment of some sort of compensation. Women took part in aristocratic society mainly in their roles as wives and mothers of men, with the role of mother of a ruler being especially prominent in Merovingian Gaul. In Anglo-Saxon society the lack of many child rulers meant a lesser role for women as queen mothers, but this was compensated for by the increased role played by abbesses of monasteries. Only in Italy does it appear that women were always considered under the protection and control of a male relative.

Peasant society is much less documented than the nobility. Most of the surviving information available to historians comes from archaeology; few detailed written records documenting peasant life remain from before the 9th century. Most of the descriptions of the lower classes come from either law codes or writers from the upper classes. Landholding patterns in the West were not uniform; some areas had greatly fragmented landholding patterns, but in other areas large contiguous blocks of land were the norm. These differences allowed for a wide variety of peasant societies, some dominated by aristocratic landholders and others having a great deal of autonomy. Land settlement also varied greatly. Some peasants lived in large settlements that numbered as many as 700 inhabitants. Others lived in small groups of a few families and still others lived on isolated farms spread over the countryside. There were also areas where the pattern was a mix of two or more of those systems. Unlike in the late Roman period, there was no sharp break between the legal status of the free peasant and the aristocrat, and it was possible for a free peasant's family to rise into the aristocracy over several generations through military service to a powerful lord.

Roman city life and culture changed greatly in the early Middle Ages. Although Italian cities remained inhabited, they contracted significantly in size. Rome, for instance, shrank from a population of hundreds of thousands to around 30,000 by the end of the 6th century. Roman temples were converted into Christian churches and city walls remained in use. In Northern Europe, cities also shrank, while civic monuments and other public buildings were raided for building materials. The establishment of new kingdoms often meant some growth for the towns chosen as capitals. Although there had been Jewish communities in many Roman cities, the Jews suffered periods of persecution after the conversion of the empire to Christianity. Officially they were tolerated, if subject to conversion efforts, and at times were even encouraged to settle in new areas.

Religious beliefs in the Eastern Empire and Iran were in flux during the late sixth and early seventh centuries. Judaism was an active proselytising faith, and at least one Arab political leader converted to it. Christianity had active missions competing with the Persians' Zoroastrianism in seeking converts, especially among residents of the Arabian Peninsula. All these strands came together with the emergence of Islam in Arabia during the lifetime of Muhammad (d. 632). After his death, Islamic forces conquered much of the Eastern Empire and Persia, starting with Syria in 634–635 and reaching Egypt in 640–641, Persia between 637 and 642, North Africa in the later seventh century, and the Iberian Peninsula in 711. By 714, Islamic forces controlled much of the peninsula in a region they called Al-Andalus.

The Islamic conquests reached their peak in the mid-eighth century. The defeat of Muslim forces at the Battle of Tours in 732 led to the reconquest of southern France by the Franks, but the main reason for the halt of Islamic growth in Europe was the overthrow of the Umayyad Caliphate and its replacement by the Abbasid Caliphate. The Abbasids moved their capital to Baghdad and were more concerned with the Middle East than Europe, losing control of sections of the Muslim lands. Umayyad descendants took over the Iberian Peninsula, the Aghlabids controlled North Africa, and the Tulunids became rulers of Egypt. By the middle of the 8th century, new trading patterns were emerging in the Mediterranean; trade between the Franks and the Arabs replaced the old Roman economy. Franks traded timber, furs, swords and slaves in return for silks and other fabrics, spices, and precious metals from the Arabs.

The migrations and invasions of the 4th and 5th centuries disrupted trade networks around the Mediterranean. African goods stopped being imported into Europe, first disappearing from the interior and by the 7th century found only in a few cities such as Rome or Naples. By the end of the 7th century, under the impact of the Muslim conquests, African products were no longer found in Western Europe. The replacement of goods from long-range trade with local products was a trend throughout the old Roman lands that happened in the Early Middle Ages. This was especially marked in the lands that did not lie on the Mediterranean, such as northern Gaul or Britain. Non-local goods appearing in the archaeological record are usually luxury goods. In the northern parts of Europe, not only were the trade networks local, but the goods carried were simple, with little pottery or other complex products. Around the Mediterranean, pottery remained prevalent and appears to have been traded over medium-range networks, not just produced locally.

The various Germanic states in the west all had coinages that imitated existing Roman and Byzantine forms. Gold continued to be minted until the end of the 7th century, when it was replaced by silver coins. The basic Frankish silver coin was the denarius or denier, while the Anglo-Saxon version was called a penny. From these areas, the denier or penny spread throughout Europe during the centuries from 700 to 1000. Copper or bronze coins were not struck, nor were gold except in Southern Europe. No silver coins denominated in multiple units were minted.

Christianity was a major unifying factor between Eastern and Western Europe before the Arab conquests, but the conquest of North Africa sundered maritime connections between those areas. Increasingly the Byzantine Church differed in language, practices, and liturgy from the Western Church. The Eastern Church used Greek instead of the Western Latin. Theological and political differences emerged, and by the early and middle 8th century issues such as iconoclasm, clerical marriage, and state control of the Church had widened to the extent that the cultural and religious differences were greater than the similarities. The formal break, known as the East–West Schism, came in 1054, when the papacy and the patriarchy of Constantinople clashed over papal supremacy and excommunicated each other, which led to the division of Christianity into two Churches—the Western branch became the Roman Catholic Church and the Eastern branch the Eastern Orthodox Church.

The ecclesiastical structure of the Roman Empire survived the movements and invasions in the west mostly intact, but the papacy was little regarded, and few of the Western bishops looked to the bishop of Rome for religious or political leadership. Many of the popes prior to 750 were more concerned with Byzantine affairs and Eastern theological controversies. The register, or archived copies of the letters, of Pope Gregory the Great (pope 590–604) survived, and of those more than 850 letters, the vast majority were concerned with affairs in Italy or Constantinople. The only part of Western Europe where the papacy had influence was Britain, where Gregory had sent the Gregorian mission in 597 to convert the Anglo-Saxons to Christianity. Irish missionaries were most active in Western Europe between the 5th and the 7th centuries, going first to England and Scotland and then on to the continent. Under such monks as Columba (d. 597) and Columbanus (d. 615), they founded monasteries, taught in Latin and Greek, and authored secular and religious works.

The Early Middle Ages witnessed the rise of monasticism in the West. The shape of European monasticism was determined by traditions and ideas that originated with the Desert Fathers of Egypt and Syria. Most European monasteries were of the type that focuses on community experience of the spiritual life, called cenobitism, which was pioneered by Pachomius (d. 348) in the 4th century. Monastic ideals spread from Egypt to Western Europe in the 5th and 6th centuries through hagiographical literature such as the "Life of Anthony". Benedict of Nursia (d. 547) wrote the Benedictine Rule for Western monasticism during the 6th century, detailing the administrative and spiritual responsibilities of a community of monks led by an abbot. Monks and monasteries had a deep effect on the religious and political life of the Early Middle Ages, in various cases acting as land trusts for powerful families, centres of propaganda and royal support in newly conquered regions, and bases for missions and proselytisation. They were the main and sometimes only outposts of education and literacy in a region. Many of the surviving manuscripts of the Latin classics were copied in monasteries in the Early Middle Ages. Monks were also the authors of new works, including history, theology, and other subjects, written by authors such as Bede (d. 735), a native of northern England who wrote in the late 7th and early 8th centuries.

The Frankish kingdom in northern Gaul split into kingdoms called Austrasia, Neustria, and Burgundy during the 6th and 7th centuries, all of them ruled by the Merovingian dynasty, who were descended from Clovis. The 7th century was a tumultuous period of wars between Austrasia and Neustria. Such warfare was exploited by Pippin (d. 640), the Mayor of the Palace for Austrasia who became the power behind the Austrasian throne. Later members of his family inherited the office, acting as advisers and regents. One of his descendants, Charles Martel (d. 741), won the Battle of Poitiers in 732, halting the advance of Muslim armies across the Pyrenees. Great Britain was divided into small states dominated by the kingdoms of Northumbria, Mercia, Wessex, and East Anglia, which were descended from the Anglo-Saxon invaders. Smaller kingdoms in present-day Wales and Scotland were still under the control of the native Britons and Picts. Ireland was divided into even smaller political units, usually known as tribal kingdoms, under the control of kings. There were perhaps as many as 150 local kings in Ireland, of varying importance.

The Carolingian dynasty, as the successors to Charles Martel are known, officially took control of the kingdoms of Austrasia and Neustria in a coup of 753 led by (r. 752–768). A contemporary chronicle claims that Pippin sought, and gained, authority for this coup from Pope (pope 752–757). Pippin's takeover was reinforced with propaganda that portrayed the Merovingians as inept or cruel rulers, exalted the accomplishments of Charles Martel, and circulated stories of the family's great piety. At the time of his death in 768, Pippin left his kingdom in the hands of his two sons, Charles (r. 768–814) and Carloman (r. 768–771). When Carloman died of natural causes, Charles blocked the succession of Carloman's young son and installed himself as the king of the united Austrasia and Neustria. Charles, more often known as Charles the Great or Charlemagne, embarked upon a programme of systematic expansion in 774 that unified a large portion of Europe, eventually controlling modern-day France, northern Italy, and Saxony. In the wars that lasted beyond 800, he rewarded allies with war booty and command over parcels of land. In 774, Charlemagne conquered the Lombards, which freed the papacy from the fear of Lombard conquest and marked the beginnings of the Papal States.

The coronation of Charlemagne as emperor on Christmas Day 800 is regarded as a turning point in medieval history, marking a return of the Western Roman Empire, since the new emperor ruled over much of the area previously controlled by the Western emperors. It also marks a change in Charlemagne's relationship with the Byzantine Empire, as the assumption of the imperial title by the Carolingians asserted their equivalence to the Byzantine state. There were several differences between the newly established Carolingian Empire and both the older Western Roman Empire and the concurrent Byzantine Empire. The Frankish lands were rural in character, with only a few small cities. Most of the people were peasants settled on small farms. Little trade existed and much of that was with the British Isles and Scandinavia, in contrast to the older Roman Empire with its trading networks centred on the Mediterranean. The empire was administered by an itinerant court that travelled with the emperor, as well as approximately 300 imperial officials called counts, who administered the counties the empire had been divided into. Clergy and local bishops served as officials, as well as the imperial officials called "missi dominici", who served as roving inspectors and troubleshooters.

Charlemagne's court in Aachen was the centre of the cultural revival sometimes referred to as the "Carolingian Renaissance". Literacy increased, as did development in the arts, architecture and jurisprudence, as well as liturgical and scriptural studies. The English monk Alcuin (d. 804) was invited to Aachen and brought the education available in the monasteries of Northumbria. Charlemagne's chancery—or writing office—made use of a new script today known as Carolingian minuscule, allowing a common writing style that advanced communication across much of Europe. Charlemagne sponsored changes in church liturgy, imposing the Roman form of church service on his domains, as well as the Gregorian chant in liturgical music for the churches. An important activity for scholars during this period was the copying, correcting, and dissemination of basic works on religious and secular topics, with the aim of encouraging learning. New works on religious topics and schoolbooks were also produced. Grammarians of the period modified the Latin language, changing it from the Classical Latin of the Roman Empire into a more flexible form to fit the needs of the Church and government. By the reign of Charlemagne, the language had so diverged from the classical that it was later called Medieval Latin.

Charlemagne planned to continue the Frankish tradition of dividing his kingdom between all his heirs, but was unable to do so as only one son, Louis the Pious (r. 814–840), was still alive by 813. Just before Charlemagne died in 814, he crowned Louis as his successor. Louis's reign of 26 years was marked by numerous divisions of the empire among his sons and, after 829, civil wars between various alliances of father and sons over the control of various parts of the empire. Eventually, Louis recognised his eldest son (d. 855) as emperor and gave him Italy. Louis divided the rest of the empire between Lothair and Charles the Bald (d. 877), his youngest son. Lothair took East Francia, comprising both banks of the Rhine and eastwards, leaving Charles West Francia with the empire to the west of the Rhineland and the Alps. Louis the German (d. 876), the middle child, who had been rebellious to the last, was allowed to keep Bavaria under the suzerainty of his elder brother. The division was disputed. of Aquitaine (d. after 864), the emperor's grandson, rebelled in a contest for Aquitaine, while Louis the German tried to annex all of East Francia. Louis the Pious died in 840, with the empire still in chaos.

A three-year civil war followed his death. By the Treaty of Verdun (843), a kingdom between the Rhine and Rhone rivers was created for Lothair to go with his lands in Italy, and his imperial title was recognised. Louis the German was in control of Bavaria and the eastern lands in modern-day Germany. Charles the Bald received the western Frankish lands, comprising most of modern-day France. Charlemagne's grandsons and great-grandsons divided their kingdoms between their descendants, eventually causing all internal cohesion to be lost. In 987 the Carolingian dynasty was replaced in the western lands, with the crowning of Hugh Capet (r. 987–996) as king. In the eastern lands the dynasty had died out earlier, in 911, with the death of Louis the Child, and the selection of the unrelated Conrad I (r. 911–918) as king.

The breakup of the Carolingian Empire was accompanied by invasions, migrations, and raids by external foes. The Atlantic and northern shores were harassed by the Vikings, who also raided the British Isles and settled there as well as in Iceland. In 911, the Viking chieftain Rollo (d. c. 931) received permission from the Frankish King Charles the Simple (r. 898–922) to settle in what became Normandy. The eastern parts of the Frankish kingdoms, especially Germany and Italy, were under continual Magyar assault until the invader's defeat at the Battle of Lechfeld in 955. The breakup of the Abbasid dynasty meant that the Islamic world fragmented into smaller political states, some of which began expanding into Italy and Sicily, as well as over the Pyrenees into the southern parts of the Frankish kingdoms.

Efforts by local kings to fight the invaders led to the formation of new political entities. In Anglo-Saxon England, King Alfred the Great (r. 871–899) came to an agreement with the Viking invaders in the late 9th century, resulting in Danish settlements in Northumbria, Mercia, and parts of East Anglia. By the middle of the 10th century, Alfred's successors had conquered Northumbria, and restored English control over most of the southern part of Great Britain. In northern Britain, Kenneth MacAlpin (d. c. 860) united the Picts and the Scots into the Kingdom of Alba. In the early 10th century, the Ottonian dynasty had established itself in Germany, and was engaged in driving back the Magyars. Its efforts culminated in the coronation in 962 of (r. 936–973) as Holy Roman Emperor. In 972, he secured recognition of his title by the Byzantine Empire, which he sealed with the marriage of his son Otto II (r. 967–983) to Theophanu (d. 991), daughter of an earlier Byzantine Emperor Romanos II (r. 959–963). By the late 10th century Italy had been drawn into the Ottonian sphere after a period of instability; Otto III (r. 996–1002) spent much of his later reign in the kingdom. The western Frankish kingdom was more fragmented, and although kings remained nominally in charge, much of the political power devolved to the local lords.

Missionary efforts to Scandinavia during the 9th and 10th centuries helped strengthen the growth of kingdoms such as Sweden, Denmark, and Norway, which gained power and territory. Some kings converted to Christianity, although not all by 1000. Scandinavians also expanded and colonised throughout Europe. Besides the settlements in Ireland, England, and Normandy, further settlement took place in what became Russia and in Iceland. Swedish traders and raiders ranged down the rivers of the Russian steppe, and even attempted to seize Constantinople in 860 and 907. Christian Spain, initially driven into a small section of the peninsula in the north, expanded slowly south during the 9th and 10th centuries, establishing the kingdoms of Asturias and León.

In Eastern Europe, Byzantium revived its fortunes under Emperor Basil I (r. 867–886) and his successors Leo VI (r. 886–912) and Constantine VII (r. 913–959), members of the Macedonian dynasty. Commerce revived and the emperors oversaw the extension of a uniform administration to all the provinces. The military was reorganised, which allowed the emperors John I (r. 969–976) and Basil II (r. 976–1025) to expand the frontiers of the empire on all fronts. The imperial court was the centre of a revival of classical learning, a process known as the Macedonian Renaissance. Writers such as John Geometres (fl. early 10th century) composed new hymns, poems, and other works. Missionary efforts by both Eastern and Western clergy resulted in the conversion of the Moravians, Bulgars, Bohemians, Poles, Magyars, and Slavic inhabitants of the Kievan Rus'. These conversions contributed to the founding of political states in the lands of those peoples—the states of Moravia, Bulgaria, Bohemia, Poland, Hungary, and the Kievan Rus'. Bulgaria, which was founded around 680, at its height reached from Budapest to the Black Sea and from the Dnieper River in modern Ukraine to the Adriatic Sea. By 1018, the last Bulgarian nobles had surrendered to the Byzantine Empire.

Few large stone buildings were constructed between the Constantinian basilicas of the 4th century and the 8th century, although many smaller ones were built during the 6th and 7th centuries. By the beginning of the 8th century, the Carolingian Empire revived the basilica form of architecture. One feature of the basilica is the use of a transept, or the "arms" of a cross-shaped building that are perpendicular to the long nave. Other new features of religious architecture include the crossing tower and a monumental entrance to the church, usually at the west end of the building.

Carolingian art was produced for a small group of figures around the court, and the monasteries and churches they supported. It was dominated by efforts to regain the dignity and classicism of imperial Roman and Byzantine art, but was also influenced by the Insular art of the British Isles. Insular art integrated the energy of Irish Celtic and Anglo-Saxon Germanic styles of ornament with Mediterranean forms such as the book, and established many characteristics of art for the rest of the medieval period. Surviving religious works from the Early Middle Ages are mostly illuminated manuscripts and carved ivories, originally made for metalwork that has since been melted down. Objects in precious metals were the most prestigious form of art, but almost all are lost except for a few crosses such as the Cross of Lothair, several reliquaries, and finds such as the Anglo-Saxon burial at Sutton Hoo and the hoards of Gourdon from Merovingian France, Guarrazar from Visigothic Spain and Nagyszentmiklós near Byzantine territory. There are survivals from the large brooches in fibula or penannular form that were a key piece of personal adornment for elites, including the Irish Tara Brooch. Highly decorated books were mostly Gospel Books and these have survived in larger numbers, including the Insular Book of Kells, the Book of Lindisfarne, and the imperial Codex Aureus of St. Emmeram, which is one of the few to retain its "treasure binding" of gold encrusted with jewels. Charlemagne's court seems to have been responsible for the acceptance of figurative monumental sculpture in Christian art, and by the end of the period near life-sized figures such as the Gero Cross were common in important churches.

During the later Roman Empire, the principal military developments were attempts to create an effective cavalry force as well as the continued development of highly specialised types of troops. The creation of heavily armoured cataphract-type soldiers as cavalry was an important feature of the 5th-century Roman military. The various invading tribes had differing emphases on types of soldiers—ranging from the primarily infantry Anglo-Saxon invaders of Britain to the Vandals and Visigoths, who had a high proportion of cavalry in their armies. During the early invasion period, the stirrup had not been introduced into warfare, which limited the usefulness of cavalry as shock troops because it was not possible to put the full force of the horse and rider behind blows struck by the rider. The greatest change in military affairs during the invasion period was the adoption of the Hunnic composite bow in place of the earlier, and weaker, Scythian composite bow. Another development was the increasing use of longswords and the progressive replacement of scale armour by mail armour and lamellar armour.

The importance of infantry and light cavalry began to decline during the early Carolingian period, with a growing dominance of elite heavy cavalry. The use of militia-type levies of the free population declined over the Carolingian period. Although much of the Carolingian armies were mounted, a large proportion during the early period appear to have been mounted infantry, rather than true cavalry. One exception was Anglo-Saxon England, where the armies were still composed of regional levies, known as the "fyrd", which were led by the local elites. In military technology, one of the main changes was the return of the crossbow, which had been known in Roman times and reappeared as a military weapon during the last part of the Early Middle Ages. Another change was the introduction of the stirrup, which increased the effectiveness of cavalry as shock troops. A technological advance that had implications beyond the military was the horseshoe, which allowed horses to be used in rocky terrain.

The High Middle Ages was a period of tremendous expansion of population. The estimated population of Europe grew from 35 to 80 million between 1000 and 1347, although the exact causes remain unclear: improved agricultural techniques, the decline of slaveholding, a more clement climate and the lack of invasion have all been suggested. As much as 90 per cent of the European population remained rural peasants. Many were no longer settled in isolated farms but had gathered into small communities, usually known as manors or villages. These peasants were often subject to noble overlords and owed them rents and other services, in a system known as manorialism. There remained a few free peasants throughout this period and beyond, with more of them in the regions of Southern Europe than in the north. The practice of assarting, or bringing new lands into production by offering incentives to the peasants who settled them, also contributed to the expansion of population.

The Open-field system of agriculture was commonly practiced in most of Europe, especially in "northwestern and central Europe." Open-field agricultural communities had three basic characteristics: individual peasant holdings in the form of strips of land were scattered among the different fields belonging to the manor; crops were rotated from year to year to preserve soil fertility; and common land was used for grazing livestock and other purposes. 

Other sections of society included the nobility, clergy, and townsmen. Nobles, both the titled nobility and simple knights, exploited the manors and the peasants, although they did not own lands outright but were granted rights to the income from a manor or other lands by an overlord through the system of feudalism. During the 11th and 12th centuries, these lands, or fiefs, came to be considered hereditary, and in most areas they were no longer divisible between all the heirs as had been the case in the early medieval period. Instead, most fiefs and lands went to the eldest son. The dominance of the nobility was built upon its control of the land, its military service as heavy cavalry, control of castles, and various immunities from taxes or other impositions. Castles, initially in wood but later in stone, began to be constructed in the 9th and 10th centuries in response to the disorder of the time, and provided protection from invaders as well as allowing lords defence from rivals. Control of castles allowed the nobles to defy kings or other overlords. Nobles were stratified; kings and the highest-ranking nobility controlled large numbers of commoners and large tracts of land, as well as other nobles. Beneath them, lesser nobles had authority over smaller areas of land and fewer people. Knights were the lowest level of nobility; they controlled but did not own land, and had to serve other nobles.

The clergy was divided into two types: the secular clergy, who lived out in the world, and the regular clergy, who lived under a religious rule and were usually monks. Throughout the period monks remained a very small proportion of the population, usually less than one percent. Most of the regular clergy were drawn from the nobility, the same social class that served as the recruiting ground for the upper levels of the secular clergy. The local parish priests were often drawn from the peasant class. Townsmen were in a somewhat unusual position, as they did not fit into the traditional three-fold division of society into nobles, clergy, and peasants. During the 12th and 13th centuries, the ranks of the townsmen expanded greatly as existing towns grew and new population centres were founded. But throughout the Middle Ages the population of the towns probably never exceeded 10 percent of the total population.

Jews also spread across Europe during the period. Communities were established in Germany and England in the 11th and 12th centuries, but Spanish Jews, long settled in Spain under the Muslims, came under Christian rule and increasing pressure to convert to Christianity. Most Jews were confined to the cities, as they were not allowed to own land or be peasants. Besides the Jews, there were other non-Christians on the edges of Europe—pagan Slavs in Eastern Europe and Muslims in Southern Europe.

Women in the Middle Ages were officially required to be subordinate to some male, whether their father, husband, or other kinsman. Widows, who were often allowed much control over their own lives, were still restricted legally. Women's work generally consisted of household or other domestically inclined tasks. Peasant women were usually responsible for taking care of the household, child-care, as well as gardening and animal husbandry near the house. They could supplement the household income by spinning or brewing at home. At harvest-time, they were also expected to help with field-work. Townswomen, like peasant women, were responsible for the household, and could also engage in trade. What trades were open to women varied by country and period. Noblewomen were responsible for running a household, and could occasionally be expected to handle estates in the absence of male relatives, but they were usually restricted from participation in military or government affairs. The only role open to women in the Church was that of nuns, as they were unable to become priests.

In central and northern Italy and in Flanders, the rise of towns that were to a degree self-governing stimulated economic growth and created an environment for new types of trade associations. Commercial cities on the shores of the Baltic entered into agreements known as the Hanseatic League, and the Italian Maritime republics such as Venice, Genoa, and Pisa expanded their trade throughout the Mediterranean. Great trading fairs were established and flourished in northern France during the period, allowing Italian and German merchants to trade with each other as well as local merchants. In the late 13th century new land and sea routes to the Far East were pioneered, famously described in "The Travels of Marco Polo" written by one of the traders, Marco Polo (d. 1324). Besides new trading opportunities, agricultural and technological improvements enabled an increase in crop yields, which in turn allowed the trade networks to expand. Rising trade brought new methods of dealing with money, and gold coinage was again minted in Europe, first in Italy and later in France and other countries. New forms of commercial contracts emerged, allowing risk to be shared among merchants. Accounting methods improved, partly through the use of double-entry bookkeeping; letters of credit also appeared, allowing easy transmission of money.

The High Middle Ages was the formative period in the history of the modern Western state. Kings in France, England, and Spain consolidated their power, and set up lasting governing institutions. New kingdoms such as Hungary and Poland, after their conversion to Christianity, became Central European powers. The Magyars settled Hungary around 900 under King Árpád (d. c. 907) after a series of invasions in the 9th century. The papacy, long attached to an ideology of independence from secular kings, first asserted its claim to temporal authority over the entire Christian world; the Papal Monarchy reached its apogee in the early 13th century under the pontificate of (pope 1198–1216). Northern Crusades and the advance of Christian kingdoms and military orders into previously pagan regions in the Baltic and Finnic north-east brought the forced assimilation of numerous native peoples into European culture.

During the early High Middle Ages, Germany was ruled by the Ottonian dynasty, which struggled to control the powerful dukes ruling over territorial duchies tracing back to the Migration period. In 1024, they were replaced by the Salian dynasty, who famously clashed with the papacy under Emperor (r. 1084–1105) over Church appointments as part of the Investiture Controversy. His successors continued to struggle against the papacy as well as the German nobility. A period of instability followed the death of Emperor (r. 1111–25), who died without heirs, until Barbarossa (r. 1155–90) took the imperial throne. Although he ruled effectively, the basic problems remained, and his successors continued to struggle into the 13th century. Barbarossa's grandson Frederick II (r. 1220–1250), who was also heir to the throne of Sicily through his mother, clashed repeatedly with the papacy. His court was famous for its scholars and he was often accused of heresy. He and his successors faced many difficulties, including the invasion of the Mongols into Europe in the mid-13th century. Mongols first shattered the Kievan Rus' principalities and then invaded Eastern Europe in 1241, 1259, and 1287.

Under the Capetian dynasty the French monarchy slowly began to expand its authority over the nobility, growing out of the Île-de-France to exert control over more of the country in the 11th and 12th centuries. They faced a powerful rival in the Dukes of Normandy, who in 1066 under William the Conqueror (duke 1035–1087), conquered England (r. 1066–87) and created a cross-channel empire that lasted, in various forms, throughout the rest of the Middle Ages. Normans also settled in Sicily and southern Italy, when Robert Guiscard (d. 1085) landed there in 1059 and established a duchy that later became the Kingdom of Sicily. Under the Angevin dynasty of (r. 1154–89) and his son Richard I (r. 1189–99), the kings of England ruled over England and large areas of France, brought to the family by Henry II's marriage to Eleanor of Aquitaine (d. 1204), heiress to much of southern France. Richard's younger brother John (r. 1199–1216) lost Normandy and the rest of the northern French possessions in 1204 to the French King Philip II Augustus (r. 1180–1223). This led to dissension among the English nobility, while John's financial exactions to pay for his unsuccessful attempts to regain Normandy led in 1215 to "Magna Carta", a charter that confirmed the rights and privileges of free men in England. Under (r. 1216–72), John's son, further concessions were made to the nobility, and royal power was diminished. The French monarchy continued to make gains against the nobility during the late 12th and 13th centuries, bringing more territories within the kingdom under the king's personal rule and centralising the royal administration. Under Louis IX (r. 1226–70), royal prestige rose to new heights as Louis served as a mediator for most of Europe.

In Iberia, the Christian states, which had been confined to the north-western part of the peninsula, began to push back against the Islamic states in the south, a period known as the "Reconquista". By about 1150, the Christian north had coalesced into the five major kingdoms of León, Castile, Aragon, Navarre, and Portugal. Southern Iberia remained under control of Islamic states, initially under the Caliphate of Córdoba, which broke up in 1031 into a shifting number of petty states known as "taifas", who fought with the Christians until the Almohad Caliphate re-established centralised rule over Southern Iberia in the 1170s. Christian forces advanced again in the early 13th century, culminating in the capture of Seville in 1248.

In the 11th century, the Seljuk Turks took over much of the Middle East, occupying Persia during the 1040s, Armenia in the 1060s, and Jerusalem in 1070. In 1071, the Turkish army defeated the Byzantine army at the Battle of Manzikert and captured the Byzantine Emperor Romanus IV (r. 1068–71). The Turks were then free to invade Asia Minor, which dealt a dangerous blow to the Byzantine Empire by seizing a large part of its population and its economic heartland. Although the Byzantines regrouped and recovered somewhat, they never fully regained Asia Minor and were often on the defensive. The Turks also had difficulties, losing control of Jerusalem to the Fatimids of Egypt and suffering from a series of internal civil wars. The Byzantines also faced a revived Bulgaria, which in the late 12th and 13th centuries spread throughout the Balkans.

The crusades were intended to seize Jerusalem from Muslim control. The First Crusade was proclaimed by Pope Urban II (pope 1088–99) at the Council of Clermont in 1095 in response to a request from the Byzantine Emperor Alexios I Komnenos (r. 1081–1118) for aid against further Muslim advances. Urban promised indulgence to anyone who took part. Tens of thousands of people from all levels of society mobilised across Europe and captured Jerusalem in 1099. One feature of the crusades was the pogroms against local Jews that often took place as the crusaders left their countries for the East. These were especially brutal during the First Crusade, when the Jewish communities in Cologne, Mainz, and Worms were destroyed, and other communities in cities between the rivers Seine and the Rhine suffered destruction. Another outgrowth of the crusades was the foundation of a new type of monastic order, the military orders of the Templars and Hospitallers, which fused monastic life with military service.

The crusaders consolidated their conquests into crusader states. During the 12th and 13th centuries, there were a series of conflicts between those states and the surrounding Islamic states. Appeals from those states to the papacy led to further crusades, such as the Third Crusade, called to try to regain Jerusalem, which had been captured by Saladin (d. 1193) in 1187. In 1203, the Fourth Crusade was diverted from the Holy Land to Constantinople, and captured the city in 1204, setting up a Latin Empire of Constantinople and greatly weakening the Byzantine Empire. The Byzantines recaptured the city in 1261, but never regained their former strength. By 1291 all the crusader states had been captured or forced from the mainland, although a titular Kingdom of Jerusalem survived on the island of Cyprus for several years afterwards.

Popes called for crusades to take place elsewhere besides the Holy Land: in Spain, southern France, and along the Baltic. The Spanish crusades became fused with the "Reconquista" of Spain from the Muslims. Although the Templars and Hospitallers took part in the Spanish crusades, similar Spanish military religious orders were founded, most of which had become part of the two main orders of Calatrava and Santiago by the beginning of the 12th century. Northern Europe also remained outside Christian influence until the 11th century or later, and became a crusading venue as part of the Northern Crusades of the 12th to 14th centuries. These crusades also spawned a military order, the Order of the Sword Brothers. Another order, the Teutonic Knights, although founded in the crusader states, focused much of its activity in the Baltic after 1225, and in 1309 moved its headquarters to Marienburg in Prussia.

During the 11th century, developments in philosophy and theology led to increased intellectual activity. There was debate between the realists and the nominalists over the concept of "universals". Philosophical discourse was stimulated by the rediscovery of Aristotle and his emphasis on empiricism and rationalism. Scholars such as Peter Abelard (d. 1142) and Peter Lombard (d. 1164) introduced Aristotelian logic into theology. In the late 11th and early 12th centuries cathedral schools spread throughout Western Europe, signalling the shift of learning from monasteries to cathedrals and towns. Cathedral schools were in turn replaced by the universities established in major European cities. Philosophy and theology fused in scholasticism, an attempt by 12th- and 13th-century scholars to reconcile authoritative texts, most notably Aristotle and the Bible. This movement tried to employ a systemic approach to truth and reason and culminated in the thought of Thomas Aquinas (d. 1274), who wrote the "Summa Theologica", or "Summary of Theology".

Chivalry and the ethos of courtly love developed in royal and noble courts. This culture was expressed in the vernacular languages rather than Latin, and comprised poems, stories, legends, and popular songs spread by troubadours, or wandering minstrels. Often the stories were written down in the "chansons de geste", or "songs of great deeds", such as "The Song of Roland" or "The Song of Hildebrand". Secular and religious histories were also produced. Geoffrey of Monmouth (d. c. 1155) composed his "Historia Regum Britanniae", a collection of stories and legends about Arthur. Other works were more clearly history, such as Otto von Freising's (d. 1158) "Gesta Friderici Imperatoris" detailing the deeds of Emperor Frederick Barbarossa, or William of Malmesbury's (d. c. 1143) "Gesta Regum" on the kings of England.

Legal studies advanced during the 12th century. Both secular law and canon law, or ecclesiastical law, were studied in the High Middle Ages. Secular law, or Roman law, was advanced greatly by the discovery of the "Corpus Juris Civilis" in the 11th century, and by 1100 Roman law was being taught at Bologna. This led to the recording and standardisation of legal codes throughout Western Europe. Canon law was also studied, and around 1140 a monk named Gratian (fl. 12th century), a teacher at Bologna, wrote what became the standard text of canon law—the "Decretum".

Among the results of the Greek and Islamic influence on this period in European history was the replacement of Roman numerals with the decimal positional number system and the invention of algebra, which allowed more advanced mathematics. Astronomy advanced following the translation of Ptolemy's "Almagest" from Greek into Latin in the late 12th century. Medicine was also studied, especially in southern Italy, where Islamic medicine influenced the school at Salerno.

In the 12th and 13th centuries, Europe produced economic growth and innovations in methods of production. Major technological advances included the invention of the windmill, the first mechanical clocks, the manufacture of distilled spirits, and the use of the astrolabe. Concave spectacles were invented around 1286 by an unknown Italian artisan, probably working in or near Pisa.

The development of a three-field rotation system for planting crops increased the usage of land from one half in use each year under the old two-field system to two-thirds under the new system, with a consequent increase in production. The development of the heavy plough allowed heavier soils to be farmed more efficiently, aided by the spread of the horse collar, which led to the use of draught horses in place of oxen. Horses are faster than oxen and require less pasture, factors that aided the implementation of the three-field system.

The construction of cathedrals and castles advanced building technology, leading to the development of large stone buildings. Ancillary structures included new town halls, houses, bridges, and tithe barns. Shipbuilding improved with the use of the rib and plank method rather than the old Roman system of mortise and tenon. Other improvements to ships included the use of lateen sails and the stern-post rudder, both of which increased the speed at which ships could be sailed.

In military affairs, the use of infantry with specialised roles increased. Along with the still-dominant heavy cavalry, armies often included mounted and infantry crossbowmen, as well as sappers and engineers. Crossbows, which had been known in Late Antiquity, increased in use partly because of the increase in siege warfare in the 10th and 11th centuries. The increasing use of crossbows during the 12th and 13th centuries led to the use of closed-face helmets, heavy body armour, as well as horse armour. Gunpowder was known in Europe by the mid-13th century with a recorded use in European warfare by the English against the Scots in 1304, although it was merely used as an explosive and not as a weapon. Cannon were being used for sieges in the 1320s, and hand-held guns were in use by the 1360s.

In the 10th century the establishment of churches and monasteries led to the development of stone architecture that elaborated vernacular Roman forms, from which the term "Romanesque" is derived. Where available, Roman brick and stone buildings were recycled for their materials. From the tentative beginnings known as the First Romanesque, the style flourished and spread across Europe in a remarkably homogeneous form. Just before 1000 there was a great wave of building stone churches all over Europe. Romanesque buildings have massive stone walls, openings topped by semi-circular arches, small windows, and, particularly in France, arched stone vaults. The large portal with coloured sculpture in high relief became a central feature of façades, especially in France, and the capitals of columns were often carved with narrative scenes of imaginative monsters and animals. According to art historian C. R. Dodwell, "virtually all the churches in the West were decorated with wall-paintings", of which few survive. Simultaneous with the development in church architecture, the distinctive European form of the castle was developed and became crucial to politics and warfare.

Romanesque art, especially metalwork, was at its most sophisticated in Mosan art, in which distinct artistic personalities including Nicholas of Verdun (d. 1205) become apparent, and an almost classical style is seen in works such as a font at Liège, contrasting with the writhing animals of the exactly contemporary Gloucester Candlestick. Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a "Last Judgement" on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.

From the early 12th century, French builders developed the Gothic style, marked by the use of rib vaults, pointed arches, flying buttresses, and large stained glass windows. It was used mainly in churches and cathedrals and continued in use until the 16th century in much of Europe. Classic examples of Gothic architecture include Chartres Cathedral and Reims Cathedral in France as well as Salisbury Cathedral in England. Stained glass became a crucial element in the design of churches, which continued to use extensive wall-paintings, now almost all lost.

During this period the practice of manuscript illumination gradually passed from monasteries to lay workshops, so that according to Janetta Benton "by 1300 most monks bought their books in shops", and the book of hours developed as a form of devotional book for lay-people. Metalwork continued to be the most prestigious form of art, with Limoges enamel a popular and relatively affordable option for objects such as reliquaries and crosses. In Italy the innovations of Cimabue and Duccio, followed by the Trecento master Giotto (d. 1337), greatly increased the sophistication and status of panel painting and fresco. Increasing prosperity during the 12th century resulted in greater production of secular art; many carved ivory objects such as gaming-pieces, combs, and small religious figures have survived.

Monastic reform became an important issue during the 11th century, as elites began to worry that monks were not adhering to the rules binding them to a strictly religious life. Cluny Abbey, founded in the Mâcon region of France in 909, was established as part of the Cluniac Reforms, a larger movement of monastic reform in response to this fear. Cluny quickly established a reputation for austerity and rigour. It sought to maintain a high quality of spiritual life by placing itself under the protection of the papacy and by electing its own abbot without interference from laymen, thus maintaining economic and political independence from local lords.

Monastic reform inspired change in the secular Church. The ideals that it was based upon were brought to the papacy by Pope Leo IX (pope 1049–1054), and provided the ideology of the clerical independence that led to the Investiture Controversy in the late 11th century. This involved Pope Gregory VII (pope 1073–85) and Emperor Henry IV, who initially clashed over episcopal appointments, a dispute that turned into a battle over the ideas of investiture, clerical marriage, and simony. The emperor saw the protection of the Church as one of his responsibilities as well as wanting to preserve the right to appoint his own choices as bishops within his lands, but the papacy insisted on the Church's independence from secular lords. These issues remained unresolved after the compromise of 1122 known as the Concordat of Worms. The dispute represents a significant stage in the creation of a papal monarchy separate from and equal to lay authorities. It also had the permanent consequence of empowering German princes at the expense of the German emperors.

The High Middle Ages was a period of great religious movements. Besides the Crusades and monastic reforms, people sought to participate in new forms of religious life. New monastic orders were founded, including the Carthusians and the Cistercians. The latter especially expanded rapidly in their early years under the guidance of Bernard of Clairvaux (d. 1153). These new orders were formed in response to the feeling of the laity that Benedictine monasticism no longer met the needs of the laymen, who along with those wishing to enter the religious life wanted a return to the simpler hermetical monasticism of early Christianity, or to live an Apostolic life. Religious pilgrimages were also encouraged. Old pilgrimage sites such as Rome, Jerusalem, and Compostela received increasing numbers of visitors, and new sites such as Monte Gargano and Bari rose to prominence.

In the 13th century mendicant orders—the Franciscans and the Dominicans—who swore vows of poverty and earned their living by begging, were approved by the papacy. Religious groups such as the Waldensians and the Humiliati also attempted to return to the life of early Christianity in the middle 12th and early 13th centuries, but they were condemned as heretical by the papacy. Others joined the Cathars, another heretical movement condemned by the papacy. In 1209, a crusade was preached against the Cathars, the Albigensian Crusade, which in combination with the medieval Inquisition, eliminated them.

The first years of the 14th century were marked by famines, culminating in the Great Famine of 1315–17. The causes of the Great Famine included the slow transition from the Medieval Warm Period to the Little Ice Age, which left the population vulnerable when bad weather caused crop failures. The years 1313–14 and 1317–21 were excessively rainy throughout Europe, resulting in widespread crop failures. The climate change—which resulted in a declining average annual temperature for Europe during the 14th century—was accompanied by an economic downturn.

These troubles were followed in 1347 by the Black Death, a pandemic that spread throughout Europe during the following three years. The death toll was probably about 35 million people in Europe, about one-third of the population. Towns were especially hard-hit because of their crowded conditions. Large areas of land were left sparsely inhabited, and in some places fields were left unworked. Wages rose as landlords sought to entice the reduced number of available workers to their fields. Further problems were lower rents and lower demand for food, both of which cut into agricultural income. Urban workers also felt that they had a right to greater earnings, and popular uprisings broke out across Europe. Among the uprisings were the "jacquerie" in France, the Peasants' Revolt in England, and revolts in the cities of Florence in Italy and Ghent and Bruges in Flanders. The trauma of the plague led to an increased piety throughout Europe, manifested by the foundation of new charities, the self-mortification of the flagellants, and the scapegoating of Jews. Conditions were further unsettled by the return of the plague throughout the rest of the 14th century; it continued to strike Europe periodically during the rest of the Middle Ages.

Society throughout Europe was disturbed by the dislocations caused by the Black Death. Lands that had been marginally productive were abandoned, as the survivors were able to acquire more fertile areas. Although serfdom declined in Western Europe it became more common in Eastern Europe, as landlords imposed it on those of their tenants who had previously been free. Most peasants in Western Europe managed to change the work they had previously owed to their landlords into cash rents. The percentage of serfs amongst the peasantry declined from a high of 90 to closer to 50 percent by the end of the period. Landlords also became more conscious of common interests with other landholders, and they joined together to extort privileges from their governments. Partly at the urging of landlords, governments attempted to legislate a return to the economic conditions that existed before the Black Death. Non-clergy became increasingly literate, and urban populations began to imitate the nobility's interest in chivalry.

Jewish communities were expelled from England in 1290 and from France in 1306. Although some were allowed back into France, most were not, and many Jews emigrated eastwards, and Hungary. The Jews were expelled from Spain in 1492, and dispersed to Turkey, France, Italy, and Holland. The rise of banking in Italy during the 13th century continued throughout the 14th century, fuelled partly by the increasing warfare of the period and the needs of the papacy to move money between kingdoms. Many banking firms loaned money to royalty, at great risk, as some were bankrupted when kings defaulted on their loans.

Strong, royalty-based nation states rose throughout Europe in the Late Middle Ages, particularly in England, France, and the Christian kingdoms of the Iberian Peninsula: Aragon, Castile, and Portugal. The long conflicts of the period strengthened royal control over their kingdoms and were extremely hard on the peasantry. Kings profited from warfare that extended royal legislation and increased the lands they directly controlled. Paying for the wars required that methods of taxation become more effective and efficient, and the rate of taxation often increased. The requirement to obtain the consent of taxpayers allowed representative bodies such as the English Parliament and the French Estates General to gain power and authority.

Throughout the 14th century, French kings sought to expand their influence at the expense of the territorial holdings of the nobility. They ran into difficulties when attempting to confiscate the holdings of the English kings in southern France, leading to the Hundred Years' War, waged from 1337 to 1453. Early in the war the English under Edward III (r. 1327–77) and his son Edward, the Black Prince (d. 1376), won the battles of Crécy and Poitiers, captured the city of Calais, and won control of much of France. The resulting stresses almost caused the disintegration of the French kingdom during the early years of the war. In the early 15th century, France again came close to dissolving, but in the late 1420s the military successes of Joan of Arc (d. 1431) led to the victory of the French and the capture of the last English possessions in southern France in 1453. The price was high, as the population of France at the end of the Wars was likely half what it had been at the start of the conflict. Conversely, the Wars had a positive effect on English national identity, doing much to fuse the various local identities into a national English ideal. The conflict with France also helped create a national culture in England separate from French culture, which had previously been the dominant influence. The dominance of the English longbow began during early stages of the Hundred Years' War, and cannon appeared on the battlefield at Crécy in 1346.

In modern-day Germany, the Holy Roman Empire continued to rule, but the elective nature of the imperial crown meant there was no enduring dynasty around which a strong state could form. Further east, the kingdoms of Poland, Hungary, and Bohemia grew powerful. In Iberia, the Christian kingdoms continued to gain land from the Muslim kingdoms of the peninsula; Portugal concentrated on expanding overseas during the 15th century, while the other kingdoms were riven by difficulties over royal succession and other concerns. After losing the Hundred Years' War, England went on to suffer a long civil war known as the Wars of the Roses, which lasted into the 1490s and only ended when Henry Tudor (r. 1485–1509 as Henry VII) became king and consolidated power with his victory over Richard III (r. 1483–85) at Bosworth in 1485. In Scandinavia, Margaret I of Denmark (r. in Denmark 1387–1412) consolidated Norway, Denmark, and Sweden in the Union of Kalmar, which continued until 1523. The major power around the Baltic Sea was the Hanseatic League, a commercial confederation of city-states that traded from Western Europe to Russia. Scotland emerged from English domination under Robert the Bruce (r. 1306–29), who secured papal recognition of his kingship in 1328.

Although the Palaeologi emperors recaptured Constantinople from the Western Europeans in 1261, they were never able to regain control of much of the former imperial lands. They usually controlled only a small section of the Balkan Peninsula near Constantinople, the city itself, and some coastal lands on the Black Sea and around the Aegean Sea. The former Byzantine lands in the Balkans were divided between the new Kingdom of Serbia, the Second Bulgarian Empire and the city-state of Venice. The power of the Byzantine emperors was threatened by a new Turkish tribe, the Ottomans, who established themselves in Anatolia in the 13th century and steadily expanded throughout the 14th century. The Ottomans expanded into Europe, reducing Bulgaria to a vassal state by 1366 and taking over Serbia after its defeat at the Battle of Kosovo in 1389. Western Europeans rallied to the plight of the Christians in the Balkans and declared a new crusade in 1396; a great army was sent to the Balkans, where it was defeated at the Battle of Nicopolis. Constantinople was finally captured by the Ottomans in 1453.

During the tumultuous 14th century, disputes within the leadership of the Church led to the Avignon Papacy of 1309–76, also called the "Babylonian Captivity of the Papacy" (a reference to the Babylonian captivity of the Jews), and then to the Great Schism, lasting from 1378 to 1418, when there were two and later three rival popes, each supported by several states. Ecclesiastical officials convened at the Council of Constance in 1414, and in the following year the council deposed one of the rival popes, leaving only two claimants. Further depositions followed, and in November 1417 the council elected Martin V (pope 1417–31) as pope.

Besides the schism, the Western Church was riven by theological controversies, some of which turned into heresies. John Wycliffe (d. 1384), an English theologian, was condemned as a heretic in 1415 for teaching that the laity should have access to the text of the Bible as well as for holding views on the Eucharist that were contrary to Church doctrine. Wycliffe's teachings influenced two of the major heretical movements of the later Middle Ages: Lollardy in England and Hussitism in Bohemia. The Bohemian movement initiated with the teaching of Jan Hus, who was burned at the stake in 1415 after being condemned as a heretic by the Council of Constance. The Hussite Church, although the target of a crusade, survived beyond the Middle Ages. Other heresies were manufactured, such as the accusations against the Knights Templar that resulted in their suppression in 1312 and the division of their great wealth between the French King Philip IV (r. 1285–1314) and the Hospitallers.

The papacy further refined the practice in the Mass in the Late Middle Ages, holding that the clergy alone was allowed to partake of the wine in the Eucharist. This further distanced the secular laity from the clergy. The laity continued the practices of pilgrimages, veneration of relics, and belief in the power of the Devil. Mystics such as Meister Eckhart (d. 1327) and Thomas à Kempis (d. 1471) wrote works that taught the laity to focus on their inner spiritual life, which laid the groundwork for the Protestant Reformation. Besides mysticism, belief in witches and witchcraft became widespread, and by the late 15th century the Church had begun to lend credence to populist fears of witchcraft with its condemnation of witches in 1484 and the publication in 1486 of the "Malleus Maleficarum", the most popular handbook for witch-hunters.

During the Later Middle Ages, theologians such as John Duns Scotus (d. 1308) and William of Ockham (d. c. 1348), led a reaction against scholasticism, objecting to the application of reason to faith. Their efforts undermined the prevailing Platonic idea of "universals". Ockham's insistence that reason operates independently of faith allowed science to be separated from theology and philosophy. Legal studies were marked by the steady advance of Roman law into areas of jurisprudence previously governed by customary law. The lone exception to this trend was in England, where the common law remained pre-eminent. Other countries codified their laws; legal codes were promulgated in Castile, Poland, and Lithuania.

Education remained mostly focused on the training of future clergy. The basic learning of the letters and numbers remained the province of the family or a village priest, but the secondary subjects of the trivium—grammar, rhetoric, logic—were studied in cathedral schools or in schools provided by cities. Commercial secondary schools spread, and some Italian towns had more than one such enterprise. Universities also spread throughout Europe in the 14th and 15th centuries. Lay literacy rates rose, but were still low; one estimate gave a literacy rate of ten percent of males and one percent of females in 1500.

The publication of vernacular literature increased, with Dante (d. 1321), Petrarch (d. 1374) and Giovanni Boccaccio (d. 1375) in 14th-century Italy, Geoffrey Chaucer (d. 1400) and William Langland (d. c. 1386) in England, and François Villon (d. 1464) and Christine de Pizan (d. c. 1430) in France. Much literature remained religious in character, and although a great deal of it continued to be written in Latin, a new demand developed for saints' lives and other devotional tracts in the vernacular languages. This was fed by the growth of the "Devotio Moderna" movement, most prominently in the formation of the Brethren of the Common Life, but also in the works of German mystics such as Meister Eckhart and Johannes Tauler (d. 1361). Theatre also developed in the guise of miracle plays put on by the Church. At the end of the period, the development of the printing press in about 1450 led to the establishment of publishing houses throughout Europe by 1500.

In the early 15th century, the countries of the Iberian peninsula began to sponsor exploration beyond the boundaries of Europe. Prince Henry the Navigator of Portugal (d. 1460) sent expeditions that discovered the Canary Islands, the Azores, and Cape Verde during his lifetime. After his death, exploration continued; Bartolomeu Dias (d. 1500) went around the Cape of Good Hope in 1486 and Vasco da Gama (d. 1524) sailed around Africa to India in 1498. The combined Spanish monarchies of Castile and Aragon sponsored the voyage of exploration by Christopher Columbus (d. 1506) in 1492 that discovered the Americas. The English crown under Henry VII sponsored the voyage of John Cabot (d. 1498) in 1497, which landed on Cape Breton Island.

One of the major developments in the military sphere during the Late Middle Ages was the increased use of infantry and light cavalry. The English also employed longbowmen, but other countries were unable to create similar forces with the same success. Armour continued to advance, spurred by the increasing power of crossbows, and plate armour was developed to protect soldiers from crossbows as well as the hand-held guns that were developed. Pole arms reached new prominence with the development of the Flemish and Swiss infantry armed with pikes and other long spears.

In agriculture, the increased usage of sheep with long-fibred wool allowed a stronger thread to be spun. In addition, the spinning wheel replaced the traditional distaff for spinning wool, tripling production. A less technological refinement that still greatly affected daily life was the use of buttons as closures for garments, which allowed for better fitting without having to lace clothing on the wearer. Windmills were refined with the creation of the tower mill, allowing the upper part of the windmill to be spun around to face the direction from which the wind was blowing. The blast furnace appeared around 1350 in Sweden, increasing the quantity of iron produced and improving its quality. The first patent law in 1447 in Venice protected the rights of inventors to their inventions.

The Late Middle Ages in Europe as a whole correspond to the Trecento and Early Renaissance cultural periods in Italy. Northern Europe and Spain continued to use Gothic styles, which became increasingly elaborate in the 15th century, until almost the end of the period. International Gothic was a courtly style that reached much of Europe in the decades around 1400, producing masterpieces such as the Très Riches Heures du Duc de Berry. All over Europe secular art continued to increase in quantity and quality, and in the 15th century the mercantile classes of Italy and Flanders became important patrons, commissioning small portraits of themselves in oils as well as a growing range of luxury items such as jewellery, ivory caskets, cassone chests, and maiolica pottery. These objects also included the Hispano-Moresque ware produced by mostly Mudéjar potters in Spain. Although royalty owned huge collections of plate, little survives except for the Royal Gold Cup. Italian silk manufacture developed, so that Western churches and elites no longer needed to rely on imports from Byzantium or the Islamic world. In France and Flanders tapestry weaving of sets like "The Lady and the Unicorn" became a major luxury industry.

The large external sculptural schemes of Early Gothic churches gave way to more sculpture inside the building, as tombs became more elaborate and other features such as pulpits were sometimes lavishly carved, as in the Pulpit by Giovanni Pisano in Sant'Andrea. Painted or carved wooden relief altarpieces became common, especially as churches created many side-chapels. Early Netherlandish painting by artists such as Jan van Eyck (d. 1441) and Rogier van der Weyden (d. 1464) rivalled that of Italy, as did northern illuminated manuscripts, which in the 15th century began to be collected on a large scale by secular elites, who also commissioned secular books, especially histories. From about 1450 printed books rapidly became popular, though still expensive. There were around 30,000 different editions of incunabula, or works printed before 1500, by which time illuminated manuscripts were commissioned only by royalty and a few others. Very small woodcuts, nearly all religious, were affordable even by peasants in parts of Northern Europe from the middle of the 15th century. More expensive engravings supplied a wealthier market with a variety of images.

The medieval period is frequently caricatured as a "time of ignorance and superstition" that placed "the word of religious authorities over personal experience and rational activity." This is a legacy from both the Renaissance and Enlightenment when scholars favourably contrasted their intellectual cultures with those of the medieval period. Renaissance scholars saw the Middle Ages as a period of decline from the high culture and civilisation of the Classical world; Enlightenment scholars saw reason as superior to faith, and thus viewed the Middle Ages as a time of ignorance and superstition.

Others argue that reason was generally held in high regard during the Middle Ages. Science historian Edward Grant writes, "If revolutionary rational thoughts were expressed [in the 18th century], they were only made possible because of the long medieval tradition that established the use of reason as one of the most important of human activities". Also, contrary to common belief, David Lindberg writes, "the late medieval scholar rarely experienced the coercive power of the Church and would have regarded himself as free (particularly in the natural sciences) to follow reason and observation wherever they led".

The caricature of the period is also reflected in some more specific notions. One misconception, first propagated in the 19th century and still very common, is that all people in the Middle Ages believed that the Earth was flat. This is untrue, as lecturers in the medieval universities commonly argued that evidence showed the Earth was a sphere. Lindberg and Ronald Numbers, another scholar of the period, state that there "was scarcely a Christian scholar of the Middle Ages who did not acknowledge [Earth's] sphericity and even know its approximate circumference". Other misconceptions such as "the Church prohibited autopsies and dissections during the Middle Ages", "the rise of Christianity killed off ancient science", or "the medieval Christian Church suppressed the growth of natural philosophy", are all cited by Numbers as examples of widely popular myths that still pass as historical truth, although they are not supported by current historical research.



</doc>
<doc id="18837" url="https://en.wikipedia.org/wiki?curid=18837" title="Median">
Median

The median is the value separating the higher half from the lower half of a data sample (a population or a probability distribution). For a data set, it may be thought of as the "middle" value. For example, in the data set {1, 3, 3, 6, 7, 8, 9}, the median is 6, the fourth largest, and also the fourth smallest, number in the sample. For a continuous probability distribution, the median is the value such that a number is equally likely to fall above or below it.

The median is a commonly used measure of the properties of a data set in statistics and probability theory. The basic advantage of the median in describing data compared to the mean (often simply described as the "average") is that it is not skewed so much by extremely large or small values, and so it may give a better idea of a "typical" value. For example, in understanding statistics like household income or assets which vary greatly, a mean may be skewed by a small number of extremely high or low values. Median income, for example, may be a better way to suggest what a "typical" income is.

Because of this, the median is of central importance in robust statistics, as it is the most resistant statistic, having a breakdown point of 50%: so long as no more than half the data are contaminated, the median will not give an arbitrarily large or small result.

The median of a finite list of numbers can be found by arranging all the numbers from smallest to greatest.

If there is an odd number of numbers, the middle one is picked. For example, consider the list of numbers

This list contains seven numbers. The median is the fourth of them, which is 6.

If there is an even number of observations, then there is no single middle value; the median is then usually defined to be the mean of the two middle values. For example, in the data set

the median is the mean of the middle two numbers: this is (4 + 5) ÷ 2, which is 4.5 or 4 1/2. (In more technical terms, this interprets the median as the fully trimmed mid-range).

The formula used to find the middle number of a data set of "n" numerically ordered numbers is (n + 1) ÷ 2. This either gives the middle number (for an odd number of values) or the halfway point between the two middle values. For example, with 14 values, the formula will give 7.5, and the median will be taken by averaging the seventh and eighth values. So median can be represented by the following formula:

One can find the median using the Stem-and-Leaf Plot.

There is no widely accepted standard notation for the median, but some authors represent the median of a variable "x" either as "x͂" or as "μ" sometimes also "M". In any of these cases, the use of these or other symbols for the median needs to be explicitly defined when they are introduced.

The median is used primarily for skewed distributions, which it summarizes differently from the arithmetic mean. Consider the multiset { 1, 2, 2, 2, 3, 14 }. The median is 2 in this case, (as is the mode), and it might be seen as a better indication of central tendency (less susceptible to the exceptionally large value in data) than the arithmetic mean of 4.

The median is a popular summary statistic used in descriptive statistics, since it is simple to understand and easy to calculate, while also giving a measure that is more robust in the presence of outlier values than is the mean. The widely cited empirical relationship between the relative locations of the mean and the median for skewed distributions is, however, not generally true. There are, however, various relationships for the "absolute" difference between them; see below.

With an even number of observations (as shown above) no value need be exactly at the value of the median. Nonetheless, the value of the median is uniquely determined with the usual definition. A related concept, in which the outcome is forced to correspond to a member of the sample, is the medoid.

In a population, at most half have values strictly less than the median and at most half have values strictly greater than it. If each group contains less than half the population, then some of the population is exactly equal to the median. For example, if "a" < "b" < "c", then the median of the list {"a", "b", "c"} is "b", and, if "a" < "b" < "c" < "d", then the median of the list {"a", "b", "c", "d"} is the mean of "b" and "c"; i.e., it is ("b" + "c")/2. Indeed, as it is based on the middle data in a group, it is not necessary to even know the value of extreme results in order to calculate a median. For example, in a psychology test investigating the time needed to solve a problem, if a small number of people failed to solve the problem at all in the given time a median can still be calculated.

The median can be used as a measure of location when a distribution is skewed, when end-values are not known, or when one requires reduced importance to be attached to outliers, e.g., because they may be measurement errors.

A median is only defined on ordered one-dimensional data, and is independent of any distance metric. A geometric median, on the other hand, is defined in any number of dimensions.

The median is one of a number of ways of summarising the typical values associated with members of a statistical population; thus, it is a possible location parameter. The median is the 2nd quartile, 5th decile, and 50th percentile. Since the median is the same as the "second quartile", its calculation is illustrated in the article on quartiles. A median can be worked out for ranked but not numerical classes (e.g. working out a median grade when students are graded from A to F), although the result might be halfway between grades if there is an even number of cases.

When the median is used as a location parameter in descriptive statistics, there are several choices for a measure of variability: the range, the interquartile range, the mean absolute deviation, and the median absolute deviation.

For practical purposes, different measures of location and dispersion are often compared on the basis of how well the corresponding population values can be estimated from a sample of data. The median, estimated using the sample median, has good properties in this regard. While it is not usually optimal if a given population distribution is assumed, its properties are always reasonably good. For example, a comparison of the efficiency of candidate estimators shows that the sample mean is more statistically efficient than the sample median when data are uncontaminated by data from heavy-tailed distributions or from mixtures of distributions, but less efficient otherwise, and that the efficiency of the sample median is higher than that for a wide range of distributions. More specifically, the median has a 64% efficiency compared to the minimum-variance mean (for large normal samples), which is to say the variance of the median will be ~50% greater than the variance of the mean—see asymptotic efficiency and references therein.

For any probability distribution on the real line R with cumulative distribution function "F", regardless of whether it is any kind of continuous probability distribution, in particular an absolutely continuous distribution (which has a probability density function), or a discrete probability distribution, a median is by definition any real number "m" that satisfies the inequalities

or, equivalently, the inequalities

in which a Lebesgue–Stieltjes integral is used. For an absolutely continuous probability distribution with probability density function "ƒ", the median satisfies

Any probability distribution on R has at least one median, but in specific cases there may be more than one median. Specifically, if a probability distribution is zero on an interval ["a", "b"], and the cumulative distribution function at "a" is 1/2, any value between "a" and "b" will also be a median.

The medians of certain types of distributions can be easily calculated from their parameters; furthermore, they exist even for some distributions lacking a well-defined mean, such as the Cauchy distribution:

The "mean absolute error" of a real variable "c" with respect to the random variable "X" is
Provided that the probability distribution of "X" is such that the above expectation exists, then "m" is a median of "X" if and only if "m" is a minimizer of the mean absolute error with respect to "X". In particular, "m" is a sample median if and only if "m" minimizes the arithmetic mean of the absolute deviations.

More generally, a median is defined as a minimum of
as discussed below in the section on multivariate medians (specifically, the spatial median).

This optimization-based definition of the median is useful in statistical data-analysis, for example, in "k"-medians clustering.

It can be shown for a unimodal distribution that the median formula_7 and the mean formula_8 lie within (3/5) ≈ 0.7746 standard deviations of each other. In symbols,

where |·| is the absolute value.

A similar relation holds between the median and the mode: they lie within 3 ≈ 1.732 standard deviations of each other:

If the distribution has finite variance, then the distance between the median and the mean is bounded by one standard deviation.

This bound was proved by Mallows, who used Jensen's inequality twice, as follows. We have

The first and third inequalities come from Jensen's inequality applied to the absolute-value function and the square function, which are each convex. The second inequality comes from the fact that a median minimizes the absolute deviation function

This proof also follows directly from Cantelli's inequality.
The result can be generalized to obtain a multivariate version of the inequality, as follows:

where "m" is a spatial median, that is, a minimizer of the function
formula_14 The spatial median is unique when the data-set's dimension is two or more. An alternative proof uses the one-sided Chebyshev inequality; it appears in .

Jensen's inequality states that for any random variable "x" with a finite expectation "E"("x") and for any convex function "f"

It has been shown that if "x" is a real variable with a unique median "m" and "f" is a C function then

A C function is a real valued function, defined on the set of real numbers "R", with the property that for any real "t"

is a closed interval, a singleton or an empty set.

Even though comparison-sorting "n" items requires operations, selection algorithms can compute the 'th-smallest of items with only operations. This includes the median, which is the 'th order statistic (or for an even number of samples, the arithmetic mean of the two middle order statistics).

Selection algorithms still have the downside of requiring memory, that is, they need to have the full sample (or a linear-sized portion of it) in memory. Because this, as well as the linear time requirement, can be prohibitive, several estimation procedures for the median have been developed. A simple one is the median of three rule, which estimates the median as the median of a three-element subsample; this is commonly used as a subroutine in the quicksort sorting algorithm, which uses an estimate of its input's median. A more robust estimator is Tukey's "ninther", which is the median of three rule applied with limited recursion: if is the sample laid out as an array, and

then

The "remedian" is an estimator for the median that requires linear time but sub-linear memory, operating in a single pass over the sample.

In individual series (if number of observation is very low) first one must arrange all the observations in order. Then count("n") is the total number of observation in given data.

If "n" is odd then Median ("M") = value of (("n" + 1)/2)th item term.

If "n" is even then Median ("M") = value of [("n"/2)th item term + ("n"/2 + 1)th item term]/2


As an example, we will calculate the sample median for the following set of observations: 1, 5, 2, 8, 7.

Start by sorting the values: 1, 2, 5, 7, 8.

In this case, the median is 5 since it is the middle observation in the ordered list.

The median is the (("n" + 1)/2)th item, where "n" is the number of values. For example, for the list {1, 2, 5, 7, 8}, we have "n" = 5, so the median is the ((5 + 1)/2)th item.


As an example, we will calculate the sample median for the following set of observations: 1, 6, 2, 8, 7, 2.

Start by sorting the values: 1, 2, 2, 6, 7, 8.

In this case, the arithmetic mean of the two middlemost terms is (2 + 6)/2 = 4. Therefore, the median is 4 since it is the arithmetic mean of the middle observations in the ordered list.

The distributions of both the sample mean and the sample median were determined by Laplace. The distribution of the sample median from a population with a density function formula_18 is asymptotically normal with mean formula_19 and variance

where formula_19 is the median of formula_18 and formula_23 is the sample size.

These results have also been extended. It is now known for the formula_24-th quantile that the distribution of the sample formula_24-th quantile is asymptotically normal around the formula_24-th quantile with variance equal to
where formula_28 is the value of the distribution density at the formula_24-th quantile.

In the case of a discrete variable, the sampling distribution of the median for small-samples can be investigated as follows. We take the sample size to be an odd number formula_30. If a given value formula_31 is to be the median of the sample then two conditions must be satisfied. The first is that at most formula_32 observations can have a value of formula_33 or less. The second is that at most formula_32 observations can have a value of formula_35 or more. Let formula_36 be the number of observations which have a value of formula_33 or less and let formula_38 be the number of observations which have a value of formula_35 or more. Then formula_36 and formula_38 both have a minimum value of 0 and a maximum of formula_32. If an observation has a value below formula_31, it is not relevant how far below formula_31 it is and conversely, if an observation has a value above formula_31, it is not relevant how far above formula_31 it is. We can therefore represent the observations as following a trinomial distribution with probabilities formula_47, formula_48 and formula_49. The probability that the median formula_50 will have a value formula_31 is then given by

Summing this over all values of formula_31 defines a proper distribution and gives a unit sum. In practice, the function formula_48 will often not be known but it can be estimated from an observed frequency distribution. An example is given in the following table where the actual distribution is not known but a sample of 3,800 observations allows a sufficiently accurate assessment of formula_48.

Using these data it is possible to investigate the effect of sample size on the standard errors of the mean and median. The observed mean is 3.16, the observed raw median is 3 and the observed interpolated median is 3.174. The following table gives some comparison statistics. The standard error of the median is given both from the above expression for formula_56 and from the asymptotic approximation given earlier.
The expected value of the median falls slightly as sample size increases while, as would be expected, the standard errors of both the median and the mean are proportionate to the inverse square root of the sample size. The asymptotic approximation errs on the side of caution by overestimating the standard error.

In the case of a continuous variable, the following argument can be used. If a given value formula_31 is to be the median, then one observation must take the value formula_31. The elemental probability of this is formula_59. Then, of the remaining formula_60 observations, exactly formula_32 of them must be above formula_31 and the remaining formula_32 below. The probability of this is the formula_32th term of a binomial distribution with parameters formula_65 and formula_60. Finally we multiply by formula_67 since any of the observations in the sample can be the median observation. Hence the elemental probability of the median at the point formula_31 is given by

Now we introduce the beta function. For integer arguments formula_70 and formula_71, this can be expressed as formula_72. Also, we note that formula_73. Using these relationships and setting both formula_70 and formula_71 equal to formula_76 allows the last expression to be written as

Hence the density function of the median is a symmetric beta distribution over the unit interval which supports formula_65. Its mean, as we would expect, is 0.5 and its variance is formula_79. The corresponding variance of the sample median is

However this finding can only be used if the density function formula_48 is known or can be assumed. As this will not always be the case, the median variance has to be estimated sometimes from the sample data.


The value of formula_82—the asymptotic value of formula_83 where formula_84 is the population median—has been studied by several authors. The standard "delete one" jackknife method produces inconsistent results. An alternative—the "delete k" method—where formula_85 grows with the sample size has been shown to be asymptotically consistent. This method may be computationally expensive for large data sets. A bootstrap estimate is known to be consistent, but converges very slowly (order of formula_86). Other methods have been proposed but their behavior may differ between large and small samples.


The efficiency of the sample median, measured as the ratio of the variance of the mean to the variance of the median, depends on the sample size and on the underlying population distribution. For a sample of size formula_87 from the normal distribution, the efficiency for large N is

The efficiency tends to formula_89 as formula_90 tends to infinity.

For univariate distributions that are "symmetric" about one median, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population median.

If data are represented by a statistical model specifying a particular family of probability distributions, then estimates of the median can be obtained by fitting that family of probability distributions to the data and calculating the theoretical median of the fitted distribution. Pareto interpolation is an application of this when the population is assumed to have a Pareto distribution.

The coefficient of dispersion (CD) is defined as the ratio of the average absolute deviation from the median to the median of the data. It is a statistical measure used by the states of Iowa, New York and South Dakota in estimating dues taxes. In symbols

where "n" is the sample size, "m" is the sample median and "x" is a variate. The sum is taken over the whole sample.

Confidence intervals for a two-sample test in which the sample sizes are large have been derived by Bonett and Seier This test assumes that both samples have the same median but differ in the dispersion around it. The confidence interval (CI) is bounded inferiorly by

where "t" is the mean absolute deviation of the "j" sample, var() is the variance and "z" is the value from the normal distribution for the chosen value of "α": for "α" = 0.05, "z" = 1.96. The following formulae are used in the derivation of these confidence intervals

where "r" is the Pearson correlation coefficient between the squared deviation scores

"a" and "b" here are constants equal to 1 and 2, "x" is a variate and "s" is the standard deviation of the sample.

Previously, this article discussed the univariate median, when the sample or population had one-dimension. When the dimension is two or higher, there are multiple concepts that extend the definition of the univariate median; each such multivariate median agrees with the univariate median when the dimension is exactly one.

Let be a set of points in a space with a distance function formula_97. Medoid is defined as

formula_98

The medoid is often used in clustering using the k-medoid algorithm.

The marginal median is defined for vectors defined with respect to a fixed set of coordinates. A marginal median is defined to be the vector whose components are univariate medians. The marginal median is easy to compute, and its properties were studied by Puri and Sen.

For "N" vectors in a normed vector space, a spatial median minimizes the average distance

where "x" and "a" are vectors. The spatial median is unique when the data-set's dimension is two or more and the norm is the Euclidean norm (or another strictly convex norm). The spatial median is also called the L1 median, even when the norm is Euclidean. Other names are used especially for finite sets of points: geometric median, Fermat point (in mechanics), or Weber or Fermat-Weber point (in geographical location theory). In the special case where the norm is an L1-norm, then the spatial median and the marginal median are the same.

More generally, a spatial median is defined as a minimizer of

this general definition is convenient for defining a spatial median of a population in a finite-dimensional normed space, for example, for distributions without a finite mean. Spatial medians are defined for random vectors with values in a Banach space.

The spatial median is a robust and highly efficient estimator of a central tendency of a population.

An alternative generalization of the spatial median in higher dimensions that does not relate to a particular metric is the centerpoint.

When dealing with a discrete variable, it is sometimes useful to regard the observed values as being midpoints of underlying continuous intervals. An example of this is a Likert scale, on which opinions or preferences are expressed on a scale with a set number of possible responses. If the scale consists of the positive integers, an observation of 3 might be regarded as representing the interval from 2.50 to 3.50. It is possible to estimate the median of the underlying variable. If, say, 22% of the observations are of value 2 or below and 55.0% are of 3 or below (so 33% have the value 3), then the median formula_50 is 3 since the median is the smallest value of formula_102 for which formula_103 is greater than a half. But the interpolated median is somewhere between 2.50 and 3.50. First we add half of the interval width formula_104 to the median to get the upper bound of the median interval. Then we subtract that proportion of the interval width which equals the proportion of the 33% which lies above the 50% mark. In other words, we split up the interval width pro rata to the numbers of observations. In this case, the 33% is split into 28% below the median and 5% above it so we subtract 5/33 of the interval width from the upper bound of 3.50 to give an interpolated median of 3.35. More formally, if the values formula_105 are known, the interpolated median can be calculated from

Alternatively, if in an observed sample there are formula_38 scores above the median category, formula_108 scores in it and formula_36 scores below it then the interpolated median is given by

For univariate distributions that are "symmetric" about one median, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population median; for non-symmetric distributions, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population "pseudo-median", which is the median of a symmetrized distribution and which is close to the population median. The Hodges–Lehmann estimator has been generalized to multivariate distributions.

The Theil–Sen estimator is a method for robust linear regression based on finding medians of slopes.

In the context of image processing of monochrome raster images there is a type of noise, known as the salt and pepper noise, when each pixel independently becomes black (with some small probability) or white (with some small probability), and is unchanged otherwise (with the probability close to 1). An image constructed of median values of neighborhoods (like 3×3 square) can effectively reduce noise in this case.

In cluster analysis, the k-medians clustering algorithm provides a way of defining clusters, in which the criterion of maximising the distance between cluster-means that is used in k-means clustering, is replaced by maximising the distance between cluster-medians.

This is a method of robust regression. The idea dates back to Wald in 1940 who suggested dividing a set of bivariate data into two halves depending on the value of the independent parameter formula_111: a left half with values less than the median and a right half with values greater than the median. He suggested taking the means of the dependent formula_112 and independent formula_111 variables of the left and the right halves and estimating the slope of the line joining these two points. The line could then be adjusted to fit the majority of the points in the data set.

Nair and Shrivastava in 1942 suggested a similar idea but instead advocated dividing the sample into three equal parts before calculating the means of the subsamples. Brown and Mood in 1951 proposed the idea of using the medians of two subsamples rather the means. Tukey combined these ideas and recommended dividing the sample into three equal size subsamples and estimating the line based on the medians of the subsamples.

Any "mean"-unbiased estimator minimizes the risk (expected loss) with respect to the squared-error loss function, as observed by Gauss. A "median"-unbiased estimator minimizes the risk with respect to the absolute-deviation loss function, as observed by Laplace. Other loss functions are used in statistical theory, particularly in robust statistics.

The theory of median-unbiased estimators was revived by George W. Brown in 1947:

Further properties of median-unbiased estimators have been reported. Median-unbiased estimators are invariant under one-to-one transformations.

There are methods of construction median-unbiased estimators that are optimal (in a sense analogous to minimum-variance property considered for mean-unbiased estimators). Such constructions exist for probability distributions having monotone likelihood-functions. One such procedure is an analogue of the Rao–Blackwell procedure for mean-unbiased estimators: The procedure holds for a smaller class of probability distributions than does the Rao—Blackwell procedure but for a larger class of loss functions.

The idea of the median appeared in the 13th century in the Talmud (further for possible older mentions)

The idea of the median also appeared later in Edward Wright's book on navigation ("Certaine Errors in Navigation") in 1599 in a section concerning the determination of location with a compass. Wright felt that this value was the most likely to be the correct value in a series of observations.

In 1757, Roger Joseph Boscovich developed a regression method based on the L1 norm and therefore implicitly on the median.

In 1774, Laplace suggested the median be used as the standard estimator of the value of a posterior pdf. The specific criterion was to minimize the expected magnitude of the error; formula_114 where formula_115 is the estimate and formula_116 is the true value. Laplaces's criterion was generally rejected for 150 years in favor of the least squares method of Gauss and Legendre which minimizes formula_117 to obtain the mean. The distribution of both the sample mean and the sample median were determined by Laplace in the early 1800s.

Antoine Augustin Cournot in 1843 was the first to use the term "median" ("valeur médiane") for the value that divides a probability distribution into two equal halves. Gustav Theodor Fechner used the median ("Centralwerth") in sociological and psychological phenomena. It had earlier been used only in astronomy and related fields. Gustav Fechner popularized the median into the formal analysis of data, although it had been used previously by Laplace.

Francis Galton used the English term "median" in 1881, having earlier used the terms "middle-most value" in 1869, and the "medium" in 1880.




</doc>
<doc id="18838" url="https://en.wikipedia.org/wiki?curid=18838" title="Mammal">
Mammal

Mammals are the vertebrates within the class Mammalia ( from Latin "mamma" "breast"), a clade of endothermic amniotes distinguished from reptiles (including birds) by the possession of a neocortex (a region of the brain), hair, three middle ear bones, and mammary glands. Females of all mammal species nurse their young with milk, secreted from the mammary glands.

Mammals include the largest animal on the planet, the blue whale. The basic body type is a terrestrial quadruped, but some mammals are adapted for life at sea, in the air, in trees, underground or on two legs. The largest group of mammals, the placentals, have a placenta, which enables the feeding of the fetus during gestation.
Mammals range in size from the bumblebee bat to the blue whale. With the exception of the five species of monotreme (egg-laying mammals), all modern mammals give birth to live young. Most mammals, including the six most species-rich orders, belong to the placental group. The largest orders are the rodents, bats and Soricomorpha (shrews and allies). The next three biggest orders, depending on the biological classification scheme used, are the Primates (apes and monkeys), the Cetartiodactyla (whales and even-toed ungulates), and the Carnivora (cats, dogs, seals, and allies).

Living mammals are divided into the Yinotheria (platypus and echidnas) and Theriiformes (all other mammals). There are around 5450 species of mammal, depending on which authority is cited. In some classifications, extant mammals are divided into two subclasses: the Prototheria, that is, the order Monotremata; and the Theria, or the infraclasses Metatheria and Eutheria. The marsupials constitute the crown group of the Metatheria, and include all living metatherians as well as many extinct ones; the placentals are the crown group of the Eutheria. While mammal classification at the family level has been relatively stable, several contending classifications regarding the higher levels—subclass, infraclass and order, especially of the marsupials—appear in contemporaneous literature. Much of the changes reflect the advances of cladistic analysis and molecular genetics. Findings from molecular genetics, for example, have prompted adopting new groups, such as the Afrotheria, and abandoning traditional groups, such as the Insectivora.

The mammals represent the only living Synapsida, which together with the Sauropsida form the Amniota clade. The early synapsid mammalian ancestors were sphenacodont pelycosaurs, a group that produced the non-mammalian "Dimetrodon". At the end of the Carboniferous period, this group diverged from the sauropsid line that led to today's reptiles and birds. The line following the stem group Sphenacodontia split-off several diverse groups of non-mammalian synapsids—sometimes referred to as mammal-like reptiles—before giving rise to the proto-mammals (Therapsida) in the early Mesozoic era. The modern mammalian orders arose in the Paleogene and Neogene periods of the Cenozoic era, after the extinction of non-avian dinosaurs, and have been among the dominant terrestrial animal groups from 66 million years ago to the present.

Most mammals are intelligent, with some possessing large brains, self-awareness and tool use. Mammals can communicate and vocalize in several different ways, including the production of ultrasound, scent-marking, alarm signals, singing, and echolocation. Mammals can organize themselves into fission-fusion societies, harems, and hierarchies, but can also be solitary and territorial. Most mammals are polygynous, but some can be monogamous or polyandrous.

In human culture, domesticated mammals played a major role in the Neolithic revolution, causing farming to replace hunting and gathering, and leading to a major restructuring of human societies with the first civilizations. They provided, and continue to provide, power for transport and agriculture, as well as various commodities such as meat, dairy products, wool, and leather. Mammals are hunted or raced for sport, and are used as model organisms in science. Mammals have been depicted in art since Palaeolithic times, and appear in literature, film, mythology, and religion. Defaunation of mammals is primarily driven by anthropogenic factors, such as poaching and habitat destruction, though there are efforts to combat this.

Mammal classification has been through several iterations since Carl Linnaeus initially defined the class. No classification system is universally accepted; McKenna & Bell (1997) and Wilson & Reader (2005) provide useful recent compendiums. George Gaylord Simpson's "Principles of Classification and a Classification of Mammals" (AMNH "Bulletin" v. 85, 1945) provides systematics of mammal origins and relationships that were universally taught until the end of the 20th century. Since Simpson's classification, the paleontological record has been recalibrated, and the intervening years have seen much debate and progress concerning the theoretical underpinnings of systematization itself, partly through the new concept of cladistics. Though field work gradually made Simpson's classification outdated, it remains the closest thing to an official classification of mammals.

Most mammals, including the six most species-rich orders, belong to the placental group. The three largest orders in numbers of species are Rodentia: mice, rats, porcupines, beavers, capybaras and other gnawing mammals; Chiroptera: bats; and Soricomorpha: shrews, moles and solenodons. The next three biggest orders, depending on the biological classification scheme used, are the Primates including the apes, monkeys and lemurs; the Cetartiodactyla including whales and even-toed ungulates; and the Carnivora which includes cats, dogs, weasels, bears, seals and allies. According to "Mammal Species of the World", 5,416 species were identified in 2006. These were grouped into 1,229 genera, 153 families and 29 orders. In 2008, the International Union for Conservation of Nature (IUCN) completed a five-year Global Mammal Assessment for its IUCN Red List, which counted 5,488 species. According to a research published in the "Journal of Mammalogy" in 2018, the number of recognized mammal species is 6,495 species included 96 recently extinct.

The word "mammal" is modern, from the scientific name "Mammalia" coined by Carl Linnaeus in 1758, derived from the Latin "mamma" ("teat, pap"). In an influential 1988 paper, Timothy Rowe defined Mammalia phylogenetically as the crown group of mammals, the clade consisting of the most recent common ancestor of living monotremes (echidnas and platypuses) and therian mammals (marsupials and placentals) and all descendants of that ancestor. Since this ancestor lived in the Jurassic period, Rowe's definition excludes all animals from the earlier Triassic, despite the fact that Triassic fossils in the Haramiyida have been referred to the Mammalia since the mid-19th century. If Mammalia is considered as the crown group, its origin can be roughly dated as the first known appearance of animals more closely related to some extant mammals than to others. "Ambondro" is more closely related to monotremes than to therian mammals while "Amphilestes" and "Amphitherium" are more closely related to the therians; as fossils of all three genera are dated about in the Middle Jurassic, this is a reasonable estimate for the appearance of the crown group.

T. S. Kemp has provided a more traditional definition: "synapsids that possess a dentary–squamosal jaw articulation and occlusion between upper and lower molars with a transverse component to the movement" or, equivalently in Kemp's view, the clade originating with the last common ancestor of "Sinoconodon" and living mammals. The earliest known synapsid satisfying Kemp's definitions is "Tikitherium", dated , so the appearance of mammals in this broader sense can be given this Late Triassic date.

In 1997, the mammals were comprehensively revised by Malcolm C. McKenna and Susan K. Bell, which has resulted in the McKenna/Bell classification. Their 1997 book, "Classification of Mammals above the Species Level", is a comprehensive work on the systematics, relationships and occurrences of all mammal taxa, living and extinct, down through the rank of genus, though molecular genetic data challenge several of the higher level groupings. The authors worked together as paleontologists at the American Museum of Natural History, New York. McKenna inherited the project from Simpson and, with Bell, constructed a completely updated hierarchical system, covering living and extinct taxa that reflects the historical genealogy of Mammalia.

Extinct groups are represented by a dagger (†).

Class Mammalia

Molecular studies based on DNA analysis have suggested new relationships among mammal families over the last few years. Most of these findings have been independently validated by retrotransposon presence/absence data. Classification systems based on molecular studies reveal three major groups or lineages of placental mammals—Afrotheria, Xenarthra and Boreoeutheria—which diverged in the Cretaceous. The relationships between these three lineages is contentious, and all three possible different hypotheses have been proposed with respect to which group is basal. These hypotheses are Atlantogenata (basal Boreoeutheria), Epitheria (basal Xenarthra) and Exafroplacentalia (basal Afrotheria). Boreoeutheria in turn contains two major lineages—Euarchontoglires and Laurasiatheria.

Estimates for the divergence times between these three placental groups range from 105 to 120 million years ago, depending on the type of DNA used (such as nuclear or mitochondrial) and varying interpretations of paleogeographic data.

The cladogram above is based on Tarver "et al". (2016)

Group I: Superorder Afrotheria
Group II: Superorder Xenarthra
Group III: Magnaorder Boreoeutheria

Synapsida, a clade that contains mammals and their extinct relatives, originated during the Pennsylvanian subperiod (~323 million to ~300 million years ago), when they split from reptilian and avian lineages. Crown group mammals evolved from earlier mammaliaforms during the Early Jurassic. The cladogram takes Mammalia to be the crown group.

The first fully terrestrial vertebrates were amniotes. Like their amphibious tetrapod predecessors, they had lungs and limbs. Amniotic eggs, however, have internal membranes that allow the developing embryo to breathe but keep water in. Hence, amniotes can lay eggs on dry land, while amphibians generally need to lay their eggs in water.

The first amniotes apparently arose in the Pennsylvanian subperiod of the Carboniferous. They descended from earlier reptiliomorph amphibious tetrapods, which lived on land that was already inhabited by insects and other invertebrates as well as ferns, mosses and other plants. Within a few million years, two important amniote lineages became distinct: the synapsids, which would later include the common ancestor of the mammals; and the sauropsids, which now include turtles, lizards, snakes, crocodilians, dinosaurs and birds. Synapsids have a single hole (temporal fenestra) low on each side of the skull. One synapsid group, the pelycosaurs, included the largest and fiercest animals of the early Permian. Nonmammalian synapsids are sometimes called "mammal-like reptiles".

Therapsids, a group of synapsids, descended from pelycosaurs in the Middle Permian, about 265 million years ago, and became the dominant land vertebrates. They differ from basal eupelycosaurs in several features of the skull and jaws, including: larger skulls and incisors which are equal in size in therapsids, but not for eupelycosaurs. The therapsid lineage leading to mammals went through a series of stages, beginning with animals that were very similar to their pelycosaur ancestors and ending with probainognathian cynodonts, some of which could easily be mistaken for mammals. Those stages were characterized by:

The Permian–Triassic extinction event about 252 million years ago, which was a prolonged event due to the accumulation of several extinction pulses, ended the dominance of carnivorous therapsids. In the early Triassic, most medium to large land carnivore niches were taken over by archosaurs which, over an extended period (35 million years), came to include the crocodylomorphs, the pterosaurs and the dinosaurs; however, large cynodonts like "Trucidocynodon" and traversodontids still occupied large sized carnivorous and herbivorous niches respectively. By the Jurassic, the dinosaurs had come to dominate the large terrestrial herbivore niches as well.

The first mammals (in Kemp's sense) appeared in the Late Triassic epoch (about 225 million years ago), 40 million years after the first therapsids. They expanded out of their nocturnal insectivore niche from the mid-Jurassic onwards; The Jurassic "Castorocauda", for example, was a close relative of true mammals that had adaptations for swimming, digging and catching fish. Most, if not all, are thought to have remained nocturnal (the Nocturnal bottleneck), accounting for much of the typical mammalian traits. The majority of the mammal species that existed in the Mesozoic Era were multituberculates, eutriconodonts and spalacotheriids. The earliest known metatherian is "Sinodelphys", found in 125 million-year-old Early Cretaceous shale in China's northeastern Liaoning Province. The fossil is nearly complete and includes tufts of fur and imprints of soft tissues.

The oldest known fossil among the Eutheria ("true beasts") is the small shrewlike "Juramaia sinensis", or "Jurassic mother from China", dated to 160 million years ago in the late Jurassic. A later eutherian relative, "Eomaia", dated to 125 million years ago in the early Cretaceous, possessed some features in common with the marsupials but not with the placentals, evidence that these features were present in the last common ancestor of the two groups but were later lost in the placental lineage. In particular, the epipubic bones extend forwards from the pelvis. These are not found in any modern placental, but they are found in marsupials, monotremes, other nontherian mammals and "Ukhaatherium", an early Cretaceous animal in the eutherian order Asioryctitheria. This also applies to the multituberculates. They are apparently an ancestral feature, which subsequently disappeared in the placental lineage. These epipubic bones seem to function by stiffening the muscles during locomotion, reducing the amount of space being presented, which placentals require to contain their fetus during gestation periods. A narrow pelvic outlet indicates that the young were very small at birth and therefore pregnancy was short, as in modern marsupials. This suggests that the placenta was a later development.

One of the earliest known monotremes was "Teinolophos", which lived about 120 million years ago in Australia. Monotremes have some features which may be inherited from the original amniotes such as the same orifice to urinate, defecate and reproduce (cloaca) – as lizards and birds also do – and they lay eggs which are leathery and uncalcified.

"Hadrocodium", whose fossils date from approximately 195 million years ago, in the early Jurassic, provides the first clear evidence of a jaw joint formed solely by the squamosal and dentary bones; there is no space in the jaw for the articular, a bone involved in the jaws of all early synapsids.
The earliest clear evidence of hair or fur is in fossils of "Castorocauda" and "Megaconus", from 164 million years ago in the mid-Jurassic. In the 1950s, it was suggested that the foramina (passages) in the maxillae and premaxillae (bones in the front of the upper jaw) of cynodonts were channels which supplied blood vessels and nerves to vibrissae (whiskers) and so were evidence of hair or fur; it was soon pointed out, however, that foramina do not necessarily show that an animal had vibrissae, as the modern lizard "Tupinambis" has foramina that are almost identical to those found in the nonmammalian cynodont "Thrinaxodon". Popular sources, nevertheless, continue to attribute whiskers to "Thrinaxodon". Studies on Permian coprolites suggest that non-mammalian synapsids of the epoch already had fur, setting the evolution of hairs possibly as far back as dicynodonts.

When endothermy first appeared in the evolution of mammals is uncertain, though it is generally agreed to have first evolved in non-mammalian therapsids. Modern monotremes have lower body temperatures and more variable metabolic rates than marsupials and placentals, but there is evidence that some of their ancestors, perhaps including ancestors of the therians, may have had body temperatures like those of modern therians. Likewise, some modern therians like afrotheres and xenarthrans have secondarily developed lower body temperatures.

The evolution of erect limbs in mammals is incomplete — living and fossil monotremes have sprawling limbs. The parasagittal (nonsprawling) limb posture appeared sometime in the late Jurassic or early Cretaceous; it is found in the eutherian "Eomaia" and the metatherian "Sinodelphys", both dated to 125 million years ago. Epipubic bones, a feature that strongly influenced the reproduction of most mammal clades, are first found in Tritylodontidae, suggesting that it is a synapomorphy between them and mammaliformes. They are omnipresent in non-placental mammaliformes, though "Megazostrodon" and "Erythrotherium" appear to have lacked them.

It has been suggested that the original function of lactation (milk production) was to keep eggs moist. Much of the argument is based on monotremes, the egg-laying mammals.

Therian mammals took over the medium- to large-sized ecological niches in the Cenozoic, after the Cretaceous–Paleogene extinction event approximately 66 million years ago emptied ecological space once filled by non-avian dinosaurs and other groups of reptiles, as well as various other mammal groups, and underwent an exponential increase in body size (megafauna). Then mammals diversified very quickly; both birds and mammals show an exponential rise in diversity. For example, the earliest known bat dates from about 50 million years ago, only 16 million years after the extinction of the dinosaurs.

Molecular phylogenetic studies initially suggested that most placental orders diverged about 100 to 85 million years ago and that modern families appeared in the period from the late Eocene through the Miocene. However, no placental fossils have been found from before the end of the Cretaceous. The earliest undisputed fossils of placentals comes from the early Paleocene, after the extinction of the dinosaurs. In particular, scientists have identified an early Paleocene animal named "Protungulatum donnae" as one of the first placental mammals. however it has been reclassified as a non-placental eutherian. Recalibrations of genetic and morphological diversity rates have suggested a Late Cretaceous origin for placentals, and a Paleocene origin for most modern clades.

The earliest known ancestor of primates is "Archicebus achilles" from around 55 million years ago. This tiny primate weighed 20–30 grams (0.7–1.1 ounce) and could fit within a human palm.

Living mammal species can be identified by the presence of sweat glands, including those that are specialized to produce milk to nourish their young. In classifying fossils, however, other features must be used, since soft tissue glands and many other features are not visible in fossils.

Many traits shared by all living mammals appeared among the earliest members of the group:


For the most part, these characteristics were not present in the Triassic ancestors of the mammals. Nearly all mammaliaforms possess an epipubic bone, the exception being modern placentals.

The majority of mammals have seven cervical vertebrae (bones in the neck), including bats, giraffes, whales and humans. The exceptions are the manatee and the two-toed sloth, which have just six, and the three-toed sloth which has nine cervical vertebrae. All mammalian brains possess a neocortex, a brain region unique to mammals. Placental mammals have a corpus callosum, unlike monotremes and marsupials.

The lungs of mammals are spongy and honeycombed. Breathing is mainly achieved with the diaphragm, which divides the thorax from the abdominal cavity, forming a dome convex to the thorax. Contraction of the diaphragm flattens the dome, increasing the volume of the lung cavity. Air enters through the oral and nasal cavities, and travels through the larynx, trachea and bronchi, and expands the alveoli. Relaxing the diaphragm has the opposite effect, decreasing the volume of the lung cavity, causing air to be pushed out of the lungs. During exercise, the abdominal wall contracts, increasing pressure on the diaphragm, which forces air out quicker and more forcefully. The rib cage is able to expand and contract the chest cavity through the action of other respiratory muscles. Consequently, air is sucked into or expelled out of the lungs, always moving down its pressure gradient. This type of lung is known as a bellows lung due to its resemblance to blacksmith bellows.

The mammalian heart has four chambers, two upper atria, the receiving chambers, and two lower ventricles, the discharging chambers. The heart has four valves, which separate its chambers and ensures blood flows in the correct direction through the heart (preventing backflow). After gas exchange in the pulmonary capillaries (blood vessels in the lungs), oxygen-rich blood returns to the left atrium via one of the four pulmonary veins. Blood flows nearly continuously back into the atrium, which acts as the receiving chamber, and from here through an opening into the left ventricle. Most blood flows passively into the heart while both the atria and ventricles are relaxed, but toward the end of the ventricular relaxation period, the left atrium will contract, pumping blood into the ventricle. The heart also requires nutrients and oxygen found in blood like other muscles, and is supplied via coronary arteries.

The integumentary system is made up of three layers: the outermost epidermis, the dermis and the hypodermis. The epidermis is typically 10 to 30 cells thick; its main function is to provide a waterproof layer. Its outermost cells are constantly lost; its bottommost cells are constantly dividing and pushing upward. The middle layer, the dermis, is 15 to 40 times thicker than the epidermis. The dermis is made up of many components, such as bony structures and blood vessels. The hypodermis is made up of adipose tissue, which stores lipids and provides cushioning and insulation. The thickness of this layer varies widely from species to species; marine mammals require a thick hypodermis (blubber) for insulation, and right whales have the thickest blubber at . Although other animals have features such as whiskers, feathers, setae, or cilia that superficially resemble it, no animals other than mammals have hair. It is a definitive characteristic of the class. Though some mammals have very little, careful examination reveals the characteristic, often in obscure parts of their bodies.
Herbivores have developed a diverse range of physical structures to facilitate the consumption of plant material. To break up intact plant tissues, mammals have developed teeth structures that reflect their feeding preferences. For instance, frugivores (animals that feed primarily on fruit) and herbivores that feed on soft foliage have low-crowned teeth specialized for grinding foliage and seeds. Grazing animals that tend to eat hard, silica-rich grasses, have high-crowned teeth, which are capable of grinding tough plant tissues and do not wear down as quickly as low-crowned teeth. Most carnivorous mammals have carnassialiforme teeth (of varying length depending on diet), long canines and similar tooth replacement patterns.

The stomach of Artiodactyls is divided into four sections: the rumen, the reticulum, the omasum and the abomasum (only ruminants have a rumen). After the plant material is consumed, it is mixed with saliva in the rumen and reticulum and separates into solid and liquid material. The solids lump together to form a bolus (or cud), and is regurgitated. When the bolus enters the mouth, the fluid is squeezed out with the tongue and swallowed again. Ingested food passes to the rumen and reticulum where cellulytic microbes (bacteria, protozoa and fungi) produce cellulase, which is needed to break down the cellulose in plants. Perissodactyls, in contrast to the ruminants, store digested food that has left the stomach in an enlarged cecum, where it is fermented by bacteria. Carnivora have a simple stomach adapted to digest primarily meat, as compared to the elaborate digestive systems of herbivorous animals, which are necessary to break down tough, complex plant fibers. The caecum is either absent or short and simple, and the large intestine is not sacculated or much wider than the small intestine.

The mammalian excretory system involves many components. Like most other land animals, mammals are ureotelic, and convert ammonia into urea, which is done by the liver as part of the urea cycle. Bilirubin, a waste product derived from blood cells, is passed through bile and urine with the help of enzymes excreted by the liver. The passing of bilirubin via bile through the intestinal tract gives mammalian feces a distinctive brown coloration. Distinctive features of the mammalian kidney include the presence of the renal pelvis and renal pyramids, and of a clearly distinguishable cortex and medulla, which is due to the presence of elongated loops of Henle. Only the mammalian kidney has a bean shape, although there are some exceptions, such as the multilobed reniculate kidneys of pinnipeds, cetaceans and bears. Most adult placental mammals have no remaining trace of the cloaca. In the embryo, the embryonic cloaca divides into a posterior region that becomes part of the anus, and an anterior region that has different fates depending on the sex of the individual: in females, it develops into the vestibule that receives the urethra and vagina, while in males it forms the entirety of the penile urethra. However, the tenrecs, golden moles, and some shrews retain a cloaca as adults. In marsupials, the genital tract is separate from the anus, but a trace of the original cloaca does remain externally. Monotremes, which translates from Greek into "single hole", have a true cloaca.

As in all other tetrapods, mammals have a larynx that can quickly open and close to produce sounds, and a supralaryngeal vocal tract which filters this sound. The lungs and surrounding musculature provide the air stream and pressure required to phonate. The larynx controls the pitch and volume of sound, but the strength the lungs exert to exhale also contributes to volume. More primitive mammals, such as the echidna, can only hiss, as sound is achieved solely through exhaling through a partially closed larynx. Other mammals phonate using vocal folds, as opposed to the vocal cords seen in birds and reptiles. The movement or tenseness of the vocal folds can result in many sounds such as purring and screaming. Mammals can change the position of the larynx, allowing them to breathe through the nose while swallowing through the mouth, and to form both oral and nasal sounds; nasal sounds, such as a dog whine, are generally soft sounds, and oral sounds, such as a dog bark, are generally loud.

Some mammals have a large larynx and thus a low-pitched voice, namely the hammer-headed bat ("Hypsignathus monstrosus") where the larynx can take up the entirety of the thoracic cavity while pushing the lungs, heart, and trachea into the abdomen. Large vocal pads can also lower the pitch, as in the low-pitched roars of big cats. The production of infrasound is possible in some mammals such as the African elephant ("Loxodonta" spp.) and baleen whales. Small mammals with small larynxes have the ability to produced ultrasound, which can be detected by modifications to the middle ear and cochlea. Ultrasound is inaudible to birds and reptiles, which might have been important during the Mesozoic, when birds and reptiles were the dominant predators. This private channel is used by some rodents in, for example, mother-to-pup communication, and by bats when echolocating. Toothed whales also use echolocation, but, as opposed to the vocal membrane that extends upward from the vocal folds, they have a melon to manipulate sounds. Some mammals, namely the primates, have air sacs attached to the larynx, which may function to lower the resonances or increase the volume of sound.

The vocal production system is controlled by the cranial nerve nuclei in the brain, and supplied by the recurrent laryngeal nerve and the superior laryngeal nerve, branches of the vagus nerve. The vocal tract is supplied by the hypoglossal nerve and facial nerves. Electrical stimulation of the periaqueductal gray (PEG) region of the mammalian midbrain elicit vocalizations. The ability to learn new vocalizations is only exemplified in humans, seals, cetaceans, elephants and possibly bats; in humans, this is the result of a direct connection between the motor cortex, which controls movement, and the motor neurons in the spinal cord.

The fur of mammals has many uses protection, sensory purposes, waterproofing, and camouflage, with the primary usage being thermoregulation. The types of hair include definitive, which may be shed after reaching a certain length; vibrissae, which are sensory hairs and are most commonly whiskers; pelage, which consists of guard hairs, under-fur, and awn hair; spines, which are a type of stiff guard hair used for defense in, for example, porcupines; bristles, which are long hairs usually used in visual signals, such as the mane of a lion; velli, often called "down fur," which insulates newborn mammals; and wool which is long, soft and often curly. Hair length is negligible in thermoregulation, as some tropical mammals, such as sloths, have the same length of fur length as some arctic mammals but with less insulation; and, conversely, other tropical mammals with short hair have the same insulating value as arctic mammals. The denseness of fur can increase an animal's insulation value, and arctic mammals especially have dense fur; for example, the musk ox has guard hairs measuring as well as a dense underfur, which forms an airtight coat, allowing them to survive in temperatures of . Some desert mammals, such as camels, use dense fur to prevent solar heat from reaching their skin, allowing the animal to stay cool; a camel's fur may reach in the summer, but the skin stays at . Aquatic mammals, conversely, trap air in their fur to conserve heat by keeping the skin dry.
Mammalian coats are colored for a variety of reasons, the major selective pressures including camouflage, sexual selection, communication and physiological processes such as temperature regulation. Camouflage is a powerful influence in a large number of mammals, as it helps to conceal individuals from predators or prey. Aposematism, warning off possible predators, is the most likely explanation of the black-and-white pelage of many mammals which are able to defend themselves, such as in the foul-smelling skunk and the powerful and aggressive honey badger. In arctic and subarctic mammals such as the arctic fox ("Alopex lagopus"), collared lemming ("Dicrostonyx groenlandicus"), stoat ("Mustela erminea"), and snowshoe hare ("Lepus americanus"), seasonal color change between brown in summer and white in winter is driven largely by camouflage. Differences in female and male coat color may indicate nutrition and hormone levels, important in mate selection. Some arboreal mammals, notably primates and marsupials, have shades of violet, green, or blue skin on parts of their bodies, indicating some distinct advantage in their largely arboreal habitat due to convergent evolution. The green coloration of sloths, however, is the result of a symbiotic relationship with algae. Coat color is sometimes sexually dimorphic, as in many primate species. Coat color may influence the ability to retain heat, depending on how much light is reflected. Mammals with a darker colored coat can absorb more heat from solar radiation, and stay warmer, and some smaller mammals, such as voles, have darker fur in the winter. The white, pigmentless fur of arctic mammals, such as the polar bear, may reflect more solar radiation directly onto the skin.

In male placentals, the penis is used both for urination and copulation. Depending on the species, an erection may be fueled by blood flow into vascular, spongy tissue or by muscular action. A penis may be contained in a sheath when not erect, and some placentals also have a penis bone (baculum). Marsupials typically have forked penises while the monotreme penis generally has four heads with only two functioning. The testes of most mammals descend into the scrotum which is typically posterior to the penis but is often anterior in marsupials. Female mammals generally have a clitoris, labia majora and labia minora on the outside, while the internal system contains paired oviducts, 1-2 uteri, 1-2 cervices and a vagina. Marsupials have two lateral vaginas and a medial vagina. The "vagina" of monotremes is better understood as a "urogenital sinus". The uterine systems of placental mammals can vary between a duplex, were there are two uteri and cervices which open into the vagina, a bipartite, were two uterine horns have a single cervix that connects to the vagina, a bicornuate, which consists where two uterine horns that are connected distally but separate medially creating a Y-shape, and a simplex, which has a single uterus.

Most mammals are viviparous, giving birth to live young. However, the five species of monotreme, the platypus and the four species of echidna, lay eggs. The monotremes have a sex determination system different from that of most other mammals. In particular, the sex chromosomes of a platypus are more like those of a chicken than those of a therian mammal.

Viviparous mammals are in the subclass Theria; those living today are in the marsupial and placental infraclasses. Marsupials have a short gestation period, typically shorter than its estrous cycle and gives birth to an undeveloped newborn that then undergoes further development; in many species, this takes place within a pouch-like sac, the marsupium, located in the front of the mother's abdomen. This is the plesiomorphic condition among viviparous mammals; the presence of epipubic bones in all non-placental mammals prevents the expansion of the torso needed for full pregnancy. Even non-placental eutherians probably reproduced this way. The placentals give birth to relatively complete and developed young, usually after long gestation periods. They get their name from the placenta, which connects the developing fetus to the uterine wall to allow nutrient uptake.

The mammary glands of mammals are specialized to produce milk, the primary source of nutrition for newborns. The monotremes branched early from other mammals and do not have the nipples seen in most mammals, but they do have mammary glands. The young lick the milk from a mammary patch on the mother's belly.

Nearly all mammals are endothermic ("warm-blooded"). Most mammals also have hair to help keep them warm. Like birds, mammals can forage or hunt in weather and climates too cold for ectothermic ("cold-blooded") reptiles and insects. Endothermy requires plenty of food energy, so mammals eat more food per unit of body weight than most reptiles. Small insectivorous mammals eat prodigious amounts for their size. A rare exception, the naked mole-rat produces little metabolic heat, so it is considered an operational poikilotherm. Birds are also endothermic, so endothermy is not unique to mammals.

Among mammals, species maximum lifespan varies significantly (for example the shrew has a lifespan of two years, whereas the oldest bowhead whale is recorded to be 211 years). Although the underlying basis for these lifespan differences is still uncertain, numerous studies indicate that the ability to repair DNA damages is an important determinant of mammalian lifespan. In a 1974 study by Hart and Setlow, it was found that DNA excision repair capability increased systematically with species lifespan among seven mammalian species. Species lifespan was observed to be robustly correlated with the capacity to recognize DNA double-strand breaks as well as the level of the DNA repair protein Ku80. In a study of the cells from sixteen mammalian species, genes employed in DNA repair were found to be up-regulated in the longer-lived species. The cellular level of the DNA repair enzyme poly ADP ribose polymerase was found to correlate with species lifespan in a study of 13 mammalian species. Three additional studies of a variety of mammalian species also reported a correlation between species lifespan and DNA repair capability.

Most vertebrates—the amphibians, the reptiles and some mammals such as humans and bears—are plantigrade, walking on the whole of the underside of the foot. Many mammals, such as cats and dogs, are digitigrade, walking on their toes, the greater stride length allowing more speed. Digitigrade mammals are also often adept at quiet movement. Some animals such as horses are unguligrade, walking on the tips of their toes. This even further increases their stride length and thus their speed. A few mammals, namely the great apes, are also known to walk on their knuckles, at least for their front legs. Giant anteaters and platypuses are also knuckle-walkers. Some mammals are bipeds, using only two limbs for locomotion, which can be seen in, for example, humans and the great apes. Bipedal species have a larger field of vision than quadrupeds, conserve more energy and have the ability to manipulate objects with their hands, which aids in foraging. Instead of walking, some bipeds hop, such as kangaroos and kangaroo rats.

Animals will use different gaits for different speeds, terrain and situations. For example, horses show four natural gaits, the slowest horse gait is the walk, then there are three faster gaits which, from slowest to fastest, are the trot, the canter and the gallop. Animals may also have unusual gaits that are used occasionally, such as for moving sideways or backwards. For example, the main human gaits are bipedal walking and running, but they employ many other gaits occasionally, including a four-legged crawl in tight spaces. Mammals show a vast range of gaits, the order that they place and lift their appendages in locomotion. Gaits can be grouped into categories according to their patterns of support sequence. For quadrupeds, there are three main categories: walking gaits, running gaits and leaping gaits. Walking is the most common gait, where some feet are on the ground at any given time, and found in almost all legged animals. Running is considered to occur when at some points in the stride all feet are off the ground in a moment of suspension.

Arboreal animals frequently have elongated limbs that help them cross gaps, reach fruit or other resources, test the firmness of support ahead and, in some cases, to brachiate (swing between trees). Many arboreal species, such as tree porcupines, silky anteaters, spider monkeys, and possums, use prehensile tails to grasp branches. In the spider monkey, the tip of the tail has either a bare patch or adhesive pad, which provides increased friction. Claws can be used to interact with rough substrates and reorient the direction of forces the animal applies. This is what allows squirrels to climb tree trunks that are so large to be essentially flat from the perspective of such a small animal. However, claws can interfere with an animal's ability to grasp very small branches, as they may wrap too far around and prick the animal's own paw. Frictional gripping is used by primates, relying upon hairless fingertips. Squeezing the branch between the fingertips generates frictional force that holds the animal's hand to the branch. However, this type of grip depends upon the angle of the frictional force, thus upon the diameter of the branch, with larger branches resulting in reduced gripping ability. To control descent, especially down large diameter branches, some arboreal animals such as squirrels have evolved highly mobile ankle joints that permit rotating the foot into a 'reversed' posture. This allows the claws to hook into the rough surface of the bark, opposing the force of gravity. Small size provides many advantages to arboreal species: such as increasing the relative size of branches to the animal, lower center of mass, increased stability, lower mass (allowing movement on smaller branches) and the ability to move through more cluttered habitat. Size relating to weight affects gliding animals such as the sugar glider. Some species of primate, bat and all species of sloth achieve passive stability by hanging beneath the branch. Both pitching and tipping become irrelevant, as the only method of failure would be losing their grip.

Bats are the only mammals that can truly fly. They fly through the air at a constant speed by moving their wings up and down (usually with some fore-aft movement as well). Because the animal is in motion, there is some airflow relative to its body which, combined with the velocity of the wings, generates a faster airflow moving over the wing. This generates a lift force vector pointing forwards and upwards, and a drag force vector pointing rearwards and upwards. The upwards components of these counteract gravity, keeping the body in the air, while the forward component provides thrust to counteract both the drag from the wing and from the body as a whole.

The wings of bats are much thinner and consist of more bones than that of birds, allowing bats to maneuver more accurately and fly with more lift and less drag. By folding the wings inwards towards their body on the upstroke, they use 35% less energy during flight than birds. The membranes are delicate, ripping easily; however, the tissue of the bat's membrane is able to regrow, such that small tears can heal quickly. The surface of their wings is equipped with touch-sensitive receptors on small bumps called Merkel cells, also found on human fingertips. These sensitive areas are different in bats, as each bump has a tiny hair in the center, making it even more sensitive and allowing the bat to detect and collect information about the air flowing over its wings, and to fly more efficiently by changing the shape of its wings in response.

Fossorial creatures live in subterranean environments. Many fossorial mammals were classified under the, now obsolete, order Insectivora, such as shrews, hedgehogs and moles. Fossorial mammals have a fusiform body, thickest at the shoulders and tapering off at the tail and nose. Unable to see in the dark burrows, most have degenerated eyes, but degeneration varies between species; pocket gophers, for example, are only semi-fossorial and have very small yet functional eyes, in the fully fossorial marsupial mole the eyes are degenerated and useless, talpa moles have vestigial eyes and the cape golden mole has a layer of skin covering the eyes. External ears flaps are also very small or absent. Truly fossorial mammals have short, stout legs as strength is more important than speed to a burrowing mammal, but semi-fossorial mammals have cursorial legs. The front paws are broad and have strong claws to help in loosening dirt while excavating burrows, and the back paws have webbing, as well as claws, which aids in throwing loosened dirt backwards. Most have large incisors to prevent dirt from flying into their mouth.

Fully aquatic mammals, the cetaceans and sirenians, have lost their legs and have a tail fin to propel themselves through the water. Flipper movement is continuous. Whales swim by moving their tail fin and lower body up and down, propelling themselves through vertical movement, while their flippers are mainly used for steering. Their skeletal anatomy allows them to be fast swimmers. Most species have a dorsal fin to prevent themselves from turning upside-down in the water. The flukes of sirenians are raised up and down in long strokes to move the animal forward, and can be twisted to turn. The forelimbs are paddle-like flippers which aid in turning and slowing.

Semi-aquatic mammals, like pinnipeds, have two pairs of flippers on the front and back, the fore-flippers and hind-flippers. The elbows and ankles are enclosed within the body. Pinnipeds have several adaptions for reducing drag. In addition to their streamlined bodies, they have smooth networks of muscle bundles in their skin that may increase laminar flow and make it easier for them to slip through water. They also lack arrector pili, so their fur can be streamlined as they swim. They rely on their fore-flippers for locomotion in a wing-like manner similar to penguins and sea turtles. Fore-flipper movement is not continuous, and the animal glides between each stroke. Compared to terrestrial carnivorans, the fore-limbs are reduced in length, which gives the locomotor muscles at the shoulder and elbow joints greater mechanical advantage; the hind-flippers serve as stabilizers. Other semi-aquatic mammals include beavers, hippopotamuses, otters and platypuses. Hippos are very large semi-aquatic mammals, and their barrel-shaped bodies have graviportal skeletal structures, adapted to carrying their enormous weight, and their specific gravity allows them to sink and move along the bottom of a river.

Many mammals communicate by vocalizing. Vocal communication serves many purposes, including in mating rituals, as warning calls, to indicate food sources, and for social purposes. Males often call during mating rituals to ward off other males and to attract females, as in the roaring of lions and red deer. The songs of the humpback whale may be signals to females; they have different dialects in different regions of the ocean. Social vocalizations include the territorial calls of gibbons, and the use of frequency in greater spear-nosed bats to distinguish between groups. The vervet monkey gives a distinct alarm call for each of at least four different predators, and the reactions of other monkeys vary according to the call. For example, if an alarm call signals a python, the monkeys climb into the trees, whereas the eagle alarm causes monkeys to seek a hiding place on the ground. Prairie dogs similarly have complex calls that signal the type, size, and speed of an approaching predator. Elephants communicate socially with a variety of sounds including snorting, screaming, trumpeting, roaring and rumbling. Some of the rumbling calls are infrasonic, below the hearing range of humans, and can be heard by other elephants up to away at still times near sunrise and sunset.

Mammals signal by a variety of means. Many give visual anti-predator signals, as when deer and gazelle stot, honestly indicating their fit condition and their ability to escape, or when white-tailed deer and other prey mammals flag with conspicuous tail markings when alarmed, informing the predator that it has been detected. Many mammals make use of scent-marking, sometimes possibly to help defend territory, but probably with a range of functions both within and between species. Microbats and toothed whales including oceanic dolphins vocalize both socially and in echolocation.

To maintain a high constant body temperature is energy expensive – mammals therefore need a nutritious and plentiful diet. While the earliest mammals were probably predators, different species have since adapted to meet their dietary requirements in a variety of ways. Some eat other animals – this is a carnivorous diet (and includes insectivorous diets). Other mammals, called herbivores, eat plants, which contain complex carbohydrates such as cellulose. An herbivorous diet includes subtypes such as granivory (seed eating), folivory (leaf eating), frugivory (fruit eating), nectarivory (nectar eating), gummivory (gum eating) and mycophagy (fungus eating). The digestive tract of an herbivore is host to bacteria that ferment these complex substances, and make them available for digestion, which are either housed in the multichambered stomach or in a large cecum. Some mammals are coprophagous, consuming feces to absorb the nutrients not digested when the food was first ingested. An omnivore eats both prey and plants. Carnivorous mammals have a simple digestive tract because the proteins, lipids and minerals found in meat require little in the way of specialized digestion. Exceptions to this include baleen whales who also house gut flora in a multi-chambered stomach, like terrestrial herbivores.

The size of an animal is also a factor in determining diet type (Allen's rule). Since small mammals have a high ratio of heat-losing surface area to heat-generating volume, they tend to have high energy requirements and a high metabolic rate. Mammals that weigh less than about are mostly insectivorous because they cannot tolerate the slow, complex digestive process of an herbivore. Larger animals, on the other hand, generate more heat and less of this heat is lost. They can therefore tolerate either a slower collection process (those that prey on larger vertebrates) or a slower digestive process (herbivores). Furthermore, mammals that weigh more than usually cannot collect enough insects during their waking hours to sustain themselves. The only large insectivorous mammals are those that feed on huge colonies of insects (ants or termites).
Some mammals are omnivores and display varying degrees of carnivory and herbivory, generally leaning in favor of one more than the other. Since plants and meat are digested differently, there is a preference for one over the other, as in bears where some species may be mostly carnivorous and others mostly herbivorous. They are grouped into three categories: mesocarnivory (50-70% meat), hypercarnivory (70% and greater of meat), and hypocarnivory (50% or less of meat). The dentition of hypocarnivores consists of dull, triangular carnassial teeth meant for grinding food. Hypercarnivores, however, have conical teeth and sharp carnassials meant for slashing, and in some cases strong jaws for bone-crushing, as in the case of hyenas, allowing them to consume bones; some extinct groups, notably the Machairodontinae, had saber-shaped canines.

Some physiological carnivores consume plant matter and some physiological herbivores consume meat. From a behavioral aspect, this would make them omnivores, but from the physiological standpoint, this may be due to zoopharmacognosy. Physiologically, animals must be able to obtain both energy and nutrients from plant and animal materials to be considered omnivorous. Thus, such animals are still able to be classified as carnivores and herbivores when they are just obtaining nutrients from materials originating from sources that do not seemingly complement their classification. For example, it is well documented that some ungulates. such as giraffes, camels, and cattle, will gnaw on bones to consume particular minerals and nutrients. Also, cats, which are generally regarded as obligate carnivores, occasionally eat grass to regurgitate indigestible material (such as hairballs), aid with hemoglobin production, and as a laxative.

Many mammals, in the absence of sufficient food requirements in an environment, suppress their metabolism and conserve energy in a process known as hibernation. In the period preceding hibernation, larger mammals, such as bears, become polyphagic to increase fat stores, whereas smaller mammals prefer to collect and stash food. The slowing of the metabolism is accompanied by a decreased heart and respiratory rate, as well as a drop in internal temperatures, which can be around ambient temperature in some cases. For example, the internal temperatures of hibernating arctic ground squirrels can drop to , however the head and neck always stay above . A few mammals in hot environments aestivate in times of drought or extreme heat, namely the fat-tailed dwarf lemur ("Cheirogaleus medius").

In intelligent mammals, such as primates, the cerebrum is larger relative to the rest of the brain. Intelligence itself is not easy to define, but indications of intelligence include the ability to learn, matched with behavioral flexibility. Rats, for example, are considered to be highly intelligent, as they can learn and perform new tasks, an ability that may be important when they first colonize a fresh habitat. In some mammals, food gathering appears to be related to intelligence: a deer feeding on plants has a brain smaller than a cat, which must think to outwit its prey.

Tool use by animals may indicate different levels of learning and cognition. The sea otter uses rocks as essential and regular parts of its foraging behaviour (smashing abalone from rocks or breaking open shells), with some populations spending 21% of their time making tools. Other tool use, such as chimpanzees using twigs to "fish" for termites, may be developed by watching others use tools and may even be a true example of animal teaching. Tools may even be used in solving puzzles in which the animal appears to experience a "Eureka moment". Other mammals that do not use tools, such as dogs, can also experience a Eureka moment.

Brain size was previously considered a major indicator of the intelligence of an animal. Since most of the brain is used for maintaining bodily functions, greater ratios of brain to body mass may increase the amount of brain mass available for more complex cognitive tasks. Allometric analysis indicates that mammalian brain size scales at approximately the ⅔ or ¾ exponent of the body mass. Comparison of a particular animal's brain size with the expected brain size based on such allometric analysis provides an encephalisation quotient that can be used as another indication of animal intelligence. Sperm whales have the largest brain mass of any animal on earth, averaging and in mature males.

Self-awareness appears to be a sign of abstract thinking. Self-awareness, although not well-defined, is believed to be a precursor to more advanced processes such as metacognitive reasoning. The traditional method for measuring this is the mirror test, which determines if an animal possesses the ability of self-recognition. Mammals that have 'passed' the mirror test include Asian elephants (some pass, some do not); chimpanzees; bonobos; orangutans; humans, from 18 months (mirror stage); bottlenose dolphins killer whales; and false killer whales.

Eusociality is the highest level of social organization. These societies have an overlap of adult generations, the division of reproductive labor and cooperative caring of young. Usually insects, such as bees, ants and termites, have eusocial behavior, but it is demonstrated in two rodent species: the naked mole-rat and the Damaraland mole-rat.

Presociality is when animals exhibit more than just sexual interactions with members of the same species, but fall short of qualifying as eusocial. That is, presocial animals can display communal living, cooperative care of young, or primitive division of reproductive labor, but they do not display all of the three essential traits of eusocial animals. Humans and some species of Callitrichidae (marmosets and tamarins) are unique among primates in their degree of cooperative care of young. Harry Harlow set up an experiment with rhesus monkeys, presocial primates, in 1958; the results from this study showed that social encounters are necessary in order for the young monkeys to develop both mentally and sexually.

A fission-fusion society is a society that changes frequently in its size and composition, making up a permanent social group called the "parent group". Permanent social networks consist of all individual members of a community and often varies to track changes in their environment. In a fission–fusion society, the main parent group can fracture (fission) into smaller stable subgroups or individuals to adapt to environmental or social circumstances. For example, a number of males may break off from the main group in order to hunt or forage for food during the day, but at night they may return to join (fusion) the primary group to share food and partake in other activities. Many mammals exhibit this, such as primates (for example orangutans and spider monkeys), elephants, spotted hyenas, lions, and dolphins.

Solitary animals defend a territory and avoid social interactions with the members of its species, except during breeding season. This is to avoid resource competition, as two individuals of the same species would occupy the same niche, and to prevent depletion of food. A solitary animal, while foraging, can also be less conspicuous to predators or prey.

In a hierarchy, individuals are either dominant or submissive. A despotic hierarchy is where one individual is dominant while the others are submissive, as in wolves and lemurs, and a pecking order is a linear ranking of individuals where there is a top individual and a bottom individual. Pecking orders may also be ranked by sex, where the lowest individual of a sex has a higher ranking than the top individual of the other sex, as in hyenas. Dominant individuals, or alphas, have a high chance of reproductive success, especially in harems where one or a few males (resident males) have exclusive breeding rights to females in a group. Non-resident males can also be accepted in harems, but some species, such as the common vampire bat ("Desmodus rotundus"), may be more strict.

Some mammals are perfectly monogamous, meaning that they mate for life and take no other partners (even after the original mate’s death), as with wolves, Eurasian beavers, and otters. There are three types of polygamy: either one or multiple dominant males have breeding rights (polygyny), multiple males that females mate with (polyandry), or multiple males have exclusive relations with multiple females (polygynandry). It is much more common for polygynous mating to happen, which, excluding leks, are estimated to occur in up to 90% of mammals. Lek mating occurs in harems, wherein one or a few males protect their harem of females from other males who would otherwise mate with the females, as in elephant seals; or males congregate around females and try to attract them with various courtship displays and vocalizations, as in harbor seals.

All higher mammals (excluding monotremes) share two major adaptations for care of the young: live birth and lactation. These imply a group-wide choice of a degree of parental care. They may build nests and dig burrows to raise their young in, or feed and guard them often for a prolonged period of time. Many mammals are K-selected, and invest more time and energy into their young than do r-selected animals. When two animals mate, they both share an interest in the success of the offspring, though often to different extremes. Mammalian females exhibit some degree of maternal aggression, another example of parental care, which may be targeted against other females of the species or the young of other females; however, some mammals may "aunt" the infants of other females, and care for them. Mammalian males may play a role in child rearing, as with tenrecs, however this varies species to species, even within the same genus. For example, the males of the southern pig-tailed macaque ("Macaca nemestrina") do not participate in child care, whereas the males of the Japanese macaque ("M. fuscata") do.

Non-human mammals play a wide variety of roles in human culture. They are the most popular of pets, with tens of millions of dogs, cats and other animals including rabbits and mice kept by families around the world. Mammals such as mammoths, horses and deer are among the earliest subjects of art, being found in Upper Paleolithic cave paintings such as at Lascaux. Major artists such as Albrecht Dürer, George Stubbs and Edwin Landseer are known for their portraits of mammals. Many species of mammals have been hunted for sport and for food; deer and wild boar are especially popular as game animals. Mammals such as horses and dogs are widely raced for sport, often combined with betting on the outcome. There is a tension between the role of animals as companions to humans, and their existence as individuals with rights of their own. Mammals further play a wide variety of roles in literature, film, mythology, and religion.

Domestic mammals form a large part of the livestock raised for meat across the world. They include (2009) around 1.4 billion cattle, 1 billion sheep, 1 billion domestic pigs, and (1985) over 700 million rabbits. Working domestic animals including cattle and horses have been used for work and transport from the origins of agriculture, their numbers declining with the arrival of mechanised transport and agricultural machinery. In 2004 they still provided some 80% of the power for the mainly small farms in the third world, and some 20% of the world's transport, again mainly in rural areas. In mountainous regions unsuitable for wheeled vehicles, pack animals continue to transport goods.
Mammal skins provide leather for shoes, clothing and upholstery. Wool from mammals including sheep, goats and alpacas has been used for centuries for clothing. Mammals serve a major role in science as experimental animals, both in fundamental biological research, such as in genetics, and in the development of new medicines, which must be tested exhaustively to demonstrate their safety. Millions of mammals, especially mice and rats, are used in experiments each year. A knockout mouse is a genetically modified mouse with an inactivated gene, replaced or disrupted with an artificial piece of DNA. They enable the study of sequenced genes whose functions are unknown. A small percentage of the mammals are non-human primates, used in research for their similarity to humans.

Charles Darwin, Jared Diamond and others have noted the importance of domesticated mammals in the Neolithic development of agriculture and of civilization, causing farmers to replace hunter-gatherers around the world. This transition from hunting and gathering to herding flocks and growing crops was a major step in human history. The new agricultural economies, based on domesticated mammals, caused "radical restructuring of human societies, worldwide alterations in biodiversity, and significant changes in the Earth's landforms and its atmosphere... momentous outcomes".

Hybrids are offspring resulting from the breeding of two genetically distinct individuals, which usually will result in a high degree of heterozygosity, though hybrid and heterozygous are not synonymous. The deliberate or accidental hybridizing of two or more species of closely related animals through captive breeding is a human activity which has been in existence for millennia and has grown for economic purposes. Hybrids between different subspecies within a species (such as between the Bengal tiger and Siberian tiger) are known as intra-specific hybrids. Hybrids between different species within the same genus (such as between lions and tigers) are known as interspecific hybrids or crosses. Hybrids between different genera (such as between sheep and goats) are known as intergeneric hybrids. Natural hybrids will occur in hybrid zones, where two populations of species within the same genera or species living in the same or adjacent areas will interbreed with each other. Some hybrids have been recognized as species, such as the red wolf (though this is controversial).

Artificial selection, the deliberate selective breeding of domestic animals, is being used to breed back recently extinct animals in an attempt to achieve an animal breed with a phenotype that resembles that extinct wildtype ancestor. A breeding-back (intraspecific) hybrid may be very similar to the extinct wildtype in appearance, ecological niche and to some extent genetics, but the initial gene pool of that wild type is lost forever with its extinction. As a result, bred-back breeds are at best vague look-alikes of extinct wildtypes, as Heck cattle are of the aurochs.

Purebred wild species evolved to a specific ecology can be threatened with extinction through the process of genetic pollution, the uncontrolled hybridization, introgression genetic swamping which leads to homogenization or out-competition from the heterosic hybrid species. When new populations are imported or selectively bred by people, or when habitat modification brings previously isolated species into contact, extinction in some species, especially rare varieties, is possible. Interbreeding can swamp the rarer gene pool and create hybrids, depleting the purebred gene pool. For example, the endangered wild water buffalo is most threatened with extinction by genetic pollution from the domestic water buffalo. Such extinctions are not always apparent from a morphological standpoint. Some degree of gene flow is a normal evolutionary process, nevertheless, hybridization threatens the existence of rare species.

The loss of species from ecological communities, defaunation, is primarily driven by human activity. This has resulted in empty forests, ecological communities depleted of large vertebrates. In the Quaternary extinction event, the mass die-off of megafaunal variety coincided with the appearance of humans, suggesting a human influence. One hypothesis is that humans hunted large mammals, such as the woolly mammoth, into extinction.
Various species are predicted to become extinct in the near future, among them the rhinoceros, primates, pangolins, and giraffes. Hunting alone threatens hundreds of mammalian species around the world. Scientists claim that the growing demand for meat is contributing to biodiversity loss as this is a significant driver of deforestation and habitat destruction; species-rich habitats, such as significant portions of the Amazon rainforest, are being converted to agricultural land for meat production. According to the World Wildlife Fund's 2016 Living Planet Index, global wildlife populations have declined 58% since 1970, primarily due to habitat destruction, over-hunting and pollution. They project that if current trends continue, 67% of wildlife could disappear by 2020. Another influence is over-hunting and poaching, which can reduce the overall population of game animals, especially those located near villages, as in the case of peccaries. The effects of poaching can especially be seen in the ivory trade with African elephants. Marine mammals are at risk from entanglement from fishing gear, notably cetaceans, with discard mortalities ranging from 65,000 to 86,000 individuals annually.

Several courses of actions are being taken globally, notably the Convention on Biological Diversity, otherwise known as the Rio Accord, which includes 189 signatory countries that are focused on identifying endangered species and habitats. Another notable conservation organization is the IUCN, which has a membership of over 1,200 governmental and non-governmental organizations.

Recent extinctions can be directly attributable to human influences. The IUCN characterizes 'recent' extinction as those that have occurred past the cut-off point of 1500, and around 80 mammal species have gone extinct since that time and 2015. Some species, such as the Père David's deer are extinct in the wild, and survive solely in captive populations. Other species, such as the Florida panther, are ecologically extinct, surviving in such low numbers that they essentially have no impact on the ecosystem. Other populations are only locally extinct (extirpated), still existing elsewhere, but reduced in distribution, as with the extinction of gray whales in the Atlantic.


<br>


</doc>
<doc id="18839" url="https://en.wikipedia.org/wiki?curid=18839" title="Music">
Music

Music is an art form and cultural activity whose medium is sound organized in time. The common elements of music are pitch (which governs melody and harmony), rhythm (and its associated concepts tempo, meter, and articulation), dynamics (loudness and softness), and the sonic qualities of timbre and texture (which are sometimes termed the "color" of a musical sound). Different styles or types of music may emphasize, de-emphasize or omit some of these elements. Music is performed with a vast range of instruments and vocal techniques ranging from singing to rapping; there are solely instrumental pieces, solely vocal pieces (such as songs without instrumental accompaniment) and pieces that combine singing and instruments. The word derives from Greek μουσική ("mousike"; "art of the Muses").
See glossary of musical terminology.

In its most general form, the activities describing music as an art form or cultural activity include the creation of works of music (songs, tunes, symphonies, and so on), the criticism of music, the study of the history of music, and the aesthetic examination of music. Ancient Greek and Indian philosophers defined music as tones ordered horizontally as melodies and vertically as harmonies. Common sayings such as "the harmony of the spheres" and "it is music to my ears" point to the notion that music is often ordered and pleasant to listen to. However, 20th-century composer John Cage thought that any sound can be music, saying, for example, "There is no noise, only sound."

The creation, performance, significance, and even the definition of music vary according to culture and social context. Indeed, throughout history, some new forms or styles of music have been criticized as "not being music", including Beethoven's "Grosse Fuge" string quartet in 1825, early jazz in the beginning of the 1900s and hardcore punk in the 1980s. There are many types of music, including popular music, traditional music, art music, music written for religious ceremonies and work songs such as chanteys. Music ranges from strictly organized compositions–such as Classical music symphonies from the 1700s and 1800s, through to spontaneously played improvisational music such as jazz, and avant-garde styles of chance-based contemporary music from the 20th and 21st centuries.

Music can be divided into genres (e.g., country music) and genres can be further divided into subgenres (e.g., country blues and pop country are two of the many country subgenres), although the dividing lines and relationships between music genres are often subtle, sometimes open to personal interpretation, and occasionally controversial. For example, it can be hard to draw the line between some early 1980s hard rock and heavy metal. Within the arts, music may be classified as a performing art, a fine art or as an auditory art. Music may be played or sung and heard live at a rock concert or orchestra performance, heard live as part of a dramatic work (a music theater show or opera), or it may be recorded and listened to on a radio, MP3 player, CD player, smartphone or as film score or TV show.

In many cultures, music is an important part of people's way of life, as it plays a key role in religious rituals, rite of passage ceremonies (e.g., graduation and marriage), social activities (e.g., dancing) and cultural activities ranging from amateur karaoke singing to playing in an amateur funk band or singing in a community choir. People may make music as a hobby, like a teen playing cello in a youth orchestra, or work as a professional musician or singer. The music industry includes the individuals who create new songs and musical pieces (such as songwriters and composers), individuals who perform music (which include orchestra, jazz band and rock band musicians, singers and conductors), individuals who record music (music producers and sound engineers), individuals who organize concert tours, and individuals who sell recordings and sheet music and scores to customers.

The word derives from Greek μουσική ("mousike"; "art of the Muses"). In Greek mythology, the nine Muses were the goddesses who inspired literature, science, and the arts and who were the source of the knowledge embodied in the poetry, song-lyrics, and myths in the Greek culture. According to the "Online Etymological Dictionary", the term "music" is derived from "mid-13c., musike, from Old French "musique" (12c.) and directly from Latin musica "the art of music," also including poetry (also [the] source of Spanish "musica", Italian "musica", Old High German "mosica", German "Musik", Dutch "muziek", Danish "musik")." This is derived from the "...Greek "mousike (techne)" "(art) of the Muses," from fem. of mousikos "pertaining to the Muses," from Mousa "Muse" (see muse (n.)). Modern spelling [dates] from [the] 1630s. In classical Greece, [the term "music" refers to] any art in which the Muses presided, but especially music and lyric poetry."

Music is composed and performed for many purposes, ranging from aesthetic pleasure, religious or ceremonial purposes, or as an entertainment product for the marketplace. When music was only available through sheet music scores, such as during the Classical and Romantic eras, music lovers would buy the sheet music of their favourite pieces and songs so that they could perform them at home on the piano. With the advent of sound recording, records of popular songs, rather than sheet music became the dominant way that music lovers would enjoy their favourite songs. With the advent of home tape recorders in the 1980s and digital music in the 1990s, music lovers could make tapes or playlists of their favourite songs and take them with them on a portable cassette player or MP3 player. Some music lovers create mix tapes of their favorite songs, which serve as a "self-portrait, a gesture of friendship, prescription for an ideal party... [and] an environment consisting solely of what is most ardently loved."

Amateur musicians can compose or perform music for their own pleasure, and derive their income elsewhere. Professional musicians are employed by a range of institutions and organisations, including armed forces (in marching bands, concert bands and popular music groups), churches and synagogues, symphony orchestras, broadcasting or film production companies, and music schools. Professional musicians sometimes work as freelancers or session musicians, seeking contracts and engagements in a variety of settings. There are often many links between amateur and professional musicians. Beginning amateur musicians take lessons with professional musicians. In community settings, advanced amateur musicians perform with professional musicians in a variety of ensembles such as community concert bands and community orchestras.

A distinction is often made between music performed for a live audience and music that is performed in a studio so that it can be recorded and distributed through the music retail system or the broadcasting system. However, there are also many cases where a live performance in front of an audience is also recorded and distributed. Live concert recordings are popular in both classical music and in popular music forms such as rock, where illegally taped live concerts are prized by music lovers. In the jam band scene, live, improvised jam sessions are preferred to studio recordings.

"Composition" is the act or practice of creating a song, an instrumental music piece, a work with both singing and instruments, or another type of music. In many cultures, including Western classical music, the act of composing also includes the creation of music notation, such as a sheet music "score", which is then performed by the composer or by other singers or musicians. In popular music and traditional music, the act of composing, which is typically called songwriting, may involve the creation of a basic outline of the song, called the lead sheet, which sets out the melody, lyrics and chord progression. In classical music, the composer typically orchestrates his or her own compositions, but in musical theatre and in pop music, songwriters may hire an arranger to do the orchestration. In some cases, a songwriter may not use notation at all, and instead compose the song in her mind and then play or record it from memory. In jazz and popular music, notable recordings by influential performers are given the weight that written scores play in classical music.

Even when music is notated relatively precisely, as in classical music, there are many decisions that a performer has to make, because notation does not specify all of the elements of music precisely. The process of deciding how to perform music that has been previously composed and notated is termed "interpretation". Different performers' interpretations of the same work of music can vary widely, in terms of the tempos that are chosen and the playing or singing style or phrasing of the melodies. Composers and songwriters who present their own music are interpreting their songs, just as much as those who perform the music of others. The standard body of choices and techniques present at a given time and a given place is referred to as performance practice, whereas interpretation is generally used to mean the individual choices of a performer.

Although a musical composition often uses musical notation and has a single author, this is not always the case. A work of music can have multiple composers, which often occurs in popular music when a band collaborates to write a song, or in musical theatre, when one person writes the melodies, a second person writes the lyrics, and a third person orchestrates the songs. In some styles of music, such as the blues, a composer/songwriter may create, perform and record new songs or pieces without ever writing them down in music notation. A piece of music can also be composed with words, images, or computer programs that explain or notate how the singer or musician should create musical sounds. Examples range from avant-garde music that uses graphic notation, to text compositions such as "Aus den sieben Tagen", to computer programs that select sounds for musical pieces. Music that makes heavy use of randomness and chance is called aleatoric music, and is associated with contemporary composers active in the 20th century, such as John Cage, Morton Feldman, and Witold Lutosławski. A more commonly known example of chance-based music is the sound of wind chimes jingling in a breeze.

The study of composition has traditionally been dominated by examination of methods and practice of Western classical music, but the definition of composition is broad enough to include the creation of popular music and traditional music songs and instrumental pieces as well as spontaneously improvised works like those of free jazz performers and African percussionists such as Ewe drummers.

In the 2000s, music notation typically means the written expression of music notes and rhythms on paper using symbols. When music is written down, the pitches and rhythm of the music, such as the notes of a melody, are notated. Music notation also often provides instructions on how to perform the music. For example, the sheet music for a song may state that the song is a "slow blues" or a "fast swing", which indicates the tempo and the genre. To read music notation, a person must have an understanding of music theory, harmony and the performance practice associated with a particular song or piece's genre.

Written notation varies with style and period of music. In the 2000s, notated music is produced as sheet music or, for individuals with computer scorewriter programs, as an image on a computer screen. In ancient times, music notation was put onto stone or clay tablets. To perform music from notation, a singer or instrumentalist requires an understanding of the rhythmic and pitch elements embodied in the symbols and the performance practice that is associated with a piece of music or a genre. In genres requiring musical improvisation, the performer often plays from music where only the chord changes and form of the song are written, requiring the performer to have a great understanding of the music's structure, harmony and the styles of a particular genre (e.g., jazz or country music).

In Western art music, the most common types of written notation are scores, which include all the music parts of an ensemble piece, and parts, which are the music notation for the individual performers or singers. In popular music, jazz, and blues, the standard musical notation is the lead sheet, which notates the melody, chords, lyrics (if it is a vocal piece), and structure of the music. Fake books are also used in jazz; they may consist of lead sheets or simply chord charts, which permit rhythm section members to improvise an accompaniment part to jazz songs. Scores and parts are also used in popular music and jazz, particularly in large ensembles such as jazz "big bands." In popular music, guitarists and electric bass players often read music notated in tablature (often abbreviated as "tab"), which indicates the location of the notes to be played on the instrument using a diagram of the guitar or bass fingerboard. Tabulature was also used in the Baroque era to notate music for the lute, a stringed, fretted instrument.

Musical improvisation is the creation of spontaneous music, often within (or based on) a pre-existing harmonic framework or chord progression. Improvisation is the act of instantaneous composition by performers, where compositional techniques are employed with or without preparation. Improvisation is a major part of some types of music, such as blues, jazz, and jazz fusion, in which instrumental performers improvise solos, melody lines and accompaniment parts. In the Western art music tradition, improvisation was an important skill during the Baroque era and during the Classical era. In the Baroque era, performers improvised ornaments and basso continuo keyboard players improvised chord voicings based on figured bass notation. In the Classical era, solo performers and singers improvised virtuoso cadenzas during concerts. However, in the 20th and early 21st century, as "common practice" Western art music performance became institutionalized in symphony orchestras, opera houses and ballets, improvisation has played a smaller role. At the same time, some modern composers have increasingly included improvisation in their creative work. In Indian classical music, improvisation is a core component and an essential criterion of performances.

Music theory encompasses the nature and mechanics of music. It often involves identifying patterns that govern composers' techniques and examining the language and notation of music. In a grand sense, music theory distills and analyzes the parameters or elements of music – rhythm, harmony (harmonic function), melody, structure, form, and texture. Broadly, music theory may include any statement, belief, or conception of or about music. People who study these properties are known as music theorists. Some have applied acoustics, human physiology, and psychology to the explanation of how and why music is perceived.

Music has many different fundamentals or elements. Depending on the definition of "element" being used, these can include: pitch, beat or pulse, tempo, rhythm, melody, harmony, texture, style, allocation of voices, timbre or color, dynamics, expression, articulation, form and structure. The elements of music feature prominently in the music curriculums of Australia, UK and US. All three curriculums identify pitch, dynamics, timbre and texture as elements, but the other identified elements of music are far from universally agreed. Below is a list of the three official versions of the "elements of music":

In relation to the UK curriculum, in 2013 the term: "appropriate musical notations" was added to their list of elements and the title of the list was changed from the "elements of music" to the "inter-related dimensions of music". The inter-related dimensions of music are listed as: pitch, duration, dynamics, tempo, timbre, texture, structure and appropriate musical notations.

The phrase "the elements of music" is used in a number of different contexts. The two most common contexts can be differentiated by describing them as the "rudimentary elements of music" and the "perceptual elements of music".

In the 1800s, the phrases "the elements of music" and "the rudiments of music" were used interchangeably. The elements described in these documents refer to aspects of music that are needed in order to become a musician, Recent writers such as Estrella seem to be using the phrase "elements of music" in a similar manner. A definition which most accurately reflects this usage is: "the rudimentary principles of an art, science, etc.: the elements of grammar." The UK's curriculum switch to the "inter-related dimensions of music" seems to be a move back to using the rudimentary elements of music.

Since the emergence of the study of psychoacoustics in the 1930s, most lists of elements of music have related more to how we "hear" music than how we learn to play it or study it. C.E. Seashore, in his book "Psychology of Music", identified four "psychological attributes of sound". These were: "pitch, loudness, time, and timbre" (p. 3). He did not call them the "elements of music" but referred to them as "elemental components" (p. 2). Nonetheless these elemental components link precisely with four of the most common musical elements: "Pitch" and "timbre" match exactly, "loudness" links with dynamics and "time" links with the time-based elements of rhythm, duration and tempo. This usage of the phrase "the elements of music" links more closely with "Webster's New 20th Century Dictionary" definition of an element as: "a substance which cannot be divided into a simpler form by known methods" and educational institutions' lists of elements generally align with this definition as well.

Although writers of lists of "rudimentary elements of music" can vary their lists depending on their personal (or institutional) priorities, the perceptual elements of music should consist of an established (or proven) list of discrete elements which can be independently manipulated to achieve an intended musical effect. It seems at this stage that there is still research to be done in this area.

Some styles of music place an emphasis on certain of these fundamentals, while others place less emphasis on certain elements. To give one example, while Bebop-era jazz makes use of very complex chords, including altered dominants and challenging chord progressions, with chords changing two or more times per bar and keys changing several times in a tune, funk places most of its emphasis on rhythm and groove, with entire songs based around a vamp on a single chord. While Romantic era classical music from the mid- to late-1800s makes great use of dramatic changes of dynamics, from whispering pianissimo sections to thunderous fortissimo sections, some entire Baroque dance suites for harpsichord from the early 1700s may use a single dynamic. To give another example, while some art music pieces, such as symphonies are very long, some pop songs are just a few minutes long.

Pitch is an aspect of a sound that we can hear, reflecting whether one musical sound, note or tone is "higher" or "lower" than another musical sound, note or tone. We can talk about the highness or lowness of pitch in the more general sense, such as the way a listener hears a piercingly high piccolo note or whistling tone as higher in pitch than a deep thump of a bass drum. We also talk about pitch in the precise sense associated with musical melodies, basslines and chords. Precise pitch can only be determined in sounds that have a frequency that is clear and stable enough to distinguish from noise. For example, it is much easier for listeners to discern the pitch of a single note played on a piano than to try to discern the pitch of a crash cymbal that is struck.
A melody (also called a "tune") is a series of pitches (notes) sounding in succession (one after the other), often in a rising and falling pattern. The notes of a melody are typically created using pitch systems such as scales or modes. Melodies also often contain notes from the chords used in the song. The melodies in simple folk songs and traditional songs may use only the notes of a single scale, the scale associated with the tonic note or key of a given song. For example, a folk song in the key of C (also referred to as C major) may have a melody that uses only the notes of the C major scale (the individual notes C, D, E, F, G, A, B and C; these are the "white notes" on a piano keyboard. On the other hand, Bebop-era jazz from the 1940s and contemporary music from the 20th and 21st centuries may use melodies with many chromatic notes (i.e., notes in addition to the notes of the major scale; on a piano, a chromatic scale would include all the notes on the keyboard, including the "white notes" and "black notes" and unusual scales, such as the whole tone scale (a whole tone scale in the key of C would contain the notes C, D, E, F, G and A). A low, deep musical line played by bass instruments such as double bass, electric bass or tuba is called a bassline.

Harmony refers to the "vertical" sounds of pitches in music, which means pitches that are played or sung together at the same time to create a chord. Usually this means the notes are played at the same time, although harmony may also be implied by a melody that outlines a harmonic structure (i.e., by using melody notes that are played one after the other, outlining the notes of a chord). In music written using the system of major-minor tonality ("keys"), which includes most classical music written from 1600 to 1900 and most Western pop, rock and traditional music, the key of a piece determines the scale used, which centres around the "home note" or tonic of the key. Simple classical pieces and many pop and traditional music songs are written so that all the music is in a single key. More complex Classical, pop and traditional music songs and pieces may have two keys (and in some cases three or more keys). Classical music from the Romantic era (written from about 1820–1900) often contains multiple keys, as does jazz, especially Bebop jazz from the 1940s, in which the key or "home note" of a song may change every four bars or even every two bars.

Rhythm is the arrangement of sounds and silences in time. Meter animates time in regular pulse groupings, called measures or bars, which in Western classical, popular and traditional music often group notes in sets of two (e.g., 2/4 time), three (e.g., 3/4 time, also known as Waltz time, or 3/8 time), or four (e.g., 4/4 time). Meters are made easier to hear because songs and pieces often (but not always) place an emphasis on the first beat of each grouping. Notable exceptions exist, such as the backbeat used in much Western pop and rock, in which a song that uses a measure that consists of four beats (called 4/4 time or common time) will have accents on beats two and four, which are typically performed by the drummer on the snare drum, a loud and distinctive-sounding percussion instrument. In pop and rock, the rhythm parts of a song are played by the rhythm section, which includes chord-playing instruments (e.g., electric guitar, acoustic guitar, piano, or other keyboard instruments), a bass instrument (typically electric bass or for some styles such as jazz and bluegrass, double bass) and a drum kit player.

Musical texture is the overall sound of a piece of music or song. The texture of a piece or sing is determined by how the melodic, rhythmic, and harmonic materials are combined in a composition, thus determining the overall nature of the sound in a piece. Texture is often described in regard to the density, or thickness, and range, or width, between lowest and highest pitches, in relative terms as well as more specifically distinguished according to the number of voices, or parts, and the relationship between these voices (see common types below). For example, a thick texture contains many 'layers' of instruments. One of these layers could be a string section, or another brass. The thickness also is affected by the amount and the richness of the instruments. Texture is commonly described according to the number of and relationship between parts or lines of music:

Music that contains a large number of independent parts (e.g., a double concerto accompanied by 100 orchestral instruments with many interweaving melodic lines) is generally said to have a "thicker" or "denser" texture than a work with few parts (e.g., a solo flute melody accompanied by a single cello).

Timbre, sometimes called "color" or "tone color" is the quality or sound of a voice or instrument. Timbre is what makes a particular musical sound different from another, even when they have the same pitch and loudness. For example, a 440 Hz A note sounds different when it is played on oboe, piano, violin or electric guitar. Even if different players of the same instrument play the same note, their notes might sound different due to differences in instrumental technique (e.g., different embouchures), different types of accessories (e.g., mouthpieces for brass players, reeds for oboe and bassoon players) or strings made out of different materials for string players (e.g., gut strings versus steel strings). Even two instrumentalists playing the same note on the same instrument (one after the other) may sound different due to different ways of playing the instrument (e.g., two string players might hold the bow differently).

The physical characteristics of sound that determine the perception of timbre include the spectrum, envelope and overtones of a note or musical sound. For electric instruments developed in the 20th century, such as electric guitar, electric bass and electric piano, the performer can also change the tone by adjusting equalizer controls, tone controls on the instrument, and by using electronic effects units such as distortion pedals. The tone of the electric Hammond organ is controlled by adjusting drawbars.

Expressive qualities are those elements in music that create change in music without changing the main pitches or substantially changing the rhythms of the melody and its accompaniment. Performers, including singers and instrumentalists, can add musical expression to a song or piece by adding phrasing, by adding effects such as vibrato (with voice and some instruments, such as guitar, violin, brass instruments and woodwinds), dynamics (the loudness or softness of piece or a section of it), tempo fluctuations (e.g., ritardando or accelerando, which are, respectively slowing down and speeding up the tempo), by adding pauses or fermatas on a cadence, and by changing the articulation of the notes (e.g., making notes more pronounced or accented, by making notes more legato, which means smoothly connected, or by making notes shorter).

Expression is achieved through the manipulation of pitch (such as inflection, vibrato, slides etc.), volume (dynamics, accent, tremolo etc.), duration (tempo fluctuations, rhythmic changes, changing note duration such as with legato and staccato, etc.), timbre (e.g. changing vocal timbre from a light to a resonant voice) and sometimes even texture (e.g. doubling the bass note for a richer effect in a piano piece). Expression therefore can be seen as a manipulation of all elements in order to convey "an indication of mood, spirit, character etc." and as such cannot be included as a unique perceptual element of music, although it can be considered an important rudimentary element of music.

In music, form describes how the overall structure or plan of a song or piece of music, and it describes the layout of a composition as divided into sections. In the early 20th century, Tin Pan Alley songs and Broadway musical songs were often in AABA 32 bar form, in which the A sections repeated the same eight bar melody and the B section provided a contrasting melody and/or harmony for 8 bars. From the 1960s onward, Western pop and rock songs are often in verse-chorus form, which is based around a sequence of verse and chorus ("refrain") sections, with new lyrics for most verses and repeating lyrics for the choruses. Popular music often makes use of strophic form, sometimes in conjunction with the twelve bar blues.

In the tenth edition of "The Oxford Companion to Music", Percy Scholes defines musical form as "a series of strategies designed to find a successful mean between the opposite extremes of unrelieved repetition and unrelieved alteration." Examples of common forms of Western music include the fugue, the invention, sonata-allegro, canon, strophic, theme and variations, and rondo.
Scholes states that European classical music had only six stand-alone forms: simple binary, simple ternary, compound binary, rondo, air with variations, and fugue (although musicologist Alfred Mann emphasized that the fugue is primarily a method of composition that has sometimes taken on certain structural conventions.)

Where a piece cannot readily be broken down into sectional units (though it might borrow some form from a poem, story or programme), it is said to be through-composed. Such is often the case with a fantasia, prelude, rhapsody, etude (or study), symphonic poem, Bagatelle, impromptu, etc. Professor Charles Keil classified forms and formal detail as "sectional, developmental, or variational."

This form is built from a sequence of clear-cut units that may be referred to by letters but also often have generic names such as introduction and coda, exposition, development and recapitulation, verse, chorus or refrain, and bridge. Introductions and codas, when they are no more than that, are frequently excluded from formal analysis. All such units may typically be eight measures long. Sectional forms include:

This form is defined by its "unrelieved repetition" (AAAA...).


Medley, potpourri is the extreme opposite, that of "unrelieved variation": it is simply an indefinite sequence of self-contained sections (ABCD...), sometimes with repeats (AABBCCDD...). Examples include orchestral overtures, which are sometimes no more than a string of the best tunes of the musical theatre show or opera to come.

This form uses two sections (AB...), each often repeated (AABB...). In 18th-century Western classical music, "simple binary" form was often used for dances and carried with it the convention that the two sections should be in different musical keys but same rhythm, duration and tone. The alternation of two tunes gives enough variety to permit a dance to be extended for as long as desired.
b
This form has three parts. In Western classical music a simple ternary form has a third section that is a recapitulation of the first (ABA). Often, the first section is repeated (AABA). This approach was popular in the 18th-century operatic aria, and was called "da capo" (i.e. "repeat from the top") form. Later, it gave rise to the 32-bar song, with the B section then often referred to as the "middle eight". A song has more need than a dance of a self-contained form with a beginning and an end of course.

This form has a recurring theme alternating with different (usually contrasting) sections called "episodes". It may be asymmetrical (ABACADAEA) or symmetrical (ABACABA). A recurring section, especially the main theme, is sometimes more thoroughly varied, or else one episode may be a "development" of it. A similar arrangement is the ritornello form of the Baroque concerto grosso. Arch form (ABCBA) resembles a symmetrical rondo without intermediate repetitions of the main theme. It is normally used in a round.

Variational forms are those in which variation is an important formative element.

Theme and Variations: a theme, which in itself can be of any shorter form (binary, ternary, etc.), forms the only "section" and is repeated indefinitely (as in strophic form) but is varied each time (A, B, A, F, Z, A), so as to make a sort of sectional chain form. An important variant of this, much used in 17th-century British music and in the Passacaglia and Chaconne, was that of the ground bass – a repeating bass theme or "basso ostinato" over and around which the rest of the structure unfolds, often, but not always, spinning polyphonic or contrapuntal threads, or improvising divisions and descants. This is said by Scholes (1977) to be the form "par excellence" of unaccompanied or accompanied solo instrumental music. The Rondo is often found with sections varied (AABACABA) or (ABACABA).

Developmental forms are built directly from smaller units, such as motifs. A well-known Classical piece with a motif is Beethoven's fifth symphony, which starts with three short repeated notes and then a long note. In Classical pieces that are based on motifs, the motif is usually combined, varied and worked out in different ways, perhaps having a symmetrical or arch-like underpinning and a progressive development from beginning to end. By far the most important developmental form in Western classical music is Sonata form.
This form, also known as "sonata" form, first movement form, compound binary, ternary and a variety of other names, developed from the binary-formed dance movement described above but is almost always cast in a greater ternary form having the nominal subdivisions of "Exposition, Development" and "Recapitulation". Usually, but not always, the "A" parts (Exposition and Recapitulation, respectively) may be subdivided into two or three themes or theme groups which are taken asunder and recombined to form the "B" part (the development) – thus e. g. (AabB[dev. of a and/or b]Aab+coda). This developmental form is generally confined to certain sections of the piece, as to the middle section of the first movement of a sonata, though 19th-century composers such as Berlioz, Liszt and Wagner made valiant efforts to derive large-scale works purely or mainly from the motif.

Prehistoric music can only be theorized based on findings from paleolithic archaeology sites. Flutes are often discovered, carved from bones in which lateral holes have been pierced; these are thought to have been blown at one end like the Japanese shakuhachi. The Divje Babe flute, carved from a cave bear femur, is thought to be at least 40,000 years old. Instruments such as the seven-holed flute and various types of stringed instruments, such as the Ravanahatha, have been recovered from the Indus Valley Civilization archaeological sites. India has one of the oldest musical traditions in the world—references to Indian classical music ("marga") are found in the Vedas, ancient scriptures of the Hindu tradition. The earliest and largest collection of prehistoric musical instruments was found in China and dates back to between 7000 and 6600 BC. The "Hurrian Hymn to Nikkal", found on clay tablets that date back to approximately 1400 BC, is the oldest surviving notated work of music.

The ancient Egyptians credited one of their gods, Thoth, with the invention of music, with Osiris in turn used as part of his effort to civilize the world. The earliest material and representational evidence of Egyptian musical instruments dates to the Predynastic period, but the evidence is more securely attested in the Old Kingdom when harps, flutes and double clarinets were played. Percussion instruments, lyres and lutes were added to orchestras by the Middle Kingdom. Cymbals frequently accompanied music and dance, much as they still do in Egypt today. Egyptian folk music, including the traditional Sufi "dhikr" rituals, are the closest contemporary music genre to ancient Egyptian music, having preserved many of its features, rhythms and instruments.

Indian classical music is one of the oldest musical traditions in the world. The Indus Valley civilization has sculptures that show dance and old musical instruments, like the seven holed flute. Various types of stringed instruments and drums have been recovered from Harappa and Mohenjo Daro by excavations carried out by Sir Mortimer Wheeler. The Rigveda has elements of present Indian music, with a musical notation to denote the metre and the mode of chanting. Indian classical music (marga) is monophonic, and based on a single melody line or raga rhythmically organized through talas. "Silappadhikaram" by Ilango Adigal provides information about how new scales can be formed by modal shifting of the tonic from an existing scale. Hindustani music was influenced by the Persian performance practices of the Afghan Mughals. Carnatic music, popular in the southern states, is largely devotional; the majority of the songs are addressed to the Hindu deities. There are also many songs emphasising love and other social issues.

Asian music covers the music cultures of Arabia, Central Asia, East Asia, South Asia, and Southeast Asia. Chinese classical music, the traditional art or court music of China, has a history stretching over around three thousand years. It has its own unique systems of musical notation, as well as musical tuning and pitch, musical instruments and styles or musical genres. Chinese music is pentatonic-diatonic, having a scale of twelve notes to an octave (5 + 7 = 12) as does European-influenced music. Persian music is the music of Persia and Persian language countries: "musiqi", the science and art of music, and "muzik", the sound and performance of music (Sakata 1983).

Music and theatre scholars studying the history and anthropology of Semitic and early Judeo-Christian culture have discovered common links in theatrical and musical activity between the classical cultures of the Hebrews and those of later Greeks and Romans. The common area of performance is found in a "social phenomenon called litany," a form of prayer consisting of a series of invocations or supplications. "The Journal of Religion and Theatre" notes that among the earliest forms of litany, "Hebrew litany was accompanied by a rich musical tradition:"

Music was an important part of social and cultural life in ancient Greece. Musicians and singers played a prominent role in Greek theater. Mixed-gender choruses performed for entertainment, celebration, and spiritual ceremonies. Instruments included the double-reed "aulos" and a plucked string instrument, the "lyre", principally the special kind called a "kithara". Music was an important part of education, and boys were taught music starting at age six. Greek musical literacy created a flowering of music development. Greek music theory included the Greek musical modes, that eventually became the basis for Western religious and classical music. Later, influences from the Roman Empire, Eastern Europe, and the Byzantine Empire changed Greek music. The Seikilos epitaph is the oldest surviving example of a complete musical composition, including musical notation, from anywhere in the world. The oldest surviving work written on the subject of music theory is "Harmonika Stoicheia" by Aristoxenus.

The medieval era (476 to 1400), which took place during the Middle Ages, started with the introduction of monophonic (single melodic line) chanting into Roman Catholic Church services. Musical notation was used since Ancient times in Greek culture, but in the Middle Ages, notation was first introduced by the Catholic church so that the chant melodies could be written down, to facilitate the use of the same melodies for religious music across the entire Catholic empire. The only European Medieval repertory that has been found in written form from before 800 is the monophonic liturgical plainsong chant of the Roman Catholic Church, the central tradition of which was called Gregorian chant. Alongside these traditions of sacred and church music there existed a vibrant tradition of secular song (non-religious songs). Examples of composers from this period are Léonin, Pérotin, Guillaume de Machaut, and Walther von der Vogelweide.

Renaissance music (c. 1400 to 1600) was more focused on secular (non-religious) themes, such as courtly love. Around 1450, the printing press was invented, which made printed sheet music much less expensive and easier to mass-produce (prior to the invention of the printing press, all notated music was hand-copied). The increased availability of sheet music helped to spread musical styles more quickly and across a larger area. Musicians and singers often worked for the church, courts and towns. Church choirs grew in size, and the church remained an important patron of music. By the middle of the 15th century, composers wrote richly polyphonic sacred music, in which different melody lines were interwoven simultaneously. Prominent composers from this era include Guillaume Dufay, Giovanni Pierluigi da Palestrina, Thomas Morley, and Orlande de Lassus. As musical activity shifted from the church to the aristocratic courts, kings, queens and princes competed for the finest composers. Many leading important composers came from the Netherlands, Belgium, and northern France. They are called the Franco-Flemish composers. They held important positions throughout Europe, especially in Italy. Other countries with vibrant musical activity included Germany, England, and Spain.

The Baroque era of music took place from 1600 to 1750, as the Baroque artistic style flourished across Europe; and during this time, music expanded in its range and complexity. Baroque music began when the first operas (dramatic solo vocal music accompanied by orchestra) were written. During the Baroque era, polyphonic contrapuntal music, in which multiple, simultaneous independent melody lines were used, remained important (counterpoint was important in the vocal music of the Medieval era). German Baroque composers wrote for small ensembles including strings, brass, and woodwinds, as well as for choirs and keyboard instruments such as pipe organ, harpsichord, and clavichord. During this period several major music forms were defined that lasted into later periods when they were expanded and evolved further, including the fugue, the invention, the sonata, and the concerto. The late Baroque style was polyphonically complex and richly ornamented. Important composers from the Baroque era include Johann Sebastian Bach, George Frideric Handel, Georg Philipp Telemann and Antonio Lucio Vivaldi.

The music of the Classical period (1730 to 1820) aimed to imitate what were seen as the key elements of the art and philosophy of Ancient Greece and Rome: the ideals of balance, proportion and disciplined expression. (Note: the music from the Classical period should not be confused with Classical music in general, a term which refers to Western art music from the 5th century to the 2000s, which includes the Classical period as one of a number of periods). Music from the Classical period has a lighter, clearer and considerably simpler texture than the Baroque music which preceded it. The main style was homophony, where a prominent melody and a subordinate chordal accompaniment part are clearly distinct. Classical instrumental melodies tended to be almost voicelike and singable. New genres were developed, and the fortepiano, the forerunner to the modern piano, replaced the Baroque era harpsichord and pipe organ as the main keyboard instrument.

Importance was given to instrumental music. It was dominated by further development of musical forms initially defined in the Baroque period: the sonata, the concerto, and the symphony. Others main kinds were the trio, string quartet, serenade and divertimento. The sonata was the most important and developed form. Although Baroque composers also wrote sonatas, the Classical style of sonata is completely distinct. All of the main instrumental forms of the Classical era, from string quartets to symphonies and concertos, were based on the structure of the sonata. The instruments used chamber music and orchestra became more standardized. In place of the basso continuo group of the Baroque era, which consisted of harpsichord, organ or lute along with a number of bass instruments selected at the discretion of the group leader (e.g., viol, cello, theorbo, serpent), Classical chamber groups used specified, standardized instruments (e.g., a string quartet would be performed by two violins, a viola and a cello). The Baroque era improvised chord-playing of the continuo keyboardist or lute player was gradually phased out between 1750 and 1800.

One of the most important changes made in the Classical period was the development of public concerts. The aristocracy still played a significant role in the sponsorship of concerts and compositions, but it was now possible for composers to survive without being permanent employees of queens or princes. The increasing popularity of classical music led to a growth in the number and types of orchestras. The expansion of orchestral concerts necessitated the building of large public performance spaces. Symphonic music including symphonies, musical accompaniment to ballet and mixed vocal/instrumental genres such as opera and oratorio became more popular.

The best known composers of Classicism are Carl Philipp Emanuel Bach, Christoph Willibald Gluck, Johann Christian Bach, Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven and Franz Schubert. Beethoven and Schubert are also considered to be composers in the later part of the Classical era, as it began to move towards Romanticism.

Romantic music (c. 1810 to 1900) from the 19th century had many elements in common with the Romantic styles in literature and painting of the era. Romanticism was an artistic, literary, and intellectual movement was characterized by its emphasis on emotion and individualism as well as glorification of all the past and nature. Romantic music expanded beyond the rigid styles and forms of the Classical era into more passionate, dramatic expressive pieces and songs. Romantic composers such as Wagner and Brahms attempted to increase emotional expression and power in their music to describe deeper truths or human feelings. With symphonic tone poems, composers tried to tell stories and evoke images or landscapes using instrumental music. Some composers promoted nationalistic pride with patriotic orchestral music inspired by folk music. The emotional and expressive qualities of music came to take precedence over tradition.

Romantic composers grew in idiosyncrasy, and went further in the syncretism of exploring different art-forms in a musical context, (such as literature), history (historical figures and legends), or nature itself. Romantic love or longing was a prevalent theme in many works composed during this period. In some cases the formal structures from the classical period continued to be used (e.g., the sonata form used in string quartets and symphonies), but these forms were expanded and altered. In many cases, new approaches were explored for existing genres, forms, and functions. Also, new forms were created that were deemed better suited to the new subject matter. Composers continued to develop opera and ballet music, exploring new styles and themes.

In the years after 1800, the music developed by Ludwig van Beethoven and Franz Schubert introduced a more dramatic, expressive style. In Beethoven's case, short motifs, developed organically, came to replace melody as the most significant compositional unit (an example is the distinctive four note figure used in his Fifth Symphony). Later Romantic composers such as Pyotr Ilyich Tchaikovsky, Antonín Dvořák, and Gustav Mahler used more unusual chords and more dissonance to create dramatic tension. They generated complex and often much longer musical works. During the late Romantic period, composers explored dramatic chromatic alterations of tonality, such as extended chords and altered chords, which created new sound "colours". The late 19th century saw a dramatic expansion in the size of the orchestra, and the industrial revolution helped to create better instruments, creating a more powerful sound. Public concerts became an important part of well-to-do urban society. It also saw a new diversity in theatre music, including operetta, and musical comedy and other forms of musical theatre.

In the 19th century, one of the key ways that new compositions became known to the public was by the sales of sheet music, which middle class amateur music lovers would perform at home on their piano or other common instruments, such as violin. With 20th-century music, the invention of new electric technologies such as radio broadcasting and the mass market availability of gramophone records meant that sound recordings of songs and pieces heard by listeners (either on the radio or on their record player) became the main way to learn about new songs and pieces. There was a vast increase in music listening as the radio gained popularity and phonographs were used to replay and distribute music, because whereas in the 19th century, the focus on sheet music restricted access to new music to the middle class and upper-class people who could read music and who owned pianos and instruments, in the 20th century, anyone with a radio or record player could hear operas, symphonies and big bands right in their own living room. This allowed lower-income people, who would never be able to afford an opera or symphony concert ticket to hear this music. It also meant that people could hear music from different parts of the country, or even different parts of the world, even if they could not afford to travel to these locations. This helped to spread musical styles.

The focus of art music in the 20th century was characterized by exploration of new rhythms, styles, and sounds. The horrors of World War I influenced many of the arts, including music, and some composers began exploring darker, harsher sounds. Traditional music styles such as jazz and folk music were used by composers as a source of ideas for classical music. Igor Stravinsky, Arnold Schoenberg, and John Cage were all influential composers in 20th-century art music. The invention of sound recording and the ability to edit music gave rise to new subgenre of classical music, including the acousmatic and Musique concrète schools of electronic composition. Sound recording was also a major influence on the development of popular music genres, because it enabled recordings of songs and bands to be widely distributed. The introduction of the multitrack recording system had a major influence on rock music, because it could do much more than record a band's performance. Using a multitrack system, a band and their music producer could overdub many layers of instrument tracks and vocals, creating new sounds that would not be possible in a live performance.

Jazz evolved and became an important genre of music over the course of the 20th century, and during the second half of that century, rock music did the same. Jazz is an American musical artform that originated in the beginning of the 20th century in African American communities in the Southern United States from a confluence of African and European music traditions. The style's West African pedigree is evident in its use of blue notes, improvisation, polyrhythms, syncopation, and the swung note.

Rock music is a genre of popular music that developed in the 1960s from 1950s rock and roll, rockabilly, blues, and country music. The sound of rock often revolves around the electric guitar or acoustic guitar, and it uses a strong back beat laid down by a rhythm section of electric bass guitar, drums, and keyboard instruments such as organ, piano, or, since the 1970s, analog synthesizers and digital ones and computers since the 1990s. Along with the guitar or keyboards, saxophone and blues-style harmonica are used as soloing instruments. In its "purest form," it "has three chords, a strong, insistent back beat, and a catchy melody."

Performance is the physical expression of music, which occurs when a song is sung or when a piano piece, electric guitar melody, symphony, drum beat or other musical part is played by musicians. In classical music, a musical work is written in music notation by a composer and then it is performed once the composer is satisfied with its structure and instrumentation. However, as it gets performed, the interpretation of a song or piece can evolve and change. In classical music, instrumental performers, singers or conductors may gradually make changes to the phrasing or tempo of a piece. In popular and traditional music, the performers have a lot more freedom to make changes to the form of a song or piece. As such, in popular and traditional music styles, even when a band plays a cover song, they can make changes to it such as adding a guitar solo to or inserting an introduction.

A performance can either be planned out and rehearsed (practiced)—which is the norm in classical music, with jazz big bands and many popular music styles–or improvised over a chord progression (a sequence of chords), which is the norm in small jazz and blues groups. Rehearsals of orchestras, concert bands and choirs are led by a conductor. Rock, blues and jazz bands are usually led by the bandleader. A rehearsal is a structured repetition of a song or piece by the performers until it can be sung and/or played correctly and, if it is a song or piece for more than one musician, until the parts are together from a rhythmic and tuning perspective. Improvisation is the creation of a musical idea–a melody or other musical line–created on the spot, often based on scales or pre-existing melodic riffs.

Many cultures have strong traditions of solo performance (in which one singer or instrumentalist performs), such as in Indian classical music, and in the Western art-music tradition. Other cultures, such as in Bali, include strong traditions of group performance. All cultures include a mixture of both, and performance may range from improvised solo playing to highly planned and organised performances such as the modern classical concert, religious processions, classical music festivals or music competitions. Chamber music, which is music for a small ensemble with only a few of each type of instrument, is often seen as more intimate than large symphonic works.

Many types of music, such as traditional blues and folk music were not written down in sheet music; instead, they were originally preserved in the memory of performers, and the songs were handed down orally, from one musician or singer to another, or aurally, in which a performer learns a song "by ear". When the composer of a song or piece is no longer known, this music is often classified as "traditional" or as a "folk song". Different musical traditions have different attitudes towards how and where to make changes to the original source material, from quite strict, to those that demand improvisation or modification to the music. A culture's history and stories may also be passed on by ear through song.

In music, an "ornament" is a decoration to a melody, bassline or other musical part. The detail included explicitly in the music notation varies between genres and historical periods. In general, art music notation from the 17th through the 19th centuries required performers to have a great deal of contextual knowledge about performing styles. For example, in the 17th and 18th centuries, music notated for solo performers typically indicated a simple, unadorned melody. However, performers were expected to know how to add stylistically appropriate ornaments to add interest to the music, such as trills and turns.

In the 19th century, art music for solo performers may give a general instruction such as to perform the music expressively, without describing in detail how the performer should do this. The performer was expected to know how to use tempo changes, accentuation, and pauses (among other devices) to obtain this "expressive" performance style. In the 20th century, art music notation often became more explicit and used a range of markings and annotations to indicate to performers how they should play or sing the piece.

Philosophy of music is a subfield of philosophy. The philosophy of music is the study of fundamental questions regarding music. The philosophical study of music has many connections with philosophical questions in metaphysics and aesthetics.
Some basic questions in the philosophy of music are:

In ancient times, such as with the Ancient Greeks, the aesthetics of music explored the mathematical and cosmological dimensions of rhythmic and harmonic organization. In the 18th century, focus shifted to the experience of hearing music, and thus to questions about its beauty and human enjoyment ("plaisir" and "jouissance") of music. The origin of this philosophic shift is sometimes attributed to Baumgarten in the 18th century, followed by Kant. Through their writing, the ancient term 'aesthetics', meaning sensory perception, received its present-day connotation. In the 2000s, philosophers have tended to emphasize issues besides beauty and enjoyment. For example, music's capacity to express emotion has been a central issue.

In the 20th century, important contributions were made by Peter Kivy, Jerrold Levinson, Roger Scruton, and Stephen Davies. However, many musicians, music critics, and other non-philosophers have contributed to the aesthetics of music. In the 19th century, a significant debate arose between Eduard Hanslick, a music critic and musicologist, and composer Richard Wagner regarding whether music can express meaning. Harry Partch and some other musicologists, such as Kyle Gann, have studied and tried to popularize microtonal music and the usage of alternate musical scales. Also many modern composers like La Monte Young, Rhys Chatham and Glenn Branca paid much attention to a scale called just intonation.

It is often thought that music has the ability to affect our emotions, intellect, and psychology; it can assuage our loneliness or incite our passions. The philosopher Plato suggests in "the Republic" that music has a direct effect on the soul. Therefore, he proposes that in the ideal regime music would be closely regulated by the state. (Book VII)

There has been a strong tendency in the aesthetics of music to emphasize the paramount importance of compositional structure; however, other issues concerning the aesthetics of music include lyricism, harmony, hypnotism, emotiveness, temporal dynamics, resonance, playfulness, and color (see also musical development).

Modern music psychology aims to explain and understand musical behavior and experience. Research in this field and its subfields are primarily empirical; their knowledge tends to advance on the basis of interpretations of data collected by systematic observation of and interaction with human participants. In addition to its focus on fundamental perceptions and cognitive processes, music psychology is a field of research with practical relevance for many areas, including music performance, composition, education, criticism, and therapy, as well as investigations of human aptitude, skill, intelligence, creativity, and social behavior.

Cognitive neuroscience of music is the scientific study of brain-based mechanisms involved in the cognitive processes underlying music. These behaviours include music listening, performing, composing, reading, writing, and ancillary activities. It also is increasingly concerned with the brain basis for musical aesthetics and musical emotion. The field is distinguished by its reliance on direct observations of the brain, using such techniques as functional magnetic resonance imaging (fMRI), transcranial magnetic stimulation (TMS), magnetoencephalography (MEG), electroencephalography (EEG), and positron emission tomography (PET).

Cognitive musicology is a branch of cognitive science concerned with computationally modeling musical knowledge with the goal of understanding both music and cognition. The use of computer models provides an exacting, interactive medium in which to formulate and test theories and has roots in artificial intelligence and cognitive science.

This interdisciplinary field investigates topics such as the parallels between language and music in the brain. Biologically inspired models of computation are often included in research, such as neural networks and evolutionary programs. This field seeks to model how musical knowledge is represented, stored, perceived, performed, and generated. By using a well-structured computer environment, the systematic structures of these cognitive phenomena can be investigated.

Psychoacoustics is the scientific study of sound perception. More specifically, it is the branch of science studying the psychological and physiological responses associated with sound (including speech and music). It can be further categorized as a branch of psychophysics.

Evolutionary musicology concerns the "origins of music, the question of animal song, selection pressures underlying music evolution", and "music evolution and human evolution". It seeks to understand music perception and activity in the context of evolutionary theory. Charles Darwin speculated that music may have held an adaptive advantage and functioned as a protolanguage, a view which has spawned several competing theories of music evolution. An alternate view sees music as a by-product of linguistic evolution; a type of "auditory cheesecake" that pleases the senses without providing any adaptive function. This view has been directly countered by numerous music researchers.

An individual's culture or ethnicity plays a role in their music cognition, including their preferences, emotional reaction, and musical memory. Musical preferences are biased toward culturally familiar musical traditions beginning in infancy, and adults' classification of the emotion of a musical piece depends on both culturally specific and universal structural features. Additionally, individuals' musical memory abilities are greater for culturally familiar music than for culturally unfamiliar music.

Many ethnographic studies demonstrate that music is a participatory, community-based activity. Music is experienced by individuals in a range of social settings ranging from being alone to attending a large concert, forming a music community, which cannot be understood as a function of individual will or accident; it includes both commercial and non-commercial participants with a shared set of common values. Musical performances take different forms in different cultures and socioeconomic milieus. In Europe and North America, there is often a divide between what types of music are viewed as a "high culture" and "low culture." "High culture" types of music typically include Western art music such as Baroque, Classical, Romantic, and modern-era symphonies, concertos, and solo works, and are typically heard in formal concerts in concert halls and churches, with the audience sitting quietly in seats.

Other types of music—including, but not limited to, jazz, blues, soul, and country—are often performed in bars, nightclubs, and theatres, where the audience may be able to drink, dance, and express themselves by cheering. Until the later 20th century, the division between "high" and "low" musical forms was widely accepted as a valid distinction that separated out better quality, more advanced "art music" from the popular styles of music heard in bars and dance halls.

However, in the 1980s and 1990s, musicologists studying this perceived divide between "high" and "low" musical genres argued that this distinction is not based on the musical value or quality of the different types of music. Rather, they argued that this distinction was based largely on the socioeconomics standing or social class of the performers or audience of the different types of music. For example, whereas the audience for Classical symphony concerts typically have above-average incomes, the audience for a rap concert in an inner-city area may have below-average incomes. Even though the performers, audience, or venue where non-"art" music is performed may have a lower socioeconomic status, the music that is performed, such as blues, rap, punk, funk, or ska may be very complex and sophisticated.

When composers introduce styles of music that break with convention, there can be a strong resistance from academic music experts and popular culture. Late-period Beethoven string quartets, Stravinsky ballet scores, serialism, bebop-era jazz, hip hop, punk rock, and electronica have all been considered non-music by some critics when they were first introduced. Such themes are examined in the sociology of music. The sociological study of music, sometimes called sociomusicology, is often pursued in departments of sociology, media studies, or music, and is closely related to the field of ethnomusicology.

Women have played a major role in music throughout history, as composers, songwriters, instrumental performers, singers, conductors, music scholars, music educators, music critics/music journalists and other musical professions. As well, it describes music movements, events and genres related to women, women's issues and feminism. In the 2010s, while women comprise a significant proportion of popular music and classical music singers, and a significant proportion of songwriters (many of them being singer-songwriters), there are few women record producers, rock critics and rock instrumentalists. Although there have been a huge number of women composers in classical music, from the Medieval period to the present day, women composers are significantly underrepresented in the commonly performed classical music repertoire, music history textbooks and music encyclopedias; for example, in the "Concise Oxford History of Music", Clara Schumann is one of the only female composers who is mentioned.

Women comprise a significant proportion of instrumental soloists in classical music and the percentage of women in orchestras is increasing. A 2015 article on concerto soloists in major Canadian orchestras, however, indicated that 84% of the soloists with the Orchestre Symphonique de Montreal were men. In 2012, women still made up just 6% of the top-ranked Vienna Philharmonic orchestra. Women are less common as instrumental players in popular music genres such as rock and heavy metal, although there have been a number of notable female instrumentalists and all-female bands. Women are particularly underrepresented in extreme metal genres. In the 1960s pop music scene, "[l]ike most aspects of the...music business, [in the 1960s,] songwriting was a male-dominated field. Though there were plenty of female singers on the radio, women ...were primarily seen as consumers:... Singing was sometimes an acceptable pastime for a girl, but playing an instrument, writing songs, or producing records simply wasn't done." Young women "...were not socialized to see themselves as people who create [music]."

Women are also underrepresented in orchestral conducting, music criticism/music journalism, music producing, and sound engineering. While women were discouraged from composing in the 19th century, and there are few women musicologists, women became involved in music education "...to such a degree that women dominated [this field] during the later half of the 19th century and well into the 20th century."

According to Jessica Duchen, a music writer for London's "The Independent", women musicians in classical music are "...too often judged for their appearances, rather than their talent" and they face pressure "...to look sexy onstage and in photos." Duchen states that while "[t]here are women musicians who refuse to play on their looks...the ones who do tend to be more materially successful."
According to the UK's Radio 3 editor, Edwina Wolstencroft, the music industry has long been open to having women in performance or entertainment roles, but women are much less likely to have positions of authority, such as being the leader of an orchestra. In popular music, while there are many women singers recording songs, there are very few women behind the audio console acting as music producers, the individuals who direct and manage the recording process. One of the most recorded artists is Asha Bhosle, an Indian singer best known as a playback singer in Hindi cinema.

The music that composers make can be heard through several media; the most traditional way is to hear it live, in the presence of the musicians (or as one of the musicians), in an outdoor or indoor space such as an amphitheatre, concert hall, cabaret room or theatre. Since the 20th century, live music can also be broadcast over the radio, television or the Internet, or recorded and listened to on a CD player or Mp3 player. Some musical styles focus on producing a sound for a performance, while others focus on producing a recording that mixes together sounds that were never played "live." Recording, even of essentially live styles such as rock, often uses the ability to edit and splice to produce recordings that may be considered "better" than the actual performance.

Technology has had an influence on music since prehistoric times, when cave people used simple tools to bore holes into bone flutes 41,000 years ago. Technology continued to influence music throughout the history of music, as it enabled new instruments and music notation reproduction systems to be used, with one of the watershed moments in music notation being the invention of the printing press in the 1400s, which meant music scores no longer had to be hand copied. In the 19th century, music technology led to the development of a more powerful, louder piano and led to the development of new valves brass instruments. In the early 20th century (in the late 1920s), as talking pictures emerged in the early 20th century, with their prerecorded musical tracks, an increasing number of moviehouse orchestra musicians found themselves out of work. During the 1920s live musical performances by orchestras, pianists, and theater organists were common at first-run theaters. With the coming of the talking motion pictures, those featured performances were largely eliminated. The American Federation of Musicians (AFM) took out newspaper advertisements protesting the replacement of live musicians with mechanical playing devices. One 1929 ad that appeared in the "Pittsburgh Press" features an image of a can labeled "Canned Music / Big Noise Brand / Guaranteed to Produce No Intellectual or Emotional Reaction Whatever"

Since legislation introduced to help protect performers, composers, publishers and producers, including the Audio Home Recording Act of 1992 in the United States, and the 1979 revised Berne Convention for the Protection of Literary and Artistic Works in the United Kingdom, recordings and live performances have also become more accessible through computers, devices and Internet in a form that is commonly known as Music-On-Demand.

In many cultures, there is less distinction between performing and listening to music, since virtually everyone is involved in some sort of musical activity, often communal. In industrialized countries, listening to music through a recorded form, such as sound recording or watching a music video, became more common than experiencing live performance, roughly in the middle of the 20th century.

Sometimes, live performances incorporate prerecorded sounds. For example, a disc jockey uses disc records for scratching, and some 20th-century works have a solo for an instrument or voice that is performed along with music that is prerecorded onto a tape. Computers and many keyboards can be programmed to produce and play Musical Instrument Digital Interface (MIDI) music. Audiences can also "become" performers by participating in karaoke, an activity of Japanese origin centered on a device that plays voice-eliminated versions of well-known songs. Most karaoke machines also have video screens that show lyrics to songs being performed; performers can follow the lyrics as they sing over the instrumental tracks.

The advent of the Internet and widespread high-speed broadband access has transformed the experience of music, partly through the increased ease of access to recordings of music via streaming video and vastly increased choice of music for consumers. Chris Anderson, in his book "", suggests that while the traditional economic model of supply and demand describes scarcity, the Internet retail model is based on abundance. Digital storage costs are low, so a company can afford to make its whole recording inventory available online, giving customers as much choice as possible. It has thus become economically viable to offer music recordings that very few people are interested in. Consumers' growing awareness of their increased choice results in a closer association between listening tastes and social identity, and the creation of thousands of niche markets.

Another effect of the Internet arose with online communities and social media websites like YouTube and Facebook, a social networking service. These sites make it easier for aspiring singers and amateur bands to distribute videos of their songs, connect with other musicians, and gain audience interest. Professional musicians also use YouTube as a free publisher of promotional material. YouTube users, for example, no longer only download and listen to MP3s, but also actively create their own. According to Don Tapscott and Anthony D. Williams, in their book "Wikinomics", there has been a shift from a traditional consumer role to what they call a "prosumer" role, a consumer who both creates content and consumes. Manifestations of this in music include the production of mashes, remixes, and music videos by fans.

The music industry refers to the businesses connected with the creation and sale of music. It consists of songwriters and composers who create new songs and musical pieces, music producers and sound engineers who record songs and pieces, record labels and publishers that distribute recorded music products and sheet music internationally and that often control the rights to those products. Some music labels are "independent," while others are subsidiaries of larger corporate entities or international media groups. In the 2000s, the increasing popularity of listening to music as digital music files on MP3 players, iPods, or computers, and of trading music on file sharing websites or buying it online in the form of digital files had a major impact on the traditional music business. Many smaller independent CD stores went out of business as music buyers decreased their purchases of CDs, and many labels had lower CD sales. Some companies did well with the change to a digital format, though, such as Apple's iTunes, an online music store that sells digital files of songs over the Internet.

In spite of some international copyright treaties, determining which music is in the public domain is complicated by of national copyright laws that may be applicable. US copyright law formerly protected printed music published after 1923 for 28 years and with renewal for another 28 years, but the Copyright Act of 1976 made renewal automatic, and the Digital Millennium Copyright Act changed the calculation of the copyright term to 70 years after the death of the creator. Recorded sound falls under mechanical licensing, often covered by a confusing patchwork of state laws; most cover versions are licensed through the Harry Fox Agency. Performance rights may be obtained by either performers or the performance venue; the two major organizations for licensing are BMI and ASCAP. Two online sources for public domain music are IMSLP (International Music Score Library Project) and Choral Public Domain Library (CPDL).

The incorporation of some music or singing training into general education from preschool to post secondary education is common in North America and Europe. Involvement in playing and singing music is thought to teach basic skills such as concentration, counting, listening, and cooperation while also promoting understanding of language, improving the ability to recall information, and creating an environment more conducive to learning in other areas. In elementary schools, children often learn to play instruments such as the recorder, sing in small choirs, and learn about the history of Western art music and traditional music. Some elementary school children also learn about popular music styles. In religious schools, children sing hymns and other religious music. In secondary schools (and less commonly in elementary schools), students may have the opportunity to perform in some types of musical ensembles, such as choirs (a group of singers), marching bands, concert bands, jazz bands, or orchestras. In some school systems, music lessons on how to play instruments may be provided. Some students also take private music lessons after school with a singing teacher or instrument teacher. Amateur musicians typically learn basic musical rudiments (e.g., learning about musical notation for musical scales and rhythms) and beginner- to intermediate-level singing or instrument-playing techniques.

At the university level, students in most arts and humanities programs can receive credit for taking a few music courses, which typically take the form of an overview course on the history of music, or a music appreciation course that focuses on listening to music and learning about different musical styles. In addition, most North American and European universities have some types of musical ensembles that students in arts and humanities are able to participate in, such as choirs, marching bands, concert bands, or orchestras. The study of Western art music is increasingly common outside of North America and Europe, such as the Indonesian Institute of the Arts in Yogyakarta, Indonesia, or the classical music programs that are available in Asian countries such as South Korea, Japan, and China. At the same time, Western universities and colleges are widening their curriculum to include music of non-Western cultures, such as the music of Africa or Bali (e.g. Gamelan music).

Individuals aiming to become professional musicians, singers, composers, songwriters, music teachers and practitioners of other music-related professions such as music history professors, sound engineers, and so on study in specialized post-secondary programs offered by colleges, universities and music conservatories. Some institutions that train individuals for careers in music offer training in a wide range of professions, as is the case with many of the top U.S. universities, which offer degrees in music performance (including singing and playing instruments), music history, music theory, music composition, music education (for individuals aiming to become elementary or high school music teachers) and, in some cases, conducting. On the other hand, some small colleges may only offer training in a single profession (e.g., sound recording).

While most university and conservatory music programs focus on training students in classical music, there are a number of universities and colleges that train musicians for careers as jazz or popular music musicians and composers, with notable U.S. examples including the Manhattan School of Music and the Berklee College of Music. Two important schools in Canada which offer professional jazz training are McGill University and Humber College. Individuals aiming at careers in some types of music, such as heavy metal music, country music or blues are less likely to become professionals by completing degrees or diplomas in colleges or universities. Instead, they typically learn about their style of music by singing and/or playing in many bands (often beginning in amateur bands, cover bands and tribute bands), studying recordings available on CD, DVD and the Internet and working with already-established professionals in their style of music, either through informal mentoring or regular music lessons. Since the 2000s, the increasing popularity and availability of Internet forums and YouTube "how-to" videos have enabled many singers and musicians from metal, blues and similar genres to improve their skills. Many pop, rock and country singers train informally with vocal coaches and singing teachers.

Undergraduate university degrees in music, including the Bachelor of Music, the Bachelor of Music Education, and the Bachelor of Arts (with a major in music) typically take about four years to complete. These degrees provide students with a grounding in music theory and music history, and many students also study an instrument or learn singing technique as part of their program. Graduates of undergraduate music programs can seek employment or go on to further study in music graduate programs. Bachelor's degree graduates are also eligible to apply to some graduate programs and professional schools outside of music (e.g., public administration, business administration, library science, and, in some jurisdictions, teacher's college, law school or medical school).

Graduate music degrees include the Master of Music, the Master of Arts (in musicology, music theory or another music field), the Doctor of Philosophy (Ph.D.) (e.g., in musicology or music theory), and more recently, the Doctor of Musical Arts, or DMA. The Master of Music degree, which takes one to two years to complete, is typically awarded to students studying the performance of an instrument, education, voice (singing) or composition. The Master of Arts degree, which takes one to two years to complete and often requires a thesis, is typically awarded to students studying musicology, music history, music theory or ethnomusicology.

The PhD, which is required for students who want to work as university professors in musicology, music history, or music theory, takes three to five years of study after the master's degree, during which time the student will complete advanced courses and undertake research for a dissertation. The DMA is a relatively new degree that was created to provide a credential for professional performers or composers that want to work as university professors in musical performance or composition. The DMA takes three to five years after a master's degree, and includes advanced courses, projects, and performances. In Medieval times, the study of music was one of the Quadrivium of the seven Liberal Arts and considered vital to higher learning. Within the quantitative Quadrivium, music, or more accurately harmonics, was the study of rational proportions.

Musicology, the academic study of the subject of music, is studied in universities and music conservatories. The earliest definitions from the 19th century defined three sub-disciplines of musicology: systematic musicology, historical musicology, and comparative musicology or ethnomusicology. In 2010-era scholarship, one is more likely to encounter a division of the discipline into music theory, music history, and ethnomusicology. Research in musicology has often been enriched by cross-disciplinary work, for example in the field of psychoacoustics. The study of music of non-Western cultures, and the cultural study of music, is called ethnomusicology. Students can pursue the undergraduate study of musicology, ethnomusicology, music history, and music theory through several different types of degrees, including bachelor's degrees, master's degrees and PhD degrees.

Music theory is the study of music, generally in a highly technical manner outside of other disciplines. More broadly it refers to any study of music, usually related in some form with compositional concerns, and may include mathematics, physics, and anthropology. What is most commonly taught in beginning music theory classes are guidelines to write in the style of the common practice period, or tonal music. Theory, even of music of the common practice period, may take many other forms. Musical set theory is the application of mathematical set theory to music, first applied to atonal music. "Speculative music theory", contrasted with "analytic music theory", is devoted to the analysis and synthesis of music materials, for example tuning systems, generally as preparation for composition.

Zoomusicology is the study of the music of non-human animals, or the musical aspects of sounds produced by non-human animals. As George Herzog (1941) asked, "do animals have music?" François-Bernard Mâche's "Musique, mythe, nature, ou les Dauphins d'Arion" (1983), a study of "ornitho-musicology" using a technique of Nicolas Ruwet's "Langage, musique, poésie" (1972) paradigmatic segmentation analysis, shows that bird songs are organised according to a repetition-transformation principle. Jean-Jacques Nattiez (1990), argues that "in the last analysis, it is a human being who decides what is and is not musical, even when the sound is not of human origin. If we acknowledge that sound is not organised and conceptualised (that is, made to form music) merely by its producer, but by the mind that perceives it, then music is uniquely human."

In the West, much of the history of music that is taught deals with the Western civilization's art music, which is known as classical music. The history of music in non-Western cultures ("world music" or the field of "ethnomusicology"), which typically covers music from
Africa and Asia is also taught in Western universities. This includes the documented classical traditions of Asian countries outside the influence of Western Europe, as well as the folk or indigenous music of various other cultures. Popular or folk styles of music in non-Western countries varied widely from culture to culture, and from period to period. Different cultures emphasised different instruments, techniques, singing styles and uses for music. Music has been used for entertainment, ceremonies, rituals, religious purposes and for practical and artistic communication. Non-Western music has also been used for propaganda purposes, as was the case with Chinese opera during the Cultural Revolution.

There is a host of music classifications for non-Western music, many of which are caught up in the argument over the definition of music. Among the largest of these is the division between classical music (or "art" music), and popular music (or commercial music – including non-Western styles of rock, country, and pop music-related styles). Some genres do not fit neatly into one of these "big two" classifications, (such as folk music, world music, or jazz-related music).

As world cultures have come into greater global contact, their indigenous musical styles have often merged with other styles, which produces new styles. For example, the United States bluegrass style contains elements from Anglo-Irish, Scottish, Irish, German and African instrumental and vocal traditions, which were able to fuse in the United States' multi-ethnic "melting pot" society. Some types of world music contain a mixture of non-Western indigenous styles with Western pop music elements. Genres of music are determined as much by tradition and presentation as by the actual music. Some works, like George Gershwin's "Rhapsody in Blue", are claimed by both jazz and classical music, while Gershwin's "Porgy and Bess" and Leonard Bernstein's "West Side Story" are claimed by both opera and the Broadway musical tradition. Many current music festivals for non-Western music include bands and singers from a particular musical genre, such as world music.

Indian music, for example, is one of the oldest and longest living types of music, and is still widely heard and performed in South Asia, as well as internationally (especially since the 1960s). Indian music has mainly three forms of classical music, Hindustani, Carnatic, and Dhrupad styles. It has also a large repertoire of styles, which involve only percussion music such as the talavadya performances famous in South India.

Music therapy is an interpersonal process in which a trained therapist uses music and all of its facets—physical, emotional, mental, social, aesthetic, and spiritual—to help clients to improve or maintain their health. In some instances, the client's needs are addressed directly through music; in others they are addressed through the relationships that develop between the client and therapist. Music therapy is used with individuals of all ages and with a variety of conditions, including: psychiatric disorders, medical problems, physical disabilities, sensory impairments, developmental disabilities, substance abuse issues, communication disorders, interpersonal problems, and aging. It is also used to improve learning, build self-esteem, reduce stress, support physical exercise, and facilitate a host of other health-related activities. Music therapists may encourage clients to sing, play instruments, create songs, or do other musical activities.

One of the earliest mentions of music therapy was in Al-Farabi's (c. 872–950) treatise "Meanings of the Intellect", which described the therapeutic effects of music on the soul. Music has long been used to help people deal with their emotions. In the 17th century, the scholar Robert Burton's "The Anatomy of Melancholy" argued that music and dance were critical in treating mental illness, especially melancholia. He noted that music has an "excellent power ...to expel many other diseases" and he called it "a sovereign remedy against despair and melancholy." He pointed out that in Antiquity, Canus, a Rhodian fiddler, used music to "make a melancholy man merry, ...a lover more enamoured, a religious man more devout." In the Ottoman Empire, mental illnesses were treated with music. In November 2006, Dr. Michael J. Crawford and his colleagues also found that music therapy helped schizophrenic patients.

Albert Einstein had a lifelong love of music (particularly the works of Bach and Mozart), once stating that life without playing music would be inconceivable to him. In some interviews Einstein even attributed much of his scientific intuition to music, with his son Hans recounting that "whenever he felt that he had come to the end of the road or into a difficult situation in his work, he would take refuge in music, and that would usually resolve all his difficulties." Something in the music, according to Michele and Robert Root-Bernstein in "Psychology Today", "would guide his thoughts in new and creative directions." It has been said that Einstein considered Mozart's music to reveal a universal harmony that Einstein believed existed in the universe, "as if the great Wolfgang Amadeus did not 'create' his beautifully clear music at all, but simply discovered it already made. This perspective parallels, remarkably, Einstein’s views on the ultimate simplicity of nature and its explanation and statement via essentially simple mathematical expressions." A review suggests that music may be effective for improving subjective sleep quality in adults with insomnia symptoms.




</doc>
<doc id="18842" url="https://en.wikipedia.org/wiki?curid=18842" title="Mode">
Mode

Mode ( meaning "manner, tune, measure, due measure, rhythm, melody") may refer to:











</doc>
<doc id="18845" url="https://en.wikipedia.org/wiki?curid=18845" title="Mouse">
Mouse

A mouse ("Mus"), plural mice, is a small rodent characteristically having a pointed snout, small rounded ears, a body-length scaly tail and a high breeding rate. The best known mouse species is the common house mouse ("Mus musculus"). It is also a popular pet. In some places, certain kinds of field mice are locally common. They are known to invade homes for food and shelter.

Domestic mice sold as pets often differ substantially in size from the common house mouse. This is attributable both to breeding and to different conditions in the wild. The most well known strain, the white lab mouse, has more uniform traits that are appropriate to its use in research.

The American white-footed mouse ("Peromyscus leucopus") and the "deer mouse" ("Peromyscus maniculatus"), as well as other common species of mouse-like rodents around the world, also sometimes live in houses. These, however, are in other genera.

Cats, wild dogs, foxes, birds of prey, snakes and even certain kinds of arthropods have been known to prey heavily upon mice. Nevertheless, because of its remarkable adaptability to almost any environment, the mouse is one of the most successful mammalian genera living on Earth today.

Mice, in certain contexts, can be considered vermin which are a major source of crop damage, causing structural damage and spreading diseases through their parasites and feces. In North America, breathing dust that has come in contact with mouse excrement has been linked to hantavirus, which may lead to hantavirus pulmonary syndrome (HPS).

Primarily nocturnal animals, mice compensate for their poor eyesight with a keen sense of hearing, and rely especially on their sense of smell to locate food and avoid predators.

Mice build long intricate burrows in the wild. These typically have long entrances and are equipped with escape tunnels or routes. In at least one species, the architectural design of a burrow is a genetic trait.

Breeding onset is at about 50 days of age in both females and males, although females may have their first estrus at 25–40 days. Mice are polyestrous and breed year round; ovulation is spontaneous. The duration of the estrous cycle is 4–5 days and estrus itself lasts about 12 hours, occurring in the evening. Vaginal smears are useful in timed matings to determine the stage of the estrous cycle. Mating is usually nocturnal and may be confirmed by the presence of a copulatory plug in the vagina up to 24 hours post-copulation. The presence of sperm on a vaginal smear is also a reliable indicator of mating.

Female mice housed together tend to go into anestrus and do not cycle. If exposed to a male mouse or the pheromones of a male mouse, most of the females will go into estrus in about 72 hours. This synchronization of the estrous cycle is known as the Whitten effect. The exposure of a recently bred mouse to the pheromones of a strange male mouse may prevent implantation (or pseudopregnancy), a phenomenon known as the Bruce effect.

The average gestation period is 20 days. A fertile postpartum estrus occurs 14–24 hours following parturition, and simultaneous lactation and gestation prolongs gestation 3–10 days owing to delayed implantation. The average litter size is 10–12 during optimum production, but is highly strain-dependent. As a general rule, inbred mice tend to have longer gestation periods and smaller litters than outbred and hybrid mice. The young are called pups and weigh at birth, are hairless, and have closed eyelids and ears. Cannibalism is uncommon, but females should not be disturbed during parturition and for at least 2 days postpartum. Pups are weaned at 3 weeks of age; weaning weight is . If the postpartum estrus is not utilized, the female resumes cycling 2–5 days post-weaning.

Newborn male mice are distinguished from newborn females by noting the greater anogenital distance and larger genital papilla in the male. This is best accomplished by lifting the tails of littermates and comparing perineums.

Mice are common experimental animals in laboratory research of biology and psychology fields primarily because they are mammals, and also because they share a high degree of homology with humans. They are the most commonly used mammalian model organism, more common than rats. The mouse genome has been sequenced, and virtually all mouse genes have human homologs. The mouse has approximately 2.7 billion base pairs and 20 chromosomes.
They can also be manipulated in ways that are illegal with humans, although animal rights activists often object. A knockout mouse is a genetically modified mouse that has had one or more of its genes made inoperable through a gene knockout.

Reasons for common selection of mice are small size, inexpensive, widely varied diet, easily maintained, and can reproduce quickly. Several generations of mice can be observed in a relatively short time. Mice are generally very docile if raised from birth and given sufficient human contact. However, certain strains have been known to be quite temperamental. Mice and rats have the same organs in the same places, with the difference of size.

All members of the genus "Mus" are referred to as mice. However, the term "mouse" can also be applied to species outside of this genus. "Mouse" often refers to any small muroid rodent, while "rat" refers to larger muroid rodents. Therefore, these terms are not taxonomically specific.

The following is a list of "Mus" subgenera, species, and subspecies:

Many people buy mice as companion pets. They can be playful, loving and can grow used to being handled. Like pet rats, pet mice should not be left unsupervised outside as they have many natural predators, including (but not limited to) birds, snakes, lizards, cats, and dogs. Male mice tend to have a stronger odor than the females. However, mice are careful groomers and as pets they never need bathing. Well looked-after mice can make ideal pets. Some common mouse care products are:

In nature, mice are largely herbivores, consuming any kind of fruit or grain from plants. However, mice adapt well to urban areas and are known for eating almost all types of food scraps. In captivity, mice are commonly fed commercial pelleted mouse diet. These diets are nutritionally complete, but they still need a large variety of vegetables. Food intake is approximately per of body weight per day; water intake is approximately per 100 g of body weight per day.

Mice are a staple in the diet of many small carnivores. Humans have eaten mice since prehistoric times and still eat them as a delicacy throughout eastern Zambia and northern Malawi, where they are a seasonal source of protein. Mice are no longer routinely consumed by humans elsewhere. However, in Victorian Britain, fried mice were still given to children as a folk remedy for bed-wetting; while Jared Diamond reports creamed mice being used in England as a dietary supplement during W. W. II rationing.

Prescribed cures in Ancient Egypt included mice as medicine. In Ancient Egypt, when infants were ill, mice were eaten as treatment by their mothers. It was believed that mouse eating by the mother would help heal the baby who was ill.

In various countries mice are used as food for pets such as snakes, lizards, frogs, tarantulas and birds of prey, and many pet stores carry mice for this purpose.

Common terms used to refer to different ages/sizes of mice when sold for pet food are "pinkies", "fuzzies", "crawlers", "hoppers", and "adults". Pinkies are newborn mice that have not yet grown fur; fuzzies have some fur but are not very mobile; hoppers have a full coat of hair and are fully mobile but are smaller than adult mice. Mice without fur are easier for the animal to consume; however, mice with fur may be more convincing as animal feed. These terms are also used to refer to the various growth stages of rats (see Fancy rat).




</doc>
<doc id="18847" url="https://en.wikipedia.org/wiki?curid=18847" title="Multics">
Multics

Multics (Multiplexed Information and Computing Service) is an influential early time-sharing operating system, based around the concept of a single-level memory. Virtually all modern operating systems were heavily influenced by Multics – often through Unix, which was created by some of the people who had worked on Multics – either directly (Linux, macOS) or indirectly (Windows NT).

Initial planning and development for Multics started in 1964, in Cambridge, Massachusetts. Originally it was a cooperative project led by MIT (Project MAC with Fernando Corbató) along with General Electric and Bell Labs. It was developed on the first GE 645 computer delivered to MIT in 1965.

Multics was conceived as a commercial product for General Electric, and became one for Honeywell, albeit not very successfully. Due to its many novel and valuable ideas, Multics had a significant impact on computer science despite its faults.

Multics had numerous features intended to ensure high availability so that it would support a computing utility similar to the telephone and electricity utilities. Modular hardware structure and software architecture were used to achieve this. The system could grow in size by simply adding more of the appropriate resource, be it computing power, main memory, or disk storage. Separate access control lists on every file provided flexible information sharing, but complete privacy when needed. Multics had a number of standard mechanisms to allow engineers to analyze the performance of the system, as well as a number of adaptive performance optimization mechanisms.
Multics implemented a single-level store for data access, discarding the clear distinction between files (called "segments" in Multics) and "process memory". The memory of a process consisted solely of segments that were mapped into its address space. To read or write to them, the process simply used normal central processing unit (CPU) instructions, and the operating system took care of making sure that all the modifications were saved to disk. In POSIX terminology, it was as if every file were codice_1ed; however, in Multics there was no concept of "process memory", separate from the memory used to hold mapped-in files, as Unix has. "All" memory in the system was part of "some" segment, which appeared in the file system; this included the temporary scratch memory of the process, its kernel stack, etc.

One disadvantage of this was that the size of segments was limited to 256 kilowords, just over 1 MiB. This was due to the particular hardware architecture of the machines on which Multics ran, having a 36-bit word size and index registers (used to address within segments) of half that size (18 bits). Extra code had to be used to work on files larger than this, called multisegment files. In the days when one megabyte of memory was prohibitively expensive, and before large databases and later huge bitmap graphics, this limit was rarely encountered.

Another major new idea of Multics was dynamic linking, in which a running process could request that other segments be added to its address space, segments which could contain code that it could then execute. This allowed applications to automatically use the latest version of any external routine they called, since those routines were kept in other segments, which were dynamically linked only when a process first tried to begin execution in them. Since different processes could use different search rules, different users could end up using different versions of external routines automatically. Equally importantly, with the appropriate settings on the Multics security facilities, the code in the other segment could then gain access to data structures maintained in a different process.

Thus, to interact with an application running in part as a daemon (in another process), a user's process simply performed a normal procedure-call instruction to a code segment to which it had dynamically linked (a code segment that implemented some operation associated with the daemon). The code in that segment could then modify data maintained and used in the daemon. When the action necessary to commence the request was completed, a simple procedure return instruction returned control of the user's process to the user's code.

The single-level store and dynamic linking are still not available to their full power in other widely used operating systems, despite the rapid and enormous advance in the computer field since the 1960s. They are becoming more widely accepted and available in more limited forms, for example, dynamic linking.

Multics also supported extremely aggressive on-line reconfiguration: central processing units, memory banks, disk drives, etc. could be added and removed while the system continued operating. At the MIT system, where most early software development was done, it was common practice to split the multiprocessor system into two separate systems during off-hours by incrementally removing enough components to form a second working system, leaving the rest still running the original logged-in users. System software development testing could be done on the second system, then the components of the second system were added back to the main user system, without ever having shut it down. Multics supported multiple CPUs; it was one of the earliest multiprocessor systems.

Multics was the first major operating system to be designed as a secure system from the outset. Despite this, early versions of Multics were broken into repeatedly. This led to further work that made the system much more secure and prefigured modern security engineering techniques. Break-ins became very rare once the second-generation hardware base was adopted; it had hardware support for ring-oriented security, a multilevel refinement of the concept of master mode.

Multics was the first operating system to provide a hierarchical file system, and file names could be of almost arbitrary length and syntax. A given file or directory could have multiple names (typically a long and short form), and symbolic links between directories were also supported. Multics was the first to use the now-standard concept of per-process stacks in the kernel, with a separate stack for each security ring. It was also the first to have a command processor implemented as ordinary user code – an idea later used in the Unix shell. It was also one of the first written in a high-level language (Multics PL/I), after the Burroughs MCP system written in ALGOL.

In 1964, Multics was developed initially for the GE-645 mainframe, a 36-bit system. GE's computer business, including Multics, was taken over by Honeywell in 1970; around 1973, Multics was supported on the Honeywell 6180 machines, which included security improvements including hardware support for protection rings.

Bell Labs pulled out of the project in 1969; some of the people who had worked on it there went on to create the Unix system. Multics development continued at MIT and General Electric.

Honeywell continued system development until 1985. About 80 multimillion-dollar sites were installed, at universities, industry, and government sites. The French university system had several installations in the early 1980s. After Honeywell stopped supporting Multics, users migrated to other systems like Unix.

In 1985, Multics was issued certification as a B2 level secure operating system using the Trusted Computer System Evaluation Criteria from the National Computer Security Center (NCSC) a division of the NSA, the first operating system evaluated to this level.

Multics was distributed from 1975 to 2000 by Groupe Bull in Europe, and by Bull HN Information Systems Inc. in the United States. In 2006, Bull SAS open sourced Multics versions MR10.2, MR11.0, MR12.0, MR12.1, MR12.2, MR12.3, MR12.4 & MR12.5.

The last known Multics installation running natively on Honeywell hardware was shut down on October 30, 2000, at the Canadian Department of National Defence in Halifax, Nova Scotia, Canada.

In 2006 Bull HN released the source code for MR12.5, the final 1992 Multics release, to MIT. Most of the system is now available as open-source software with the exception of some optional pieces such as TCP/IP.

In 2014 Multics was successfully run on current hardware using a simulator. The 1.0 release of the simulator is now available. Release 12.6f of Multics accompanies the 1.0 release of the emulator, and adds a few new features, including command line recall and editing using the video system.

Dr. Peter H. Salus, author of the definitive history of Unix's early years stated one position: "With Multics they tried to have a much more versatile and flexible operating system, and it failed miserably." This position, however, has been widely discredited in the computing community as many of the technical innovations of the Multics project have found their way into modern commercial computing systems.

The permanently resident kernel of this powerful multiprocessor mainframe "computing utility", much derided in its day as being too large and complex, was only 135 KB of code. In comparison, a Linux system in 2007 might have occupied 18 MB. The first MIT GE-645 had 512 kilowords of memory (2 MiB), a truly enormous amount at the time, and the kernel used only a moderate portion of Multics main memory.

The entire system, including the operating system and the complex PL/1 compiler, user commands, and subroutine libraries, consisted of about 1500 source modules. These averaged roughly 200 lines of source code each, and compiled to produce a total of roughly 4.5 MiB of procedure code, which was fairly large by the standards of the day.

Multics compilers generally optimised more for code density than CPU performance, for example using small sub-routines called "operators" for short standard code-sequences, making direct comparison of object code size with more modern systems less useful. High code density was a good optimisation choice for a multi-user system with expensive main memory, such as Multics.

The design and features of Multics greatly influenced the Unix operating system, which was originally written by two Multics programmers, Ken Thompson and Dennis Ritchie. Superficial influence of Multics on Unix is evident in many areas, including the naming of some commands. But the internal design philosophy was quite different, focusing on keeping the system small and simple, and so correcting some deficiencies of Multics because of its high resource demands on the limited computer hardware of the time.

The name "Unix" (originally "Unics") is itself a pun on "Multics". The "U" in Unix is rumored to stand for "uniplexed" as opposed to the "multiplexed" of Multics, further underscoring the designers' rejections of Multics' complexity in favor of a more straightforward and workable approach for smaller computers. (Garfinkel and Abelson cite an alternative origin: Peter Neumann at Bell Labs, watching a demonstration of the prototype, suggested the pun name UNICS – pronounced "eunuchs" – as a "castrated Multics", although Dennis Ritchie is claimed to have denied this.)

Ken Thompson, in a transcribed 2007 interview with Peter Seibel refers to Multics as "…overdesigned and overbuilt and over everything. It was close to unusable. They [Massachusetts Institute of Technology] still claim it’s a monstrous success, but it just clearly wasn't." He admits, however, that "the things that I liked enough (about Multics) to actually take were the hierarchical file system and the shell—a separate process that you can replace with some other process."

The Prime Computer operating system, PRIMOS, was referred to as "Multics in a shoebox" by William Poduska, a founder of the company. Poduska later moved on to found Apollo Computer, whose AEGIS and later Domain/OS operating systems, sometimes called "Multics in a matchbox", extended the Multics design to a heavily networked graphics workstation environment.

The Stratus VOS operating system of Stratus Computer (now Stratus Technologies) was very strongly influenced by Multics, and both its external user interface and internal structure bear many close resemblances to the older project. The high-reliability, availability, and security features of Multics were extended in Stratus VOS to support a new line of fault tolerant computer systems supporting secure, reliable transaction processing. Stratus VOS is the most directly-related descendant of Multics still in active development and production usage today.

The protection architecture of Multics, restricting the ability of code at one level of the system to access resources at another, was adopted as the basis for the security features of ICL's VME operating system.


The literature contains a large number of papers about Multics, and various components of it; a fairly complete list is available at the Multics Bibliography page. The most important and/or informative ones are listed below.





</doc>
<doc id="18849" url="https://en.wikipedia.org/wiki?curid=18849" title="Marxist film theory">
Marxist film theory

Marxist film theory is one of the oldest forms of film theory. 

Sergei Eisenstein and many other Soviet filmmakers in the 1920s expressed ideas of Marxism through film. In fact, the Hegelian dialectic was considered best displayed in film editing through the Kuleshov Experiment and the development of montage.

While this structuralist approach to Marxism and filmmaking was used, the more vociferous complaint that the Russian filmmakers had was with the narrative structure of the cinema of the United States.

Eisenstein's solution was to shun narrative structure by eliminating the individual protagonist and tell stories where the action is moved by the group and the story is told through a clash of one image against the next (whether in composition, motion, or idea) so that the audience is never lulled into believing that they are watching something that has not been worked over.

Eisenstein himself, however, was accused by the Soviet authorities under Joseph Stalin of "formalist error," of highlighting form as a thing of beauty instead of portraying the worker nobly.

French Marxist film makers, such as Jean-Luc Godard, would employ radical editing and choice of subject matter, as well as subversive parody, to heighten class consciousness and promote Marxist ideas.

Situationist film maker Guy Debord, author of "The Society of the Spectacle", began his film "In girum imus nocte et consumimur igni" [Wandering around in the night we are consumed by fire] with a radical critique of the spectator who goes to the cinema to forget about his dispossessed daily life.

Situationist film makers produced a number of important films, where the only contribution by the situationist film cooperative was the sound-track. In "Can dialectics break bricks?" (1973) a Chinese Kung Fu film was transformed by redubbing into an epistle on state capitalism and Proletarian revolution. The intellectual technique of using capitalism's own structures against itself is known as détournement. 

Marxist film theory has developed from these precise and historical beginnings and is now sometimes viewed in a wider way to refer to any power relationships or structures within a moving image text. 




</doc>
<doc id="18851" url="https://en.wikipedia.org/wiki?curid=18851" title="Mars (disambiguation)">
Mars (disambiguation)

Mars is a planet in the Solar System.

Mars also commonly refers to:

Mars may also refer to:
























</doc>
<doc id="18852" url="https://en.wikipedia.org/wiki?curid=18852" title="Morpheme">
Morpheme

A morpheme is the smallest grammatical unit in a language. A morpheme is not identical to a word, and the principal difference between the two is that a morpheme may or may not stand alone, whereas a word, by definition, is freestanding. The linguistics field of study dedicated to morphemes is called morphology. When a morpheme stands by itself, it is considered as a root because it has a meaning of its own (e.g. the morpheme "cat") and when it depends on another morpheme to express an idea, it is an affix because it has a grammatical function (e.g. the "–s" in "cats" to indicate that it is plural). Every word comprises one or more morphemes.

Every morpheme can be classified as either free or bound. These categories are mutually exclusive, and as such, a given morpheme will belong to exactly one of them.

Bound morphemes can be further classified as derivational or inflectional.



Allomorphs are variants of a morpheme that differ in pronunciation but are semantically identical. For example, in English, the plural marker "-(e)s" of regular nouns can be pronounced ("bats"), , ("bugs"), or , ("buses"), depending on the final sound of the noun's plural form.

Generally, these types of morphemes have no visible changes. For instance, the singular form of sheep is "sheep" and its plural is also "sheep". The intended meaning is thus derived from the co-occurring determiner (e.g. in this case "some-" or "a-").

Content morphemes express a concrete meaning or "content", while function morphemes have more of a grammatical role. For example, the morphemes "fast" and "sad" can be considered content morphemes. On the other hand, the suffix "–ed" belongs to the function morphemes given that it has the grammatical function of indicating past tense. Although these categories seem very clear and intuitive, the idea behind it can be harder to grasp given that they overlap with each other. Examples of an ambiguous situation are the preposition "over" and the determiner "your", which seem to have a concrete meaning, but are considered function morphemes because their role is to connect ideas grammatically. A general rule to follow to determine the category of a morpheme is:


Roots are composed of only one morpheme, while stems can be composed of more than one morpheme. Any additional affixes are considered morphemes. An example of this is the word "quirkiness". The root is "quirk", but the stem is "quirky" which has two morphemes. Moreover, there exist pairs of affixes that have the same phonological form but have a different meaning. For example, the suffix "–er" can be derivative (e.g. "sell" ⇒ "seller") or inflectional (e.g. "small" ⇒ "smaller"). These types of morphemes are called homophonous.

Some words might seem to be composed of multiple morphemes, but in fact, they are not. This is why one has to consider form and meaning when identifying morphemes. For example, the word "relate" might seem to be composed of two morphemes, "re-" (prefix) and the word "late", but this is not correct. These morphemes have no relationship with the definitions relevant to the word like “feel sympathy”, “narrate”, or “being connected by blood or marriage”. Furthermore, the length of the words does not determine if it has multiple morphemes or not. To demonstrate, the word "Madagascar" is long and it might seem to have morphemes like "mad", "gas", and "car", but it does not. Conversely, small words can have multiple morphemes (e.g. "dogs").

In natural language processing for Korean, Japanese, Chinese and other languages, morphological analysis is the process of segmenting a sentence into a row of morphemes. Morphological analysis is closely related to part-of-speech tagging, but word segmentation is required for these languages because word boundaries are not indicated by blank spaces.

The purpose of the morphological analysis is to determine the minimal units of meaning in a language or morphemes by using comparisons of similar forms—for example, comparing forms such as “She is walking” and “They are walking” rather than comparing either of these with something completely different like "You are reading". Thus, we can effectively break down the forms in parts and distinguish the different morphemes. Similarly, the meaning and the form are equally important for the identification of morphemes. For instance, agent and comparative morphemes illustrate this point. An agent morpheme is an affix like "-er" that transforms a verb into a noun (e.g. "teach" → "teacher"). On the other hand, "–er" can also be a comparative morpheme that changes an adjective into another degree of the same adjective (e.g. "small" → "smaller"). In this case, the form is the same, but the meaning of both morphemes is different. Also, the opposite can occur in which the meaning is the same but the form is different.

In generative grammar, the definition of a morpheme depends heavily on whether syntactic trees have morphemes as leaves or features as leaves.

Given the definition of a morpheme as "the smallest meaningful unit", nanosyntax aims to account for idioms where it is often an entire syntactic tree which contributes "the smallest meaningful unit." An example idiom is "Don't let the cat out of the bag" where the idiom is composed of "let the cat out of the bag" and that might be considered a semantic morpheme, which is composed of many syntactic morphemes. Other cases where the "smallest meaningful unit" is larger than a word include some collocations such as "in view of" and "business intelligence" where the words together have a specific meaning.

The definition of morphemes also plays a significant role in the interfaces of generative grammar in the following theoretical constructs;





</doc>
<doc id="18856" url="https://en.wikipedia.org/wiki?curid=18856" title="MTV">
MTV

MTV (originally an initialism of Music Television) is an American pay television channel owned by Viacom Media Networks (a division of Viacom) and headquartered in New York City.

Launched on August 1, 1981, the channel originally aired music videos as guided by television personalities known as "video jockeys" (VJs). At first, MTV's main target demographic was young adults, but today it is primarily teenagers, particularly high school and college students. MTV has toned down its music video programming significantly in recent years, and its programming now consists mainly of original reality, comedy and drama programming and some off-network syndicated programs and films, with limited music video programming in off-peak time periods. 

In recent years, MTV had struggled with the secular decline of music-related subscription-based media. Its ratings had been said to be failing systematically, as younger viewers increasingly shift towards other media platforms, with yearly ratings drops as high as 29%; thus there was doubt of the lasting relevance of MTV towards young audiences. In April 2016, newly appointed MTV president Sean Atkins announced plans to restore music programming to the channel, including the return of "MTV Unplugged". After nine years off air, "TRL", MTV's flagship series, returned on October 2, 2017.

MTV has spawned numerous sister channels in the US and affiliated channels internationally, some of which have gone independent, with approximately 90.6 million American households in the United States receiving MTV as of January 2016.

Several earlier concepts for music video-based television programming had been around since the early 1960s. The Beatles had used music videos to promote their records starting in the mid-1960s. The creative use of music videos within their 1964 film "A Hard Day's Night", particularly the performance of the song "Can't Buy Me Love", led MTV later on June 26, 1999, to honor the film's director Richard Lester with an award for "basically inventing the music video".

In his book "The Mason Williams FCC Rapport", author Mason Williams states that he pitched an idea to CBS for a television program that featured "video-radio", where disc jockeys would play avant-garde art pieces set to music. CBS rejected the idea, but Williams premiered his own musical composition "Classical Gas" on the "Smothers Brothers Comedy Hour", where he was head writer. In 1970, Philadelphia-based disc jockey Bob Whitney created "The Now Explosion", a television series filmed in Atlanta and broadcast in syndication to other local television stations throughout the United States. The series featured promotional clips from various popular artists, but was canceled by its distributor in 1971. Several music programs originating outside of the US, including Australia's "Countdown" and the United Kingdom's "Top of the Pops", which had initially aired music videos in lieu of performances from artists who were not available to perform live, began to feature them regularly by the mid-1970s.

In 1974, Gary Van Haas, vice president of Televak Corporation, introduced a concept to distribute a music video channel to record stores across the United States, and promoted the channel, named Music Video TV, to distributors and retailers in a May 1974 issue of "Billboard". The channel, which featured video disc jockeys, signed a deal with US Cable in 1978 to expand its audience from retail to cable television. The service was no longer active by the time MTV launched in 1981.

In 1977, Warner Cable a division of Warner Communications and the precursor of Warner-Amex Satellite Entertainment launched the first two-way interactive cable television system named QUBE in Columbus, Ohio. The QUBE system offered many specialized channels. One of these specialized channels was Sight on Sound, a music channel that featured concert footage and music-oriented television programs. With the interactive QUBE service, viewers could vote for their favorite songs and artists.

The original programming format of MTV was created by media executive Robert W. Pittman, who later became president and chief executive officer (CEO) of MTV Networks. Pittman had test-driven the music format by producing and hosting a 15-minute show, "Album Tracks", on New York City television station WNBC-TV in the late 1970s.

Pittman's boss Warner-Amex executive vice president John Lack had shepherded "PopClips", a television series created by former Monkee-turned solo artist Michael Nesmith, whose attention had turned to the music video format in the late 1970s. The inspiration for "PopClips" came from a similar program on New Zealand's TVNZ network named "Radio with Pictures", which premiered in 1976. The concept itself had been in the works since 1966, when major record companies began supplying the New Zealand Broadcasting Corporation with promotional music clips to play on the air at no charge. Few artists made the long trip to New Zealand to appear live.

On Saturday, August 1, 1981, at 12:01am Eastern Time, MTV launched with the words "Ladies and gentlemen, rock and roll," spoken by John Lack and played over footage of the first Space Shuttle launch countdown of "Columbia" (which took place earlier that year) and of the launch of Apollo 11. Those words were immediately followed by the original MTV theme song, a crunching rock tune composed by Jonathan Elias and John Petersen, playing over the American flag changed to show MTV's logo changing into various textures and designs. MTV producers Alan Goodman and Fred Seibert used this public domain footage as a concept; Seibert said that they had originally planned to use Neil Armstrong's "One small step" quote, but lawyers said that Armstrong owned his name and likeness and that he had refused, so the quote was replaced with a beeping sound. A shortened version of the shuttle launch ID ran at the top of every hour in various forms, from MTV's first day until it was pulled in early 1986 in the wake of the "Challenger" disaster.

The first music video shown on MTV was The Buggles' "Video Killed the Radio Star", originally only available to homes in New Jersey. This was followed by the video for Pat Benatar's "You Better Run". Sporadically, the screen would go black when an employee at MTV inserted a tape into a VCR. MTV's lower third graphics that appeared near the beginning and end of music videos would eventually use the recognizable Kabel typeface for about 25 years. But these graphics differed on MTV's first day of broadcast; they were set in a different typeface and included information such as the year and record label name.

As programming chief, Robert W. Pittman recruited and managed a team for the launch that included Tom Freston (who succeeded Pittman as CEO of MTV Networks), Fred Seibert, John Sykes, Carolyn Baker (original head of talent and acquisition), Marshall Cohen (original head of research), Gail Sparrow (of talent and acquisition), Sue Steinberg (executive producer), Julian Goldberg, Steve Lawrence, Geoff Bolton; studio producers and MTV News writers/associate producers Liz Nealon, Nancy LaPook and Robin Zorn; Steve Casey (creator of the name "MTV" and its first program director), Marcy Brafman, Ronald E. "Buzz" Brindle, and Robert Morton. Kenneth M. Miller is credited as being the first technical director to officially launch MTV from its New York City-based network operations facility.

MTV's effect was immediate in areas where the new music video channel was carried. Within two months, record stores in areas where MTV was available were selling music that local radio stations were not playing, such as Men at Work, Bow Wow Wow and the Human League. MTV sparked the Second British Invasion, with British acts, who had been accustomed to using music videos for half a decade, featuring heavily on the channel.

MTV targeted an audience between the ages of twelve to thirty-four. However, according to MTV's self conducted research over 50% of its audience is between twelve and twenty-four. Furthermore, this particular group would watch MTV for an average of thirty minutes to two hours a day. 

The original purpose of MTV was to be "music television", playing music videos 24 hours a day and seven days a week, guided by on-air personalities known as VJs, or video jockeys. The original slogans of the channel were "You'll never look at music the same way again", and "On cable. In stereo."

MTV's earliest format was modeled after AOR (album-oriented rock) radio; MTV would transition to mimic a full Top 40 station in 1984. Fresh-faced young men and women were hired to host the channel's programming and to introduce music videos that were being played. The term VJ was coined, which was a play on the initialism DJ (disc jockey). Many VJs eventually became celebrities in their own right. The original five MTV VJs in 1981 were Nina Blackwood, Mark Goodman, Alan Hunter, J.J. Jackson and Martha Quinn.

The VJs would record intro and outro voiceovers before broadcast, along with music news, interviews, concert dates and promotions. These segments would appear to air live and debut across the MTV program schedule 24 hours a day and seven days a week, although the segments themselves were pre-taped within a regular work week at MTV's studios.

The early music videos that made up the bulk of MTV's programming in the 1980s were promotional videos (or "promos", a term that originated in the United Kingdom) that record companies had commissioned for international use or concert clips from any available sources.

Rock bands and performers of the 1980s who had airplay on MTV ranged from new wave to hard rock or heavy metal bands such as Adam Ant, Bryan Adams, The Pretenders, Blondie, Eurythmics, Tom Petty and the Heartbreakers, Culture Club, Mötley Crüe, Split Enz, Prince, Ultravox, Duran Duran, Van Halen, Bon Jovi, RATT, Def Leppard, The Police, and The Cars. The channel also rotated the music videos of "Weird Al" Yankovic, who made a career out of parodying other artists' videos. MTV also aired several specials by "Weird Al" in the 1980s and 1990s under the title "Al TV".

MTV also played classic rock acts from the 1980s and earlier decades, including David Bowie, Dire Straits (whose 1985 song and video "Money for Nothing" both referenced MTV and also included the slogan "I want my MTV" in its lyrics), Journey, Rush, Linda Ronstadt, Genesis, Billy Squier, Aerosmith, The Rolling Stones, Eric Clapton, The Moody Blues, John Mellencamp, Daryl Hall & John Oates, Billy Joel, Robert Palmer, Rod Stewart, The Who, and ZZ Top; newly solo acts such as Peter Gabriel, Robert Plant, Phil Collins, Paul McCartney, David Lee Roth, and Pete Townshend; supergroup acts such as Asia, The Power Station, Yes, The Firm, and Traveling Wilburys, as well as forgotten acts such as Michael Stanley Band, Shoes, Blotto, Ph.D., Rockpile, Bootcamp, Silicon Teens and Taxxi. The hard rock band Kiss publicly appeared without their trademark makeup for the first time on MTV in 1983. The first country-music video aired on MTV was "Angel of the Morning" by Juice Newton, which first aired on MTV in 1981.

During the early days of the channel, MTV would occasionally let other stars take over the channel within an hour as "guest VJs". These guests included musicians such as Adam Ant, Billy Idol, Phil Collins, Simon LeBon, and Nick Rhodes of Duran Duran, Tina Turner; and comedians such as Eddie Murphy, Martin Short, Dan Aykroyd, and Steven Wright; as they chose their favorite music videos.

The 1983 film "Flashdance" was the first film in which its promoters excerpted musical segments from it and supplied them to MTV as music videos, which the channel then aired in regular rotation.

In addition to bringing lesser-known artists into view, MTV was instrumental in adding to the booming eighties dance wave. Videos' budgets increased, and artists began to add fully choreographed dance sections. Michael Jackson's music became synonymous with dance. In addition to learning the lyrics, fans also learned his choreography so they could dance along. Madonna capitalized on dance in her videos, using classically trained jazz and break-dancers. Along with extensive costuming and make-up, Duran Duran used tribal elements, pulled from Dunham technique, in "The Wild Boys", and Kate Bush used a modern dance duet in "Running Up That Hill". MTV brought more than music into public view, it added to the ever-growing resurgence of dance in the early 1980s that has carried through to today.

In 1984, more record companies and artists began making video clips for their music than in the past, realizing the popularity of MTV and the growing medium. In keeping with the influx of videos, MTV announced changes to its playlists in the November 3, 1984, issue of "Billboard" magazine, that would take effect the following week. The playlist categories would be expanded to seven, from three (light, medium, heavy); including New, Light, Breakout, Medium, Active, Heavy and Power. This would ensure artists with hit records on the charts would be get the exposure they deserved, with Medium being a home for the established hits still on the climb up to the top 10; and Heavy being a home for the big hitswithout the bells and whistlesjust the exposure they commanded.

In 1985, MTV spearheaded a safe-sex initiative as a response to the AIDS epidemic that continues to influence sexual health currently. In this light, MTV pushed teens to pay more attention to safe-sex because they were most likely more willing to hear this message from MTV than their parents. This showed that MTV was not always influencing youth negatively. Even though in other aspects, MTV was provocative, they had this campaign to showcase their positive influence on youths and safe sexa campaign that still is alive today: "Its Your Sex Life".

During MTV's first few years on the air, very few black artists were included in rotation on the channel. The select few who were in MTV's rotation were Michael Jackson, Prince, Eddy Grant, Donna Summer, Joan Armatrading, Musical Youth, and Herbie Hancock. The very first people of color to perform on MTV was the British band The Specials, which featured an integrated line-up of white and black musicians and vocalists. The Specials' video "Rat Race" was played as the 58th video on the station's first day of broadcasting.

MTV rejected other black artists' videos, such as Rick James' "Super Freak", because they did not fit the channel's carefully selected album-oriented rock format at the time. The exclusion enraged James; he publicly advocated the addition of more black artists' videos on the channel. Rock legend David Bowie also questioned MTV's lack of black artists during an on-air interview with VJ Mark Goodman in 1983. MTV's original head of talent and acquisition, Carolyn B. Baker, who was black, had questioned why the definition of music had to be so narrow, as had a few others outside the network. "The party line at MTV was that we weren't playing black music because of the 'research, said Baker years later. "But the research was based on ignorance ... we were young, we were cutting edge. We didn't have to be on the cutting edge of racism." Nevertheless, it was Baker who had personally rejected Rick James' video for "Super Freak" "because there were half-naked women in it, and it was a piece of crap. As a black woman, I did not want that representing my people as the first black video on MTV."

The network's director of music programming, Buzz Brindle, told an interviewer in 2006, "MTV was originally designed to be a rock music channel. It was difficult for MTV to find African American artists whose music fit the channel's format that leaned toward rock at the outset." Writers Craig Marks and Rob Tannenbaum noted that the channel "aired videos by plenty of white artists who didn't play rock." Andrew Goodwin later wrote, "[MTV] denied racism, on the grounds that it merely followed the rules of the rock business." MTV senior executive vice president Les Garland complained decades later, "The worst thing was that 'racism' bullshit... there were hardly any videos being made by black artists. Record companies weren't funding them. "They" never got charged with racism." However, critics of that defense pointed out that record companies were not funding videos for black artists because they knew that they would have difficulty persuading MTV to play them.

Before 1983, Michael Jackson also struggled to receive airtime on MTV. To resolve the struggle and finally "break the color barrier", the president of CBS Records at the time, Walter Yetnikoff, denounced MTV in a strong, profane statement, threatening to take away MTV's ability to play any of the record label's music videos. However, Les Garland, then acquisitions head, said he decided to air Jackson's "Billie Jean" video without pressure from CBS. This was contradicted by CBS head of Business Affairs David Benjamin in Vanity Fair.

According to "The Austin Chronicle", Jackson's video for the song "Billie Jean" was "the video that broke the color barrier, even though the channel itself was responsible for erecting that barrier in the first place." But change was not immediate. "Billie Jean" was not added to MTV's "medium rotation" playlist (two to three airings per day) until after it had already reached #1 on the "Billboard" Hot 100 chart. In the final week of March, it was in "heavy rotation", one week before the MTV debut of Jackson's "Beat It" video. Prince's "Little Red Corvette" joined both videos in heavy rotation at the end of April. At the beginning of June, "Electric Avenue" by Eddy Grant would join "Billie Jean", which was still in heavy rotation until mid-June. At the end of August, "She Works Hard for the Money" by Donna Summer was in heavy rotation on the channel. Herbie Hancock's "Rockit" and Lionel Richie's "All Night Long" would be placed in heavy rotation at the end of October and the beginning of November respectively. In final week of November, Donna Summer's "Unconditional Love" would be in heavy rotation. When Jackson's elaborate video for "Thriller" was released late in the year, which raised the ambition bar for what a video could be, the network's support for it was total; subsequently, more pop and R&B videos were played on MTV.

Regardless of the timeline, many black artists had their videos played in "heavy" rotation the following year (1984). Along with Herbie Hancock, Prince, Donna Summer, other black artists such as Billy Ocean, Stevie Wonder, Tina Turner, Lionel Richie, Ray Parker Jr, Rockwell, The Pointer Sisters, The Jacksons, Sheila E and Deniece Williams all had videos played in heavy rotation on MTV.

Eventually, videos from the emerging genre of rap and hip hop would also begin to enter rotation on MTV. A majority of the rap artists appearing on MTV in the mid-1980s such as Run-DMC, The Fat Boys, Whodini, LL Cool J, and the Beastie Boys were from the East Coast.

In 1984, the channel produced its first "MTV Video Music Awards" show, or VMAs. The first award show, in 1984, was punctuated by a live performance by Madonna of "Like A Virgin". The statuettes that are handed out at the "Video Music Awards" are of the MTV moonman, the channel's original image from its first broadcast in 1981. Presently, the "Video Music Awards" are MTV's most watched annual event.

MTV began its annual "Spring Break" coverage in 1986, setting up temporary operations in Daytona Beach, Florida, for a week in March, broadcasting live eight hours per day. "Spring break is a youth culture event", MTV's vice president Doug Herzog said at the time. "We wanted to be part of it for that reason. It makes good sense for us to come down and go live from the center of it, because obviously the people there are the kinds of people who watch MTV." The channel's coverage featured numerous live performances from artists and bands on location. The annual tradition would continue into the 2000s, when it would become de-emphasized and handed off to mtvU, the spin-off channel of MTV targeted at college campuses.

The channel would later expand its beach-themed events to the summer, dedicating most of each summer season to broadcasting live from a beach house at various locations away from New York City, eventually leading to channel-wide branding throughout the summer in the 1990s and early 2000s such as "Motel California", "Summer Share", "Isle of MTV", "SoCal Summer", "Summer in the Keys", and "Shore Thing". MTV VJs would host blocks of music videos, interview artists and bands, and introduce live performances and other programs from the beach house location each summer. In the 2000s, as the channel reduced its airtime for music videos and eliminated much of its in-house programming, its annual summer-long events came to an end.

MTV would also hold week-long music events that would take over the presentation of the channel. Examples from the 1990s and 2000s include "All Access Week", a week in the summer dedicated to live concerts and festivals; "Spankin' New Music Week", a week in the fall dedicated to brand new music videos; and week-long specials that culminated in a particular live event, such as "Wanna be a VJ" and the "Video Music Awards".

At the end of each year, MTV takes advantage of its home location in New York City to broadcast live coverage on New Year's Eve in Times Square. Several live music performances are featured alongside interviews with artists and bands that were influential throughout the year. For many years from the 1980s to the 2000s, the channel upheld a tradition of having a band perform a cover song at midnight immediately following the beginning of the new year.

Throughout its history, MTV has covered global benefit concert series live. For most of July 13, 1985, MTV showed the Live Aid concerts, held in London and Philadelphia and organized by Bob Geldof and Midge Ure to raise funds for famine relief in Ethiopia. While the ABC network showed only selected highlights during primetime, MTV broadcast 16 hours of coverage.

Along with VH1, MTV broadcast the Live 8 concerts, a series of concerts set in the G8 states and South Africa, on July 2, 2005. Live 8 preceded the 31st G8 summit and the 20th anniversary of Live Aid. MTV drew heavy criticism for its coverage of Live 8. The network cut to commercials, VJ commentary, or other performances during performances. Complaints surfaced on the Internet over MTV interrupting the reunion of Pink Floyd. In response, MTV president Van Toeffler stated that he wanted to broadcast highlights from every venue of Live 8 on MTV and VH1, and clarified that network hosts talked over performances only in transition to commercials, informative segments or other musical performances. Toeffler acknowledged that "MTV should not have placed such a high priority on showing so many acts, at the expense of airing complete sets by key artists." He also blamed the Pink Floyd interruption on a mandatory cable affiliate break. MTV averaged 1.4million viewers for its original July 2 broadcast of Live 8. Consequently, MTV and VH1 aired five hours of uninterrupted Live 8 coverage on July 9, with each channel airing different blocks of artists.

MTV had debuted "Dial MTV" in 1986, a daily top ten music video countdown show for which viewers could call the toll-free telephone number 1-800-DIAL-MTV to request a music video. The show would be replaced by "MTV Most Wanted" in 1991, which ran until 1996, and later saw a spiritual successor in "Total Request Live". The phone number remained in use for video requests until 2006.

Also in 1986, the channel introduced "120 Minutes", a show that would feature low-rotation, alternative rock and other "underground" videos for the next 14 years on MTV and three additional years on sister channel MTV2. The program then became known as "Subterranean" on MTV2. Eight years later, on July 31, 2011, "120 Minutes" was resurrected with Matt Pinfield taking over hosting duties once again and airing monthly on MTV2.

Another late night music video show was added in 1987, "Headbangers Ball", which featured heavy metal music and news. Before its abrupt cancellation in 1995, it featured several hosts including Riki Rachtman and Adam Curry. A weekly block of music videos with the name "Headbangers Ball" aired from 2003 to 2011 on sister channel MTV2, before spending an additional two years as a web-only series on MTV2's website, until "Headbangers Ball" was discontinued once again in 2013.

In 1988, MTV debuted "Yo! MTV Raps", a hip hop/rap formatted program. The program continued until August 1995. It was renamed to simply "Yo!" and aired as a one-hour program from 1995 to 1999. The concept was reintroduced as "Direct Effect" in 2000, which became "Sucker Free" in 2006 and was cancelled in 2008, after briefly celebrating the 20th anniversary of "Yo! MTV Raps" throughout the months of April and May 2008. Despite its cancellation on MTV, a weekly countdown of hip hop videos known as "Sucker Free" still airs on MTV2 through the present day.

In 1989, MTV began to premiere music-based specials such as "MTV Unplugged", an acoustic performance show, which has featured dozens of acts as its guests and has remained active in numerous iterations on various platforms for over 20 years.

To further cater to the growing success of R&B, MTV introduced the weekly "Fade to Black" in the summer of 1991, which was hosted by Al B. Sure!. The show would be reformatted into the better known "MTV Jams" the following year, which incorporated mainstream hip-hop into the playlist. Bill Bellamy would become the new and ongoing host. The show became so successful it spawned its own Most Wanted spinoff titled "Most Wanted Jams".

By the early 1990s, MTV was playing a combination of pop-friendly hard rock acts, chart-topping metal and hard rock acts such as Metallica, Nirvana and Guns N' Roses, pop singers such as Michael Jackson, Madonna, 2 Unlimited, and New Kids on the Block, and R&B groups such as New Edition, En Vogue, Bell Biv Devoe, SWV, Tony Toni Tone, TLC and Boyz II Men, while introducing hit rappers Vanilla Ice and MC Hammer. MTV progressively increased its airing of hip hop acts, such as LL Cool J, Queen Latifah, Salt-n-Pepa, Naughty By Nature, Onyx, MC Lyte, and Sir-Mix-A-Lot, and by 1993, the channel added West Coast rappers previously associated with gangsta rap, with a less pop-friendly sound, such as Tupac Shakur, Ice Cube, Warren G, Ice-T, Dr. Dre, Tone Loc, and Snoop Doggy Dogg.

To accompany the new sounds, a new form of music videos came about: more creative, funny, artistic, experimental, and technically accomplished than those in the 1980s. Several noted film directors got their start creating music videos. After pressure from the Music Video Production Association, MTV began listing the names of the videos' directors at the bottom of the credits by December 1992. As a result, MTV's viewers became familiar with the names of Spike Jonze, Michel Gondry, David Fincher, Mary Lambert, Samuel Bayer, Matt Mahurin, Mark Romanek, Jonathan Dayton and Valerie Faris, Anton Corbijn, Mark Pellington, Tarsem, Hype Williams, Jake Scott, Jonathan Glazer, Marcus Nispel, F. Gary Gray, Jim Yukich, Russell Mulcahy, Steve Barron, Marty Callner, and Michael Bay, among others.

As the PBS series "Frontline" explored, MTV was a driving force that catapulted music videos to a mainstream audience, turning music videos into an art form as well as a marketing machine that became beneficial to artists. Danny Goldberg, chairman and CEO of Artemis Records, said the following about the art of music videos: "I know when I worked with Nirvana, Kurt Cobain cared as much about the videos as he did about the records. He wrote the scripts for them, he was in the editing room, and they were part of his art. And I think they stand up as part of his art, and I think that's true of the great artists today. Not every artist is a great artist and not every video is a good video, but in general having it available as a tool, to me, adds to the business. And I wish there had been music videos in the heyday of The Beatles, and The Rolling Stones. I think they would've added to their creative contribution, not subtracted from it."

Nirvana led a sweeping transition into the rise of alternative rock music on MTV in 1991 with their video for "Smells Like Teen Spirit". By late 1991 going into 1992, MTV began frequently airing videos from their heavily promoted "Buzz Bin", such as Nirvana, Pearl Jam, Alice in Chains, Soundgarden, Nine Inch Nails, Tori Amos, PM Dawn, Arrested Development, Björk, and Gin Blossoms. MTV increased rotation of its weekly alternative music program "120 Minutes" and added the daily "Alternative Nation" to play videos of these and other underground music acts. Subsequently, grunge and alternative rock had a rise in mainstream tastes, while 1980s-style glam bands and traditional rockers were phased out, with some exceptions such as Aerosmith and Tom Petty. Older acts such as R.E.M. and U2 remained relevant by making their music more experimental or unexpected.

They also played lots of hard rock acts such as Pantera, Death and other heavy/death metal acts at the time period.

In 1993, more hit alternative rock acts were on heavy rotation, such as Stone Temple Pilots, Soul Asylum, Rage Against the Machine, Marilyn Manson, Tool, Beck, Therapy?, Radiohead, and The Smashing Pumpkins. Other hit acts such as Weezer, Collective Soul, Blind Melon, The Cranberries, Bush, and Silverchair would follow in the next couple of years. Alternative bands that appeared on "Beavis and Butt-Head" included White Zombie.

By the next few years, 1994 through 1997, MTV began promoting new power pop acts, most successfully Green Day and The Offspring, and ska-rock acts such as No Doubt, The Mighty Mighty Bosstones and Sublime. Pop singers were added to the rotation with success as long as they were considered "alternative," such as Alanis Morissette, Jewel, Fiona Apple, and Sarah McLachlan.

By 1997, MTV focused heavily on introducing electronica acts into the mainstream, adding them to its musical rotation, including The Prodigy, The Chemical Brothers, Moby, Aphex Twin, Pendulum, Daft Punk, The Crystal Method, Butthole Surfers and Fatboy Slim. Some musicians who proceeded to experiment with electronica were still played on MTV including Madonna, U2, David Bowie, Radiohead, and Smashing Pumpkins. That year, MTV also attempted to introduce neo-swing bands, but they did not meet with much success.

However, in late 1997, MTV began shifting more progressively towards pop music, inspired by the success of the Spice Girls and the rise of boy bands in Europe. Between 1998 and 1999, MTV's musical content consisted heavily of videos of boy bands such as Backstreet Boys and NSYNC, as well as teen pop "princesses" such as Britney Spears, Christina Aguilera, Lynda Thomas, Mandy Moore, and Jessica Simpson. Airplay of rock, electronica, and alternative acts was reduced. Hip-hop music continued in heavy rotation, through the likes of Puff Daddy, Jermaine Dupri, Master P, DMX, Busta Rhymes, Lil' Kim, Jay-Z, Missy Elliott, Lauryn Hill, Eminem, Foxy Brown, Ja Rule, Nas, Timbaland, and their associates. R&B was also heavily represented with acts such as Aaliyah, Janet Jackson, Destiny's Child, 702, Monica, and Brandy.

Beginning in late 1997, MTV progressively reduced its airing of rock music videos, leading to the slogan among skeptics, "Rock is dead." The facts that at the time rock music fans were less materialistic, and bought less music based on television suggestion, were cited as reasons that MTV abandoned its once staple music. MTV instead devoted its musical airtime mostly to pop and hip hop/R&B music. All rock-centric shows were eliminated and the rock-related categories of the "Video Music Awards" were pared down to one.

From this time until 2004, MTV made some periodic efforts to reintroduce pop rock music videos to the channel. By 1998 through 1999, the punk-rock band Blink-182 received regular airtime on MTV due in large part to their "All the Small Things" video that made fun of the boy bands that MTV was airing at the time. Meanwhile, some rock bands that were not receiving MTV support, such as Korn and Creed, continued to sell albums. Then, upon the release of Korn's rock/rap hybrid album "Follow the Leader", MTV began playing Korn's videos "Got the Life" and "Freak on a Leash".

A band sponsored by Korn, Limp Bizkit, received airtime for its cover of George Michael's "Faith", which became a hit. Subsequently, MTV began airing more rap/rock hybrid acts, such as Limp Bizkit and Kid Rock. Some rock acts with more comical videos, such as Rob Zombie, Red Hot Chili Peppers and Foo Fighters, also received airtime.

In the fall of 1999, MTV announced a special "Return of the Rock" weekend, in which new rock acts received airtime, after which a compilation album was released. System of a Down, Staind, Godsmack, Green Day, Incubus, Papa Roach, P.O.D., Sevendust, Powerman 5000, Slipknot, Kittie, Static X, and CKY were among the featured bands. These bands received some airtime on MTV and more so on MTV2, though both channels gave emphasis to the rock/rap acts.

By 2000, Sum 41, Linkin Park, Jimmy Eat World, Mudvayne, Cold, At the Drive-In, Alien Ant Farm, and other acts were added to the musical rotation. MTV also launched subscription channel MTVX to play rock music videos exclusively, an experiment that lasted until 2002. A daily music video program on MTV that carried the name "Return of the Rock" ran through early 2001, replaced by a successor, "All Things Rock", from 2002 until 2004.

Also by 1997, MTV was criticized heavily for not playing as many music videos as it had in the past. In response, MTV created four shows that centered on music videos: "MTV Live", "Total Request", "Say What?", and "12 Angry Viewers". Also at this time, MTV introduced its new studios in Times Square.

A year later, in 1998, MTV merged "Total Request" and "MTV Live" into a live daily top ten countdown show, "Total Request Live", which would become known as "TRL" (the original host being Carson Daly) and secure its place as the channel's unofficial flagship program. In the fall of 1999, a live studio audience was added to the show. By spring 2000, the countdown reached its peak. The program enjoyed success playing the top ten pop, rock, R&B, and hip hop music videos, and featuring live interviews with artists and celebrities.

From 1998 to 2001, MTV also aired several other music video programs from its studios in Times Square and on location at various beach-themed locations each summer. These programs included "Say What? Karaoke", a game show hosted by Dave Holmes that evolved from "Say What?", MTV's earlier program that ran the lyrics of music videos across the screen. "TRL Wannabes" aired from 1999 to 2000 and featured a selection of music videos that just missed the "TRL" top ten. "VJ for a Day", hosted by Raymond Munns, continued this concept in early 2001. "VJ for a Day" was an extension of an annual event, "Wanna be a VJ", which aired each spring from 1998 to 2000 to select a new VJ to host programs on MTV.

MTV also aired "Hot Zone", hosted by Ananda Lewis, which featured pop music videos during the midday time period and was a casual alternative to "TRL"; it later became "MTV Hits". Other programs were "Direct Effect", "Return of the Rock", "MTV Jams", "BeatSuite", "MTV Soul", and blocks of music videos hosted by VJs simply called "Music Television" in the spirit of the channel's original purpose.

During the September 11, 2001 terror attacks on the World Trade Center and The Pentagon, MTV suspended all of its programming, along with its sister channel VH1, and it began simulcasting coverage from CBS News (the news division of CBS), which was acquired by MTV parent Viacom two years earlier) until about 11:00pm. ET that night. The channels then played a looped set of music videos without commercial interruption until an MTV News special edition of "TRL" aired on September 14, 2001.

In 2002, Carson Daly left MTV and "TRL" to pursue a late night talk show on NBC. After his departure, the relevance and impact of "Total Request Live" slowly diminished. "TRL" ultimately remained a part of MTV's regular program schedule for ten years. The series came to an end with a special finale episode, "Total Finale Live", which aired November 16, 2008, and featured all the show's hosts from over the years, many special guests from the history of the show, and played its last music video, "...Baby One More Time" by Britney Spears.

Around 1999 through 2001, as MTV aired fewer music videos throughout the day, it regularly aired compilation specials from its then 20-year history to look back on its roots. An all-encompassing special, "MTV Uncensored", premiered in 1999 and was later released as a book.

MTV celebrated its 20th anniversary on August 1, 2001, beginning with a 12-hour long retrospective called "MTV20: Buggles to Bizkit", which featured over 100 classic videos played chronologically, hosted by various VJs in reproductions of MTV's old studios. The day of programming culminated in a three-hour celebratory live event called "MTV20: Live and Almost Legal", which was hosted by Carson Daly and featured numerous guests from MTV's history, including the original VJs from 1981. Various other related "MTV20" specials aired in the months surrounding the event.

Janet Jackson became the inaugural honoree of the "mtvICON" award, "an annual recognition of artists who have made significant contributions to music, music video and pop culture while tremendously impacting the MTV generation." Subsequent recipients included Aerosmith, Metallica and The Cure.

Five years later, on August 1, 2006, MTV celebrated its 25th anniversary. On their website, MTV.com, visitors could watch the very first hour of MTV, including airing the original promos and commercials from Mountain Dew, Atari, Chewels gum, and Jovan. Videos were also shown from The Buggles, Pat Benatar, Rod Stewart, and others. The introduction of the first five VJs was also shown. Additionally, MTV.com put together a "yearbook" consisting of the greatest videos of each year from 1981 to 2006. MTV itself only mentioned the anniversary once on "TRL".

Although MTV reached its 30th year of broadcasting in 2011, the channel itself passed over this milestone in favor of its current programming schedule. The channel instead aired its 30th anniversary celebrations on its sister networks MTV2 and VH1 Classic. Nathaniel Brown, senior vice president of communications for MTV, confirmed that there were no plans for an on-air MTV celebration similar to the channel's 20th anniversary. Brown explained, "MTV as a brand doesn't age with our viewers. We are really focused on our current viewers, and our feeling was that our anniversary wasn't something that would be meaningful to them, many of whom weren't even alive in 1981."

From 1995 to 2000, MTV played 36.5% fewer music videos. MTV president Van Toeffler explained: "Clearly, the novelty of just showing music videos has worn off. It's required us to reinvent ourselves to a contemporary audience." Despite targeted efforts to play certain types of music videos in limited rotation, MTV greatly reduced its overall rotation of music videos by the mid-2000s. While music videos were featured on MTV up to eight hours per day in 2000, the year 2008 saw an average of just three hours of music videos per day on MTV. The rise of social media and websites like YouTube as a convenient outlet for the promotion and viewing of music videos signaled this reduction.

As the decade progressed, MTV continued to play some music videos instead of relegating them exclusively to its sister channels, but around this time, the channel began to air music videos only in the early morning hours or in a condensed form on "Total Request Live". As a result of these programming changes, Justin Timberlake implored MTV to "play more damn videos!" while giving an acceptance speech at the "2007 Video Music Awards".

Despite the challenge from Timberlake, MTV continued to decrease its total rotation time for music videos in 2007, and the channel eliminated its long-running special tags for music videos such as "Buzzworthy" (for under-represented artists), "Breakthrough" (for visually stunning videos), and "Spankin' New" (for brand new videos). Additionally, the historic Kabel typeface, which MTV displayed at the beginning and end of all music videos since 1981, was phased out in favor of larger text and less information about the video's record label and director. The classic font can still be seen in "prechyroned" versions of old videos on sister network MTV Classic, which had their title information recorded onto the same tape as the video itself.

Prior to its finale in 2008, MTV's main source of music videos was "Total Request Live", airing four times per week, featuring short clips of music videos along with VJs and guests. MTV was experimenting at the time with new ideas for music programs to replace the purpose of "TRL" but with a new format.

In mid-2008, MTV premiered new music video programming blocks called "FNMTV" and a weekly special event called "FNMTV Premieres", hosted from Los Angeles by Pete Wentz of the band Fall Out Boy, which was designed to premiere new music videos and have viewers provide instantaneous feedback.

The "FNMTV Premieres" event ended before the "2008 Video Music Awards" in September. With the exception of a holiday themed episode in December 2008 and an unrelated "Spring Break" special in March 2009 with the same title, "FNMTV Premieres" never returned to the channel's regular program schedule, leaving MTV without any music video programs hosted by VJs for the first time in its history.
Music video programming returned to MTV in March 2009 as "AMTV", an early morning block of music videos that originally aired from 3am to 9am on most weekdays. It was renamed "Music Feed" in 2013 with a reduced schedule. Unlike the "FNMTV" block that preceded it, "Music Feed" features many full-length music videos, including some older videos that have been out of regular rotation for many years on MTV. It also features music news updates, interviews, and performances. "Music Feed" is the only current program on MTV's main channel that is dedicated to music videos.

During the rest of the day, MTV also plays excerpts from music videos in split screen format during the closing credits of most programs, along with the address of a website to encourage the viewer to watch the full video online. MTV has positioned its website, MTV.com, as its primary destination for music videos.

MTV again resurrected the long-running series "MTV Unplugged" in 2009 with performances from acts such as Adele and Paramore. However, unlike past "Unplugged" specials, these new recordings usually only aired in their entirety on MTV's website, MTV.com. Nevertheless, short clips of the specials are shown on MTV during the "AMTV" block of music videos in the early morning hours. On June 12, 2011, MTV aired a traditional television premiere of a new installment of "MTV Unplugged" instead of a web debut. The featured artist was rapper Lil Wayne and the show debuted both on MTV and MTV2. The channel followed up with a similar television premiere of "MTV Unplugged" with Florence and the Machine on April 8, 2012.

MTV launched "10 on Top" in May 2010, a weekly program airing on Saturdays and hosted by Lenay Dunn, that counts down the top 10 most trending and talked about topics of the week (generally focused on entertainment). Dunn also appeared in segments between MTV's shows throughout the day as a recognizable personality and face of the channel in the absence of traditional VJs aside from its MTV News correspondents.

The animated series "Beavis and Butt-head" returned to MTV in October 2011, with new episodes. As with the original version of the series that ran from 1993 to 1997, the modern-day "Beavis and Butt-head" features segments in which its main characters watch and criticize music videos.

Sometime in 2012, MTV debuted "Clubland", which previously existed as an hour of EDM videos during the "AMTV" video block. The show has no host, but most editorial content is pushed online by the show's Tumblr and other social media outlets like Facebook and Twitter.

MTV launched a new talk show based on hip hop music on April 12, 2012, called "Hip Hop POV", hosted by Amanda Seales, Bu Thiam, Charlamagne, Devi Dev, and Sowmya Krishnamurthy. The show featured hosted commentary on the headlines in hip hop culture, providing opinions on new music, granting insider access to major events, and including artist interviews. "Hip Hip POV" lasted several episodes before going on hiatus. The show was supposed to return in Fall 2012, but was moved to MTV2 instead, where it was rebranded and merged with "Sucker Free Countdown". The new show debuted as "The Week in Jams" on October 28, 2012.

MTV launched a live talk show, "It's On with Alexa Chung", on June 15, 2009. The host of the program, Alexa Chung, was described as a "younger, more Web 2.0" version of Jimmy Fallon. Although it was filmed in the same Times Square studio where "TRL" used to be broadcast, the network stated that "the only thing the two shows have in common is the studio location." "It's On" was cancelled in December of the same year, which again eliminated the only live in-studio programming from MTV's schedule, just one year after "TRL" was also cancelled.

Shortly after Michael Jackson died on June 25, 2009, the channel aired several hours of Jackson's music videos, accompanied by live news specials featuring reactions from MTV personalities and other celebrities. The temporary shift in MTV's programming culminated the following week with the channel's live coverage of Jackson's memorial service. MTV aired similar one-hour live specials with music videos and news updates following the death of Whitney Houston on February 11, 2012, and the death of Adam Yauch of the Beastie Boys on May 4, 2012.

The channel tried its hand again at live programming with the premiere of a half-hour program called "The Seven" in September 2010. The program counted down seven entertainment-related stories of interest to viewers (and included some interview segments among them), having aired weekdays at 5pm with a weekend wrap-up at 10 am ET. Shortly after its debut, the show was slightly retooled as it dropped co-host Julie Alexandria but kept fellow co-host Kevin Manno; the Saturday recap show was eliminated as well. "The Seven" was cancelled on June 13, 2011. Manno's only assignment at MTV post-"Seven" was conducting an interview with a band which only aired on MTV.com. Manno is no longer employed with MTV and has since appeared as an occasional correspondent on the LXTV-produced NBC series "1st Look".

Presently, MTV airs sporadic live specials called "MTV First". The short program, produced by MTV News, debuted in early 2011 and continues to air typically once every couple of weeks on any given weekday. The specials usually begin at 7:53 pm. ET, led by one of MTV News' correspondents who will conduct a live interview with a featured artist or actor who has come to MTV to premiere a music video or movie trailer. MTV starts its next scheduled program at 8:00 pm, while the interview and chat with fans continues on MTV.com for another 30 to 60 minutes. Since its debut in 2011, "MTV First" has featured high-profile acts such as Lady Gaga, Katy Perry, Usher and Justin Bieber. In the absence of daily live programs such as "TRL", "It's on with Alexa Chung" and "The Seven" to facilitate such segments, the channel now uses "MTV First" as its newest approach to present music video premieres and bring viewers from its main television channel to its website for real-time interaction with artists and celebrities.

On April 21, 2016, MTV announced that new "Unplugged" episodes will begin airing, as well as a new weekly performance series called "Wonderland". On that same day, immediately after the death of Prince, MTV interrupted its usual programming to air Prince's music videos. In 2017, the network shifted focus away from scripted to reality while keeping the notion from 2016 to incorporate more music. In July 2017, it was announced that "TRL" would be returning to the network on October 2, 2017.

As MTV expanded, music videos were no longer the centerpiece of its programming. Conventional television shows came to replace the VJ-guided music video programming. Today, MTV presents a wide variety of non-music-related television shows aimed primarily at the 12- to 18-year-old demographic.

In 1985, Viacom bought Warner-Amex Satellite Entertainment, which owned MTV and Nickelodeon, renaming the company MTV Networks and beginning this expansion. Before 1987, MTV featured almost exclusively music videos, but as time passed, they introduced a variety of other shows, including some that were originally intended for other channels.

Non-music video programming began in the late 1980s, with the introduction of a music news show "The Week in Rock", which was also the beginning of MTV's news division, MTV News. Around this time, MTV also introduced a fashion news show, "House of Style"; a dance show, "Club MTV"; and a game show, "Remote Control". Programs like these did not feature music videos, but they were still largely based around the world of music.

Following the success of the "MTV Video Music Awards", in an effort to branch out from music into movies and broader pop culture, MTV started the "MTV Movie Awards" in 1992, which continues presently. MTV also created an award show for Europe after the success of the "Video Music Awards". The "MTV Europe Music Awards", or the EMAs, were created in 1994, ten years after the debut of the VMAs.

These new shows would be just the beginning of new genres of shows to make an impact on MTV. As the format of the network continued to evolve, more genres of shows began to appear. In the early 1990s, MTV debuted its first reality shows, "The Real World" and "Road Rules".

During the latter half of the 1990s and early 2000s, MTV placed a stronger focus on reality shows and related series, building on the success of "The Real World" and "Road Rules". The first round of these shows came in the mid-1990s, with game shows such as "Singled Out", reality-based comedy shows such as "Buzzkill", and late-night talk shows such as "The Jon Stewart Show" and "Loveline".

The next round of these shows came in approximately the late 1990s, as MTV shifted its focus to prank/comedic shows such as "The Tom Green Show" and "Jackass", and game shows such as "The Challenge" (aka "Real World/Road Rules Challenge"), "The Blame Game", "webRIOT", and "Say What? Karaoke". A year later, in 2000, "MTV's Fear" became one of the first scare-based reality shows and the first reality show in which contestants filmed themselves.

MTV continued to experiment with late night talk shows in the early 2000s with relatively short-lived programs such as "Kathy's So-Called Reality", starring Kathy Griffin; and "The New Tom Green Show".

Some of the reality shows on the network also followed the lives of musicians. "The Osbournes", a reality show based on the everyday life of Black Sabbath frontman Ozzy Osbourne, his wife Sharon, and two of their children, Jack and Kelly, premiered on MTV in 2002. The show went on to become one of the network's biggest-ever successes and was also recognized for the Osbourne family members' heavy use of profanity, which MTV censored for broadcast. It also kick-started a musical career for Kelly Osbourne, while Sharon Osbourne went on to host her own self-titled talk show on US television. Production ended on "The Osbournes" in November 2004. In the fall of 2004, Ozzy Osbourne's reality show "Battle for Ozzfest" aired; the show hosted competitions between bands vying to play as part of Ozzfest, a yearly heavy metal music tour across the United States hosted by Osbourne.

In 2003, MTV added "Punk'd", a project by Ashton Kutcher to play pranks on various celebrities, and "Pimp My Ride", a show about adding aesthetic and functional modifications to cars and other vehicles. Another show was "", a reality series that followed the lives of pop singers Jessica Simpson and Nick Lachey, a music celebrity couple. It began in 2003 and ran for four seasons, ending in early 2005; the couple later divorced. The success of "Newlyweds" was followed in June 2004 by "The Ashlee Simpson Show", which documented the beginnings of the music career of Ashlee Simpson, Jessica Simpson's younger sister.

In 2005 and 2006, MTV continued its focus on reality shows, with the debuts of shows such as "8th & Ocean", "", "Next", "The Hills", "Two-A-Days", "My Super Sweet 16", "Parental Control", and "Viva La Bam", featuring Bam Margera.

In 2007, MTV aired the reality show "A Shot at Love with Tila Tequila", chronicling MySpace sensation Tila Tequila's journey to find a companion. Her bisexuality played into the seriesboth male and female contestants were vying for loveand was the subject of criticism. It was the #2 show airing on MTV at that time, behind "The Hills". A spin-off series from "A Shot at Love", titled "That's Amoré!", followed a similar pursuit from previous "A Shot at Love" contestant Domenico Nesci.

MTV also welcomed Paris Hilton to its lineup in October 2008, with the launch of her new reality series, "Paris Hilton's My New BFF". In 2009, MTV aired Snoop Dogg's second program with the channel, "Dogg After Dark", and the show "College Life", based at the University of Wisconsin-Madison.
In late 2009, MTV shifted its focus back to "Real World"-style reality programming with the premiere of "Jersey Shore", a program that brought high ratings to the channel and also caused controversy due to some of its content.

With backlash towards what some consider too much superficial content on the network, a 2009 "New York Times" article also stated the intention of MTV to shift its focus towards more socially conscious media, which the article labels "MTV for the Obama era." Shows in that vein included "T.I.'s Road to Redemption" and Fonzworth Bentley's finishing school show "From G's to Gents".

The channel also aired a new show around this time titled "16 and Pregnant", which documented the lives of teenagers expecting babies. This had a follow-up show after the first season titled "Teen Mom", which follows some of the teens through the first stages with their newborns.

MTV found further success with "The Buried Life", a program about four friends traveling across the country to check off a list of "100 things to do before I die" and helping others along the way. Another recent reality program is MTV's "Hired", which follows the employment interviewing process; candidates meet with career coach Ryan Kahn from University of Dreams and at the end of each episode one candidate lands the job of their dreams.

In 2011, MTV premiered an amateur video clip show "Ridiculousness", in the vein of "Tosh.0" on sister network Comedy Central. In 2012, "Punk'd" returned with a revolving door of new hosts per episode. Meanwhile, spin-offs from "Jersey Shore" such as "The Pauly D Project" and "Snooki & JWoww" were produced. MTV announced plans to re-enter the late-night comedy space in 2012, with "Nikki & Sara Live", an unscripted series by comedians Nikki Glaser and Sara Schaefer. The program initially aired weekly from MTV's studios in Times Square.

In a continuing bid to become a more diverse network focusing on youth and culture as well as music, MTV added animated shows to its lineup in the early 1990s. The animation showcase "Liquid Television" (a co-production between BBC and MTV produced in San Francisco by Colossal Pictures) was one of the channel's first programs to focus on the medium. In addition to airing original shows created specifically for MTV, the channel also occasionally aired episodes of original cartoon series produced by sister channel Nickelodeon ("Nicktoons") in the early 1990s.

MTV has a history of cartoons with mature themes including "Beavis and Butt-Head", "Æon Flux", "The Brothers Grunt", "Celebrity Deathmatch", "Undergrads", "Clone High", and "Daria". Although the channel has gone on to debut many other animated shows, few of MTV's other cartoon series have been renewed for additional seasons, regardless of their reception.

In September 2009, the channel aired "Popzilla", which showcased and imitated celebrities in an animated form. MTV again reintroduced animated programming to its lineup with the return of "Beavis and Butt-Head" in 2011 after 14 years off the air, alongside brand new animated program "Good Vibes". In January 2016, MTV returned to animation with "Greatest Party Story Ever".

MTV has a long history of airing both comedy and drama programs with scripted or improvised premises. Examples from the 1990s and 2000s include sketch-based comedies such as "Just Say Julie", "The Ben Stiller Show", "The State", "The Jenny McCarthy Show", "The Lyricist Lounge Show", and "Doggy Fizzle Televizzle", as well as soap operas such as "Undressed" and "Spyder Games".

The channel expanded its programming focus in late 2000s and early 2010s to include more scripted programs. The resurgence of scripted programming on MTV saw the introduction of comedy shows such as "Awkward." and "The Hard Times of RJ Berger", and dramas such as "Skins" and "Teen Wolf". In June 2012, MTV confirmed that it would develop a series based on the "Scream" franchise. The series is now in its third season.

In recent years, MTV has re-aired other programs from other Viacom-owned networks, such as BET's "College Hill", and VH1 programs "I Love New York" and "Flavor of Love". Other programs from non-Viacom networks include reruns of the shows "Fastlane" (from Fox), "Life As We Know It" (from ABC), "Scrubs" (from ABC and NBC), and CW programs "America's Next Top Model", "Beauty and the Geek", and "Hidden Palms".

MTV also began showing movies targeted toward the young adult demographic, including "8 Mile", "My Boss's Daughter", "Shaun of the Dead", and "Napoleon Dynamite" (the latter of which the network had a hand in producing). The channel has also broadcast several of its own films from its production division MTV Films, such as "Crossroads" and "", and airs original made-for-television movies from MTV Studios such as "".

In 2010, a study by the Gay and Lesbian Alliance Against Defamation found that of 207.5 hours of prime time programming on MTV, 42% included content reflecting the lives of gay, bisexual and transgender people. This was the highest in the industry and the highest percentage ever.

MTV's now-iconic logo was designed in 1981 by Manhattan Design (a collective formed by Frank Olinsky, Pat Gorman and Patty Rogoff) under the guidance of original creative director Fred Seibert. The block letter "M" was sketched by Rogoff, with the scribbled word "TV" spraypainted by Olinksky. The primary variant of MTV's logo at the time had the "M" in yellow and the "TV" in red. But unlike most television networks' logos at the time, the logo was constantly branded with different colors, patterns and images on a variety of station IDs. The only constant aspects of MTV's logo at the time were its general shape and proportions, with everything else being dynamic.

MTV launched on August 1, 1981, with an extended network ID featuring the first landing on the moon (with still images acquired directly from NASA), which was a concept of Seibert's executed by Buzz Potamkin and Perpetual Motion Pictures. The ID then cut to the American flag planted on the moon's surface changed to show the MTV logo on it, which rapidly changed into different colors and patterns several times per second as the network's original guitar-driven jingle was played for the first time. After MTV's launch, the "moon landing" ID was edited to show only its ending, and was shown at the top of every hour until early 1986, when the ID was scrapped in light of the Space Shuttle Challenger disaster. The ID ran "more than 75,000 times each year (48 times each day), at the top and bottom of every hour every day" according to Seibert.

From the late 1990s to the early 2000s, MTV updated its on-air appearance at the beginning of every year and each summer, creating a consistent brand across all of its music-related shows. This style of channel-wide branding came to an end as MTV drastically reduced its number of music-related shows in the early to mid 2000s. Around this time, MTV introduced a static and single color digital on-screen graphic to be shown during all of its programming.

Starting with the premiere of the short-lived program "FNMTV" in 2008, MTV started using a revised and chopped down version of its original logo during most of its on-air programming. It became MTV's official logo on February 8, 2010 and officially debuted on its website. The channel's full name "Music Television" was officially dropped, with the revised logo largely the same as the original logo, but without the initialism, the bottom of the "M" being cropped and the "V" in "TV" being branched off. This change was most likely made to reflect MTV's more prominent focus on reality and comedy programming and less on music-related programming. However, much like the original logo, the new logo was designed to be filled in with a seemingly unlimited variety of images. It is used worldwide, but not everywhere existentially. The new logo was first used on MTV Films logo with the 2010 film "Jackass 3D". MTV's rebranding was overseen by Popkern.

On June 25, 2015, MTV International rebranded its on-air look with a new vaporwave and seapunk-inspired graphics package. It included a series of new station IDs featuring 3D renderings of objects and people, much akin to vaporwave and seapunk "aesthetics". Many have derided MTV's choice of rebranding, insisting that the artistic style was centered on denouncing corporate capitalism (many aesthetic pieces heavily incorporate corporate logos of the 1970s, 80s and 90s, which coincidentally include MTV's original logo) rather than being embraced by major corporations like MTV. Many have also suggested that MTV made an attempt to be relevant in the modern entertainment world with the rebrand. In addition to this, the rebrand was made on exactly the same day that the social media site Tumblr introduced Tumblr TV, an animated GIF viewer which featured branding inspired by MTV's original 1980s on-air look. Tumblr has been cited as a prominent location of aesthetic art, and thus many have suggested MTV and Tumblr "switched identities". The rebrand also incorporated a modified version of MTV's classic "I Want My MTV!" slogan, changed to read "I Am My MTV". "Vice" has suggested that the slogan change represents "the current generation's movement towards self-examination, identity politics and apparent narcissism." MTV also introduced MTV Bump, a website that allows Instagram and Vine users to submit videos to be aired during commercial breaks, as well as MTV Canvas, an online program where users submit custom IDs to also be aired during commercial breaks.

The channel's iconic "I want my MTV!" advertising campaign was launched in 1982. It was first developed by George Lois and was based on a cereal commercial from the 1950s with the slogan "I want my Maypo!" that Lois adapted unsuccessfully from the original created by animator John Hubley.

Lois's first pitch to the network was roundly rejected when Lois insisted that rock stars like Mick Jagger should be crying when they said the tag line, not unlike his failed 'Maypo' revamp. His associate, and Seibert mentor Dale Pon took over the campaign, strategically and creatively, and was able to get the campaign greenlit when he laughed the tears out of the spots. From then on –with the exception of the closely logos on the first round of commercials– Pon was the primary creative force.

All the commercials were produced produced by Buzz Potamkin and his new company Buzzco Productions, directed first by Thomas Schlamme and Alan Goodman and eventually by Candy Kugel.

The campaign featured popular artists and celebrities, including Pete Townshend, Pat Benatar, Adam Ant, David Bowie, The Police, Kiss, Culture Club, Billy Idol, Hall & Oates, Cyndi Lauper, Madonna, Lionel Richie, Ric Ocasek, John Mellencamp, Peter Wolf, Joe Elliot, Stevie Nicks, Rick Springfield and Mick Jagger, interacting with the MTV logo on-air and encouraging viewers to call their pay television providers and request that MTV be added to their local channel lineups. Eventually, the slogan became so ubiquitous that it made an appearance as a lyric sung by Sting on the Dire Straits song "Money for Nothing", whose music video aired in regular rotation on MTV when it was first released in 1985.

The channel has been a target of criticism by various groups about programming choices, social issues, political correctness, sensitivity, censorship, and a perceived negative social influence on young people. Portions of the content of MTV's programs and productions have come under controversy in the general news media and among social groups that have taken offense. Some within the music industry criticized what they saw as MTV's homogenization of rock 'n' roll, including the punk band the Dead Kennedys, whose song "M.T.V.Get Off the Air" was released on their 1985 album "Frankenchrist", just as MTV's influence over the music industry was being solidified. MTV was also the major influence on the growth of music videos during the 1980s.

HBO also had a 30-minute program of music videos called "Video Jukebox", that first aired around the time of MTV's launch and would last until late 1986. Also around this time, HBO, as well as other premium channels such as Cinemax, Showtime and The Movie Channel, would occasionally play one or a few music videos between movies.

SuperStation WTBS launched "Night Tracks" on June 3, 1983, with up to 14 hours of music video airplay each late night weekend by 1985. Its most noticeable difference was that black artists received airplay that MTV initially ignored. The program ran until the end of May 1992.

A few markets also launched music-only channels including Las Vegas' KVMY (channel 21), which debuted in the summer of 1984 as KRLR-TV and branded as "Vusic 21". The first video played on that channel was "Video Killed the Radio Star", following in the footsteps of MTV.

Shortly after TBS began "Night Tracks", NBC launched a music video program called "Friday Night Videos", which was considered network television's answer to MTV. Later renamed simply "Friday Night", the program ran from 1983 to 2002. ABC's contribution to the music video program genre in 1984, "ABC Rocks", was far less successful, lasting only a year.

TBS founder Ted Turner started the Cable Music Channel in 1984, designed to play a broader mix of music videos than MTV's rock format allowed. But after one month as a money-losing venture, Turner sold it to MTV, who redeveloped the channel into VH1.

Shortly after its launch, The Disney Channel aired a program called "D-TV", a play on the MTV acronym. The program used music cuts, both from current and past artists. Instead of music videos, the program used clips of various vintage Disney cartoons and animated films to go with the songs. The program aired in multiple formats, sometimes between shows, sometimes as its own program, and other times as one-off specials. The specials tended to air both on The Disney Channel and NBC. The program aired at various times between 1984 and 1999. In 2009, Disney Channel revived the "D-TV" concept with a new series of short-form segments called "Re-Micks".

MTV has edited a number of music videos to remove references to drugs, sex, violence, weapons, racism, homophobia and/or advertising. Many music videos aired on the channel were either censored, moved to late-night rotation, or banned entirely from the channel.

In the 1980s, parent media watchdog groups such as the Parents Music Resource Center (PMRC) criticized MTV over certain music videos that were claimed to have explicit imagery of satanism. As a result, MTV developed a strict policy on refusal to air videos that may depict Satanism or anti-religious themes. This policy led MTV to ban music videos such as "Jesus Christ Pose" by Soundgarden in 1991 and "Megalomaniac" by Incubus in 2004; however, the controversial band Marilyn Manson was among the most popular rock bands on MTV during the late 1990s and early 2000s.

On September 28, 2016, on an AfterBuzz TV live stream, Scout Durwood said that MTV had a "no appropriation policy" that forbid her from wearing her hair in cornrows in an episode of "Mary + Jane". She said, "I wanted to cornrow my hair, and they were like, 'That's racist.'"

During the 1989 MTV Video Music Awards ceremony, comedian Andrew Dice Clay did his usual "adult nursery rhymes" routine (which he had done in his stand-up acts), after which the network executives imposed a lifetime ban. Billy Idol's music video for the song "Cradle of Love" originally had scenes from Clay's film "The Adventures of Ford Fairlane" when it was originally aired; scenes from the film were later excised. During the "2011 MTV Video Music Awards", Clay was in attendance where he confirmed that the channel lifted the ban.

In the wake of controversy that involved a child burning down his house after allegedly watching "Beavis and Butt-head", MTV moved the show from its original 7p.m. time slot to an 11p.m. time slot. Also, Beavis' tendency to flick a lighter and yell "fire" was removed from new episodes, and controversial scenes were removed from existing episodes before their rebroadcast. Some extensive edits were noted by series creator Mike Judge after compiling his , saying that "some of those episodes may not even exist actually in their original form."

A pilot for a show called "Dude, This Sucks" was canceled after teens attending a taping at the Snow Summit Ski Resort in January 2001 were sprayed with liquidized fecal matter by a group known as "The Shower Rangers". The teens later sued, with MTV later apologizing and ordering the segment's removal.

After Viacom's purchase of CBS, MTV was selected to produce the Super Bowl XXXV halftime show in 2001, airing on CBS and featuring Britney Spears, NSYNC and Aerosmith. Due to its success, MTV was invited back to produce another halftime show in 2004, which would spark a nationwide debate and controversy that would drastically change Super Bowl halftime shows, MTV's programming, and radio censorship.

When CBS aired Super Bowl XXXVIII in 2004, MTV was again chosen to produce the halftime show, with performances by such artists as Nelly, Diddy, Janet Jackson, and Justin Timberlake. The show became controversial, however, after Timberlake tore off part of Jackson's outfit while performing "Rock Your Body" with her, revealing her right breast. All involved parties apologized for the incident, and Timberlake referred to the incident as a "wardrobe malfunction".

Michael Powell, former chairman of the Federal Communications Commission, ordered an investigation the day after broadcast. In the weeks following the halftime show, MTV censored much of its programming. Several music videos, including "This Love" and "I Miss You", were edited for sexual content. In September 2004, the FCC ruled that the halftime show was indecent and fined CBS $550,000. The FCC upheld it in 2006, but federal judges reversed the fine in 2008.

The Super Bowl itself would not feature another modern act for its halftime show until The Black Eyed Peas performed at Super Bowl XLV in 2011.

Timberlake and Jackson's controversial event gave way to a "wave of self-censorship on American television unrivaled since the McCarthy era". After the sudden event, names surfaced such as nipplegate, Janet moment, and boobgate, and this spread politically, furthering the discussion into the 2004 presidential election surrounding "moral values" and "media decency".

The Christian right organization American Family Association has also criticized MTV from perceptions of negative moral influence, describing MTV as promoting a "pro-sex, anti-family, pro-choice, drug culture".

In 2005, the Parents Television Council (PTC) released a study titled "MTV Smut Peddlers", which sought to expose excessive sexual, profane, and violent content on the channel, based on MTV's spring break programming from 2004. Jeanette Kedas, an MTV network executive, called the PTC report "unfair and inaccurate" and "underestimating young people's intellect and level of sophistication", while L. Brent Bozell III, then-president of the PTC, stated: "the incessant sleaze on MTV presents the most compelling case yet for consumer cable choice", referring to the practice of pay television companies to allow consumers to pay for channels "à la carte".

In April 2008, PTC released "The Rap on Rap", a study covering hip-hop and R&B music videos rotated on programs "106 & Park" and "Rap City", both shown on BET, and "Sucker Free" on MTV. PTC urged advertisers to withdraw sponsorship of those programs, whose videos PTC stated targeted children and teenagers containing adult content.

MTV received significant criticism from Italian American organizations for "Jersey Shore", which premiered in 2009. The controversy was due in large part to the manner in which MTV marketed the show, as it liberally used the word "guido" to describe the cast members. The word "guido" is generally regarded as an ethnic slur when referring to Italians and Italian Americans. One promotion stated that the show was to follow, "eight of the hottest, tannest, craziest Guidos," while yet another advertisement stated, ""Jersey Shore" exposes one of the tri-state area's most misunderstood species ... the GUIDO. Yes, they really do exist! Our Guidos and Guidettes will move into the ultimate beach house rental and indulge in everything the Seaside Heights, New Jersey scene has to offer."

Prior to the series debut, Unico National formally requested that MTV cancel the show. In a formal letter, the company called the show a "direct, deliberate and disgraceful attack on Italian Americans." Unico National President Andre DiMino said in a statement, "MTV has festooned the 'bordello-like' house set with Italian flags and red, white and green maps of New Jersey while every other cutaway shot is of Italian signs and symbols. They are blatantly as well as subliminally bashing Italian Americans with every technique possible." Around this time, other Italian organizations joined the fight, including the NIAF and the Order Sons of Italy in America.

MTV responded by issuing a press release which stated in part, "The Italian American cast takes pride in their ethnicity. We understand that this show is not intended for every audience and depicts just one aspect of youth culture." Following the calls for the show's removal, several sponsors requested that their ads not be aired during the show. These sponsors included Dell, Domino's Pizza and American Family Insurance. Despite the loss of certain advertisers, MTV did not cancel the show. Moreover, the show saw its audience increase from its premiere in 2009, and continued to place as MTV's top-rated programs during "Jersey Shore's" six-season run, ending in 2012.

In December 2016, MTV online published a social justice-oriented New Year's resolution-themed video directed towards cisgender heterosexual white men. The video caused widespread outrage online, including video responses from well-known online personas, and was deleted from MTV's YouTube channel. The video was then reuploaded to their channel, with MTV claiming the new video contained "updated graphical elements". The new video quickly received over 10,000 dislikes and fewer than 100 likes from only 20,000 views, and MTV deleted the video for a second time.

In addition to its regular programming, MTV has a long history of promoting social, political, and environmental activism in young people. The channel's vehicles for this activism have been "Choose or Lose", encompassing political causes and encouraging viewers to vote in elections; "Fight For Your Rights", encompassing anti-violence and anti-discrimination causes; "think MTV"; and "MTV Act" and "Power of 12", the newest umbrellas for MTV's social activism.

In 1992, MTV started a pro-democracy campaign called "Choose or Lose", to encourage over 20 million people to register to vote, and the channel hosted a town hall forum for then-candidate Bill Clinton.

In recent years, other politically diverse programs on MTV have included "True Life", which documents people's lives and problems, and MTV News specials, which center on very current events in both the music industry and the world. One special show covered the 2004 US Presidential election, airing programs focused on the issues and opinions of young people, including a program where viewers could ask questions of Senator John Kerry. MTV worked with P. Diddy's "Vote or Die" campaign, designed to encourage young people to vote.

Additionally, MTV aired a documentary covering a trip by the musical group Sum 41 to the Democratic Republic of the Congo, documenting the conflict there. The group ended up being caught in the midst of an attack outside of the hotel and were subsequently flown out of the country.

The channel also began showing presidential campaign commercials for the first time during the 2008 US presidential election. This has led to criticism, with Jonah Goldberg opining that "MTV serves as the Democrats' main youth outreach program."

Campaign to motivate young adults to register and vote.

In 2012, MTV launched "MTV Act" and "Power of 12", its current social activism campaigns. "MTV Act" focuses on a wide array of social issues, while "Power of 12" was a replacement for MTV's "Choose or Lose" and focused on the 2012 US presidential election.

In 2016, MTV continued its pro-democracy campaign with "Elect This", an issue-oriented look at the 2016 election targeting Millennials. Original content under the "Elect This" umbrella includes "Infographica," short animations summarizing MTV News polls; "Robo-Roundtable," a digital series hosted by animatronic robots; "The Racket," a multi-weekly digital series; and "The Stakes," a weekly political podcast.

Since its launch in 1981, the brand "MTV" has expanded to include many additional properties beyond the original MTV channel, including a variety of sister channels in the US, dozens of affiliated channels around the world, and an Internet presence through MTV.com and related websites.

MTV operates a group of channels under MTV Networksa name that continues to be used for the individual units of the now Viacom Media Networks, a division of corporate parent Viacom. In 1985, MTV saw the introduction of its first true sister channel, VH1, which was originally an acronym for "Video Hits One" and was designed to play adult contemporary music videos. Today, VH1 is aimed at celebrity and popular culture programming which include many reality shows. Another sister channel, CMT, targets the country music and southern culture market.

The advent of satellite television and digital cable brought MTV greater channel diversity, including its current sister channels MTV2 and MTV Tr3́s (now Tr3́s), which initially played music videos exclusively but now focus on other programming. MTV also broadcasts MTVU, a college-oriented channel on campus at various universities.

In the 2000s, MTV launched MTV HD, a 1080i high definition simulcast feed of MTV. Until Viacom's main master control was upgraded in 2013, only the network's original series after 2010 (with some pre-2010 content) are broadcast in high definition, while music videos, despite being among the first television works to convert to high definition presentation in the early 2000s, were presented in 4:3 standard definition, forcing them into a windowboxing type of presentation; since that time all music videos are presented in HD, and are framed to their on director's preference. "Jersey Shore", despite being shot with widescreen HD cameras, was also presented with SD windowboxing (though the 2018 "" revival is in full HD). The vast majority of providers carry MTV HD.

MTV Networks also operates MTV Live, a high-definition channel that features original HD music programming and HD versions of music related programs from MTV, VH1 and CMT. The channel was launched in January 2006 as MHD (Music: High Definition). The channel was officially rebranded as MTV Live on February 1, 2016.

In 2005 and 2006, MTV launched a series of channels for Asian Americans. The first channel was MTV Desi, launched in July 2005, dedicated toward South-Asian Americans. Next was MTV Chi, in December 2005, which catered to Chinese Americans. The third was MTV K, launched in June 2006 and targeted toward Korean Americans. Each of these channels featured music videos and shows from MTV's international affiliates as well as original US programming, promos, and packaging. All three of these channels ceased broadcasting on April 30, 2007.

On August 1, 2016, the 35th anniversary of the original MTV's launch, VH1 Classic was rebranded as MTV Classic. The channel's programming focused on classic music videos and programming (including notable episodes of "MTV Unplugged" and "VH1 Storytellers"), but skews more towards the 1980s, 1990s and 2000s. The network aired encores of 2000s MTV series such as "Beavis and Butt-Head" and "". The network's relaunch included a broadcast of MTV's first hour on the air, which was also simulcast on MTV and online via Facebook live streaming. MTV Classic only retained three original VH1 Classic programs, which were "That Metal Show", "Metal Evolution" and "Behind the Music Remastered", although repeats of current and former VH1 programs such as "Pop-Up Video" and "VH1 Storytellers" remained on the schedule. However, the rebranded MTV Classic had few viewers, and declined quickly to become the least-watched English-language subscription network rated by Nielsen at the end of 2016. At the start of 2017 it was re-programmed into an all-video network.

In the late 1980s, before the World Wide Web, MTV VJ Adam Curry began experimenting on the Internet. He registered the then-unclaimed domain name "MTV.com" in 1993 with the idea of being MTV's unofficial new voice on the Internet. Although this move was sanctioned by his supervisors at MTV Networks at the time, when Curry left to start his own web-portal design and hosting company, MTV subsequently sued him for the domain name, which led to an out-of-court settlement.

The service hosted at the domain name was originally branded "MTV Online" during MTV's first few years of control over it in the mid-1990s. It served as a counterpart to the America Online portal for MTV content, which existed at AOL keyword MTV until approximately the end of the 1990s. After this time, the website became known as simply "MTV.com" and served as the Internet hub for all MTV and MTV News content.

MTV.com experimented with entirely video-based layouts between 2005 and 2007. The experiment began in April 2005 as "MTV Overdrive", a streaming video service that supplemented the regular MTV.com website. Shortly after the "2006 Video Music Awards", which were streamed on MTV.com and heavily utilized the "MTV Overdrive" features, MTV introduced a massive change for MTV.com, transforming the entire site into a Flash video-based entity. Much of users' feedback about the Flash-based site was negative, demonstrating a dissatisfaction with videos that played automatically, commercials that could not be skipped or stopped, and the slower speed of the entire website. The experiment ended in February 2006 as MTV.com reverted to a traditional HTML-based website design with embedded video clips, in the style of YouTube and some other video-based websites.

From 2006 to 2007, MTV operated an online channel, MTV International, targeted to the broad international market. The purpose of the online channel was to air commercial-free music videos once the television channels started concentrating on shows unrelated to music videos or music-related programming.

The channel responded to the rise of the Internet as the new central place to watch music videos in October 2008 by launching MTV Music (later called MTV Hive), a website that featured thousands of music videos from MTV and VH1's video libraries, dating back to the earliest videos from 1981.

A newly created division of the company, MTV New Media, announced in 2008 that it would produce its own original web series, in an attempt to create a bridge between old and new media. The programming is available to viewers via personal computers, cell phones, iPods, and other digital devices.

In the summer of 2012, MTV launched a music discovery web site called Artists.MTV. MTV stated, "While technology has made it way easier for artists to produce and distribute their own music on their own terms, it hasn't made it any simpler to find a way to cut through all the Internet noise and speak directly to all of their potential fans. The summer launch of Artists.MTV is an attempt to help music junkies and musicians close the gap by providing a one-stop place where fans can listen to and buy music and purchase concert tickets and merchandise."

Today, MTV.com remains the official website of MTV, and it expands on the channel's broadcasts by bringing additional content to its viewers. The site's features include an online version of MTV News, podcasts, a commercial streaming service, movie features, profiles and interviews with recording artists and from MTV's television programs.

MTV Networks has launched numerous native-language regional variants of MTV-branded channels to countries around the world.





</doc>
<doc id="18857" url="https://en.wikipedia.org/wiki?curid=18857" title="Mustelidae">
Mustelidae

The Mustelidae (; from Latin "mustela", weasel) are a family of carnivorous mammals, including weasels, badgers, otters, martens, mink, and wolverines, among others. Mustelids are diverse and the largest family in the order Carnivora. The internal classification is still disputed, with rival proposals containing between two and eight subfamilies. One study, published in 2008, questions the long-accepted Mustelinae subfamily, and suggests that the Mustelidae consist of four major clades and three much smaller lineages.

Mustelids vary greatly in size and behaviour. The least weasel can be under a foot in length, while the giant otter can measure up to and sea otters can exceed in weight. The wolverine can crush bones as thick as the femur of a moose to get at the marrow, and has been seen attempting to drive bears away from their kills. The sea otter uses rocks to break open shellfish to eat. The marten is largely arboreal, while the badger digs extensive networks of tunnels, called setts. Some mustelids have been domesticated: the ferret and the tayra are kept as pets (although the tayra requires a Dangerous Wild Animals licence in the UK), or as working animals for hunting or vermin control. Others have been important in the fur trade—the mink is often raised for its fur.

As well as being one of the most species-rich families in the order Carnivora, the family Mustelidae is one of the oldest. Mustelid-like forms first appeared about 40 million years ago, roughly coinciding with the appearance of rodents. The direct ancestors of the modern mustelids first appeared about 15 million years ago.

Within a large range of variation, the mustelids exhibit some common characteristics. They are typically small animals with short legs, short, round ears, and thick fur. Most mustelids are solitary, nocturnal animals, and are active year-round.

With the exception of the sea otter, they have anal scent glands that produce a strong-smelling secretion the animals use for sexual signaling and for marking territory.
Most mustelid reproduction involves embryonic diapause. The embryo does not immediately implant in the uterus, but remains dormant for some time. No development takes place as long as the embryo remains unattached to the uterine lining. As a result, the normal gestation period is extended, sometimes up to a year. This allows the young to be born under more favorable environmental conditions. Reproduction has a large energy cost and it is to a female's benefit to have available food and mild weather. The young are more likely to survive if birth occurs after previous offspring have been weaned.

Mustelids are predominantly carnivorous, although some eat vegetable matter at times. While not all mustelids share an identical dentition, they all possess teeth adapted for eating flesh, including the presence of shearing carnassials. With variation between species, the most common dental formula is .

Several members of the family are aquatic to varying degrees, ranging from the semiaquatic mink, to the river otters, and to the highly aquatic sea otter. The sea otter is one of the few nonprimate mammals known to use a tool while foraging. It uses "anvil" stones to crack open the shellfish that form a significant part of its diet. It is a "keystone species", keeping its prey populations in balance so some do not outcompete the others and destroy the kelp in which they live.

The black-footed ferret is entirely dependent on another keystone species, the prairie dog. A family of four ferrets eats 250 prairie dogs in a year; this requires a stable population of prairie dogs from an area of some .

The skunks were formerly included as a subfamily of the mustelids, but are now regarded as a separate family (the Mephitidae). The mongoose and the meerkat bear a striking resemblance to many mustelids, but belong to a distinctly different suborder—the Feliformia (all those carnivores sharing more recent origins with the cats) and not the Caniformia (those sharing more recent origins with the dogs). Because the mongooses and the mustelids occupy similar ecological niches, convergent evolution has led to some similarity in form and behavior.

The oldest fossil of the mustelids were dated at the end of the Oligocene to the beginning of the Miocene. “There is debate regarding which fossils from these epochs represent possible ancestral forms that led to Mustelidae and which fossils represent the first modern mustelids.”(Wund, M. 2005. "Mustelidae" (On-line), Animal Diversity Web.) From the fossil record we can see that Mustelids appeared in the late Oligocene period (33 mya) in Eurasia and migrated throughout the continents. The Mustelids inhabit every continent except Antarctica and Australia. The mustelids migrated all throughout the continents that were connected during the early Miocene. The Mustelids made their way to North and South America via the Bering land bridge.

Several mustelids, including the mink, the sable (a type of marten) and the stoat (ermine), boast exquisite and valuable furs, and have been accordingly hunted since prehistoric times. Since the early Middle Ages, the trade in furs was of great economic importance for northern and eastern European nations with large native populations of fur-bearing mustelids, and was a major economic impetus behind Russian expansion into Siberia and French and English expansion in North America. In recent centuries, fur farming, notably of mink, has also become widespread and provides the majority of the fur brought to market.

One species, the sea mink ("Neovison macrodon") of New England and Canada, was driven to extinction by fur trappers. Its appearance and habits are almost unknown today because no complete specimens can be found and no systematic contemporary studies were conducted.

The sea otter, which has the densest fur of any animal, narrowly escaped the fate of the sea mink. The discovery of large populations in the North Pacific was the major economic driving force behind Russian expansion into Kamchatka, the Aleutian Islands, and Alaska, as well as a cause for conflict with Japan and foreign hunters in the Kuril Islands. Together with widespread hunting in California and British Columbia, the species was brought to the brink of extinction until an international moratorium came into effect in 1911.

Today, some mustelids are threatened for other reasons. Sea otters are vulnerable to oil spills and the indirect effects of overfishing; the black-footed ferret, a relative of the European polecat, suffers from the loss of American prairie; and wolverine populations are slowly declining because of habitat destruction and persecution. The rare European mink "Mustela lutreola" is one of the most endangered mustelid species.

One mustelid, the ferret, has been domesticated and is a fairly common pet.

Traditionally the Mustelidae have been divided into two subfamilies: the Lutrinae, comprising the otters, and the Mustelinae, comprising the weasels, martens, badgers and other mustelids. Accordingly, the 57 living species of mustelid have been traditionally classified as follows:

(two subfamilies and 59 living species in 22 genera)


Extinct genera of the family Mustelidae include:


The traditional classification of the family Mustelidae has recently been questioned. A multigene phylogeny constructed by Koepfli "et al" (2008) found that the Mustelidae comprise four major clades and three monotypic lineages. This scheme makes the traditional Mustelinae paraphyletic. The revised Mustelinae is a sister clade to the Lutrinae and together these two subfamilies make up the largest clade. The early mustelids appear to have undergone two rapid bursts of diversification in Eurasia, with the resulting species only spreading to other continents later.

Examination of the mitochondrial DNA suggests that the Taxidiinae diverged first, followed by the Melinae. The position of the Helictidinae is unclear because the mitochondrial evidence suggests the subfamily is related to the Lutrinae-Mustelinae clade, while the intron data suggest a relationship to the Guloninae.

The following cladogram illustrates the eight subfamily scheme of Koepfli "et al" (2008), with additional American species as placed by Harding & Smith (2009).The genetic studies on which the scheme was based did not include the genus "Lyncodon", which is therefore unplaced, but it is probably allied with "Mustela" and "Neovison".


</doc>
<doc id="18858" url="https://en.wikipedia.org/wiki?curid=18858" title="Maryland">
Maryland

Maryland ( ) is a state in the Mid-Atlantic region of the United States, bordering Virginia, West Virginia, and the District of Columbia to its south and west; Pennsylvania to its north; and Delaware to its east. The state's largest city is Baltimore, and its capital is Annapolis. Among its occasional nicknames are "Old Line State", the "Free State", and the "Chesapeake Bay State". The state is named after the English queen Henrietta Maria of France.

One of the original Thirteen Colonies, Maryland is considered the birthplace of religious freedom in America. The state was founded by George Calvert, a trusted foreign minister and personal friend of King James I. When Calvert converted to Catholicism in 1625 it meant his disqualification from holding public office, but his friendship with King James remained. Calvert had had an early interest in the administration of colonial affairs and petitioned James for a charter to provide a religious haven for Catholics persecuted in England as well as to extend the territories of the English Empire. Consequently, in 1632 James’ son, Charles, granted Calvert a charter to settle lands in America held by the Crown, to wit: to "transport ... a numerous Colony of the English Nation" to settle there. Unlike the Pilgrims and Puritans, who began enforcing conformity with their beliefs as soon as they settled in America, Calvert envisioned a colony where people of different religious sects would coexist under the principle of toleration. Some historians believe that Calvert's aspiration towards such a society may have been inspired by the works of Thomas More, most notably the book "Utopia". Accordingly, in 1649 the Maryland General Assembly passed an Act Concerning Religion, which enshrined the principle of toleration by penalizing anyone who "reproached" a fellow Marylander as a "heritick, Scismatick, Idolator, puritan, Independant, Prespiterian popish prest, Jesuite, Jesuited papist, Lutheran, Calvenist, Anabaptist, Brownist, Antinomian, Barrowist, Roundhead [or] Separatist."

Sixteen of Maryland's twenty three counties border on the tidal waters of the Chesapeake Bay estuary and its many tributaries, which combined total more than 4,000 miles of the shoreline. The population is approximately six million residents. , Maryland had the highest median household income of any state, owing in large part to its close proximity to the nation's capital and a highly diversified economy spanning manufacturing, services, and biotechnology.

Maryland has an area of and is comparable in overall area with Belgium (). It is the 42nd largest and 9th smallest state and is closest in size to the state of Hawaii (), the next smallest state. The next largest state, its neighbor West Virginia, is almost twice the size of Maryland ().

Maryland possesses a variety of topography within its borders, contributing to its nickname "America in Miniature". It ranges from sandy dunes dotted with seagrass in the east, to low marshlands teeming with wildlife and large bald cypress near the Chesapeake Bay, to gently rolling hills of oak forests in the Piedmont Region, and pine groves in the Maryland mountains to the west.

Maryland is bounded on its north by Pennsylvania, on its west by West Virginia, on its east by Delaware and the Atlantic Ocean, and on its south, across the Potomac River, by West Virginia and Virginia. The mid-portion of this border is interrupted by District of Columbia, which sits on land that was originally part of Montgomery and Prince George's counties and including the town of Georgetown, Maryland. This land was ceded to the United States Federal Government in 1790 to form the District of Columbia. (The Commonwealth of Virginia gave land south of the Potomac, including the town of Alexandria, Virginia, however Virginia retroceded its portion in 1846). The Chesapeake Bay nearly bisects the state and the counties east of the bay are known collectively as the "Eastern Shore".

Most of the state's waterways are part of the Chesapeake Bay watershed, with the exceptions of a tiny portion of extreme western Garrett County (drained by the Youghiogheny River as part of the watershed of the Mississippi River), the eastern half of Worcester County (which drains into Maryland's Atlantic coastal bays), and a small portion of the state's northeast corner (which drains into the Delaware River watershed). So prominent is the Chesapeake in Maryland's geography and economic life that there has been periodic agitation to change the state's official nickname to the "Bay State", a nickname that has been used by Massachusetts for decades.

The highest point in Maryland, with an elevation of , is Hoye Crest on Backbone Mountain, in the southwest corner of Garrett County, near the border with West Virginia, and near the headwaters of the North Branch of the Potomac River. Close to the small town of Hancock, in western Maryland, about two-thirds of the way across the state, there are between its borders. This geographical curiosity makes Maryland the narrowest state, bordered by the Mason–Dixon line to the north, and the northwards-arching Potomac River to the south.

Portions of Maryland are included in various official and unofficial geographic regions. For example, the Delmarva Peninsula is composed of the Eastern Shore counties of Maryland, the entire state of Delaware, and the two counties that make up the Eastern Shore of Virginia, whereas the westernmost counties of Maryland are considered part of Appalachia. Much of the Baltimore–Washington corridor lies just south of the Piedmont in the Coastal Plain, though it straddles the border between the two regions.

Earthquakes in Maryland are infrequent and small due to the state's distance from seismic/earthquake zones. The M5.8 Virginia earthquake in 2011 was felt moderately throughout Maryland. Buildings in the state are not well-designed for earthquakes and can suffer damage easily.

The lack of any glacial history accounts for the scarcity of Maryland's natural lakes, yet the oft-repeated claim that Maryland is the only state without natural lakes is not true. Laurel Oxbow Lake is an over one-hundred-year-old 55-acre natural lake two miles north of Maryland City and adjacent to Russett.
"Chews Lake" is a seven-acre natural lake two miles south-southeast of Upper Marlboro. There are numerous man-made lakes, the largest of them being the Deep Creek Lake, a reservoir in Garrett County in westernmost Maryland.

Maryland has shale formations containing natural gas, where fracking is theoretically possible.

As is typical of states on the East Coast, Maryland's plant life is abundant and healthy. A good dose of annual precipitation helps to support many types of plants, including seagrass and various reeds at the smaller end of the spectrum to the gigantic Wye Oak, a huge example of white oak, the state tree, which can grow in excess of tall.

Middle Atlantic coastal forests, typical of the southeastern Atlantic coastal plain, grow around Chesapeake Bay and on the Delmarva Peninsula. Moving west, a mixture of Northeastern coastal forests and Southeastern mixed forests cover the central part of the state. The Appalachian Mountains of western Maryland are home to Appalachian-Blue Ridge forests. These give way to Appalachian mixed mesophytic forests near the West Virginia border.

Many foreign species are cultivated in the state, some as ornamentals, others as novelty species. Included among these are the crape myrtle, Italian cypress, southern magnolia, live oak in the warmer parts of the state, and even hardy palm trees in the warmer central and eastern parts of the state. USDA plant hardiness zones in the state range from Zones 5 and 6 in the extreme western part of the state to Zone 7 in the central part, and Zone 8 around the southern part of the coast, the bay area, and parts of metropolitan Baltimore. Invasive plant species, such as kudzu, tree of heaven, multiflora rose, and Japanese stiltgrass, stifle growth of endemic plant life. Maryland's state flower, the black-eyed susan, grows in abundance in wild flower groups throughout the state.

The state harbors a great number of white tailed deer, especially in the woody and mountainous west of the state, and overpopulation can become a problem. Mammals can be found ranging from the mountains in the west to the central areas and include black bears, bobcats, foxes, coyotes, raccoons, and otters.

There is a population of rare wild (feral) horses found on Assateague Island. They are believed to be descended from horses who escaped from shipwrecks. Every year during the last week of July, they are captured and waded across a shallow bay for sale at Chincoteague, Virginia, a conservation technique which ensures the tiny island is not overrun by the horses. The ponies and their sale were popularized by the children's book, "Misty of Chincoteague."

The purebred Chesapeake Bay Retriever dog was bred specifically for water sports, hunting and search and rescue in the Chesapeake area. In 1878 the Chesapeake Bay Retriever was the first individual retriever breed recognized by the American Kennel Club. and was later adopted by the University of Maryland, Baltimore County as their mascot.

Maryland's reptile and amphibian population includes the diamondback terrapin turtle, which was adopted as the mascot of University of Maryland, College Park, as well as the threatened Eastern box turtle 
. The state is part of the territory of the Baltimore oriole, which is the official state bird and mascot of the MLB team the Baltimore Orioles. Aside from the oriole, 435 other species of birds have been reported from Maryland.

The state insect is the Baltimore checkerspot butterfly, although it is not as common in Maryland as it is in the southern edge of its range.

Maryland joined with neighboring states during the end of the 20th century to improve the health of the Chesapeake Bay. The bay's aquatic life and seafood industry have been threatened by development and by fertilizer and livestock waste entering the bay.

In 2007, Forbes.com rated Maryland as the fifth "Greenest" state in the country behind three of the Pacific States and Vermont. Maryland ranks 40th in total energy consumption nationwide, and it managed less toxic waste per capita than all but six states in 2005. In April 2007 Maryland joined the Regional Greenhouse Gas Initiative (RGGI)—a regional initiative formed by all of the Northeastern states, Washington D.C., and three Canadian provinces to reduce greenhouse gas emissions. In March 2017, Maryland became the first state with proven gas reserves to ban fracking by passing a law against it. Vermont has such a law, but no shale gas, and New York has such a ban, though it was made by executive order.

Maryland has a wide array of climates, due to local variances in elevation, proximity to water, and protection from colder weather due to downslope winds.

The eastern half of Maryland—which includes the cities of Ocean City, Salisbury, Annapolis, and the southern and eastern suburbs of Washington, D.C. and Baltimore—lies on the Atlantic Coastal Plain, with flat topography and sandy or muddy soil. This region has a humid subtropical climate (Köppen "Cfa"), with hot, humid summers and a short, mild to cool winter; it falls under USDA Hardiness zone 8a.

The Piedmont region—which includes northern and western greater Baltimore, Westminster, Gaithersburg, Frederick, and Hagerstown—has average seasonal snowfall totals generally exceeding and, as part of USDA Hardiness zones 7b and 7a, temperatures below are less rare. From the Cumberland Valley on westward, the climate begins to transition to a humid continental climate (Köppen "Dfa").

In western Maryland, the higher elevations of Allegany and Garrett counties—including the cities of Cumberland, Frostburg, and Oakland—display more characteristics of the humid continental zone, due in part to elevation. They fall under USDA Hardiness zones 6b and below.

Precipitation in the state is characteristic of the East Coast. Annual rainfall ranges from with more in higher elevations. Nearly every part of Maryland receives per month of rain. Average annual snowfall varies from in the coastal areas to over in the western mountains of the state.

Because of its location near the Atlantic Coast, Maryland is somewhat vulnerable to tropical cyclones, although the Delmarva Peninsula and the outer banks of North Carolina provide a large buffer, such that strikes from major hurricanes (category 3 or above) occur infrequently. More often, Maryland gets the remnants of a tropical system which has already come ashore and released most of its energy. Maryland averages around 30–40 days of thunderstorms a year, and averages around six tornado strikes annually.

George Calvert, 1st Lord Baltimore (1579–1632), sought a charter from King Charles I for the territory between Massachusetts to the north and Virginia to the immediate south.
After the first Lord Baltimore died in April 1632, the charter was granted to his son, Cecilius Calvert, 2nd Baron Baltimore (1605–1675), on June 20, 1632. Officially, the new "Maryland Colony" was named in honor of Henrietta Maria of France, wife of Charles I of England. George Calvert initially proposed the name “Crescentia,” the land of growth or increase, but “the King proposed Terra Mariae [Mary Land], which was concluded on and Inserted in the bill.”

The original capital of Maryland was St. Mary's City, on the north shore of the Potomac River, and the county surrounding it, the first erected/created in the province, was first called Augusta Carolina, after the King, and later named St. Mary’s County.

Lord Baltimore's first settlers arrived in the new colony in March 1634, with his younger brother Leonard Calvert (1606–1647), as first provincial Governor of Maryland. They made their first permanent settlement at St. Mary's City in what is now St. Mary's County. They purchased the site from the paramount chief of the region, who was eager to establish trade. St. Mary's became the first capital of Maryland, and remained so for 60 years until 1695. More settlers soon followed. Their tobacco crops were successful and quickly made the new colony profitable. However, given the incidence of malaria, yellow fever and typhoid, life expectancy in Maryland was about 10 years less than in New England.

Maryland was founded for the purpose of providing religious toleration of England's Roman Catholic minority.
Although Maryland was the most heavily Catholic of the England mainland colonies, this religious group was still in the minority, consisting of less than 10% of the total population.

In 1642 a number of Puritans left Virginia for Maryland and founded Providence (now called Annapolis) on the western shore of the upper Chesapeake Bay. A dispute with traders from Virginia over Kent Island in the Chesapeake led to armed conflict. In 1644 William Claiborne, a Puritan, seized Kent Island while his associate, the pro-Parliament Puritan Richard Ingle, took over St. Mary's. Both used religion as a tool to gain popular support. The two years from 1644–1646 that Claiborne and his Puritan associates held sway were known as "The Plundering Time". They captured Jesuit priests, imprisoned them, then sent them back to England.

In 1646 Leonard Calvert returned with troops, recaptured St. Mary's City, and restored order. The House of Delegates passed the "Act concerning Religion" in 1649 granting religious liberty to all Trinitarian Christians.

In 1650 the Puritans revolted against the proprietary government. "Protestants swept the Catholics out of the legislature ...and religious strife returned".
The Puritans set up a new government prohibiting both Roman Catholicism and Anglicanism. The Puritan revolutionary government persecuted Maryland Catholics during its reign, known as the "plundering time". Mobs burned down all the original Catholic churches of southern Maryland. The Puritan rule lasted until 1658 when the Calvert family and Lord Baltimore regained proprietary control and re-enacted the Toleration Act.

After England's "Glorious Revolution" of 1688, Maryland outlawed Catholicism. In 1704, the Maryland General Assembly prohibited Catholics from operating schools, limited the corporate ownership of property to hamper religious orders from expanding or supporting themselves, and encouraged the conversion of Catholic children. The celebration of the Catholic sacraments was also officially restricted. This state of affairs lasted until after the American Revolutionary War (1775–1783). Wealthy Catholic planters built chapels on their land to practice their religion in relative secrecy.

Into the 18th century, individual priests and lay leaders claimed Maryland farms belonging to the Jesuits as personal property and bequeathed these and other properties to other religious or lay people in order to evade the legal restrictions on religious organizations – such as the Society of Jesus – owning property.

The royal charter granted Maryland the land north of the Potomac River up to the 40th parallel. A problem arose when Charles II granted a charter for Pennsylvania. The grant defined Pennsylvania's southern border as identical to Maryland's northern border, the 40th parallel. But the grant indicated that Charles II and William Penn assumed the 40th parallel would pass close to New Castle, Delaware when it falls north of Philadelphia, the site of which Penn had already selected for his colony's capital city. Negotiations ensued after the problem was discovered in 1681.

A compromise proposed by Charles II in 1682 was undermined by Penn's receiving the additional grant of what is now Delaware. Penn successfully argued that the Maryland charter entitled Lord Baltimore only to unsettled lands, and Dutch settlement in Delaware predated his charter. The dispute remained unresolved for nearly a century, carried on by the descendants of William Penn and Lord Baltimore — the Calvert family, which controlled Maryland, and the Penn family, which controlled Pennsylvania.

The border dispute with Pennsylvania led to Cresap's War in the 1730s. Hostilities erupted in 1730 and escalated through the first half of the decade, culminating in the deployment of military forces by Maryland in 1736 and by Pennsylvania in 1737. The armed phase of the conflict ended in May 1738 with the intervention of King George II, who compelled the negotiation of a cease-fire. A provisional agreement had been established in 1732.

Negotiations continued until a final agreement was signed in 1760. The agreement defined the border between Maryland and Pennsylvania as the line of latitude now known as the Mason–Dixon line. Maryland's border with Delaware was based on a Transpeninsular Line and the Twelve-Mile Circle around New Castle.

Most of the English colonists arrived in Maryland as indentured servants, and had to serve a several years' term as laborers to pay for their passage. In the early years, the line between indentured servants and African slaves or laborers was fluid, and white and black laborers commonly lived and worked together, and formed unions. Mixed-race children born to white mothers were considered free by the principle of "partus sequitur ventrem", by which children took the social status of their mothers, a principle of slave law that was adopted throughout the colonies, following Virginia in 1662. During the colonial era, families of free people of color were formed most often by unions of white women and African men.
Many of the free black families migrated to Delaware, where land was cheaper. As the flow of indentured laborers to the colony decreased with improving economic conditions in England, planters in Maryland imported thousands more slaves and racial caste lines hardened. The economy's growth and prosperity was based on slave labor, devoted first to the production of tobacco as the commodity crop.

Maryland was one of the thirteen colonies that revolted against British rule in the American Revolution. Near the end of the American Revolutionary War (1775–1783), on February 2, 1781, Maryland became the last and 13th state to approve the ratification of the Articles of Confederation and Perpetual Union, first proposed in 1776 and adopted by the Second Continental Congress in 1778, which brought into being the United States as a united, sovereign and national state. It also became the seventh state admitted to the Union after ratifying the new federal Constitution in 1788. In December 1790, Maryland donated land selected by first President George Washington to the federal government for the creation of the new national capital of Washington, D.C. The land was provided along the north shore of the Potomac River from Montgomery and Prince George's counties, as well as from Fairfax County and Alexandria on the south shore of the Potomac in Virginia; however, the land donated by the Commonwealth of Virginia was later returned to that state by the District of Columbia retrocession in 1846.

Influenced by a changing economy, revolutionary ideals, and preaching by ministers, numerous planters in Maryland freed their slaves in the 20 years after the Revolutionary War. Across the Upper South the free black population increased from less than 1% before the war to 14% by 1810.

During the War of 1812, the British military attempted to capture Baltimore, which was protected by Fort McHenry. During this bombardment the song "Star Spangled Banner" was written by Francis Scott Key; it was later adopted as the national anthem.

The National Road (U.S. Hwy 40 today) was authorized in 1817 and ran from Baltimore to St. Louis – the first federal highway.
The Baltimore and Ohio Railroad (B&O) was the first chartered railroad in the United States. It opened its first section of track for regular operation in 1830 between Baltimore and Ellicott City, and in 1852 it became the first rail line to reach the Ohio River from the eastern seaboard.

The state remained with the Union during the Civil War, due in significant part to demographics and Federal intervention. The 1860 census, held shortly before the outbreak of the civil war, showed that 49% of Maryland's African Americans were free blacks.

Governor Thomas Holliday Hicks suspended the state legislature, and to help ensure the election of a new pro-union governor and legislature, President Abraham Lincoln had a number of its pro-slavery politicians arrested, including the Mayor of Baltimore, George William Brown; suspended several civil liberties, including "habeas corpus"; and ordered artillery placed on Federal Hill overlooking Baltimore. Historians debate the constitutionality of these wartime actions, and the suspension of civil liberties was later deemed illegal by the U.S. Supreme Court.

In April 1861 Federal units and state regiments were attacked as they marched through Baltimore, sparking the Baltimore riot of 1861, the first bloodshed in the Civil War. Of the 115,000 men from Maryland who joined the military during the Civil War, 85,000, or 77%, joined the Union army, while the remainder joined the Confederate Army. The largest and most significant battle in the state was the Battle of Antietam on September 17, 1862, near Sharpsburg. Although a tactical draw, the battle was considered a strategic Union victory and a turning point of the war.

A new state constitution in 1864 abolished slavery and Maryland was first recognized as a "Free State" in that context. Following passage of constitutional amendments that granted voting rights to freedmen, in 1867 the state extended suffrage to non-white males.

The Democratic Party rapidly regained power in the state from Republicans. Democrats replaced the Constitution of 1864 with the Constitution of 1867. Following the end of Reconstruction in 1877, Democrats devised means of disfranchising blacks, initially by physical intimidation and voter fraud, later by constitutional amendments and laws.
Blacks and immigrants, however, resisted Democratic Party disfranchisement efforts in the state. Maryland blacks were part of a biracial Republican coalition elected to state government in 1896–1904 and comprised 20% of the electorate.

Compared to some other states, blacks were better established both before and after the civil war. Nearly half the black population was free before the war, and some had accumulated property. Half the population lived in cities. Literacy was high among blacks and, as Democrats crafted means to exclude them, suffrage campaigns helped reach blacks and teach them how to resist. Whites did impose racial segregation in public facilities and Jim Crow laws, which effectively lasted until passage of federal civil rights legislation in the mid-1960s.

Baltimore grew significantly during the Industrial Revolution, due in large part to its seaport and good railroad connections, attracting European immigrant labor. Many manufacturing businesses were established in the Baltimore area after the Civil War. Baltimore businessmen, including Johns Hopkins, Enoch Pratt, George Peabody, and Henry Walters, founded notable city institutions that bear their names, including a university, library, music school and art museum.

Cumberland was Maryland's second-largest city in the 19th century. Nearby supplies of natural resources along with railroads fostered its growth into a major manufacturing center.

The Progressive Era of the late 19th and early 20th centuries brought political reforms. In a series of laws passed between 1892 and 1908, reformers worked for standard state-issued ballots (rather than those distributed and marked by the parties); obtained closed voting booths to prevent party workers from "assisting" voters; initiated primary elections to keep party bosses from selecting candidates; and had candidates listed without party symbols, which discouraged the illiterate from participating. These measures worked against ill-educated whites and blacks. Blacks resisted such efforts, with suffrage groups conducting voter education.
Blacks defeated three efforts to disfranchise them, making alliances with immigrants to resist various Democratic campaigns. Disfranchising bills in 1905, 1907, and 1911 were rebuffed, in large part because of black opposition. Blacks comprised 20% of the electorate and immigrants comprised 15%, and the legislature had difficulty devising requirements against blacks that did not also disadvantage immigrants.

The Progressive Era also brought reforms in working conditions for Maryland's labor force. In 1902 the state regulated conditions in mines; outlawed child laborers under the age of 12; mandated compulsory school attendance; and enacted the nation's first workers' compensation law. The workers' compensation law was overturned in the courts, but was redrafted and finally enacted in 1910.

The Great Baltimore Fire of 1904 burned for more than 30 hours, destroying 1,526 buildings and spanning 70 city blocks. More than 1,231 firefighters worked to bring the blaze under control.

With the nation's entry into World War I in 1917, new military bases such as Camp Meade, the Aberdeen Proving Ground, and the Edgewood Arsenal were established. Existing facilities, including Fort McHenry, were greatly expanded.

After Georgia congressman William D. Upshaw criticized Maryland openly in 1923 for not passing Prohibition laws, "Baltimore Sun" editor Hamilton Owens coined the "Free State" nickname for Maryland in that context, which was popularized by H. L. Mencken in a series of newspaper editorials.

Maryland's urban and rural communities had different experiences during the Great Depression. The "Bonus Army" marched through the state in 1932 on its way to Washington, D.C. Maryland instituted its first ever income tax in 1937 to generate revenue for schools and welfare.
Baltimore was a major war production center during World War II. The biggest operations were Bethlehem Steel's Fairfield Yard, which built Liberty ships; and Glenn Martin, an aircraft manufacturer.

Maryland experienced population growth following World War II, particularly in the Baltimore and Washington, D.C. suburbs. Agricultural tracts gave way to residential communities such as Columbia and Montgomery Village. Concurrently the Interstate Highway System was built throughout the state, most notably I-95 and the Capital Beltway, altering travel patterns. In 1952 the eastern and western halves of Maryland were linked for the first time by the Chesapeake Bay Bridge, which replaced a nearby ferry service.

Maryland's regions experienced economic changes following WWII. Heavy manufacturing declined in Baltimore. In Maryland's four westernmost counties, industrial, railroad, and coal mining jobs declined. On the lower Eastern Shore, family farms were bought up by major concerns and large-scale poultry farms and vegetable farming became prevalent. In Southern Maryland, tobacco farming nearly vanished due to suburban development and a state tobacco buy-out program.

In an effort to reverse depopulation due to the loss of working-class industries, Baltimore initiated urban renewal projects in the 1960s with Charles Center and the Baltimore World Trade Center. Some resulted in the break-up of intact residential neighborhoods, producing social volatility, and some older residential areas around the harbor have had units renovated and have become popular with new populations.

The United States Census Bureau estimates that the population of Maryland was 6,006,401 on July 1, 2015, a 4.03% increase since the 2010 United States Census.

In 2015 Maryland had an estimated population of 6,006,401, which is an increase of 29,994, from the prior year and an increase of 232,849, or 4.03% percent, since 2010. This includes a natural increase since the last census of 189,158 people (that is 464,251 births minus 275,093 deaths) and an increase due to net migration of 116,713 people into the state. Immigration from outside the United States resulted in a net increase of 129,730 people, and migration within the country produced a net loss of 13,017 people.

The center of population of Maryland is located on the county line between Anne Arundel County and Howard County, in the unincorporated community of Jessup.

Maryland's history as a border state has led it to exhibit characteristics of both the Northern and Southern regions of the United States. Generally, rural Western Maryland between the West Virginian Panhandle and Pennsylvania has an Appalachian culture; the Southern and Eastern Shore regions of Maryland embody a Southern culture,
while densely populated Central Maryland—radiating outward from Baltimore and Washington, D.C.—has more in common with that of the Northeast.
The U.S. Census Bureau designates Maryland as one of the South Atlantic States, but it is commonly associated with the Mid-Atlantic States and/or Northeastern United States by other federal agencies, the media, and some residents.

As of 2011, 58.0 percent of Maryland's population younger than age 1 were non-white.

"Note: Births in table don't add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number."

Spanish (including Spanish Creole) is the second-most-spoken language in Maryland, after English. The third- and fourth-most-spoken languages are French (including Patois and Cajun) and Chinese. Other commonly spoken languages include various African languages, Korean, German, Tagalog, Russian, Vietnamese, Italian, various Asian languages, Persian, Hindi and other Indic languages, Greek and Arabic.

Most of the population of Maryland lives in the central region of the state, in the Baltimore Metropolitan Area and Washington Metropolitan Area, both of which are part of the Baltimore-Washington Metropolitan Area.
The majority of Maryland's population is concentrated in the cities and suburbs surrounding Washington, D.C., as well as in and around Maryland's most populous city, Baltimore. Historically, these and many other Maryland cities developed along the Fall Line, the line along which rivers, brooks, and streams are interrupted by rapids and/or waterfalls. Maryland's capital city, Annapolis, is one exception to this pattern, since it lies along the banks of the Severn River, close to where it empties into the Chesapeake Bay.

The Eastern Shore is less populous and more rural, as are the counties of western Maryland. The two westernmost counties of Maryland, Allegany and Garrett, are mountainous and sparsely populated, resembling West Virginia and Appalachia more than they do the rest of Maryland. Both eastern and western Maryland are, however, dotted with cities of regional importance, such as Ocean City, Princess Anne, and Salisbury on the Eastern Shore and Cumberland, Frostburg, and Hancock in Western Maryland.
Southern Maryland is still somewhat rural, but suburbanization from Washington, D.C. has encroached significantly since the 1960s; important local population centers include Lexington Park, Prince Frederick, and Waldorf.

In 1970 the Census Bureau reported Maryland's population as 17.8 percent African-American and 80.4 percent non-Hispanic White.

African Americans form a sizable portion of the state's population – nearly 30 percent in 2010. Most are descendants of people transported to the area as slaves from West Africa, and many are of mixed race, including European and Native American ancestry.
New residents of African descent include 20th-century and later immigrants from Nigeria, particularly of the Igbo and Yoruba tribes. Concentrations of African Americans live in Baltimore City, Prince George's County, a suburb of Washington, D.C., where many work; Charles County, western parts of Baltimore County, and the southern Eastern Shore.

The top reported ancestries by Maryland residents are: German (15%), Irish (11%), English (8%), American (7%), Italian (6%), and Polish (3%).

Irish American populations can be found throughout the Baltimore area, and the Northern and Eastern suburbs of Washington D.C. in Maryland (descendants of those who moved out to the suburbs of Washington's once predominantly Irish neighborhoods), as well as Western Maryland, where Irish immigrant laborers helped to build the B & O Railroad. Smaller but much older Irish populations can be found in Southern Maryland, with some roots dating as far back as the early Maryland colony. This population, however, still remains culturally very active and yearly festivals are held.

A large percentage of the population of the Eastern Shore and Southern Maryland are descendants of British American ancestry. The Eastern Shore was settled by Protestants, chiefly Methodist and the southern counties were initially settled by English Catholics. Western and northern Maryland have large German-American populations. More recent European immigrants of the late 19th and early 20th century settled first in Baltimore, attracted to its industrial jobs. Many of their ethnic Italian, Polish, Czech, Lithuanian, and Greek descendants still live in the area.

Large ethnic minorities include Eastern Europeans such as Croatians, Belarusians, Russians and Ukrainians. The shares of European immigrants born in Eastern Europe increased significantly between 1990 and 2010. Following the dissolution of the Soviet Union, Yugoslavia, and Czechoslovakia, many immigrants from Eastern Europe came to the United States - 12 percent of which currently reside in Maryland.

Hispanic immigrants of the later 20th century have settled in Aspen Hill, Hyattsville/Langley Park, Glenmont/Wheaton, Bladensburg, Riverdale Park, Gaithersburg, as well as Highlandtown and Greektown in East Baltimore. Salvadorans are the largest Hispanic group in Maryland. Other Hispanic groups with significant populations in the state include Mexicans and Puerto Ricans and Hondurans. Though the Salvadoran population is more concentrated in the area around Washington, D.C., and the Puerto Rican population is more concentrated in the Baltimore area, all other major Hispanic groups in the state are evenly dispersed between these two areas. Maryland has one of the most diverse Hispanic populations in the country, with significant populations from various Caribbean and Central American nations.

Jews are numerous throughout Montgomery County and in Pikesville and Owings Mills northwest of Baltimore. Asian Americans are concentrated in the suburban counties surrounding Washington, D.C. and in Howard County, with Korean American and Taiwanese American communities in Rockville, Gaithersburg, and Germantown and a Filipino American community in Fort Washington. Numerous Indian Americans live across the state, especially in central Maryland. Amish/Mennonite communities are found in St. Mary's, Garrett, and Cecil counties.

Attracting educated Asians and Africans to the professional jobs in the region, Maryland has the fifth-largest proportions of racial minorities in the country.

In 2006 645,744 were counted as foreign born, which represents mainly people from Latin America and Asia. About 4.0 percent are undocumented immigrants. Maryland also has a large Korean American population. In fact, 1.7 percent are Korean, while as a whole, almost 6.0 percent are Asian.

According to The Williams Institute's analysis of the 2010 U.S. Census, 12,538 same-sex couples are living in Maryland, representing 5.8 same-sex couples per 1,000 households.

As of 2016, non-Hispanic white Americans were 51.5% of Maryland's population, making Maryland on the verge of becoming a majority minority state. 48.5% of Maryland's population is non-white and/or Hispanic/Latino, the highest percentage of any state on the East Coast and the highest percentage after the majority minority states of Hawaii, New Mexico, Texas, California and Nevada. Non-Hispanic White Americans in Maryland, the majority as of 2016, are expected to become the plurality ethnic group within 5 years of 2015. After Nevada in 2016, Maryland is projected to be the next state to become majority minority due to growing African-American, Asian and Latino populations. By 2031, minorities are projected to become the majority of voting eligible residents of Maryland.

Maryland has been historically prominent to American Catholic tradition because the English colony of Maryland was intended by George Calvert as a haven for English Catholics. Baltimore was the seat of the first Catholic bishop in the U.S. (1789), and Emmitsburg was the home and burial place of the first American-born citizen to be canonized, St. Elizabeth Ann Seton. Georgetown University, the first Catholic University, was founded in 1789 in what was then part of Maryland. The Basilica of the National Shrine of the Assumption of the Virgin Mary in Baltimore was the first Roman Catholic cathedral built in the United States, and the Archbishop of Baltimore is, albeit without formal primacy, the United States' quasi-primate, and often a cardinal. Among the immigrants of the 19th and 20th century from eastern and southern Europe were many Catholics.

Despite its historic relevance to the Catholic Church in the United States, the percentage of Catholics in the state of Maryland is below the national average of 20%. Demographically, both Protestants and those identifying no religion are more numerous than Catholics.

According to Pew research 69 percent of Maryland's population identifies as Christian. The largest religious groups in Maryland as of 2010 were: the Catholic Church with 837,338 adherents in Maryland, followed by non-denominational Evangelical Protestants with 298,921 members, and the United Methodist Church with 238,774. The Southern Baptist Convention has 150,345 members.
Judaism is the largest non-Christian religion in Maryland with 241,000 adherents, or 4 percent of the total population.
The Seventh-day Adventist Church's World Headquarters and Ahmadiyya Muslims national Headquarters is located in Silver Spring, just outside the District of Columbia.

The Bureau of Economic Analysis estimates that Maryland's gross state product in 2016 was $382.4 billion. However, Maryland has been using Genuine Progress Indicator, an indicator of well-being, to guide the state's development, rather than relying only on growth indicators like GDP. According to the U.S. Census Bureau, Maryland households are currently the wealthiest in the country, with a 2013 median household income of $72,483 which puts it ahead of New Jersey and Connecticut, which are second and third respectively. Two of Maryland's counties, Howard and Montgomery, are the second and eleventh wealthiest counties in the nation respectively. Maryland ranked No. 1 with the most millionaires per capita in 2013, with a ratio of 7.7 percent. Also, the state's poverty rate of 7.8 percent is the lowest in the country. per capita personal income in 2006 was $43,500, fifth in the nation. As of February 2018, the state's unemployment rate was 4.2 percent.

Maryland's economy benefits from the state's close proximity to the federal government in Washington, D.C. with an emphasis on technical and administrative tasks for the defense/aerospace industry and bio-research laboratories, as well as staffing of satellite government headquarters in the suburban or exurban Baltimore/Washington area. Ft. Meade serves as the headquarters of the Defense Information Systems Agency, United States Cyber Command, and the National Security Agency/Central Security Service. In addition, a number of educational and medical research institutions are located in the state. In fact, the various components of The Johns Hopkins University and its medical research facilities are now the largest single employer in the Baltimore area. Altogether, white collar technical and administrative workers comprise 25 percent of Maryland's labor force, attributable in part to nearby Maryland being a part of the Washington Metro Area where the federal government office employment is relatively high.

Manufacturing, while large in dollar value, is highly diversified with no sub-sector contributing over 20 percent of the total. Typical forms of manufacturing include electronics, computer equipment, and chemicals. The once mighty primary metals sub-sector, which at one time included what was then the largest steel factory in the world at Sparrows Point, still exists, but is pressed with foreign competition, bankruptcies, and mergers. During World War II the Glenn Martin Company (now part of Lockheed Martin) airplane factory employed some 40,000 people.

Mining other than construction materials is virtually limited to coal, which is located in the mountainous western part of the state. The brownstone quarries in the east, which gave Baltimore and Washington much of their characteristic architecture in the mid-19th century, were once a predominant natural resource. Historically, there used to be small gold-mining operations in Maryland, some near Washington, but these no longer exist.

One major service activity is transportation, centered on the Port of Baltimore and its related rail and trucking access. The port ranked 17th in the U.S. by tonnage in 2008. Although the port handles a wide variety of products, the most typical imports are raw materials and bulk commodities, such as iron ore, petroleum, sugar, and fertilizers, often distributed to the relatively close manufacturing centers of the inland Midwest via good overland transportation. The port also receives several different brands of imported motor vehicles and is the number one auto port in the U.S.

Baltimore City is the eighth largest port in the nation, and was at the center of the February 2006 controversy over the Dubai Ports World deal because it was considered to be of such strategic importance. The state as a whole is heavily industrialized, with a booming economy and influential technology centers. Its computer industries are some of the most sophisticated in the United States, and the federal government has invested heavily in the area. Maryland is home to several large military bases and scores of high level government jobs.

The Chesapeake and Delaware Canal is a canal on the Eastern Shore that connects the waters of the Delaware River with those of the Chesapeake Bay, and in particular with the Port of Baltimore, carrying 40 percent of the port's ship traffic.

Maryland has a large food-production sector. A large component of this is commercial fishing, centered in the Chesapeake Bay, but also including activity off the short Atlantic seacoast. The largest catches by species are the blue crab, oysters, striped bass, and menhaden. The Bay also has overwintering waterfowl in its wildlife refuges. The waterfowl support a tourism sector of sportsmen.
Maryland has large areas of fertile agricultural land in its coastal and Piedmont zones, though this land use is being encroached upon by urbanization. Agriculture is oriented to dairy farming (especially in foothill and piedmont areas) for nearby large city milksheads plus specialty perishable horticulture crops, such as cucumbers, watermelons, sweet corn, tomatoes, muskmelons, squash, and peas (Source:USDA Crop Profiles). In addition, the southern counties of the western shoreline of Chesapeake Bay are warm enough to support a tobacco cash crop zone, which has existed since early Colonial times but declined greatly after a state government buyout in the 1990s. There is also a large automated chicken-farming sector in the state's southeastern part; Salisbury is home to Perdue Farms. Maryland's food-processing plants are the most significant type of manufacturing by value in the state.

Maryland is a major center for life sciences research and development. With more than 400 biotechnology companies located there, Maryland is the fourth-largest nexus in this field in the United States.

Institutions and government agencies with an interest in research and development located in Maryland include the Johns Hopkins University, the Johns Hopkins Applied Physics Laboratory, more than one campus of the University System of Maryland, Goddard Space Flight Center, the United States Census Bureau, the National Institutes of Health (NIH), the National Institute of Standards and Technology (NIST), the National Institute of Mental Health (NIMH), the Walter Reed National Military Medical Center, the federal Food and Drug Administration (FDA), the Howard Hughes Medical Institute, the Celera Genomics company, the J. Craig Venter Institute (JCVI), and MedImmune – recently purchased by AstraZeneca.

Maryland is home to defense contractor Emergent BioSolutions, which manufactures and provides an anthrax vaccine to U.S. government military personnel.

Tourism is popular in Maryland, with tourists visiting the city of Baltimore, the beaches of the Eastern Shore, and the nature of western Maryland, as well as many passing through en route to Washington, D.C. Baltimore attractions include the Harborplace, the Baltimore Aquarium, Fort McHenry, as well as the Camden Yards baseball stadium.
Ocean City on the Atlantic Coast has been a popular beach destination in summer, particularly since the Chesapeake Bay Bridge was built in 1952 connecting the Eastern Shore to the more populated Maryland cities. The state capital of Annapolis offers sites such as the state capitol building, the historic district, and the waterfront.
Maryland also has several sites of interest to military history, given Maryland's role in the American Civil War and in the War of 1812. Other attractions include the historic and picturesque towns along the Chesapeake Bay, such as Saint Mary's, Maryland's first colonial settlement and original capital.

The Maryland Department of Transportation oversees most transportation in the state through its various administration-level agencies. The independent Maryland Transportation Authority maintains and operates the state's eight toll facilities.

Maryland's Interstate highways include of Interstate 95 (I-95), which enters the northeast portion of the state, travels through Baltimore, and becomes part of the eastern section of the Capital Beltway to the Woodrow Wilson Bridge. I-68 travels , connecting the western portions of the state to I-70 at the small town of Hancock. I-70 enters from Pennsylvania north of Hancock and continues east for to Baltimore, connecting Hagerstown and Frederick along the way.

I-83 has in Maryland and connects Baltimore to southern central Pennsylvania (Harrisburg and York, Pennsylvania). Maryland also has an portion of I-81 that travels through the state near Hagerstown. I-97, fully contained within Anne Arundel County and the second shortest () one- or two-digit interstate highway in the US, connects the Baltimore area to the Annapolis area. (Hawaii has one that is shorter.)

There are also several auxiliary Interstate highways in Maryland. Among them are two beltways encircling the major cities of the region: I-695, the McKeldin (Baltimore) Beltway, which encircles Baltimore; and a portion of I-495, the Capital Beltway, which encircles Washington, D.C. I-270, which connects the Frederick area with Northern Virginia and the District of Columbia through major suburbs to the northwest of Washington, is a major commuter route and is as wide as fourteen lanes at points.

Both I-270 and the Capital Beltway were extremely congested; however, the Intercounty Connector (ICC; MD 200) has alleviated some of the congestion over time. Construction of the ICC was a major part of the campaign platform of former Governor Robert Ehrlich, who was in office from 2003 until 2007, and of Governor Martin O'Malley, who succeeded him. I-595, which is an unsigned highway concurrent with US 50/US 301, is the longest unsigned interstate in the country and connects Prince George's County and Washington D.C. with Annapolis and the Eastern Shore via the Chesapeake Bay Bridge.

Maryland also has a state highway system that contains routes numbered from 2 through 999, however most of the higher-numbered routes are either unsigned or are relatively short. Major state highways include Routes 2 (Governor Ritchie Highway/Solomons Island Road/Southern Maryland Blvd.), 4 (Pennsylvania Avenue/Southern Maryland Blvd./Patuxent Beach Road/St. Andrew's Church Road), 5 (Branch Avenue/Leonardtown Road/Point Lookout Road), 32, 45 (York Road), 97 (Georgia Avenue), 100 (Paul T. Pitcher Memorial Highway), 210 (Indian Head Highway), 235 (Three Notch Road), 295 (Baltimore-Washington Parkway), 355 (Wisconsin Avenue/Rockville Pike/Frederick Road), 404 (Queen Anne Highway/ Shore Highway), and 650 (New Hampshire Avenue).

Maryland's largest airport is Baltimore-Washington International Thurgood Marshall Airport, more commonly referred to as BWI. The airport is named for the Baltimore-born Thurgood Marshall, the first African-American Supreme Court justice. The only other airports with commercial service are at Hagerstown and Salisbury.

The Maryland suburbs of Washington, D.C. are also served by the other two airports in the region, Ronald Reagan Washington National Airport and Dulles International Airport, both in Northern Virginia. The College Park Airport is the nation's oldest, founded in 1909, and is still used. Wilbur Wright trained military aviators at this location.

Amtrak trains, including the high speed Acela Express serve Baltimore's Penn Station, BWI Airport, New Carrollton, and Aberdeen along the Washington D.C. to Boston Northeast Corridor. In addition, train service is provided to Rockville and Cumberland by Amtrak's Washington, D.C., to Chicago Capitol Limited.
The WMATA's Metrorail rapid transit and Metrobus local bus systems (the 2nd and 6th busiest in the nation of their respective modes) provide service in Montgomery and Prince George's counties and connect them to Washington D.C., with the express Metrobus "Route B30" serving BWI Airport. The Maryland Transit Administration (often abbreviated as "MTA Maryland"), a state agency part of the Maryland Department of Transportation also provides transit services within the state. Headquartered in Baltimore, MTA's transit services are largely focused on central Maryland, as well as some portions of the Eastern Shore and Southern MD. Baltimore's Light Rail and Metro Subway systems serve its densely populated inner-city and the surrounding suburbs. The MTA also serves the city and its suburbs with its local bus service (the 9th largest system in the nation). The MTA's Commuter Bus system provides express coach service on longer routes connecting Washington D.C. and Baltimore to parts of Central and Southern MD as well as the Eastern Shore. The commuter rail service, known as MARC, operates three lines which all terminate at Washington Union Station and provide service to Baltimore's Penn and Camden stations, Perryville, Frederick, and Martinsburg, WV. In addition, many suburban counties operate their own local bus systems which connect to and complement the larger MTA and WMATA/Metro services.

Freight rail transport is handled principally by two Class I railroads, as well as several smaller regional and local carriers. CSX Transportation has more extensive trackage throughout the state, with , followed by Norfolk Southern Railway. Major rail yards are located in Baltimore and Cumberland, with an intermodal terminal (rail, truck and marine) in Baltimore.

The government of Maryland is conducted according to the state constitution. The government of Maryland, like the other 49 state governments, has exclusive authority over matters that lie entirely within the state's borders, except as limited by the Constitution of the United States.

Power in Maryland is divided among three branches of government: executive, legislative, and judicial. The Maryland General Assembly is composed of the Maryland House of Delegates and the Maryland Senate. Maryland's governor is unique in the United States as the office is vested with significant authority in budgeting. The legislature may not increase the governor's proposed budget expenditures. Unlike many other states, significant autonomy is granted to many of Maryland's counties.

Most of the business of government is conducted in Annapolis, the state capital. Elections for governor and most statewide offices, as well as most county elections, are held in midterm-election years (even-numbered years not divisible by four).

The judicial branch of state government consists of one united District Court of Maryland that sits in every county and Baltimore City, as well as 24 Circuit Courts sitting in each County and Baltimore City, the latter being courts of general jurisdiction for all civil disputes over $30,000.00, all equitable jurisdiction and major criminal proceedings. The intermediate appellate court is known as the Court of Special Appeals and the state supreme court is the Court of Appeals. The appearance of the judges of the Maryland Court of Appeals is unique; Maryland is the only state whose judges wear red robes.

Maryland imposes five income tax brackets, ranging from 2 to 6.25 percent of personal income. The city of Baltimore and Maryland's 23 counties levy local "piggyback" income taxes at rates between 1.25 and 3.2 percent of Maryland taxable income. Local officials set the rates and the revenue is returned to the local governments quarterly. The top income tax bracket of 9.45 percent is the fifth highest combined state and local income tax rates in the country, behind New York City's 11.35 percent, California's 10.3 percent, Rhode Island's 9.9 percent, and Vermont's 9.5 percent.

Maryland's state sales tax is 6 percent. All real property in Maryland is subject to the property tax. Generally, properties that are owned and used by religious, charitable, or educational organizations or property owned by the federal, state or local governments are exempt. Property tax rates vary widely. No restrictions or limitations on property taxes are imposed by the state, meaning cities and counties can set tax rates at the level they deem necessary to fund governmental services.

Since before the Civil War, Maryland's elections have been largely controlled by the Democrats, which account for 54.9% of all registered voters as of May 2017.

State elections are dominated by Baltimore and the populous suburban counties bordering Washington, D.C. and Baltimore: Montgomery, Prince George's, Anne Arundel, and Baltimore counties. As of July 2017, sixty-six percent of the state's population resides in these six jurisdictions, most of which contain large, traditionally Democratic voting bloc(s): African Americans in Baltimore City and Prince George's, federal employees in Prince George's, Anne Arundel, and Montgomery, and postgraduates in Montgomery. The remainder of the state, particularly Western Maryland and the Eastern Shore, is more supportive of Republicans. One of Maryland's best known political figures is a Republican – former Governor Spiro Agnew, who served under President Richard Nixon as the U.S. Vice President from 1969 to 1973, when he resigned in the aftermath of findings that he had taken bribes while he was Governor of Maryland. Though claiming innocence, Agnew negotiated a plea bargain and appeared before the federal court in Baltimore in October 1973, where he pled no contest to one tax evasion felony charge and submitted his letter of resignation.

In 1980, Maryland was one of six states to vote for Jimmy Carter. In 1992, Bill Clinton fared better in Maryland than any other state except his home state of Arkansas. In 1996, Maryland was Clinton's sixth best; in 2000, Maryland ranked fourth for Gore; and in 2004, John Kerry showed his fifth-best performance in Maryland. In 2008, Barack Obama won the state's 10 electoral votes with 61.9 percent of the vote to John McCain's 36.5 percent.

In 2002, former Governor Robert Ehrlich was the first Republican to be elected to that office in four decades, and after one term lost his seat to Baltimore Mayor and Democrat Martin O'Malley. Ehrlich ran again for governor in 2010, losing again to O'Malley.

The 2006 election brought no change in the pattern of Democratic dominance. After Democratic Senator Paul Sarbanes announced that he was retiring, Democratic Congressman Benjamin Cardin defeated Republican Lieutenant Governor Michael S. Steele, with 55 percent of the vote, against Steele's 44 percent.

While Republicans usually win more counties, by piling up large margins in the west and east, they are also usually swamped by the more densely populated and heavily Democratic Baltimore–Washington axis. In 2008, for instance, McCain won 17 counties to Obama's six; Obama also carried Baltimore City. While McCain won most of the western and eastern counties by margins of 2-to-1 or more, he was almost completely shut out in the larger counties surrounding Baltimore and Washington; every large county except Anne Arundel went for Obama.

From 2007 to 2011, U.S. Congressman Steny Hoyer (MD-5), a Democrat, was elected as Majority Leader for the 110th Congress of the House of Representatives, and 111th Congress, serving in that post. His district covers parts of Anne Arundel and Prince George's counties, in addition to all of Charles, Calvert and St. Mary's counties in southern Maryland.

In 2010, Republicans won control of most counties. The Democratic Party remained in control of eight county governments including Baltimore City.

In 2014, Larry Hogan, a Republican, was elected Governor of Maryland. Hogan is the second Republican to become the Governor of Maryland after Spiro Agnew, who resigned in 1969 to become Vice President.

A well known newspaper is The Baltimore Sun.

Education Week ranked Maryland #1 in its nationwide 2009–2013 Quality Counts reports. The College Board's 9th Annual AP Report to the Nation also ranked Maryland first. Primary and secondary education in Maryland is overseen by the Maryland State Department of Education, which is headquartered in Baltimore. The highest educational official in the state is the State Superintendent of Schools, who is appointed by the State Board of Education to a four-year term of office. The Maryland General Assembly has given the Superintendent and State Board autonomy to make educationally related decisions, limiting its own influence on the day-to-day functions of public education. Each county and county-equivalent in Maryland has a local Board of Education charged with running the public schools in that particular jurisdiction.

The budget for education was $5.5 billion in 2009, representing about 40 percent of the state's general fund.

Maryland has a broad range of private primary and secondary schools. Many of these are affiliated with various religious sects, including parochial schools of the Catholic Church, Quaker schools, Seventh-day Adventist schools, and Jewish schools. In 2003, Maryland law was changed to allow for the creation of publicly funded charter schools, although the charter schools must be approved by their local Board of Education and are not exempt from state laws on education, including collective bargaining laws.

In 2008, the state led the entire country in the percentage of students passing Advanced Placement examinations. 23.4 percent of students earned passing grades on the AP tests given in May 2008. This marks the first year that Maryland earned this honor. Three Maryland high schools (in Montgomery County) were ranked among the top 100 in the country by US News in 2009, based in large part on AP test scores.

Maryland has several historic and renowned private colleges and universities, the most prominent of which is Johns Hopkins University, founded in 1876 with a grant from Baltimore entrepreneur Johns Hopkins.

The first public university in the state is the University of Maryland, Baltimore, which was founded in 1807 and contains the University of Maryland's only public academic health, human services, and one of two law centers (the other being the University of Baltimore School of Law). Seven professional and graduate schools train the majority of the state's physicians, nurses, dentists, lawyers, social workers, and pharmacists. The flagship university and largest undergraduate institution in Maryland is the University of Maryland, College Park which was founded as the Maryland Agricultural College in 1856 and became a public land grant college in 1864. Towson University, founded in 1866, is the state's second largest university. Baltimore is home to the University of Maryland, Baltimore County and the Maryland Institute College of Art. The majority of public universities in the state are affiliated with the University System of Maryland. Two state-funded institutions, Morgan State University and St. Mary's College of Maryland, as well as two federally funded institutions, the Uniformed Services University of the Health Sciences and the United States Naval Academy, are not affiliated with the University System of Maryland.

St. John's College in Annapolis, Maryland and Washington College in Chestertown, Maryland, both private institutions, are the two oldest colleges in the state, and are among the oldest in the country. Other private institutions include Mount St. Mary's University, McDaniel College (formerly known as Western Maryland College), Hood College, Stevenson University (formerly known as Villa Julie College), Loyola University Maryland, and Goucher College, among others.

Maryland's 24 public library systems deliver public education for everyone in the state of Maryland through a curriculum that comprises three pillars: Self-Directed Education (books and materials in all formats, e-resources), Research Assistance & Instruction (individualized research assistance, classes for students of all ages), and Instructive & Enlightening Experiences (e.g., Summer Reading Clubs, author events).

Maryland's library systems include, in part:


Many of the library systems have established formalized partnerships with other educational institutions in their counties and regions.

With two major metropolitan areas, Maryland has a number of major and minor professional sports franchises. Two National Football League teams play in Maryland, the Baltimore Ravens in Baltimore and the Washington Redskins in Landover. The Baltimore Colts represented the NFL in Baltimore from 1953 to 1983 before moving to Indianapolis.

The Baltimore Orioles are the state's Major League Baseball franchise. The National Hockey League's Washington Capitals and the National Basketball Association's Washington Wizards formerly played in Maryland, until the construction of an arena in Downtown D.C. in 1997 (now known as Capital One Arena).

Maryland enjoys considerable historical repute for the talented sports players of its past, including Cal Ripken Jr. and Babe Ruth. In 2012, "The Baltimore Sun" published a list of Maryland's top ten athletes in the state's history. The list includes Babe Ruth, Cal Ripken Jr, Johnny Unitas, Brooks Robinson, Frank Robinson, Ray Lewis, Michael Phelps, Jimmie Foxx, Jim Parker, and Wes Unseld.

Other professional sports franchises in the state include five affiliated minor league baseball teams, one independent league baseball team, the Baltimore Blast indoor soccer team, two indoor football teams, three low-level outdoor soccer teams, and the Chesapeake Bayhawks of Major League Lacrosse. Maryland is also home to one of the three races in horse racing's annual Triple Crown, the Preakness Stakes, which is run every spring at Pimlico Race Course in Baltimore.

The Congressional Country Club has hosted three golf tournaments for the U.S. Open and a PGA Championship.

The official state sport of Maryland, since 1962, is jousting; the official team sport since 2004 is lacrosse. The National Lacrosse Hall of Fame is located on the Johns Hopkins University campus in Baltimore. In 2008, intending to promote physical fitness for all ages, walking became the official state exercise. Maryland is the first state with an official state exercise.





</doc>
<doc id="18859" url="https://en.wikipedia.org/wiki?curid=18859" title="Michigan">
Michigan

Michigan () is a state in the Great Lakes and Midwestern regions of the United States.

The state's name, Michigan, originates from the (Ojibwe word) "mishigamaa", meaning "large water" or "large lake". Michigan is the tenth most populous of the 50 United States, with the 11th most extensive total area, and is the largest state by total area east of the Mississippi River

Michigan has a population of about 10 million. Its capital is Lansing. Metro Detroit is among the nation's most populous and largest metropolitan economies.

Michigan is the only state to consist of two peninsulas. The Lower Peninsula, to which the name Michigan was originally applied, is often noted as shaped like a mitten. The Upper Peninsula (often called "the U.P.") is separated from the Lower Peninsula by the Straits of Mackinac, a channel that joins Lake Huron to Lake Michigan. The Mackinac Bridge connects the peninsulas. 

The state has the longest freshwater coastline of any political subdivision in the world, being bounded by four of the five Great Lakes, plus Lake Saint Clair. As a result, it is one of the leading U.S. states for recreational boating.

Michigan also has 64,980 inland lakes and ponds. A person in the state is never more than from a natural water source or more than from a Great Lakes shoreline.

The area was first settled by Native American tribes, whose successive cultures occupied the territory for thousands of years. Colonized by French explorers in the 17th century, it was claimed as part of New France. After France's defeat in the French and Indian War in 1762, the region came under British rule.

Britain ceded this territory to the newly independent United States after Britain's defeat in the American Revolutionary War. The area was part of the larger Northwest Territory until 1800, when western Michigan became part of the Indiana Territory. Michigan Territory was formed in 1805, but some of the northern border with Canada was not agreed upon until after the War of 1812. Michigan was admitted into the Union in 1837 as the 26th state, a free one. It soon became an important center of industry and trade in the Great Lakes region and a popular immigrant destination in the late 19th and early 20th centuries.

Although Michigan developed a diverse economy, it is widely known as the center of the U.S. automotive industry, which developed as a major economic force in the early 20th century. It is home to the country's three major automobile companies (whose headquarters are all within the Detroit metropolitan area). While sparsely populated, the Upper Peninsula is important for tourism thanks to its abundance of natural resources, while the Lower Peninsula is a center of manufacturing, forestry, agriculture, services, and high-tech industry.

When the first European explorers arrived, the most populous tribes were Algonquian peoples, which include the Anishinaabe groups of Ojibwe (referred to as "Chippewa" in the United States), Odaawaa/Odawa (Ottawa), and the Boodewaadamii/Bodéwadmi (Potawatomi). The three nations co-existed peacefully as part of a loose confederation called the Council of Three Fires. The Ojibwe, whose numbers are estimated to have been between 25,000 and 35,000, were the largest.

The Ojibwe were established in Michigan's Upper Peninsula and northern and central Michigan, and also inhabited Ontario and southern Manitoba, Canada; and northern Wisconsin, and northern and north-central Minnesota. The Ottawa lived primarily south of the Straits of Mackinac in northern, western and southern Michigan, but also in southern Ontario, northern Ohio and eastern Wisconsin. The Potawatomi were in southern and western Michigan, in addition to northern and central Indiana, northern Illinois, southern Wisconsin, and southern Ontario. Other Algonquian tribes in Michigan, in the south and east, were the Mascouten, the Menominee, the Miami, the Sac (or Sauk), and the Fox. The Wyandot were an Iroquoian-speaking people in this area; they were historically known as the Huron by the French. 

French "voyageurs" and "coureurs des bois" explored and settled in Michigan in the 17th century. The first Europeans to reach what became Michigan were those of Étienne Brûlé's expedition in 1622. The first permanent European settlement was founded in 1668 on the site where Père Jacques Marquette established Sault Ste. Marie, Michigan as a base for Catholic missions. Missionaries in 1671–75 founded outlying stations at Saint Ignace and Marquette. Jesuit missionaries were well received by the area's Indian populations, with few difficulties or hostilities. In 1679, Robert Cavelier, Sieur de la Salle built Fort Miami at present-day St. Joseph. In 1691, the French established a trading post and Fort St. Joseph along the St. Joseph River at the present-day city of Niles.

In 1701, French explorer and army officer Antoine de la Mothe Cadillac founded Fort Pontchartrain du Détroit or "Fort Pontchartrain on-the-Strait" on the strait, known as the Detroit River, between lakes Saint Clair and Erie. Cadillac had convinced King Louis XIV's chief minister, Louis Phélypeaux, Comte de Pontchartrain, that a permanent community there would strengthen French control over the upper Great Lakes and discourage British aspirations.

The hundred soldiers and workers who accompanied Cadillac built a fort enclosing one arpent (about , the equivalent of just under per side) and named it Fort Pontchartrain. Cadillac's wife, Marie Thérèse Guyon, soon moved to Detroit, becoming one of the first European women to settle in what was considered the wilderness of Michigan. The town quickly became a major fur-trading and shipping post. The "Église de Saint-Anne" (Church of Saint Ann) was founded the same year. While the original building does not survive, the congregation remains active. Cadillac later departed to serve as the French governor of Louisiana from 1710 to 1716. French attempts to consolidate the fur trade led to the Fox Wars involving the Meskwaki (Fox) and their allies versus the French and their Native allies.

At the same time, the French strengthened Fort Michilimackinac at the Straits of Mackinac to better control their lucrative fur-trading empire. By the mid-18th century, the French also occupied forts at present-day Niles and Sault Ste. Marie, though most of the rest of the region remained unsettled by Europeans. France offered free land to attract families to Detroit, which grew to 800 people in 1765, and was the largest city between Montreal and New Orleans. French settlers also established small farms south of the Detroit River opposite the fort, near a Jesuit mission and Huron village.

From 1660 until the end of French rule, Michigan was part of the Royal Province of New France. In 1760, Montreal fell to the British forces ending the French and Indian War (1754–1763). Under the 1763 Treaty of Paris, Michigan and the rest of New France east of the Mississippi River passed to Great Britain. After the Quebec Act was passed in 1774, Michigan became part of the British Province of Quebec. By 1778, Detroit's population was up to 2,144 and it was the third-largest city in Quebec.

During the American Revolutionary War, Detroit was an important British supply center. Most of the inhabitants were French-Canadians or Native Americans, many of whom had been allied with the French because of long trading ties. Because of imprecise cartography and unclear language defining the boundaries in the 1783 Treaty of Paris, the British retained control of Detroit and Michigan after the American Revolution. When Quebec split into Lower and Upper Canada in 1791, Michigan was part of Kent County, Upper Canada. It held its first democratic elections in August 1792 to send delegates to the new provincial parliament at Newark (now Niagara-on-the-Lake).

Under terms negotiated in the 1794 Jay Treaty, Britain withdrew from Detroit and Michilimackinac in 1796. It retained control of territory east and south of the Detroit River, which are now included in Ontario, Canada. Questions remained over the boundary for many years, and the United States did not have uncontested control of the Upper Peninsula and Drummond Island until 1818 and 1847, respectively.

During the War of 1812, the United States forces at Fort Detroit surrendered Michigan Territory (effectively consisting of Detroit and the surrounding area) after a nearly bloodless siege in 1812. A US attempt to retake Detroit resulted in a severe American defeat in the River Raisin Massacre. This battle, still ranked as the bloodiest ever fought in the state, had the highest number of American casualties of any battle in the war. 

Michigan was recaptured by Americans in 1813 after the Battle of Lake Erie. They used Michigan as a base to launch an invasion of Canada, which culminated in the Battle of the Thames. But the more northern areas of Michigan were held by the British until the peace treaty restored the old boundaries. A number of forts, including Fort Wayne, were built by the United States in Michigan during the 19th century out of fears of renewed fighting with Britain.
The population grew slowly until the opening in 1825 of the Erie Canal through the Mohawk Valley in New York, connecting the Great Lakes to the Hudson River and New York City. The new route attracted a large influx of settlers to the Michigan territory. They worked as farmers, lumbermen, shipbuilders, and merchants and shipped out grain, lumber, and iron ore. By the 1830s, Michigan had 80,000 residents, more than enough to apply and qualify for statehood.

A Constitutional Convention of Assent, led by Gershom Mott Williams, was held to lead the territory to statehood.
In October 1835 the people approved the Constitution of 1835, thereby forming a state government, although Congressional recognition was delayed pending resolution of a boundary dispute with Ohio known as the Toledo War. Congress awarded the "Toledo Strip" to Ohio. Michigan received the western part of the Upper Peninsula as a concession and formally entered the Union as a free state on January 26, 1837. The Upper Peninsula proved to be a rich source of lumber, iron, and copper. Michigan led the nation in lumber production from the 1850s to the 1880s. Railroads became a major engine of growth from the 1850s onward, with Detroit the chief hub.

A second wave of French-Canadian immigrants settled in Michigan during the late 19th to early 20th century, working in lumbering areas in counties on the Lake Huron side of the Lower Peninsula, such as the Saginaw Valley, Alpena, and Cheboygan counties, as well as throughout the Upper Peninsula, with large concentrations in Escanaba and the Keweenaw Peninsula. This was also a period of development of the gypsum industry in Alabaster, Michigan, which became nationally prominent. 

The first statewide meeting of the Republican Party took place July 6, 1854, in Jackson, Michigan, where the party adopted its platform. The state was heavily Republican until the 1930s. Michigan made a significant contribution to the Union in the American Civil War and sent more than forty regiments of volunteers to the federal armies.

Modernizers and boosters set up systems for public education, including founding the University of Michigan (1817, moved to Ann Arbor in 1837) for a classical academic education; and Michigan State Normal School (1849), now Eastern Michigan University, for the training of teachers. It adopted this model from the German educational system. In 1899, Michigan State became the first normal college in the nation to offer a four-year curriculum. Michigan Agricultural College (1855), now Michigan State University in East Lansing, was founded as the pioneer land-grant college, a model for those authorized under the Morrill Act (1862). Many private colleges were founded as well, and the smaller cities established high schools late in the century.

Michigan's economy underwent a transformation at the turn of the 20th century. Many individuals, including Ransom E. Olds, John and Horace Dodge, Henry Leland, David Dunbar Buick, Henry Joy, Charles King, and Henry Ford, provided the concentration of engineering know-how and technological enthusiasm to develop the automotive industry. Ford's development of the moving assembly line in Highland Park marked a new era in transportation. Like the steamship and railroad, mass production of automobiles was a far-reaching development. More than the forms of public transportation, the affordable automobile transformed private life. Automobile production became the major industry of Detroit and Michigan, and permanently altered the socio-economic life of the United States and much of the world.

With the growth, the auto industry created jobs in Detroit that attracted immigrants from Europe and migrants from across the United States, including both blacks and whites from the rural South. By 1920, Detroit was the fourth-largest city in the US. Residential housing was in short supply, and it took years for the market to catch up with the population boom. By the 1930s, so many immigrants had arrived that more than 30 languages were spoken in the public schools, and ethnic communities celebrated in annual heritage festivals. Over the years immigrants and migrants contributed greatly to Detroit's diverse urban culture, including popular music trends. The influential Motown Sound of the 1960s was led by a variety of individual singers and groups.

Grand Rapids, the second-largest city in Michigan, is also an important center of manufacturing. Since 1838, the city has been noted for its furniture industry. In the 21st century, it is home to five of the world's leading office furniture companies. Grand Rapids is home to a number of major companies including Steelcase, Amway, and Meijer. Grand Rapids is also an important center for GE Aviation Systems.

Michigan held its first United States presidential primary election in 1910. With its rapid growth in industry, it was an important center of industry-wide union organizing, such as the rise of the United Auto Workers.

In 1920 WWJ (AM) in Detroit became the first radio station in the United States to regularly broadcast commercial programs. Throughout that decade, some of the country's largest and most ornate skyscrapers were built in the city. Particularly noteworthy are the Fisher Building, Cadillac Place, and the Guardian Building, each of which has been designated as a National Historic Landmark (NHL).

In 1927 a school bombing took place in Clinton County. The Bath School disaster, perpetrated by an adult man, resulted in the deaths of 38 schoolchildren and constitutes the deadliest mass murder in a school in U.S. history.

Michigan converted much of its manufacturing to satisfy defense needs during World War II; it manufactured 10.9 percent of the United States military armaments produced during the war, ranking second (behind New York) among the 48 states.

Detroit continued to expand through the 1950s, at one point doubling its population in a decade. After World War II, housing was developed in suburban areas outside city cores to meet demand for residences. The federal government subsidized the construction of interstate highways, which were intended to strengthen military access, but also allowed commuters and business traffic to travel the region more easily. Since 1960, modern advances in the auto industry have led to increased automation, high-tech industry, and increased suburban growth.

Michigan is the leading auto-producing state in the US, with the industry primarily located throughout the Midwestern United States, Ontario, Canada, and the Southern United States. With almost ten million residents, Michigan is a large and influential state, ranking tenth in population among the fifty states. Detroit is the centrally located metropolitan area of the Great Lakes Megalopolis and the second-largest metropolitan area in the U.S. linking the Great Lakes system.
The Metro Detroit area in Southeast Michigan is the state's largest metropolitan area (roughly 50% of the population resides there) and the eleventh largest in the USA. The Grand Rapids metropolitan area in Western Michigan is the state's fastest-growing metro area, with over 1.3 million residents as of 2006. Metro Detroit receives more than 15 million visitors each year. Michigan has many popular tourist destinations, including areas such as Frankenmuth in The Thumb, and Traverse City on the Grand Traverse Bay in Northern Michigan. Tourists spend about $17 billion annually in Michigan supporting 193,000 jobs.

Michigan typically ranks third or fourth in overall Research & development (R&D) expenditures in the US. The state's leading research institutions include the University of Michigan, Michigan State University, and Wayne State University, which are important partners in the state's economy and the state's University Research Corridor. Michigan's public universities attract more than $1.5 B in research and development grants each year. Agriculture also serves a significant role, making the state a leading grower of fruit in the US, including blueberries, cherries, apples, grapes, and peaches.

Michigan is governed as a republic, with three branches of government: the executive branch consisting of the Governor of Michigan and the other independently elected constitutional officers; the legislative branch consisting of the House of Representatives and Senate; and the judicial branch. The Michigan Constitution allows for the direct participation of the electorate by statutory initiative and referendum, recall, and constitutional initiative and referral (Article II, § 9, defined as "the power to propose laws and to enact and reject laws, called the initiative, and the power to approve or reject laws enacted by the legislature, called the referendum. The power of initiative extends only to laws which the legislature may enact under this constitution"). Lansing is the state capital and is home to all three branches of state government.

The governor and the other state constitutional officers serve four-year terms and may be re-elected only once. The governor is Rick Snyder. Michigan has two official Governor's Residences; one is in Lansing, and the other is at Mackinac Island. The other constitutionally elected executive officers are the lieutenant governor, who is elected on a joint ticket with the governor, the secretary of state, and the attorney general. The lieutenant governor presides over the Senate but only voting when ties occur, and is also a member of the cabinet. The secretary of state is the chief elections officer and is charged with running many licensure programs including motor vehicles, all of which are done through the branch offices of the secretary of state.

The Michigan Legislature consists of a 38-member Senate and 110-member House of Representatives. Members of both houses of the legislature are elected through first past the post elections by single-member electoral districts of near-equal population that often have boundaries which coincide with county and municipal lines. Senators serve four-year terms concurrent to those of the governor, while representatives serve two-year terms. The Michigan State Capitol was dedicated in 1879 and has hosted the executive and legislative branches of the state ever since.

The Michigan judiciary consists of two courts with primary jurisdiction (the Circuit Courts and the District Courts), one intermediate level appellate court (the Michigan Court of Appeals), and the Michigan Supreme Court. There are several administrative courts and specialized courts. District courts are trial courts of limited jurisdiction, handling most traffic violations, small claims, misdemeanors, and civil suits where the amount contended is below $25,000. District courts are often responsible for handling the preliminary examination and for setting bail in felony cases. District court judges are elected to terms of six years. In a few locations, municipal courts have been retained to the exclusion of the establishment of district courts. There are 57 circuit courts in the State of Michigan, which have original jurisdiction over all civil suits where the amount contended in the case exceeds $25,000 and all criminal cases involving felonies. Circuit courts are also the only trial courts in the State of Michigan which possess the power to issue equitable remedies. Circuit courts have appellate jurisdiction from district and municipal courts, as well as from decisions and decrees of state agencies. Most counties have their own circuit court, but sparsely populated counties often share them. Circuit court judges are elected to terms of six years. State appellate court judges are elected to terms of six years, but vacancies are filled by an appointment by the governor. There are four divisions of the Court of Appeals in Detroit, Grand Rapids, Lansing, and Marquette. Cases are heard by the Court of Appeals by panels of three judges, who examine the application of the law and not the facts of the case unless there has been grievous error pertaining to questions of fact. The Michigan Supreme Court consists of seven members who are elected on non-partisan ballots for staggered eight-year terms. The Supreme Court has original jurisdiction only in narrow circumstances but holds appellate jurisdiction over the entire state judicial system.

Michigan has had four constitutions, the first of which was ratified on October 5 and 6, 1835. There were also constitutions from 1850 and 1908, in addition to the current constitution from 1963. The current document has a preamble, 11 articles, and one section consisting of a schedule and temporary provisions. Michigan, like every U.S. state except Louisiana, has a common law legal system.

Voters in the state elect candidates from both major parties. Economic issues are important in Michigan elections.

The three-term Republican Governor John Engler (1991–2003) preceded the former two-term Democratic Governor Jennifer Granholm (2003–2011). The state has elected successive Republican attorneys general twice since 2003. The Republican Party holds a majority in both the House and Senate of the Michigan Legislature. Michigan supported the election of Republican Presidents Ronald Reagan, George H. W. Bush, and Donald Trump. The Governor Rick Snyder (2011–present) is a Republican.

In contrast, the state-supported Democratic candidates in each presidential election from 1992 to 2012. In 2012, Barack Obama carried the state over Mitt Romney, winning Michigan's 16 electoral votes with 54% of the vote. Michigan's two U.S. Senators are both Democrats, while Republicans hold nine of the state's fourteen US House seats. Michigan's senior U.S. Senator Debbie Stabenow, a Democrat, has served since 2001 after narrowly beating former Republican U.S. Senator Spencer Abraham in the 2000 elections. Democratic U.S. Senator Gary Peters was elected in 2014, beating former Republican Michigan Secretary of State Terri Lynn Land. Congressman Fred Upton, a Republican, serves as Chairman of the US House Committee on Energy and Commerce. Congresswoman Debbie Dingell, a Democrat, became the first person to succeed a living spouse when she replaced former Dean of the House of Representatives John Dingell in 2015.

Republican strongholds of the state include rural areas of Western and Northern Michigan, the Grand Rapids metropolitan area, and Livingston County. Areas of Democratic strength include Wayne County, home to Detroit, Washtenaw County (Ann Arbor), Ingham County (Lansing), and Genesee County (Flint). Much of suburban Detroit—which includes parts of Oakland, Macomb, and Wayne counties—is politically competitive between the two parties.

Historically, the first county-level meeting of the Republican Party took place in Jackson on July 6, 1854, and the party thereafter dominated Michigan until the Great Depression. In the 1912 election, Michigan was one of the six states to support progressive Republican and third-party candidate Theodore Roosevelt for president after he lost the Republican nomination to William Howard Taft.

Michigan remained fairly reliably Republican at the presidential level for much of the 20th century. It was part of Greater New England, the northern tier of states settled chiefly by migrants from New England who carried their culture with them. The state was one of only a handful to back Wendell Willkie over Franklin Roosevelt in 1940, and supported Thomas E. Dewey in his losing bid against Harry S. Truman in 1948. Michigan went to the Democrats in presidential elections during the 1960s and voted for the Republican candidate in every election from 1972 to 1988. Between 1992 and 2012 it supported the Democrats; early on in 2016, it was pegged as a swing state, and was narrowly won by the G.O.P. candidate, Donald Trump.

Michigan was the home of Gerald Ford, the 38th President of the United States. Born in Nebraska, he moved as an infant to Grand Rapids. The Gerald R. Ford Museum is in Grand Rapids, and the Gerald R. Ford Presidential Library is on the campus of his alma mater, the University of Michigan in Ann Arbor.

In 1846 Michigan became the first state in the Union, as well as the first English-speaking government in the world, to abolish the death penalty. Historian David Chardavoyne has suggested the movement to abolish capital punishment in Michigan grew as a result of enmity toward the state's neighbor, Canada. Under British rule, it made public executions a regular practice.

Michigan has recognized and performed same-sex marriages since June 26, 2015, following the Supreme Court ruling in "Obergefell v. Hodges". Previously, such unions were prohibited under a 2004 state constitutional amendment.

Michigan has approved plans to expand Medicaid coverage in 2014 to adults with incomes up to 133% of the federal poverty level (approximately $15,500 for a single adult in 2014).

State government is decentralized among three tiers—statewide, county and township. Counties are administrative divisions of the state, and townships are administrative divisions of a county. Both of them exercise state government authority, localized to meet the particular needs of their jurisdictions, as provided by state law. There are 83 counties in Michigan.

Cities, state universities, and villages are vested with home rule powers of varying degrees. Home rule cities can generally do anything not prohibited by law. The fifteen state universities have broad power and can do anything within the parameters of their status as educational institutions that is not prohibited by the state constitution. Villages, by contrast, have limited home rule and are not completely autonomous from the county and township in which they are located.

There are two types of township in Michigan: "general law" township and "charter". Charter township status was created by the Legislature in 1947 and grants additional powers and stream-lined administration in order to provide greater protection against annexation by a city. As of April 2001, there were 127 charter townships in Michigan. In general, charter townships have many of the same powers as a city but without the same level of obligations. For example, a charter township can have its own fire department, water and sewer department, police department, and so on—just like a city—but it is not "required" to have those things, whereas cities "must" provide those services. Charter townships can opt to use county-wide services instead, such as deputies from the county sheriff's office instead of a home-based force of ordinance officers.

Michigan consists of two peninsulas that lie between 82°30' to about 90°30' west longitude, and are separated by the Straits of Mackinac. The 45th parallel north runs through the state—marked by highway signs and the Polar-Equator Trail—along a line including Mission Point Light near Traverse City, the towns of Gaylord and Alpena in the Lower Peninsula and Menominee in the Upper Peninsula. With the exception of two small areas that are drained by the Mississippi River by way of the Wisconsin River in the Upper Peninsula and by way of the Kankakee-Illinois River in the Lower Peninsula, Michigan is drained by the Great Lakes-St. Lawrence watershed and is the only state with the majority of its land thus drained.

The Great Lakes that border Michigan from east to west are Lake Erie, Lake Huron, Lake Michigan and Lake Superior. It has more public golf courses, registered boats, and lighthouses than any other state. The state is bounded on the south by the states of Ohio and Indiana, sharing land and water boundaries with both. Michigan's western boundaries are almost entirely water boundaries, from south to north, with Illinois and Wisconsin in Lake Michigan; then a land boundary with Wisconsin and the Upper Peninsula, that is principally demarcated by the Menominee and Montreal Rivers; then water boundaries again, in Lake Superior, with Wisconsin and Minnesota to the west, capped around by the Canadian province of Ontario to the north and east.
The heavily forested Upper Peninsula is relatively mountainous in the west. The Porcupine Mountains, which are part of one of the oldest mountain chains in the world, rise to an altitude of almost above sea level and form the watershed between the streams flowing into Lake Superior and Lake Michigan. The surface on either side of this range is rugged. The state's highest point, in the Huron Mountains northwest of Marquette, is Mount Arvon at . The peninsula is as large as Connecticut, Delaware, Massachusetts, and Rhode Island combined but has fewer than 330,000 inhabitants. They are sometimes called "Yoopers" (from "U.P.'ers"), and their speech (the "Yooper dialect") has been heavily influenced by the numerous Scandinavian and Canadian immigrants who settled the area during the lumbering and mining boom of the late 19th century.
The Lower Peninsula is shaped like a mitten and many residents hold up a hand to depict where they are from. It is long from north to south and from east to west and occupies nearly two-thirds of the state's land area. The surface of the peninsula is generally level, broken by conical hills and glacial moraines usually not more than a few hundred feet tall. It is divided by a low water divide running north and south. The larger portion of the state is on the west of this and gradually slopes toward Lake Michigan. The highest point in the Lower Peninsula is either Briar Hill at , or one of several points nearby in the vicinity of Cadillac. The lowest point is the surface of Lake Erie at .

The geographic orientation of Michigan's peninsulas makes for a long distance between the ends of the state. Ironwood, in the far western Upper Peninsula, lies 630 highway miles (1,015 km) from Lambertville in the Lower Peninsula's southeastern corner. The geographic isolation of the Upper Peninsula from Michigan's political and population centers makes the U.P. culturally and economically distinct. Occasionally U.P. residents have called for secession from Michigan and establishment as a new state to be called "Superior".

A feature of Michigan that gives it the distinct shape of a mitten is the Thumb. This peninsula projects out into Lake Huron and the Saginaw Bay. The geography of the Thumb is mainly flat with a few rolling hills. Other peninsulas of Michigan include the Keweenaw Peninsula, making up the Copper Country region of the state. The Leelanau Peninsula lies in the Northern Lower Michigan region. "See Also Michigan Regions"

Numerous lakes and marshes mark both peninsulas, and the coast is much indented. Keweenaw Bay, Whitefish Bay, and the Big and Little Bays De Noc are the principal indentations on the Upper Peninsula. The Grand and Little Traverse, Thunder, and Saginaw bays indent the Lower Peninsula. Michigan has the second longest shoreline of any state—, including of island shoreline.
The state has numerous large islands, the principal ones being the North Manitou and South Manitou, Beaver, and Fox groups in Lake Michigan; Isle Royale and Grande Isle in Lake Superior; Marquette, Bois Blanc, and Mackinac islands in Lake Huron; and Neebish, Sugar, and Drummond islands in St. Mary's River. Michigan has about 150 lighthouses, the most of any U.S. state. The first lighthouses in Michigan were built between 1818 and 1822. They were built to project light at night and to serve as a landmark during the day to safely guide the passenger ships and freighters traveling the Great Lakes. See Lighthouses in the United States.

The state's rivers are generally small, short and shallow, and few are navigable. The principal ones include the Detroit River, St. Marys River, and St. Clair River which connect the Great Lakes; the Au Sable, Cheboygan, and Saginaw, which flow into Lake Huron; the Ontonagon, and Tahquamenon, which flow into Lake Superior; and the St. Joseph, Kalamazoo, Grand, Muskegon, Manistee, and Escanaba, which flow into Lake Michigan. The state has 11,037 inland lakes—totaling of inland water—in addition to of Great Lakes waters. No point in Michigan is more than from an inland lake or more than from one of the Great Lakes.

The state is home to several areas maintained by the National Park Service including: Isle Royale National Park, in Lake Superior, about southeast of Thunder Bay, Ontario. Other national protected areas in the state include: Keweenaw National Historical Park, Pictured Rocks National Lakeshore, Sleeping Bear Dunes National Lakeshore, Huron National Forest, Manistee National Forest, Hiawatha National Forest, Ottawa National Forest and Father Marquette National Memorial. The largest section of the North Country National Scenic Trail passes through Michigan.

With 78 state parks, 19 state recreation areas, and 6 state forests, Michigan has the largest state park and state forest system of any state. These parks and forests include Holland State Park, Mackinac Island State Park, Au Sable State Forest, and Mackinaw State Forest.

Michigan has a continental climate, although there are two distinct regions. The southern and central parts of the Lower Peninsula (south of Saginaw Bay and from the Grand Rapids area southward) have a warmer climate (Köppen climate classification "Dfa") with hot summers and cold winters. The northern part of Lower Peninsula and the entire Upper Peninsula has a more severe climate (Köppen "Dfb"), with warm, but shorter summers and longer, cold to very cold winters. Some parts of the state average high temperatures below freezing from December through February, and into early March in the far northern parts. During the winter through the middle of February, the state is frequently subjected to heavy lake-effect snow. The state averages from of precipitation annually; however, some areas in the northern lower peninsula and the upper peninsula average almost of snowfall per year. Michigan's highest recorded temperature is at Mio on July 13, 1936, and the coldest recorded temperature is at Vanderbilt on February 9, 1934.

The state averages 30 days of thunderstorm activity per year. These can be severe, especially in the southern part of the state. The state averages 17 tornadoes per year, which are more common in the state's extreme southern section. Portions of the southern border have been almost as vulnerable historically as states further west and in Tornado Alley. For this reason, many communities in the very southern portions of the state have tornado sirens to warn residents of approaching tornadoes. Farther north, in Central Michigan, Northern Michigan, and the Upper Peninsula, tornadoes are rare.

The geological formation of the state is greatly varied, with the Michigan Basin being the most major formation. Primary boulders are found over the entire surface of the Upper Peninsula (being principally of primitive origin), while Secondary deposits cover the entire Lower Peninsula. The Upper Peninsula exhibits Lower Silurian sandstones, limestones, copper and iron bearing rocks, corresponding to the Huronian system of Canada. The central portion of the Lower Peninsula contains coal measures and rocks of the Pennsylvanian period. Devonian and sub-Carboniferous deposits are scattered over the entire state.

Michigan rarely experiences earthquakes, thus far mostly smaller ones that do not cause significant damage. A 4.6-magnitude earthquake struck in August 1947. More recently, a 4.2-magnitude earthquake occurred on Saturday, May 2, 2015, shortly after noon, about 5 miles south of Galesburg, Michigan (9 miles southeast of Kalamazoo) in central Michigan, about 140 miles west of Detroit, according to the Colorado-based U.S. Geological Survey's National Earthquake Information Center. No major damage or injuries were reported, according to Governor Rick Snyder's office.

The United States Census Bureau estimates the population of Michigan was 9,962,311 on July 1, 2017, an increase of 0.79% from 9,883,635 recorded at the 2010 United States Census.

The center of population of Michigan is in Shiawassee County, in the southeastern corner of the civil township of Bennington, which is northwest of the village of Morrice.

As of the 2010 American Community Survey for the U.S. Census, the state had a foreign-born population of 592,212, or 6.0% of the total. Michigan has the largest Dutch, Finnish, and Macedonian populations in the United States.

The 2010 Census reported:

In the same year Hispanics or Latinos (of any race) made up 4.4% of the population.

The large majority of Michigan's population is Caucasian. Americans of European descent live throughout Michigan and most of Metro Detroit. Large European American groups include those of German, British, Irish, Polish and Belgian ancestry. People of Scandinavian descent, and those of Finnish ancestry, have a notable presence in the Upper Peninsula. Western Michigan is known for the Dutch heritage of many residents (the highest concentration of any state), especially in Holland and metropolitan Grand Rapids.

African-Americans, who came to Detroit and other northern cities in the Great Migration of the early 20th century, form a majority of the population of the city of Detroit and of other cities, including Flint and Benton Harbor.

As of 2007 about 300,000 people in Southeastern Michigan trace their descent from the Middle East. Dearborn has a sizeable Arab community, with many Assyrian/Chaldean/Syriac, and Lebanese who immigrated for jobs in the auto industry in the 1920s along with more recent Yemenis and Iraqis.

As of 2007, almost 8,000 Hmong people lived in the State of Michigan, about double their 1999 presence in the state. As of 2007 most lived in northeastern Detroit, but they had been increasingly moving to Pontiac and Warren. By 2015 the number of Hmong in the Detroit city limits had significantly declined. Lansing hosts a statewide Hmong New Year Festival. The Hmong community also had a prominent portrayal in the 2008 film "Gran Torino", which was set in Detroit.

As of 2015, 80% of Michigan's Japanese population lived in the counties of Macomb, Oakland, Washtenaw, and Wayne in the Detroit and Ann Arbor areas. As of April 2013, the largest Japanese national population is in Novi, with 2,666 Japanese residents, and the next largest populations are respectively in Ann Arbor, West Bloomfield Township, Farmington Hills, and Battle Creek. The state has 481 Japanese employment facilities providing 35,554 local jobs. 391 of them are in Southeast Michigan, providing 20,816 jobs, and the 90 in other regions in the state provide 14,738 jobs. The Japanese Direct Investment Survey of the Consulate-General of Japan, Detroit stated over 2,208 additional Japanese residents were employed in the State of Michigan as of October 1, 2012, than in 2011. During the 1990s the Japanese population of Michigan experienced an increase, and many Japanese people with children moved to particular areas for their proximity to Japanese grocery stores and high-performing schools.

A person from Michigan is called a Michigander or Michiganian; also at times, but rarely, a "Michiganite". Residents of the Upper Peninsula are sometimes referred to as "Yoopers" (a phonetic pronunciation of "U.P.ers"), and Upper Peninsula residents sometimes refer to those from the Lower Peninsula as "trolls" because they live below the bridge.

As of 2011, 34.3% of Michigan's children under the age of one belonged to racial or ethnic minority groups, meaning they had at least one parent who was not non-Hispanic white.

"Note: Percentages in the table can exceed 100% as Hispanics are counted both by their ethnicity and by their race."


As of 2010, 91.11% (8,507,947) of Michigan residents age five and older spoke only English at home, while 2.93% (273,981) spoke Spanish, 1.04% (97,559) Arabic, 0.44% (41,189) German, 0.36% (33,648) Chinese (which includes Mandarin), 0.31% (28,891) French, 0.29% (27,019) Polish, and Syriac languages (such as Modern Aramaic and Northeastern Neo-Aramaic) was spoken as a main language by 0.25% (23,420) of the population over the age of five. In total, 8.89% (830,281) of Michigan's population age 5 and older spoke a mother language other than English.

The Roman Catholic Church has six dioceses and one archdiocese in Michigan; Gaylord, Grand Rapids, Kalamazoo, Lansing, Marquette, Saginaw and Detroit. The Roman Catholic Church is the largest denomination by number of adherents, according to the Association of Religion Data Archives (ARDA) 2010 survey, with 1,717,296 adherents. The Roman Catholic Church was the only organized religion in Michigan until the 19th century, reflecting the territory's French colonial roots. Detroit's Saint Anne's parish, established in 1701 by Antoine de la Mothe Cadillac, is the second-oldest Roman Catholic parish in the United States. On March 8, 1833, the Holy See formally established a diocese in the Michigan territory, which included all of Michigan, Wisconsin, Minnesota, and the Dakotas east of the Mississippi River. When Michigan became a state in 1837, the boundary of the Diocese of Detroit was redrawn to coincide with that of the State; the other dioceses were later carved out from the Diocese of Detroit but remain part of the Ecclesiastical Province of Detroit.

In 2010, the largest Protestant denominations were the United Methodist Church with 228,521 adherents; followed by the Lutheran Church–Missouri Synod with 219,618, and the Evangelical Lutheran Church in America with 120,598 adherents. The Christian Reformed Church in North America had almost 100,000 members and over 230 congregations in Michigan. The Reformed Church in America had 76,000 members and 154 congregations in the state. In the same survey, Jewish adherents in the state of Michigan were estimated at 44,382, and Muslims at 120,351. The Lutheran Church was introduced by German and Scandinavian immigrants; Lutheranism is the second largest religious denomination in the state. The first Jewish synagogue in the state was Temple Beth El, founded by twelve German Jewish families in Detroit in 1850. In West Michigan, Dutch immigrants fled from the specter of religious persecution and famine in the Netherlands around 1850 and settled in and around what is now Holland, Michigan, establishing a "colony" on American soil that fervently held onto Calvinist doctrine that established a significant presence of Reformed churches. Islam was introduced by immigrants from the Near East during the 20th century. Michigan is home to the largest mosque in North America, the Islamic Center of America in Dearborn. Battle Creek, Michigan is also the birthplace of the Seventh-day Adventist Church, which was founded on May 21, 1863.

The U.S. Bureau of Economic Analysis estimated Michigan's 2017 gross state product to be $504.967 billion, ranking 14th out of the 50 states. According to the Bureau of Labor Statistics, as of June 2018, the state's seasonally adjusted unemployment rate is estimated at 4.5%.

Products and services include automobiles, food products, information technology, aerospace, military equipment, furniture, and mining of copper and iron ore. Michigan is the third leading grower of Christmas trees with of land dedicated to Christmas tree farming. The beverage Vernors was invented in Michigan in 1866, sharing the title of oldest soft drink with Hires Root Beer. Faygo was founded in Detroit on November 4, 1907. Two of the top four pizza chains were founded in Michigan and are headquartered there: Domino's Pizza by Tom Monaghan and Little Caesars Pizza by Mike Ilitch. Michigan became the 24th Right to Work state in U.S. in 2012.

Since 2009, GM, Ford and Chrysler have managed a significant reorganization of their benefit funds structure after a volatile stock market which followed the September 11 attacks and early 2000s recession impacted their respective U.S. pension and benefit funds (OPEB). General Motors, Ford, and Chrysler reached agreements with the United Auto Workers Union to transfer the liabilities for their respective health care and benefit funds to a 501(c)(9) Voluntary Employee Beneficiary Association (VEBA). Manufacturing in the state grew 6.6% from 2001 to 2006, but the high speculative price of oil became a factor for the U.S. auto industry during the economic crisis of 2008 impacting industry revenues. In 2009, GM and Chrysler emerged from Chapter 11 restructurings with financing provided in part by the U.S. and Canadian governments. GM began its initial public offering (IPO) of stock in 2010. For 2010, the Big Three domestic automakers have reported significant profits indicating the beginning of rebound.

, Michigan ranked fourth in the U.S. in high tech employment with 568,000 high tech workers, which includes 70,000 in the automotive industry. Michigan typically ranks third or fourth in overall Research & development (R&D) expenditures in the United States. Its research and development, which includes automotive, comprises a higher percentage of the state's overall gross domestic product than for any other U.S. state. The state is an important source of engineering job opportunities. The domestic auto industry accounts directly and indirectly for one of every ten jobs in the U.S.

Michigan was second in the U.S. in 2004 for new corporate facilities and expansions. From 1997 to 2004, Michigan was the only state to top the 10,000 mark for the number of major new developments; however, the effects of the late 2000s recession have slowed the state's economy. In 2008, Michigan placed third in a site selection survey among the states for luring new business which measured capital investment and new job creation per one million population. In August 2009, Michigan and Detroit's auto industry received $1.36 B in grants from the U.S. Department of Energy for the manufacture of electric vehicle technologies which is expected to generate 6,800 immediate jobs and employ 40,000 in the state by 2020. From 2007 to 2009, Michigan ranked 3rd in the U.S. for new corporate facilities and expansions.

As leading research institutions, the University of Michigan, Michigan State University, and Wayne State University are important partners in the state's economy and its University Research Corridor. Michigan's public universities attract more than $1.5 B in research and development grants each year. The National Superconducting Cyclotron Laboratory is at Michigan State University. Michigan's workforce is well-educated and highly skilled, making it attractive to companies. It has the third highest number of engineering graduates nationally.

Detroit Metropolitan Airport is one of the nation's most recently expanded and modernized airports with six major runways, and large aircraft maintenance facilities capable of servicing and repairing a Boeing 747 and is a major hub for Delta Air Lines. Michigan's schools and colleges rank among the nation's best. The state has maintained its early commitment to public education. The state's infrastructure gives it a competitive edge; Michigan has 38 deep water ports. In 2007, Bank of America announced that it would commit $25 billion to community development in Michigan following its acquisition of LaSalle Bank in Troy.

Michigan led the nation in job creation improvement in 2010.

Michigan's personal income tax is set to a flat rate of 4.25%. In addition, 22 cities impose income taxes; rates are set at 1% for residents and 0.5% for non-residents in all but four cities. Michigan's state sales tax is 6%, though items such as food and medication are exempted from sales tax. Property taxes are assessed on the local level, but every property owner's local assessment contributes six mills (a rate of $6 per $1000 of property value) to the statutory State Education Tax. Property taxes are appealable to local boards of review and need the approval of the local electorate to exceed millage rates prescribed by state law and local charters. In 2011, the state repealed its business tax and replaced it with a 6% corporate income tax which substantially reduced taxes on business. Article IX of the Constitution of the State of Michigan also provides limitations on how much the state can tax.

The state also levies a 6% sales tax within the state and a Use tax on goods purchased outside the state (that are brought in and used in state). The use tax applies to internet sales/purchases from outside Michigan and is equivalent to the sales tax.

A wide variety of commodity crops, fruits, and vegetables are grown in Michigan, making it second only to California among U.S. states in the diversity of its agriculture. The state has 54,800 farms utilizing of land which sold $6.49 billion worth of products in 2010. The most valuable agricultural product is milk. Leading crops include corn, soybeans, flowers, wheat, sugar beets, and potatoes. Livestock in the state included 1 million cattle, 1 million hogs, 78,000 sheep and over 3 million chickens. Livestock products accounted for 38% of the value of agricultural products while crops accounted for the majority.

Michigan is a leading grower of fruit in the U.S., including blueberries, tart cherries, apples, grapes, and peaches. Plums, pears, and strawberries are also grown in Michigan. These fruits are mainly grown in West Michigan due to the moderating effect of Lake Michigan on the climate. There is also significant fruit production, especially cherries, but also grapes, apples, and other fruits, in Northwest Michigan along Lake Michigan. Michigan produces wines, beers and a multitude of processed food products. Kellogg's cereal is based in Battle Creek, Michigan and processes many locally grown foods. Thornapple Valley, Ball Park Franks, Koegel Meat Company, and Hebrew National sausage companies are all based in Michigan.

Michigan is home to very fertile land in the Saginaw Valley and "Thumb" areas. Products grown there include corn, sugar beets, navy beans, and soybeans. Sugar beet harvesting usually begins the first of October. It takes the sugar factories about five months to process the 3.7 million tons of sugarbeets into 485,000 tons of pure, white sugar. Michigan's largest sugar refiner, Michigan Sugar Company is the largest east of the Mississippi River and the fourth largest in the nation. Michigan Sugar brand names are Pioneer Sugar and the newly incorporated Big Chief Sugar. Potatoes are grown in Northern Michigan, and corn is dominant in Central Michigan. Alfalfa, cucumbers, and asparagus are also grown.

Michigan's tourists spend $17.2 billion per year in the state, supporting 193,000 tourism jobs. Michigan's tourism website ranks among the busiest in the nation. Destinations draw vacationers, hunters, and nature enthusiasts from across the United States and Canada. Michigan is fifty percent forest land, much of it quite remote. The forests, lakes and thousands of miles of beaches are top attractions. Event tourism draws large numbers to occasions like the Tulip Time Festival and the National Cherry Festival.
In 2006, the Michigan State Board of Education mandated all public schools in the state hold their first day of school after the Labor Day holiday, in accordance with the new Post Labor Day School law. A survey found 70% of all tourism business comes directly from Michigan residents, and the Michigan Hotel, Motel, & Resort Association claimed the shorter summer in between school years cut into the annual tourism season in the state.

Tourism in metropolitan Detroit draws visitors to leading attractions, especially The Henry Ford, the Detroit Institute of Arts, the Detroit Zoo, and to sports in Detroit. Other museums include the Detroit Historical Museum, the Charles H. Wright Museum of African American History, museums in the Cranbrook Educational Community, and the Arab American National Museum. The metro area offers four major casinos, MGM Grand Detroit, Greektown, Motor City, and Caesars Windsor in Windsor, Ontario, Canada; moreover, Detroit is the largest American city and metropolitan region to offer casino resorts.

Hunting and fishing are significant industries in the state. Charter boats are based in many Great Lakes cities to fish for salmon, trout, walleye, and perch. Michigan ranks first in the nation in licensed hunters (over one million) who contribute $2 billion annually to its economy. Over three-quarters of a million hunters participate in white-tailed deer season alone. Many school districts in rural areas of Michigan cancel school on the opening day of firearm deer season, because of attendance concerns.

Michigan's Department of Natural Resources manages the largest dedicated state forest system in the nation. The forest products industry and recreational users contribute $12 billion and 200,000 associated jobs annually to the state's economy. Public hiking and hunting access has also been secured in extensive commercial forests. The state has the highest number of golf courses and registered snowmobiles in the nation.

The state has numerous historical markers, which can themselves become the center of a tour. The Great Lakes Circle Tour is a designated scenic road system connecting all of the Great Lakes and the St. Lawrence River.

With its position in relation to the Great Lakes and the countless ships that have foundered over the many years they have been used as a transport route for people and bulk cargo, Michigan is a world-class scuba diving destination. The Michigan Underwater Preserves are 11 underwater areas where wrecks are protected for the benefit of sport divers.

Michigan has nine international road crossings with Ontario, Canada:

A second international bridge between Detroit and Windsor is under consideration.

Michigan is served by four Class I railroads: the Canadian National Railway, the Canadian Pacific Railway, CSX Transportation, and the Norfolk Southern Railway. These are augmented by several dozen short line railroads. The vast majority of rail service in Michigan is devoted to freight, with Amtrak and various scenic railroads the exceptions.
Amtrak passenger rail services the state, connecting many southern and western Michigan cities to Chicago, Illinois. There are plans for commuter rail for Detroit and its suburbs (see SEMCOG Commuter Rail).


The Detroit Metropolitan Wayne County Airport, in the western suburb of Romulus, was in 2010 the 16th busiest airfield in North America measured by passenger traffic. The Gerald R. Ford International Airport in Grand Rapids is the next busiest airport in the state, served by eight airlines to 23 destinations. Flint Bishop International Airport is the third largest airport in the state, served by four airlines to several primary hubs. Smaller regional and local airports are located throughout the state including on several islands. Cherry Capital Airport is in Traverse City.

Other economically significant cities include:

Half of the wealthiest communities in the state are in Oakland County, just north of Detroit. Another wealthy community is just east of the city, in Grosse Pointe. Only three of these cities are outside of Metro Detroit. The city of Detroit, with a per capita income of $14,717, ranks 517th on the list of Michigan locations by per capita income. Benton Harbor is the poorest city in Michigan, with a per capita income of $8,965, while Barton Hills is the richest with a per capita income of $110,683.

Michigan's education system provides services to 1.6 million K-12 students in public schools. More than 124,000 students attend private schools and an uncounted number are home-schooled under certain legal requirements. The public school system has a $14.5 billion budget in 2008–2009. Michigan has a number of public universities spread throughout the state and numerous private colleges as well. Michigan State University has the eighth largest campus population of any U.S. school. Seven of the state's universities—Central Michigan University, University of Michigan, Michigan State University, Michigan Technological University, Oakland University, Wayne State University, and Western Michigan University—are classified as research universities by the Carnegie Foundation.

Michigan music is known for three music trends: early punk rock, Motown/soul music and techno music. Michigan musicians include Bill Haley & His Comets, The Supremes, The Marvelettes, The Temptations, The Four Tops, Stevie Wonder, Marvin Gaye "The Prince of Soul", Smokey Robinson and the Miracles, Aretha Franklin, Mary Wells, Tommy James and the Shondells, ? and the Mysterians, Al Green, The Spinners, Grand Funk Railroad, The Stooges, the MC5, The Knack, Madonna "The Queen of Pop", Bob Seger, Ray Parker Jr., Aaliyah, Eminem, Kid Rock, Jack White and Meg White (The White Stripes), Big Sean, and Alice Cooper.

Major theaters in Michigan include the Fox Theatre, Music Hall, Gem Theatre, Masonic Temple Theatre, the Detroit Opera House, Fisher Theatre, The Fillmore Detroit, Saint Andrew's Hall, Majestic Theater, and Orchestra Hall.

The Nederlander Organization, the largest controller of Broadway productions in New York City, originated in Detroit. Detroit Symphony Orchestra

Motown Motion Picture Studios with produces movies in Detroit and the surrounding area based at the Pontiac Centerpoint Business Campus.

Michigan's major-league sports teams include: Detroit Tigers baseball team, Detroit Lions football team, Detroit Red Wings ice hockey team, and the Detroit Pistons men's basketball team. All of Michigan's major league teams play in the Metro Detroit area.

The Pistons played at Detroit's Cobo Arena until 1978 and at the Pontiac Silverdome until 1988 when they moved into The Palace of Auburn Hills. In 2017, the team moved to the newly built Little Caesars Arena in downtown Detroit. The Detroit Lions played at Tiger Stadium in Detroit until 1974, then moved to the Pontiac Silverdome where they played for 27 years between 1975 and 2002 before moving to Ford Field in Detroit in 2002. The Detroit Tigers played at Tiger Stadium (formerly known as Navin Field and Briggs Stadium) from 1912 to 1999. In 2000 they moved to Comerica Park. The Red Wings played at Olympia Stadium before moving to Joe Louis Arena in 1979. They later moved to Little Caesars Arena to join the Pistons as tenants in 2017. Professional hockey got its start in Houghton, when the Portage Lakers were formed.

The Michigan International Speedway is the site of NASCAR races and Detroit was formerly the site of a Formula One World Championship Grand Prix race. From 1959 to 1961, Detroit Dragway hosted the NHRA's U.S. Nationals. Michigan is home to one of the major canoeing marathons: the Au Sable River Canoe Marathon. The Port Huron to Mackinac Boat Race is also a favorite.

Twenty-time Grand Slam champion Serena Williams was born in Saginaw. The 2011 World Champion for Women's Artistic Gymnastics, Jordyn Wieber is from DeWitt. Wieber was also a member of the gold medal winning team at the London Olympics in 2012.

Collegiate sports in Michigan are popular in addition to professional sports. The state's two largest athletic programs are the Michigan Wolverines and Michigan State Spartans, which play in the NCAA Big Ten Conference. Michigan Stadium in Ann Arbor, home to the Michigan Wolverines football team, is the largest stadium in the Western Hemisphere and the second-largest stadium worldwide behind Rungrado May Day Stadium in Pyongyang, North Korea.

The Michigan High School Athletic Association features around 300,000 participants.

Michigan is, by tradition, known as "The Wolverine State," and the University of Michigan takes the wolverine as its mascot. The association is well and long established: for example, many Detroiters volunteered to fight during the American Civil War and George Armstrong Custer, who led the Michigan Brigade, called them the "Wolverines". The origins of this association are obscure; it may derive from a busy trade in wolverine furs in Sault Ste. Marie in the 18th century or may recall a disparagement intended to compare early settlers in Michigan with the vicious mammal. Wolverines are, however, extremely rare in Michigan. A sighting in February 2004 near Ubly was the first confirmed sighting in Michigan in 200 years. The animal was found dead in 2010.





</doc>
<doc id="18862" url="https://en.wikipedia.org/wiki?curid=18862" title="Minimum wage">
Minimum wage

A minimum wage is the lowest remuneration that employers can legally pay their workers. Equivalently, it is the price floor below which workers may not sell their labor. Although minimum wage laws are in effect in many jurisdictions, differences of opinion exist about the benefits and drawbacks of a minimum wage. Supporters of the minimum wage say it increases the standard of living of workers, reduces poverty, reduces inequality, and boosts morale. In contrast, opponents of the minimum wage say it increases poverty, increases unemployment (particularly among unskilled or inexperienced workers) and is damaging to businesses, because excessively high minimum wages require businesses to raise the prices of their product or service to accommodate the extra expense of paying a higher wage.

Supply and demand models point to welfare and employment losses from minimum wages. However, if the labor market is not perfectly competitive, minimum wages can increase the efficiency of the market. For example, in a monopsony labor market, a minimum wage set modestly above equilibrium wages can increase wages, employment, and economic efficiency. Considerable debate still exists among economists about the real-world effect of minimum wages.

Modern national laws enforcing compulsory union membership which prescribed minimum wages for their members were first passed in New Zealand and Australia in the 1890s. The movement for minimum wages was first motivated as a way to stop the exploitation of workers in sweatshops, by employers who were thought to have unfair bargaining power over them. Over time, minimum wages came to be seen as a way to help lower-income families. Most countries had introduced minimum wage legislation by the end of the 20th century.

Modern minimum wage laws trace their origin to the Ordinance of Labourers (1349), which was a decree by King Edward III that set a" maximum wage" for laborers in medieval England. King Edward III, who was a wealthy landowner, was dependent, like his lords, on serfs to work the land. In the autumn of 1348, the Black Plague reached England and decimated the population. The severe shortage of labor caused wages to soar and encouraged King Edward III to set a wage ceiling. Subsequent amendments to the ordinance, such as the Statute of Labourers (1351), increased the penalties for paying a wage above the set rates.

While the laws governing wages initially set a ceiling on compensation, they were eventually used to set a living wage. An amendment to the Statute of Labourers in 1389 effectively fixed wages to the price of food. As time passed, the Justice of the Peace, who was charged with setting the maximum wage, also began to set formal minimum wages. The practice was eventually formalized with the passage of the Act Fixing a Minimum Wage in 1604 by King James I for workers in the textile industry.

By the early 19th century, the Statutes of Labourers was repealed as increasingly capitalistic England embraced "laissez-faire" policies which disfavored regulations of wages (whether upper or lower limits). The subsequent 19th century saw significant labor unrest affect many industrial nations. As trade unions were decriminalized during the century, attempts to control wages through collective agreement were made. However, this meant that a uniform minimum wage was not possible. In "Principles of Political Economy" in 1848, John Stuart Mill argued that because of the collective action problems that workers faced in organisation, it was a justified departure from "laissez-faire" policies (or freedom of contract) to regulate people's wages and hours by law.

It was not until the 1890s that the first modern legislative attempts to regulate minimum wages were seen in New Zealand and Australia. The movement for a minimum wage was initially focused on stopping sweatshop labor and controlling the proliferation of sweatshops in manufacturing industries. The sweatshops employed large numbers of women and young workers, paying them what were considered to be substandard wages. The sweatshop owners were thought to have unfair bargaining power over their employees, and a minimum wage was proposed as a means to make them pay fairly. Over time, the focus changed to helping people, especially families, become more self-sufficient.

The first modern national minimum wages were enacted by the government recognition of unions which in turn established minimum wage policy among their members, as in New Zealand in 1894, followed by Australia in 1896 and the United Kingdom in 1909. In the United States, statutory minimum wages were first introduced nationally in 1938, and they were reintroduced and expanded in the United Kingdom in 1998. There is now legislation or binding collective bargaining regarding minimum wage in more than 90 percent of all countries. In the European Union, 22 member states out of 28 currently have national minimum wages. Other countries, such as Sweden, Finland, Denmark, Switzerland, Austria, and Italy, have no minimum wage laws, but rely on employer groups and trade unions to set minimum earnings through collective bargaining.

Minimum wage rates vary greatly across many different jurisdictions, not only in setting a particular amount of money—for example $7.25 per hour ($14,500 per year) under certain US state laws (or $2.13 for employees who receive tips, which is known as the tipped minimum wage), $11.00 in the US state of Washington, or £7.83 (for those aged 25+) in the United Kingdom—but also in terms of which pay period (for example Russia and China set monthly minimum wages) or the scope of coverage. Currently the American federal minimum wage rests at seven dollars, twenty-five cents ($7.25) per hour. However, some states do not recognize the minimum wage law such as Louisiana and Tennessee. Other states operate below the federal minimum wage such as Georgia and Wyoming. Some jurisdictions even allow employers to count tips given to their workers as credit towards the minimum wage levels. India was one of the first developing countries to introduce minimum wage policy. It also has one of the most complicated systems with more than 1,200 minimum wage rates.

New York City passed a new law that allow to rise the minimum wage to $15 per hour by the end of 2018. New York is the second state that support to rise the minimum wage after president Donald Trump has been elected. In the beginning of 2018, eighteen states began to increase the minimum wage based on the local cost of living. These eighteen states include Alaska, Florida, Minnesota, Missouri, Montana, New Jersey, Ohio, South Dakota, Arizona, California, Colorado, Hawaii, Maine, Michigan, New York, Rhode Island, Vermont and Washington.
Customs and extra-legal pressures from governments or labor unions can produce a "de facto" minimum wage. So can international public opinion, by pressuring multinational companies to pay Third World workers wages usually found in more industrialized countries. The latter situation in Southeast Asia and Latin America was publicized in the 2000s, but it existed with companies in West Africa in the middle of the twentieth century.

Among the indicators that might be used to establish an initial minimum wage rate are ones that minimize the loss of jobs while preserving international competitiveness. Among these are general economic conditions as measured by real and nominal gross domestic product; inflation; labor supply and demand; wage levels, distribution and differentials; employment terms; productivity growth; labor costs; business operating costs; the number and trend of bankruptcies; economic freedom rankings; standards of living and the prevailing average wage rate.

In the business sector, concerns include the expected increased cost of doing business, threats to profitability, rising levels of unemployment (and subsequent higher government expenditure on welfare benefits raising tax rates), and the possible knock-on effects to the wages of more experienced workers who might already be earning the new statutory minimum wage, or slightly more. Among workers and their representatives, political consideration weigh in as labor leaders seek to win support by demanding the highest possible rate. Other concerns include purchasing power, inflation indexing and standardized working hours.

In the United States, the minimum wage promulgated by the Fair Labor Standards Act of 1938. According to the Economic Policy Institute, the minimum wage in the United States would have been $18.28 in 2013 if the minimum wage had kept pace with labor productivity. To adjust for increased rates of worker productivity in the United States, raising the minimum wage to $22 (or more) an hour has been presented.

According to the supply and demand model of the labor market shown in many economics textbooks, increasing the minimum wage decreases the employment of minimum-wage workers. One such textbook states:
A firm's cost is an increasing function of the wage rate, the higher the wage rate, the fewer hours an employer will demand of employees. This is because, as the wage rate rises, it becomes more expensive for firms to hire workers and so firms hire fewer workers (or hire them for fewer hours). The demand of labor curve is therefore shown as a line moving down and to the right. Since higher wages increase the quantity supplied, the supply of labor curve is upward sloping, and is shown as a line moving up and to the right. If no minimum wage is in place, wages will adjust until quantity of labor demanded is equal to quantity supplied, reaching equilibrium, where the supply and demand curves intersect. Minimum wage behaves as a classical price floor on labor. Standard theory says that, if set above the equilibrium price, more labor will be willing to be provided by workers than will be demanded by employers, creating a surplus of labor, i.e. unemployment. The economic model of markets predicts the same of other commodities (like milk and wheat, for example): Artificially raising the price of the commodity tends to cause the supply of it to increase and the demand for it to lessen. The result is a surplus of the commodity. When there is a wheat surplus, the government buys it. Since the government does not hire surplus labor, the labor surplus takes the form of unemployment, which tends to be higher with minimum wage laws than without them.

The supply and demand model implies that by mandating a price floor above the equilibrium wage, minimum wage laws will cause unemployment. This is because a greater number of people are willing to work at the higher wage while a smaller number of jobs will be available at the higher wage. Companies can be more selective in those whom they employ thus the least skilled and least experienced will typically be excluded. An imposition or increase of a minimum wage will generally only affect employment in the low-skill labor market, as the equilibrium wage is already at or below the minimum wage, whereas in higher skill labor markets the equilibrium wage is too high for a change in minimum wage to affect employment.

The supply and demand model predicts that raising the minimum wage helps workers whose wages are raised, and hurts people who are not hired (or lose their jobs) when companies cut back on employment. But proponents of the minimum wage hold that the situation is much more complicated than the model can account for. One complicating factor is possible monopsony in the labor market, whereby the individual employer has some market power in determining wages paid. Thus it is at least theoretically possible that the minimum wage may boost employment. Though single employer market power is unlikely to exist in most labor markets in the sense of the traditional 'company town,' asymmetric information, imperfect mobility, and the personal element of the labor transaction give some degree of wage-setting power to most firms.

Modern economic theory predicts that although an excessive minimum wage may raise unemployment as it fixes a price above most demand for labor, a minimum wage at a more reasonable level can increase employment, and enhance growth and efficiency. This is because labor markets are monopsonistic and workers persistently lack bargaining power. When poorer workers have more to spend it stimulates effective aggregate demand for goods and services.

The argument that a minimum wage decreases employment is based on a simple supply and demand model of the labor market. A number of economists (for example Pierangelo Garegnani, Robert L. Vienneau, and Arrigo Opocher & Ian Steedman), building on the work of Piero Sraffa, argue that that model, even given all its assumptions, is logically incoherent. Michael Anyadike-Danes and Wynne Godley argue, based on simulation results, that little of the empirical work done with the textbook model constitutes a potentially falsifiable theory, and consequently empirical evidence hardly exists for that model. Graham White argues, partially on the basis of Sraffianism, that the policy of increased labor market flexibility, including the reduction of minimum wages, does not have an "intellectually coherent" argument in economic theory.Gary Fields, Professor of Labor Economics and Economics at Cornell University, argues that the standard textbook model for the minimum wage is ambiguous, and that the standard theoretical arguments incorrectly measure only a one-sector market. Fields says a two-sector market, where "the self-employed, service workers, and farm workers are typically excluded from minimum-wage coverage... [and with] one sector with minimum-wage coverage and the other without it [and possible mobility between the two]," is the basis for better analysis. Through this model, Fields shows the typical theoretical argument to be ambiguous and says "the predictions derived from the textbook model definitely do not carry over to the two-sector case. Therefore, since a non-covered sector exists nearly everywhere, the predictions of the textbook model simply cannot be relied on."

An alternate view of the labor market has low-wage labor markets characterized as monopsonistic competition wherein buyers (employers) have significantly more market power than do sellers (workers). This monopsony could be a result of intentional collusion between employers, or naturalistic factors such as segmented markets, search costs, information costs, imperfect mobility and the personal element of labor markets. In such a case a simple supply and demand graph would not yield the quantity of labor clearing and the wage rate. This is because while the upward sloping aggregate labor supply would remain unchanged, instead of using the upward labor supply curve shown in a supply and demand diagram, monopsonistic employers would use a steeper upward sloping curve corresponding to marginal expenditures to yield the intersection with the supply curve resulting in a wage rate lower than would be the case under competition. Also, the amount of labor sold would also be lower than the competitive optimal allocation.

Such a case is a type of market failure and results in workers being paid less than their marginal value. Under the monopsonistic assumption, an appropriately set minimum wage could increase both wages and employment, with the optimal level being equal to the marginal product of labor. This view emphasizes the role of minimum wages as a market regulation policy akin to antitrust policies, as opposed to an illusory "free lunch" for low-wage workers.

Another reason minimum wage may not affect employment in certain industries is that the demand for the product the employees produce is highly inelastic. For example, if management is forced to increase wages, management can pass on the increase in wage to consumers in the form of higher prices. Since demand for the product is highly inelastic, consumers continue to buy the product at the higher price and so the manager is not forced to lay off workers. Economist Paul Krugman argues this explanation neglects to explain why the firm was not charging this higher price absent the minimum wage.

Three other possible reasons minimum wages do not affect employment were suggested by Alan Blinder: higher wages may reduce turnover, and hence training costs; raising the minimum wage may "render moot" the potential problem of recruiting workers at a higher wage than current workers; and minimum wage workers might represent such a small proportion of a business's cost that the increase is too small to matter. He admits that he does not know if these are correct, but argues that "the list demonstrates that one can accept the new empirical findings and still be a card-carrying economist."

Economists disagree as to the measurable impact of minimum wages in practice. This disagreement usually takes the form of competing empirical tests of the elasticities of supply and demand in labor markets and the degree to which markets differ from the efficiency that models of perfect competition predict.

Economists have done empirical studies on different aspects of the minimum wage, including:
Until the mid-1990s, a general consensus existed among economists, both conservative and liberal, that the minimum wage reduced employment, especially among younger and low-skill workers. In addition to the basic supply-demand intuition, there were a number of empirical studies that supported this view. For example, Gramlich (1976) found that many of the benefits went to higher income families, and that teenagers were made worse off by the unemployment associated with the minimum wage.

Brown et al. (1983) noted that time series studies to that point had found that for a 10 percent increase in the minimum wage, there was a decrease in teenage employment of 1–3 percent. However, the studies found wider variation, from 0 to over 3 percent, in their estimates for the effect on teenage unemployment (teenagers without a job and looking for one). In contrast to the simple supply and demand diagram, it was commonly found that teenagers withdrew from the labor force in response to the minimum wage, which produced the possibility of equal reductions in the supply as well as the demand for labor at a higher minimum wage and hence no impact on the unemployment rate. Using a variety of specifications of the employment and unemployment equations (using ordinary least squares vs. generalized least squares regression procedures, and linear vs. logarithmic specifications), they found that a 10 percent increase in the minimum wage caused a 1 percent decrease in teenage employment, and no change in the teenage unemployment rate. The study also found a small, but statistically significant, increase in unemployment for adults aged 20–24.

Wellington (1991) updated Brown et al.'s research with data through 1986 to provide new estimates encompassing a period when the real (i.e., inflation-adjusted) value of the minimum wage was declining, because it had not increased since 1981. She found that a 10% increase in the minimum wage decreased the absolute teenage employment by 0.6%, with no effect on the teen or young adult unemployment rates.

Some research suggests that the unemployment effects of small minimum wage increases are dominated by other factors. In Florida, where voters approved an increase in 2004, a follow-up comprehensive study after the increase confirmed a strong economy with increased employment above previous years in Florida and better than in the US as a whole. When it comes to on-the-job training, some believe the increase in wages is taken out of training expenses. A 2001 empirical study found that there is "no evidence that minimum wages reduce training, and little evidence that they tend to increase training."

Some empirical studies have tried to ascertain the benefits of a minimum wage beyond employment effects. In an analysis of census data, Joseph Sabia and Robert Nielson found no statistically significant evidence that minimum wage increases helped reduce financial, housing, health, or food insecurity. This study was undertaken by the Employment Policies Institute, a think tank funded by the food, beverage and hospitality industries. In 2012, Michael Reich published an economic analysis that suggested that a proposed minimum wage hike in San Diego might stimulate the city's economy by about $190 million.

"The Economist" wrote in December 2013: "A minimum wage, providing it is not set too high, could thus boost pay with no ill effects on jobs...America's federal minimum wage, at 38% of median income, is one of the rich world's lowest. Some studies find no harm to employment from federal or state minimum wages, others see a small one, but none finds any serious damage. ... High minimum wages, however, particularly in rigid labour markets, do appear to hit employment. France has the rich world’s highest wage floor, at more than 60% of the median for adults and a far bigger fraction of the typical wage for the young. This helps explain why France also has shockingly high rates of youth unemployment: 26% for 15- to 24-year-olds."

In 1992, the minimum wage in New Jersey increased from $4.25 to $5.05 per hour (an 18.8% increase), while in the adjacent state of Pennsylvania it remained at $4.25. David Card and Alan Krueger gathered information on fast food restaurants in New Jersey and eastern Pennsylvania in an attempt to see what effect this increase had on employment within New Jersey. A basic supply and demand model predicts that relative employment should have decreased in New Jersey. Card and Krueger surveyed employers before the April 1992 New Jersey increase, and again in November–December 1992, asking managers for data on the full-time equivalent staff level of their restaurants both times. Based on data from the employers' responses, the authors concluded that the increase in the minimum wage slightly increased employment in the New Jersey restaurants.

Card and Krueger expanded on this initial article in their 1995 book "Myth and Measurement: The New Economics of the Minimum Wage". They argued that the negative employment effects of minimum wage laws are minimal if not non-existent. For example, they look at the 1992 increase in New Jersey's minimum wage, the 1988 rise in California's minimum wage, and the 1990–91 increases in the federal minimum wage. In addition to their own findings, they reanalyzed earlier studies with updated data, generally finding that the older results of a negative employment effect did not hold up in the larger datasets.

In 1996, David Neumark and William Wascher reexamined Card and Krueger's result using administrative payroll records from a sample of large fast food restaurant chains, and reported that minimum wage increases were followed by decreases in employment. An assessment of data collected and analyzed by Neumark and Wascher did not initially contradict the Card and Krueger results, but in a later edited version they found a four percent decrease in employment, and reported that "the estimated disemployment effects in the payroll data are often statistically significant at the 5- or 10-percent level although there are some estimators and subsamples that yield insignificant—although almost always negative" employment effects. Neumark and Wascher's conclusions were subsequently rebutted in a 2000 paper by Card and Krueger. A 2011 paper has reconciled the difference between Card and Krueger's survey data and Neumark and Wascher's payroll-based data. The paper shows that both datasets evidence conditional employment effects that are positive for small restaurants, but are negative for large fast-food restaurants. A 2014 analysis based on panel data found that the minimum wage reduces employment among teenagers.

In 1996 and 1997, the federal minimum wage was increased from $4.25 to $5.15, thereby increasing the minimum wage by $0.90 in Pennsylvania but by just $0.10 in New Jersey; this allowed for an examination of the effects of minimum wage increases in the same area, subsequent to the 1992 change studied by Card and Krueger. A study by Hoffman and Trace found the result anticipated by traditional theory: a detrimental effect on employment.

Further application of the methodology used by Card and Krueger by other researchers yielded results similar to their original findings, across additional data sets. A 2010 study by three economists (Arindrajit Dube of the University of Massachusetts Amherst, William Lester of the University of North Carolina at Chapel Hill, and Michael Reich of the University of California, Berkeley), compared adjacent counties in different states where the minimum wage had been raised in one of the states. They analyzed employment trends for several categories of low-wage workers from 1990 to 2006 and found that increases in minimum wages had no negative effects on low-wage employment and successfully increased the income of workers in food services and retail employment, as well as the narrower category of workers in restaurants.

However, a 2011 study by Baskaya and Rubinstein of Brown University found that at the federal level, "a rise in minimum wage have ["sic"] an instantaneous impact on wage rates and a corresponding negative impact on employment", stating, "Minimum wage increases boost teenage wage rates and reduce teenage employment." Another 2011 study by Sen, Rybczynski, and Van De Waal found that "a 10% increase in the minimum wage is significantly correlated with a 3−5% drop in teen employment." A 2012 study by Sabia, Hansen, and Burkhauser found that "minimum wage increases can have substantial adverse labor demand effects for low-skilled individuals", with the largest effects on those aged 16 to 24.

A 2013 study by Meer and West concluded that "the minimum wage reduces net job growth, primarily through its effect on job creation by expanding establishments ... most pronounced for younger workers and in industries with a higher proportion of low-wage workers." This study by Meer and West was later critiqued for its trends of assumption in the context of narrowly defined low-wage groups. The authors replied to the critiques and released additional data which addressed the criticism of their methodology, but did not resolve the issue of whether their data showed a causal relationship. Another 2013 study by Suzana Laporšek of the University of Primorska, on youth unemployment in Europe claimed there was "a negative, statistically significant impact of minimum wage on youth employment." A 2013 study by labor economists Tony Fang and Carl Lin which studied minimum wages and employment in China, found that "minimum wage changes have significant adverse effects on employment in the Eastern and Central regions of China, and result in disemployment for females, young adults, and low-skilled workers".

Several researchers have conducted statistical meta-analyses of the employment effects of the minimum wage. In 1995, Card and Krueger analyzed 14 earlier time-series studies on minimum wages and concluded that there was clear evidence of publication bias (in favor of studies that found a statistically significant negative employment effect). They point out that later studies, which had more data and lower standard errors, did not show the expected increase in t-statistic (almost all the studies had a t-statistic of about two, just above the level of statistical significance at the .05 level). Though a serious methodological indictment, opponents of the minimum wage largely ignored this issue; as Thomas Leonard noted, "The silence is fairly deafening."

In 2005, T.D. Stanley showed that Card and Krueger's results could signify either publication bias or the absence of a minimum wage effect. However, using a different methodology, Stanley concluded that there is evidence of publication bias and that correction of this bias shows no relationship between the minimum wage and unemployment. In 2008, Hristos Doucouliagos and T.D. Stanley conducted a similar meta-analysis of 64 U.S. studies on disemployment effects and concluded that Card and Krueger's initial claim of publication bias is still correct. Moreover, they concluded, "Once this publication selection is corrected, little or no evidence of a negative association between minimum wages and employment remains." In 2013, a meta-analysis of 16 UK studies found no significant effects on employment attributable to the minimum wage.

Minimum wage laws affect workers in most low-paid fields of employment and have usually been judged against the criterion of reducing poverty. Minimum wage laws receive less support from economists than from the general public. Despite decades of experience and economic research, debates about the costs and benefits of minimum wages continue today.

Various groups have great ideological, political, financial, and emotional investments in issues surrounding minimum wage laws. For example, agencies that administer the laws have a vested interest in showing that "their" laws do not create unemployment, as do labor unions whose members' finances are protected by minimum wage laws. On the other side of the issue, low-wage employers such as restaurants finance the Employment Policies Institute, which has released numerous studies opposing the minimum wage. The presence of these powerful groups and factors means that the debate on the issue is not always based on dispassionate analysis. Additionally, it is extraordinarily difficult to separate the effects of minimum wage from all the other variables that affect employment.

The following table summarizes the arguments made by those for and against minimum wage laws:

A widely circulated argument that the minimum wage was ineffective at reducing poverty was provided by George Stigler in 1949:
In 2006, the International Labour Organization (ILO) argued that the minimum wage could not be directly linked to unemployment in countries that have suffered job losses. In April 2010, the Organisation for Economic Co-operation and Development (OECD) released a report arguing that countries could alleviate teen unemployment by "lowering the cost of employing low-skilled youth" through a sub-minimum training wage. A study of U.S. states showed that businesses' annual and average payrolls grow faster and employment grew at a faster rate in states with a minimum wage. The study showed a correlation, but did not claim to prove causation.

Although strongly opposed by both the business community and the Conservative Party when introduced in the UK in 1999, the Conservatives reversed their opposition in 2000. Accounts differ as to the effects of the minimum wage. The Centre for Economic Performance found no discernible impact on employment levels from the wage increases, while the Low Pay Commission found that employers had reduced their rate of hiring and employee hours employed, and found ways to cause current workers to be more productive (especially service companies). The Institute for the Study of Labor found prices in the minimum wage sector rose significantly faster than prices in non-minimum wage sectors, in the four years following the implementation of the minimum wage. Neither trade unions nor employer organizations contest the minimum wage, although the latter had especially done so heavily until 1999.

In 2014, supporters of minimum wage cited a study that found that job creation within the United States is faster in states that raised their minimum wages. In 2014, supporters of minimum wage cited news organizations who reported the state with the highest minimum-wage garnered more job creation than the rest of the United States.

In 2014, in Seattle, Washington, liberal and progressive business owners who had supported the city's new $15 minimum wage said they might hold off on expanding their businesses and thus creating new jobs, due to the uncertain timescale of the wage increase implementation. However, subsequently at least two of the business owners quoted did expand.

The dollar value of the minimum wage loses purchasing power over time due to inflation. Minimum wage laws, for instance proposals to index the minimum wage to average wages, have the potential to keep the dollar value of the minimum wage relevant and predictable.

With regard to the economic effects of introducing minimum wage legislation in Germany in January 2015, recent developments have shown that the feared increase in unemployment has not materialized, however, in some economic sectors and regions of the country, it came to a decline in job opportunities particularly for temporary and part-time workers, and some low-wage jobs have disappeared entirely. Because of this overall positive development, the Deutsche Bundesbank revised its opinion, and ascertained that “the impact of the introduction of the minimum wage on the total volume of work appears to be very limited in the present business cycle”.

According to a 1978 article in the "American Economic Review", 90% of the economists surveyed agreed that the minimum wage increases unemployment among low-skilled workers. By 1992 the survey found 79% of economists in agreement with that statement, and by 2000, 46% were in full agreement with the statement and 28% agreed with provisos (74% total). The authors of the 2000 study also reweighted data from a 1990 sample to show that at that time 62% of academic economists agreed with the statement above, while 20% agreed with provisos and 18% disagreed. They state that the reduction on consensus on this question is "likely" due to the Card and Krueger research and subsequent debate.

A similar survey in 2006 by Robert Whaples polled PhD members of the American Economic Association (AEA). Whaples found that 47% respondents wanted the minimum wage eliminated, 38% supported an increase, 14% wanted it kept at the current level, and 1% wanted it decreased. Another survey in 2007 conducted by the University of New Hampshire Survey Center found that 73% of labor economists surveyed in the United States believed 150% of the then-current minimum wage would result in employment losses and 68% believed a mandated minimum wage would cause an increase in hiring of workers with greater skills. 31% felt that no hiring changes would result.

Surveys of labor economists have found a sharp split on the minimum wage. Fuchs et al. (1998) polled labor economists at the top 40 research universities in the United States on a variety of questions in the summer of 1996. Their 65 respondents were nearly evenly divided when asked if the minimum wage should be increased. They argued that the different policy views were not related to views on whether raising the minimum wage would reduce teen employment (the median economist said there would be a reduction of 1%), but on value differences such as income redistribution. Daniel B. Klein and Stewart Dompe conclude, on the basis of previous surveys, "the average level of support for the minimum wage is somewhat higher among labor economists than among AEA members."

In 2007, Klein and Dompe conducted a non-anonymous survey of supporters of the minimum wage who had signed the "Raise the Minimum Wage" statement published by the Economic Policy Institute. 95 of the 605 signatories responded. They found that a majority signed on the grounds that it transferred income from employers to workers, or equalized bargaining power between them in the labor market. In addition, a majority considered disemployment to be a moderate potential drawback to the increase they supported.

In 2013, a diverse group of 37 economics professors was surveyed on their view of the minimum wage's impact on employment. 34% of respondents agreed with the statement, "Raising the federal minimum wage to $9 per hour would make it noticeably harder for low-skilled workers to find employment." 32% disagreed and the remaining respondents were uncertain or had no opinion on the question. 47% agreed with the statement, "The distortionary costs of raising the federal minimum wage to $9 per hour and indexing it to inflation are sufficiently small compared with the benefits to low-skilled workers who can find employment that this would be a desirable policy", while 11% disagreed.

Economists and other political commentators have proposed alternatives to the minimum wage. They argue that these alternatives may address the issue of poverty better than a minimum wage, as it would benefit a broader population of low wage earners, not cause any unemployment, and distribute the costs widely rather than concentrating it on employers of low wage workers.

A basic income (or negative income tax) is a system of social security that periodically provides each citizen with a sum of money that is sufficient to live on frugally. It is argued that recipients of the basic income would have considerably more bargaining power when negotiating a wage with an employer as there would be no risk of destitution for not taking the employment. As a result, the jobseeker could spend more time looking for a more appropriate or satisfying job, or they could wait until a higher-paying job appeared. Alternatively, they could spend more time increasing their skills in university, which would make them more suitable for higher-paying jobs, as well as provide numerous other benefits. Experiments on Basic Income and NIT in Canada and the USA show that people spent more time studying while the program was running.

Proponents argue that a basic income that is based on a broad tax base would be more economically efficient, as the minimum wage effectively imposes a high marginal tax on employers, causing losses in efficiency.

A guaranteed minimum income is another proposed system of social welfare provision. It is similar to a basic income or negative income tax system, except that it is normally conditional and subject to a means test. Some proposals also stipulate a willingness to participate in the labor market, or a willingness to perform community services.

A refundable tax credit is a mechanism whereby the tax system can reduce the tax owed by a household to below zero, and result in a net payment to the taxpayer beyond their own payments into the tax system. Examples of refundable tax credits include the earned income tax credit and the additional child tax credit in the US, and working tax credits and child tax credits in the UK. Such a system is slightly different from a negative income tax, in that the refundable tax credit is usually only paid to households that have earned at least some income. This policy is more targeted against poverty than the minimum wage, because it avoids subsidizing low-income workers who are supported by high-income households (for example, teenagers still living with their parents).

In the United States, earned income tax credit rates, also known as EITC or EIC, vary by state—some are refundable while other states do not allow a refundable tax credit. The federal EITC program has been expanded by a number of presidents including Jimmy Carter, Ronald Reagan, George H.W. Bush, and Bill Clinton. In 1986, President Reagan described the EITC as "the best anti poverty, the best pro-family, the best job creation measure to come out of Congress." The ability of the earned income tax credit to deliver larger monetary benefits to the poor workers than an increase in the minimum wage and at a lower cost to society was documented in a 2007 report by the Congressional Budget Office.

The Adam Smith Institute prefers cutting taxes on the poor and middle class instead of raising wages as an alternative to the minimum wage.

Italy, Sweden, Norway, Finland, and Denmark are examples of developed nations where there is no minimum wage that is required by legislation. Such nations, particularly the Nordics, have very high union participation rates. Instead, minimum wage standards in different sectors are set by collective bargaining.

Some economists such as Scott Sumner and Edmund Phelps advocate a wage subsidy program. A wage subsidy is a payment made by a government for work people do. It is based either on an hourly basis or by income earned. Advocates argue that the primary deficiencies of the EITC and the minimum wage are best avoided by a wage subsidy. However, the wage subsidy in the United States suffers from a lack of political support from either major political party.

In January 2014, seven Nobel economists—Kenneth Arrow, Peter Diamond, Eric Maskin, Thomas Schelling, Robert Solow, Michael Spence, and Joseph Stiglitz—and 600 other economists wrote a letter to the US Congress and the US President urging that, by 2016, the US government should raise the minimum wage to $10.10. They endorsed the Minimum Wage Fairness Act which was introduced by US Senator Tom Harkin in 2013. U.S. Senator Bernie Sanders introduced a bill in 2015 that would raise the minimum wage to $15, and in his 2016 campaign for president ran on a platform of increasing it. Although Sanders did not become the nominee, the Democratic National Committee adopted his $15 minimum wage push in their 2016 party platform.

Reactions from former McDonald's USA Ed Rensi about raising minimum wage to $15 is to completely push humans out of the picture when it comes to labor if they are to pay minimum wage at $15 they would look into replacing humans with machines as that would be the more cost-effective than having employees that are ineffective. During an interview on FOX Business Network’s Mornings with Maria, he stated that he believes an increase to $15 an hour would cause job loss at an extraordinary level. Rensi also believes it does not only affect the fast food industry, franchising he sees as the best business model in the United States, it is dependent on people that have low job skills that have to grow and if you cannot pay them a reasonable wage then they are going to be replaced with machines.

In late March 2016, Governor of California Jerry Brown reached a deal to raise the minimum wage to $15 by 2022 for big businesses and 2023 for smaller businesses.

In contrast, the relatively high minimum wage in Puerto Rico has been blamed by various politicians and commentators as a highly significant factor in the Puerto Rican government-debt crisis. One study concluded that "Employers are disinclined to hire workers because the US federal minimum wage is very high relative to the local average".

, unions were exempt from recent minimum wage increases in Chicago, Illinois, SeaTac, Washington, and Milwaukee County, Wisconsin, as well as the California cities of Los Angeles, San Francisco, Long Beach, San Jose, Richmond, and Oakland.




</doc>
<doc id="18864" url="https://en.wikipedia.org/wiki?curid=18864" title="Mullet">
Mullet

Mullet may refer to:







</doc>
<doc id="18866" url="https://en.wikipedia.org/wiki?curid=18866" title="Macbeth">
Macbeth

Macbeth (; full title The Tragedy of Macbeth) is a tragedy by William Shakespeare; it is thought to have been first performed in 1606. It dramatises the damaging physical and psychological effects of political ambition on those who seek power for its own sake. Of all the plays that Shakespeare wrote during the reign of James I, who was patron of Shakespeare's acting company, "Macbeth" most clearly reflects the playwright's relationship with his sovereign. It was first published in the Folio of 1623, possibly from a prompt book, and is Shakespeare's shortest tragedy.

A brave Scottish general named Macbeth receives a prophecy from a trio of witches that one day he will become King of Scotland. Consumed by ambition and spurred to action by his wife, Macbeth murders King Duncan and takes the Scottish throne for himself. He is then wracked with guilt and paranoia. Forced to commit more and more murders to protect himself from enmity and suspicion, he soon becomes a tyrannical ruler. The bloodbath and consequent civil war swiftly take Macbeth and Lady Macbeth into the realms of madness and death.

Shakespeare's source for the story is the account of Macbeth, King of Scotland; Macduff; and Duncan in "Holinshed's Chronicles" (1587), a history of England, Scotland, and Ireland familiar to Shakespeare and his contemporaries, although the events in the play differ extensively from the history of the real Macbeth. The events of the tragedy are usually associated with the execution of Henry Garnet for complicity in the Gunpowder Plot of 1605.

In the backstage world of theatre, some believe that the play is cursed, and will not mention its title aloud, referring to it instead as "The Scottish Play". Over the course of many centuries, the play has attracted some of the most renowned actors to the roles of Macbeth and Lady Macbeth. It has been adapted to film, television, opera, novels, comics, and other media.

The play opens amidst thunder and lightning, and the Three Witches decide that their next meeting shall be with Macbeth. In the following scene, a wounded sergeant reports to King Duncan of Scotland that his generals Macbeth, who is the Thane of Glamis, and Banquo have just defeated the allied forces of Norway and Ireland, who were led by the traitorous Macdonwald, and the Thane of Cawdor. Macbeth, the King's kinsman, is praised for his bravery and fighting prowess.

In the following scene, Macbeth and Banquo discuss the weather and their victory. As they wander onto a heath, the Three Witches enter and greet them with prophecies. Though Banquo challenges them first, they address Macbeth, hailing him as "Thane of Glamis," "Thane of Cawdor," and that he shall "be King hereafter." Macbeth appears to be stunned to silence. When Banquo asks of his own fortunes, the witches respond paradoxically, saying that he will be less than Macbeth, yet happier, less successful, yet more. He will father a line of kings though he himself will not be one. While the two men wonder at these pronouncements, the witches vanish, and another thane, Ross, arrives and informs Macbeth of his newly bestowed title: Thane of Cawdor. The first prophecy is thus fulfilled, and Macbeth, previously sceptical, immediately begins to harbour ambitions of becoming king.

King Duncan welcomes and praises Macbeth and Banquo, and declares that he will spend the night at Macbeth's castle at Inverness; he also names his son Malcolm as his heir. Macbeth sends a message ahead to his wife, Lady Macbeth, telling her about the witches' prophecies. Lady Macbeth suffers none of her husband's uncertainty and wishes him to murder Duncan in order to obtain kingship. When Macbeth arrives at Inverness, she overrides all of her husband's objections by challenging his manhood and successfully persuades him to kill the king that very night. He and Lady Macbeth plan to get Duncan's two chamberlains drunk so that they will black out; the next morning they will blame the chamberlains for the murder. They will be defenceless as they will remember nothing.

While Duncan is asleep, Macbeth stabs him, despite his doubts and a number of supernatural portents, including a hallucination of a bloody dagger. He is so shaken that Lady Macbeth has to take charge. In accordance with her plan, she frames Duncan's sleeping servants for the murder by placing bloody daggers on them. Early the next morning, Lennox, a Scottish nobleman, and Macduff, the loyal Thane of Fife, arrive. A porter opens the gate and Macbeth leads them to the king's chamber, where Macduff discovers Duncan's body. Macbeth murders the guards to prevent them from professing their innocence, but claims he did so in a fit of anger over their misdeeds. Duncan's sons Malcolm and Donalbain flee to England and Ireland, respectively, fearing that whoever killed Duncan desires their demise as well. The rightful heirs' flight makes them suspects and Macbeth assumes the throne as the new King of Scotland as a kinsman of the dead king. Banquo reveals this to the audience, and while sceptical of the new King Macbeth, he remembers the witches' prophecy about how his own descendants would inherit the throne; this makes him suspicious of Macbeth.

Despite his success, Macbeth, also aware of this part of the prophecy, remains uneasy. Macbeth invites Banquo to a royal banquet, where he discovers that Banquo and his young son, Fleance, will be riding out that night. Fearing Banquo's suspicions, Macbeth arranges to have him murdered, by hiring two men to kill them, later sending a Third Murderer. The assassins succeed in killing Banquo, but Fleance escapes. Macbeth becomes furious: he fears that his power remains insecure as long as an heir of Banquo remains alive.

At a banquet, Macbeth invites his lords and Lady Macbeth to a night of drinking and merriment. Banquo's ghost enters and sits in Macbeth's place. Macbeth raves fearfully, startling his guests, as the ghost is only visible to him. The others panic at the sight of Macbeth raging at an empty chair, until a desperate Lady Macbeth tells them that her husband is merely afflicted with a familiar and harmless malady. The ghost departs and returns once more, causing the same riotous anger and fear in Macbeth. This time, Lady Macbeth tells the lords to leave, and they do so.

Macbeth, disturbed, visits the three witches once more and asks them to reveal the truth of their prophecies to him. To answer his questions, they summon horrible apparitions, each of which offers predictions and further prophecies to put Macbeth's fears at rest. First, they conjure an armoured head, which tells him to beware of Macduff (IV.i.72). Second, a bloody child tells him that no one born of a woman shall be able to harm him. Thirdly, a crowned child holding a tree states that Macbeth will be safe until Great Birnam Wood comes to Dunsinane Hill. Macbeth is relieved and feels secure because he knows that all men are born of women and forests cannot move. Macbeth also asks whether Banquo's sons will ever reign in Scotland: the witches conjure a procession of eight crowned kings, all similar in appearance to Banquo, and the last carrying a mirror that reflects even more kings. Macbeth realises that these are all Banquo's descendants having acquired kingship in numerous countries. After the witches perform a mad dance and leave, Lennox enters and tells Macbeth that Macduff has fled to England. Macbeth orders Macduff's castle be seized, and, most cruelly, sends murderers to slaughter Macduff, as well as Macduff's wife and children. Although Macduff is no longer in the castle, everyone in Macduff's castle is put to death, including Lady Macduff and their young son.

Meanwhile, Lady Macbeth becomes racked with guilt from the crimes she and her husband have committed. At night, in the king's palace at Dunsinane, a doctor and a gentlewoman discuss Lady Macbeth's strange habit of sleepwalking. Suddenly, Lady Macbeth enters in a trance with a candle in her hand. Bemoaning the murders of Duncan, Lady Macduff, and Banquo, she tries to wash off imaginary bloodstains from her hands, all the while speaking of the terrible things she knows she pressed her husband to do. She leaves, and the doctor and gentlewoman marvel at her descent into madness. Her belief that nothing can wash away the blood on her hands is an ironic reversal of her earlier claim to Macbeth that "[a] little water clears us of this deed" (II.ii.66).

In England, Macduff is informed by Ross that his "castle is surprised; wife and babes / Savagely slaughter'd" (IV.iii.204–05). When this news of his family's execution reaches him, Macduff is stricken with grief and vows revenge. Prince Malcolm, Duncan's son, has succeeded in raising an army in England, and Macduff joins him as he rides to Scotland to challenge Macbeth's forces. The invasion has the support of the Scottish nobles, who are appalled and frightened by Macbeth's tyrannical and murderous behaviour. Malcolm leads an army, along with Macduff and Englishmen Siward (the Elder), the Earl of Northumberland, against Dunsinane Castle. While encamped in Birnam Wood, the soldiers are ordered to cut down and carry tree limbs to camouflage their numbers.

Before Macbeth's opponents arrive, he receives news that Lady Macbeth has killed herself, causing him to sink into a deep and pessimistic despair and deliver his "Tomorrow, and tomorrow, and tomorrow" soliloquy (V.v.17–28). Though he reflects on the brevity and meaninglessness of life, he nevertheless awaits the English and fortifies Dunsinane. He is certain that the witches' prophecies guarantee his invincibility, but is struck with fear when he learns that the English army is advancing on Dunsinane shielded with boughs cut from Birnam Wood, in apparent fulfillment of one of the prophecies.

A battle culminates in Macduff's confrontation with Macbeth, who kills Young Siward in combat. The English forces overwhelm his army and castle. Macbeth boasts that he has no reason to fear Macduff, for he cannot be killed by any man born of woman. Macduff declares that he was "from his mother's womb / Untimely ripp'd" (V.8.15–16), (i.e., born by Caesarean section) and is not "of woman born" (an example of a literary quibble), fulfilling the second prophecy. Macbeth realises too late that he has misinterpreted the witches' words. Though he realises that he is doomed, he continues to fight. Macduff kills and beheads him, thus fulfilling the remaining prophecy.

Macduff carries Macbeth's head onstage and Malcolm discusses how order has been restored. His last reference to Lady Macbeth, however, reveals "'tis thought, by self and violent hands / Took off her life" (V.ix.71–72), but the method of her suicide is undisclosed. Malcolm, now the King of Scotland, declares his benevolent intentions for the country and invites all to see him crowned at Scone.

Although Malcolm, and not Fleance, is placed on the throne, the witches' prophecy concerning Banquo ("Thou shalt get kings") was known to the audience of Shakespeare's time to be true: James VI of Scotland (later also James I of England) was supposedly a descendant of Banquo.

A principal source comes from the "Daemonologie" of King James published in 1597 which included a news pamphlet titled "Newes from Scotland" that detailed the famous North Berwick Witch Trials of 1590. The publication of "Daemonologie" came just a few years before the tragedy of "Macbeth" with the themes and setting in a direct and comparative contrast with King James' personal experiences with witchcraft. Not only had this trial taken place in Scotland, the witches involved were recorded to have also conducted rituals with the same mannerisms as the three witches. One of the evidenced passages is referenced when the witches involved in the trial confessed to attempt the use of witchcraft to raise a tempest and sabotage the very boat King James and his queen were on board during their return trip from Denmark. This was significant as one ship sailing with King James' fleet actually sank in the storm. The following quote from "Macbeth" is one such reference:

"purposely to be cassin into the sea to raise winds for destruction of ships."

"Macbeth" has been compared to Shakespeare's "Antony and Cleopatra." Both Antony and Macbeth as characters seek a new world, even at the cost of the old one. Both are fighting for a throne and have a 'nemesis' to face to achieve that throne. For Antony, the nemesis is Octavius; for Macbeth, it is Banquo. At one point Macbeth even compares himself to Antony, saying "under Banquo / My Genius is rebuk'd, as it is said / Mark Antony's was by Caesar." Lastly, both plays contain powerful and manipulative female figures: Cleopatra and Lady Macbeth.

Shakespeare borrowed the story from several tales in "Holinshed's Chronicles", a popular history of the British Isles well known to Shakespeare and his contemporaries. In "Chronicles", a man named Donwald finds several of his family put to death by his king, King Duff, for dealing with witches. After being pressured by his wife, he and four of his servants kill the King in his own house. In "Chronicles", Macbeth is portrayed as struggling to support the kingdom in the face of King Duncan's ineptitude. He and Banquo meet the three witches, who make exactly the same prophecies as in Shakespeare's version. Macbeth and Banquo then together plot the murder of Duncan, at Lady Macbeth's urging. Macbeth has a long, ten-year reign before eventually being overthrown by Macduff and Malcolm. The parallels between the two versions are clear. However, some scholars think that George Buchanan's "Rerum Scoticarum Historia" matches Shakespeare's version more closely. Buchanan's work was available in Latin in Shakespeare's day.

No medieval account of the reign of Macbeth mentions the Weird Sisters, Banquo or Lady Macbeth, and with the exception of the latter none actually existed. The characters of Banquo, the Weird Sisters and Lady Macbeth were first mentioned in 1527 by a Scottish historian Hector Boece in his book "Historia Gentis Scotorum" ("History of the Scottish People") who wanted to denigrate Macbeth in order to strengthen the claim of the House of Stewart to the Scottish throne. Boece portrayed Banquo as an ancestor of the Stewart kings of Scotland, adding in a "prophecy" that the descendants of Banquo would be the rightful kings of Scotland while the Weird Sisters served to give a picture of King Macbeth as gaining the throne via dark supernatural forces. Macbeth did have a wife, but it not clear if she was as power-hungry and ambitious as Boece portrayed her, which served his purpose of having even Macbeth realise he lacked a proper claim to the throne, and only took it at the urging of his wife. Holinshed accepted Boece's version of Macbeth's reign at face value and included it in his "Chronicles". Shakespeare saw the dramatic possibilities in the story as related by Holinshed, and used it as the basis for the play.

No other version of the story has Macbeth kill the king in Macbeth's own castle. Scholars have seen this change of Shakespeare's as adding to the darkness of Macbeth's crime as the worst violation of hospitality. Versions of the story that were common at the time had Duncan being killed in an ambush at Inverness, not in a castle. Shakespeare conflated the story of Donwald and King Duff in what was a significant change to the story.

Shakespeare made another important change. In "Chronicles", Banquo is an accomplice in Macbeth's murder of King Duncan, and plays an important part in ensuring that Macbeth, not Malcolm, takes the throne in the coup that follows. In Shakespeare's day, Banquo was thought to be an ancestor of the Stuart King James I. (In the 19th century it was established that Banquo is an unhistorical character, the Stuarts are actually descended from a Breton family which migrated to Scotland slightly later than Macbeth's time.) The Banquo portrayed in earlier sources is significantly different from the Banquo created by Shakespeare. Critics have proposed several reasons for this change. First, to portray the king's ancestor as a murderer would have been risky. Other authors of the time who wrote about Banquo, such as Jean de Schelandre in his "Stuartide", also changed history by portraying Banquo as a noble man, not a murderer, probably for the same reasons. Second, Shakespeare may have altered Banquo's character simply because there was no dramatic need for another accomplice to the murder; there was, however, a need to give a dramatic contrast to Macbeth—a role which many scholars argue is filled by Banquo.

Other scholars maintain that a strong argument can be made for associating the tragedy with the Gunpowder Plot of 1605. As presented by Harold Bloom in 2008: "[S]cholars cite the existence of several topical references in "Macbeth" to the events of that year, namely the execution of the Father Henry Garnett for his alleged complicity in the Gunpowder Plot of 1605, as referenced in the porter's scene." Those arrested for their role in the Gunpowder Plot refused to give direct answers to the questions posed to them by their interrogators, which reflected the influence of the Jesuit practice of equivocation. Shakespeare by having Macbeth say that demons "palter...in a double sense" and "keep the promise to our ear/And break it to our hope" confirmed James's belief that equivocation was a "wicked" practice, which reflected in turn the "wickedness" of the Catholic Church. Garnett had in his possession "A Treatise on Equivocation", and in the play the Weird Sisters often engage in equivocation, for instance telling Macbeth that he could never be overthrown until "Great Birnan wood to high Dunsinane hill/Shall Come". Macbeth interprets the prophecy as meaning never, but in fact, the Three Sisters refer only to branches of the trees of Great Birnan coming to Dunsinane hill.

"Macbeth" cannot be dated precisely but it is usually dated as contemporaneous to the other canonical tragedies ("King Lear", "Hamlet", and "Othello"). Some scholars have placed the original writing of the play as early as 1599. As the play is widely seen to celebrate King James' ancestors and the Stuart accession to the throne in 1603 (James believed himself to be descended from Banquo), most scholars believe that the play is unlikely to have been composed earlier than 1603 and suggest that the parade of eight kings—which the witches show Macbeth in a vision in Act IV—is a compliment to King James. Many scholars think the play was written in 1606 in the aftermath of the Gunpowder Plot because of possible internal allusions to the 1605 plot and its ensuing trials. In fact, there are a great number of allusions and possible pieces of evidence alluding to the Plot, and, for this reason, a great many critics agree that "Macbeth" was written in the year 1606. Lady Macbeth's instructions to her husband, "Look like the innocent flower, but be the serpent under't" (1.5.74–75), may be an allusion to a medal that was struck in 1605 to commemorate King James' escape that depicted a serpent hiding among lilies and roses.

Particularly, the Porter's speech (2.3.1–21) in which he welcomes an "equivocator", a farmer, and a tailor to hell (2.3.8–13), has been argued to be an allusion to the 28 March 1606 trial and execution on 3 May 1606 of the Jesuit Henry Garnet, who used the alias "Farmer", with "equivocator" referring to Garnet's defence of "equivocation". The porter says that the equivocator "committed treason enough for God's sake" (2.3.9–10), which specifically connects equivocation and treason and ties it to the Jesuit belief that equivocation was only lawful when used "for God's sake", strengthening the allusion to Garnet. The porter goes on to say that the equivocator "yet could not equivocate to heaven" (2.3.10–11), echoing grim jokes that were current on the eve of Garnet's execution: i.e. that Garnet would be "hanged without equivocation" and at his execution he was asked "not to equivocate with his last breath." The "English tailor" the porter admits to hell (2.3.13), has been seen as an allusion to Hugh Griffin, a tailor who was questioned by the Archbishop of Canterbury on 27 November and 3 December 1607 for the part he played in Garnet's "miraculous straw", an infamous head of straw that was stained with Garnet's blood that had congealed into a form resembling Garnet's portrait, which was hailed by Catholics as a miracle. The tailor Griffin became notorious and the subject of verses published with his portrait on the title page.

When James became king of England, a feeling of uncertainty settled over the nation. James was a Scottish king and the son of Mary, Queen of Scots, a staunch Catholic and English traitor. In the words of critic Robert Crawford, ""Macbeth" was a play for a post-Elizabethan England facing up to what it might mean to have a Scottish king. England seems comparatively benign, while its northern neighbour is mired in a bloody, monarch-killing past. ... "Macbeth" may have been set in medieval Scotland, but it was filled with material of interest to England and England's ruler." Critics argue that the content of the play is clearly a message to James, the new Scottish King of England. Likewise, the critic Andrew Hadfield noted the contrast the play draws between the saintly King Edward the Confessor of England who has the power of the royal touch to cure scrofula and whose realm is portrayed as peaceful and prosperous vs. the bloody chaos of Scotland. James in his 1598 book "The Trew Law of Free Monarchies" had asserted that kings are always right, if not just, and his subjects own him total loyalty at all times, writing that even if a king is a tyrant, his subjects must never rebel and just endure his tyranny for their own good. James had argued that the tyranny was preferable to the problems caused by rebellion which were even worse; Shakespeare by contrast in "Macbeth" argued for the right of the subjects to overthrow a tyrant king, in what appeared to be an implied criticism of James's theories if applied to England. Hadfield also noted a curious aspect of the play in that it implies that primogeniture is the norm in Scotland, but Duncan has to nominate his son Malcolm to be his successor while Macbeth is accepted without protest by the Scottish lairds as their king despite being an usurper. Hadfield argued this aspect of the play with the thanes apparently choosing their king was a reference to the Stuart claim to the English throne, and the attempts of the English parliament to block the succession of James's Catholic mother, Mary, Queen of Scots, from succeeding to the English throne. Hadfield argued that Shakespeare that implying that James was indeed the rightful king of England, but he owned his throne not to divine favor as James would have it, but rather due to the willingness of the English Parliament to accept the Protestant son of the Catholic Mary, Queen of Scots, as their king.

Garry Wills provides further evidence that "Macbeth" is a Gunpowder Play (a type of play that emerged immediately following the events of the Gunpowder Plot). He points out that every Gunpowder Play contains "a necromancy scene, regicide attempted or completed, references to equivocation, scenes that test loyalty by use of deceptive language, and a character who sees through plots—along with a vocabulary similar to the Plot in its immediate aftermath (words like "train, blow, vault") and an ironic recoil of the Plot upon the Plotters (who fall into the pit they dug)."

The play utilizes a few key words that the audience at the time would recognize as allusions to the Plot. In one sermon in 1605, Lancelot Andrewes stated, regarding the failure of the Plotters on God's day, "Be they fair or foul, glad or sad (as the poet calleth Him) the great Diespiter, 'the Father of days' hath made them both." Shakespeare begins the play by using the words "fair" and "foul" in the first speeches of the witches and Macbeth. In the words of Jonathan Gil Harris, the play expresses the "horror unleashed by a supposedly loyal subject who seeks to kill a king and the treasonous role of equivocation. The play even echoes certain keywords from the scandal – the 'vault' beneath the House of Parliament in which Guy Fawkes stored thirty kegs of gunpowder and the 'blow' about which one of the conspirators had secretly warned a relative who planned to attend the House of Parliament on 5 November...Even though the Plot is never alluded to directly, its presence is everywhere in the play, like a pervasive odor."
Scholars also cite an entertainment seen by King James at Oxford in the summer of 1605 that featured three "sibyls" like the weird sisters; Kermode surmises that Shakespeare could have heard about this and alluded to it with the weird sisters. However, A. R. Braunmuller in the New Cambridge edition finds the 1605–06 arguments inconclusive, and argues only for an earliest date of 1603.

One suggested allusion supporting a date in late 1606 is the first witch's dialogue about a sailor's wife: "'Aroint thee, witch!' the rump-fed ronyon cries./Her husband's to Aleppo gone, master o' the "Tiger"" (1.3.6–7). This has been thought to allude to the "Tiger", a ship that returned to England 27 June 1606 after a disastrous voyage in which many of the crew were killed by pirates. A few lines later the witch speaks of the sailor, "He shall live a man forbid:/Weary se'nnights nine times nine" (1.3.21–22). The real ship was at sea 567 days, the product of 7x9x9, which has been taken as a confirmation of the allusion, which if correct, confirms that the witch scenes were either written or amended later than July 1606.

The play is not considered to have been written any later than 1607, since, as Kermode notes, there are "fairly clear allusions to the play in 1607." One notable reference is in Francis Beaumont's "Knight of the Burning Pestle", first performed in 1607. The following lines (Act V, Scene 1, 24–30) are, according to scholars, a clear allusion to the scene in which Banquo's ghost haunts Macbeth at the dinner table:

<poem>
When thou art at thy table with thy friends,
Merry in heart, and filled with swelling wine,
I'll come in midst of all thy pride and mirth,
Invisible to all men but thyself,
And whisper such a sad tale in thine ear
Shall make thee let the cup fall from thy hand,
And stand as mute and pale as death itself.</poem>

"Macbeth" was first printed in the First Folio of 1623 and the Folio is the only source for the text. Some scholars contend that the Folio text was abridged and rearranged from an earlier manuscript or prompt book. Often cited as interpolation are stage cues for two songs, whose lyrics are not included in the Folio but are included in Thomas Middleton's play "The Witch", which was written between the accepted date for "Macbeth" (1606) and the printing of the Folio. Many scholars believe these songs were editorially inserted into the Folio, though whether they were Middleton's songs or preexisting songs is not certain. It is also widely believed that the character of Hecate, as well as some lines of the First Witch (4.1 124–31), were not part of Shakespeare's original play but were added by the Folio editors and possibly written by Middleton, though "there is no completely objective proof" of such interpolation.

The 'reconstructive movement' was concerned with the recreation of Elizabethan acting conditions, and would eventually lead to the creation of Shakespeare's Globe and similar replicas. One of the movement's offshoots was in the reconstruction of Elizabethan pronunciation: for example Bernard Miles' 1951 "Macbeth", for which linguists from University College London were employed to create a transcript of the play in Elizabethan English, then an audio recording of that transcription, from which the actors, in turn, learned their lines.

The pronunciation of many words evolves over time. In Shakespeare's day, for example, "heath" was pronounced as "heth" ("or a slightly elongated 'e' as in the modern 'get'"), so it rhymed with "Macbeth" in the sentences by the Witches at the beginning of the play:

Second Witch: Upon the heath.Third Witch: There to meet with Macbeth.

A scholar of antique pronunciation writes, ""Heath" would have made a close (if not exact) rhyme with the "-eth" of "Macbeth", which was pronounced with a short 'i' as in 'it'."

In the theatre programme notes, "much was made of how OP [Original Pronunciation] performance reintroduces lost rhymes such as the final couplet: 'So thanks to all at once, and each to one, / Whom we invite to see us crowned at Scone'" (5.11.40–41) where 'one' sounds like 'own'. The Witches, the play's great purveyors of rhyme, benefited most in this regard. So, 'babe' (4.1.30) sounded like 'bab' and rhymed with 'drab' (4.1.31)..."

Eoin Price wrote, "I found the OP rendition of Banquo's brilliant question 'Or have we eaten on the insane root / That takes the raison prisoner?' unduly amusing"; and he adds,

"Macbeth" is an anomaly among Shakespeare's tragedies in certain critical ways. It is short: more than a thousand lines shorter than "Othello" and "King Lear", and only slightly more than half as long as "Hamlet". This brevity has suggested to many critics that the received version is based on a heavily cut source, perhaps a prompt-book for a particular performance. This would reflect other Shakespearean plays existing in both Quarto and the Folio, where the Quarto versions are usually longer than the Folio versions. "Macbeth" was first printed in the First Folio, but has no Quarto version – if there were a Quarto, it would probably be longer than the Folio version.That brevity has also been connected to other unusual features: the fast pace of the first act, which has seemed to be "stripped for action"; the comparative flatness of the characters other than Macbeth; and the oddness of Macbeth himself compared with other Shakespearean tragic heroes. A. C. Bradley, in considering this question, concluded the play "always was an extremely short one", noting the witch scenes and battle scenes would have taken up some time in performance, remarking, "I do not think that, in reading, we "feel" Macbeth to be short: certainly we are astonished when we hear it is about half as long as "Hamlet". Perhaps in the Shakespearean theatre too it seemed to occupy a longer time than the clock recorded."

At least since the days of Alexander Pope and Samuel Johnson, analysis of the play has centred on the question of Macbeth's ambition, commonly seen as so dominant a trait that it defines the character. Johnson asserted that Macbeth, though esteemed for his military bravery, is wholly reviled. This opinion recurs in critical literature, and, according to Caroline Spurgeon, is supported by Shakespeare himself, who apparently intended to degrade his hero by vesting him with clothes unsuited to him and to make Macbeth look ridiculous by several nimisms he applies: His garments seem either too big or too small for him – as his ambition is too big and his character too small for his new and unrightful role as king. When he feels as if "dressed in borrowed robes", after his new title as Thane of Cawdor, prophesied by the witches, has been confirmed by Ross (I, 3, ll. 108–09), Banquo comments: "New honours come upon him, / Like our strange garments, cleave not to their mould, / But with the aid of use" (I, 3, ll. 145–46). And, at the end, when the tyrant is at bay at Dunsinane, Caithness sees him as a man trying in vain to fasten a large garment on him with too small a belt: "He cannot buckle his distemper'd cause / Within the belt of rule" (V, 2, ll. 14–15), while Angus, in a similar nimism, sums up what everybody thinks ever since Macbeth's accession to power: "now does he feel his title / Hang loose about him, like a giant's robe / upon a dwarfish thief" (V, 2, ll. 18–20).

Like Richard III, but without that character's perversely appealing exuberance, Macbeth wades through blood until his inevitable fall. As Kenneth Muir writes, "Macbeth has not a predisposition to murder; he has merely an inordinate ambition that makes murder itself seem to be a lesser evil than failure to achieve the crown." Some critics, such as E. E. Stoll, explain this characterisation as a holdover from Senecan or medieval tradition. Shakespeare's audience, in this view, expected villains to be wholly bad, and Senecan style, far from prohibiting a villainous protagonist, all but demanded it.

Yet for other critics, it has not been so easy to resolve the question of Macbeth's motivation. Robert Bridges, for instance, perceived a paradox: a character able to express such convincing horror before Duncan's murder would likely be incapable of committing the crime. For many critics, Macbeth's motivations in the first act appear vague and insufficient. John Dover Wilson hypothesised that Shakespeare's original text had an extra scene or scenes where husband and wife discussed their plans. This interpretation is not fully provable; however, the motivating role of ambition for Macbeth is universally recognised. The evil actions motivated by his ambition seem to trap him in a cycle of increasing evil, as Macbeth himself recognises: "I am in blood/Stepp'd in so far that, should I wade no more,/Returning were as tedious as go o'er."

While working on Russian translations of Shakespeare's works, Boris Pasternak compared Macbeth to Raskolnikov, the protagonist of "Crime and Punishment" by Fyodor Dostoevsky. Pasternak argues that "neither Macbeth or Raskolnikov is a born criminal or a villain by nature. They are turned into criminals by faulty rationalizations, by deductions from false premises." He goes on to argue that Lady Macbeth is "feminine ... one of those active, insistent wives" who becomes her husband's "executive, more resolute and consistent than he is himself." According to Pasternak, she is only helping Macbeth carry out his own wishes, to her own detriment.

The disastrous consequences of Macbeth's ambition are not limited to him. Almost from the moment of the murder, the play depicts Scotland as a land shaken by inversions of the natural order. Shakespeare may have intended a reference to the great chain of being, although the play's images of disorder are mostly not specific enough to support detailed intellectual readings. He may also have intended an elaborate compliment to James's belief in the divine right of kings, although this hypothesis, outlined at greatest length by Henry N. Paul, is not universally accepted. As in "Julius Caesar", though, perturbations in the political sphere are echoed and even amplified by events in the material world. Among the most often depicted of the inversions of the natural order is sleep. Macbeth's announcement that he has "murdered sleep" is figuratively mirrored in Lady Macbeth's sleepwalking.

"Macbeth"'s generally accepted indebtedness to medieval tragedy is often seen as significant in the play's treatment of moral order. Glynne Wickham connects the play, through the Porter, to a mystery play on the harrowing of hell. Howard Felperin argues that the play has a more complex attitude toward "orthodox Christian tragedy" than is often admitted; he sees a kinship between the play and the tyrant plays within the medieval liturgical drama.

The theme of androgyny is often seen as a special aspect of the theme of disorder. Inversion of normative gender roles is most famously associated with the witches and with Lady Macbeth as she appears in the first act. Whatever Shakespeare's degree of sympathy with such inversions, the play ends with a thorough return to normative gender values. Some feminist psychoanalytic critics, such as Janet Adelman, have connected the play's treatment of gender roles to its larger theme of inverted natural order. In this light, Macbeth is punished for his violation of the moral order by being removed from the cycles of nature (which are figured as female); nature itself (as embodied in the movement of Birnam Wood) is part of the restoration of moral order.

Critics in the early twentieth century reacted against what they saw as an excessive dependence on the study of character in criticism of the play. This dependence, though most closely associated with Andrew Cecil Bradley, is clear as early as the time of Mary Cowden Clarke, who offered precise, if fanciful, accounts of the predramatic lives of Shakespeare's female leads. She suggested, for instance, that the child Lady Macbeth refers to in the first act died during a foolish military action.

In the play, the Three Witches represent darkness, chaos, and conflict, while their role is as agents and witnesses. Their presence communicates treason and impending doom. During Shakespeare's day, witches were seen as worse than rebels, "the most notorious traytor and rebell that can be." They were not only political traitors, but spiritual traitors as well. Much of the confusion that springs from them comes from their ability to straddle the play's borders between reality and the supernatural. They are so deeply entrenched in both worlds that it is unclear whether they control fate, or whether they are merely its agents. They defy logic, not being subject to the rules of the real world. The witches' lines in the first act: "Fair is foul, and foul is fair: Hover through the fog and filthy air" are often said to set the tone for the rest of the play by establishing a sense of confusion. Indeed, the play is filled with situations where evil is depicted as good, while good is rendered evil. The line "Double, double toil and trouble," communicates the witches' intent clearly: they seek only trouble for the mortals around them. The witches' spells are remarkably similar to the spells of the witch Medusa in Anthony Munday's play "Fidele and Fortunio" published in 1584, and Shakespeare may have been influenced by these.

While the witches do not tell Macbeth directly to kill King Duncan, they use a subtle form of temptation when they tell Macbeth that he is destined to be king. By placing this thought in his mind, they effectively guide him on the path to his own destruction. This follows the pattern of temptation used at the time of Shakespeare. First, they argued, a thought is put in a man's mind, then the person may either indulge in the thought or reject it. Macbeth indulges in it, while Banquo rejects.

According to J. A. Bryant Jr., Macbeth also makes use of Biblical parallels, notably between King Duncan's murder and the murder of Christ:

While many today would say that any misfortune surrounding a production is mere coincidence, actors and others in the theatre industry often consider it bad luck to mention "Macbeth" by name while inside a theatre, and sometimes refer to it indirectly, for example as "The Scottish Play", or "MacBee", or when referring to the character and not the play, "Mr. and Mrs. M", or "The Scottish King".

This is because Shakespeare (or the play's revisers) are said to have used the spells of real witches in his text, purportedly angering the witches and causing them to curse the play. Thus, to say the name of the play inside a theatre is believed to doom the production to failure, and perhaps cause physical injury or death to cast members. There are stories of accidents, misfortunes and even deaths taking place during runs of "Macbeth".

According to the actor Sir Donald Sinden, in his Sky Arts TV series "Great West End Theatres", contrary to popular myth, Shakespeare's tragedy Macbeth is not the unluckiest play as superstition likes to portray it. Exactly the opposite! The origin of the unfortunate moniker dates back to repertory theatre days when each town and village had at least one theatre to entertain the public. If a play was not doing well, it would invariably get 'pulled' and replaced with a sure-fire audience pleaser – Macbeth guaranteed full-houses. So when the weekly theatre newspaper, "The Stage" was published, listing what was on in each theatre in the country, it was instantly noticed what shows had "not" worked the previous week, as they had been replaced by a definite crowd-pleaser. More actors have died during performances of Hamlet than in the "Scottish play" as the profession still calls it. It is forbidden to quote from it backstage as this could cause the current play to collapse and have to be replaced, causing possible unemployment.

Several methods exist to dispel the curse, depending on the actor. One, attributed to Michael York, is to immediately leave the building the stage is in with the person who uttered the name, walk around it three times, spit over their left shoulders, say an obscenity then wait to be invited back into the building. A related practice is to spin around three times as fast as possible on the spot, sometimes accompanied by spitting over their shoulder, and uttering an obscenity. Another popular "ritual" is to leave the room, knock three times, be invited in, and then quote a line from "Hamlet". Yet another is to recite lines from "The Merchant of Venice", thought to be a lucky play.

The only eyewitness account of "Macbeth" in Shakespeare's lifetime was recorded by Simon Forman, who saw a performance at the Globe on 20 April 1610. Scholars have noted discrepancies between Forman's account and the play as it appears in the Folio. For example, he makes no mention of the apparition scene, or of Hecate, of the man not of woman born, or of Birnam Wood. However, Clark observes that Forman's accounts were often inaccurate and incomplete (for instance omitting the statue scene from "The Winter's Tale") and his interest did not seem to be in "giving full accounts of the productions."

As mentioned above, the Folio text is thought by some to be an alteration of the original play. This has led to the theory that the play as we know it from the Folio was an adaptation for indoor performance at the Blackfriars Theatre (which was operated by the King's Men from 1608) – and even speculation that it represents a specific performance before King James. The play contains more musical cues than any other play in the canon as well as a significant use of sound effects.

All theatres were closed down by the Puritan government on 6 September 1642. Upon the restoration of the monarchy in 1660, two patent companies (the King's Company and the Duke's Company) were established, and the existing theatrical repertoire divided between them. Sir William Davenant, founder of the Duke's Company, adapted Shakespeare's play to the tastes of the new era, and his version would dominate on stage for around eighty years. Among the changes he made were the expansion of the role of the witches, introducing new songs, dances and 'flying', and the expansion of the role of Lady Macduff as a foil to Lady Macbeth. There were, however, performances outside the patent companies: among the evasions of the Duke's Company's monopoly was a puppet version of "Macbeth".

"Macbeth" was a favourite of the seventeenth-century diarist Samuel Pepys, who saw the play on 5 November 1664 ("admirably acted"), 28 December 1666 ("most excellently acted"), ten days later on 7 January 1667 ("though I saw it lately, yet [it] appears a most excellent play in all respects"), on 19 April 1667 ("one of the best plays for a stage ... that ever I saw"), again on 16 October 1667 ("was vexed to see Young, who is but a bad actor at best, act Macbeth in the room of Betterton, who, poor man! is sick"), and again three weeks later on 6 November 1667 ("[at] "Macbeth", which we still like mightily"), yet again on 12 August 1668 ("saw "Macbeth", to our great content"), and finally on 21 December 1668, on which date the king and court were also present in the audience.

The first professional performances of "Macbeth" in North America were probably those of The Hallam Company.

In 1744, David Garrick revived the play, abandoning Davenant's version and instead advertising it "as written by Shakespeare". In fact this claim was largely false: he retained much of Davenant's more popular business for the witches, and himself wrote a lengthy death speech for Macbeth. And he cut more than 10% of Shakespeare's play, including the drunken porter, the murder of Lady Macduff's son, and Malcolm's testing of Macduff. Hannah Pritchard was his greatest stage partner, having her premiere as his Lady Macbeth in 1747. He would later drop the play from his repertoire upon her retirement from the stage. Mrs. Pritchard was the first actress to achieve acclaim in the role of Lady Macbeth – at least partly due to the removal of Davenant's material, which made irrelevant moral contrasts with Lady Macduff. Garrick's portrayal focused on the inner life of the character, endowing him with an innocence vacillating between good and evil, and betrayed by outside influences. He portrayed a man capable of observing himself, as if a part of him remained untouched by what he had done, the play moulding him into a man of sensibility, rather than him descending into a tyrant.

John Philip Kemble first played Macbeth in 1778. Although usually regarded as the antithesis of Garrick, Kemble nevertheless refined aspects of Garrick's portrayal into his own. However it was the "towering and majestic" Sarah Siddons (Kemble's sister) who became a legend in the role of Lady Macbeth. In contrast to Hannah Pritchard's savage, demonic portrayal, Siddons' Lady Macbeth, while terrifying, was nevertheless – in the scenes in which she expresses her regret and remorse – tenderly human. And in portraying her actions as done out of love for her husband, Siddons deflected from him some of the moral responsibility for the play's carnage. Audiences seem to have found the sleepwalking scene particularly mesmerising: Hazlitt said of it that "all her gestures were involuntary and mechanical ... She glided on and off the stage almost like an apparition."

In 1794, Kemble dispensed with the ghost of Banquo altogether, allowing the audience to see Macbeth's reaction as his wife and guests see it, and relying upon the fact that the play was so well known that his audience would already be aware that a ghost enters at that point.

Ferdinand Fleck, notable as the first German actor to present Shakespeare's tragic roles in their fullness, played Macbeth at the Berlin National Theatre from 1787. Unlike his English counterparts, he portrayed the character as achieving his stature after the murder of Duncan, growing in presence and confidence: thereby enabling stark contrasts, such as in the banquet scene, which he ended babbling like a child.

Performances outside the patent theatres were instrumental in bringing the monopoly to an end. Robert Elliston, for example, produced a popular adaptation of "Macbeth" in 1809 at the Royal Circus described in its publicity as "this matchless piece of pantomimic and choral performance", which circumvented the illegality of speaking Shakespeare's words through mimed action, singing, and doggerel verse written by J. C. Cross.

In 1809, in an unsuccessful attempt to take Covent Garden upmarket, Kemble installed private boxes, increasing admission prices to pay for the improvements. The inaugural run at the newly renovated theatre was "Macbeth", which was disrupted for over two months with cries of "Old prices!" and "No private boxes!" until Kemble capitulated to the protestors' demands.

Edmund Kean at Drury Lane gave a psychological portrayal of the central character, with a common touch, but was ultimately unsuccessful in the role. However he did pave the way for the most acclaimed performance of the nineteenth century, that of William Charles Macready. Macready played the role over a 30-year period, firstly at Covent Garden in 1820 and finally in his retirement performance. Although his playing evolved over the years, it was noted throughout for the tension between the idealistic aspects and the weaker, venal aspects of Macbeth's character. His staging was full of spectacle, including several elaborate royal processions.
In 1843 the Theatres Regulation Act finally brought the patent companies' monopoly to an end. From that time until the end of the Victorian era, London theatre was dominated by the actor-managers, and the style of presentation was "pictorial" – proscenium stages filled with spectacular stage-pictures, often featuring complex scenery, large casts in elaborate costumes, and frequent use of tableaux vivant. Charles Kean (son of Edmund), at London's Princess's Theatre from 1850 to 1859, took an antiquarian view of Shakespeare performance, setting his "Macbeth" in a historically accurate eleventh-century Scotland. His leading lady, Ellen Tree, created a sense of the character's inner life: "The Times" critic saying "The countenance which she assumed ... when luring on Macbeth in his course of crime, was actually appalling in intensity, as if it denoted a hunger after guilt." At the same time, special effects were becoming popular: for example in Samuel Phelps' "Macbeth" the witches performed behind green gauze, enabling them to appear and disappear using stage lighting.

In 1849, rival performances of the play sparked the Astor Place riot in Manhattan. The popular American actor Edwin Forrest, whose Macbeth was said to be like "the ferocious chief of a barbarous tribe" played the central role at the Broadway Theatre to popular acclaim, while the "cerebral and patrician" English actor Macready, playing the same role at the Astor Place Opera House, suffered constant heckling. The existing enmity between the two men (Forrest had openly hissed Macready at a recent performance of "Hamlet" in Britain) was taken up by Forrest's supporters – formed from the working class and lower middle class and anti-British agitators, keen to attack the upper-class pro-British patrons of the Opera House and the colonially-minded Macready. Nevertheless, Macready performed the role again three days later to a packed house while an angry mob gathered outside. The militia tasked with controlling the situation fired into the mob. In total, 31 rioters were killed and over 100 injured.
Charlotte Cushman is unique among nineteenth century interpreters of Shakespeare in achieving stardom in roles of both genders. Her New York debut was as Lady Macbeth in 1836, and she would later be admired in London in the same role in the mid-1840s. Helen Faucit was considered the embodiment of early-Victorian notions of femininity. But for this reason she largely failed when she eventually played Lady Macbeth in 1864: her serious attempt to embody the coarser aspects of Lady Macbeth's character jarred harshly with her public image. Adelaide Ristori, the great Italian actress, brought her Lady Macbeth to London in 1863 in Italian, and again in 1873 in an English translation cut in such a way as to be, in effect, Lady Macbeth's tragedy.

Henry Irving was the most successful of the late-Victorian actor-managers, but his "Macbeth" failed to curry favour with audiences. His desire for psychological credibility reduced certain aspects of the role: He described Macbeth as a brave soldier but a moral coward, and played him untroubled by conscience – clearly already contemplating the murder of Duncan before his encounter with the witches. Irving's leading lady was Ellen Terry, but her Lady Macbeth was unsuccessful with the public, for whom a century of performances influenced by Sarah Siddons had created expectations at odds with Terry's conception of the role.

Late nineteenth-century European Macbeths aimed for heroic stature, but at the expense of subtlety: Tommaso Salvini in Italy and Adalbert Matkowsky in Germany were said to inspire awe, but elicited little pity.

Two developments changed the nature of "Macbeth" performance in the 20th century: first, developments in the craft of acting itself, especially the ideas of Stanislavski and Brecht; and second, the rise of the dictator as a political icon. The latter has not always assisted the performance: it is difficult to sympathise with a Macbeth based on Hitler, Stalin, or Idi Amin.

Barry Jackson, at the Birmingham Repertory Theatre in 1923, was the first of the 20th-century directors to costume "Macbeth" in modern dress.

In 1936, a decade before his film adaptation of the play, Orson Welles directed "Macbeth" for the Negro Theatre Unit of the Federal Theatre Project at the Lafayette Theatre in Harlem, using black actors and setting the action in Haiti: with drums and Voodoo rituals to establish the Witches scenes. The production, dubbed "The Voodoo Macbeth", proved inflammatory in the aftermath of the Harlem riots, accused of making fun of black culture and as "a campaign to burlesque negroes" until Welles persuaded crowds that his use of black actors and voodoo made important cultural statements.
A performance which is frequently referenced as an example of the play's curse was the outdoor production directed by Burgess Meredith in 1953 in the British colony of Bermuda, starring Charlton Heston. Using the imposing spectacle of Fort St. Catherine as a key element of the set, the production was plagued by a host of mishaps, including Charlton Heston being burned when his tights caught fire.

The critical consensus is that there have been three great Macbeths on the English-speaking stage in the 20th century, all of them commencing at Stratford-upon-Avon: Laurence Olivier in 1955, Ian McKellen in 1976 and Antony Sher in 1999. Olivier's portrayal (directed by Glen Byam Shaw, with Vivien Leigh as Lady Macbeth) was immediately hailed as a masterpiece. Kenneth Tynan expressed the view that it succeeded because Olivier built the role to a climax at the end of the play, whereas most actors spend all they have in the first two acts.

The play caused grave difficulties for the Royal Shakespeare Company, especially at the (then) Shakespeare Memorial Theatre. Peter Hall's 1967 production was (in Michael Billington's words) "an acknowledged disaster" with the use of real leaves from Birnham Wood getting unsolicited first-night laughs, and Trevor Nunn's 1974 production was (Billington again) "an over-elaborate religious spectacle". But Nunn achieved success for the RSC in his 1976 production at the intimate Other Place, with Ian McKellen and Judi Dench in the central roles. A small cast worked within a simple circle, and McKellen's Macbeth had nothing noble or likeable about him, being a manipulator in a world of manipulative characters. They were a young couple, physically passionate, "not monsters but recognisable human beings", but their relationship atrophied as the action progressed.

In Soviet-controlled Prague in 1977, faced with the illegality of working in theatres, Pavel Kohout adapted "Macbeth" into a 75-minute abridgement for five actors, suitable for "bringing a show in a suitcase to people's homes".

Spectacle was unfashionable in Western theatre throughout the 20th century. In East Asia, however, spectacular productions have achieved great success, including Yukio Ninagawa's 1980 production with Masane Tsukayama as Macbeth, set in the 16th century Japanese Civil War. The same director's tour of London in 1987 was widely praised by critics, even though (like most of their audience) they were unable to understand the significance of Macbeth's gestures, the huge Buddhist altar dominating the set, or the petals falling from the cherry trees. Xu Xiaozhong's 1980 Central Academy of Drama production in Beijing made every effort to be unpolitical (necessary in the aftermath of the Cultural Revolution): yet audiences still perceived correspondences between the central character (whom the director had actually modelled on Louis Napoleon) and Mao Zedong. Shakespeare has often been adapted to indigenous theatre traditions, for example the "Kunju Macbeth" of Huang Zuolin performed at the inaugural Chinese Shakespeare Festival of 1986. Similarly, B. V. Karanth's "Barnam Vana" of 1979 had adapted "Macbeth" to the Yakshagana tradition of Karnataka, India. In 1997, Lokendra Arambam created "Stage of Blood", merging a range of martial arts, dance and gymnastic styles from Manipur, performed in Imphal and in England. The stage was literally a raft on a lake.

The RSC again achieved critical success in Gregory Doran's 1999 production at The Swan, with Antony Sher and Harriet Walter in the central roles, once again demonstrating the suitability of the play for smaller venues. Doran's witches spoke their lines to a theatre in absolute darkness, and the opening visual image was the entrance of Macbeth and Banquo in the berets and fatigues of modern warfare, carried on the shoulders of triumphant troops. In contrast to Nunn, Doran presented a world in which king Duncan and his soldiers were ultimately benign and honest, heightening the deviance of Macbeth (who seems genuinely surprised by the witches' prophesies) and Lady Macbeth in plotting to kill the king. The play said little about politics, instead powerfully presenting its central characters' psychological collapse.

The play has been translated and performed in various languages in different parts of the world, and "Media Artists" was the first to stage its Punjabi adaptation in India. The adaptation by Balram and the play directed by Samuel John have been universally acknowledged as a milestone in Punjabi theatre. The unique attempt involved trained theatre experts and the actors taken from a rural background in Punjab. Punjabi folk music imbued the play with the native ethos as the Scottish setting of Shakespeare's play was transposed into a Punjabi milieu.


All references to "Macbeth", unless otherwise specified, are taken from the Arden Shakespeare, second series edition edited by Kenneth Muir. Under their referencing system, III.I.55 means act 3, scene 1, line 55. All references to other Shakespeare plays are to The Oxford Shakespeare "Complete Works of Shakespeare" edited by Stanley Wells and Gary Taylor.



</doc>
<doc id="18870" url="https://en.wikipedia.org/wiki?curid=18870" title="Minor Threat">
Minor Threat

Minor Threat was an American hardcore punk band, formed in 1980 in Washington, D.C. by vocalist Ian MacKaye and drummer Jeff Nelson. MacKaye and Nelson had played in several other bands together, and recruited bassist Brian Baker and guitarist Lyle Preslar to form Minor Threat. They added a fifth member, Steve Hansgen, in 1982. The band was relatively short-lived, having disbanded after only four years together, but had a strong influence on the punk scene, both stylistically and in establishing a "do it yourself" ethic for music distribution and concert promotion. Minor Threat's song "Straight Edge" became the eventual basis of the straight edge movement, which emphasized a lifestyle without alcohol, drugs, or promiscuous sex. AllMusic described Minor Threat's music as "iconic" and noted that their groundbreaking music "has held up better than [that of] most of their contemporaries."

Along with the fellow Washington, D.C. hardcore band Bad Brains and California band Black Flag, Minor Threat set the standard for many hardcore punk bands in the 1980s and 1990s. All of Minor Threat's recordings were released on Ian MacKaye and Jeff Nelson's own label, Dischord Records. The "Minor Threat" EP and their only full-length studio album "Out of Step" have received a number of accolades and are cited as landmarks of the hardcore punk genre.

Prior to forming Minor Threat in 1980, vocalist Ian MacKaye and drummer Jeff Nelson had played bass and drums respectively in the Teen Idles while attending Wilson High School. During their two-year career within the flourishing Washington D.C. hardcore punk scene, the Teen Idles had gained a following of around one hundred fans (a sizable amount at the time), and were seen as only second within the scene to the contemporary Bad Brains. MacKaye and Nelson were strong believers in the DIY mentality and an independent, underground music scene. After the breakup of the Teen Idles, they used the money earned through the band to create Dischord Records, an independent record label that would host the releases of the Teen Idles, Minor Threat, and numerous other D.C. punk bands.

Eager to start a new band after the Teen Idles, MacKaye and Nelson recruited guitarist Lyle Preslar and bassist Brian Baker. They played their first performance in December 1980 to fifty people in a basement, opening for Bad Brains, The Untouchables, Black Market Baby and S.O.A., all D.C. bands.

The band's first 7" EPs, "Minor Threat" and "In My Eyes", were released in 1981. The group became popular regionally and toured the east coast and Midwest.

"Straight Edge", a song from the band's first EP, helped to inspire the straight edge movement. The lyrics of the song call for abstinence from alcohol and other drugs, a novel ideology for rock musicians which initially found a small but dedicated following. Other prominent groups that subsequently advocated the straight edge stance include SS Decontrol and 7 Seconds.

"Out of Step", A Minor Threat song from their second EP, further demonstrates the said belief: "Don't smoke/Don't drink/Don't fuck/At least I can fucking think/I can't keep up/I'm out of step with the world." The "I" in the lyrics was usually only implied, mainly because it did not quite fit the rhythm of the song, like the updated version on the 1983 album "Out of Step", which is slower, allotting a bridge where MacKaye explains his philosophy of straight edge, explaining that straight edge "is not a set of rules; I'm not telling you what to do. All I'm saying is there are three things, that are like so important to the whole world that I don't happen to find much importance in, whether it's fucking, or whether it's playing golf, because of that, I feel... (chorus)". Some of the other members of Minor Threat, Jeff Nelson in particular, took exception to what they saw as MacKaye's imperious attitude on the song.

Minor Threat's song "Guilty of Being White" led to some accusations of racism, but MacKaye has strongly denied such intentions and said that some listeners misinterpreted his words. He claims that his experiences attending Wilson High School, whose student population was 70 percent black, inspired the song. There, many students bullied MacKaye and his friends. Thrash metal band Slayer later covered the song, with the last iteration of the lyric "Guilty of being white" changed to "Guilty of being right." In an interview, MacKaye stated that he was offended that some perceived racist overtones in the lyrics, saying, "To me, at the time and now, it seemed clear it's an anti-racist song. Of course, it didn't occur to me at the time I wrote it that anybody outside of my twenty or thirty friends who I was singing to would ever have to actually ponder the lyrics or even consider them."

In the time between the release of the band's second seven-inch EP and the "Out of Step" record, the band briefly split when guitarist Lyle Preslar moved to Illinois to attend college for a semester at Northwestern University, Preslar was a member of Big Black for a few tempestuous rehearsals. During that period, MacKaye and Nelson put together a studio-only project called Skewbald/Grand Union; in a reflection of the slowly increasing disagreements between the two musicians, they were unable to decide on one name. The group recorded three untitled songs, which would be released posthumously as Dischord's 50th release. During Minor Threat's inactive period, Brian Baker also briefly played guitar for Government Issue and appeared on the "Make an Effort" EP.

In March 1982, at the urging of Bad Brains' H.R., Preslar left college to reform Minor Threat. The reunited band featured an expanded lineup: Steve Hansgen joined as the band's bassist and Baker switched to second guitar.

When "Out of Step" was rerecorded for the LP "Out of Step", MacKaye inserted a spoken section explaining, "This is not a set of rules..." An ideological door had already been opened, however, and by 1982, some straight-edge punks, such as followers of the band SS Decontrol, were swatting beers out of people's hands at clubs.

Minor Threat broke up in 1983. A contributing factor was disagreement over musical direction. MacKaye was allegedly skipping practice sessions towards the end of the band's career, and he wrote the lyrics to the songs on the "Salad Days" EP in the studio. That was quite a contrast with the earlier recordings, as he had written and co-written the music for much of the band's early material. Minor Threat, which had returned to being a four-piece group with the departure of Hansgen, played its final show on September 23, 1983, at the Lansburgh Cultural Center in Washington, D.C., sharing the bill with the go-go band Trouble Funk, and the Austin, Texas punk funk act the Big Boys. In a meaningful way, Minor Threat ended their final set with "Last Song", which was the original title of their song "Salad Days".

Following the breakup, MacKaye stated that he did not "check out" on hardcore, but in fact hardcore "checked out." Explaining this, he stated that at a 1984 Minutemen show, a fan struck MacKaye's younger brother Alec in the face, and he punched the fan back, then realizing that the violence was "stupid," and that he saw his role in the stupidity. MacKaye claimed that immediately after this he decided to leave the hardcore scene.

In March 1984, six months after the band broke up, the EPs "Minor Threat" and "In My Eyes" were compiled together and re-released as the "Minor Threat" album.

MacKaye went on to found Embrace with former members of the Faith, Egg Hunt with Jeff Nelson, and later Fugazi and the Evens, as well as collaborating on Pailhead.

Baker went on to play in Junkyard, the Meatmen, Dag Nasty and Government Issue. He currently plays in Bad Religion.

Preslar was briefly a member of Glenn Danzig's Samhain, and his playing appears on a few songs on the band's first record. He joined The Meatmen in 1984, along with fellow Minor Threat member Brian Baker. He later ran Caroline Records, signing and working with (among others) Peter Gabriel, Ben Folds, Chemical Brothers, and Idaho, and ran marketing for Sire Records. He graduated from Rutgers University School of Law and lives in New Jersey.

Nelson played less-frantic alternative rock with Three and The High-Back Chairs before retiring from live performance. He runs his own label, Adult Swim Records, distributed by Dischord, and is a graphic artist and a political activist in Toledo, Ohio. The band's own Dischord Records released material by many bands from the Washington, D.C., area, such as Government Issue, Void, Scream, Fugazi, Artificial Peace, Rites of Spring, Gray Matter, and Dag Nasty, and has become a respected independent record label.

Hansgen formed Second Wind with Rich Moore, a former Minor Threat roadie and drummer for the Untouchables. He also worked with Tool in 1992 on the production of their first EP, "Opiate".

In 2005, a mock-up of the cover of Minor Threat's first EP (also used on the "Minor Threat" LP and "Complete Discography" CD) was copied by athletic footwear manufacturer Nike for use on a promotional poster for a skateboarding tour called "Major Threat". Nike also altered Minor Threat's logo (designed by Jeff Nelson) for the same campaign, as well as featuring Nike shoes in the new picture, rather than the combat boots worn by Ian MacKaye's younger brother Alec on the original.

MacKaye issued a press statement condemning Nike's actions and said that he would discuss legal options with the other members of the band. Meanwhile, fans, at the encouragement of Dischord, organized a letter-writing campaign protesting Nike's infringement. On June 27, 2005, Nike issued a statement apologizing to Minor Threat, Dischord Records, and their fans for the "Major Threat" campaign and said that all promotional artwork (print and digital) that they could acquire were destroyed.

On October 29, 2005, Fox played the first few seconds of Minor Threat's "Salad Days" during an NFL broadcast. Use of the song was not cleared by Dischord Records or any of the members of Minor Threat. Fox claimed that the clip was too short to have violated any copyrights.

In 2007, Brooklyn-based company Wheelhouse Pickles marketed a pepper sauce named "Minor Threat Sauce". Requesting only that the original label design (which was based on the "Bottled Violence" artwork) be amended, Ian MacKaye gave the product his endorsement. A small mention of this was made in music magazine "Revolver", where MacKaye commented "I don't really like hot sauce but I like the Minor Threat stuff".

In 2013, Minor Threat shirts began appearing in Urban Outfitters stores. Ian MacKaye confirmed that the shirts were officially licensed. Having spent what he described as "a complete waste of time" trying to track down bootlegged Minor Threat merchandise, MacKaye and Dischord made arrangements with a merchandise company in California to manage licensing of the bands shirts. In comments that appeared in Rolling Stone, MacKaye called it "absurd" for the shirts to be sold for $28 but concluded that "my time is better spent doing other things" than dealing with shirts. Dischord had taken action against Forever 21 in 2009 for marketing unlicensed Minor Threat shirts.










</doc>
<doc id="18875" url="https://en.wikipedia.org/wiki?curid=18875" title="Mental event">
Mental event

A mental event is anything which happens within the mind or mind substitute of a conscious individual. Examples include thoughts, feelings, decisions, dreams, and realizations.

Some believe that mental events are not limited to human thought but can be associated with animals and artificial intelligence as well. Whether mental events are identical to complex physical events, or whether such an identity even makes sense, is central to the mind-body problem.

Some state that the mental and the physical are the very same property which cause any event(s). This view is known as substance monism. An opposing view is substance dualism, which claims that the mental and physical are fundamentally different and can exist independently.

Physicalism, a form of substance monism, states that everything that exists is either physical or depends on that which is physical. The existence of mental events has been used by philosophers as an argument against physicalism. For example, in his 1974 paper "What Is it Like to Be a Bat?", Thomas Nagel argues that physicalist theories of mind cannot explain an organism’s subjective experience because they cannot account for its mental events.





</doc>
<doc id="18878" url="https://en.wikipedia.org/wiki?curid=18878" title="Monopoly">
Monopoly

A monopoly (from Greek μόνος "mónos" ["alone" or "single"] and πωλεῖν "pōleîn" ["to sell"]) exists when a specific person or enterprise is the only supplier of a particular commodity. This contrasts with a monopsony which relates to a single entity's control of a market to purchase a good or service, and with oligopoly which consists of a few sellers dominating a market. Monopolies are thus characterized by a lack of economic competition to produce the good or service, a lack of viable substitute goods, and the possibility of a high monopoly price well above the seller's marginal cost that leads to a high monopoly profit. The verb "monopolise" or "monopolize" refers to the "process" by which a company gains the ability to raise prices or exclude competitors. In economics, a monopoly is a single seller. In law, a monopoly is a business entity that has significant market power, that is, the power to charge overly high prices. Although monopolies may be big businesses, size is not a characteristic of a monopoly. A small business may still have the power to raise prices in a small industry (or market).

A monopoly is distinguished from a monopsony, in which there is only one "buyer" of a product or service; a monopoly may also have monopsony control of a sector of a market. Likewise, a monopoly should be distinguished from a cartel (a form of oligopoly), in which several providers act together to coordinate services, prices or sale of goods. Monopolies, monopsonies and oligopolies are all situations in which one or a few entities have market power and therefore interact with their customers (monopoly or oligopoly), or suppliers (monopsony) in ways that distort the market.

Monopolies can be established by a government, form naturally, or form by integration.

In many jurisdictions, competition laws restrict monopolies. Holding a dominant position or a monopoly in a market is often not illegal in itself, however certain categories of behavior can be considered abusive and therefore incur legal sanctions when business is dominant. A government-granted monopoly or "legal monopoly", by contrast, is sanctioned by the state, often to provide an incentive to invest in a risky venture or enrich a domestic interest group. Patents, copyrights, and trademarks are sometimes used as examples of government-granted monopolies. The government may also reserve the venture for itself, thus forming a government monopoly.

Monopolies may be naturally occurring due to limited competition because the industry is resource intensive and requires substantial costs to operate.

In economics, the idea of monopoly is important in the study of management structures, which directly concerns normative aspects of economic competition, and provides the basis for topics such as industrial organization and economics of regulation. There are four basic types of market structures in traditional economic analysis: perfect competition, monopolistic competition, oligopoly and monopoly. A monopoly is a structure in which a single supplier produces and sells a given product. If there is a single seller in a certain market and there are no close substitutes for the product, then the market structure is that of a "pure monopoly". Sometimes, there are many sellers in an industry and/or there exist many close substitutes for the goods being produced, but nevertheless companies retain some market power. This is termed monopolistic competition, whereas in oligopoly the companies interact strategically.

In general, the main results from this theory compare price-fixing methods across market structures, analyze the effect of a certain structure on welfare, and vary technological/demand assumptions in order to assess the consequences for an abstract model of society. Most economic textbooks follow the practice of carefully explaining the "perfect competition" model, mainly because this helps to understand "departures" from it (the so-called "imperfect competition" models).

The boundaries of what constitutes a market and what does not are relevant distinctions to make in economic analysis. In a general equilibrium context, a good is a specific concept including geographical and time-related characteristics ("grapes sold during October 2009 in Moscow" is a different good from "grapes sold during October 2009 in New York"). Most studies of market structure relax a little their definition of a good, allowing for more flexibility in the identification of substitute goods.


Monopolies derive their market power from barriers to entry – circumstances that prevent or greatly impede a potential competitor's ability to compete in a market. There are three major types of barriers to entry: economic, legal and deliberate.
In addition to barriers to entry and competition, barriers to exit may be a source of market power. Barriers to exit are market conditions that make it difficult or expensive for a company to end its involvement with a market. High liquidation costs are a primary barrier to exiting. Market exit and shutdown are sometimes separate events. The decision whether to shut down or operate is not affected by exit barriers. A company will shut down if price falls below minimum average variable costs.

While monopoly and perfect competition mark the extremes of market structures there is some similarity. The cost functions are the same. Both monopolies and perfectly competitive (PC) companies minimize cost and maximize profit. The shutdown decisions are the same. Both are assumed to have perfectly competitive factors markets. There are distinctions, some of the most important distinctions are as follows:

The most significant distinction between a PC company and a monopoly is that the monopoly has a downward-sloping demand curve rather than the "perceived" perfectly elastic curve of the PC company. Practically all the variations mentioned above relate to this fact. If there is a downward-sloping demand curve then by necessity there is a distinct marginal revenue curve. The implications of this fact are best made manifest with a linear demand curve. Assume that the inverse demand curve is of the form x = a − by. Then the total revenue curve is TR = ay − by and the marginal revenue curve is thus MR = a − 2by. From this several things are evident. First the marginal revenue curve has the same y intercept as the inverse demand curve. Second the slope of the marginal revenue curve is twice that of the inverse demand curve. Third the x intercept of the marginal revenue curve is half that of the inverse demand curve. What is not quite so evident is that the marginal revenue curve is below the inverse demand curve at all points. Since all companies maximise profits by equating MR and MC it must be the case that at the profit-maximizing quantity MR and MC are less than price, which further implies that a monopoly produces less quantity at a higher price than if the market were perfectly competitive.

The fact that a monopoly has a downward-sloping demand curve means that the relationship between total revenue and output for a monopoly is much different than that of competitive companies. Total revenue equals price times quantity. A competitive company has a perfectly elastic demand curve meaning that total revenue is proportional to output. Thus the total revenue curve for a competitive company is a ray with a slope equal to the market price. A competitive company can sell all the output it desires at the market price. For a monopoly to increase sales it must reduce price. Thus the total revenue curve for a monopoly is a parabola that begins at the origin and reaches a maximum value then continuously decreases until total revenue is again zero. Total revenue has its maximum value when the slope of the total revenue function is zero. The slope of the total revenue function is marginal revenue. So the revenue maximizing quantity and price occur when MR = 0. For example, assume that the monopoly’s demand function is P = 50 − 2Q. The total revenue function would be TR = 50Q − 2Q and marginal revenue would be 50 − 4Q. Setting marginal revenue equal to zero we have

So the revenue maximizing quantity for the monopoly is 12.5 units and the revenue maximizing price is 25.

A company with a monopoly does not experience price pressure from competitors, although it may experience pricing pressure from potential competition. If a company increases prices too much, then others may enter the market if they are able to provide the same good, or a substitute, at a lesser price. The idea that monopolies in markets with easy entry need not be regulated against is known as the "revolution in monopoly theory".

A monopolist can extract only one premium, and getting into complementary markets does not pay. That is, the total profits a monopolist could earn if it sought to leverage its monopoly in one market by monopolizing a complementary market are equal to the extra profits it could earn anyway by charging more for the monopoly product itself. However, the one monopoly profit theorem is not true if customers in the monopoly good are stranded or poorly informed, or if the tied good has high fixed costs.

A pure monopoly has the same economic rationality of perfectly competitive companies, i.e. to optimise a profit function given some constraints. By the assumptions of increasing marginal costs, exogenous inputs' prices, and control concentrated on a single agent or entrepreneur, the optimal decision is to equate the marginal cost and marginal revenue of production. Nonetheless, a pure monopoly can – unlike a competitive company – alter the market price for its own convenience: a decrease of production results in a higher price. In the economics' jargon, it is said that pure monopolies have "a downward-sloping demand". An important consequence of such behaviour is worth noticing: typically a monopoly selects a higher price and lesser quantity of output than a price-taking company; again, less is available at a higher price.

A monopoly chooses that price that maximizes the difference between total revenue and total cost. The basic markup rule (as measured by the Lerner index) can be expressed as
formula_4,
where formula_5 is the price elasticity of demand the firm faces. The markup rules indicate that the ratio between profit margin and the price is inversely proportional to the price elasticity of demand. The implication of the rule is that the more elastic the demand for the product the less pricing power the monopoly has.

Market power is the ability to increase the product's price above marginal cost without losing all customers. Perfectly competitive (PC) companies have zero market power when it comes to setting prices. All companies of a PC market are price takers. The price is set by the interaction of demand and supply at the market or aggregate level. Individual companies simply take the price determined by the market and produce that quantity of output that maximizes the company's profits. If a PC company attempted to increase prices above the market level all its customers would abandon the company and purchase at the market price from other companies. A monopoly has considerable although not unlimited market power. A monopoly has the power to set prices or quantities although not both. A monopoly is a price maker. The monopoly is the market and prices are set by the monopolist based on their circumstances and not the interaction of demand and supply. The two primary factors determining monopoly market power are the company's demand curve and its cost structure.

Market power is the ability to affect the terms and conditions of exchange so that the price of a product is set by a single company (price is not imposed by the market as in perfect competition). Although a monopoly's market power is great it is still limited by the demand side of the market. A monopoly has a negatively sloped demand curve, not a perfectly inelastic curve. Consequently, any price increase will result in the loss of some customers.

Price discrimination allows a monopolist to increase its profit by charging higher prices for identical goods to those who are willing or able to pay more. For example, most economic textbooks cost more in the United States than in developing countries like Ethiopia. In this case, the publisher is using its government-granted copyright monopoly to price discriminate between the generally wealthier American economics students and the generally poorer Ethiopian economics students. Similarly, most patented medications cost more in the U.S. than in other countries with a (presumed) poorer customer base. Typically, a high general price is listed, and various market segments get varying discounts. This is an example of framing to make the process of charging some people higher prices more socially acceptable. Perfect price discrimination would allow the monopolist to charge each customer the exact maximum amount he would be willing to pay. This would allow the monopolist to extract all the consumer surplus of the market. While such perfect price discrimination is a theoretical construct, advances in information technology and micromarketing may bring it closer to the realm of possibility.

It is very important to realize that partial price discrimination can cause some customers who are inappropriately pooled with high price customers to be excluded from the market. For example, a poor student in the U.S. might be excluded from purchasing an economics textbook at the U.S. price, which the student may have been able to purchase at the Ethiopian price'. Similarly, a wealthy student in Ethiopia may be able to or willing to buy at the U.S. price, though naturally would hide such a fact from the monopolist so as to pay the reduced third world price. These are deadweight losses and decrease a monopolist's profits. As such, monopolists have substantial economic interest in improving their market information and "market segmenting".

There is important information for one to remember when considering the monopoly model diagram (and its associated conclusions) displayed here. The result that monopoly prices are higher, and production output lesser, than a competitive company follow from a requirement that the monopoly not charge different prices for different customers. That is, the monopoly is restricted from engaging in price discrimination (this is termed first degree price discrimination, such that all customers are charged the same amount). If the monopoly were permitted to charge individualised prices (this is termed third degree price discrimination), the quantity produced, and the price charged to the "marginal" customer, would be identical to that of a competitive company, thus eliminating the deadweight loss; however, all gains from trade (social welfare) would accrue to the monopolist and none to the consumer. In essence, every consumer would be indifferent between (1) going completely without the product or service and (2) being able to purchase it from the monopolist.

As long as the price elasticity of demand for most customers is less than one in absolute value, it is advantageous for a company to increase its prices: it receives more money for fewer goods. With a price increase, price elasticity tends to increase, and in the optimum case above it will be greater than one for most customers.

A company maximizes profit by selling where marginal revenue equals marginal cost. A company that does not engage in price discrimination will charge the profit maximizing price, P*, to all its customers. In such circumstances there are customers who would be willing to pay a higher price than P* and those who will not pay P* but would buy at a lower price. A price discrimination strategy is to charge less price sensitive buyers a higher price and the more price sensitive buyers a lower price. Thus additional revenue is generated from two sources. The basic problem is to identify customers by their willingness to pay.

The purpose of price discrimination is to transfer consumer surplus to the producer. Consumer surplus is the difference between the value of a good to a consumer and the price the consumer must pay in the market to purchase it. Price discrimination is not limited to monopolies.

Market power is a company’s ability to increase prices without losing all its customers. Any company that has market power can engage in price discrimination. Perfect competition is the only market form in which price discrimination would be impossible (a perfectly competitive company has a perfectly elastic demand curve and has zero market power).

There are three forms of price discrimination. First degree price discrimination charges each consumer the maximum price the consumer is willing to pay. Second degree price discrimination involves quantity discounts. Third degree price discrimination involves grouping consumers according to willingness to pay as measured by their price elasticities of demand and charging each group a different price. Third degree price discrimination is the most prevalent type.

There are three conditions that must be present for a company to engage in successful price discrimination. First, the company must have market power. Second, the company must be able to sort customers according to their willingness to pay for the good. Third, the firm must be able to prevent resell.

A company must have some degree of market power to practice price discrimination. Without market power a company cannot charge more than the market price. Any market structure characterized by a downward sloping demand curve has market power – monopoly, monopolistic competition and oligopoly. The only market structure that has no market power is perfect competition.

A company wishing to practice price discrimination must be able to prevent middlemen or brokers from acquiring the consumer surplus for themselves. The company accomplishes this by preventing or limiting resale. Many methods are used to prevent resale. For example, persons are required to show photographic identification and a boarding pass before boarding an airplane. Most travelers assume that this practice is strictly a matter of security. However, a primary purpose in requesting photographic identification is to confirm that the ticket purchaser is the person about to board the airplane and not someone who has repurchased the ticket from a discount buyer.

The inability to prevent resale is the largest obstacle to successful price discrimination. Companies have however developed numerous methods to prevent resale. For example, universities require that students show identification before entering sporting events. Governments may make it illegal to resale tickets or products. In Boston, Red Sox baseball tickets can only be resold legally to the team.

The three basic forms of price discrimination are first, second and third degree price discrimination. In "first degree price discrimination" the company charges the maximum price each customer is willing to pay. The maximum price a consumer is willing to pay for a unit of the good is the reservation price. Thus for each unit the seller tries to set the price equal to the consumer’s reservation price. Direct information about a consumer’s willingness to pay is rarely available. Sellers tend to rely on secondary information such as where a person lives (postal codes); for example, catalog retailers can use mail high-priced catalogs to high-income postal codes. First degree price discrimination most frequently occurs in regard to professional services or in transactions involving direct buyer/seller negotiations. For example, an accountant who has prepared a consumer's tax return has information that can be used to charge customers based on an estimate of their ability to pay.

In "second degree price discrimination" or quantity discrimination customers are charged different prices based on how much they buy. There is a single price schedule for all consumers but the prices vary depending on the quantity of the good bought. The theory of second degree price discrimination is a consumer is willing to buy only a certain quantity of a good at a given price. Companies know that consumer’s willingness to buy decreases as more units are purchased. The task for the seller is to identify these price points and to reduce the price once one is reached in the hope that a reduced price will trigger additional purchases from the consumer. For example, sell in unit blocks rather than individual units.

In "third degree price discrimination" or multi-market price discrimination the seller divides the consumers into different groups according to their willingness to pay as measured by their price elasticity of demand. Each group of consumers effectively becomes a separate market with its own demand curve and marginal revenue curve. The firm then attempts to maximize profits in each segment by equating MR and MC, Generally the company charges a higher price to the group with a more price inelastic demand and a relatively lesser price to the group with a more elastic demand. Examples of third degree price discrimination abound. Airlines charge higher prices to business travelers than to vacation travelers. The reasoning is that the demand curve for a vacation traveler is relatively elastic while the demand curve for a business traveler is relatively inelastic. Any determinant of price elasticity of demand can be used to segment markets. For example, seniors have a more elastic demand for movies than do young adults because they generally have more free time. Thus theaters will offer discount tickets to seniors.

Assume that by a uniform pricing system the monopolist would sell five units at a price of $10 per unit. Assume that his marginal cost is $5 per unit. Total revenue would be $50, total costs would be $25 and profits would be $25. If the monopolist practiced price discrimination he would sell the first unit for $50 the second unit for $40 and so on. Total revenue would be $150, his total cost would be $25 and his profit would be $125.00. Several things are worth noting. The monopolist acquires all the consumer surplus and eliminates practically all the deadweight loss because he is willing to sell to anyone who is willing to pay at least the marginal cost. Thus the price discrimination promotes efficiency. Secondly, by the pricing scheme price = average revenue and equals marginal revenue. That is the monopolist behaving like a perfectly competitive company. Thirdly, the discriminating monopolist produces a larger quantity than the monopolist operating by a uniform pricing scheme.
Successful price discrimination requires that companies separate consumers according to their willingness to buy. Determining a customer's willingness to buy a good is difficult. Asking consumers directly is fruitless: consumers don't know, and to the extent they do they are reluctant to share that information with marketers. The two main methods for determining willingness to buy are observation of personal characteristics and consumer actions. As noted information about where a person lives (postal codes), how the person dresses, what kind of car he or she drives, occupation, and income and spending patterns can be helpful in classifying.

According to the standard model, in which a monopolist sets a single price for all consumers, the monopolist will sell a lesser quantity of goods at a higher price than would companies by perfect competition. Because the monopolist ultimately forgoes transactions with consumers who value the product or service more than its price, monopoly pricing creates a deadweight loss referring to potential gains that went neither to the monopolist nor to consumers. Given the presence of this deadweight loss, the combined surplus (or wealth) for the monopolist and consumers is necessarily less than the total surplus obtained by consumers by perfect competition. Where efficiency is defined by the total gains from trade, the monopoly setting is less efficient than perfect competition.

It is often argued that monopolies tend to become less efficient and less innovative over time, becoming "complacent", because they do not have to be efficient or innovative to compete in the marketplace. Sometimes this very loss of psychological efficiency can increase a potential competitor's value enough to overcome market entry barriers, or provide incentive for research and investment into new alternatives. The theory of contestable markets argues that in some circumstances (private) monopolies are forced to behave "as if" there were competition because of the risk of losing their monopoly to new entrants. This is likely to happen when a market's barriers to entry are low. It might also be because of the availability in the longer term of substitutes in other markets. For example, a canal monopoly, while worth a great deal during the late 18th century United Kingdom, was worth much less during the late 19th century because of the introduction of railways as a substitute.

A natural monopoly is an organization that experiences increasing returns to scale over the relevant range of output and relatively high fixed costs. A natural monopoly occurs where the average cost of production "declines throughout the relevant range of product demand". The relevant range of product demand is where the average cost curve is below the demand curve. When this situation occurs, it is always cheaper for one large company to supply the market than multiple smaller companies; in fact, absent government intervention in such markets, will naturally evolve into a monopoly. An early market entrant that takes advantage of the cost structure and can expand rapidly can exclude smaller companies from entering and can drive or buy out other companies. A natural monopoly suffers from the same inefficiencies as any other monopoly. Left to its own devices, a profit-seeking natural monopoly will produce where marginal revenue equals marginal costs. Regulation of natural monopolies is problematic. Fragmenting such monopolies is by definition inefficient. The most frequently used methods dealing with natural monopolies are government regulations and public ownership. Government regulation generally consists of regulatory commissions charged with the principal duty of setting prices.

To reduce prices and increase output, regulators often use average cost pricing. By average cost pricing, the price and quantity are determined by the intersection of the average cost curve and the demand curve. This pricing scheme eliminates any positive economic profits since price equals average cost. Average-cost pricing is not perfect. Regulators must estimate average costs. Companies have a reduced incentive to lower costs. Regulation of this type has not been limited to natural monopolies. Average-cost pricing does also have some disadvantages. By setting price equal to the intersection of the demand curve and the average total cost curve, the firm's output is allocatively inefficient as the price is less than the marginal cost (which is the output quantity for a perfectly competitive and allocatively efficient market).

A government-granted monopoly (also called a "de jure monopoly") is a form of coercive monopoly by which a government grants exclusive privilege to a private individual or company to be the sole provider of a commodity; potential competitors are excluded from the market by law, regulation, or other mechanisms of government enforcement.
Samuel Insull, a British-born American electrical industry business magnate invented regulated monopoly.
This came from a combination of his business persona and his political one. On the one hand, he abhorred the waste of competing power producers, whose inefficiency would often double the cost of production. On the other hand, he believed in the citizen's right to fair treatment. So while he bought up rival companies and created a monopoly, he kept his prices low and campaigned vigorously for regulation.

A monopolist should shut down when price is less than average variable cost for every output level – in other words where the demand curve is entirely below the average variable cost curve. Under these circumstances at the profit maximum level of output (MR = MC) average revenue would be less than average variable costs and the monopolists would be better off shutting down in the short term.

In a free market, monopolies can be ended at any time by new competition, breakaway businesses, or consumers seeking alternatives. In a highly regulated market environment a government will often either regulate the monopoly, convert it into a publicly owned monopoly environment, or forcibly fragment it (see Antitrust law and trust busting). Public utilities, often being naturally efficient with only one operator and therefore less susceptible to efficient breakup, are often strongly regulated or publicly owned. American Telephone & Telegraph (AT&T) and Standard Oil are debatable examples of the breakup of a private monopoly by government: When AT&T, a monopoly previously protected by force of law, was broken up into various components in 1984, MCI, Sprint, and other companies were able to compete effectively in the long distance phone market.

The law regulating dominance in the European Union is governed by Article 102 of the Treaty on the Functioning of the European Union which aims at enhancing the consumer’s welfare and also the efficiency of allocation of resources by protecting competition on the downstream market. The existence of a very high market share does not always mean consumers are paying excessive prices since the threat of new entrants to the market can restrain a high-market-share company's price increases. Competition law does not make merely having a monopoly illegal, but rather abusing the power a monopoly may confer, for instance through exclusionary practices (i.e. pricing high just because you are the only one around.) It may also be noted that it is illegal to try to obtain a monopoly, by practices of buying out the competition, or equal practices. If one occurs naturally, such as a competitor going out of business, or lack of competition, it is not illegal until such time as the monopoly holder abuses the power.

First it is necessary to determine whether a company is dominant, or whether it behaves "to an appreciable extent independently of its competitors, customers and ultimately of its consumer". Establishing dominance is a two stage test. The first thing to consider is market definition which is one of the crucial factors of the test. It includes relevant product market and relevant geographic market. 

As the definition of the market is of a matter of interchangeability, if the goods or services are regarded as interchangeable then they are within the same product market. For example, in the case of "United Brands v Commission", it was argued in this case that bananas and other fresh fruit were in the same product market and later on dominance was found because the special features of the banana made it could only be interchangeable with other fresh fruits in a limited extent and other and is only exposed to their competition in a way that is hardly perceptible. The demand substitutability of the goods and services will help in defining the product market and it can be access by the ‘hypothetical monopolist’ test or the ‘SSNIP’ test .

It is necessary to define it because some goods can only be supplied within a narrow area due to technical, practical or legal reasons and this may help to indicate which undertakings impose a competitive constraint on the other undertakings in question. Since some goods are too expensive to transport where it might not be economic to sell them to distant markets in relation to their value, therefore the cost of transporting is a crucial factor here. Other factors might be legal controls which restricts an undertaking in a Member States from exporting goods or services to another.

Market definition may be difficult to measure but is important because if it is defined too broadly, the undertaking may be more likely to be found dominant and if it is defined too narrowly, the less likely that it will be found dominant.

As with collusive conduct, market shares are determined with reference to the particular market in which the company and product in question is sold. It does not in itself determine whether an undertaking is dominant but work as an indicator of the states of the existing competition within the market. The Herfindahl-Hirschman Index (HHI) is sometimes used to assess how competitive an industry is. It sums up the squares of the individual market shares of all of the competitors within the market. The lower the total, the less concentrated the market and the higher the total, the more concentrated the market. In the US, the merger guidelines state that a post-merger HHI below 1000 is viewed as not concentrated while HHIs above that will provoke further review.

By European Union law, very large market shares raise a presumption that a company is dominant, which may be rebuttable. A market share of 100% may be very rare but it is still possible to be found and in fact it has been identified in some cases, for instance the "AAMS v Commission" case. Undertakings possessing market share that is lower than 100% but over 90% had also been found dominant, for example, Microsoft v Commission case. In the AKZO v Commission case, the undertaking is presumed to be dominant if it has a market share of 50%. There are also findings of dominance that are below a market share of 50%, for instance, United Brands v Commission, it only possessed a market share of 40% to 45% and still to be found dominant with other factors. The lowest yet market share of a company considered "dominant" in the EU was 39.7%.If a company has a dominant position, then there is a special responsibility not to allow its conduct to impair competition on the common market however these will all falls away if it is not dominant.

When considering whether an undertaking is dominant, it involves a combination of factors. Each of them cannot be taken separately as if they are, they will not be as determinative as they are when they are combined together. Also, in cases where an undertaking has previously been found dominant, it is still necessary to redefine the market and make a whole new analysis of the conditions of competition based on the available evidence at the appropriate time.

According to the Guidance, there are three more issues that must be examined. They are actual competitors that relates to the market position of the dominant undertaking and its competitors, potential competitors that concerns the expansion and entry and lastly the countervailing buyer power.


Market share may be a valuable source of information regarding the market structure and the market position when it comes to accessing it. The dynamics of the market and the extent to which the goods and services differentiated are relevant in this area.


It concerns with the competition that would come from other undertakings which are not yet operating in the market but will enter it in the future. So, market shares may not be useful in accessing the competitive pressure that is exerted on an undertaking in this area. The potential entry by new firms and expansions by an undertaking must be taken into account, therefore the barriers to entry and barriers to expansion is an important factor here.


Competitive Constraints may not always come from actual or potential competitors. Sometimes, it may also come from powerful customers who have sufficient bargaining strength which come from its size or its commercial significance for a dominant firm.

There are three main types of abuses which are exploitative abuse, exclusionary abuse and single market abuse.


It arises when a monopolist has such significant market power that it can restrict its output while increasing the price above the competitive level without losing customers. This type is less concerned by the Commission than other types.


This is most concerned about by the Commissions because it is capable of causing long- term consumer damage and is more likely to prevent the development of competition. An example of it is exclusive dealing agreements.


It arises when a dominant undertaking carrying out excess pricing which would not only have an exploitative effect but also prevent parallel imports and limits intra- brand competition.


Despite wide agreement that the above constitute abusive practices, there is some debate about whether there needs to be a causal connection between the dominant position of a company and its actual abusive conduct. Furthermore, there has been some consideration of what happens when a company merely attempts to abuse its dominant position.

The term "monopoly" first appears in Aristotle's "Politics". Aristotle describes Thales of Miletus's cornering of the market in olive presses as a monopoly ("μονοπώλιον").

Another early reference to the concept of “monopoly” in a commercial sense appears in tractate Demai of the Mishna (2nd century C.E.), regarding the purchasing of agricultural goods from a dealer who has a monopoly on the produce (chapter 5; 4).

The meaning and understanding of the English word 'monopoly' has changed over the years.

Vending of common salt (sodium chloride) was historically a natural monopoly. Until recently, a combination of strong sunshine and low humidity or an extension of peat marshes was necessary for producing salt from the sea, the most plentiful source. Changing sea levels periodically caused salt "famines" and communities were forced to depend upon those who controlled the scarce inland mines and salt springs, which were often in hostile areas (e.g. the Sahara desert) requiring well-organised security for transport, storage, and distribution.

The Salt Commission was a legal monopoly in China. Formed in 758, the Commission controlled salt production and sales in order to raise tax revenue for the Tang Dynasty.

The "Gabelle" was a notoriously high tax levied upon salt in the Kingdom of France. The much-hated levy had a role in the beginning of the French Revolution, when strict legal controls specified who was allowed to sell and distribute salt. First instituted in 1286, the Gabelle was not permanently abolished until 1945.

Robin Gollan argues in "The Coalminers of New South Wales" that anti-competitive practices developed in the coal industry of Australia's Newcastle as a result of the business cycle. The monopoly was generated by formal meetings of the local management of coal companies agreeing to fix a minimum price for sale at dock. This collusion was known as "The Vend". The Vend ended and was reformed repeatedly during the late 19th century, ending by recession in the business cycle. "The Vend" was able to maintain its monopoly due to trade union assistance, and material advantages (primarily coal geography). During the early 20th century, as a result of comparable monopolistic practices in the Australian coastal shipping business, the Vend developed as an informal and illegal collusion between the steamship owners and the coal industry, eventually resulting in the High Court case Adelaide Steamship Co. Ltd v. R. & AG.

Standard Oil was an American oil producing, transporting, refining, and marketing company. Established in 1870, it became the largest oil refiner in the world. John D. Rockefeller was a founder, chairman and major shareholder. The company was an innovator in the development of the business trust. The Standard Oil trust streamlined production and logistics, lowered costs, and undercut competitors. "Trust-busting" critics accused Standard Oil of using aggressive pricing to destroy competitors and form a monopoly that threatened consumers. Its controversial history as one of the world's first and largest multinational corporations ended in 1911, when the United States Supreme Court ruled that Standard was an illegal monopoly. The Standard Oil trust was dissolved into 33 smaller companies; two of its surviving "child" companies are ExxonMobil and the Chevron Corporation.

U.S. Steel has been accused of being a monopoly. J. P. Morgan and Elbert H. Gary founded U.S. Steel in 1901 by combining Andrew Carnegie's Carnegie Steel Company with Gary's Federal Steel Company and William Henry "Judge" Moore's National Steel Company. At one time, U.S. Steel was the largest steel producer and largest corporation in the world. In its first full year of operation, U.S. Steel made 67 percent of all the steel produced in the United States. However, U.S. Steel's share of the expanding market slipped to 50 percent by 1911, and anti-trust prosecution that year failed.

De Beers settled charges of price fixing in the diamond trade in the 2000s. De Beers is well known for its monopoloid practices throughout the 20th century, whereby it used its dominant position to manipulate the international diamond market. The company used several methods to exercise this control over the market. Firstly, it convinced independent producers to join its single channel monopoly, it flooded the market with diamonds similar to those of producers who refused to join the cartel, and lastly, it purchased and stockpiled diamonds produced by other manufacturers in order to control prices through limiting supply.

In 2000, the De Beers business model changed due to factors such as the decision by producers in Russia, Canada and Australia to distribute diamonds outside the De Beers channel, as well as rising awareness of blood diamonds that forced De Beers to "avoid the risk of bad publicity" by limiting sales to its own mined products. De Beers' market share by value fell from as high as 90% in the 1980s to less than 40% in 2012, having resulted in a more fragmented diamond market with more transparency and greater liquidity.

In November 2011 the Oppenheimer family announced its intention to sell the entirety of its 40% stake in De Beers to Anglo American plc thereby increasing Anglo American's ownership of the company to 85%.[30] The transaction was worth £3.2 billion ($5.1 billion) in cash and ended the Oppenheimer dynasty's 80-year ownership of De Beers.

A public utility (or simply "utility") is an organization or company that maintains the infrastructure for a public service or provides a set of services for public consumption. Common examples of utilities are electricity, natural gas, water, sewage, cable television, and telephone. In the United States, public utilities are often natural monopolies because the infrastructure required to produce and deliver a product such as electricity or water is very expensive to build and maintain.

Western Union was criticized as a "price gouging" monopoly in the late 19th century.

American Telephone & Telegraph was a telecommunications giant. AT&T was broken up in 1984.

In the case of Telecom New Zealand, local loop unbundling was enforced by central government.

Telkom is a semi-privatised, part state-owned South African telecommunications company.

Deutsche Telekom is a former state monopoly, still partially state owned. Deutsche Telekom currently monopolizes high-speed VDSL broadband network.

The Long Island Power Authority (LIPA) provided electric service to over 1.1 million customers in Nassau and Suffolk counties of New York, and the Rockaway Peninsula in Queens.

The Comcast Corporation is the largest mass media and communications company in the world by revenue. It is the largest cable company and home Internet service provider in the United States, and the nation's third largest home telephone service provider. Comcast has a monopoly in Boston, Philadelphia, and many other small towns across the US.

The United Aircraft and Transport Corporation was an aircraft manufacturer holding company that was forced to divest itself of airlines in 1934.

Iarnród Éireann, the Irish Railway authority, is a current monopoly as Ireland does not have the size for more companies.

The Long Island Rail Road (LIRR) was founded in 1834, and since the mid-1800s has provided train service between Long Island and New York City. In the 1870s, LIRR became the sole railroad in that area through a series of acquisitions and consolidations. In 2013, the LIRR's commuter rail system is the busiest commuter railroad in North America, serving nearly 335,000 passengers daily.

Dutch East India Company was created as a legal trading monopoly in 1602. The "Vereenigde Oost-Indische Compagnie" enjoyed huge profits from its spice monopoly through most of the 17th century.

The British East India Company was created as a legal trading monopoly in 1600. The East India Company was formed for pursuing trade with the East Indies but ended up trading mainly with the Indian subcontinent, North-West Frontier Province, and Balochistan. The Company traded in basic commodities, which included cotton, silk, indigo dye, salt, saltpetre, tea and opium.

Major League Baseball survived U.S. anti-trust litigation in 1922, though its special status is still in dispute as of 2009.

The National Football League survived anti-trust lawsuit in the 1960s but was convicted of being an illegal monopoly in the 1980s.


According to professor Milton Friedman, laws against monopolies cause more harm than good, but unnecessary monopolies should be countered by removing tariffs and other regulation that upholds monopolies.

However, professor Steve H. Hanke believes that although private monopolies are more efficient than public ones, often by a factor of two, sometimes private natural monopolies, such as local water distribution, should be regulated (not prohibited) by, e.g., price auctions.

Thomas DiLorenzo asserts, however, that during the early days of utility companies where there was little regulation, there were no natural monopolies and there was competition. Only when companies realized that they could gain power through government did monopolies begin to form.

Baten, Bianchi and Moser find historical evidence that monopolies which are protected by patent laws may have adverse effects on the creation of innovation in an economy. They argue that under certain circumstances, compulsory licensing – which allows governments to license patents without the consent of patent-owners – may be effective in promoting invention by increasing the threat of competition in fields with low pre-existing levels of competition.



</doc>
<doc id="18879" url="https://en.wikipedia.org/wiki?curid=18879" title="Massachusetts Institute of Technology">
Massachusetts Institute of Technology

The Massachusetts Institute of Technology (MIT) is a private research university located in Cambridge, Massachusetts, United States. Founded in 1861 in response to the increasing industrialization of the United States, MIT adopted a European polytechnic university model and stressed laboratory instruction in applied science and engineering. The Institute is traditionally known for its research and education in the physical sciences and engineering, but more recently in biology, economics, linguistics and management as well. MIT is often ranked among the world's top universities.

, 91 Nobel laureates, 25 Turing Award winners, and 6 Fields Medalists have been affiliated with MIT as alumni, faculty members or researchers. In addition, 52 National Medal of Science recipients, 65 Marshall Scholars, 45 Rhodes Scholars, 38 MacArthur Fellows, 34 astronauts and 16 Chief Scientists of the U.S. Air Force have been affiliated with MIT. The school also has a strong entrepreneurial culture and the aggregated annual revenues of companies founded by MIT alumni ($1.9 trillion) would rank roughly as the tenth-largest economy in the world (2014). MIT is a member of the Association of American Universities (AAU).

In 1859, a proposal was submitted to the Massachusetts General Court to use newly filled lands in Back Bay, Boston for a "Conservatory of Art and Science", but the proposal failed. A charter for the incorporation of the Massachusetts Institute of Technology, proposed by William Barton Rogers, was signed by the governor of Massachusetts on April 10, 1861.

Rogers, a professor from the University of Virginia, wanted to establish an institution to address rapid scientific and technological advances. He did not wish to found a professional school, but a combination with elements of both professional and liberal education, proposing that:

The true and only practicable object of a polytechnic school is, as I conceive, the teaching, not of the minute details and manipulations of the arts, which can be done only in the workshop, but the inculcation of those scientific principles which form the basis and explanation of them, and along with this, a full and methodical review of all their leading processes and operations in connection with physical laws.

The Rogers Plan reflected the German research university model, emphasizing an independent faculty engaged in research, as well as instruction oriented around seminars and laboratories.

Two days after MIT was chartered, the first battle of the Civil War broke out. After a long delay through the war years, MIT's first classes were held in the Mercantile Building in Boston in 1865. The new institute was founded as part of the Morrill Land-Grant Colleges Act to fund institutions "to promote the liberal and practical education of the industrial classes" and was a land-grant school. In 1863 under the same act, the Commonwealth of Massachusetts founded the Massachusetts Agricultural College, which developed as the University of Massachusetts Amherst. In 1866, the proceeds from land sales went toward new buildings in the Back Bay.

MIT was informally called "Boston Tech". The institute adopted the European polytechnic university model and emphasized laboratory instruction from an early date. Despite chronic financial problems, the institute saw growth in the last two decades of the 19th century under President Francis Amasa Walker. Programs in electrical, chemical, marine, and sanitary engineering were introduced, new buildings were built, and the size of the student body increased to more than one thousand.

The curriculum drifted to a vocational emphasis, with less focus on theoretical science. The fledgling school still suffered from chronic financial shortages which diverted the attention of the MIT leadership. During these "Boston Tech" years, MIT faculty and alumni rebuffed Harvard University president (and former MIT faculty) Charles W. Eliot's repeated attempts to merge MIT with Harvard College's Lawrence Scientific School. There would be at least six attempts to absorb MIT into Harvard. In its cramped Back Bay location, MIT could not afford to expand its overcrowded facilities, driving a desperate search for a new campus and funding. Eventually the MIT Corporation approved a formal agreement to merge with Harvard, over the vehement objections of MIT faculty, students, and alumni. However, a 1917 decision by the Massachusetts Supreme Judicial Court effectively put an end to the merger scheme.
In 1916, the MIT administration and the MIT charter crossed the Charles River on the ceremonial barge "Bucentaur" built for the occasion, to signify MIT's move to a spacious new campus largely consisting of filled land on a mile-long tract along the Cambridge side of the Charles River. The neoclassical "New Technology" campus was designed by William W. Bosworth and had been funded largely by anonymous donations from a mysterious "Mr. Smith", starting in 1912. In January 1920, the donor was revealed to be the industrialist George Eastman of Rochester, New York, who had invented methods of film production and processing, and founded Eastman Kodak. Between 1912 and 1920, Eastman donated $20 million ($ million in 2015 dollars) in cash and Kodak stock to MIT.

In the 1930s, President Karl Taylor Compton and Vice-President (effectively Provost) Vannevar Bush emphasized the importance of pure sciences like physics and chemistry and reduced the vocational practice required in shops and drafting studios. The Compton reforms "renewed confidence in the ability of the Institute to develop leadership in science as well as in engineering". Unlike Ivy League schools, MIT catered more to middle-class families, and depended more on tuition than on endowments or grants for its funding. The school was elected to the Association of American Universities in 1934.

Still, as late as 1949, the Lewis Committee lamented in its report on the state of education at MIT that "the Institute is widely conceived as basically a vocational school", a "partly unjustified" perception the committee sought to change. The report comprehensively reviewed the undergraduate curriculum, recommended offering a broader education, and warned against letting engineering and government-sponsored research detract from the sciences and humanities. The School of Humanities, Arts, and Social Sciences and the MIT Sloan School of Management were formed in 1950 to compete with the powerful Schools of Science and Engineering. Previously marginalized faculties in the areas of economics, management, political science, and linguistics emerged into cohesive and assertive departments by attracting respected professors and launching competitive graduate programs. The School of Humanities, Arts, and Social Sciences continued to develop under the successive terms of the more humanistically oriented presidents Howard W. Johnson and Jerome Wiesner between 1966 and 1980.

MIT's involvement in military science surged during World War II. In 1941, Vannevar Bush was appointed head of the federal Office of Scientific Research and Development and directed funding to only a select group of universities, including MIT. Engineers and scientists from across the country gathered at MIT's Radiation Laboratory, established in 1940 to assist the British military in developing microwave radar. The work done there significantly affected both the war and subsequent research in the area. Other defense projects included gyroscope-based and other complex control systems for gunsight, bombsight, and inertial navigation under Charles Stark Draper's Instrumentation Laboratory; the development of a digital computer for flight simulations under Project Whirlwind; and high-speed and high-altitude photography under Harold Edgerton. By the end of the war, MIT became the nation's largest wartime R&D contractor (attracting some criticism of Bush), employing nearly 4000 in the Radiation Laboratory alone and receiving in excess of $100 million ($ billion in 2015 dollars) before 1946. Work on defense projects continued even after then. Post-war government-sponsored research at MIT included SAGE and guidance systems for ballistic missiles and Project Apollo.

These activities affected MIT profoundly. A 1949 report noted the lack of "any great slackening in the pace of life at the Institute" to match the return to peacetime, remembering the "academic tranquility of the prewar years", though acknowledging the significant contributions of military research to the increased emphasis on graduate education and rapid growth of personnel and facilities. The faculty doubled and the graduate student body quintupled during the terms of Karl Taylor Compton, president of MIT between 1930 and 1948; James Rhyne Killian, president from 1948 to 1957; and Julius Adams Stratton, chancellor from 1952 to 1957, whose institution-building strategies shaped the expanding university. By the 1950s, MIT no longer simply benefited the industries with which it had worked for three decades, and it had developed closer working relationships with new patrons, philanthropic foundations and the federal government.

In late 1960s and early 1970s, student and faculty activists protested against the Vietnam War and MIT's defense research. In this period MIT's various departments were researching helicopters, smart bombs and counterinsurgency techniques for the war in Vietnam as well as guidance systems for nuclear missiles. The Union of Concerned Scientists was founded on March 4, 1969 during a meeting of faculty members and students seeking to shift the emphasis on military research toward environmental and social problems. MIT ultimately divested itself from the Instrumentation Laboratory and moved all classified research off-campus to the MIT Lincoln Laboratory facility in 1973 in response to the protests. The student body, faculty, and administration remained comparatively unpolarized during what was a tumultuous time for many other universities. Johnson was seen to be highly successful in leading his institution to "greater strength and unity" after these times of turmoil.
However six MIT students were sentenced to prison terms at this time and some former student leaders, such as Michael Albert and George Katsiaficas, are still indignant about MIT's role in military research and its suppression of these protests. (Richard Leacock's film, "November Actions", records some of these tumultuous events.)

In the 1980s, there was more controversy at MIT over its involvement in SDI (space weaponry) and CBW (chemical and biological warfare) research. More recently, MIT’s research for the military has included work on robots, drones and ‘battle suits’.

MIT has kept pace with and helped to advance the digital age. In addition to developing the predecessors to modern computing and networking technologies, students, staff, and faculty members at Project MAC, the Artificial Intelligence Laboratory, and the Tech Model Railroad Club wrote some of the earliest interactive computer video games like "Spacewar!" and created much of modern hacker slang and culture. Several major computer-related organizations have originated at MIT since the 1980s: Richard Stallman's GNU Project and the subsequent Free Software Foundation were founded in the mid-1980s at the AI Lab; the MIT Media Lab was founded in 1985 by Nicholas Negroponte and Jerome Wiesner to promote research into novel uses of computer technology; the World Wide Web Consortium standards organization was founded at the Laboratory for Computer Science in 1994 by Tim Berners-Lee; the OpenCourseWare project has made course materials for over 2,000 MIT classes available online free of charge since 2002; and the One Laptop per Child initiative to expand computer education and connectivity to children worldwide was launched in 2005.

MIT was named a sea-grant college in 1976 to support its programs in oceanography and marine sciences and was named a space-grant college in 1989 to support its aeronautics and astronautics programs. Despite diminishing government financial support over the past quarter century, MIT launched several successful development campaigns to significantly expand the campus: new dormitories and athletics buildings on west campus; the Tang Center for Management Education; several buildings in the northeast corner of campus supporting research into biology, brain and cognitive sciences, genomics, biotechnology, and cancer research; and a number of new "backlot" buildings on Vassar Street including the Stata Center. Construction on campus in the 2000s included expansions of the Media Lab, the Sloan School's eastern campus, and graduate residences in the northwest. In 2006, President Hockfield launched the MIT Energy Research Council to investigate the interdisciplinary challenges posed by increasing global energy consumption.

In 2001, inspired by the open source and open access movements, MIT launched OpenCourseWare to make the lecture notes, problem sets, syllabi, exams, and lectures from the great majority of its courses available online for no charge, though without any formal accreditation for coursework completed. While the cost of supporting and hosting the project is high, OCW expanded in 2005 to include other universities as a part of the OpenCourseWare Consortium, which currently includes more than 250 academic institutions with content available in at least six languages. In 2011, MIT announced it would offer formal certification (but not credits or degrees) to online participants completing coursework in its "MITx" program, for a modest fee. The "edX" online platform supporting MITx was initially developed in partnership with Harvard and its analogous "Harvardx" initiative. The courseware platform is open source, and other universities have already joined and added their own course content. In March 2009 the MIT faculty adopted an open-access policy to make its scholarship publicly accessible online.

Three days after the Boston Marathon bombing of April 2013, MIT Police patrol officer Sean Collier was fatally shot by the suspects Dzhokhar and Tamerlan Tsarnaev, setting off a violent manhunt that shut down the campus and much of the Boston metropolitan area for a day. One week later, Collier's memorial service was attended by more than 10,000 people, in a ceremony hosted by the MIT community with thousands of police officers from the New England region and Canada. On November 25, 2013, MIT announced the creation of the Collier Medal, to be awarded annually to "an individual or group that embodies the character and qualities that Officer Collier exhibited as a member of the MIT community and in all aspects of his life". The announcement further stated that "Future recipients of the award will include those whose contributions exceed the boundaries of their profession, those who have contributed to building bridges across the community, and those who consistently and selflessly perform acts of kindness".

In September 2017, the school announced the creation of an artificial intelligence research lab called the MIT-IBM Watson AI Lab. IBM will spend $240 million over the next decade and the lab will be staffed by MIT and IBM scientists.

MIT's campus in the city of Cambridge spans approximately a mile along the north side of the Charles River basin. The campus is divided roughly in half by Massachusetts Avenue, with most dormitories and student life facilities to the west and most academic buildings to the east. The bridge closest to MIT is the Harvard Bridge, which is known for being marked off in a non-standard unit of length – the smoot.

The Kendall MBTA Red Line station is located on the northeastern edge of the campus, in Kendall Square. The Cambridge neighborhoods surrounding MIT are a mixture of high tech companies occupying both modern office and rehabilitated industrial buildings, as well as socio-economically diverse residential neighborhoods. In early 2016, MIT presented its updated Kendall Square Initiative to the City of Cambridge, with plans for mixed-use educational, retail, residential, startup incubator, and office space in a dense high-rise transit-oriented development plan. The MIT Museum will eventually be moved immediately adjacent to a Kendall Square subway entrance, joining the List Visual Arts Center on the eastern end of the campus.

Each building at MIT has a number (possibly preceded by a "W", "N", "E", or "NW") designation and most have a name as well. Typically, academic and office buildings are referred to primarily by number while residence halls are referred to by name. The organization of building numbers roughly corresponds to the order in which the buildings were built and their location relative (north, west, and east) to the original center cluster of Maclaurin buildings. Many of the buildings are connected above ground as well as through an extensive network of underground tunnels, providing protection from the Cambridge weather as well as a venue for roof and tunnel hacking.

MIT's on-campus nuclear reactor is one of the most powerful university-based nuclear reactors in the United States. The prominence of the reactor's containment building in a densely populated area has been controversial, but MIT maintains that it is well-secured. In 1999 Bill Gates donated US$20 million to MIT for the construction of a computer laboratory named the "William H. Gates Building", and designed by architect Frank Gehry. While Microsoft had previously given financial support to the institution, this was the first personal donation received from Gates.

Other notable campus facilities include a pressurized wind tunnel for testing aerodynamic research and a towing tank for testing ship and ocean structure designs. MIT's campus-wide wireless network was completed in the fall of 2005 and consists of nearly 3,000 access points covering of campus.

In 2001, the Environmental Protection Agency sued MIT for violating the Clean Water Act and the Clean Air Act with regard to its hazardous waste storage and disposal procedures. MIT settled the suit by paying a $155,000 fine and launching three environmental projects. In connection with capital campaigns to expand the campus, the Institute has also extensively renovated existing buildings to improve their energy efficiency. MIT has also taken steps to reduce its environmental impact by running alternative fuel campus shuttles, subsidizing public transportation passes, and building a low-emission cogeneration plant that serves most of the campus electricity, heating, and cooling requirements.

The MIT Police with state and local authorities, in the 2009-2011 period, have investigated reports of 12 forcible sex offenses, 6 robberies, 3 aggravated assaults, 164 burglaries, 1 case of arson, and 4 cases of motor vehicle theft on campus; affecting a community of around 22,000 students and employees.

MIT has substantial commercial real estate holdings in Cambridge on which it pays property taxes, plus an additional voluntary payment in lieu of taxes (PILOT) on academic buildings which are legally tax-exempt. , it is the largest taxpayer in the city, contributing approximately 14% of the city's annual revenues. Holdings include Technology Square, parts of Kendall Square, and many properties in Cambridgeport and Area 4 neighboring the educational buildings. The land is held for investment purposes and potential long-term expansion.

MIT's School of Architecture, now the School of Architecture and Planning, was the first in the United States, and it has a history of commissioning progressive buildings. The first buildings constructed on the Cambridge campus, completed in 1916, are sometimes called the "Maclaurin buildings" after Institute president Richard Maclaurin who oversaw their construction. Designed by William Welles Bosworth, these imposing buildings were built of reinforced concrete, a first for a non-industrial – much less university – building in the U.S. Bosworth's design was influenced by the City Beautiful Movement of the early 1900s and features the Pantheon-esque Great Dome housing the Barker Engineering Library. The Great Dome overlooks Killian Court, where graduation ceremonies are held each year. The friezes of the limestone-clad buildings around Killian Court are engraved with the names of important scientists and philosophers. The spacious Building 7 atrium at 77 Massachusetts Avenue is regarded as the entrance to the Infinite Corridor and the rest of the campus.

Alvar Aalto's Baker House (1947), Eero Saarinen's MIT Chapel and Kresge Auditorium (1955), and I.M. Pei's Green, Dreyfus, Landau, and Wiesner buildings represent high forms of post-war modernist architecture. More recent buildings like Frank Gehry's Stata Center (2004), Steven Holl's Simmons Hall (2002), Charles Correa's Building 46 (2005), and Fumihiko Maki's Media Lab Extension (2009) stand out among the Boston area's classical architecture and serve as examples of contemporary campus "starchitecture". These buildings have not always been well received; in 2010, "The Princeton Review" included MIT in a list of twenty schools whose campuses are "tiny, unsightly, or both".

Undergraduates are guaranteed four-year housing in one of MIT's 10 undergraduate dormitories. Those living on campus can receive support and mentoring from live-in graduate student tutors, resident advisors, and faculty housemasters. Because housing assignments are made based on the preferences of the students themselves, diverse social atmospheres can be sustained in different living groups; for example, according to the "Yale Daily News" staff's "The Insider's Guide to the Colleges, 2010", "The split between East Campus and West Campus is a significant characteristic of MIT. East Campus has gained a reputation as a thriving counterculture." MIT also has 5 dormitories for single graduate students and 2 apartment buildings on campus for married student families.

MIT has an active Greek and co-op housing system, including thirty-six fraternities, sororities, and independent living groups (FSILGs). , 98% of all undergraduates lived in MIT-affiliated housing; 54% of the men participated in fraternities and 20% of the women were involved in sororities. Most FSILGs are located across the river in Back Bay near where MIT was founded, and there is also a cluster of fraternities on MIT's West Campus that face the Charles River Basin. After the 1997 alcohol-related death of Scott Krueger, a new pledge at the Phi Gamma Delta fraternity, MIT required all freshmen to live in the dormitory system starting in 2002. Because FSILGs had previously housed as many as 300 freshmen off-campus, the new policy could not be implemented until Simmons Hall opened in that year. Recently, MIT has also shut down Senior House. Last year, MIT administrators released data showing just 60 percent of Senior House residents graduated in four years. Campus-wide, the four-year graduation rate is 84 percent.

MIT is chartered as a non-profit organization and is owned and governed by a privately appointed board of trustees known as the MIT Corporation. The current board consists of 43 members elected to five-year terms, 25 life members who vote until their 75th birthday, 3 elected officers (President, Treasurer, and Secretary), and 4 "ex officio" members (the president of the alumni association, the Governor of Massachusetts, the Massachusetts Secretary of Education, and the Chief Justice of the Massachusetts Supreme Judicial Court). The board is chaired by Robert Millard, a co-founder of
L-3 Communications Holdings. The Corporation approves the budget, new programs, degrees and faculty appointments, and elects the President to serve as the chief executive officer of the university and preside over the Institute's faculty. MIT's endowment and other financial assets are managed through a subsidiary called MIT Investment Management Company (MITIMCo). Valued at $13.182 billion in 2016, MIT's endowment is the sixth-largest among American colleges and universities.

MIT has five schools (Science, Engineering, Architecture and Planning, Management, and Humanities, Arts, and Social Sciences) and one college (Whitaker College of Health Sciences and Technology), but no schools of law or medicine. While faculty committees assert substantial control over many areas of MIT's curriculum, research, student life, and administrative affairs, the chair of each of MIT's 32 academic departments reports to the dean of that department's school, who in turn reports to the Provost under the President. The current president is L. Rafael Reif, who formerly served as provost under President Susan Hockfield, the first woman to hold the post.

MIT is a large, highly residential, research university with a majority of enrollments in graduate and professional programs. The university has been accredited by the New England Association of Schools and Colleges since 1929. MIT operates on a 4–1–4 academic calendar with the fall semester beginning after Labor Day and ending in mid-December, a 4-week "Independent Activities Period" in the month of January, and the spring semester commencing in early February and ceasing in late May.

MIT students refer to both their majors and classes using numbers or acronyms alone. Departments and their corresponding majors are numbered in the approximate order of their foundation; for example, Civil and Environmental Engineering is , while Linguistics and Philosophy is . Students majoring in Electrical Engineering and Computer Science (EECS), the most popular department, collectively identify themselves as "Course 6". MIT students use a combination of the department's course number and the number assigned to the class to identify their subjects; for instance, the introductory calculus-based classical mechanics course is simply "8.01" at MIT.

The four-year, full-time undergraduate program maintains a balance between professional majors and those in the arts and sciences, and has been dubbed "most selective" by "U.S. News", admitting few transfer students and 6.7% of its applicants in the 2017-2018 admissions cycle. MIT offers 44 undergraduate degrees across its five schools. In the 2010–2011 academic year, 1,161 bachelor of science degrees (abbreviated "SB") were granted, the only type of undergraduate degree MIT now awards. In the 2011 fall term, among students who had designated a major, the School of Engineering was the most popular division, enrolling 63% of students in its 19 degree programs, followed by the School of Science (29%), School of Humanities, Arts, & Social Sciences (3.7%), Sloan School of Management (3.3%), and School of Architecture and Planning (2%). The largest undergraduate degree programs were in Electrical Engineering and Computer Science (), Computer Science and Engineering (), Mechanical Engineering (), Physics (), and Mathematics ().

All undergraduates are required to complete a core curriculum called the General Institute Requirements (GIRs). The Science Requirement, generally completed during freshman year as prerequisites for classes in science and engineering majors, comprises two semesters of physics, two semesters of calculus, one semester of chemistry, and one semester of biology. There is a Laboratory Requirement, usually satisfied by an appropriate class in a course major. The Humanities, Arts, and Social Sciences (HASS) Requirement consists of eight semesters of classes in the humanities, arts, and social sciences, including at least one semester from each division as well as the courses required for a designated concentration in a HASS division. Under the Communication Requirement, two of the HASS classes, plus two of the classes taken in the designated major must be "communication-intensive", including "substantial instruction and practice in oral presentation". Finally, all students are required to complete a swimming test; non-varsity athletes must also take four quarters of physical education classes.

Most classes rely on a combination of lectures, recitations led by associate professors or graduate students, weekly problem sets ("p-sets"), and periodic quizzes or tests. While the pace and difficulty of MIT coursework has been compared to "drinking from a fire hose", the freshmen retention rate at MIT is similar to other research universities. The "pass/no-record" grading system relieves some pressure for first-year undergraduates. For each class taken in the fall term, freshmen transcripts will either report only that the class was passed, or otherwise not have any record of it. In the spring term, passing grades (A, B, C) appear on the transcript while non-passing grades are again not recorded. (Grading had previously been "pass/no record" all freshman year, but was amended for the Class of 2006 to prevent students from gaming the system by completing required major classes in their freshman year.) Also, freshmen may choose to join alternative learning communities, such as Experimental Study Group, Concourse, or Terrascope.

In 1969, Margaret MacVicar founded the Undergraduate Research Opportunities Program (UROP) to enable undergraduates to collaborate directly with faculty members and researchers. Students join or initiate research projects ("UROPs") for academic credit, pay, or on a volunteer basis through postings on the UROP website or by contacting faculty members directly. A substantial majority of undergraduates participate. Students often become published, file patent applications, and/or launch start-up companies based upon their experience in UROPs.

In 1970, the then-Dean of Institute Relations, Benson R. Snyder, published "The Hidden Curriculum," arguing that education at MIT was often slighted in favor of following a set of unwritten expectations, and that graduating with good grades was more often the product of figuring out the system rather than a solid education. The successful student, according to Snyder, was the one who was able to discern which of the formal requirements were to be ignored in favor of which unstated norms. For example, organized student groups had compiled "course bibles"—collections of problem-set and examination questions and answers for later students to use as references. This sort of gamesmanship, Snyder argued, hindered development of a creative intellect and contributed to student discontent and unrest.

MIT's graduate program has high coexistence with the undergraduate program, and many courses are taken by qualified students at both levels. MIT offers a comprehensive doctoral program with degrees in the humanities, social sciences, and STEM fields as well as professional degrees. The Institute offers graduate programs leading to academic degrees such as the Master of Science (which is abbreviated as SM at MIT), various Engineer's Degrees, Doctor of Philosophy (PhD), and Doctor of Science (ScD) and interdisciplinary graduate programs such as the MD-PhD (with Harvard Medical School).

Admission to graduate programs is decentralized; applicants apply directly to the department or degree program. More than 90% of doctoral students are supported by fellowships, research assistantships (RAs), or teaching assistantships (TAs).

MIT awarded 1,547 master's degrees and 609 doctoral degrees in the academic year 2010–11. In the 2011 fall term, the School of Engineering was the most popular academic division, enrolling 45.0% of graduate students, followed by the Sloan School of Management (19%), School of Science (16.9%), School of Architecture and Planning (9.2%), Whitaker College of Health Sciences (5.1%), and School of Humanities, Arts, and Social Sciences (4.7%). The largest graduate degree programs were the Sloan MBA, Electrical Engineering and Computer Science, and Mechanical Engineering.

MIT also places among the top ten in many overall rankings of universities (see right) and rankings based on students' revealed preferences. For several years, "U.S. News & World Report", the QS World University Rankings, and the Academic Ranking of World Universities have ranked MIT's School of Engineering first, as did the 1995 National Research Council report. In the same lists, MIT's strongest showings apart from in engineering are in computer science, the natural sciences, business, architecture, economics, linguistics, mathematics, and, to a lesser extent, political science and philosophy.

In 2014, "Money" magazine ranked MIT at number three for "Best Colleges for Your Money" in the US, based on its assessment of getting "the most bang for your tuition buck", factoring in quality of education, affordability, and career outcomes. , "Forbes" magazine rated MIT as the second "Most Entrepreneurial University", based on the percentage of alumni and students self-identifying as founders or business owners on LinkedIn. In 2015, Brookings Fellow Jonathan Rothwell issued a report "Beyond College Rankings", placing MIT as third in the US, with an estimated 45% value-added to mid-career salary.

Times Higher Education has recognized MIT as one of the world's "six super brands" on its "World Reputation Rankings", along with Berkeley, Cambridge, Harvard, Oxford and Stanford. In 2017, the Times Higher Education World University Rankings rated MIT the #2 university for arts and humanities.

The university historically pioneered research and training collaborations between academia, industry and government.  In 1946, President Compton, Harvard Business School professor Georges Doriot, and Massachusetts Investor Trust chairman Merrill Grisswold founded American Research and Development Corporation, the first American venture-capital firm.  In 1948, Compton established the MIT Industrial Liaison Program. Throughout the late 1980s and early 1990s, American politicians and business leaders accused MIT and other universities of contributing to a declining economy by transferring taxpayer-funded research and technology to international – especially Japanese – firms that were competing with struggling American businesses. On the other hand, MIT's extensive collaboration with the federal government on research projects has led to several MIT leaders serving as presidential scientific advisers since 1940. MIT established a Washington Office in 1991 to continue effective lobbying for research funding and national science policy.

The U.S. Justice Department began an investigation in 1989, and in 1991 filed an antitrust suit against MIT, the eight Ivy League colleges, and eleven other institutions for allegedly engaging in price-fixing during their annual "Overlap Meetings", which were held to prevent bidding wars over promising prospective students from consuming funds for need-based scholarships. While the Ivy League institutions settled, MIT contested the charges, arguing that the practice was not anti-competitive because it ensured the availability of aid for the greatest number of students. MIT ultimately prevailed when the Justice Department dropped the case in 1994.

MIT's proximity to Harvard University ("the other school up the river") has led to a substantial number of research collaborations such as the Harvard-MIT Division of Health Sciences and Technology and the Broad Institute. In addition, students at the two schools can cross-register for credits toward their own school's degrees without any additional fees. A cross-registration program between MIT and Wellesley College has also existed since 1969, and in 2002 the Cambridge–MIT Institute launched an undergraduate exchange program between MIT and the University of Cambridge. MIT has more modest cross-registration programs with Boston University, Brandeis University, Tufts University, Massachusetts College of Art and the School of the Museum of Fine Arts, Boston.

MIT maintains substantial research and faculty ties with independent research organizations in the Boston area, such as the Charles Stark Draper Laboratory, the Whitehead Institute for Biomedical Research, and the Woods Hole Oceanographic Institution. Ongoing international research and educational collaborations include the Amsterdam Institute for Advanced Metropolitan Solutions (AMS Institute), Singapore-MIT Alliance, MIT-Politecnico di Milano, MIT-Zaragoza International Logistics Program, and projects in other countries through the MIT International Science and Technology Initiatives (MISTI) program.

The mass-market magazine "Technology Review" is published by MIT through a subsidiary company, as is a special edition that also serves as an alumni magazine. The MIT Press is a major university press, publishing over 200 books and 30 journals annually, emphasizing science and technology as well as arts, architecture, new media, current events and social issues.

The MIT library system consists of five subject libraries: Barker (Engineering), Dewey (Economics), Hayden (Humanities and Science), Lewis (Music), and Rotch (Arts and Architecture). There are also various specialized libraries and archives. The libraries contain more than 2.9 million printed volumes, 2.4 million microforms, 49,000 print or electronic journal subscriptions, and 670 reference databases. The past decade has seen a trend of increased focus on digital over print resources in the libraries. Notable collections include the Lewis Music Library with an emphasis on 20th and 21st-century music and electronic music, the List Visual Arts Center's rotating exhibitions of contemporary art, and the Compton Gallery's cross-disciplinary exhibitions. MIT allocates a percentage of the budget for all new construction and renovation to commission and support its extensive public art and outdoor sculpture collection.

The MIT Museum was founded in 1971 and collects, preserves, and exhibits artifacts significant to the culture and history of MIT. The museum now engages in significant educational outreach programs for the general public, including the annual Cambridge Science Festival, the first celebration of this kind in the United States. Since 2005, its official mission has been, "to engage the wider community with MIT's science, technology and other areas of scholarship in ways that will best serve the nation and the world in the 21st century".

MIT was elected to the Association of American Universities in 1934 and remains a research university with a very high level of research activity; research expenditures totaled $718.2 million in 2009. The federal government was the largest source of sponsored research, with the Department of Health and Human Services granting $255.9 million, Department of Defense $97.5 million, Department of Energy $65.8 million, National Science Foundation $61.4 million, and NASA $27.4 million. MIT employs approximately 1300 researchers in addition to faculty. In 2011, MIT faculty and researchers disclosed 632 inventions, were issued 153 patents, earned $85.4 million in cash income, and received $69.6 million in royalties. Through programs like the Deshpande Center, MIT faculty leverage their research and discoveries into multi-million-dollar commercial ventures.

In electronics, magnetic core memory, radar, single electron transistors, and inertial guidance controls were invented or substantially developed by MIT researchers. Harold Eugene Edgerton was a pioneer in high speed photography and sonar. Claude E. Shannon developed much of modern information theory and discovered the application of Boolean logic to digital circuit design theory. In the domain of computer science, MIT faculty and researchers made fundamental contributions to cybernetics, artificial intelligence, computer languages, machine learning, robotics, and cryptography. At least nine Turing Award laureates and seven recipients of the Draper Prize in engineering have been or are currently associated with MIT.

Current and previous physics faculty have won eight Nobel Prizes, four Dirac Medals, and three Wolf Prizes predominantly for their contributions to subatomic and quantum theory. Members of the chemistry department have been awarded three Nobel Prizes and one Wolf Prize for the discovery of novel syntheses and methods. MIT biologists have been awarded six Nobel Prizes for their contributions to genetics, immunology, oncology, and molecular biology. Professor Eric Lander was one of the principal leaders of the Human Genome Project. Positronium atoms, synthetic penicillin, synthetic self-replicating molecules, and the genetic bases for Amyotrophic lateral sclerosis (also known as ALS or Lou Gehrig's disease) and Huntington's disease were first discovered at MIT. Jerome Lettvin transformed the study of cognitive science with his paper "What the frog's eye tells the frog's brain". Researchers developed a system to convert MRI scans into 3D printed physical models.

In the domain of humanities, arts, and social sciences, MIT economists have been awarded five Nobel Prizes and nine John Bates Clark Medals. Linguists Noam Chomsky and Morris Halle authored seminal texts on generative grammar and phonology. The MIT Media Lab, founded in 1985 within the School of Architecture and Planning and known for its unconventional research, has been home to influential researchers such as constructivist educator and Logo creator Seymour Papert.

Spanning many of the above fields, MacArthur Fellowships (the so-called "Genius Grants") have been awarded to 38 people associated with MIT. Four Pulitzer Prize–winning writers currently work at or have retired from MIT. Four current or former faculty are members of the American Academy of Arts and Letters.

Allegations of research misconduct or improprieties have received substantial press coverage. Professor David Baltimore, a Nobel Laureate, became embroiled in a misconduct investigation starting in 1986 that led to Congressional hearings in 1991. Professor Ted Postol has accused the MIT administration since 2000 of attempting to whitewash potential research misconduct at the Lincoln Lab facility involving a ballistic missile defense test, though a final investigation into the matter has not been completed. Associate Professor Luk Van Parijs was dismissed in 2005 following allegations of scientific misconduct and found guilty of the same by the United States Office of Research Integrity in 2009.



MIT alumni and faculty have founded numerous companies, some of which are shown below.

The faculty and student body place a high value on meritocracy and on technical proficiency. MIT has never awarded an honorary degree, nor does it award athletic scholarships, ad eundem degrees, or Latin honors upon graduation. However, MIT has twice awarded honorary professorships: to Winston Churchill in 1949 and Salman Rushdie in 1993.

Many upperclass students and alumni wear a large, heavy, distinctive class ring known as the "Brass Rat". Originally created in 1929, the ring's official name is the "Standard Technology Ring." The undergraduate ring design (a separate graduate student version exists as well) varies slightly from year to year to reflect the unique character of the MIT experience for that class, but always features a three-piece design, with the MIT seal and the class year each appearing on a separate face, flanking a large rectangular bezel bearing an image of a beaver. The initialism IHTFP, representing the informal school motto "I Hate This Fucking Place" and jocularly euphemized as "I Have Truly Found Paradise," "Institute Has The Finest Professors," "It's Hard to Fondle Penguins," and other variations, has occasionally been featured on the ring given its historical prominence in student culture.

MIT has over 500 recognized student activity groups, including a campus radio station, "The Tech" student newspaper, an annual entrepreneurship competition, and weekly screenings of popular films by the Lecture Series Committee. Less traditional activities include the "world's largest open-shelf collection of science fiction" in English, a model railroad club, and a vibrant folk dance scene. Students, faculty, and staff are involved in over 50 educational outreach and public service programs through the MIT Museum, Edgerton Center, and MIT Public Service Center.

The Independent Activities Period is a four-week-long "term" offering hundreds of optional classes, lectures, demonstrations, and other activities throughout the month of January between the Fall and Spring semesters. Some of the most popular recurring IAP activities are Autonomous Robot Design (course 6.270), Robocraft Programming (6.370), and MasLab competitions, the annual "mystery hunt", and Charm School. More than 250 students pursue externships annually at companies in the US and abroad.

Many MIT students also engage in "hacking", which encompasses both the physical exploration of areas that are generally off-limits (such as rooftops and steam tunnels), as well as elaborate practical jokes. Recent high-profile hacks have included the abduction of Caltech's cannon, reconstructing a Wright Flyer atop the Great Dome, and adorning the John Harvard statue with the Master Chief's Mjölnir Helmet.

MIT sponsors 31 varsity sports and has one of the three broadest NCAA Division III athletic programs.  MIT participates in the NCAA's Division III, the New England Women's and Men's Athletic Conference, the New England Football Conference, NCAA's Division I Eastern Association of Women's Rowing Colleges (EAWRC) for women's crew, and the Collegiate Water Polo Association (CWPA) for Men's Water Polo. Men's crew competes outside the NCAA in the Eastern Association of Rowing Colleges (EARC). In April 2009, budget cuts led to MIT eliminating eight of its 41 sports, including the mixed men's and women's teams in alpine skiing and pistol; separate teams for men and women in ice hockey and gymnastics; and men's programs in golf and wrestling.

MIT enrolled 4,384 undergraduates and 6,510 graduate students in 2011–2012. Women constituted 45 percent of undergraduate students. Undergraduate and graduate students came from all 50 U.S. states as well as from 115 foreign countries.

MIT received 20,247 applications for admission to the undergraduate Class of 2021: it admitted 1,452 (7.1 percent) and enrolled 1,102 (76 percent). 19,446 applications were received for graduate and advanced degree programs across all departments; 2,991 were admitted (15.4 percent) and 1,880 enrolled (62.8 percent).

The interquartile range on the SAT was 2090–2340 and 97 percent of students ranked in the top tenth of their high school graduating class. 97 percent of the Class of 2012 returned as sophomores; 82 percent of the Class of 2007 graduated within 4 years, and 93 percent (91 percent of the men and 95 percent of the women) graduated within 6 years.

Undergraduate tuition and fees total $40,732 per student and annual expenses are estimated at $52,507 . 62 percent of students received need-based financial aid in the form of scholarships and grants from federal, state, institutional, and external sources averaging $38,964 per student. Students were awarded a total of $102 million in scholarships and grants, primarily from institutional support ($84 million). The annual increase in expenses has led to a student tradition (dating back to the 1960s) of tongue-in-cheek "tuition riots".

MIT has been nominally co-educational since admitting Ellen Swallow Richards in 1870. Richards also became the first female member of MIT's faculty, specializing in sanitary chemistry. Female students remained a minority prior to the completion of the first wing of a women's dormitory, McCormick Hall, in 1963. Between 1993 and 2009 the proportion of women rose from 34 percent to 45 percent of undergraduates and from 20 percent to 31 percent of graduate students. Women currently outnumber men in Biology, Brain & Cognitive Sciences, Architecture, Urban Planning and Biological Engineering.

A number of student deaths in the late 1990s and early 2000s resulted in considerable media attention focussing on MIT's culture and student life. After the alcohol-related death of Scott Krueger in September 1997 as a new member at the Phi Gamma Delta fraternity, MIT began requiring all freshmen to live in the dormitory system. The 2000 suicide of MIT undergraduate Elizabeth Shin drew attention to suicides at MIT and created a controversy over whether MIT had an unusually high suicide rate. In late 2001 a task force's recommended improvements in student mental health services were implemented, including expanding staff and operating hours at the mental health center. These and later cases were significant as well because they sought to prove the negligence and liability of university administrators "in loco parentis".

, MIT had 1,030 faculty members, of whom 225 were women. Faculty are responsible for lecturing classes, advising both graduate and undergraduate students, and sitting on academic committees, as well as conducting original research. Between 1964 and 2009, a total of seventeen faculty and staff members affiliated with MIT were awarded Nobel Prizes (thirteen in the last 25 years). MIT faculty members past or present have won a total of twenty-seven Nobel Prizes, the majority in Economics or Physics. , among current faculty and teaching staff there are 67 Guggenheim Fellows, 6 Fulbright Scholars, and 22 MacArthur Fellows. Faculty members who have made extraordinary contributions to their research field as well as the MIT community are granted appointments as Institute Professors for the remainder of their tenures.

A 1998 MIT study concluded that a systemic bias against female faculty existed in its School of Science, although the study's methods were controversial. Since the study, though, women have headed departments within the Schools of Science and of Engineering, and MIT has appointed several female vice presidents, although allegations of sexism continue to be made. Susan Hockfield, a molecular neurobiologist, was MIT's president from 2004 to 2012 and was the first woman to hold the post.

Tenure outcomes have vaulted MIT into the national spotlight on several occasions. The 1984 dismissal of David F. Noble, a historian of technology, became a "cause célèbre" about the extent to which academics are granted freedom of speech after he published several books and papers critical of MIT's and other research universities' reliance upon financial support from corporations and the military. Former materials science professor Gretchen Kalonji sued MIT in 1994 alleging that she was denied tenure because of sexual discrimination. Several years later, the lawsuit was settled with undisclosed payments, and establishment of a project to encourage women and minorities to seek faculty positions. In 1997, the Massachusetts Commission Against Discrimination issued a probable cause finding supporting UMass Boston Professor James Jennings' allegations of racial discrimination after a senior faculty search committee in the Department of Urban Studies and Planning did not offer him reciprocal tenure.

In 2006–2007, MIT's denial of tenure to African-American stem cell scientist professor James Sherley reignited accusations of racism in the tenure process, eventually leading to a protracted public dispute with the administration, a brief hunger strike, and the resignation of Professor Frank L. Douglas in protest. "The Boston Globe" reported on February 6, 2007: "Less than half of MIT's junior faculty members are granted tenure. After Sherley was initially denied tenure, his case was examined three times before the university established that neither racial discrimination nor conflict of interest affected the decision. Twenty-one of Sherley's colleagues later issued a statement saying that the professor was treated fairly in tenure review."

MIT faculty members have often been recruited to lead other colleges and universities. Founding faculty member Charles W. Eliot was recruited in 1869 to become president of Harvard University, a post he would hold for 40 years, during which he wielded considerable influence on both American higher education and secondary education. MIT alumnus and faculty member George Ellery Hale played a central role in the development of the California Institute of Technology (Caltech), and other faculty members have been key founders of Franklin W. Olin College of Engineering in nearby Needham, Massachusetts.

, former provost Robert A. Brown is president of Boston University; former provost Mark Wrighton is chancellor of Washington University in St. Louis; former associate provost Alice Gast is president of Lehigh University; and former professor Suh Nam-pyo is president of KAIST. Former dean of the School of Science Robert J. Birgeneau was the chancellor of the University of California, Berkeley (2004–2013); former professor John Maeda was president of Rhode Island School of Design (RISD, 2008–2013); former professor David Baltimore was president of Caltech (1997–2006); and MIT alumnus and former assistant professor Hans Mark served as chancellor of the University of Texas system (1984–1992).

In addition, faculty members have been recruited to lead governmental agencies; for example, former professor Marcia McNutt is president of the National Academy of Sciences, urban studies professor Xavier de Souza Briggs is currently the associate director of the White House Office of Management and Budget, and biology professor Eric Lander was a co-chair of the President's Council of Advisors on Science and Technology. In 2013, faculty member Ernest Moniz was nominated by President Obama and later confirmed as United States Secretary of Energy. Former professor Hans Mark served as Secretary of the Air Force from 1979 to 1981. Alumna and Institute Professor Sheila Widnall served as Secretary of the Air Force between 1993 and 1997, making her the first female Secretary of the Air Force and first woman to lead an entire branch of the US military in the Department of Defense.

, MIT was the second-largest employer in the city of Cambridge. Based on feedback from employees, MIT was ranked #7 as a place to work, among US colleges and universities . Surveys cited a "smart", "creative", "friendly" environment, noting that the work-life balance tilts towards a "strong work ethic" but complaining about "low pay" compared to an industry position.

Many of MIT's over 120,000 alumni have had considerable success in scientific research, public service, education, and business. , 27 MIT alumni have won the Nobel Prize, 47 have been selected as Rhodes Scholars, and 61 have been selected as Marshall Scholars.

Alumni in American politics and public service include former Chairman of the Federal Reserve Ben Bernanke, former MA-1 Representative John Olver, former CA-13 Representative Pete Stark, former National Economic Council chairman Lawrence H. Summers, and former Council of Economic Advisors chairman Christina Romer. MIT alumni in international politics include Foreign Affairs Minister of Iran Ali Akbar Salehi, Israeli Prime Minister Benjamin Netanyahu, President of Colombia Virgilio Barco Vargas, President of the European Central Bank Mario Draghi, former Governor of the Reserve Bank of India Raghuram Rajan, former British Foreign Minister David Miliband, former Greek Prime Minister Lucas Papademos, former UN Secretary General Kofi Annan, former Iraqi Deputy Prime Minister Ahmed Chalabi, former Minister of Education and Culture of The Republic of Indonesia Yahya Muhaimin, former Jordanian Minister of Education, Higher Education and Scientific Research & former Jordanian Minister of Energy and Mineral Resources Khaled Toukan. Alumni in sports have included Olympic fencing champion Johan Harmenberg.

MIT alumni founded or co-founded many notable companies, such as Intel, McDonnell Douglas, Texas Instruments, 3Com, Qualcomm, Bose, Raytheon, Apotex, Koch Industries, Rockwell International, Genentech, Dropbox, and Campbell Soup. According to the British newspaper, "The Guardian", "a survey of living MIT alumni found that they have formed 25,800 companies, employing more than three million people including about a quarter of the workforce of Silicon Valley. Those firms collectively generate global revenues of about $1.9 trillion (£1.2 trillion) a year. If MIT were a country, it would have the 11th highest GDP of any nation in the world."

Prominent institutions of higher education have been led by MIT alumni, including the University of California system, Harvard University, New York Institute of Technology, Johns Hopkins University, Carnegie Mellon University, Tufts University, Rochester Institute of Technology, Rhode Island School of Design (RISD), New Jersey Institute of Technology, Northeastern University, Lahore University of Management Sciences, Rensselaer Polytechnic Institute, Tecnológico de Monterrey, Purdue University, Virginia Polytechnic Institute, KAIST, and Quaid-e-Azam University. Berklee College of Music, the largest independent college of contemporary music in the world, was founded and led by MIT alumnus Lawrence Berk for more than three decades.

More than one third of the United States' manned spaceflights have included MIT-educated astronauts (among them Apollo 11 Lunar Module Pilot Buzz Aldrin), more than any university excluding the United States service academies. Alumnus and former faculty member Qian Xuesen was instrumental in the PRC rocket program.

Noted alumni in non-scientific fields include author Hugh Lofting, sculptor Daniel Chester French, guitarist Tom Scholz of the band Boston, the British "BBC" and "ITN" correspondent and political advisor David Walter, "The New York Times" columnist and Nobel Prize Winning economist Paul Krugman, "The Bell Curve" author Charles Murray, United States Supreme Court building architect Cass Gilbert, Pritzker Prize-winning architects I.M. Pei and Gordon Bunshaft.




</doc>
<doc id="18880" url="https://en.wikipedia.org/wiki?curid=18880" title="Monopolistic competition">
Monopolistic competition

Monopolistic competition is a type of imperfect competition such that many producers sell products that are differentiated from one another (e.g. by branding or quality) and hence are not perfect substitutes. In monopolistic competition, a firm takes the prices charged by its rivals as given and ignores the impact of its own prices on the prices of other firms. In the presence of coercive government, monopolistic competition will fall into government-granted monopoly. Unlike perfect competition, the firm maintains spare capacity. Models of monopolistic competition are often used to model industries. Textbook examples of industries with market structures similar to monopolistic competition include restaurants, cereal, clothing, shoes, and service industries in large cities. The "founding father" of the theory of monopolistic competition is Edward Hastings Chamberlin, who wrote a pioneering book on the subject, "Theory of Monopolistic Competition" (1933). Joan Robinson published a book "The Economics of Imperfect Competition" with a comparable theme of distinguishing perfect from imperfect competition.

Monopolistically competitive markets have the following characteristics:





The long-run characteristics of a monopolistically competitive market are almost the same as a perfectly competitive market. Two differences between the two are that monopolistic competition produces heterogeneous products and that monopolistic competition involves a great deal of non-price competition, which is based on subtle product differentiation. A firm making profits in the short run will nonetheless only break even in the long run because demand will decrease and average total cost will increase. This means in the long run, a monopolistically competitive firm will make zero economic profit. This illustrates the amount of influence the firm has over the market; because of brand loyalty, it can raise its prices without losing all of its customers. This means that an individual firm's demand curve is downward sloping, in contrast to perfect competition, which has a perfectly elastic demand schedule.

There are six characteristics of monopolistic competition (MC):


MC firms sell products that have real or perceived non-price differences. However, the differences are not so great as to eliminate other goods as substitutes. Technically, the cross price elasticity of demand between goods in such a market is positive. In fact, the XED would be high. MC goods are best described as close but imperfect substitutes. The goods perform the same basic functions but have differences in qualities such as type, style, quality, reputation, appearance, and location that tend to distinguish them from each other. For example, the basic function of motor vehicles is the same—to move people and objects from point to point in reasonable comfort and safety. Yet there are many different types of motor vehicles such as motor scooters, motor cycles, trucks and cars, and many variations even within these categories.

There are many firms in each MC product group and many firms on the side lines prepared to enter the market. A product group is a "collection of similar products". The fact that there are "many firms" gives each MC firm the freedom to set prices without engaging in strategic decision making regarding the prices of other firms and each firm's actions have a negligible impact on the market. For example, a firm could cut prices and increase sales without fear that its actions will prompt retaliatory responses from competitors.

How many firms will an MC market structure support at market equilibrium? The answer depends on factors such as fixed costs, economies of scale and the degree of product differentiation. For example, the higher the fixed costs, the fewer firms the market will support.

Like perfect competition, under monopolistic competition also, the firms can enter or exit freely. The firms will enter when the existing firms are making super-normal profits. With the entry of new firms, the supply would increase which would reduce the price and hence the existing firms will be left only with normal profits. Similarly, if the existing firms are sustaining losses, some of the marginal firms will exit. It will reduce the supply due to which price would rise and the existing firms will be left only with normal profit.

Each MC firm independently sets the terms of exchange for its product. The firm gives no consideration to what effect its decision may have on competitors. The theory is that any action will have such a negligible effect on the overall market demand that an MC firm can act without fear of prompting heightened competition. In other words, each firm feels free to set prices as if it were a monopoly rather than an oligopoly.

MC firms have some degree of market power. Market power means that the firm has control over the terms and conditions of exchange. An MC firm can raise its prices without losing all its customers. The firm can also lower prices without triggering a potentially ruinous price war with competitors. The source of an MC firm's market power is not barriers to entry since they are low. Rather, an MC firm has market power because it has relatively few competitors, those competitors do not engage in strategic decision making and the firms sells differentiated product. Market power also means that an MC firm faces a downward sloping demand curve. The demand curve is highly elastic although not "flat".

No sellers or buyers have complete market information, like market demand or market supply.

There are two sources of inefficiency in the MC market structure. First, at its optimum output the firm charges a price that exceeds marginal costs, The MC firm maximizes profits where marginal revenue = marginal cost. Since the MC firm's demand curve is downward sloping this means that the firm will be charging a price that exceeds marginal costs. The monopoly power possessed by a MC firm means that at its profit maximizing level of production there will be a net loss of consumer (and producer) surplus. The second source of inefficiency is the fact that MC firms operate with excess capacity. That is, the MC firm's profit maximizing output is less than the output associated with minimum average cost. Both a PC and MC firm will operate at a point where demand or price equals average cost. For a PC firm this equilibrium condition occurs where the perfectly elastic demand curve equals minimum average cost. A MC firm’s demand curve is not flat but is downward sloping. Thus in the long run the demand curve will be tangential to the long run average cost curve at a point to the left of its minimum. The result is excess capacity.


Monopolistically competitive firms are inefficient, it is usually the case that the costs of regulating prices for products sold in monopolistic competition exceed the benefits of such regulation. . A monopolistically competitive firm might be said to be marginally inefficient because the firm produces at an output where average total cost is not a minimum. A monopolistically competitive market is productively inefficient market structure because marginal cost is less than price in the long run. Monopolistically competitive markets are also allocatively inefficient, as the price given is higher than Marginal cost. Product differentiation increases total utility by better meeting people's wants than homogenous products in a perfectly competitive market. 

Another concern is that monopolistic competition fosters advertising and the creation of brand names. Advertising induces customers into spending more on products because of the name associated with them rather than because of rational factors. Defenders of advertising dispute this, arguing that brand names can represent a guarantee of quality and that advertising helps reduce the cost to consumers of weighing the tradeoffs of numerous competing brands. There are unique information and information processing costs associated with selecting a brand in a monopolistically competitive environment. In a monopoly market, the consumer is faced with a single brand, making information gathering relatively inexpensive. In a perfectly competitive industry, the consumer is faced with many brands, but because the brands are virtually identical information gathering is also relatively inexpensive. In a monopolistically competitive market, the consumer must collect and process information on a large number of different brands to be able to select the best of them. In many cases, the cost of gathering information necessary to selecting the best brand can exceed the benefit of consuming the best brand instead of a randomly selected brand. The result is that the consumer is confused. Some brands gain prestige value and can extract an additional price for that.

Evidence suggests that consumers use information obtained from advertising not only to assess the single brand advertised, but also to infer the possible existence of brands that the consumer has, heretofore, not observed, as well as to infer consumer satisfaction with brands similar to the advertised brand.

In many markets, such as toothpaste, soap, air conditioning, smartphones and toilet paper, producers practice product differentiation by altering the physical composition of products, using special packaging, or simply claiming to have superior products based on brand images or advertising.



</doc>
<doc id="18881" url="https://en.wikipedia.org/wiki?curid=18881" title="Mathematical induction">
Mathematical induction

Mathematical induction is a mathematical proof technique. It is essentially used to prove that a property "P"("n") holds for every natural number "n", i.e. for "n" = 0, 1, 2, 3, and so on. Metaphors can be informally used to understand the concept of mathematical induction, such as the metaphor of falling dominoes or climbing a ladder:
The method of induction requires two cases to be proved. The first case, called the base case (or, sometimes, the basis), proves that the property holds for the number 0. The second case, called the induction step, proves that, if the property holds for one natural number "n", then it holds for the next natural number "n" + 1. These two steps establish the property "P"("n") for every natural number "n" = 0, 1, 2, 3, ... The base step need not begin with zero. Often it begins with the number one, and it can begin with any natural number, establishing the truth of the property for all natural numbers greater than or equal to the starting number.

The method can be extended to prove statements about more general well-founded structures, such as trees; this generalization, known as structural induction, is used in mathematical logic and computer science. Mathematical induction in this extended sense is closely related to recursion. Mathematical induction, in some form, is the foundation of all correctness proofs for computer programs.

Although its name may suggest otherwise, mathematical induction should not be misconstrued as a form of inductive reasoning as used in philosophy (also see Problem of induction). Mathematical induction is an inference rule used in formal proofs. Proofs by mathematical induction are, in fact, examples of deductive reasoning.

In 370 BC, Plato's Parmenides may have contained an early example of an implicit inductive proof. The earliest implicit traces of mathematical induction may be found in Euclid's proof that the number of primes is infinite and in Bhaskara's "cyclic method". An opposite iterated technique, counting "down" rather than up, is found in the Sorites paradox, where it was argued that if 1,000,000 grains of sand formed a heap, and removing one grain from a heap left it a heap, then a single grain of sand (or even no grains) forms a heap.

An implicit proof by mathematical induction for arithmetic sequences was introduced in the al-Fakhri written by al-Karaji around 1000 AD, who used it to prove the binomial theorem and properties of Pascal's triangle.

None of these ancient mathematicians, however, explicitly stated the induction hypothesis. Another similar case (contrary to what Vacca has written, as Freudenthal carefully showed) was that of Francesco Maurolico in his "Arithmeticorum libri duo" (1575), who used the technique to prove that the sum of the first "n" odd integers is "n". The first explicit formulation of the principle of induction was given by Pascal in his "Traité du triangle arithmétique" (1665). Another Frenchman, Fermat, made ample use of a related principle, indirect proof by infinite descent. The induction hypothesis was also employed by the Swiss Jakob Bernoulli, and from then on it became more or less well known. The modern rigorous and systematic treatment of the principle came only in the 19th century, with George Boole, Augustus de Morgan, Charles Sanders Peirce, Giuseppe Peano, and Richard Dedekind.

The simplest and most common form of mathematical induction infers that a statement involving a natural number "n" holds for all values of "n". The proof consists of two steps:

The hypothesis in the inductive step, that the statement holds for some "n", is called the induction hypothesis or inductive hypothesis. To prove the inductive step, one assumes the induction hypothesis and then uses this assumption, involving "n", to prove the statement for "n" + 1.

Whether "n" = 0 or "n" = 1 is taken as the standard base case depends on the preferred definition of the natural numbers. In the fields of combinatorics and mathematical logic it is common to consider 0 as a natural number.

Mathematical induction can be used to prove that the following statement, "P"("n"), holds for all natural numbers "n".

"P"("n") gives a formula for the sum of the natural numbers less than or equal to number "n". The proof that "P"("n") is true for each natural number "n" proceeds as follows.

Base case: Show that the statement holds for "n" = 0. <br>
"P"(0) is easily seen to be true:
Inductive step: Show that "if" "P"("k") holds, then also holds. This can be done as follows.

Assume "P"("k") holds (for some unspecified value of "k"). It must then be shown that holds, that is:
Using the induction hypothesis that "P"("k") holds, the left-hand side can be rewritten to:

Algebraically:

thereby showing that indeed holds.

Since both the base case and the inductive step have been performed, by mathematical induction the statement "P"("n") holds for all natural numbers "n". Q.E.D.

In practice, proofs by induction are often structured differently, depending on the exact nature of the property to be proved.n

If one wishes to prove a statement not for all natural numbers but only for all numbers "n" greater than or equal to a certain number "b", then the proof by induction consists of:
This can be used, for example, to show that for "n" ≥ 3.

In this way, one can prove that some statement "P"("n") holds for all "n" ≥ 1, or even "n" ≥ −5. This form of mathematical induction is actually a special case of the previous form, because if the statement to be proved is "P"("n") then proving it with these two rules is equivalent with proving "P"("n" + "b") for all natural numbers "n" with an induction base case 0.

Assume an infinite supply of 4- and 5-dollar coins. Induction can be used to prove that any whole amount of dollars greater than formula_6 can be formed by a combination of such coins. The amount formula_7 is chosen to begin on formula_6 as the statement does not hold true for every lower number; in particular, it is violated for formula_9.

In more precise terms, we wish to show that for any amount formula_10 there exist natural numbers formula_11 such that formula_12, where 0 is included as a natural number. The statement to be shown is thus:

Base case: Showing that formula_14 holds for formula_15 is trivial: let formula_16 and formula_17. Then, formula_18.

Step case: Given that formula_14 holds for some value of formula_20 ("induction hypothesis"), prove that formula_21 holds, too. That is, given that formula_22 for some natural numbers formula_11, prove that there exist natural numbers formula_24 such that formula_25.

Here we need to consider two cases.

For the first case, assume that formula_26. By some algebraic manipulation and by assumption, we see that in that case

where formula_28 and formula_29 are natural numbers.

This shows that to add formula_30 to the total amount—any amount whatsoever, so long as it is greater than formula_6—it is sufficient to remove a single 4-dollar coin while adding a 5-dollar coin. However, this construction fails in the case that formula_32, or in words, when there is no 4-dollar coin. 

So it remains to prove the case formula_32. Then formula_34, which implies that formula_35.

where formula_37 and formula_38 are again natural numbers.

The above calculation shows that in the case there are no 4-dollar coins, we can add formula_30 to the amount by removing three 5-dollar coins while adding four 4-dollar coins.

Thus, with the inductive step, we have shown that formula_14 implies formula_21 for all natural numbers formula_20, and the proof is complete. Q.E.D.

It is sometimes desirable to prove a statement involving two natural numbers, "n" and "m", by iterating the induction process. That is, one proves a base case and an inductive step for "n", and in each of those proves a base case and an inductive step for "m". See, for example, the proof of commutativity accompanying "addition of natural numbers". More complicated arguments involving three or more counters are also possible.

The method of infinite descent is a variation of mathematical induction which was used by Pierre de Fermat. It is used to show that some statement "Q"("n") is false for all natural numbers "n". Its traditional form consists of showing that if "Q"("n") is true for some natural number "n", it also holds for some strictly smaller natural number "m". Because there are no infinite decreasing sequences of natural numbers, this situation would be impossible, showing by contradiction that "Q"("n") cannot be true for any "n". 

The validity of this method can be verified from the usual principle of mathematical induction. Using mathematical induction on the statement "P"("n") defined as ""Q"("m") is false for all natural numbers "m" less than or equal to "n"", it follows that "P"("n") holds for all "n", which means that "Q"("n") is false for every natural number "n".

The most common form of proof by mathematical induction requires proving in the inductive step that

whereupon the induction principle "automates" "n" applications of this step in getting from "P"(0) to "P"("n"). This could be called "predecessor induction" because each step proves something about a number from something about that number's predecessor.

A variant of interest in computational complexity is "prefix induction", in which one needs to prove

or equivalently

The induction principle then "automates" log "n" applications of this inference in getting from "P"(0) to "P"("n"). (It is called "prefix induction" because each step proves something about a number from something about the "prefix" of that number formed by truncating the low bit of its binary representation. It can be viewed as an application of traditional induction on the length of that binary representation.)

If traditional predecessor induction is interpreted computationally as an "n"-step loop, prefix induction corresponds to a log "n"-step loop, and thus proofs using prefix induction are "more feasibly constructive" than proofs using predecessor induction.

Predecessor induction can trivially simulate prefix induction on the same statement. Prefix induction can simulate predecessor induction, but only at the cost of making the statement more syntactically complex (adding a bounded universal quantifier), so the interesting results relating prefix induction to polynomial-time computation depend on excluding unbounded quantifiers entirely, and limiting the alternation of bounded universal and existential quantifiers allowed in the statement.

One can take the idea a step further: one must prove

whereupon the induction principle "automates" log log "n" applications of this inference in getting from "P"(0) to "P"("n"). This form of induction has been used, analogously, to study log-time parallel computation.

Another variant, called complete induction, course of values induction or strong induction (in contrast to which the basic form of induction is sometimes known as weak induction) makes the inductive step easier to prove by using a stronger hypothesis: one proves the statement under the assumption that "P"("n") holds for all natural "n" less than ; by contrast, the basic form only assumes "P"("m"). The name "strong induction" does not mean that this method can prove more than "weak induction", but merely refers to the stronger hypothesis used in the inductive step; in fact the two methods are equivalent, as explained below. In this form of complete induction one still has to prove the base case, "P"(0), and it may even be necessary to prove extra base cases such as "P"(1) before the general argument applies, as in the example below of the Fibonacci number "F".

Although the form just described requires one to prove the base case, this is unnecessary if one can prove "P"("m") (assuming "P"("n") for all lower "n") for all . This is a special case of transfinite induction as described below. In this form the base case is subsumed by the case , where "P"(0) is proved with no other "P"("n") assumed;
this case may need to be handled separately, but sometimes the same argument applies for "m" = 0 and , making the proof simpler and more elegant.
In this method it is, however, vital to ensure that the proof of "P"("m") does not implicitly assume that , e.g. by saying "choose an arbitrary " or assuming that a set of "m" elements has an element.

Complete induction is equivalent to ordinary mathematical induction as described above, in the sense that a proof by one method can be transformed into a proof by the other. Suppose there is a proof of "P"("n") by complete induction. Let Q("n") mean ""P"("m") holds for all "m" such that ". Then Q("n") holds for all "n" if and only if P("n") holds for all "n", and our proof of "P"("n") is easily transformed into a proof of Q("n") by (ordinary) induction. If, on the other hand, "P"("n") had been proven by ordinary induction, the proof would already effectively be one by complete induction: "P"(0) is proved in the base case, using no assumptions, and is proved in the inductive step, in which one may assume all earlier cases but need only use the case "P"("n").

Complete induction is most useful when several instances of the inductive hypothesis are required for each inductive step. For example, complete induction can be used to show that
where "F" is the "n"th Fibonacci number, (the golden ratio) and are the roots of the polynomial . By using the fact that for each , the identity above can be verified by direct calculation for if one assumes that it already holds for both "F" and "F". To complete the proof, the identity must be verified in the two base cases and .

Another proof by complete induction uses the hypothesis that the statement holds for "all" smaller "n" more thoroughly. Consider the statement that "every natural number greater than 1 is a product of (one or more) prime numbers", which is the "existence" part of the fundamental theorem of arithmetic. For proving the inductive step, the induction hyposthesis is that for a given the statement holds for all smaller . If "m" is prime then it is certainly a product of primes, and if not, then by definition it is a product: , where neither of the factors is equal to 1; hence neither is equal to "m", and so both are smaller than "m". The induction hypothesis now applies to "n" and "n", so each one is a product of primes. Thus "m" is a product of products of primes; therefore itself a product of primes.

We shall look to prove the same example as , this time with a variant called "strong induction". The statement remains the same:

formula_48

However, there will be slight differences with the structure and assumptions of the proof. Let us begin with the base case.

Base case: Show that formula_14 holds for formula_50.

The base case holds.

Induction hypothesis: Given some formula_52 such that formula_53 holds for all formula_54 with formula_55.

Inductive step: Prove that formula_56 holds.

Choosing formula_57, and observing that formula_58 shows that formula_59 holds, by inductive hypothesis. That is, the sum formula_60 can be formed by some combination of formula_61 and formula_62 dollar coins. Then, simply adding a formula_61 dollar coin to that combination yields the sum formula_64. That is, formula_56 holds. Q.E.D.

The inductive step must be proved for all values of "n". To illustrate this, Joel E. Cohen proposed the following argument, which purports to prove by mathematical induction that all horses are of the same color:

The base case "n" = 1 is trivial (as any horse is the same color as itself), and the inductive step is correct in all cases "n" > 1. However, the logic of the inductive step is incorrect for "n" = 1, because the statement that "the two sets overlap" is false (there are only "n" + 1 = 2 horses prior to either removal, and after removal the sets of one horse each do not overlap).

In second-order logic, we can write down the "axiom of induction" as follows:
where "P"(.) is a variable for predicates involving one natural number and "k" and "n" are variables for natural numbers.

In words, the base case "P"(0) and the inductive step (namely, that the induction hypothesis "P"("k") implies "P"("k" + 1)) together imply that "P"("n") for any natural number "n". The axiom of induction asserts that the validity of inferring that "P"("n") holds for any natural number "n" from the base case and the inductive step.

Note that the first quantifier in the axiom ranges over "predicates" rather than over individual numbers. This is a second-order quantifier, which means that this axiom is stated in second-order logic. Axiomatizing arithmetic induction in first-order logic requires an axiom schema containing a separate axiom for each possible predicate. The article Peano axioms contains further discussion of this issue.

The axiom of structural induction for the natural numbers was first formulated by Peano, who used it to specify the natural numbers together with four other axioms saying that (1) 0 is a natural number, (2) the successor function s of every natural number yields a natural number (s(x)=x+1), (3) the successor function is injective, and (4) 0 is not in the range of s.

In first-order ZFC set theory, quantification over predicates is not allowed, but we can still phrase induction by quantification over sets:
formula_68 may be read as a set representing a proposition, and containing natural numbers, for which the proposition holds. This is not an axiom, but a theorem, given that natural numbers are defined in the language of ZFC set theory by axioms, analogous to Peano's.

The principle of complete induction is not only valid for statements about natural numbers, but for statements about elements of any well-founded set, that is, a set with an irreflexive relation < that contains no infinite descending chains. Any set of cardinal numbers is well-founded, which includes the set of natural numbers.

Applied to a well-founded set, it can be formulated as a single step:

This form of induction, when applied to a set of ordinals (which form a well-ordered and hence well-founded class), is called "transfinite induction". It is an important proof technique in set theory, topology and other fields.

Proofs by transfinite induction typically distinguish three cases:

Strictly speaking, it is not necessary in transfinite induction to prove a base case, because it is a vacuous special case of the proposition that if "P" is true of all , then "P" is true of "m". It is vacuously true precisely because there are no values of that could serve as counterexamples. So the special cases are special cases of the general case.

The principle of mathematical induction is usually stated as an axiom of the natural numbers; see Peano axioms. However, it can be proved from the well-ordering principle. Indeed, suppose the following:

To derive simple induction from these axioms, one must show that if "P"("n") is some proposition predicated of "n" for which:
then "P"("n") holds for all "n".

"Proof." Let "S" be the set of all natural numbers for which "P"("m") is false. Let us see what happens if one asserts that "S" is nonempty. Well-ordering tells us that "S" has a least element, say "n". Moreover, since "P"(0) is true, "n" is not 0. Since every natural number is either 0 or some , there is some natural number "m" such that . Now "m" is less than "n", and "n" is the least element of "S". It follows that "m" is not in "S", and so "P"("m") is true. This means that ) is true; in other words, "P"("n") is true. This is a contradiction, since "n" was in "S". Therefore, "S" is empty.

It can also be proved that induction, given the other axioms, implies the well-ordering principle.

"Proof." Suppose there exists a non-empty set, "S", of naturals that has no least element. Let "P"("n") be the assertion that "n" is not in "S". Then "P"(0) is true, for if it were false then 0 is the least element of "S". Furthermore, suppose "P"(1), "P"(2)..., "P"("n") are all true. Then if "P"("n"+1) is false "n"+1 is in "S", thus being a minimal element in "S", a contradiction. Thus "P"("n"+1) is true. Therefore, by the induction axiom, "P"("n") holds for all 'n", so "S" is empty, a contradiction.





</doc>
<doc id="18884" url="https://en.wikipedia.org/wiki?curid=18884" title="Matrix">
Matrix

Matrix or MATRIX may refer to:













</doc>
