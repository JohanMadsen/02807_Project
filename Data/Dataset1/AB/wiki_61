<doc id="19960" url="https://en.wikipedia.org/wiki?curid=19960" title="Mark Antony">
Mark Antony

Marcus Antonius (Latin: ; 14 January 1 August 30 BC), commonly known in English as Mark Antony or Marc Antony, was a Roman politician and general who played a critical role in the transformation of the Roman Republic from an oligarchy into the autocratic Roman Empire.

Antony was a supporter of Julius Caesar, and served as one of his generals during the conquest of Gaul and the Civil War. Antony was appointed administrator of Italy while Caesar eliminated political opponents in Greece, North Africa, and Spain. After Caesar's death in 44 BC, Antony joined forces with Marcus Aemilius Lepidus, another of Caesar's generals, and Octavian, Caesar's great-nephew and adopted son, forming a three-man dictatorship known to historians as the Second Triumvirate. The Triumvirs defeated Caesar's murderers, the Liberatores, at the Battle of Philippi in 42 BC, and divided the government of the Republic between themselves. Antony was assigned Rome's eastern provinces, including the client kingdom of Egypt, then ruled by Cleopatra VII Philopator, and was given the command in Rome's war against Parthia.

Relations among the triumvirs were strained as the various members sought greater political power. Civil war between Antony and Octavian was averted in 40 BC, when Antony married Octavian's sister, Octavia. Despite this marriage, Antony carried on a love affair with Cleopatra, who bore him three children, further straining Antony's relations with Octavian. Lepidus was expelled from the association in 36 BC, and in 33 BC disagreements between Antony and Octavian caused a split between the remaining Triumvirs. Their ongoing hostility erupted into civil war in 31 BC, as the Roman Senate, at Octavian's direction, declared war on Cleopatra and proclaimed Antony a traitor. Later that year, Antony was defeated by Octavian's forces at the Battle of Actium. Antony and Cleopatra fled to Egypt, where they committed suicide.

With Antony dead, Octavian became the undisputed master of the Roman world. In 27 BC, Octavian was granted the title of "Augustus," marking the final stage in the transformation of the Roman Republic into an empire, with himself as the first Roman emperor.

A member of the plebeian Antonia gens, Antony was born in Rome on 14 January 83 BC. His father and namesake was Marcus Antonius Creticus, son of the noted orator by the same name who had been murdered during the Marian Terror of the winter of 87–86 BC. His mother was Julia Antonia, a distant cousin of Julius Caesar. Antony was an infant at the time of Lucius Cornelius Sulla's march on Rome in 82 BC.

According to the Roman orator Marcus Tullius Cicero, Antony's father was incompetent and corrupt, and was only given power because he was incapable of using or abusing it effectively. In 74 BC he was given military command to defeat the pirates of the Mediterranean, but he died in Crete in 71 BC without making any significant progress. The elder Antony's death left Antony and his brothers, Lucius and Gaius, in the care of their mother, Julia, who later married Publius Cornelius Lentulus Sura, an eminent member of the old Patrician nobility. Lentulus, despite exploiting his political success for financial gain, was constantly in debt due to the extravagance of his lifestyle. He was a major figure in the Second Catilinarian Conspiracy and was summarily executed on the orders of the Consul Cicero in 63 BC for his involvement. His death resulted in a feud between the Antonia and the famous orator.

Antony's early life was characterized by a lack of proper parental guidance. According to the historian Plutarch, he spent his teenage years wandering through Rome with his brothers and friends gambling, drinking, and becoming involved in scandalous love affairs. Antony's contemporary and enemy, Cicero, claimed he had a homosexual relationship with Gaius Scribonius Curio. There is little reliable information on his political activity as a young man, although it is known that he was an associate of Publius Clodius Pulcher and his street gang. He may also have been involved in the Lupercal cult as he was referred to as a priest of this order later in life. By age twenty, Antony had amassed an enormous debt. Hoping to escape his creditors, Antony fled to Greece in 58 BC, where he studied philosophy and rhetoric at Athens.

In 57 BC, Antony joined the military staff of Aulus Gabinius, the Proconsul of Syria, as chief of the cavalry. This appointment marks the beginning of his military career. As Consul the previous year, Gabinius had consented to the exile of Cicero by Antony's mentor, Publius Clodius Pulcher.

Hyrcanus II, the Roman-supported Hasmonean High Priest of Judea, fled Jerusalem to Gabinius to seek protection against his rival and son-in-law Alexander. Years earlier in 63 BC, the Roman general Pompey had captured him and his father, King Aristobulus II, during his war against the remnant of the Seleucid Empire. Pompey had deposed Aristobulus and installed Hyrcanus as Rome's client ruler over Judea. Antony achieved his first military distinctions after securing important victories at Alexandrium and Machaerus. With the rebellion defeated by 56 BC, Gabinius restored Hyrcanus to his position as High Priest in Judea.

The following year, in 55 BC, Gabinius intervened in the political affairs of Ptolemaic Egypt. Pharaoh Ptolemy XII Auletes had been deposed in a rebellion led by his daughter Berenice IV in 58 BC, forcing him to seek asylum in Rome. During Pompey's conquests years earlier, Ptolemy had received the support of Pompey, who named him an ally of Rome. Gabinius' invasion sought to restore Ptolemy to his throne. This was done against the orders of the Senate but with the approval of Pompey, then Rome's leading politician, and only after the deposed king provided a 10,000 talent bribe. The Greek historian Plutarch records it was Antony who convinced Gabinius to finally act. After defeating the frontier forces of the Egyptian kingdom, Gabinius's army proceeded to attack the palace guards but they surrendered before a battle commenced. With Ptolemy XII restored as Rome's client king, Gabinius garrisoned two thousand Roman soldiers, later known as the "Gabiniani", in Alexandria to ensure Ptolemy's authority. In return for its support, Rome exercised considerable power over the kingdom's affairs, particularly control of the kingdom's revenues and crop yields.

During the campaign in Egypt, Antony first met Cleopatra, the 14-year-old daughter of Ptolemy XII.

While Antony was serving Gabinius in the East, the domestic political situation had changed in Rome. In 60 BC, a secret agreement (known as the "First Triumvirate") was entered into between three men to control the Republic: Marcus Licinius Crassus, Gnaeus Pompey Magnus, and Gaius Julius Caesar. Crassus, Rome's wealthiest man, had defeated the slave rebellion of Spartacus in 70 BC; Pompey conquered much of the Eastern Mediterranean in the 60's BC; Caesar was Rome's Pontifex Maximus and a former general in Spain. In 59 BC, Caesar, with funding from Crassus, was elected Consul to pursue legislation favorable to Crassus and Pompey's interests. In return, Caesar was assigned the governorship of Illyricum, Cisalpine Gaul, and Transalpine Gaul for five years beginning in 58 BC. Caesar used his governorship as a launching point for his conquest of free Gaul. In 55 BC, Crassus and Pompey served as Consuls while Caesar's command was extended for another five years. Rome was effectively under the absolute power of these three men. The Triumvirate used the demagogue Publius Clodius Pulcher, Antony's patron, to exile their political rivals, notably Cicero and Cato the Younger.

During his early military service, Antony married his cousin Antonia Hybrida Minor, the daughter of Gaius Antonius Hybrida. Sometime between 54 and 47 BC, the union produced a single daughter, Antonia Prima. It is unclear if this was Antony's first marriage.

Antony's association with Publius Clodius Pulcher allowed him to achieve greater prominence. Clodius, through the influence of his benefactor Marcus Licinius Crassus, had developed a positive political relationship with Julius Caesar. Clodius secured Antony a position on Caesar's military staff in 54 BC, joining his conquest of Gaul. Serving under Caesar, Antony demonstrated excellent military leadership. Despite a temporary alienation later in life, Antony and Caesar developed friendly relations which would continue until Caesar's assassination in 44 BC. Caesar's influence secured greater political advancement for Antony. After a year of service in Gaul, Caesar dispatched Antony to Rome to formally begin his political career, receiving election as Quaestor for 52 BC as a member of the Populares faction. Assigned to assist Caesar, Antony returned to Gaul and commanded Caesar's cavalry during his victory at the Battle of Alesia against the Gallic High King Vercingetorix. Following his year in office, Antony was promoted by Caesar to the rank of Legate and assigned command of two legions (approximately 7,500 total soldiers).

During this time, the alliance among Caesar, Pompey, and Crassus had effectively ended. Caesar's daughter Julia, who had married Pompey to secure the alliance, died in 54 BC while Crassus was killed at the Battle of Carrhae in 53 BC. Without the stability they provided, the divide between Caesar and Pompey grew ever larger. Caesar's glory in conquering Gaul had served to further strain his alliance with Pompey, who, having grown jealous of his former ally, had drifted away from Caesar's democratic Populares party towards the oligarchic Optimates faction led by Cato. The supporters of Caesar, led by Clodius, and the supporters of Pompey, led by Titus Annius Milo, routinely clashed. In 52 BC, Milo succeeded in assassinating Clodius, resulting in widespread riots and the burning of the Senate meeting house, the Curia Hostilia, by Clodius' street gang. Anarchy resulted, causing the Senate to look to Pompey. Fearing the persecutions of Lucius Cornelius Sulla only thirty-years earlier, they avoided granting Pompey the dictatorship by instead naming him sole Consul for the year, giving him extraordinary but limited powers. Pompey ordered armed soldiers into the city to restore order and to eliminate the remnants of Clodius' gang.

Antony remained on Caesar's military staff until 50 BC, helping mopping-up actions across Gaul to secure Caesar's conquest. With the war over, Antony was sent back to Rome to act as Caesar's protector against Pompey and the other Optimates. With the support of Caesar, who as Pontifex Maximus was head of the Roman religion, Antony was appointed the College of Augurs, an important priestly office responsible for interpreting the will of the Roman gods by studying the flight of birds. All public actions required favorable auspices, granting the college considerable influence. Antony was then elected as one of the ten People's Tribunes for 49 BC. In this position, Antony could protect Caesar from his political enemies by vetoing any actions unfavorable to his patron.

The feud between Caesar and Pompey erupted into open confrontation by early 49 BC. The Consuls for the year, Gaius Claudius Marcellus Maior and Lucius Cornelius Lentulus Crus, were firm Optimates opposed to Caesar. Pompey, though remaining in Rome, was then serving as the governor of Spain and commanded several legions. Upon assuming office in January, Antony immediately summoned a meeting of the Senate to resolve the conflict: he proposed both Caesar and Pompey lay down their commands and return to the status of mere private citizens. His proposal was well received by most of the senators but the Consuls and Cato vehemently opposed it. Antony then made a new proposal: Caesar would retain only two of his eight legions and the governorship of Illyrium if he was allowed to stand for the Consulship "in absentia". This arrangement ensured his immunity from suit would continue: he had needed the Consulship to protect himself from prosecution by Pompey. Though Pompey found the concession satisfactory, Cato and Lentulus refused to back down, with Lentulus even expelling Antony from the Senate meeting by force. Antony fled Rome, fearing for his life, and returned to Caesar's camp on the banks of the Rubicon River, the southern limit of Caesar's lawful command.

Within days of Antony's expulsion, on 7 January 49 BC, the Senate reconvened. Under the leadership of Cato and with the tacit support of Pompey, the Senate passed the "final decree" ("senatus consultum ultimum") stripping Caesar of his command and ordering him to return to Rome and stand trial for war crimes. The Senate further declared Caesar a traitor and a public enemy if he did not immediately disband his army. With all hopes of finding a peaceful solution gone after Antony's expulsion, Caesar used Antony as a pretext for marching on Rome. As Tribune, Antony's person was sacrosanct and therefore it was unlawful to harm him or refuse to recognize his veto. Three days later, on 10 January, Caesar crossed the Rubicon River, starting a civil war. During the southern march, Caesar placed Antony as his second in command.

Caesar's rapid advance surprised Pompey, who, along with the other chief members of the Optimates, fled Italy for Greece. After entering Rome, instead of pursuing Pompey, Caesar marched to Spain to defeat Pompeian-loyalists there. Meanwhile, Antony, with the rank of Propraetor despite never having served as Praetor, was installed as governor of Italy and commander of the army, stationed there while Marcus Lepidus, one of Caesar's staff officers, ran the provisional administration of Rome itself. Though Antony was well liked by his soldiers, most other citizens despised him for his lack of interest in the hardships they faced from the civil war.

By the end of the year 49 BC, Caesar, already the ruler of Gaul, had captured Italy, Spain, Sicily, and Sardinia out of Optimates control. In early 48 BC, he prepared to sail with seven legions to Greece to face Pompey. Caesar had entrusted the defense of Illyricum to Gaius Antonius, Antony's younger brother, and Publius Cornelius Dolabella. Pompey's forces, however, defeated them and assumed control of the Adriatic Sea along with it. Additionally, the two legions they commanded defected to Pompey. Without their fleet, Caesar lacked the necessary transport ships to cross into Greece with his seven legions. Instead, he sailed with only two and placed Antony in command of the remaining five at Brundisium with instructions to join him as soon as he was able. In early 48 BC, Lucius Scribonius Libo was given command of Pompey's fleet, comprising some fifty galleys. Moving off to Brundisium, he blockaded Antony. Antony, however, managed to trick Libo into pursuing some decoy ships, causing Libo's squadron to be trapped and attacked. Most of Libo's fleet managed to escape, but several of his troops were trapped and captured. With Libo gone, Antony joined Caesar in Greece by March 48 BC.
During the Greek campaign, Plutarch records Antony was Caesar's top general and second to only him in reputation. Antony joined Caesar at the western Balkan Peninsula and besieged Pompey's larger army at Dyrrhachium. With food sources running low, Caesar, in July, ordered a nocturnal assault on Pompey's camp, but Pompey's larger forces pushed back the assault. Though an indecisive result, the victory was a tactical win for Pompey. Pompey, however, did not order a counter-assault on Caesar's camp, allowing Caesar to retreat unhindered. Caesar would later remark the civil war would have ended that day if Pompey had only attacked him. Caesar managed to retreat to Thessaly, with Pompey in pursuit.

Assuming a defensive position at the plain of Pharsalus, Caesar's army prepared for pitched battle with Pompey's, which outnumbered his own two to one. At the Battle of Pharsalus on 9 August 48 BC, Caesar commanded the right wing opposite Pompey while Antony commanded the left, indicating Antony's status as Caesar's top general. The resulting battle was a decisive victory for Caesar. Though the civil war had not ended at Pharsulus, the battle marked the pinnacle of Caesar's power and effectively ended the Republic. The battle gave Caesar a much needed boost in legitimacy, as prior to the battle much of the Roman world outside Italy supported Pompey and the Optimates as the legitimate government of Rome. After Pompey's defeat, most of the Senate defected to Caesar, including many of the soldiers who had fought under Pompey. Pompey himself fled to Ptolemaic Egypt, but Pharaoh Ptolemy XIII Theos Philopator feared retribution from Caesar and had Pompey assassinated upon his arrival.

Instead of immediately pursuing Pompey and the remaining Optimates, Caesar returned to Rome and was appointed Dictator with Antony as his Master of the Horse and second in command. Caesar presided over his own election to a second Consulship for 47 BC and then, after eleven days in office, resigned this dictatorship. Caesar then sailed to Egypt, where he deposed Ptolemy XIII in favor of his sister Cleopatra in 47 BC. The young Cleopatra became Caesar's mistress and bore him a son, Caesarion. Caesar's actions further strengthened Roman control over the already Roman-dominated kingdom.

While Caesar was away in Egypt, Antony remained in Rome to govern Italy and restore order. Without Caesar to guide him, however, Antony quickly faced political difficulties and proved himself unpopular. The chief cause of his political challenges concerned debt forgiveness. One of the Tribunes for 47 BC, Publius Cornelius Dolabella, a former general under Pompey, proposed a law which would have canceled all outstanding debts. Antony opposed the law for political and personal reasons: he believed Caesar would not support such massive relief and suspected Dolabella had seduced his wife Antonia Hybrida Minor. When Dolabella sought to enact the law by force and seized the Roman Forum,
Antony responded by unleashing his soldiers upon the assembled mass. The resulting instability, especially among Caesar's veterans who would have benefited from the law, forced Caesar to return to Italy by October 47 BC.

Antony's handling of the affair with Dolabella caused a cooling of his relationship with Caesar. Antony's violent reaction had caused Rome to fall into a state of anarchy. Caesar sought to mend relations with the populist leader; he was elected to a third term as Consul for 46 BC, but proposed the Senate should transfer the consulship to Dolabella. When Antony protested, Caesar was forced to withdraw the motion out of shame. Later, Caesar sought to exercise his prerogatives as Dictator and directly proclaim Dolabella as Consul instead. Antony again protested and, in his capacity as an Augur, declared the omens were unfavorable and Caesar again backed down. Seeing the expediency of removing Dolabella from Rome, Caesar ultimately pardoned him for his role in the riots and took him as one of his generals in his campaigns against the remaining Optimates resistance. Antony, however, was stripped of all official positions and received no appointments for the year 46 BC or 45 BC. Instead of Antony, Caesar appointed Marcus Aemilius Lepidus to be his Consular colleague for 46 BC. While Caesar campaigned in North Africa, Antony remained in Rome as a mere private citizen. After returning victorious from North Africa, Caesar was appointed Dictator for ten years and brought Cleopatra and their son to Rome. Antony again remained in Rome while Caesar, in 45 BC, sailed to Spain to defeat the final opposition to his rule. When Caesar returned in late 45 BC, the civil war was over.

During this time Antony married his third wife, Fulvia. Following the scandal with Dolabella, Antony had divorced his second wife and quickly married Fulvia. Fulvia had previously been married to both Publius Clodius Pulcher and Gaius Scribonius Curio, having been a widow since Curio's assassination in 52 BC. Though Antony and Fulvia were formally married in 47 BC, Cicero suggests the two had been in a relationship since at least 58 BC. The union produced two children: Marcus Antonius Antyllus (born 47) and Iullus Antonius (born 45)

Whatever conflicts existed between himself and Caesar, Antony remained faithful to Caesar, ensuring their estrangement did not last long. Antony reunited with Caesar at Narbo in 45 BC with full reconciliation coming in 44 BC when Antony was elected Consul alongside Caesar. Caesar planned a new invasion of Parthia and desired to leave Antony in Italy to govern Rome in his name. The reconciliation came soon after Antony rejected an offer by Gaius Trebonius, one of Caesar's generals, to join a conspiracy to assassinate Caesar.

Soon after they assumed office together, the Lupercalia festival was held on 15 February 44 BC. The festival was held in honor of Lupa, the she-wolf who suckled the infant orphans Romulus and Remus, the founders of Rome. The political atmosphere of Rome at the time of the festival was deeply divided. Caesar had enacted a number of constitutional reforms which centralized effectively all political powers within his own hands. He was granted further honors, including a form of semi-official cult, with Antony as his high priest. Additionally, the day before the festival, Caesar had been named Dictator for Life, effectively granting unlimited power. Caesar's political rivals feared these reforms were his attempts at transforming the Republic into an open monarchy. During the festival's activities, Antony publicly offered Caesar a diadem, which Caesar refused. The event presented a powerful message: a diadem was a symbol of a king. By refusing it, Caesar demonstrated he had no intention of making himself King of Rome. Antony's motive for such actions is not clear and it is unknown if he acted with Caesar's prior approval or on his own.
A group of Senators resolved to kill Caesar to prevent him from seizing the throne. Chief among them were Marcus Junius Brutus and Gaius Cassius Longinus. Although Cassius was "the moving spirit" in the plot, winning over the chief assassins to the cause of tyrannicide, Brutus, with his family's history of deposing Rome's kings, became their leader. Cicero, though not personally involved in the conspiracy, later claimed Antony's actions sealed Caesar's fate as such an obvious display of Caesar's preeminence motivated them to act. Originally, the conspirators had planned to eliminate not only Caesar but also many of his supporters, including Antony, but Brutus rejected the proposal, limiting the conspiracy to Caesar alone. With Caesar preparing to depart for Parthia in late March, the conspirators prepared to act when Caesar appeared for the Senate meeting on the Ides of March (15 March).

Antony was supposed to attend with Caesar, but was waylaid at the door by one of the plotters and prevented from intervening. According to the Greek historian Plutarch, as Caesar arrived at the Senate, Lucius Tillius Cimber presented him with a petition to recall his exiled brother. The other conspirators crowded round to offer their support. Within moments, the entire group, including Brutus, was striking out at the dictator. Caesar attempted to get away, but, blinded by blood, he tripped and fell; the men continued stabbing him as he lay defenseless on the lower steps of the portico. According to Roman historian Eutropius, around 60 or more men participated in the assassination. Caesar was stabbed 23 times and died from the blood loss attributable to multiple stab wounds.

In the turmoil surrounding the assassination, Antony escaped Rome dressed as a slave, fearing Caesar's death would be the start of a bloodbath among his supporters. When this did not occur, he soon returned to Rome. The conspirators, who styled themselves the "Liberatores" ("The Liberators"), had barricaded themselves on the Capitoline Hill for their own safety. Though they believed Caesar's death would restore the Republic, Caesar had been immensely popular with the Roman middle and lower classes, who became enraged upon learning a small group of aristocrats had killed their champion.

Antony, as the sole Consul, soon took the initiative and seized the state treasury. Calpurnia, Caesar's widow, presented him with Caesar's personal papers and custody of his extensive property, clearly marking him as Caesar's heir and leader of the Caesarian faction. Caesar's Master of the Horse Marcus Aemilius Lepidus marched over 6,000 troops into Rome on 16 March to restore order and to act as the bodyguards of the Caesarian faction. Lepidus wanted to storm the Capitol, but Antony preferred a peaceful solution as a majority of both the Liberators and Caesar's own supporters preferred a settlement over civil war. On 17 March, at Antony's arrangement, the Senate met to discuss a compromise, which, due to the presence of Caesar's veterans in the city, was quickly reached. Caesar's assassins would be pardoned of their crimes and, in return, all of Caesar's actions would be ratified. In particular, the offices assigned to both Brutus and Cassius by Caesar were likewise ratified. Antony also agreed to accept the appointment of his rival Dolabella as his Consular colleague to replace Caesar. Having neither troops, money, nor popular support, the Liberatores were forced to accept Antony's proposal. This compromise was a great success for Antony, who managed to simultaneously appease Caesar's veterans, reconcile the Senate majority, and appear to the Liberatores as their partner and protector.
On 19 March, Caesar's will was opened and read. In it, Caesar posthumously adopted his great-nephew Gaius Octavius and named him his principal heir. Then only 19 years old and stationed with Caesar's army in Macedonia, the youth became a member of Caesar's Julian clan, changing his name to "Gaius Julius Caesar Octavianus" (Octavian) in accordance with the conventions of Roman adoption. Though not the chief beneficiary, Antony did receive some bequests.

Shortly after the compromise was reached, as a sign of good faith, Brutus, against the advice of Cassius and Cicero, agreed Caesar would be given a public funeral and his will would be validated. Caesar's funeral was held on 20 March. Antony, as Caesar's faithful lieutenant and reigning Consul, was chosen to preside over the ceremony and to recite the elegy. During the demagogic speech, he enumerated the deeds of Caesar and, publicly reading his will, detailed the donations Caesar had left to the Roman people. Antony then seized the blood-stained toga from Caesar's body and presented it to the crowd. Worked into a fury by the bloody spectacle, the assembly rioted. Several buildings in the Forum and some houses of the conspirators were burned to the ground. Panicked, many of the conspirators fled Italy. Under the pretext of not being able to guarantee their safety, Antony relieved Brutus and Cassius of their judicial duties in Rome and instead assigned them responsibility for procuring wheat for Rome from Sicily and Asia. Such an assignment, in addition to being unworthy of their rank, would have kept them far from Rome and shifted the balance towards Antony. Refusing such secondary duties, the two traveled to Greece instead. Additionally, Cleopatra left Rome to return to Egypt.

Despite the provisions of Caesar's will, Antony proceeded to act as leader of the Caesarian faction, including appropriating for himself a portion of Caesar's fortune rightfully belonging to Octavian. Antony enacted the Lex Antonia, which formally abolished the Dictatorship, in an attempt to consolidate his power by gaining the support of the Senatorial class. He also enacted a number of laws he claimed to have found in Caesar's papers to ensure his popularity with Caesar's veterans, particularly by providing land grants to them. Lepidus, with Antony's support, was named Pontifex Maximus to succeed Caesar. To solidify the alliance between Antony and Lepidus, Antony's daughter Antonia Prima was engaged to Lepidus's son, also named Lepidus. Surrounding himself with a bodyguard of over six thousand of Caesar's veterans, Antony presented himself as Caesar's true successor, largely ignoring Octavian.

Octavian arrived in Rome in May to claim his inheritance. Although Antony had amassed political support, Octavian still had opportunity to rival him as the leading member of the Caesarian faction. The Senatorial Republicans increasingly viewed Antony as a new tyrant. Antony had lost the support of many Romans and supporters of Caesar when he opposed the motion to elevate Caesar to divine status. When Antony refused to relinquish Caesar's vast fortune to him, Octavian borrowed heavily to fulfill the bequests in Caesar's will to the Roman people and to his veterans, as well as to establish his own bodyguard of veterans. This earned him the support of Caesarian sympathizers who hoped to use him as a means of eliminating Antony. The Senate, and Cicero in particular, viewed Antony as the greater danger of the two. By summer 44 BC, Antony was in a difficult position due to his actions regarding his compromise with the Liberatores following Caesar's assassination. He could either denounce the Liberatores as murderers and alienate the Senate or he could maintain his support for the compromise and risk betraying the legacy of Caesar, strengthening Octavian's position. In either case, his situation as ruler of Rome would be weakened. Roman historian Cassius Dio later recorded that while Antony, as reigning Consul, maintained the advantage in the relationship, the general affection of the Roman people was shifting to Octavian due to his status as Caesar's son.
Supporting the Senatorial faction against Antony, Octavian, in September 44 BC, encouraged the leading Senator Marcus Tullius Cicero to attack Antony in a series of speeches portraying him as a threat to the Republican order. Risk of civil war between Antony and Octavian grew. Octavian continued to recruit Caesar's veterans to his side, away from Antony, with two of Antony's legions defecting in November 44 BC. At that time, Octavian, only a private citizen, lacked legal authority to command the Republic's armies, making his command illegal. With popular opinion in Rome turning against him and his Consular term nearing its end, Antony attempted to secure a favorable military assignment to secure an army to protect himself. The Senate, as was custom, assigned Antony and Dolabella the provinces of Macedonia and Syria, respectively, to govern in 43 BC after their Consular terms expired. Antony, however, objected to the assignment, preferring to govern Cisalpine Gaul which had been assigned to Decimus Junius Brutus Albinus, one of Caesar's assassins. When Decimus refused to surrender his province, Antony marched north in December 44 BC with his remaining soldiers to take the province by force, besieging Decimus at Mutina. The Senate, led by a fiery Cicero, denounced Antony's actions and declared him an outlaw.

Ratifying Octavian's extraordinary command on 1 January 43 BC, the Senate dispatched him along with Consuls Hirtius and Pansa to defeat Antony and his five legions. Antony's forces were defeated at the Battle of Mutina in April 43 BC, forcing Antony to retreat to Transalpine Gaul. Both consuls were killed, however, leaving Octavian in sole command of their armies, some eight legions.

With Antony defeated, the Senate, hoping to eliminate Octavian and the remainder of the Caesarian party, assigned command of the Republic's legions to Decimus. Sextus Pompey, son of Caesar's old rival Pompey Magnus, was given command of the Republic's fleet from his base in Sicily while Brutus and Cassius were granted the governorships of Macedonia and Syria respectively. These appointments attempted to renew the "Republican" cause. However, the eight legions serving under Octavian, composed largely of Caesar's veterans, refused to follow one of Caesar's murderers, allowing Octavian to retain his command. Meanwhile, Antony recovered his position by joining forces with Marcus Aemilius Lepidus, who had been assigned the governorship of Transalpine Gaul and Nearer Spain. Antony sent Lepidus to Rome to broker a conciliation. Though he was an ardent Caesarian, Lepidus had maintained friendly relations with the Senate and with Sextus Pompey. His legions, however, quickly joined Antony, giving him control over seventeen legions, the largest army in the West.

By mid-May, Octavian began secret negotiations to form an alliance with Antony to provide a united Caesarian party against the Liberators. Remaining in Cisalpine Gaul, Octavian dispatched emissaries to Rome in July 43 BC demanding he be appointed Consul to replace Hirtius and Pansa and that the decree declaring Antony a public enemy be rescinded. When the Senate refused, Octavian marched on Rome with his eight legions and assumed control of the city in August 43 BC. Octavian proclaimed himself Consul, rewarded his soldiers, and then set about prosecuting Caesar's murderers. By the lex Pedia, all of the conspirators and Sextus Pompey were convicted ″in absentia″ and declared public enemies. Then, at the instigation of Lepidus, Octavian went to Cisalpine Gaul to meet Antony.

In November 43 BC, Octavian, Lepidus, and Antony met near Bononia. After two days of discussions, the group agreed to establish a three man dictatorship to govern the Republic for five years, known as the "Three Men for the Restoration of the Republic" (Latin: "Triumviri Rei publicae Constituendae"), known to modern historians as the Second Triumvirate. They shared military command of the Republic's armies and provinces among themselves: Antony received Gaul, Lepidus Spain, and Octavian (as the junior partner) Africa. They jointly governed Italy. The Triumvirate would have to conquer the rest of Rome's holdings; Brutus and Cassius held the Eastern Mediterranean, and Sextus Pompey held the Mediterranean islands. On 27 November 43 BC, the Triumvirate was formally established by a new law, the lex Titia. Octavian and Antony reinforced their alliance through Octavian's marriage to Antony's stepdaughter, Clodia Pulchra.

The primary objective of the Triumvirate was to avenge Caesar's death and to make war upon his murderers. Before marching against Brutus and Cassius in the East, the Triumvirs issued proscriptions against their enemies in Rome. The Dictator Lucius Cornelius Sulla had taken similar action to purge Rome of his opponents in 82 BC. The proscribed were named on public lists, stripped of citizenship, and outlawed. Their wealth and property were confiscated by the state, and rewards were offered to anyone who secured their arrest or death. With such encouragements, the proscription produced deadly results; two thousand Roman knights were executed, and one third of the Senate, among them Cicero, who was executed on 7 December. The confiscations helped replenish the State Treasury, which had been depleted by Caesar's civil war the decade before; when this seemed insufficient to fund the imminent war against Brutus and Cassius, the Triumvirs imposed new taxes, especially on the wealthy. By January 42 BC the proscription had ended; it had lasted two months, and though less bloody than Sulla's, it traumatized Roman society. A number of those named and outlawed had fled to either Sextus Pompey in Sicily or to the Liberators in the East. Senators who swore loyalty to the Triumvirate were allowed to keep their positions; on 1 January 42 BC, the senate officially deified Caesar as "The Divine Julius", and confirmed Antony's position as his high priest.

Due to the infighting within the Triumvirate during 43 BC, Brutus and Cassius had assumed control of much of Rome's eastern territories, including amassing a large army. Before the Triumvirate could cross the Adriatic Sea into Greece where the Liberators had stationed their army, the Triumvirate had to address the threat posed by Sextus Pompey and his fleet. From his base in Sicily, Sextus raided the Italian coast and blockaded the Triumvirs. Octavian's friend and admiral Quintus Rufus Salvidienus thwarted an attack by Sextus against the southern Italian mainland at Rhegium, but Salvidienus was then defeated in the resulting naval battle because of the inexperience of his crews. Only when Antony arrived with his fleet was the blockade broken. Though the blockade was defeated, control of Sicily remained in Sextus's hand, but the defeat of the Liberators was the Triumvirate's first priority.

In the summer of 42 BC, Octavian and Antony sailed for Macedonia to face the Liberators with nineteen legions, the vast majority of their army (approximately 100,000 regular infantry plus supporting cavalry and irregular auxiliary units), leaving Rome under the administration of Lepidus. Likewise, the army of the Liberators also commanded an army of nineteen legions; their legions, however, were not at full strength while the legions of Antony and Octavian were. While the Triumvirs commanded a larger number of infantry, the Liberators commanded a larger cavalry contingent. The Liberators, who controlled Macedonia, did not wish to engage in a decisive battle, but rather to attain a good defensive position and then use their naval superiority to block the Triumvirs' communications with their supply base in Italy. They had spent the previous months plundering Greek cities to swell their war-chest and had gathered in Thrace with the Roman legions from the Eastern provinces and levies from Rome's client kingdoms.

Brutus and Cassius held a position on the high ground along both sides of the via Egnatia west of the city of Philippi. The south position was anchored to a supposedly impassable marsh, while the north was bordered by impervious hills. They had plenty of time to fortify their position with a rampart and a ditch. Brutus put his camp on the north while Cassius occupied the south of the via Egnatia. Antony arrived shortly and positioned his army on the south of the via Egnatia, while Octavian put his legions north of the road. Antony offered battle several times, but the Liberators were not lured to leave their defensive stand. Thus, Antony tried to secretly outflank the Liberators' position through the marshes in the south. This provoked a pitched battle on 3 October 42 BC. Antony commanded the Triumvirate's army due to Octavian's sickness on the day, with Antony directly controlling the right flank opposite Cassius. Because of his health, Octavian remained in camp while his lieutenants assumed a position on the left flank opposite Brutus. In the resulting first battle of Philippi, Antony defeated Cassius and captured his camp while Brutus overran Octavian's troops and penetrated into the Triumvirs' camp but was unable to capture the sick Octavian. The battle was a tactical draw but due to poor communications Cassius believed the battle was a complete defeat and committed suicide to prevent being captured.

Brutus assumed sole command of the Liberator army and preferred a war of attrition over open conflict. His officers, however, were dissatisfied with these defensive tactics and his Caesarian veterans threatened to defect, forcing Brutus to give battle at the second battle of Philippi on 23 October. While the battle was initially evenly matched, Antony's leadership routed Brutus's forces. Brutus committed suicide the day after the defeat and the remainder of his army swore allegiance to the Triumvirate. Over fifty thousand Romans died in the two battles. While Antony treated the losers mildly, Octavian dealt cruelly with his prisoners and even beheaded Brutus's corpse.

The battles of Philippi ended the civil war in favor of the Caesarian faction. With the defeat of the Liberators, only Sextus Pompey and his fleet remained to challenge the Triumvirate's control over the Republic.

The victory at Philippi left the members of the Triumvirate as masters of the Republic, save Sextus Pompey in Sicily. Upon returning to Rome, the Triumvirate repartitioned rule of Rome's provinces among themselves, with Antony as the clear senior partner. He received the largest distribution, governing all of the Eastern provinces while retaining Gaul in the West. Octavian's position improved, as he received Spain, which was taken from Lepidus. Lepidus was then reduced to holding only Africa, and he assumed a clearly tertiary role in the Triumvirate. Rule over Italy remained undivided, but Octavian was assigned the difficult and unpopular task of demobilizing their veterans and providing them with land distributions in Italy. Antony assumed direct control of the East while he installed one of his lieutenants as the ruler of Gaul. During his absence, several of his supporters held key positions in Rome to protect his interests there.

The East was in need of reorganization after the rule of the Liberators in the previous years. In addition, Rome contended with the Parthian Empire for dominance of the Near East. The Parthian threat to the Triumvirate's rule was urgent due to the fact that the Parthians supported the Liberators in the recent civil war, aid which included the supply troops at Philippi. As ruler of the East, Antony also assumed responsibility for overseeing Caesar's planned invasion of Parthia to avenge the defeat of Marcus Licinius Crassus at the Battle of Carrhae in 53 BC.

In 42 BC, the Roman East was composed of several directly controlled provinces and client kingdoms. The provinces included Macedonia, Asia, Bithynia, Cilicia, Cyprus, Syria, and Cyrenaica. Approximately half of the eastern territory was controlled by Rome's client kingdoms, nominally independent kingdoms subject to Roman direction. These kingdoms included:

Antony spent the winter of 42 BC in Athens, where he ruled generously towards the Greek cities. A proclaimed "philhellene" ("Friend of all things Greek"), Antony supported Greek culture to win the loyalty of the inhabitants of the Greek East. He attended religious festivals and ceremonies, including initiation into the Eleusinian Mysteries, a secret cult dedicated to the worship of the goddesses Demeter and Persephone. Beginning in 41 BC, he traveled across the Aegean Sea to Anatolia, leaving his friend Lucius Marcius Censorius as governor of Macedonia and Achaea. Upon his arrival in Ephesus in Asia, Antony was worshiped as the god Dionysus born anew. He demanded heavy taxes from the Hellenic cities in return for his pro-Greek culture policies, but exempted those cities which had remained loyal to Caesar during the civil war and compensated those cities which had suffered under Caesar's assassins, including Rhodes, Lycia, and Tarsus. He granted pardons to all Roman nobles living in the East who had supported the Republican cause, except for Caesar's assassins.

Ruling from Ephesus, Antony consolidated Rome's hegemony in the East, receiving envoys from Rome's client kingdoms and intervening in their dynastic affairs, extracting enormous financial "gifts" from them in the process. Though King Deiotarus of Galatia supported Brutus and Cassius following Caesar's assassination, Antony allowed him to retain his position. He also confirmed Ariarathes X as king of Cappadocia after the execution of his brother Ariobarzanes III of Cappadocia by Cassius before the Battle of Philippi. In Hasmonean Judea, several Jewish delegations complained to Antony of the harsh rule of Phasael and Herod, the sons of Rome's assassinated chief Jewish minister Antipater the Idumaean. After Herod offered him a large financial gift, Antony confirmed the brothers in their positions. Subsequently, influenced by the beauty and charms of Glaphyra, the widow of Archelaüs (formerly the high priest of Comana), Antony deposed Ariarathes, and appointed Glaphyra's son, Archelaüs, to rule Cappadocia.
In October 41, Antony requested Rome's chief eastern vassal, the queen of Ptolemaic Egypt Cleopatra, meet him at Tarsus in Cilicia. Antony had first met a young Cleopatra while campaigning in Egypt in 55 BC and again in 48 BC when Caesar had backed her as queen of Egypt over the claims of her half-sister Arsinoe. Cleopatra would bear Caesar a son, Caesarion, in 47 BC and the two living in Rome as Caesar's guests until his assassination in 44 BC. After Caesar's assassination, Cleopatra and Caesarion returned to Egypt, where she named the child as her co-ruler. In 42 BC, the Triumvirate, in recognition for Cleopatra's help towards Publius Cornelius Dolabella in opposition to the Liberators, granted official recognition to Caesarion's position as king of Egypt. Arriving in Tarsus aboard her magnificent ship, Cleopatra invited Antony to a grand banquet to solidify their alliance. As the most powerful of Rome's eastern vassals, Egypt was indispensable in Rome's planned military invasion of the Parthian Empire. At Cleopatra's request, Antony ordered the execution of Arsinoe, who, though marched in Caesar's triumphal parade in 46 BC, had been granted sanctuary at the temple of Artemis in Ephesus. Antony and Cleopatra then spent the winter of 41 BC together in Alexandria. Cleopatra bore Antony twin children, Alexander Helios and Cleopatra Selene II, in 40 BC, and a third, Ptolemy Philadelphus, in 36 BC. Antony also granted formal control over Cyprus, which had been under Egyptian control since 47 BC during the turmoil of Caesar's civil war, to Cleopatra in 40 BC as a gift for her loyalty to Rome.

Antony, in his first months in the East, raised money, reorganized his troops, and secured the alliance of Rome's client kingdoms. He also promoted himself as Hellenistic ruler, which won him the affection of the Greek peoples of the East but also made him the target of Octavian's propaganda in Rome. According to some ancient authors, Antony led a carefree life of luxury in Alexandria. Upon learning the Parthian Empire had invaded Rome's territory in early 40 BC, Antony left Egypt for Syria to confront the invasion. However, after a short stay in Tyre, he was forced to sail with his army to Italy to confront Octavian due to Octavian's war against Antony's wife and brother.

Following the defeat of Brutus and Cassius, while Antony was stationed in the East, Octavian had authority over the West. Octavian's chief responsibility was distributing land to tens of thousands of Caesar's veterans who had fought for the Triumvirate. Additionally, tens of thousands of veterans who had fought for the Republican cause in the war also required land grants. This was necessary to ensure they would not support a political opponent of the Triumvirate. However, the Triumvirs did not possess sufficient state-controlled land to allot to the veterans. This left Octavian with two choices: alienating many Roman citizens by confiscating their land, or alienating many Roman soldiers who might back a military rebellion against the Triumvirate's rule. Octavian chose the former. As many as eighteen Roman towns through Italy were affected by the confiscations of 41 BC, with entire populations driven out.

Led by Fulvia, the wife of Antony, the Senators grew hostile towards Octavian over the issue of the land confiscations. According to the ancient historian Cassius Dio, Fulvia was the most powerful woman in Rome at the time. According to Dio, while Publius Servilius Vatia and Lucius Antonius were the Consuls for the year 41 BC, real power was vested in Fulvia. As the mother-in-law of Octavian and the wife of Antony, no action was taken by the Senate without her support. Fearing Octavian's land grants would cause the loyalty of the Caesarian veterans to shift away from Antony, Fulvia traveled constantly with her children to the new veteran settlements in order to remind the veterans of their debt to Antony. Fulvia also attempted to delay the land settlements until Antony returned to Rome, so that he could share credit for the settlements. With the help of Antony's brother, the Consul of 41 BC Lucius Antonius, Fulvia encouraged the Senate to oppose Octavian's land policies.

The conflict between Octavian and Fulvia caused great political and social unrest throughout Italy. Tensions escalated into open war, however, when Octavian divorced Clodia Pulchra, Fulvia's daughter from her first husband Publius Clodius Pulcher. Outraged, Fulvia, supported by Lucius, raised an army to fight for Antony's rights against Octavian. According to the ancient historian Appian, Fulvia's chief reason for the war was her jealousy of Antony's affairs with Cleopatra in Egypt and desire to draw Antony back to Rome. Lucius and Fulvia took a political and martial gamble in opposing Octavian and Lepidus, however, as the Roman army still depended on the Triumvirs for their salaries. Lucius and Fulvia, supported by their army, marched on Rome and promised the people an end to the Triumvirate in favor of Antony's sole rule. However, when Octavian returned to the city with his army, the pair was forced to retreat to Perusia in Etruria. Octavian placed the city under siege while Lucius waited for Antony's legions in Gaul to come to his aid. Away in the East and embarrassed by Fulvia's actions, Antony gave no instructions to his legions. Without reinforcements, Lucius and Fulvia were forced to surrender in February 40 BC. While Octavian pardoned Lucius for his role in the war and even granted him command in Spain as his chief lieutenant there, Fulvia was forced to flee to Greece with her children. With the war over, Octavian was left in sole control over Italy. When Antony's governor of Gaul died, Octavian took over his legions there, further strengthening his control over the West.

Despite the Parthian Empire's invasion of Rome's eastern territories, Fulvia's civil war forced Antony to leave the East and return to Rome in order to secure his position. Meeting her in Athens, Antony rebuked Fulvia for her actions before sailing on to Italy with his army to face Octavian, laying siege to Brundisium. This new conflict proved untenable for both Octavian and Antony, however. Their centurions, who had become important figures politically, refused to fight due to their shared service under Caesar. The legions under their command followed suit. Meanwhile, in Sicyon, Fulvia died of a sudden and unknown illness. Fulvia's death and the mutiny of their soldiers allowed the triumvirs to effect a reconciliation through a new power sharing agreement in September 40 BC. The Roman world was redivided, with Antony receiving the Eastern provinces, Octavian the Western provinces, and Lepidus relegated to a clearly junior position as governor of Africa. This agreement, known as the "Treaty of Brundisium", reinforced the Triumvirate and allowed Antony to begin preparing for Caesar's long-awaited campaign against the Parthian Empire. As a symbol of their renewed alliance, Antony married Octavia, Octavian's sister, in October 40 BC.

The rise of the Parthian Empire in the 3rd century BC and Rome's expansion into the Eastern Mediterranean during the 2nd century BC brought the two powers into direct contact, causing centuries of tumultuous and strained relations. Though periods of peace developed cultural and commercial exchanges, war was a constant threat. Influence over the buffer state of the Kingdom of Armenia, located to the north-east of Roman Syria, was often a central issue in the Roman-Parthian conflict. In 95 BC, Parthian Shah Mithridates II, installed Tigranes the Great as Parthian's client-king over Armenia. Tigranes would wage a series of three wars against Rome before being ultimately defeated by Pompey in 66 BC. Thereafter, with his son Artavasdes II in Rome as a hostage, Tigranes would rule Armenia as an ally of Rome until his death in 55 BC. Rome then installed Artavasdes II as king and continued its influence over Armenia.

In 53 BC, Rome's governor of Syria, Marcus Licinius Crassus, led an expedition across the Euphrates River into Parthian territory to confront the Parthian Shah Orodes II. Artavasdes II offered Crassus the aid of nearly forty thousand troops to assist his Parthian expedition on the condition that Crassus invade through Armenia as the safer route. Crassus refused, choosing instead the more direct route by crossing the Euphrates directly into desert Parthian territory. Crassus' actions proved disastrous as his army was defeated at the Battle of Carrhae by a numerically inferior Parthian force. Crassus' defeat forced Armenia to shift its loyalty to Parthia, with Artavasdes II's sister marrying Orodes' son and heir Pacorus.

In early 44 BC, Julius Caesar announced his intentions to invade Parthia and restore Roman power in the East. His reasons were to punish the Parthians for assisting Pompey in the recent civil war, to avenge Crassus' defeat at Carrhae, and especially to match the glory of Alexander the Great for himself. Before Caesar could launch his campaign, however, he was assassinated. As part of the compromise between Antony and the Republicans to restore order following Caesar's murder, Publius Cornelius Dolabella was assigned the governorship of Syria and command over Caesar's planned Parthian campaign. The compromise did not hold, however, and the Republicans were forced to flee to the East. The Republicans directed Quintus Labienus to attract the Parthians to their side in the resulting war against Antony and Octavian. After the Republicans were defeated at the Battle of Philippi, Labienus joined the Parthians. Despite Rome's internal turmoil during the time, the Parthians did not immediately benefit from the power vacuum in the East due to Orodes II's reluctance despite Labienus' urgings to the contrary.

In the summer of 41 BC, Antony, to resassert Roman power in the East, conquered Palmyra on the Roman-Parthian border. Antony then spent the winter of 41 BC in Alexandria with Cleopatra, leaving only two legions to defend the Syrian border against Parthian incursions. The legions, however, were composed of former Republican troops and Labienus convinced Orodes II to invade.

A Parthian army, led by Orodes II's eldest son Pacorus, invaded Syria in early 40 BC. Labienus, the Republican ally of Brutus and Cassius, accompanied him to advise him and to rally the former Republican soldiers stationed in Syria to the Parthian cause. Labienus recruited many of the former Republican soldiers to the Parthian campaign in opposition to Antony. The joint Parthian–Roman force, after initial success in Syria, separated to lead their offensive in two directions: Pacorus marched south toward Hasmonean Judea while Labienus crossed the Taurus Mountains to the north into Cilicia. Labienus conquered southern Anatolia with little resistance. The Roman governor of Asia, Lucius Munatius Plancus, a partisan of Antony, was forced to flee his province, allowing Labienus to recruit the Roman soldiers stationed there. For his part, Pacorus advanced south to Phoenicia and Palestine. In Hasmonean Judea, the exiled prince Antigonus allied himself with the Parthians. When his brother, Rome's client king Hyrcanus II, refused to accept Parthian domination, he was deposed in favor of Antigonus as Parthia's client king in Judea. Pacorus' conquest had captured much of the Syrian and Palestinian interior, with much of the Phoenician coast occupied as well. The city of Tyre remained the last major Roman outpost in the region.

Antony, then in Egypt with Cleopatra, did not respond immediately to the Parthian invasion. Though he left Alexandria for Tyre in early 40 BC, when he learned of the civil war between his wife and Octavian, he was forced to return to Italy with his army to secure his position in Rome rather than defeat the Parthians. Instead, Antony dispatched Publius Ventidius Bassus to check the Parthian advance. Arriving in the East in spring 39 BC, Ventidius surprised Labienus near the Taurus Mountains, claiming victory at the Cilician Gates. Ventidius ordered Labienus executed as a traitor and the formerly rebellious Roman soldiers under his command were reincorporated under Antony's control. He then met a Parthian army at the border between Cilicia and Syria, defeating it and killing a large portion of the Parthian soldiers at the Amanus Pass. Ventidius's actions temporarily halted the Parthian advance and restored Roman authority in the East, forcing Pacorus to abandon his conquests and return to Parthia.

In the spring of 38 BC, the Parthians resumed their offensive with Pacorus leading an army across the Euphrates. Ventidius, in order to gain time, leaked disinformation to Pacorus implying that he should cross the Euphrates River at their usual ford. Pacorus did not trust this information and decided to cross the river much farther downstream; this was what Ventidius hoped would occur and gave him time to get his forces ready. The Parthians faced no opposition and proceeded to the town of Gindarus in Cyrrhestica where Ventidius's army was waiting. At the Battle of Cyrrhestica, Ventidius inflicted an overwhelming defeat against the Parthians which resulted in the death of Pacorus. Overall, the Roman army had achieved a complete victory with Ventidius' three successive victories forcing the Parthians back across the Euphrates. Pacorus' death threw the Parthian Empire into chaos. Shah Orodes II, overwhelmed by the grief of his son's death, appointed his younger son Phraates IV as his successor. However, Phraates IV assassinated Orodes II in late 38 BC, succeeding him on the throne.

Ventidius feared Antony's wrath if he invaded Parthian territory, thereby stealing his glory; so instead he attacked and subdued the eastern kingdoms, which had revolted against Roman control following the disastrous defeat of Crassus at Carrhae. One such rebel was King Antiochus of Commagene, whom he besieged in Samosata. Antiochus tried to make peace with Ventidius, but Ventidius told him to approach Antony directly. After peace was concluded, Antony sent Ventidius back to Rome where he celebrated a triumph, the first Roman to triumph over the Parthians.

While Antony and the other Triumvirs ratified the Treaty of Brundisium to redivide the Roman world among themselves, the rebel general Sextus Pompey, the son of Caesar's rival Pompey the Great, was largely ignored. From his stronghold on Sicily, he continued his piratical activities across Italy and blocked the shipment of grain to Rome. The lack of food in Rome caused the public to blame the Triumvirate and shift its sympathies towards Pompey. This pressure forced the Triumvirs to meet with Sextus in early 39 BC.

While Octavian wanted an end to the ongoing blockade of Italy, Antony sought peace in the West in order to make the Triumvirate's legions available for his service in his planned campaign against the Parthians. Though the Triumvirs rejected Sextus' initial request to replace Lepidus as the third man within the Triumvirate, they did grant other concessions. Under the terms of the Treaty of Misenum, Sextus was allowed to retain control over Sicily and Sardinia, with the provinces of Corsica and Greece being added to his territory. He was also promised a future position with the Priestly College of Augurs and the Consulship for 35 BC. In exchange, Sextus agreed to end his naval blockade of Italy, supply Rome with grain, and halt his piracy of Roman merchant ships. However, the most important provision of the Treaty was the end of the proscription the Trimumvirate had begun in late 43 BC. Many of the proscribed Senators, rather than face death, fled to Sicily seeking Sextus' protection. With the exception of those responsible for Caesar's assassination, all those proscribed were allowed to return to Rome and promised compensation. This caused Sextus to lose many valuable allies as the formerly exiled Senators gradually aligned themselves with either Octavian or Antony. To secure the peace, Octavian betrothed his three-year-old nephew and Antony's stepson Marcus Claudius Marcellus to Sextus' daughter Pompeia. With peace in the West secured, Antony planned to retaliate against Parthia by invading their territory. Under an agreement with Octavian, Antony would be supplied with extra troops for his campaign. With this military purpose on his mind, Antony sailed to Greece with Octavia, where he behaved in a most extravagant manner, assuming the attributes of the Greek god Dionysus in 39 BC.

The peace with Sextus was short lived, however. When Sextus demanded control over Greece as the agreement provided, Antony demanded the province's tax revenues be to fund the Parthian campaign. Sextus refused. Meanwhile, Sextus' admiral Menas betrayed him, shifting his loyalty to Octavian and thereby granting him control of Corsica, Sardinia, three of Sextus' legions, and a larger naval force. These actions worked to renew Sextus' blockade of Italy, preventing Octavian from sending the promised troops to Antony for the Parthian campaign. This new delay caused Antony to quarrel with Octavian, forcing Octavia to mediate a truce between them. Under the Treaty of Tarentum, Antony provided a large naval force for Octavian's use against Sextus while Octavian promised to raise new legions for Antony to support his invasion of Parthia. As the term of the Triumvirate was set to expire at the end of 38 BC, the two unilaterally extended their term of office another five years until 33 BC without seeking approval of the Senate or the popular assemblies. To seal the Treaty, Antony's elder son Marcus Antonius Antyllus, then only 6 years old, was betrothed to Octavian's only daughter Julia, then only an infant. With the Treaty signed, Antony returned to the East, leaving Octavia in Italy.

With Publius Ventidius Bassus returned to Rome in triumph for his defensive campaign against the Parthians, Antony appointed Gaius Sosius as the new governor of Syria and Cilicia in early 38 BC. Antony, still in the West negotiating with Octavian, ordered Sosius to depose Antigonus, who had been installed in the recent Parthian invasion as the ruler of Hasmonean Judea, and to make Herod the new Roman client king in the region. Years before in 40 BC, the Roman Senate had proclaimed Herod "King of the Jews" because Herod had been a loyal supporter of Hyrcanus II, Rome's previous client king before the Parthian invasion, and was from a family with long standing connections to Rome. The Romans hoped to use Herod as a bulwark against the Parthians in the coming campaign.

Advancing south, Sosius captured the island-city of Aradus on the coast of Phoenicia by the end of 38 BC. The following year, the Romans besieged Jerusalem. After a forty-day siege, the Roman soldiers stormed the city and, despite Herod's pleas for restraint, acted without mercy, pillaging and killing all in their path, prompting Herod to complain to Antony. Herod finally resorted to bribing Sosius and his troops in order that they would not leave him "king of a desert". Antigonus was forced to surrender to Sosius, and was sent to Antony for the triumphal procession in Rome. Herod, however, fearing that Antigonus would win backing in Rome, bribed Antony to execute Antigonus. Antony, who recognized that Antigonus would remain a permanent threat to Herod, ordered him beheaded in Antioch. Now secure on his throne, Herod would rule the Herodian Kingdom until his death in 4 BC, and would be an ever-faithful client king of Rome.

With the Triumvirate renewed in 38 BC, Antony returned to Athens in the winter with his new wife Octavia, the sister of Octavian. With the assassination of the Parthian Shah Orodes II by his son Phraates IV, who then seized the Parthian throne, in late 38 BC, Antony prepared to invade Parthia himself.

Antony, however, realized Octavian had no intention of sending him the additional legions he had promised under the Treaty of Tarentum. To supplement his own armies, Antony instead looked to Rome's principal vassal in the East: his lover Cleopatra. In addition to significant financial resources, Cleopatra's backing of his Parthian campaign allowed Antony to amass the largest army Rome had ever assembled in the East. Wintering in Antioch during 37, Antony's combined Roman–Egyptian army numbered some 200,000, including sixteen legions (approximately 160,000 soldiers) plus an additional 40,000 auxiliaries. Such a force was twice the size of Marcus Licinius Crassus's army from his failed Parthian invasion of 53 BC and three times those of Lucius Licinius Lucullus and Lucius Cornelius Sulla during the Mithridatic Wars. The size of his army indicated Antony's intention to conquer Parthia, or at least receive its submission by capturing the Parthian capital of Ecbatana. Antony's rear was protected by Rome's client kingdoms in Anatolia, Syria, and Judea, while the client kingdoms of Cappadocia, Pontus, and Commagene would provide supplies along the march.

Antony's first target for his invasion was the Kingdom of Armenia. Ruled by King Artavasdes II of Armenia, Armenia had been an ally of Rome since the defeat of Tigranes the Great by Pompey the Great in 66 BC during the Third Mithridatic War. However, following Marcus Licinius Crassus's defeat at the Battle of Carrhae in 53 BC, Armenia was forced into an alliance with Parthia due to Rome's weakened position in the East. Antony dispatched Publius Canidius Crassus to Armenia, receiving Artavasdes II's surrender without opposition. Canidius then led an invasion into the Transcaucasia, subduing Iberia. There, Canidius forced the Iberian King Pharnavaz II into an alliance against Zober, king of neighboring Albania, subduing the kingdom and reducing it to a Roman protectorate.

With Armenia and the Caucasus secured, Antony marched south, crossing into the Parthian province of Media Atropatene. Though Antony desired a pitched battle, the Parthians would not engage, allowing Antony to march deep into Parthian territory by mid-August of 36 BC. This forced Antony to leave his logistics train in the care of two legions (approximately 10,000 soldiers), which was then attacked and completely destroyed by the Parthian army before Antony could rescue them. Though the Armenian King Artavasdes II and his cavalry were present during the massacre, they did not intervene. Despite the ambush, Antony continued the campaign. However, Antony was soon forced to retreat in mid-October after a failed two-month siege of the provincial capital.

The retreat soon proved a disaster as Antony's demoralized army faced increasing supply difficulties in the mountainous terrain during winter while constantly being harassed by the Parthian army. According to the Greek historian Plutarch, eighteen battles were fought between the retreating Romans and the Parthians during the month-long march back to Armenia, with approximately 20,000 infantry and 4,000 cavalry dying during the retreat alone. Once in Armenia, Antony quickly marched back to Syria to protect his interests there by late 36 BC, losing an additional 8,000 soldiers along the way. In all, two-fifths of his original army (some 80,000 men) had died during his failed campaign.

Meanwhile, in Rome, the triumvirate was no more. Octavian forced Lepidus to resign after the older triumvir attempted to take control of Sicily after the defeat of Sextus. Now in sole power, Octavian was occupied in wooing the traditional Republican aristocracy to his side. He married Livia and started to attack Antony in order to raise himself to power. He argued that Antony was a man of low morals to have left his faithful wife abandoned in Rome with the children to be with the promiscuous queen of Egypt. Antony was accused of everything, but most of all, of "going native", an unforgivable crime to the proud Romans. Several times Antony was summoned to Rome, but remained in Alexandria with Cleopatra.

Again with Egyptian money, Antony invaded Armenia, this time successfully. In the return, a mock Roman triumph was celebrated in the streets of Alexandria. The parade through the city was a pastiche of Rome's most important military celebration. For the finale, the whole city was summoned to hear a very important political statement. Surrounded by Cleopatra and her children, Antony ended his alliance with Octavian.

He distributed kingdoms among his children: Alexander Helios was named king of Armenia, Media and Parthia (territories which were not for the most part under the control of Rome), his twin Cleopatra Selene got Cyrenaica and Libya, and the young Ptolemy Philadelphus was awarded Syria and Cilicia. As for Cleopatra, she was proclaimed Queen of Kings and Queen of Egypt, to rule with Caesarion (Ptolemy XV Caesar, son of Cleopatra by Julius Caesar), King of Kings and King of Egypt. Most important of all, Caesarion was declared legitimate son and heir of Caesar. These proclamations were known as the "Donations of Alexandria" and caused a fatal breach in Antony's relations with Rome.

While the distribution of nations among Cleopatra's children was hardly a conciliatory gesture, it did not pose an immediate threat to Octavian's political position. Far more dangerous was the acknowledgment of Caesarion as legitimate and heir to Caesar's name. Octavian's base of power was his link with Caesar through adoption, which granted him much-needed popularity and loyalty of the legions. To see this convenient situation attacked by a child borne by the richest woman in the world was something Octavian could not accept. The triumvirate expired on the last day of 33 BC and was not renewed. Another civil war was beginning.

During 33 and 32 BC, a propaganda war was fought in the political arena of Rome, with accusations flying between sides. Antony (in Egypt) divorced Octavia and accused Octavian of being a social upstart, of usurping power, and of forging the adoption papers by Caesar. Octavian responded with treason charges: of illegally keeping provinces that should be given to other men by lots, as was Rome's tradition, and of starting wars against foreign nations (Armenia and Parthia) without the consent of the Senate.

Antony was also held responsible for Sextus Pompey's execution without a trial. In 32 BC, the Senate deprived him of his powers and declared war against Cleopatra – not Antony, because Octavian had no wish to advertise his role in perpetuating Rome's internecine bloodshed. Both consuls, Gnaeus Domitius Ahenobarbus and Gaius Sosius, and a third of the Senate abandoned Rome to meet Antony and Cleopatra in Greece.
In 31 BC, the war started. Octavian's general Marcus Vipsanius Agrippa captured the Greek city and naval port of Methone, loyal to Antony. The enormous popularity of Octavian with the legions secured the defection of the provinces of Cyrenaica and Greece to his side. On September 2, the naval Battle of Actium took place. Antony and Cleopatra's navy was destroyed, and they were forced to escape to Egypt with 60 ships.

Octavian, now close to absolute power, did not intend to give Antony and Cleopatra any rest. In August 30 BC, assisted by Agrippa, he invaded Egypt. With no other refuge to escape to, Antony committed suicide by stabbing himself with his sword in the mistaken belief that Cleopatra had already done so. When he found out that Cleopatra was still alive, his friends brought him to Cleopatra's monument in which she was hiding, and he died in her arms.

Cleopatra was allowed to conduct Antony's burial rites after she had been captured by Octavian. Realising that she was destined for Octavian's triumph in Rome, she made several attempts to take her life and finally succeeded in mid-August. Octavian had Caesarion murdered, but he spared Antony's children by Cleopatra, who were paraded through the streets of Rome. Antony's daughters by Octavia were spared, as was his son, Iullus Antonius. But his elder son, Marcus Antonius Antyllus, was killed by Octavian's men while pleading for his life in the Caesareum.

Cicero's son, Cicero Minor, announced Antony's death to the senate. Antony's honours were revoked and his statues removed, but he was not subject to a complete damnatio memoriae. Cicero Minor also made a decree that no member of the Antonii would ever bear the name Marcus again. "In this way Heaven entrusted the family of Cicero the final acts in the punishment of Antony."

When Antony died, Octavian became uncontested ruler of Rome. In the following years, Octavian, who was known as Augustus after 27 BC, managed to accumulate in his person all administrative, political, and military offices. When Augustus died in AD 14, his political powers passed to his adopted son Tiberius; the Roman Principate had begun.

The rise of Caesar and the subsequent civil war between his two most powerful adherents effectively ended the credibility of the Roman oligarchy as a governing power and ensured that all future power struggles would centre upon which one individual would achieve supreme control of the government, eliminating the Senate and the former magisterial structure as important foci of power in these conflicts. Thus, in history, Antony appears as one of Caesar's main adherents, he and Octavian Augustus being the two men around whom power coalesced following the assassination of Caesar, and finally as one of the three men chiefly responsible for the demise of the Roman Republic.

Antony had been married in succession to Fadia, Antonia, Fulvia, Octavia and Cleopatra, and left behind him a number of children. Through his daughters by Octavia, he would be ancestor to the Roman Emperors Caligula, Claudius and Nero.

Through his daughters by Octavia, he would become the paternal great grandfather of Roman Emperor Caligula, the maternal grandfather of Emperor Claudius, and both maternal great-great-grandfather and paternal great-great uncle of the Emperor Nero of the Julio-Claudian dynasty, the very family, as represented by Octavian Augustus, that he had fought to defeat. Through his eldest daughter, he would become ancestor to the long line of kings and co-rulers of the Bosporan Kingdom, the longest-living Roman client kingdom, as well as the rulers and royalty of several other Roman client states. Through his daughter by Cleopatra, Antony would become ancestor to the royal family of Mauretania, another Roman client kingdom, while through his sole surviving son Iullus, he would be ancestor to several famous Roman statesmen.

Works in which the character of Mark Antony plays a central role:






|-


</doc>
<doc id="19961" url="https://en.wikipedia.org/wiki?curid=19961" title="Manchester United F.C.">
Manchester United F.C.

Manchester United Football Club, commonly known as Man United or simply United, is a professional football club based in Old Trafford, Greater Manchester, England, that competes in the Premier League, the top flight of English football. Nicknamed "the Red Devils", the club was founded as Newton Heath LYR Football Club in 1878, changed its name to Manchester United in 1902 and moved to its current stadium, Old Trafford, in 1910.

Manchester United have won more trophies than any other club in English football, with a record 20 League titles, 12 FA Cups, 5 League Cups and a record 21 FA Community Shields. United have also won three UEFA Champions Leagues, one UEFA Europa League, one UEFA Cup Winners' Cup, one UEFA Super Cup, one Intercontinental Cup and one FIFA Club World Cup. In 1998–99, the club became the first in the history of English football to achieve the treble of the Premier League, the FA Cup and the UEFA Champions League. By winning the UEFA Europa League in 2016–17, they became one of five clubs to have won all three main UEFA club competitions, and the only English club to have won every ongoing top-flight honour available to them.

The 1958 Munich air disaster claimed the lives of eight players. In 1968, under the management of Matt Busby, Manchester United became the first English football club to win the European Cup. Alex Ferguson won 38 trophies as manager, including 13 Premier League titles, 5 FA Cups and 2 UEFA Champions Leagues, between 1986 and 2013, when he announced his retirement. José Mourinho is the club's current manager, having been appointed on 27 May 2016.

Manchester United was the highest-earning football club in the world for 2016–17, with an annual revenue of €676.3 million, and the world's most valuable football club in 2018, valued at £3.1 billion. As of June 2015, it is the world's most valuable football brand, estimated to be worth $1.2 billion. After being floated on the London Stock Exchange in 1991, the club was purchased by Malcolm Glazer in May 2005 in a deal valuing the club at almost £800 million, after which the company was taken private again, before going public once more in August 2012, when they made an initial public offering on the New York Stock Exchange. Manchester United is one of the most widely supported football clubs in the world, and has rivalries with Liverpool, Manchester City, Arsenal, and Leeds United.

Manchester United was formed in 1878 as Newton Heath LYR Football Club by the Carriage and Wagon department of the Lancashire and Yorkshire Railway (LYR) depot at Newton Heath. The team initially played games against other departments and railway companies, but on 20 November 1880, they competed in their first recorded match; wearing the colours of the railway company – green and gold – they were defeated 6–0 by Bolton Wanderers' reserve team. By 1888, the club had become a founding member of The Combination, a regional football league. Following the league's dissolution after only one season, Newton Heath joined the newly formed Football Alliance, which ran for three seasons before being merged with the Football League. This resulted in the club starting the 1892–93 season in the First Division, by which time it had become independent of the railway company and dropped the "LYR" from its name. After two seasons, the club was relegated to the Second Division.
In January 1902, with debts of £2,670 – equivalent to £ in 2018 – the club was served with a winding-up order. Captain Harry Stafford found four local businessmen, including John Henry Davies (who became club president), each willing to invest £500 in return for a direct interest in running the club and who subsequently changed the name; on 24 April 1902, Manchester United was officially born. Under Ernest Mangnall, who assumed managerial duties in 1903, the team finished as Second Division runners-up in 1906 and secured promotion to the First Division, which they won in 1908 – the club's first league title. The following season began with victory in the first ever Charity Shield and ended with the club's first FA Cup title. Manchester United won the First Division for the second time in 1911, but at the end of the following season, Mangnall left the club to join Manchester City.

In 1922, three years after the resumption of football following the First World War, the club was relegated to the Second Division, where it remained until regaining promotion in 1925. Relegated again in 1931, Manchester United became a yo-yo club, achieving its all-time lowest position of 20th place in the Second Division in 1934. Following the death of principal benefactor John Henry Davies in October 1927, the club's finances deteriorated to the extent that Manchester United would likely have gone bankrupt had it not been for James W. Gibson, who, in December 1931, invested £2,000 and assumed control of the club. In the 1938–39 season, the last year of football before the Second World War, the club finished 14th in the First Division.

In October 1945, the impending resumption of football led to the managerial appointment of Matt Busby, who demanded an unprecedented level of control over team selection, player transfers and training sessions. Busby led the team to second-place league finishes in 1947, 1948 and 1949, and to FA Cup victory in 1948. In 1952, the club won the First Division, its first league title for 41 years. With an average age of 22, the back-to-back title winning side of 1956 were labelled "the Busby Babes" by the media, a testament to Busby's faith in his youth players. In 1957, Manchester United became the first English team to compete in the European Cup, despite objections from The Football League, who had denied Chelsea the same opportunity the previous season. En route to the semi-final, which they lost to Real Madrid, the team recorded a 10–0 victory over Belgian champions Anderlecht, which remains the club's biggest victory on record.
The following season, on the way home from a European Cup quarter-final victory against Red Star Belgrade, the aircraft carrying the Manchester United players, officials and journalists crashed while attempting to take off after refuelling in Munich, Germany. The Munich air disaster of 6 February 1958 claimed 23 lives, including those of eight players – Geoff Bent, Roger Byrne, Eddie Colman, Duncan Edwards, Mark Jones, David Pegg, Tommy Taylor and Billy Whelan – and injured several more.
Assistant manager Jimmy Murphy took over as manager while Busby recovered from his injuries and the club's makeshift side reached the FA Cup final, which they lost to Bolton Wanderers. In recognition of the team's tragedy, UEFA invited the club to compete in the 1958–59 European Cup alongside eventual League champions Wolverhampton Wanderers. Despite approval from the FA, the Football League determined that the club should not enter the competition, since it had not qualified. Busby rebuilt the team through the 1960s by signing players such as Denis Law and Pat Crerand, who combined with the next generation of youth players – including George Best – to win the FA Cup in 1963. The following season, they finished second in the league, then won the title in 1965 and 1967. In 1968, Manchester United became the first English (and second British) club to win the European Cup, beating Benfica 4–1 in the final with a team that contained three European Footballers of the Year: Bobby Charlton, Denis Law and George Best. Matt Busby resigned as manager in 1969 and was replaced by the reserve team coach, former Manchester United player Wilf McGuinness.

Following an eighth-place finish in the 1969–70 season and a poor start to the 1970–71 season, Busby was persuaded to temporarily resume managerial duties, and McGuinness returned to his position as reserve team coach. In June 1971, Frank O'Farrell was appointed as manager, but lasted less than 18 months before being replaced by Tommy Docherty in December 1972. Docherty saved Manchester United from relegation that season, only to see them relegated in 1974; by that time the trio of Best, Law, and Charlton had left the club. The team won promotion at the first attempt and reached the FA Cup final in 1976, but were beaten by Southampton. They reached the final again in 1977, beating Liverpool 2–1. Docherty was dismissed shortly afterwards, following the revelation of his affair with the club physiotherapist's wife.

Dave Sexton replaced Docherty as manager in the summer of 1977. Despite major signings, including Joe Jordan, Gordon McQueen, Gary Bailey, and Ray Wilkins, the team failed to achieve any significant results; they finished in the top two in 1979–80 and lost to Arsenal in the 1979 FA Cup Final. Sexton was dismissed in 1981, even though the team won the last seven games under his direction. He was replaced by Ron Atkinson, who immediately broke the British record transfer fee to sign Bryan Robson from West Bromwich Albion. Under Atkinson, Manchester United won the FA Cup twice in three years – in 1983 and 1985. In 1985–86, after 13 wins and two draws in its first 15 matches, the club was favourite to win the league, but finished in fourth place. The following season, with the club in danger of relegation by November, Atkinson was dismissed.

Alex Ferguson and his assistant Archie Knox arrived from Aberdeen on the day of Atkinson's dismissal, and guided the club to an 11th-place finish in the league. Despite a second-place finish in 1987–88, the club was back in 11th place the following season. Reportedly on the verge of being dismissed, victory over Crystal Palace in the 1990 FA Cup Final replay (after a 3–3 draw) saved Ferguson's career. The following season, Manchester United claimed its first Cup Winners' Cup title and competed in the 1991 UEFA Super Cup, beating European Cup holders Red Star Belgrade 1–0 in the final at Old Trafford. A second consecutive League Cup final appearance followed in 1992, in which the team beat Nottingham Forest 1–0 at Wembley. In 1993, the club won its first league title since 1967, and a year later, for the first time since 1957, it won a second consecutive title – alongside the FA Cup – to complete the first "Double" in the club's history.
In the 1998–99 season, Manchester United became the first team to win the Premier League, FA Cup and UEFA Champions League – "The Treble" – in the same season. Losing 1–0 going into injury time in the 1999 UEFA Champions League Final, Teddy Sheringham and Ole Gunnar Solskjær scored late goals to claim a dramatic victory over Bayern Munich, in what is considered one of the greatest comebacks of all time. The club also won the Intercontinental Cup after beating Palmeiras 1–0 in Tokyo. Ferguson was subsequently knighted for his services to football.

Manchester United won the league again in the 1999–2000 and 2000–01 seasons. The team finished third in 2001–02, before regaining the title in 2002–03. They won the 2003–04 FA Cup, beating Millwall 3–0 in the final at the Millennium Stadium in Cardiff to lift the trophy for a record 11th time. In the 2005–06 season, Manchester United failed to qualify for the knockout phase of the UEFA Champions League for the first time in over a decade, but recovered to secure a second-place league finish and victory over Wigan Athletic in the 2006 Football League Cup Final. The club regained the Premier League in the 2006–07 and 2007–08 seasons, and completed the European double by beating Chelsea 6–5 on penalties in the 2008 UEFA Champions League Final in Moscow's Luzhniki Stadium. Ryan Giggs made a record 759th appearance for the club in this game, overtaking previous record holder Bobby Charlton. In December 2008, the club won the 2008 FIFA Club World Cup and followed this with the 2008–09 Football League Cup, and its third successive Premier League title. That summer, Cristiano Ronaldo was sold to Real Madrid for a world record £80 million. In 2010, Manchester United defeated Aston Villa 2–1 at Wembley to retain the League Cup, its first successful defence of a knockout cup competition.

After finishing as runner-up to Chelsea in the 2009–10 season, United achieved a record 19th league title in 2010–11, securing the championship with a 1–1 away draw against Blackburn Rovers on 14 May 2011. This was extended to 20 league titles in 2012–13, securing the championship with a 3–0 home win against Aston Villa on 22 April 2013.

On 8 May 2013, Ferguson announced that he was to retire as manager at the end of the football season, but would remain at the club as a director and club ambassador. The club announced the next day that Everton manager David Moyes would replace him from 1 July, having signed a six-year contract. Ryan Giggs took over as interim player-manager 10 months later, on 22 April 2014, when Moyes was sacked after a poor season in which the club failed to defend their Premier League title and failed to qualify for the UEFA Champions League for the first time since 1995–96. They also failed to qualify for the Europa League, meaning that it was the first time Manchester United hadn't qualified for a European competition since 1990. On 19 May 2014, it was confirmed that Louis van Gaal would replace Moyes as Manchester United manager on a three-year deal, with Giggs as his assistant. Malcolm Glazer, the patriarch of the Glazer family that owns the club, died on 28 May 2014.

Although Van Gaal's first season saw United once again qualify for the Champions League through a fourth-place finish in the Premier League, his second season saw United go out of the same tournament in the group stage. United also fell behind in the title race for the third consecutive season, finishing in 5th place, in spite of several expensive signings during Van Gaal's tenure. However, that same season, Manchester United won the FA Cup for a 12th time, this being their first major trophy won since the departure of Sir Alex Ferguson. Despite this victory, Van Gaal was sacked as manager just two days later, with José Mourinho appointed in his place on 27 May, signing a three-year contract. That season, United finished in sixth place while winning the EFL Cup for the fifth time and the Europa League for the first time, as well as the FA Community Shield for a record 21st time. Despite not finishing in the top four, United qualified for the Champions League through their Europa League win. Wayne Rooney scored his 250th goal with United, surpassing Sir Bobby Charlton as United's all-time top scorer, before leaving the club at the end of the season to return to Everton.

The club crest is derived from the Manchester City Council coat of arms, although all that remains of it on the current crest is the ship in full sail. The devil stems from the club's nickname "The Red Devils"; it was included on club programmes and scarves in the 1960s, and incorporated into the club crest in 1970, although the crest was not included on the chest of the shirt until 1971.

Newton Heath's uniform in 1879, four years before the club played its first competitive match, has been documented as 'white with blue cord'. A photograph of the Newton Heath team, taken in 1892, is believed to show the players wearing red-and-white quartered jerseys and navy blue knickerbockers. Between 1894–96, the players wore distinctive green and gold jerseys which were replaced in 1896 by white shirts, which were worn with navy blue shorts.

After the name change in 1902, the club colours were changed to red shirts, white shorts, and black socks, which has become the standard Manchester United home kit. Very few changes were made to the kit until 1922 when the club adopted white shirts bearing a deep red "V" around the neck, similar to the shirt worn in the 1909 FA Cup Final. They remained part of their home kits until 1927. For a period in 1934, the cherry and white hooped change shirt became the home colours, but the following season the red shirt was recalled after the club's lowest ever league placing of 20th in the Second Division and the hooped shirt dropped back to being the change. The black socks were changed to white from 1959 to 1965, where they were replaced with red socks up until 1971 with white used on occasion, when the club reverted to black. Black shorts and/or white socks were sometimes worn with the home strip, most often in away games, if there is a clash with the opponent's kit. For 2018–19, black shorts and red socks became the primary choice for the home kit. Since 1997–98, white socks have been the preferred choice for European games, which are typically played on weeknights, to aid with player visibility. The current home kit is a red shirt with black stripes gradually getting thicker towards the base and the trademark Adidas three stripes in black on the shoulders, black shorts, and red socks with black tops and gradual stripes in between.

The Manchester United away strip has often been a white shirt, black shorts and white socks, but there have been several exceptions. These include an all-black strip with blue and gold trimmings between 1993 and 1995, the navy blue shirt with silver horizontal pinstripes worn during the 1999–2000 season, and the 2011–12 away kit, which had a royal blue body and sleeves with hoops made of small midnight navy blue and black stripes, with black shorts and blue socks. An all-grey away kit worn during the 1995–96 season was dropped after just five games, most notoriously against Southampton where Alex Ferguson forced the team to change into the third kit during half-time of its final outing. The reason for dropping it being that the players claimed to have trouble finding their teammates against the crowd, United failed to win a competitive game in the kit. In 2001, to celebrate 100 years as "Manchester United", a reversible white/gold away kit was released, although the actual match day shirts were not reversible.

The club's third kit is often all-blue, this was most recently the case during the 2014–15 season. Exceptions include a green-and-gold halved shirt worn between 1992 and 1994, a blue-and-white striped shirt worn during the 1994–95 and 1995–96 seasons and once in 1996–97, an all-black kit worn during the Treble-winning 1998–99 season, and a white shirt with black-and-red horizontal pinstripes worn between 2003–04 and 2005–06. From 2006–07 to 2013–14, the third kit was the previous season's away kit, albeit updated with the new club sponsor in 2006–07 and 2010–11, apart from 2008–09 when an all-blue kit was launch to mark the 40th anniversary of the 1967–68 European Cup success.


Newton Heath initially played on a field on North Road, close to the railway yard; the original capacity was about 12,000, but club officials deemed the facilities inadequate for a club hoping to join The Football League. Some expansion took place in 1887, and in 1891, Newton Heath used its minimal financial reserves to purchase two grandstands, each able to hold 1,000 spectators. Although attendances were not recorded for many of the earliest matches at North Road, the highest documented attendance was approximately 15,000 for a First Division match against Sunderland on 4 March 1893. A similar attendance was also recorded for a friendly match against Gorton Villa on 5 September 1889.

In June 1893, after the club was evicted from North Road by its owners, Manchester Deans and Canons, who felt it was inappropriate for the club to charge an entry fee to the ground, secretary A. H. Albut procured the use of the Bank Street ground in Clayton. It initially had no stands, by the start of the 1893–94 season, two had been built; one spanning the full length of the pitch on one side and the other behind the goal at the "Bradford end". At the opposite end, the "Clayton end", the ground had been "built up, thousands thus being provided for". Newton Heath's first league match at Bank Street was played against Burnley on 1 September 1893, when 10,000 people saw Alf Farman score a hat-trick, Newton Heath's only goals in a 3–2 win. The remaining stands were completed for the following league game against Nottingham Forest three weeks later. In October 1895, before the visit of Manchester City, the club purchased a 2,000-capacity stand from the Broughton Rangers rugby league club, and put up another stand on the "reserved side" (as distinct from the "popular side"). However, weather restricted the attendance for the Manchester City match to just 12,000.

When the Bank Street ground was temporarily closed by bailiffs in 1902, club captain Harry Stafford raised enough money to pay for the club's next away game at Bristol City and found a temporary ground at Harpurhey for the next reserves game against Padiham. Following financial investment, new club president John Henry Davies paid £500 for the erection of a new 1,000-seat stand at Bank Street. Within four years, the stadium had cover on all four sides, as well as the ability to hold approximately 50,000 spectators, some of whom could watch from the viewing gallery atop the Main Stand.

However, following Manchester United's first league title in 1908 and the FA Cup a year later, it was decided that Bank Street was too restrictive for Davies' ambition; in February 1909, six weeks before the club's first FA Cup title, Old Trafford was named as the home of Manchester United, following the purchase of land for around £60,000. Architect Archibald Leitch was given a budget of £30,000 for construction; original plans called for seating capacity of 100,000, though budget constraints forced a revision to 77,000. The building was constructed by Messrs Brameld and Smith of Manchester. The stadium's record attendance was registered on 25 March 1939, when an FA Cup semi-final between Wolverhampton Wanderers and Grimsby Town drew 76,962 spectators.

Bombing in the Second World War destroyed much of the stadium; the central tunnel in the South Stand was all that remained of that quarter. After the war, the club received compensation from the War Damage Commission in the amount of £22,278. While reconstruction took place, the team played its "home" games at Manchester City's Maine Road ground; Manchester United was charged £5,000 per year, plus a nominal percentage of gate receipts. Later improvements included the addition of roofs, first to the Stretford End and then to the North and East Stands. The roofs were supported by pillars that obstructed many fans' views, and they were eventually replaced with a cantilevered structure. The Stretford End was the last stand to receive a cantilevered roof, completed in time for the 1993–94 season. First used on 25 March 1957 and costing £40,000, four pylons were erected, each housing 54 individual floodlights. These were dismantled in 1987 and replaced by a lighting system embedded in the roof of each stand, which remains in use today.

The Taylor Report's requirement for an all-seater stadium lowered capacity at Old Trafford to around 44,000 by 1993. In 1995, the North Stand was redeveloped into three tiers, restoring capacity to approximately 55,000. At the end of the 1998–99 season, second tiers were added to the East and West Stands, raising capacity to around 67,000, and between July 2005 and May 2006, 8,000 more seats were added via second tiers in the north-west and north-east quadrants. Part of the new seating was used for the first time on 26 March 2006, when an attendance of 69,070 became a new Premier League record. The record was pushed steadily upwards before reaching its peak on 31 March 2007, when 76,098 spectators saw Manchester United beat Blackburn Rovers 4–1, with just 114 seats (0.15 per cent of the total capacity of 76,212) unoccupied. In 2009, reorganisation of the seating resulted in a reduction of capacity by 255 to 75,957. Manchester United has the second highest average attendance of European football clubs only behind Borussia Dortmund.

Manchester United is one of the most popular football clubs in the world, with one of the highest average home attendance in Europe. The club states that its worldwide fan base includes more than 200 officially recognised branches of the Manchester United Supporters Club (MUSC), in at least 24 countries. The club takes advantage of this support through its worldwide summer tours. Accountancy firm and sports industry consultants Deloitte estimate that Manchester United has 75 million fans worldwide, while other estimates put this figure closer to 333 million. The club has the third highest social media following in the world among sports teams (after Barcelona and Real Madrid), with over 71 million Facebook fans as of September 2016. A 2014 study showed that Manchester United had the loudest fans in the Premier League.

Supporters are represented by two independent bodies; the Independent Manchester United Supporters' Association (IMUSA), which maintains close links to the club through the MUFC Fans Forum, and the Manchester United Supporters' Trust (MUST). After the Glazer family's takeover in 2005, a group of fans formed a splinter club, F.C. United of Manchester. The West Stand of Old Trafford – the "Stretford End" – is the home end and the traditional source of the club's most vocal support.

Manchester United has rivalries with Arsenal, Leeds United, Liverpool, and Manchester City, against whom they contest the Manchester derby.

The rivalry with Liverpool is rooted in competition between the cities during the Industrial Revolution when Manchester was famous for its textile industry while Liverpool was a major port. The two clubs are the most successful English teams in both domestic and European competitions; and between them they have won 38 league titles, 8 European Cups, 4 UEFA Cups, 4 UEFA Super Cups, 19 FA Cups, 13 League Cups, 1 FIFA Club World Cup, 1 Intercontinental Cup and 36 FA Community Shields. It is considered to be one of the biggest rivalries in the association football world and is considered the most famous fixture in English football.

The "Roses Rivalry" with Leeds stems from the Wars of the Roses, fought between the House of Lancaster and the House of York, with Manchester United representing Lancashire and Leeds representing Yorkshire.

The rivalry with Arsenal arises from the numerous times the two teams, as well as managers Alex Ferguson and Arsène Wenger, have battled for the Premier League title. With 33 titles between them (20 for Manchester United, 13 for Arsenal) this fixture has become known as one of the finest Premier League match-ups in history.

Manchester United has been described as a global brand; a 2011 report by Brand Finance, valued the club's trademarks and associated intellectual property at £412 million – an increase of £39 million on the previous year, valuing it at £11 million more than the second best brand, Real Madrid – and gave the brand a strength rating of AAA (Extremely Strong). In July 2012, Manchester United was ranked first by "Forbes" magazine in its list of the ten most valuable sports team brands, valuing the Manchester United brand at $2.23 billion. The club is ranked third in the Deloitte Football Money League (behind Real Madrid and Barcelona). In January 2013, the club became the first sports team in the world to be valued at $3 billion. Forbes Magazine valued the club at $3.3 billion – $1.2 billion higher than the next most valuable sports team. They were overtaken by Real Madrid for the next four years, but Manchester United returned to the top of the Forbes list in June 2017, with a valuation of $3.689 billion.

The core strength of Manchester United's global brand is often attributed to Matt Busby's rebuilding of the team and subsequent success following the Munich air disaster, which drew worldwide acclaim. The "iconic" team included Bobby Charlton and Nobby Stiles (members of England's World Cup winning team), Denis Law and George Best. The attacking style of play adopted by this team (in contrast to the defensive-minded "catenaccio" approach favoured by the leading Italian teams of the era) "captured the imagination of the English footballing public". Busby's team also became associated with the liberalisation of Western society during the 1960s; George Best, known as the "Fifth Beatle" for his iconic haircut, was the first footballer to significantly develop an off-the-field media profile.

As the second English football club to float on the London Stock Exchange in 1991, the club raised significant capital, with which it further developed its commercial strategy. The club's focus on commercial and sporting success brought significant profits in an industry often characterised by chronic losses. The strength of the Manchester United brand was bolstered by intense off-the-field media attention to individual players, most notably David Beckham (who quickly developed his own global brand). This attention often generates greater interest in on-the-field activities, and hence generates sponsorship opportunities – the value of which is driven by television exposure. During his time with the club, Beckham's popularity across Asia was integral to the club's commercial success in that part of the world.

Because higher league placement results in a greater share of television rights, success on the field generates greater income for the club. Since the inception of the Premier League, Manchester United has received the largest share of the revenue generated from the BSkyB broadcasting deal. Manchester United has also consistently enjoyed the highest commercial income of any English club; in 2005–06, the club's commercial arm generated £51 million, compared to £42.5 million at Chelsea, £39.3 million at Liverpool, £34 million at Arsenal and £27.9 million at Newcastle United. A key sponsorship relationship was with sportswear company Nike, who managed the club's merchandising operation as part of a £303 million 13-year partnership between 2002 and 2015. Through Manchester United Finance and the club's membership scheme, One United, those with an affinity for the club can purchase a range of branded goods and services. Additionally, Manchester United-branded media services – such as the club's dedicated television channel, MUTV – have allowed the club to expand its fan base to those beyond the reach of its Old Trafford stadium.

In an initial five-year deal worth £500,000, Sharp Electronics became the club's first shirt sponsor at the beginning of the 1982–83 season, a relationship that lasted until the end of the 1999–2000 season, when Vodafone agreed a four-year, £30 million deal. Vodafone agreed to pay £36 million to extend the deal by four years, but after two seasons triggered a break clause in order to concentrate on its sponsorship of the Champions League.

To commence at the start of the 2006–07 season, American insurance corporation AIG agreed a four-year £56.5 million deal which in September 2006 became the most valuable in the world. At the beginning of the 2010–11 season, American reinsurance company Aon became the club's principal sponsor in a four-year deal reputed to be worth approximately £80 million, making it the most lucrative shirt sponsorship deal in football history. Manchester United announced their first training kit sponsor in August 2011, agreeing a four-year deal with DHL reported to be worth £40 million; it is believed to be the first instance of training kit sponsorship in English football. The DHL contract lasted for over a year before the club bought back the contract in October 2012, although they remained the club's official logistics partner. The contract for the training kit sponsorship was then sold to Aon in April 2013 for a deal worth £180 million over eight years, which also included purchasing the naming rights for the Trafford Training Centre.

The club's first kit manufacturer was Umbro, until a five-year deal was agreed with Admiral Sportswear in 1975. Adidas received the contract in 1980, before Umbro started a second spell in 1992. Umbro's sponsorship lasted for ten years, followed by Nike's record-breaking £302.9 million deal that lasted until 2015; 3.8 million replica shirts were sold in the first 22 months with the company. In addition to Nike and Chevrolet, the club also has several lower-level "platinum" sponsors, including Aon and Budweiser.

On 30 July 2012, United signed a seven-year deal with American automotive corporation General Motors, which replaced Aon as the shirt sponsor from the 2014–15 season. The new $80m-a-year shirt deal is worth $559m over seven years and features the logo of General Motors brand Chevrolet. Nike announced that they would not renew their kit supply deal with Manchester United after the 2014–15 season, citing rising costs. Since the start of the 2015–16 season, Adidas has manufactured Manchester United's kit as part of a world-record 10-year deal worth a minimum of £750 million. Plumbing products manufacturer Kohler became the club's first sleeve sponsor ahead of the 2018–19 season.

Originally funded by the Lancashire and Yorkshire Railway Company, the club became a limited company in 1892 and sold shares to local supporters for £1 via an application form. In 1902, majority ownership passed to the four local businessmen who invested £500 to save the club from bankruptcy, including future club president John Henry Davies. After his death in 1927, the club faced bankruptcy yet again, but was saved in December 1931 by James W. Gibson, who assumed control of the club after an investment of £2,000. Gibson promoted his son, Alan, to the board in 1948, but died three years later; the Gibson family retained ownership of the club through James' wife, Lillian, but the position of chairman passed to former player Harold Hardman.

Promoted to the board a few days after the Munich air disaster, Louis Edwards, a friend of Matt Busby, began acquiring shares in the club; for an investment of approximately £40,000, he accumulated a 54 per cent shareholding and took control in January 1964. When Lillian Gibson died in January 1971, her shares passed to Alan Gibson who sold a percentage of his shares to Louis Edwards' son, Martin, in 1978; Martin Edwards went on to become chairman upon his father's death in 1980. Media tycoon Robert Maxwell attempted to buy the club in 1984, but did not meet Edwards' asking price. In 1989, chairman Martin Edwards attempted to sell the club to Michael Knighton for £20 million, but the sale fell through and Knighton joined the board of directors instead.

Manchester United was floated on the stock market in June 1991 (raising £6.7 million), and received yet another takeover bid in 1998, this time from Rupert Murdoch's British Sky Broadcasting Corporation. This resulted in the formation of "Shareholders United Against Murdoch" – now the "Manchester United Supporters' Trust" – who encouraged supporters to buy shares in the club in an attempt to block any hostile takeover. The Manchester United board accepted a £623 million offer, but the takeover was blocked by the Monopolies and Mergers Commission at the final hurdle in April 1999. A few years later, a power struggle emerged between the club's manager, Alex Ferguson, and his horse-racing partners, John Magnier and J. P. McManus, who had gradually become the majority shareholders. In a dispute that stemmed from contested ownership of the horse Rock of Gibraltar, Magnier and McManus attempted to have Ferguson removed from his position as manager, and the board responded by approaching investors to attempt to reduce the Irishmen's majority.

In May 2005, Malcolm Glazer purchased the 28.7 per cent stake held by McManus and Magnier, thus acquiring a controlling interest through his investment vehicle Red Football Ltd in a highly leveraged takeover valuing the club at approximately £800 million (then approx. $1.5 billion). Once the purchase was complete, the club was taken off the stock exchange. In July 2006, the club announced a £660 million debt refinancing package, resulting in a 30 per cent reduction in annual interest payments to £62 million a year. In January 2010, with debts of £716.5 million ($1.17 billion), Manchester United further refinanced through a bond issue worth £504 million, enabling them to pay off most of the £509 million owed to international banks. The annual interest payable on the bonds – which mature on 1 February 2017 – is approximately £45 million per annum. Despite restructuring, the club's debt prompted protests from fans on 23 January 2010, at Old Trafford and the club's Trafford Training Centre. Supporter groups encouraged match-going fans to wear green and gold, the colours of Newton Heath. On 30 January, reports emerged that the Manchester United Supporters' Trust had held meetings with a group of wealthy fans, dubbed the "Red Knights", with plans to buying out the Glazers' controlling interest.

In August 2011, the Glazers were believed to have approached Credit Suisse in preparation for a $1 billion (approx. £600 million) initial public offering (IPO) on the Singapore stock exchange that would value the club at more than £2 billion. However, in July 2012, the club announced plans to list its IPO on the New York Stock Exchange instead. Shares were originally set to go on sale for between $16 and $20 each, but the price was cut to $14 by the launch of the IPO on 10 August, following negative comments from Wall Street analysts and Facebook's disappointing stock market debut in May. Even after the cut, Manchester United was valued at $2.3 billion, making it the most valuable football club in the world.







Manchester United are one of the most successful clubs in Europe in terms of trophies won. The club's first trophy was the Manchester Cup, which it won as Newton Heath LYR in 1886. In 1908, the club won its first league title, and won the FA Cup for the first time the following year. Manchester United won the most trophies in the 1990s; five league titles, four FA Cups, one League Cup, five Charity Shields (one shared), one UEFA Champions League, one UEFA Cup Winners' Cup, one UEFA Super Cup and one Intercontinental Cup.

The club holds the record for most top-division titles (20) – including a record 13 Premier League titles – and FA Community Shields (21). It was also the first English club to win the European Cup in 1968, and, , is the only British club to have won the Club World Cup, in 2008. United also became the sole British club to win the Intercontinental Cup, in 1999. The club's most recent trophy came in May 2017, with the 2016–17 UEFA Europa League.

In winning the 2016–17 UEFA Europa League, United became the fifth club in history to have won the "European Treble" of European Cup/UEFA Champions League, European Cup Winners' Cup/UEFA Cup Winners' Cup, and UEFA Cup/UEFA Europa League after Juventus, Ajax, Bayern Munich and Chelsea.






Especially short competitions such as the Charity/Community Shield, Intercontinental Cup (now defunct), FIFA Club World Cup or UEFA Super Cup are not generally considered to contribute towards a Double or Treble.

Manchester United formed a women's football team in 2018.





</doc>
<doc id="19962" url="https://en.wikipedia.org/wiki?curid=19962" title="Mesa (programming language)">
Mesa (programming language)

Mesa is a programming language developed in the late 1970s at the Xerox Palo Alto Research Center in Palo Alto, California, United States. The language name was a pun based upon the programming language catchphrases of the time, because Mesa is a "high level" programming language.

Mesa is an ALGOL-like language with strong support for modular programming. Every library module has at least two source files: a "definitions" file specifying the library's interface plus one or more "program" files specifying the implementation of the procedures in the interface. To use a library, a program or higher-level library must "import" the definitions. The Mesa compiler type-checks all uses of imported entities; this combination of separate compilation with type-checking was unusual at the time.

Mesa introduced several other innovations in language design and implementation, notably in the handling of software exceptions, thread synchronization, and incremental compilation.

Mesa was developed on the Xerox Alto, one of the first personal computers with a graphical user interface, however most of the Alto's system software was written in BCPL. Mesa was the system programming language of the later Xerox Star workstations, and for the GlobalView desktop environment. Xerox PARC later developed Cedar, which was a superset of Mesa.

Mesa and Cedar had a major influence on the design of other important languages, such as Modula-2 and Java, and was an important vehicle for the development and dissemination of the fundamentals of GUIs, networked environments, and the other advances Xerox contributed to the field of computer science.

Mesa was originally designed in the Computer Systems Laboratory (CSL), a branch of the Xerox Palo Alto Research Center, for the Alto, an experimental micro-coded workstation. Initially its spread was confined to PARC and a few universities to which Xerox had donated some Altos.

Mesa was later adopted as the systems programming language for Xerox's commercial workstations such as the Xerox 8010 (Xerox Star, Dandelion) and Xerox 6085 (Daybreak), in particular for the Pilot operating system.

A secondary development environment, called the Xerox Development Environment (XDE) allowed developers to debug both the operating system Pilot as well as ViewPoint GUI applications using a world swap mechanism. This allowed the entire "state" of the world to be swapped out, and allowed low level system crashes which paralyzed the whole system to be debugged. This technique did not scale very well to large application images (several megabytes), and so the Pilot/Mesa world in later releases moved away from the world swap view when the micro-coded machines were phased out in favor of SPARC workstations and Intel PCs running a Mesa PrincOps emulator for the basic hardware instruction set.

Mesa was compiled into a stack-machine language, purportedly with the highest code density ever achieved (roughly 4 bytes per high-level language statement). This was touted in a 1981 paper where implementors from the Xerox Systems Development Department (then, the development arm of PARC), tuned up the instruction set and published a paper on the resultant code density.

Mesa was taught via the Mesa Programming Course that took people through the wide range of technology Xerox had available at the time and ended with the programmer writing a "hack", a workable program designed to be useful. An actual example of such a hack is the BWSMagnifier, which was written in 1988 and allowed people to magnify sections of the workstation screen as defined by a resizable window and a changeable magnification factor. Trained Mesa programmers from Xerox were well versed in the fundamental of GUIs, networking, exceptions, and multi-threaded programming, almost a decade before they became standard tools of the trade.

Within Xerox, Mesa was eventually superseded by the Cedar programming language. Many Mesa programmers and developers left Xerox in 1985; some of them went to DEC Systems Research Center where they used their experience with Mesa in the design of Modula-2+, and later of Modula-3.

Mesa was a strongly typed programming language with type-checking across module boundaries, but with enough flexibility in its type system that heap allocators could be written in Mesa.

Because of its strict separation between interface and implementation, Mesa allows true incremental compilation and encourages architecture- and platform-independent programming. They also simplified source-level debugging, including remote debugging via the Ethernet.

Mesa had rich exception handling facilities, with four types of exceptions. It had support for thread synchronization via monitors. Mesa was the first language to implement monitor BROADCAST, a concept introduced by the Pilot operating system.

Mesa has an "imperative" and "algebraic" syntax, based on ALGOL and Pascal rather than on BCPL or C; for instance, compound commands are indicated by the and keywords rather than braces. In Mesa, all keywords are written in uppercase.

Due to a peculiarity of the ASCII variant used at PARC, the Alto's character set included a left-pointing arrow (←) rather than an underscore. The result of this is that Alto programmers (including those using Mesa, Smalltalk etc.) conventionally used CamelCase for compound identifiers, a practice which was incorporated in PARC's standard programming style. On the other hand, the availability of the left-pointing arrow allowed them to use it for the assignment operator, as it originally had been in ALGOL.

When the Mesa designers wanted to implement an exception facility, they hired a recent M.S. graduate from Colorado who had written his thesis on exception handling facilities in algorithmic languages. This led to the richest exception facility for its time, with primitives , , , , , and . Because the language did not have type-safe checks to verify full coverage for signal handling, uncaught exceptions were a common cause of bugs in released software.

Xerox PARC later developed Cedar, which was a superset of Mesa, with a number of additions including garbage collection; better string support, called Ropes; and later a native compiler for Sun SPARC workstations. Most importantly, Cedar contained a type-safe subset and the compiler had a subset-checking mode to ensure deterministic execution and no memory leaks from conformant Cedar code.

Mesa was the precursor to the programming language Cedar. Cedar's main additions were garbage collection, dynamic types, a limited form of type parameterization, and special syntax to identify the "type-safe" parts of a multi-module software package.





</doc>
<doc id="19963" url="https://en.wikipedia.org/wiki?curid=19963" title="Marsilio Ficino">
Marsilio Ficino

Marsilio Ficino (; Latin name: "Marsilius Ficinus"; 19 October 1433 – 1 October 1499) was an Italian scholar and Catholic priest who was one of the most influential humanist philosophers of the early Italian Renaissance. He was an astrologer, a reviver of Neoplatonism in touch with the major academics of his day and the first translator of Plato's complete extant works into Latin. His Florentine Academy, an attempt to revive Plato's Academy, influenced the direction and tenor of the Italian Renaissance and the development of European philosophy.

Ficino was born at Figline Valdarno. His father Diotifeci d'Agnolo was a physician under the patronage of Cosimo de' Medici, who took the young man into his household and became the lifelong patron of Marsilio, who was made tutor to his grandson, Lorenzo de' Medici. Giovanni Pico della Mirandola, the Italian humanist philosopher and scholar was another of his students.

During the sessions at Florence of the Council of Ferrara-Florence in 1438–1445, during the failed attempts to heal the schism of the Orthodox and Catholic churches, Cosimo de' Medici and his intellectual circle had made acquaintance with the Neoplatonic philosopher George Gemistos Plethon, whose discourses upon Plato and the Alexandrian mystics so fascinated the learned society of Florence that they named him the second Plato. In 1459 John Argyropoulos was lecturing on Greek language and literature at Florence, and Ficino became his pupil.

When Cosimo decided to refound Plato's Academy at Florence he chose Ficino as its head. In 1462, Cosimo supplied Ficino with Greek manuscripts of Plato's work, whereupon Ficino started translating the entire corpus to Latin (draft translation of the dialogues finished 1468–9; published 1484). Ficino also produced a translation of a collection of Hellenistic Greek documents found by Leonardo da Pistoia later called Hermetica, and the writings of many of the Neoplatonists, including Porphyry, Iamblichus and Plotinus.

Among his many students was Francesco Cattani da Diacceto, who was considered by Ficino to be his successor as the head of the Florentine Platonic Academy. Diacceto's student, Giovanni di Bardo Corsi, produced a short biography of Ficino in 1506.

A physician and a vegetarian, Ficino became a priest in 1473.

In 1474 Ficino completed his treatise on the immortality of the soul, "Theologia Platonica de immortalitate animae" (Platonic Theology). In the rush of enthusiasm for every rediscovery from Antiquity, he exhibited a great interest in the arts of astrology, which landed him in trouble with the Roman Catholic Church. In 1489 he was accused of magic before Pope Innocent VIII and needed strong defense to preserve him from the condemnation of heresy.

Writing in 1492 Ficino proclaimed: "This century, like a golden age, has restored to light the liberal arts, which were almost extinct: grammar, poetry, rhetoric, painting, sculpture, architecture, music ... this century appears to have perfected astrology."

Ficino's letters, extending over the years 1474–1494, survive and have been published. He wrote "De amore" (1484). "De vita libri tres" (Three books on life), or "De triplici vita", published in 1489, provides a great deal of medical and astrological advice for maintaining health and vigor, as well as espousing the Neoplatonist view of the world's ensoulment and its integration with the human soul:
One metaphor for this integrated "aliveness" is Ficino's astrology. In the "Book of Life", he details the interlinks between behavior and consequence. It talks about a list of things that hold sway over a man's destiny.

Probably due to early influences from his father Diotifeci, who was a doctor to Cosimo de' Medici, Ficino published Latin and Italian treatises on medical subjects such as "Consiglio contro la pestilenza" (Recommendations for the treatment of the plague) and "De vita libri tres" (Three books on life). His medical works exerted considerable influence on Renaissance physicians such as Paracelsus, with whom he shared the perception on the unity of the micro- and macrocosmos, and their interactions, through somatic and psychological manifestations, with the aim to investigate their signatures to cure diseases. Those works, which were very popular at the time, dealt with astrological and alchemical concepts. Thus Ficino came under the suspicion of heresy; especially after the publication of the third book in 1489, which contained specific instructions on healthful living.

Ficino introduced the term and concept of "platonic love" in the West. It first appeared in a letter to Alamanno Donati in 1476, but was later fully developed all along his work, mainly his famous "De amore". He also practiced this love metaphysic with Giovanni Cavalcanti, whom he made the principal character in his commentary on the "Convivio", and to whom he wrote ardent love letters in Latin that were published in his "Epistulae" in 1492; there are also numerous other indications to suggest that Ficino's erotic impulses were directed exclusively towards men. After his death his biographers had a difficult task trying to refute those who spoke of his homosexual tendencies. But his sincere and deep faith, and membership of the clergy, put him beyond the reach of gossip, and while praising love for the same sex, he also condemned sodomy in the "Convivium". His Latin translations of Plato's texts put into practice the theories of anti-homosexuality in his "Convivium".

Ficino died on 1 October 1499 at Careggi. In 1521 his memory was honored with a bust sculpted by Andrea Ferrucci, which is located in the south side of the nave in the Cathedral of Santa Maria del Fiore.







</doc>
<doc id="19965" url="https://en.wikipedia.org/wiki?curid=19965" title="Morphogenesis">
Morphogenesis

Morphogenesis (from the Greek "morphê" shape and "genesis" creation, literally, "beginning of the shape") is the biological process that causes an organism to develop its shape. It is one of three fundamental aspects of developmental biology along with the control of cell growth and cellular differentiation, unified in evolutionary developmental biology (evo-devo).

The process controls the organized spatial distribution of cells during the embryonic development of an organism. Morphogenesis can take place also in a mature organism, in cell culture or inside tumor cell masses. Morphogenesis also describes the development of unicellular life forms that do not have an embryonic stage in their life cycle, or describes the evolution of a body structure within a taxonomic group.

Morphogenetic responses may be induced in organisms by hormones, by environmental chemicals ranging from substances produced by other organisms to toxic chemicals or radionuclides released as pollutants, and other plants, or by mechanical stresses induced by spatial patterning of the cells.

Some of the earliest ideas and mathematical descriptions on how physical processes and constraints affect biological growth, and hence natural patterns such as the spirals of phyllotaxis, were written by D'Arcy Wentworth Thompson in his 1917 book "On Growth and Form" and Alan Turing in his "The Chemical Basis of Morphogenesis" (1952). Where Thompson explained animal body shapes as being created by varying rates of growth in different directions, for instance to create the spiral shell of a snail, Turing correctly predicted a mechanism of morphogenesis, the diffusion of two different chemical signals, one activating and one deactivating growth, to set up patterns of development, decades before the formation of such patterns was observed. The fuller understanding of the mechanisms involved in actual organisms required the discovery of the structure of DNA in 1953, and the development of molecular biology and biochemistry.

Several types of molecules are important in morphogenesis. Morphogens are soluble molecules that can diffuse and carry signals that control cell differentiation via concentration gradients. Morphogens typically act through binding to specific protein receptors. An important class of molecules involved in morphogenesis are transcription factor proteins that determine the fate of cells by interacting with DNA. These can be coded for by master regulatory genes, and either activate or deactivate the transcription of other genes; in turn, these secondary gene products can regulate the expression of still other genes in a regulatory cascade of gene regulatory networks. At the end of this cascade are classes of molecules that control cellular behaviors such as cell migration, or, more generally, their properties, such as cell adhesion or cell contractility. For example, during gastrulation, clumps of stem cells switch off their cell-to-cell adhesion, become migratory, and take up new positions within an embryo where they again activate specific cell adhesion proteins and form new tissues and organs. Developmental signaling pathways implicated in morphogenesis include Wnt, Hedgehog, and ephrins.

At a tissue level, ignoring the means of control, morphogenesis arises because of cellular proliferation and motility. Morphogenesis also involves changes in the cellular structure or how cells interact in tissues. These changes can result in tissue elongation, thinning, folding, invasion or separation of one tissue into distinct layers. The latter case is often referred as cell sorting. Cell "sorting out" consists of cells moving so as to sort into clusters that maximize contact between cells of the same type. The ability of cells to do this has been proposed to arise from differential cell adhesion by Malcolm Steinberg through his Differential Adhesion Hypothesis. Tissue separation can also occur via more dramatic cellular differentiation events during which epithelial cells become mesenchymal (see Epithelial-mesenchymal transition). Mesenchymal cells typically leave the epithelial tissue as a consequence of changes in cell adhesive and contractile properties. Following epithelial-mesenchymal transition, cells can migrate away from an epithelium and then associate with other similar cells in a new location.

During embryonic development, cells are restricted to different layers due to differential affinities. One of the ways this can occur is when cells share the same cell-to-cell adhesion molecules. For instance, homotypic cell adhesion can maintain boundaries between groups of cells that have different adhesion molecules. Furthermore, cells can sort based upon differences in adhesion between the cells, so even two populations of cells with different levels of the same adhesion molecule can sort out. In cell culture cells that have the strongest adhesion move to the center of a mixed aggregates of cells. Moreover, cell-cell adhesion is often modulated by cell contractility, which can exert forces on the cell-cell contacts so that two cell populations with equal levels of the same adhesion molecule can sort out. The molecules responsible for adhesion are called cell adhesion molecules (CAMs). Several types of cell adhesion molecules are known and one major class of these molecules are cadherins. There are dozens of different cadherins that are expressed on different cell types. Cadherins bind to other cadherins in a like-to-like manner: E-cadherin (found on many epithelial cells) binds preferentially to other E-cadherin molecules. Mesenchymal cells usually express other cadherin types such as N-cadherin.

The extracellular matrix (ECM) is involved in keeping tissues separated, providing structural support or providing a structure for cells to migrate on. Collagen, laminin, and fibronectin are major ECM molecules that are secreted and assembled into sheets, fibers, and gels. Multisubunit transmembrane receptors called integrins are used to bind to the ECM. Integrins bind extracellularly to fibronectin, laminin, or other ECM components, and intracellularly to microfilament-binding proteins α-actinin and talin to link the cytoskeleton with the outside. Integrins also serve as receptors to trigger signal transduction cascades when binding to the ECM. A well-studied example of morphogenesis that involves ECM is mammary gland ductal branching.

Tissues can change their shape and separate into distinct layers via cell contractility. Just as in muscle cells, myosin can contract different parts of the cytoplasm to change its shape or structure. Myosin-driven contractility in embryonic tissue morphogenesis is seen during the separation of germ layers in the model organisms "Caenorhabditis elegans", "Drosophila" and zebrafish. There are often periodic pulses of contraction in embryonic morphogenesis. A model called the cell state splitter involves alternating cell contraction and expansion, initiated by a bistable organelle at the apical end of each cell. The organelle consists of microtubules and microfilaments in mechanical opposition. It responds to local mechanical perturbations caused by morphogenetic movements. These then trigger traveling embryonic differentiation waves of contraction or expansion over presumptive tissues that determine cell type and is followed by cell differentiation. The cell state splitter was first proposed to explain neural plate morphogenesis during gastrulation of the axolotl and the model was later generalized to all of morphogenesis.

Cancer can result from disruption of normal morphogenesis, including both tumor formation and tumor metastasis. Mitochondrial dysfunction can result in increased cancer risk due to disturbed morphogen signaling.

a. Thompson's book is often cited. An abridged version, comprising 349 pages, remains in print and readily obtainable.
An unabridged version, comprising 1116 pages, has also been published.




</doc>
<doc id="19967" url="https://en.wikipedia.org/wiki?curid=19967" title="Medium">
Medium

Medium may refer to:











</doc>
<doc id="19971" url="https://en.wikipedia.org/wiki?curid=19971" title="MCA">
MCA

MCA may refer to:













</doc>
<doc id="19972" url="https://en.wikipedia.org/wiki?curid=19972" title="Magical organization">
Magical organization

A magical organization or magical order is an organization created for the practice of magic or to further the knowledge of magic among its members. "Magic" in this case refers to occult, metaphysical and paranormal activities, not to the performance of stage magic. Magical organizations can include hermetic orders, Wiccan covens or Wiccan circles, esoteric societies, arcane colleges, witches' covens, and other groups which may use different terminology and similar though diverse practices.

The Hermetic Order of the Golden Dawn has been credited with a vast revival of occult literature and practices and was founded in 1887 or 1888 by William Wynn Westcott, Samuel Liddell MacGregor Mathers and William Robert Woodman. The teachings of the Order include Enochian magic, Christian mysticism, Qabalah, Hermeticism, the paganism of ancient Egypt, theurgy, and alchemy.

Ordo Templi Orientis was founded by Carl Kellner in 1895. The Order was reworked by Aleister Crowley after he took control of the Order in the early 1920s.

Ordo Aurum Solis, founded in 1897, is a Western Mystery Tradition group teaching Hermetic Qabalah. Its rituals and system are different from the more popular Golden Dawn, this is because the group follows the ogdoadic tradition instead of rosicrucianism.

The A∴A∴ was created in 1907 by Aleister Crowley and also teaches magick and Thelema. Their main text is "The Book of the Law".

Builders of the Adytum (or B.O.T.A.) was created in 1922 by Paul Foster Case and was extended by Dr. Ann Davies. It teaches Hermetic Qabalah, astrology and occult tarot.

Some (in many cases, equally notable) organizations.






</doc>
<doc id="19975" url="https://en.wikipedia.org/wiki?curid=19975" title="Muhammad ibn Abd al-Wahhab">
Muhammad ibn Abd al-Wahhab

Muhammad ibn Abd al-Wahhab (; ; 1703 – 22 June 1792) was a religious leader and theologian from Najd in central Arabia who founded the movement now called Wahhabism. Born to a family of jurists, Ibn 'Abd al-Wahhab's early education consisted of learning a fairly standard curriculum of orthodox jurisprudence according to the Hanbali school of law, which was the school of law most prevalent in his area of birth. Despite his initial rudimentary training in classical Sunni Muslim tradition, Ibn 'Abd al-Wahhab gradually became opposed to many of the most popular Sunni practices such as the visitation to and the veneration of the tombs of saints, which he felt amounted to heretical religious innovation or even idolatry. Despite his teachings being rejected and opposed by many of the most notable Sunni Muslim scholars of the period, including his own father and brother, Ibn 'Abd al-Wahhab charted a religio-political pact with Muhammad bin Saud to help him to establish the Emirate of Diriyah, the first Saudi state, and began a dynastic alliance and power-sharing arrangement between their families which continues to the present day in the Kingdom of Saudi Arabia. The Al ash-Sheikh, Saudi Arabia's leading religious family, are the descendants of Ibn ʿAbd al-Wahhab, and have historically led the ulama in the Saudi state, dominating the state's clerical institutions.

Ibn 'Abd al-Wahhab is generally acknowledged to have been born in 1703 into the sedentary and impoverished Arab clan of Banu Tamim in 'Uyayna, a village in the Najd region of central Arabia. Before the emergence of Wahhabism there was a very limited history of Islamic education in the area. For this reason, Ibn 'Abd al-Wahhab had modest access to Islamic education during his youth. Despite this, the area had nevertheless produced several notable jurists of the Hanbali school of orthodox Sunni jurisprudence, which was the school of law most prominently practiced in the area. In fact, Ibn 'Abd-al-Wahhab's own family "had produced several doctors of the school," with his father, Sulaymān b. Muḥammad, having been the Hanbali jurisconsult of the Najd and his grandfather, ʿAbd al-Wahhāb, having been a judge of Hanbali law.

Ibn 'Abd-al-Wahhab's early education consisted of learning the Quran by heart and studying a rudimentary level of Hanbali jurisprudence and theology as outlined in the works of Ibn Qudamah (d. 1223), one of the most influential medieval representatives of the Hanbali school whose works were regarded "as having great authority" in the Najd. As the veneration of saints and the belief in their ability to perform miracles by the grace of God had become one of the most omnipresent and established aspects of Sunni Muslim practice throughout the Islamic world, being an agreed upon tenet of the faith by the vast majority of the classical scholars, it was not long before Ibn 'Abd-al-Wahhab began to encounter the omnipresence of saint-veneration in his area as well; and it is probable that he chose to leave Najd and look elsewhere for studies in order to see if the honoring of saints was as popular in the neighboring places of the Muslim world.

After leaving 'Uyayna, Ibn 'Abd al-Wahhab performed the Greater Pilgrimage in Mecca, where the scholars appear to have held opinions and espoused teachings that were unpalatable to him. After this, he went to Medina, the stay at which seems to have been "decisive in shaping the later direction of his thought." In Medina, he met a Hanbali theologian from Najd named ʿAbd Allāh b. Ibrāhīm al-Najdī, who had been a supporter of the neo-Hanbali works of Ibn Taymiyyah (d. 1328), the controversial medieval scholar whose teachings had been considered heterodox and misguided on a number of important points by the vast majority of Sunni Muslim scholars up to that point in history.

Ibn 'Abd al-Wahhab's teacher Abdallah ibn Ibrahim ibn Sayf introduced the relatively young man to Mohammad Hayya Al-Sindhi in Medina who belonged to the Naqshbandi order (tariqa) of Sufism and recommended him as a student. Mohammad Ibn Abd-al-Wahhab and al-Sindhi became very close and Mohammad Ibn Abd-al-Wahhab stayed with him for some time. Muhammad Hayya also taught Mohammad Ibn ʿAbd-al-Wahhab to reject popular religious practices associated with walis and their tombs that resembles later Wahhabi teachings.

Following his early education in Medina, Ibn Abdul Wahhab traveled outside of the peninsula, venturing first to Basra.

After his return home, Ibn ʿAbd al-Wahhab began to attract followers, including the ruler of 'Uyayna, Uthman ibn Mu'ammar. With Ibn Mu'ammar, Ibn ʿAbd al-Wahhab came to an agreement to support Ibn Mu'ammar's political ambitions to expand his rule "over Najd and possibly beyond", in exchange for the ruler's support for Ibn ʿAbd al-Wahhab's religious teachings. Ibn ʿAbd al-Wahhab began to implement some of his ideas for reform. First, he persuaded Ibn Mu'ammar to help him level the grave of Zayd ibn al-Khattab, a companion of Muhammad, whose grave was revered by locals. Secondly, he ordered the cutting down of trees considered sacred by locals, cutting down "the most glorified of all of the trees" himself. Third, he organised the stoning of a woman who confessed to having committed adultery.

These actions gained the attention of Sulaiman ibn Muhammad ibn Ghurayr of the tribe of Bani Khalid, the chief of Al-Hasa and Qatif, who held substantial influence in Najd. Ibn Ghurayr threatened Ibn Mu'ammar with denying him the ability to collect a land tax for some properties that Ibn Mu'ammar owned in Al-Hasa if he did not kill or drive away Ibn ʿAbd al-Wahhab. Consequently, Ibn Mu'ammar forced Ibn ʿAbd al-Wahhab to leave.

Upon his expulsion from 'Uyayna, Ibn ʿAbd al-Wahhab was invited to settle in neighboring Diriyah by its ruler Muhammad bin Saud. After some time in Diriyah, Muhammad ibn ʿAbd al-Wahhab concluded his second and more successful agreement with a ruler. Ibn ʿAbd al-Wahhab and Muhammad bin Saud agreed that, together, they would bring the Arabs of the peninsula back to the "true" principles of Islam as they saw it. According to one source, when they first met, bin Saud declared:

Muhammad ibn ʿAbd al-Wahhab replied: 

The agreement was confirmed with a mutual oath of loyalty ("bay'ah") in 1744.
Ibn Abd al-Wahhab would be responsible for religious matters and Ibn Saud in charge of political and military issues. This agreement became a "mutual support pact"

The 1744 pact between Muhammad bin Saud and Muhammad ibn ʿAbd al-Wahhab marked the emergence of the first Saudi state, the Emirate of Diriyah. By offering the Al Saud a clearly defined religious mission, the alliance provided the ideological impetus to Saudi expansion. First conquering Najd, Saud's forces expanded the Salafi influence to most of the present-day territory of Saudi Arabia, eradicating various popular practices they viewed as akin to polytheism and propagating the doctrines of ʿAbd al-Wahhab.

According to academic publications such as the Encyclopædia Britannica while in Baghdad, Ibn ʿAbd al-Wahhab married an affluent woman. When she died, he inherited her property and wealth. Muhammad ibn 'Abd Al-Wahhab had six sons; Hussain, Abdullah, Hassan, Ali and Ibrahim and Abdul-Aziz who died in his youth. All his surviving sons established religious schools close to their homes and taught the young students from Diriyah and other places.
The descendants of Ibn ʿAbd al-Wahhab, the Al ash-Sheikh, have historically led the ulama in the Saudi state, dominating the state's religious institutions. Within Saudi Arabia, the family is held in prestige similar to the Saudi royal family, with whom they share power, and has included several religious scholars and officials. The arrangement between the two families is based on the Al Saud maintaining the Al ash-Sheikh's authority in religious matters and upholding and propagating Salafi doctrine. In return, the Al ash-Sheikh support the Al Saud's political authority thereby using its religious-moral authority to legitimise the royal family's rule.

Ibn ʿAbd al-Wahhab considered his movement an effort to purify Islam by returning Muslims to what, he believed, were the original principles of that religion. He taught that the primary doctrine of Islam was the uniqueness and unity of God ("Tawhid"). He also denounced popular beliefs as polytheism (shirk), rejected much of the medieval law of the scholars (ulema) and called for a new interpretation of Islam.

The "core" of Ibn ʿAbd al-Wahhab's teaching is found in "Kitab al-Tawhid", a short essay which draws from material in the Quran and the recorded doings and sayings ("hadith") of the Islamic prophet Muhammad. It preaches that worship in Islam includes conventional acts of worship such as the five daily prayers ("salat"); fasting ("sawm"); supplication ("Dua"); seeking protection or refuge ("Istia'dha"); seeking help ("Ist'ana" and "Istighatha") of Allah.

Ibn ʿAbd al-Wahhab was keen on emphasizing that other acts, such as making "dua" or calling upon/supplication to or seeking help, protection or intercession from anyone or anything other than Allah, are acts of "shirk" and contradict the tenets of tawhid and that those who tried would never be forgiven.

Traditionally, most Muslims throughout history have held the view that declaring the testimony of faith is sufficient in becoming a Muslim. Ibn 'Abd al-Wahhab did not agree with this. He held the view that an individual who believed that there could be intercessors with God was actually performing shirk. This was the major difference between him and his opponents and led him to declare Muslims outside of his group to be apostates (takfir) and idolators (mushrikin).

Ibn 'Abd al-Wahhab's movement is today often known as Wahhabism, although many adherents see this as a derogatory term coined by his opponents, and prefer it to be known as the Salafi movement. Scholars point out that Salafism is a term applicable to several forms of puritanical Islam in various parts of the world, while Wahhabism refers to the specific Saudi school, which is seen as a more strict form of Salafism. According to Ahmad Moussalli, professor of political science at the American University of Beirut, "As a rule, all Wahhabis are Salafists, but not all Salafists are Wahhabis". Yet others say that while Wahhabism and Salafism originally were two different things, they became practically indistinguishable in the 1970s.

At the end of his treatise, "Al-Hadiyyah al-Suniyyah", Ibn ʿAbd al-Wahhab's son 'Abd Allah speaks positively on the practice of tazkiah (purification of the inner self).

According to author Dore Gold, in "Kitab al-Tawhid", Ibn Abd al-Wahhab described followers of both the Christian and Jewish faiths as sorcerers who believed in devil worship, and cited a hadith of Muhammad stating that punishment for the sorcerer is `that he be struck with the sword.` Wahhab asserted that both religions had improperly made the graves of their prophet into places of worship and warned Muslims not to imitate this practice. Wahhab concluded that `The ways of the people of the book are condemned as those of polytheists.`

However author Natana J. DeLong-Bas defends Abdul Wahhab, stating that

despite his at times vehement denunciations of other religious groups for their supposedly heretical beliefs, Ibn Abd al Wahhab never called for their destruction or death … he assumed that these people would be punished in the Afterlife …"
Historical accounts of Wahhab also state that, "Muhammad ibn ʿAbd al-Wahhab saw it as his mission to restore a more purer and original form of the faith of Islam. … Anyone who didn't adhere to this interpretation were considered polytheists worthy of death, including fellow Muslims (especially Shi'ite who venerate the family of Muhammad), Christians and others. He also advocated for a literalist interpretation of the Quran and its laws"

Despite his great aversion to venerating the saints after their earthly passing and seeking their intercession, it should nevertheless be noted that Ibn 'Abd-al-Wahhab did not deny the existence of saints as such; on the contrary, he acknowledged that "the miracles of saints ("karāmāt al-awliyāʾ") are not to be denied, and their right guidance by God is acknowledged" when they acted properly during their life.

Ibn ʿAbd al-Wahhab's teachings were criticized by a number of Islamic scholars during his life for disregarding Islamic history, monuments, traditions and the sanctity of Muslim life. One scholar named Ibn Muhammad compared Ibn 'Abd al-Wahhab with Musaylimah the liar alayhi la'na. He also accused Ibn 'Abd al-Wahhab of wrongly declaring the Muslims to be infidels based on a misguided reading of Qur'anic passages and Prophetic traditions and of wrongly declaring all scholars as infidels who did not agree with his "deviant innovation".

The traditional Hanbali scholar Ibn Fayruz al-Tamimi (d. 1801/1802) publicly repudiated Ibn 'Abd al-Wahhab's teachings when he sent an envoy to him and referred to the Wahhabis as the "seditious Kharijites" of Najd. In response, the Wahhabis considered Ibn Fayruz an idolater (mushrik) and one of their worst enemies.

According to the historian Ibn Humayd, Ibn 'Abd al-Wahhab's father criticized his son for his unwillingness to specialize in jurisprudence and disagreed with his doctrine and declared that he would be the cause of wickedness. Similarly his own brother, Suleyman ibn 'Abd al-Wahhab wrote one of the first treatises' refuting Wahhabi doctrine claiming he was ill-educated and intolerant, and classing Ibn ʿAbd al-Wahhab's views as fringe and fanatical.

The Shafi'i mufti of Mecca, Ahmed ibn Zayni Dehlan, wrote an anti-Wahhabi treatise, the bulk of which consists of arguments and proof from the sunna to uphold the validity of practices the Wahhabis considered idolatrous: Visiting the tombs of Muhammad, seeking the intercession of saints, venerating Muhammad and obtaining the blessings of saints. He also accused Ibn 'Abd al-Wahhab of not adhering to the Hanbali school and that he was deficient in learning.

Pakistani Muslim scholars such as Israr Ahmed have spoken positively on him. Ibn ʿAbd al-Wahhab is accepted by Salafi scholars as an authority and source of reference. 20th century Albanian scholar Nasiruddin Albani refers to Ibn Abdul Wahhab's activism as "Najdi da'wah."

A list of scholars with opposing views, along with names of their books and related information, was compiled by the Islamic scholar Muhammad Hisham.

The state mosque of Qatar is named after him. The "Imam Muhammad ibn Abd al-Wahhab Mosque" was opened in 2011, with the Emir of Qatar presiding over the occasion. The mosque can hold a congregation of 30,000 people. There has been request from descendants of Ibn Abd al-Wahhab that the name of the mosque be changed.

Despite Wahhabi destruction of many Islamic, non-Islamic, and historical sites associated with the first Muslims (the Prophet's family and companions) and the strict prohibition of visiting such sites (including mosques), the Saudi government renovated the tomb of Muhammad ibn Abd al-Wahhab at Diriyah, turning it into a major tourist attraction and an important place of visitation within the kingdom's modern borders. Other features in the area include the "Sheikh Muhammad bin Abdul Wahab Foundation" which will incoprorate a light and sound presentation located near the "Mosque of Sheikh Mohammad bin Abdulwahab".



There are two contemporary histories of Muhammed ibn ʿAbd al-Wahhab and his religious movement from the point of view of his supporters: Ibn Ghannam's "Rawdhat al-Afkar wal-Afham" or "Tarikh Najd" (History of Najd) and Ibn Bishr's "Unwan al-Majd fi Tarikh Najd". Husain ibn Ghannam (d. 1811), an alim from al-Hasa was the only historian to have observed the beginnings of Ibn ʿAbd al-Wahhab's movement first-hand. His chronicle ends at the year 1797. Ibn Bishr's chronicle, which stops at the year 1854, was written a generation later than Ibn Ghannam's, but is considered valuable partly because Ibn Bishr was a native of Najd and because he adds many details to Ibn Ghannam's account.

A third account, dating from around 1817 is "Lam' al-Shihab", written by an anonymous Sunni author who respectfully disapproved of Ibn ʿAbd al-Wahhab's movement, regarding it as a "bid'ah". It is also commonly cited because it is considered to be a relatively objective contemporary treatment of the subject. However, unlike Ibn Ghannam and Ibn Bishr, its author did not live in Najd and his work is believed to contain some apocryphal and legendary material with respect to the details of Ibn ʿAbd al-Wahhab's life.



</doc>
<doc id="19977" url="https://en.wikipedia.org/wiki?curid=19977" title="Maine">
Maine

Maine () is a U.S. state in the New England region of the northeastern United States. Maine is the 12th smallest by area, the 9th least populous, and the 38th most densely populated of the 50 U.S. states. It is bordered by New Hampshire to the west, the Atlantic Ocean to the southeast, and the Canadian provinces of New Brunswick and Quebec to the northeast and northwest respectively. Maine is the easternmost state in the contiguous United States, and the northernmost east of the Great Lakes. It is known for its jagged, rocky coastline; low, rolling mountains; heavily forested interior; and picturesque waterways, as well as its seafood cuisine, especially clams and lobster. There is a humid continental climate throughout the state, even in coastal areas such as its most populous city of Portland. The capital is Augusta.

For thousands of years, indigenous peoples were the only inhabitants of the territory that is now Maine. At the time of European arrival in what is now Maine, several Algonquian-speaking peoples inhabited the area. The first European settlement in the area was by the French in 1604 on Saint Croix Island, by Pierre Dugua, Sieur de Mons. The first English settlement was the short-lived Popham Colony, established by the Plymouth Company in 1607. A number of English settlements were established along the coast of Maine in the 1620s, although the rugged climate, deprivations, and conflict with the local peoples caused many to fail over the years.

As Maine entered the 18th century, only a half dozen European settlements had survived. Loyalist and Patriot forces contended for Maine's territory during the American Revolution and the War of 1812. At the close of the War of 1812, it was occupied by British forces, but the territory of Maine was returned to the United States as part of a peace treaty that was to include dedicated land on the Michigan peninsula for Native American peoples. Maine was part of the Commonwealth of Massachusetts until 1820, when it voted to secede from Massachusetts to become a separate state. On March 15, 1820, under the Missouri Compromise, it was admitted to the Union as the 23rd state.

There is no definitive explanation for the origin of the name "Maine", but the most likely origin is that the name was given by early explorers after the former province of Maine in France. Whatever the origin, the name was fixed for English settlers in 1665 when the English King's Commissioners ordered that the "Province of Maine" be entered from then on in official records. The state legislature in 2001 adopted a resolution establishing Franco-American Day, which stated that the state was named after the former French province of Maine.

Other theories mention earlier places with similar names, or claim it is a nautical reference to the mainland. Attempts to uncover the history of the name of Maine began with James Sullivan's 1795 "History of the District of Maine". He made the unsubstantiated claim that the Province of Maine was a compliment to the queen of Charles I, Henrietta Maria, who once "owned" the Province of Maine in France. This was quoted by Maine historians until the 1845 biography of that queen by Agnes Strickland established that she had no connection to the province; further, King Charles I married Henrietta Maria in 1625, three years after the name Maine first appeared on the charter. A new theory, put forward by Carol B. Smith Fisher in 2002, is that Sir Ferdinando Gorges chose the name in 1622 to honor the village where his ancestors first lived in England, rather than the province in France. "MAINE" appears in the Domesday Book of 1086 in reference to the county of Dorset, which is today Broadmayne, just southeast of Dorchester.

The view generally held among British place name scholars is that Mayne in Dorset is Brythonic, corresponding to modern Welsh "maen", plural "main" or "meini". Some early spellings are: MAINE 1086, MEINE 1200, MEINES 1204, MAYNE 1236. Today the village is known as Broadmayne, which is primitive Welsh or Brythonic, "main" meaning rock or stone, considered a reference to the many large sarsen stones still present around Little Mayne farm, half a mile northeast of Broadmayne village.

The first known record of the name appears in an August 10, 1622 land charter to Sir Ferdinando Gorges and Captain John Mason, English Royal Navy veterans, who were granted a large tract in present-day Maine that Mason and Gorges "intend to name the Province of Maine". Mason had served with the Royal Navy in the Orkney Islands, where the chief island is called Mainland, a possible name derivation for these English sailors. In 1623, the English naval captain Christopher Levett, exploring the New England coast, wrote: "The first place I set my foote upon in New England was the Isle of Shoals, being Ilands in the sea, above two Leagues from the Mayne." Initially, several tracts along the coast of New England were referred to as "Main" or "Maine" (cf. the Spanish Main). A reconfirmed and enhanced April 3, 1639, charter, from England's King Charles I, gave Sir Ferdinando Gorges increased powers over his new province and stated that it "shall forever hereafter, be called and named the PROVINCE OR COUNTIE OF MAINE, and not by any other name or names whatsoever..." Maine is the only U.S. state whose name has exactly one syllable.

The original inhabitants of the territory that is now Maine were Algonquian-speaking Wabanaki peoples, including the Passamaquoddy, Maliseet, Penobscot, Androscoggin and Kennebec. During the later King Phillip's War, many of these peoples would merge in one form or another to become the Wabanaki Confederacy, aiding the Wampanoag of Massachusetts & the Mahican of New York. Afterwards, many of these people were driven from their natural territories, but most of the tribes of Maine continued, unchanged, until the American Revolution. Before this point, however, most of these people were considered separate nations. Many had adapted to living in permanent, Iroquois-inspired settlements, while those along the coast tended to be semi-nomadic—traveling from settlement to settlement on a yearly cycle. They would usually winter inland & head to the coasts by summer.

European contact with what is now called Maine started around 1200 CE when Norwegians interacted with the native Penobscot in present-day Hancock County, most likely through trade. About 200 years earlier, from the settlements in Iceland and Greenland, Norwegians had first identified America and attempted to settle areas such as Newfoundland, but failed to establish a permanent settlement there. Archeological evidence suggests that Norwegians in Greenland returned to North America for several centuries after the initial discovery to collect timber and to trade, with the most relevant evidence being the Maine Penny, an 11th-century Norwegian coin found at a Native American dig site in 1954.

The first European settlement in Maine was in 1604 on Saint Croix Island, led by French explorer Pierre Dugua, Sieur de Mons; his party included Samuel de Champlain, noted as an explorer. The French named the entire area Acadia, including the portion that later became the state of Maine. The first English settlement in Maine was established by the Plymouth Company at the Popham Colony in 1607, the same year as the settlement at Jamestown, Virginia. The Popham colonists returned to Britain after 14 months.

The French established two Jesuit missions: one on Penobscot Bay in 1609, and the other on Mount Desert Island in 1613. The same year, Castine was established by Claude de La Tour. In 1625, Charles de Saint-Étienne de la Tour erected Fort Pentagouet to protect Castine. The coastal areas of western Maine first became the Province of Maine in a 1622 land patent. The part of Eastern Maine north of the Kennebec River was more sparsely settled, and was known in the 17th century as the Territory of Sagadahock. A second settlement was attempted in 1623 by English explorer and naval Captain Christopher Levett at a place called York, where he had been granted by King Charles I of England. It also failed.

Central Maine was formerly inhabited by people of the Androscoggin tribe of the Abenaki nation, also known as Arosaguntacook. They were driven out of the area in 1690 during King William's War. They were relocated at St. Francis, Canada, which was destroyed by Rogers' Rangers in 1759, and is now Odanak. The other Abenaki tribes suffered several severe defeats, particularly during Dummer's War, with the capture of Norridgewock in 1724 and the defeat of the Pequawket in 1725, which greatly reduced their numbers. They finally withdrew to Canada, where they were settled at Bécancour and Sillery, and later at St. Francis, along with other refugee tribes from the south.

The province within its current boundaries became part of Massachusetts Bay Colony in 1652. Maine was much fought over by the French, English, and allied natives during the 17th and early 18th centuries, who conducted raids against each other, taking captives for ransom or, in some cases, adoption by Native American tribes. A notable example was the early 1692 Abenaki raid on York, where about 100 English settlers were killed and another estimated 80 taken hostage. The Abenaki took captives taken during raids of Massachusetts in Queen Anne's War of the early 1700s to Kahnewake, a Catholic Mohawk village near Montreal, where some were adopted and others ransomed.

After the British defeated the French in Acadia in the 1740s, the territory from the Penobscot River east fell under the nominal authority of the Province of Nova Scotia, and together with present-day New Brunswick formed the Nova Scotia county of Sunbury, with its court of general sessions at Campobello. American and British forces contended for Maine's territory during the American Revolution and the War of 1812, with the British occupying eastern Maine in both conflicts. The territory of Maine was confirmed as part of Massachusetts when the United States was formed following the Treaty of Paris ending the revolution, although the final border with British North America was not established until the Webster–Ashburton Treaty of 1842.

Maine was physically separate from the rest of Massachusetts. Long-standing disagreements over land speculation and settlements led to Maine residents and their allies in Massachusetts proper forcing an 1807 vote in the Massachusetts Assembly on permitting Maine to secede; the vote failed. Secessionist sentiment in Maine was stoked during the War of 1812 when Massachusetts pro-British merchants opposed the war and refused to defend Maine from British invaders. In 1819, Massachusetts agreed to permit secession, sanctioned by voters of the rapidly growing region the following year. Formal secession and formation of the state of Maine as the 23rd state occurred on March 15, 1820, as part of the Missouri Compromise, which geographically limited the spread of slavery and enabled the admission to statehood of Missouri the following year, keeping a balance between slave and free states.

Maine's original state capital was Portland, Maine's largest city, until it was moved to the more central Augusta in 1832. The principal office of the Maine Supreme Judicial Court remains in Portland.

The 20th Maine Volunteer Infantry Regiment, under the command of Colonel Joshua Lawrence Chamberlain, prevented the Union Army from being flanked at Little Round Top by the Confederate Army during the Battle of Gettysburg.

Four U.S. Navy ships have been named USS "Maine", most famously the armored cruiser , whose sinking by an explosion on 15 February 1898 precipitated the Spanish–American War.

To the south and east is the Atlantic Ocean and to the north and northeast is New Brunswick, a province of Canada. The Canadian province of Quebec is to the northwest. Maine is both the northernmost state in New England and the largest, accounting for almost half the region's entire land area. Maine is the only state to border exactly one other state (New Hampshire to the west).

Maine is the easternmost state in the United States in both its extreme points and its geographic center. The municipalities of Eastport and Lubec are, respectively, the easternmost city and town in the United States. Estcourt Station is Maine's northernmost point, as well as the northernmost point in New England. (For more information see extreme points of the United States.)

Maine's Moosehead Lake is the largest lake wholly in New England, as Lake Champlain is located between Vermont, New York and Quebec. A number of other Maine lakes, such as South Twin Lake, are described by Thoreau in "The Maine Woods" (1864). Mount Katahdin is both the northern terminus of the Appalachian Trail, which extends southerly to Springer Mountain, Georgia, and the southern terminus of the new International Appalachian Trail which, when complete, will run to Belle Isle, Newfoundland and Labrador.

Maine has several unique geographical features. Machias Seal Island and North Rock, off its easternmost point, are claimed by both the U.S. and Canada and are within one of four areas between the two countries whose sovereignty is still in dispute, but it is the only one of the disputed areas containing land. Also in this easternmost area in the Bay of Fundy is the Old Sow, the largest tidal whirlpool in the Western Hemisphere.

Maine is the least densely populated U.S. state east of the Mississippi River. It is called the Pine Tree State; about 83% of its land is forested, the most forest cover of any U.S. state. In the forested areas of the interior lies much uninhabited land, some of which does not have formal political organization into local units (a rarity in New England). The Northwest Aroostook, Maine unorganized territory in the northern part of the state, for example, has an area of and a population of 10, or one person for every .

Maine is in the temperate broadleaf and mixed forests biome. The land near the southern and central Atlantic coast is covered by the mixed oaks of the Northeastern coastal forests. The remainder of the state, including the North Woods, is covered by the New England-Acadian forests.

Maine has almost of coastline (and of tidal coastline). West Quoddy Head, in Lubec, Maine, is the easternmost point of land in the 48 contiguous states. Along the famous rock-bound coast of Maine are lighthouses, beaches, fishing villages, and thousands of offshore islands, including the Isles of Shoals which straddle the New Hampshire border. There are jagged rocks and cliffs and many bays and inlets. Inland are lakes, rivers, forests, and mountains. This visual contrast of forested slopes sweeping down to the sea has been summed up by American poet Edna St. Vincent Millay of Rockland and Camden, Maine, in "Renascence":

Geologists describe this type of landscape as a "drowned coast", where a rising sea level has invaded former land features, creating bays out of valleys and islands out of mountain tops. A rise in the elevation of the land due to the melting of heavy glacier ice caused a slight rebounding effect of underlying rock; this land rise, however, was not enough to eliminate all the effect of the rising sea level and its invasion of former land features.

Much of Maine's geomorphology was created by extended glacial activity at the end of the last ice age. Prominent glacial features include Somes Sound and Bubble Rock, both part of Acadia National Park on Mount Desert Island. Carved by glaciers, Somes Sound is considered to be the only fjord on the eastern seaboard and reaches depths of . The extreme depth and steep drop-off allow large ships to navigate almost the entire length of the sound. These features also have made it attractive for boat builders, such as the prestigious Hinckley Yachts.

Bubble Rock, a glacial erratic, is a large boulder perched on the edge of Bubble Mountain in Acadia National Park. By analyzing the type of granite, geologists were able to discover that glaciers carried Bubble Rock to its present location from near Lucerne – away. The Iapetus Suture runs through the north and west of the state, being underlain by the ancient Laurentian terrane, and the south and east underlain by the Avalonian terrane.

Acadia National Park is the only national park in New England. Areas under the protection and management of the National Park Service include:

Maine has a humid continental climate (Köppen climate classification "Dfb"), with warm (although generally not hot), humid summers. Winters are cold and snowy throughout the state, and are especially severe in the northern parts of Maine. Coastal areas are moderated somewhat by the Atlantic Ocean, resulting in milder winters and cooler summers in immediate coastal areas. Daytime highs are generally in the range throughout the state in July, with overnight lows in the high 50s °F (around 15 °C). January temperatures range from highs near on the southern coast to overnight lows averaging below in the far north.

The state's record high temperature is , set in July 1911, at North Bridgton.
Precipitation in Maine is evenly distributed year-round, but with a slight summer maximum in northern/northwestern Maine and a slight late-fall or early-winter maximum along the coast due to "nor'easters" or intense cold-season storms. In coastal Maine, the late spring and summer months are usually driest – a rarity across the Eastern United States. Maine has fewer days of thunderstorms than any other state east of the Rockies, with most of the state averaging less than 20 days of thunderstorms a year. Tornadoes are rare in Maine, with the state averaging fewer than two per year, mostly occurring in the southern part of the state. Maine rarely sees tropical cyclones.

In January 2009, a new record low temperature for the state was set at Big Black River of , tying the New England record.

Annual precipitation varies from in Presque Isle, to in Acadia National Park.

The United States Census Bureau estimates that the population of Maine was 1,329,328 on July 1, 2015, a 0.07% increase since the 2010 United States Census. The population density of the state is 41.3 people per square mile, making it the least densely populated state in New England, the American northeast, the eastern seaboard, of all of the states with an Atlantic coastline and of all of the states east of the Mississippi River.

The mean population center of Maine is located in Kennebec County, just east of Augusta. The Greater Portland metropolitan area is the most densely populated with nearly 40% of Maine's population. Portland's estimated population in 2016 was 66,937. As explained in detail under "Geography", there are large tracts of uninhabited land in some remote parts of the interior.

Maine has experienced a very slow rate of population growth since the 1990 census; its rate of growth (0.57%) since the 2010 census ranks 45th of the 50 states. The modest population growth in the state has been concentrated in the southern coastal counties; the northern, more rural areas of the state have experienced a significant decline in population in recent years.

At the 2010 Census, 94.4% of the population was non-Hispanic White, 1.1% non-Hispanic Black or African American, 0.6% American Indian and Alaska Native, 1.0% Asian, 0.1% from some other race and 1.4% of two or more races. 1.3% of Maine's population was of Hispanic, Latino, or Spanish origin.

People citing that they are American are of overwhelmingly English descent, but have ancestry that has been in the region for so long (often since the 1600s) that they choose to identify simply as Americans.

Maine has the highest percentage of French Americans among American states. It also has the highest percentage of non-Hispanic whites of any state, at 94.4% of the total population, according to the 2010 Census. In 2011, 89.0% of all births in the state were to non-Hispanic white parents. The state also has the highest percentage of French-speakers of any state. Most of the French in Maine are of Canadian Origin, but in some cases have been living there since prior to the American Revolutionary War. There are particularly high concentrations of French in the northern part of Maine in Aroostook County, which is part of a cultural region known as Acadia that goes over the border into New Brunswick. Along with the Acadian population in the north, many French came from Quebec as immigrants between 1840 and 1930. Census figures show that Maine has the highest percentage of people speaking French at home of any state: 5.28% of Maine households are French-speaking, compared with 4.68% in Louisiana, which is the second highest state. French-speakers are the state's chief linguistic minority; the 2000 Census reported 92.25% of Maine residents aged five and older spoke only English at home. Maine does not have an official language, but the most widely spoken language in the state is English. Spanish is the third-most-spoken language in Maine, after English and French.

The upper Saint John River valley area was once part of the so-called Republic of Madawaska, before the frontier was decided in the Webster-Ashburton Treaty of 1842. Over one quarter of the population of Lewiston, Waterville, and Biddeford are Franco-American. Most of the residents of the Mid Coast and Down East sections are chiefly of British heritage. Smaller numbers of various other groups, including Irish, Italian and Polish, have settled throughout the state since the late 19th and early 20th century immigration waves.

"Note: Births in table don't add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number."


According to the Association of Religion Data Archives (ARDA), the religious affiliations of Maine in 2010 were:

The Catholic Church was the largest religious institution with 202,106 members, the United Methodist Church had 28,329 members, the United Church of Christ had 22,747 members

In 2010, a study named Maine as the least religious state in the United States.

The Bureau of Economic Analysis estimates that Maine's total gross state product for 2010 was $52 billion. Its per capita personal income for 2007 was US$33,991, 34th in the nation. As of May 2018, Maine's unemployment rate is 3.0%
Maine's agricultural outputs include poultry, eggs, dairy products, cattle, wild blueberries, apples, maple syrup, and maple sugar. Aroostook County is known for its potato crops. Commercial fishing, once a mainstay of the state's economy, maintains a presence, particularly lobstering and groundfishing. Western Maine aquifers and springs are a major source of bottled water.

Maine's industrial outputs consist chiefly of paper, lumber and wood products, electronic equipment, leather products, food products, textiles, and bio-technology. Naval shipbuilding and construction remain key as well, with Bath Iron Works in Bath and Portsmouth Naval Shipyard in Kittery.

Brunswick Landing, formerly Naval Air Station Brunswick, is also in Maine. Formerly a large support base for the U.S. Navy, the BRAC campaign initiated the Naval Air Station's closing, despite a government-funded effort to upgrade its facilities. The former base has since been changed into a civilian business park, as well as a new satellite campus for Southern Maine Community College.

Maine is the number one US producer of low-bush blueberries (Vaccinium angustifolium). Preliminary data from the USDA for 2012 also indicate Maine was the largest blueberry producer of the major blueberry producing states in the US, with 91,100,000 lbs. This data includes both low (wild), and high-bush (cultivated) blueberries: Vaccinium corymbosum. The largest toothpick manufacturing plant in the United States used to be located in Strong, Maine. The Strong Wood Products plant produced 20 million toothpicks a day. It closed in May 2003.

Tourism and outdoor recreation play a major and increasingly important role in Maine's economy. The state is a popular destination for sport hunting (particularly deer, moose and bear), sport fishing, snowmobiling, skiing, boating, camping and hiking, among other activities.

Historically, Maine ports played a key role in national transportation. Beginning around 1880, Portland's rail link and ice-free port made it Canada's principal winter port, until the aggressive development of Halifax, Nova Scotia, in the mid-1900s. In 2013, 12,039,600 short tons passed into and out of Portland by sea, which places it 45th of US water ports. Portland Maine's Portland International Jetport was recently expanded, providing the state with increased air traffic from carriers such as JetBlue and Southwest Airlines.

Maine has very few large companies that maintain headquarters in the state, and that number has fallen due to consolidations and mergers, particularly in the pulp and paper industry. Some of the larger companies that do maintain headquarters in Maine include Fairchild Semiconductor in South Portland; IDEXX Laboratories, in Westbrook; Hannaford Bros. Co. in Scarborough; Unum in Portland; TD Bank in Portland; L.L.Bean in Freeport; and Cole Haan in Yarmouth. Maine is also the home of The Jackson Laboratory, the world's largest non-profit mammalian genetic research facility and the world's largest supplier of genetically purebred mice.

Maine has an income tax structure containing two brackets, 6.5% to 7.95% of personal income. Before July 2013 Maine had four brackets: 2%, 4.5%, 7%, and 8.5%. Maine's general sales tax rate is 5.5%. The state also levies charges of 9% on lodging and prepared food and 10% on short-term auto rentals. Commercial sellers of blueberries, a Maine staple, must keep records of their transactions and pay the state 1.5 cents per pound ($1.50 per 100 pounds) of the fruit sold each season. All real and tangible personal property located in the state of Maine is taxable unless specifically exempted by statute. The administration of property taxes is handled by the local assessor in incorporated cities and towns, while property taxes in the unorganized territories are handled by the State Tax Assessor.

Maine has a long-standing tradition of being home to many shipbuilding companies. In the 18th and 19th centuries, Maine was home to many shipyards that produced wooden sailing ships. The main function of these ships was to transport either cargos or passengers overseas. One of these yards was located in Pennellville Historic District in what is now Brunswick, Maine. This yard, owned by the Pennell family, was typical of the many family-owned shipbuilding companies of the time period. Other such examples of shipbuilding families were the Skolfields and the Morses. During the 18th and 19th centuries, wooden shipbuilding of this sort made up a sizable portion of the economy.

Maine receives passenger jet service at its two largest airports, the Portland International Jetport in Portland, and the Bangor International Airport in Bangor. Both are served daily by many major airlines to destinations such as New York, Atlanta, and Orlando. Essential Air Service also subsidizes service to a number of smaller airports in Maine, bringing small turboprop aircraft to regional airports such as the Augusta State Airport, Hancock County-Bar Harbor Airport, Knox County Regional Airport, and the Northern Maine Regional Airport at Presque Isle. These airports are served by Cape Air with Cessna 402s and Penair with Saab 340s.

Many smaller airports are scattered throughout Maine, only serving general aviation traffic. The Eastport Municipal Airport, for example, is a city-owned public-use airport with 1,200 general aviation aircraft operations each year from single-engine and ultralight aircraft.

Interstate 95 (I-95) travels through Maine, as well as its easterly branch I-295 and spurs 195, 395 and the unsigned I-495. In addition, U.S. Route 1 (US 1) starts in Fort Kent and travels to Florida. The eastern terminus of the eastern section of US 2 starts in Houlton, near the New Brunswick, Canada border to Rouses Point, New York, at US 11. US 2A connects Old Town and Orono, primarily serving the University of Maine campus. US 201 and US 202 flow through the state. US 2, Maine State Route 6 (Route 6), and Route 9 are often used by truckers and other motorists of the Maritime Provinces "en route" to other destinations in the United States or as a short cut to Central Canada.

The "Downeaster" passenger train, operated by Amtrak, provides passenger service between Brunswick and Boston's North Station, with stops in Freeport, Portland, Old Orchard Beach, Saco, and Wells. The "Downeaster" makes five daily trips, three of which continue past Portland to Brunswick.

Freight service throughout the state is provided by a handful of regional and shortline carriers: Pan Am Railways (formerly known as Guilford Rail System), which operates the former Boston & Maine and Maine Central railroads; St. Lawrence and Atlantic Railroad; Maine Eastern Railroad; Central Maine and Quebec Railway; and New Brunswick Southern Railway.

The Maine Constitution structures Maine's state government, composed of three co-equal branches—the executive, legislative, and judicial branches. The state of Maine also has three Constitutional Officers (the Secretary of State, the State Treasurer, and the State Attorney General) and one Statutory Officer (the State Auditor).

The legislative branch is the Maine Legislature, a bicameral body composed of the Maine House of Representatives, with 151 members, and the Maine Senate, with 35 members. The Legislature is charged with introducing and passing laws.

The executive branch is responsible for the execution of the laws created by the Legislature and is headed by the Governor of Maine (currently Paul LePage). The Governor is elected every four years; no individual may serve more than two consecutive terms in this office. The current attorney general of Maine is Janet T. Mills. As with other state legislatures, the Maine Legislature can by a two-thirds majority vote from both the House and Senate override a gubernatorial veto. Maine is one of seven states that do not have a lieutenant governor.

The judicial branch is responsible for interpreting state laws. The highest court of the state is the Maine Supreme Judicial Court. The lower courts are the District Court, Superior Court and Probate Court. All judges except for probate judges serve full-time, are nominated by the Governor and confirmed by the Legislature for terms of seven years. Probate judges serve part-time and are elected by the voters of each county for four-year terms.

Maine is divided into political jurisdictions designated as counties. Since 1860 there have been 16 counties in the state, ranging in size from .

In state general elections, Maine voters tend to accept independent and third-party candidates more frequently than most states. Maine has had two independent governors recently (James B. Longley, 1975–1979 and current U.S. Senator Angus King, 1995–2003). Maine state politicians, Democrats and Republicans alike, are noted for having more moderate views than many in the national wings of their respective parties.

Maine is an alcoholic beverage control state.

On May 6, 2009, Maine became the fifth state to legalize same-sex marriage; however, the law was repealed by voters on November 3, 2009. On November 6, 2012, Maine, along with Maryland and Washington, became the first state to legalize same-sex marriage at the ballot box.

In the 1930s, Maine was one of very few states which retained Republican sentiments. In the 1936 presidential election, Franklin D. Roosevelt received the electoral votes of every state other than Maine and Vermont; these were the only two states in the nation that never voted for Roosevelt in any of his presidential campaigns, though Maine was closely fought in 1940 and 1944. In the 1960s, Maine began to lean toward the Democrats, especially in presidential elections. In 1968, Hubert Humphrey became just the second Democrat in half a century to carry Maine, perhaps because of the presence of his running mate, Maine Senator Edmund Muskie, although the state voted Republican in every presidential election in the 1970s and 1980s.

Since 1969, two of Maine's four electoral votes have been awarded based on the winner of the statewide election; the other two go to the highest vote-getter in each of the state's two congressional districts. Every other state except Nebraska gives all its electoral votes to the candidate who wins the popular vote in the state at large, without regard to performance within districts. Maine split its electoral vote for the first time in 2016, with Donald Trump's strong showing in the more rural central and northern Maine allowing him to capture one of the state's four votes in the Electoral College.

Ross Perot achieved a great deal of success in Maine in the presidential elections of 1992 and 1996. In 1992, as an independent candidate, Perot came in second to Democrat Bill Clinton, despite the long-time presence of the Bush family summer home in Kennebunkport. In 1996, as the nominee of the Reform Party, Perot did better in Maine than in any other state.

Maine has voted for Democratic Bill Clinton twice, Al Gore in 2000, John Kerry in 2004, and Barack Obama in 2008 and 2012. In 2016, Republican Donald Trump won one of Maine's electoral votes with Democratic opponent Hillary Clinton winning the other three. Although Democrats have mostly carried the state in presidential elections in recent years, Republicans have largely maintained their control of the state's U.S. Senate seats, with Edmund Muskie, William Hathaway and George J. Mitchell being the only Maine Democrats serving in the U.S. Senate in the past fifty years.

In the 2010 midterm elections, Republicans made major gains in Maine. They captured the governor's office as well as majorities in both chambers of the state legislature for the first time since the early 1970s. However, in the 2012 elections Democrats managed to recapture both houses of Maine Legislature.

Maine's U.S. senators are Republican Susan Collins and Independent Angus King. The governor is Republican Paul LePage. The state's two members of the United States House of Representatives are Democrat Chellie Pingree and Republican Bruce Poliquin.

An organized municipality has a form of elected local government which administers and provides local services, keeps records, collects licensing fees, and can pass locally binding ordinances, among other responsibilities of self-government. The governmental format of most organized towns and plantations is the town meeting, while the format of most cities is the council-manager form. As of 2013 the organized municipalities of Maine consist of 23 cities, 431 towns, and 34 plantations. Collectively these 488 organized municipalities cover less than half of the state's territory. Maine also has 3 Reservations: Indian Island, Indian Township Reservation, and Pleasant Point Indian Reservation.

Unorganized territory has no local government. Administration, services, licensing, and ordinances are handled by the state government. The unorganized territory of Maine consists of over 400 townships (towns are incorporated, townships are unincorporated), plus many coastal islands that do not lie within any municipal bounds. The UT land area is slightly over one half the entire area of the State of Maine. Year-round residents in the UT number approximately 9,000, about 1.3% of the state's total population, with many more people residing only seasonally within the UT. Only four of Maine's sixteen counties (Androscoggin, Cumberland, Waldo and York) are entirely incorporated, although a few others are nearly so, and most of the unincorporated area is in the vast and sparsely populated Great North Woods of Maine.

QuickFacts US Census Maine Portland:

Throughout Maine, many municipalities, although each separate governmental entities, nevertheless form portions of a much larger population base. There are many such population clusters throughout Maine, but some examples from the municipalities appearing in the above listing are:

There are thirty institutions of higher learning in Maine. These institutions include the University of Maine, which is the oldest, largest and only research university in the state. UMaine was founded in 1865 and is the state's only land grant and sea grant college. The University of Maine is located in the town of Orono and is the flagship of Maine. There are also branch campuses in Augusta, Farmington, Fort Kent, Machias, and Presque Isle.

Bowdoin College is a liberal arts college founded in 1794 in Brunswick, making it the oldest institution of higher learning in the state. Colby College in Waterville was founded in 1813 making it the second oldest college in Maine. Bates College in Lewiston was founded in 1855 making it the third oldest institution in the state and the oldest coeducational college in New England. The three colleges collectively form the Colby-Bates-Bowdoin Consortium and are ranked among the best colleges in the United States; often placing in the top 10% of all liberal arts colleges.Maine's per-student public expenditure for elementary and secondary schools was 21st in the nation in 2012, at $12,344.

The collegiate system of Maine also includes numerous baccalaureate colleges such as: the Maine Maritime Academy (MMA), Unity College, and Thomas College. There is only one medical school in the state, (University of New England's College of Osteopathic Medicine) and only one law school (The University of Maine School of Law).

Private schools in Maine are funded independently of the state and its furthered domains. Private schools are less common than public schools. A large number of private elementary schools with under 20 students exist, but most private high schools in Maine can be described as "semi-private".


Adapted from the Maine facts site.


A citizen of Maine is known as a "Mainer", though the term is often reserved for those whose roots in Maine go back at least three generations. The term "Downeaster" may be applied to residents of the northeast coast of the state. The term "Mainiac" is considered by some to be derogatory, but embraced with pride by others, and is used for a variety of organizations and for events such as the YMCA Mainiac Sprint Triathlon & Duathlon.


State government

U.S. government

Information


</doc>
<doc id="19978" url="https://en.wikipedia.org/wiki?curid=19978" title="Montana">
Montana

Montana is a state in the Northwestern United States. Montana has several nicknames, although none are official, including "Big Sky Country" and "The Treasure State", and slogans that include "Land of the Shining Mountains" and more recently "The Last Best Place".

Montana is the 4th largest in area, the 8th least populous, and the 3rd least densely populated of the 50 U.S. states. The western half of Montana contains numerous mountain ranges. Smaller island ranges are found throughout the state. In total, 77 named ranges are part of the Rocky Mountains. The eastern half of Montana is characterized by western prairie terrain and badlands. Montana is bordered by Idaho to the west, Wyoming to the south, North Dakota and South Dakota to the east, and the Canadian provinces of British Columbia, Alberta, and Saskatchewan to the north.

The economy is primarily based on agriculture, including ranching and cereal grain farming. Other significant economic resources include oil, gas, coal, hard rock mining, and lumber. The health care, service, and government sectors also are significant to the state's economy.

The state's fastest-growing sector is tourism. Nearly 13 million tourists annually visit Glacier National Park, Yellowstone National Park, the Beartooth Highway, Flathead Lake, Big Sky Resort, and other attractions.

The name Montana comes from the Spanish word "Montaña" which in turn comes from the Latin word "Montanea", meaning "mountain", or more broadly, "mountainous country". "Montaña del Norte" was the name given by early Spanish explorers to the entire mountainous region of the west. The name Montana was added to a bill by the United States House Committee on Territories, which was chaired at the time by Rep. James Ashley of Ohio, for the territory that would become Idaho Territory. The name was changed by Representatives Henry Wilson (Massachusetts) and Benjamin F. Harding (Oregon), who complained Montana had "no meaning". When Ashley presented a bill to establish a temporary government in 1864 for a new territory to be carved out of Idaho, he again chose Montana Territory. This time Rep. Samuel Cox, also of Ohio, objected to the name. Cox complained that the name was a misnomer given most of the territory was not mountainous and that a Native American name would be more appropriate than a Spanish one. Other names such as Shoshone were suggested, but it was decided that the Committee on Territories could name it whatever they wanted, so the original name of Montana was adopted.

Montana is one of the nine Mountain States, located in the north of the region known as the Western United States. It borders North Dakota and South Dakota to the east. Wyoming is to the south, Idaho is to the west and southwest, and three Canadian provinces, British Columbia, Alberta, and Saskatchewan, are to the north.

With an area of , Montana is slightly larger than Japan. It is the fourth largest state in the United States after Alaska, Texas, and California; it is the largest landlocked U.S. state.

The state's topography is roughly defined by the Continental Divide, which splits much of the state into distinct eastern and western regions. Most of Montana's 100 or more named mountain ranges are in the state's western half, most of which is geologically and geographically part of the Northern Rocky Mountains. The Absaroka and Beartooth ranges in the state's south-central part are technically part of the Central Rocky Mountains. The Rocky Mountain Front is a significant feature in the state's north-central portion, and isolated island ranges that interrupt the prairie landscape common in the central and eastern parts of the state. About 60 percent of the state is prairie, part of the northern Great Plains.

The Bitterroot Mountains—one of the longest continuous ranges in the Rocky Mountain chain from Alaska to Mexico—along with smaller ranges, including the Coeur d'Alene Mountains and the Cabinet Mountains, divide the state from Idaho. The southern third of the Bitterroot range blends into the Continental Divide. Other major mountain ranges west of the Divide include the Cabinet Mountains, the Anaconda Range, the Missions, the Garnet Range, Sapphire Mountains, and Flint Creek Range.
The Divide's northern section, where the mountains rapidly give way to prairie, is part of the Rocky Mountain Front. The front is most pronounced in the Lewis Range, located primarily in Glacier National Park. Due to the configuration of mountain ranges in Glacier National Park, the Northern Divide (which begins in Alaska's Seward Peninsula) crosses this region and turns east in Montana at Triple Divide Peak. It causes the Waterton River, Belly, and Saint Mary rivers to flow north into Alberta, Canada. There they join the Saskatchewan River, which ultimately empties into Hudson Bay.

East of the divide, several roughly parallel ranges cover the state's southern part, including the Gravelly Range, the Madison Range, Gallatin Range, Absaroka Mountains and the Beartooth Mountains. The Beartooth Plateau is the largest continuous land mass over high in the continental United States. It contains the state's highest point, Granite Peak, high. North of these ranges are the Big Belt Mountains, Bridger Mountains, Tobacco Roots, and several island ranges, including the Crazy Mountains and Little Belt Mountains.

Between many mountain ranges are rich river valleys. The Big Hole Valley, Bitterroot Valley, Gallatin Valley, Flathead Valley, and Paradise Valley have extensive agricultural resources and multiple opportunities for tourism and recreation.

East and north of this transition zone are the expansive and sparsely populated Northern Plains, with tableland prairies, smaller island mountain ranges, and badlands. The isolated island ranges east of the Divide include the Bear Paw Mountains, Bull Mountains, Castle Mountains, Crazy Mountains, Highwood Mountains, Judith Mountains, Little Belt Mountains, Little Rocky Mountains, the Pryor Mountains, Snowy Mountains, Sweet Grass Hills, and—in the state's southeastern corner near Ekalaka—the Long Pines. Many of these isolated eastern ranges were created about 120 to 66 million years ago when magma welling up from the interior cracked and bowed the earth's surface here.

The area east of the divide in the state' north-central portion is known for the Missouri Breaks and other significant rock formations. Three buttes south of Great Falls are major landmarks: Cascade, Crown, Square, Shaw and Buttes. Known as laccoliths, they formed when igneous rock protruded through cracks in the sedimentary rock. The underlying surface consists of sandstone and shale. Surface soils in the area are highly diverse, and greatly affected by the local geology, whether glaciated plain, intermountain basin, mountain foothills, or tableland. Foothill regions are often covered in weathered stone or broken slate, or consist of uncovered bare rock (usually igneous, quartzite, sandstone, or shale). The soil of intermountain basins usually consists of clay, gravel, sand, silt, and volcanic ash, much of it laid down by lakes which covered the region during the Oligocene 33 to 23 million years ago. Tablelands are often topped with argillite gravel and weathered quartzite, occasionally underlain by shale. The glaciated plains are generally covered in clay, gravel, sand, and silt left by the proglacial Lake Great Falls or by moraines or gravel-covered former lake basins left by the Wisconsin glaciation 85,000 to 11,000 years ago. Farther east, areas such as Makoshika State Park near Glendive and Medicine Rocks State Park near Ekalaka contain some of the most scenic badlands regions in the state.

The Hell Creek Formation in Northeast Montana is a major source of dinosaur fossils. Paleontologist Jack Horner of the Museum of the Rockies in Bozeman brought this formation to the world's attention with several major finds.

Montana has thousands of named rivers and creeks, of which are known for "blue-ribbon" trout fishing. Montana's water resources provide for recreation, hydropower, crop and forage irrigation, mining, and water for human consumption. Montana is one of few geographic areas in the world whose rivers form parts of three major watersheds (i.e. where two continental divides intersect). Its rivers feed the Pacific Ocean, the Gulf of Mexico, and Hudson Bay. The watersheds divide at Triple Divide Peak in Glacier National Park.

West of the divide, the Clark Fork of the Columbia (not to be confused with the Clarks Fork of the Yellowstone River) rises near Butte and flows northwest to Missoula, where it is joined by the Blackfoot River and Bitterroot River. Farther downstream, it is joined by the Flathead River before entering Idaho near Lake Pend Oreille. The Pend Oreille River forms the outflow of Lake Pend Oreille. The Pend Oreille River joined the Columbia River, which flows to the Pacific Ocean—making the long Clark Fork/Pend Oreille (considered a single river system) the longest river in the Rocky Mountains. The Clark Fork discharges the greatest volume of water of any river exiting the state. The Kootenai River in northwest Montana is another major tributary of the Columbia.

East of the divide the Missouri River, which is formed by the confluence of the Jefferson, Madison and Gallatin rivers near Three Forks, flows due north through the west-central part of the state to Great Falls. From this point, it then flows generally east through fairly flat agricultural land and the Missouri Breaks to Fort Peck reservoir. The stretch of river between Fort Benton and the Fred Robinson Bridge at the western boundary of Fort Peck Reservoir was designated a National Wild and Scenic River in 1976. The Missouri enters North Dakota near Fort Union, having drained more than half the land area of Montana (). Nearly one-third of the Missouri River in Montana lies behind 10 dams: Toston, Canyon Ferry, Hauser, Holter, Black Eagle, Rainbow, Cochrane, Ryan, Morony, and Fort Peck.

The Yellowstone River rises on the continental divide near Younts Peak in Wyoming's Teton Wilderness. It flows north through Yellowstone National Park, enters Montana near Gardiner, and passes through the Paradise Valley to Livingston. It then flows northeasterly across the state through Billings, Miles City, Glendive, and Sidney. The Yellowstone joins the Missouri in North Dakota just east of Fort Union. It is the longest undammed, free-flowing river in the contiguous United States, and drains about a quarter of Montana ().

Other major Montana tributaries of the Missouri include the Smith, Milk, Marias, Judith, and Musselshell Rivers. Montana also claims the disputed title of possessing the world's shortest river, the Roe River, just outside Great Falls. Through the Missouri, these rivers ultimately join the Mississippi River and flow into the Gulf of Mexico.

Major tributaries of the Yellowstone include the Boulder, Stillwater, Clarks Fork, Bighorn, Tongue, and Powder Rivers.

The Northern Divide turns east in Montana at Triple Divide Peak, causing the Waterton River, Belly, and Saint Mary rivers to flow north into Alberta. There they join the Saskatchewan River, which ultimately empties into Hudson Bay.

There are some 3,000 named lakes and reservoirs in Montana, including Flathead Lake, the largest natural freshwater lake in the western United States. Other major lakes include Whitefish Lake in the Flathead Valley and Lake McDonald and St. Mary Lake in Glacier National Park. The largest reservoir in the state is Fort Peck Reservoir on the Missouri river, which is contained by the second largest earthen dam and largest hydraulically filled dam in the world. Other major reservoirs include Hungry Horse on the Flathead River; Lake Koocanusa on the Kootenai River; Lake Elwell on the Marias River; Clark Canyon on the Beaverhead River; Yellowtail on the Bighorn River, Canyon Ferry, Hauser, Holter, Rainbow; and Black Eagle on the Missouri River.

Vegetation of the state includes lodgepole pine, ponderosa pine; Douglas fir, larch, spruce; aspen, birch, red cedar, hemlock, ash, alder; rocky mountain maple and cottonwood trees. Forests cover approximately 25 percent of the state. Flowers native to Montana include asters, bitterroots, daisies, lupins, poppies, primroses, columbine, lilies, orchids, and dryads. Several species of sagebrush and cactus and many species of grasses are common. Many species of mushrooms and lichens are also found in the state.

Montana is home to a diverse array of fauna that includes 14 amphibian, 90 fish, 117 mammal, 20 reptile and 427 bird species. Additionally, there are over 10,000 invertebrate species, including 180 mollusks and 30 crustaceans. Montana has the largest grizzly bear population in the lower 48 states. Montana hosts five federally endangered species–black-footed ferret, whooping crane, least tern, pallid sturgeon and white sturgeon and seven threatened species including the grizzly bear, Canadian lynx and bull trout. The Montana Department of Fish, Wildlife and Parks manages fishing and hunting seasons for at least 17 species of game fish including seven species of trout, walleye and smallmouth bass and at least 29 species of game birds and animals including ring-neck pheasant, grey partridge, elk, pronghorn antelope, mule deer, whitetail deer, gray wolf and bighorn sheep.

Montana contains Glacier National Park, "The Crown of the Continent"; and portions of Yellowstone National Park, including three of the park's five entrances. Other federally recognized sites include the Little Bighorn National Monument, Bighorn Canyon National Recreation Area, Big Hole National Battlefield, and the National Bison Range. Approximately , or 35 percent of Montana's land is administered by federal or state agencies. The U.S. Department of Agriculture Forest Service administers of forest land in ten National Forests. There are approximately of wilderness in 12 separate wilderness areas that are part of the National Wilderness Preservation System established by the Wilderness Act of 1964. The U.S. Department of the Interior Bureau of Land Management controls of federal land. The U.S. Department of the Interior Fish and Wildlife Service administers of 1.1 million acres of National Wildlife Refuges and waterfowl production areas in Montana. The U.S. Department of the Interior Bureau of Reclamation administers approximately of land and water surface in the state. The Montana Department of Fish, Wildlife and Parks operates approximately of state parks and access points on the state's rivers and lakes. The Montana Department of Natural Resources and Conservation manages of School Trust Land ceded by the federal government under the Land Ordinance of 1785 to the state in 1889 when Montana was granted statehood. These lands are managed by the state for the benefit of public schools and institutions in the state.

Areas managed by the National Park Service include:

Montana is a large state with considerable variation in geography, topography and altitude, and the climate is, therefore, equally varied. The state spans from below the 45th parallel (the line equidistant between the equator and North Pole) to the 49th parallel, and elevations range from under to nearly above sea level. The western half is mountainous, interrupted by numerous large valleys. Eastern Montana comprises plains and badlands, broken by hills and isolated mountain ranges, and has a semi-arid, continental climate (Köppen climate classification "BSk"). The Continental Divide has a considerable effect on the climate, as it restricts the flow of warmer air from the Pacific from moving east, and drier continental air from moving west. The area west of the divide has a modified northern Pacific coast climate, with milder winters, cooler summers, less wind and a longer growing season. Low clouds and fog often form in the valleys west of the divide in winter, but this is rarely seen in the east.

Average daytime temperatures vary from in January to in July. The variation in geography leads to great variation in temperature. The highest observed summer temperature was at Glendive on July 20, 1893, and Medicine Lake on July 5, 1937. Throughout the state, summer nights are generally cool and pleasant. Extreme hot weather is less common above . Snowfall has been recorded in all months of the year in the more mountainous areas of central and western Montana, though it is rare in July and August.

The coldest temperature on record for Montana is also the coldest temperature for the entire contiguous U.S. On January 20, 1954, was recorded at a gold mining camp near Rogers Pass. Temperatures vary greatly on cold nights, and Helena, to the southeast had a low of only on the same date, and an all-time record low of . Winter cold spells are usually the result of cold continental air coming south from Canada. The front is often well defined, causing a large temperature drop in a 24-hour period. Conversely, air flow from the southwest results in "chinooks." These steady (or more) winds can suddenly warm parts of Montana, especially areas just to the east of the mountains, where temperatures sometimes rise up to for periods of ten days or longer.

Loma is the site of the most extreme recorded temperature change in a 24-hour period in the United States. On January 15, 1972, a chinook wind blew in and the temperature rose from .

Average annual precipitation is , but great variations are seen. The mountain ranges block the moist Pacific air, holding moisture in the western valleys, and creating rain shadows to the east. Heron, in the west, receives the most precipitation, . On the eastern (leeward) side of a mountain range, the valleys are much drier; Lonepine averages , and Deer Lodge of precipitation. The mountains can receive over , for example the Grinnell Glacier in Glacier National Park gets . An area southwest of Belfry averaged only over a sixteen-year period. Most of the larger cities get of snow each year. Mountain ranges can accumulate of snow during a winter. Heavy snowstorms may occur from September through May, though most snow falls from November to March.

The climate has become warmer in Montana and continues to do so. The glaciers in Glacier National Park have receded and are predicted to melt away completely in a few decades. Many Montana cities set heat records during July 2007, the hottest month ever recorded in Montana. Winters are warmer, too, and have fewer cold spells. Previously these cold spells had killed off bark beetles, but these are now attacking the forests of western Montana. The warmer winters in the region have allowed various species to expand their ranges and proliferate. The combination of warmer weather, attack by beetles, and mismanagement during past years has led to a substantial increase in the severity of forest fires in Montana. According to a study done for the U.S. Environmental Protection Agency by the Harvard School of Engineering and Applied Science, portions of Montana will experience a 200-percent increase in area burned by wildfires, and an 80-percent increase in related air pollution.

The table below lists average temperatures for the warmest and coldest month for Montana's seven largest cities. The coldest month varies between December and January depending on location, although figures are similar throughout.

Montana is one of only two continental US states (along with Colorado) which is antipodal to land. The Kerguelen Islands are antipodal to the Montana–Saskatchewan–Alberta border. No towns are precisely antipodal to Kerguelen, though Chester and Rudyard are close.

Various indigenous peoples lived in the territory of the present-day state of Montana for thousands of years. Historic tribes encountered by Europeans and settlers from the United States included the Crow in the south-central area; the Cheyenne in the very southeast; the Blackfeet, Assiniboine and Gros Ventres in the central and north-central area; and the Kootenai and Salish in the west. The smaller Pend d'Oreille and Kalispel tribes lived near Flathead Lake and the western mountains, respectively. A part of southeastern Montana was used as a corridor between the Crows and the related Hidatsas in North Dakota.

The land in Montana east of the continental divide was part of the Louisiana Purchase in 1803. Subsequent to and particularly in the decades following the Lewis and Clark Expedition, American, British and French traders operated a fur trade, typically working with indigenous peoples, in both eastern and western portions of what would become Montana. These dealings were not always peaceful, and though the fur trade brought some material gain for indigenous tribal groups it also brought exposure to European diseases and altered their economic and cultural traditions. The trading post Fort Raymond (1807-1811) was constructed in Crow Indian country in 1807. Until the Oregon Treaty (1846), land west of the continental divide was disputed between the British and U.S. and was known as the Oregon Country. The first permanent settlement by Euro-Americans in what today is Montana was St. Mary's (1841) near present-day Stevensville. In 1847, Fort Benton was established as the uppermost fur-trading post on the Missouri River. In the 1850s, settlers began moving into the Beaverhead and Big Hole valleys from the Oregon Trail and into the Clark's Fork valley.

The first gold discovered in Montana was at Gold Creek near present-day Garrison in 1852. A series of major mining discoveries in the western third of the state starting in 1862 found gold, silver, copper, lead, coal (and later oil) that attracted tens of thousands of miners to the area. The richest of all gold placer diggings was discovered at Alder Gulch, where the town of Virginia City was established. Other rich placer deposits were found at Last Chance Gulch, where the city of Helena now stands, Confederate Gulch, Silver Bow, Emigrant Gulch, and Cooke City. Gold output from 1862 through 1876 reached $144 million; silver then became even more important. The largest mining operations were in the city of Butte, which had important silver deposits and gigantic copper deposits.

Before the creation of Montana Territory (1864–1889), various parts of what is now Montana were parts of Oregon Territory (1848–1859), Washington Territory (1853–1863), Idaho Territory (1863–1864), and Dakota Territory (1861–1864). Montana became a United States territory (Montana Territory) on May 26, 1864. The first territorial capital was at Bannack. The first territorial governor was Sidney Edgerton. The capital moved to Virginia City in 1865 and to Helena in 1875. In 1870, the non-Indian population of Montana Territory was 20,595. The Montana Historical Society, founded on February 2, 1865, in Virginia City is the oldest such institution west of the Mississippi (excluding Louisiana). In 1869 and 1870 respectively, the Cook–Folsom–Peterson and the Washburn–Langford–Doane Expeditions were launched from Helena into the Upper Yellowstone region and directly led to the creation of Yellowstone National Park in 1872.

As white settlers began populating Montana from the 1850s through the 1870s, disputes with Native Americans ensued, primarily over land ownership and control. In 1855, Washington Territorial Governor Isaac Stevens negotiated the Hellgate treaty between the United States Government and the Salish, Pend d'Oreille, and the Kootenai people of western Montana, which established boundaries for the tribal nations. The treaty was ratified in 1859. While the treaty established what later became the Flathead Indian Reservation, trouble with interpreters and confusion over the terms of the treaty led whites to believe that the Bitterroot Valley was opened to settlement, but the tribal nations disputed those provisions. The Salish remained in the Bitterroot Valley until 1891.

The first U.S. Army post established in Montana was Camp Cooke in 1866, on the Missouri River, to protect steamboat traffic going to Fort Benton, Montana. More than a dozen additional military outposts were established in the state. Pressure over land ownership and control increased due to discoveries of gold in various parts of Montana and surrounding states. Major battles occurred in Montana during Red Cloud's War, the Great Sioux War of 1876, the Nez Perce War and in conflicts with Piegan Blackfeet. The most notable of these were the Marias Massacre (1870), Battle of the Little Bighorn (1876), Battle of the Big Hole (1877) and Battle of Bear Paw (1877). The last recorded conflict in Montana between the U.S. Army and Native Americans occurred in 1887 during the Battle of Crow Agency in the Big Horn country. Indian survivors who had signed treaties were generally required to move onto reservations.
Simultaneously with these conflicts, bison, a keystone species and the primary protein source that Native people had survived on for centuries were being destroyed. Some estimates say there were over 13 million bison in Montana in 1870. In 1875, General Philip Sheridan pleaded to a joint session of Congress to authorize the slaughtering of herds in order to deprive the Indians of their source of food. By 1884, commercial hunting had brought bison to the verge of extinction; only about 325 bison remained in the entire United States.

Cattle ranching has been central to Montana's history and economy since Johnny Grant began wintering cattle in the Deer Lodge Valley in the 1850s and traded cattle fattened in fertile Montana valleys with emigrants on the Oregon Trail. Nelson Story brought the first Texas Longhorn cattle into the territory in 1866. Granville Stuart, Samuel Hauser and Andrew J. Davis started a major open range cattle operation in Fergus County in 1879. The Grant-Kohrs Ranch National Historic Site in Deer Lodge is maintained today as a link to the ranching style of the late 19th century. Operated by the National Park Service, it is a working ranch.

Tracks of the Northern Pacific Railroad (NPR) reached Montana from the west in 1881 and from the east in 1882. However, the railroad played a major role in sparking tensions with Native American tribes in the 1870s. Jay Cooke, the NPR president launched major surveys into the Yellowstone valley in 1871, 1872 and 1873 which were challenged forcefully by the Sioux under chief Sitting Bull. These clashes, in part, contributed to the Panic of 1873, a financial crisis that delayed construction of the railroad into Montana. Surveys in 1874, 1875 and 1876 helped spark the Great Sioux War of 1876. The transcontinental NPR was completed on September 8, 1883, at Gold Creek.

Tracks of the Great Northern Railroad (GNR) reached eastern Montana in 1887 and when they reached the northern Rocky Mountains in 1890, the GNR became a significant promoter of tourism to Glacier National Park region. The transcontinental GNR was completed on January 6, 1893, at Scenic, Washington.

In 1881, the Utah and Northern Railway a branch line of the Union Pacific completed a narrow gauge line from northern Utah to Butte. A number of smaller spur lines operated in Montana from 1881 into the 20th century including the Oregon Short Line, Montana Railroad and Milwaukee Road.

Under Territorial Governor Thomas Meagher, Montanans held a constitutional convention in 1866 in a failed bid for statehood. A second constitutional convention was held in Helena in 1884 that produced a constitution ratified 3:1 by Montana citizens in November 1884. For political reasons, Congress did not approve Montana statehood until 1889. Congress approved Montana statehood in February 1889 and President Grover Cleveland signed an omnibus bill granting statehood to Montana, North Dakota, South Dakota and Washington once the appropriate state constitutions were crafted. In July 1889, Montanans convened their third constitutional convention and produced a constitution accepted by the people and the federal government. On November 8, 1889 President Benjamin Harrison proclaimed Montana the forty-first state in the union. The first state governor was Joseph K. Toole. In the 1880s, Helena (the current state capital) had more millionaires per capita than any other United States city.

The Homestead Act of 1862 provided free land to settlers who could claim and "prove-up" of federal land in the midwest and western United States. Montana did not see a large influx of immigrants from this act because 160 acres was usually insufficient to support a family in the arid territory. The first homestead claim under the act in Montana was made by David Carpenter near Helena in 1868. The first claim by a woman was made near Warm Springs Creek by Gwenllian Evans, the daughter of Deer Lodge Montana pioneer, Morgan Evans. By 1880, there were farms in the more verdant valleys of central and western Montana, but few on the eastern plains.

The Desert Land Act of 1877 was passed to allow settlement of arid lands in the west and allotted to settlers for a fee of $.25 per acre and a promise to irrigate the land. After three years, a fee of one dollar per acre would be paid and the land would be owned by the settler. This act brought mostly cattle and sheep ranchers into Montana, many of whom grazed their herds on the Montana prairie for three years, did little to irrigate the land and then abandoned it without paying the final fees. Some farmers came with the arrival of the Great Northern and Northern Pacific Railroads throughout the 1880s and 1890s, though in relatively small numbers.

In the early 1900s, James J. Hill of the Great Northern began promoting settlement in the Montana prairie to fill his trains with settlers and goods. Other railroads followed suit. In 1902, the Reclamation Act was passed, allowing irrigation projects to be built in Montana's eastern river valleys. In 1909, Congress passed the Enlarged Homestead Act that expanded the amount of free land from per family and in 1912 reduced the time to "prove up" on a claim to three years. In 1916, the Stock-Raising Homestead Act allowed homesteads of 640 acres in areas unsuitable for irrigation. This combination of advertising and changes in the Homestead Act drew tens of thousands of homesteaders, lured by free land, with World War I bringing particularly high wheat prices. In addition, Montana was going through a temporary period of higher-than-average precipitation. Homesteaders arriving in this period were known as "Honyockers", or "scissorbills." Though the word "honyocker", possibly derived from the ethnic slur "hunyak," was applied in a derisive manner at homesteaders as being "greenhorns", "new at his business" or "unprepared", the reality was that a majority of these new settlers had previous farming experience, though there were also many who did not.

However, farmers faced a number of problems. Massive debt was one. Also, most settlers were from wetter regions, unprepared for the dry climate, lack of trees, and scarce water resources. In addition, small homesteads of fewer than were unsuited to the environment. Weather and agricultural conditions are much harsher and drier west of the 100th meridian. Then, the droughts of 1917–1921 proved devastating. Many people left, and half the banks in the state went bankrupt as a result of providing mortgages that could not be repaid. As a result, farm sizes increased while the number of farms decreased

By 1910, homesteaders filed claims on over five million acres, and by 1923, over 93 million acres were farmed. In 1910, the Great Falls land office alone saw over 1,000 homestead filings per month, and the peak of 1917– 1918 saw 14,000 new homesteads each year. But significant drop occurred following drought in 1919.

As World War I broke out, Jeannette Rankin, the first woman in the United States to be a member of Congress, was a pacifist and voted against the United States' declaration of war. Her actions were widely criticized in Montana, where public support for the war was strong, and wartime sentiment reached high levels of patriotism among many Montanans. In 1917–18, due to a miscalculation of Montana's population, approximately 40,000 Montanans, ten percent of the state's population, either volunteered or were drafted into the armed forces. This represented a manpower contribution to the war that was 25 percent higher than any other state on a per capita basis. Approximately 1500 Montanans died as a result of the war and 2437 were wounded, also higher than any other state on a per capita basis. Montana's Remount station in Miles City provided 10,000 cavalry horses for the war, more than any other Army post in the US. The war created a boom for Montana mining, lumber and farming interests as demand for war materials and food increased.

In June 1917, the U.S. Congress passed the Espionage Act of 1917 which was later extended by the Sedition Act of 1918, enacted in May 1918. In February 1918, the Montana legislature had passed the Montana Sedition Act, which was a model for the federal version. In combination, these laws criminalized criticism of the U.S. government, military, or symbols through speech or other means. The Montana Act led to the arrest of over 200 individuals and the conviction of 78, mostly of German or Austrian descent. Over 40 spent time in prison. In May 2006, then-Governor Brian Schweitzer posthumously issued full pardons for all those convicted of violating the Montana Sedition Act.

The Montanans who opposed U.S. entry into the war included certain immigrant groups of German and Irish heritage as well as pacifist Anabaptist people such as the Hutterites and Mennonites, many of whom were also of Germanic heritage. In turn, pro-War groups formed, such as the Montana Council of Defense, created by Governor Samuel V. Stewart as well as local "loyalty committees."

War sentiment was complicated by labor issues. The Anaconda Copper Company, which was at its historic peak of copper production, was an extremely powerful force in Montana, but also faced criticism and opposition from socialist newspapers and unions struggling to make gains for their members. In Butte, a multi-ethnic community with significant European immigrant population, labor unions, particularly the newly formed Metal Mine Workers' Union, opposed the war on grounds that it mostly profited large lumber and mining interests. In the wake of ramped-up mine production and the Speculator Mine disaster in June 1917, Industrial Workers of the World organizer Frank Little arrived in Butte to organize miners. He gave some speeches with inflammatory anti-war rhetoric. On August 1, 1917, he was dragged from his boarding house by masked vigilantes, and hanged from a railroad trestle, considered a lynching. Little's murder and the strikes that followed resulted in the National Guard being sent to Butte to restore order. Overall, anti-German and anti-labor sentiment increased and created a movement that led to the passage of the Montana Sedition Act the following February. In addition, the Council of Defense was made a state agency with the power to prosecute and punish individuals deemed in violation of the Act. The Council also passed rules limiting public gatherings and prohibiting the speaking of German in public.

In the wake of the legislative action in 1918, emotions rose. U.S. Attorney Burton K. Wheeler and several District Court Judges who hesitated to prosecute or convict people brought up on charges were strongly criticized. Wheeler was brought before the Council of Defense, though he avoided formal proceedings, and a District Court judge from Forsyth was impeached. There were burnings of German-language books and several near-hangings. The prohibition on speaking German remained in effect into the early 1920s. Complicating the wartime struggles, the 1918 Influenza epidemic claimed the lives of over 5,000 Montanans. The period has been dubbed "Montana's Agony" by some historians due to the suppression of civil liberties that occurred.

An economic depression began in Montana after World War I and lasted through the Great Depression until the beginning of World War II. This caused great hardship for farmers, ranchers, and miners. The wheat farms in eastern Montana make the state a major producer; the wheat has a relatively high protein content and thus commands premium prices.

When the U.S. entered World War II on December 7, 1941, many Montanans already had enlisted in the military to escape the poor national economy of the previous decade. Another 40,000-plus Montanans entered the armed forces in the first year following the declaration of war, and over 57,000 joined up before the war ended. These numbers constituted about 10 percent of the state's total population, and Montana again contributed one of the highest numbers of soldiers per capita of any state. Many Native Americans were among those who served, including soldiers from the Crow Nation who became Code Talkers. At least 1500 Montanans died in the war. Montana also was the training ground for the First Special Service Force or "Devil's Brigade," a joint U.S-Canadian commando-style force that trained at Fort William Henry Harrison for experience in mountainous and winter conditions before deployment. Air bases were built in Great Falls, Lewistown, Cut Bank and Glasgow, some of which were used as staging areas to prepare planes to be sent to allied forces in the Soviet Union. During the war, about 30 Japanese Fu-Go balloon bombs were documented to have landed in Montana, though no casualties nor major forest fires were attributed to them.

In 1940, Jeannette Rankin was again elected to Congress. In 1941, as she had in 1917, she voted against the United States' declaration of war after the Japanese attack on Pearl Harbor. Hers was the only vote against the war, and in the wake of public outcry over her vote, Rankin required police protection for a time. Other pacifists tended to be those from "peace churches" who generally opposed war. Many individuals claiming conscientious objector status from throughout the U.S. were sent to Montana during the war as smokejumpers and for other forest fire-fighting duties.

During World War II, the planned battleship USS "Montana" was named in honor of the state. However, the battleship was never completed. Montana is the only one of the first 48 states lacking a completed battleship being named for it. Alaska and Hawaii have both had nuclear submarines named after them. Montana is the only state in the union without a modern naval ship named in its honor. However, in August 2007 Senator Jon Tester made a request to the Navy that a submarine be christened USS "Montana". Secretary of the Navy Ray Mabus announced on September 3, 2015 that Virginia Class attack Submarine SSN-794 will bear the state's namesake. This will be the second commissioned warship to bear the name "Montana."

In the post-World War II Cold War era, Montana became host to U.S. Air Force Military Air Transport Service (1947) for airlift training in C-54 Skymasters and eventually, in 1953 Strategic Air Command air and missile forces were based at Malmstrom Air Force Base in Great Falls. The base also hosted the 29th Fighter Interceptor Squadron, Air Defense Command from 1953 to 1968. In December 1959, Malmstrom AFB was selected as the home of the new Minuteman I ballistic missile. The first operational missiles were in-place and ready in early 1962. In late 1962 missiles assigned to the 341st Strategic Missile Wing would play a major role in the Cuban Missile Crisis. When the Soviets removed their missiles from Cuba, President John F. Kennedy said the Soviets backed down because they knew he had an "ace in the hole," referring directly to the Minuteman missiles in Montana. Montana eventually became home to the largest ICBM field in the U.S. covering .

The United States Census Bureau estimates that the population of Montana was 1,032,949 on July 1, 2015, a 4.40% increase since the 2010 United States Census. The 2010 Census put Montana's population at 989,415 which is an increase of 43,534 people, or 4.40 percent, since 2010. During the first decade of the new century, growth was mainly concentrated in Montana's seven largest counties, with the highest percentage growth in Gallatin County, which saw a 32 percent increase in its population from 2000–2010. The city seeing the largest percentage growth was Kalispell with 40.1 percent, and the city with the largest increase in actual residents was Billings with an increase in population of 14,323 from 2000–2010.

On January 3, 2012, the Census and Economic Information Center (CEIC) at the Montana Department of Commerce estimated Montana had hit the one million population mark sometime between November and December 2011. The United States Census Bureau estimates that the population of Montana was 1,005,141 on July 1, 2012, a 1.6 percent increase since the 2010 United States Census.

According to the 2010 Census, 89.4 percent of the population was White (87.8 percent Non-Hispanic White), 6.3 percent American Indian and Alaska Native, 2.9 percent Hispanics and Latinos of any race, 0.6 percent Asian, 0.4 percent Black or African American, 0.1 percent Native Hawaiian and Other Pacific Islander, 0.6 percent from Some Other Race, and 2.5 percent from two or more races. The largest European ancestry groups in Montana as of 2010 are: German (27.0 percent), Irish (14.8 percent), English (12.6 percent), Norwegian (10.9 percent), French (4.7 percent) and Italian (3.4 percent).

Montana has a larger Native American population numerically and percentage-wise than most U.S. states. Although the state ranked 45th in population (according to the 2010 U.S. Census), it ranked 19th in total native people population. Native people constituted 6.5 percent of the state's total population, the sixth highest percentage of all 50 states. Montana has three counties in which Native Americans are a majority: Big Horn, Glacier, and Roosevelt. Other counties with large Native American populations include Blaine, Cascade, Hill, Missoula, and Yellowstone counties. The state's Native American population grew by 27.9 percent between 1980 and 1990 (at a time when Montana's entire population rose just 1.6 percent), and by 18.5 percent between 2000 and 2010. As of 2009, almost two-thirds of Native Americans in the state live in urban areas. Of Montana's 20 largest cities, Polson (15.7 percent), Havre (13.0 percent), Great Falls (5.0 percent), Billings (4.4 percent), and Anaconda (3.1 percent) had the greatest percentage of Native American residents in 2010. Billings (4,619), Great Falls (2,942), Missoula (1,838), Havre (1,210), and Polson (706) have the most Native Americans living there. The state's seven reservations include more than twelve distinct Native American ethnolinguistic groups.

While the largest European-American population in Montana overall is German, pockets of significant Scandinavian ancestry are prevalent in some of the farming-dominated northern and eastern prairie regions, parallel to nearby regions of North Dakota and Minnesota. Farmers of Irish, Scots, and English roots also settled in Montana. The historically mining-oriented communities of western Montana such as Butte have a wider range of European-American ethnicity; Finns, Eastern Europeans and especially Irish settlers left an indelible mark on the area, as well as people originally from British mining regions such as Cornwall, Devon and Wales. The nearby city of Helena, also founded as a mining camp, had a similar mix in addition to a small Chinatown. Many of Montana's historic logging communities originally attracted people of Scottish, Scandinavian, Slavic, English and Scots-Irish descent.

The Hutterites, an Anabaptist sect originally from Switzerland, settled here, and today Montana is second only to South Dakota in U.S. Hutterite population with several colonies spread across the state. Beginning in the mid-1990s, the state also saw an influx of Amish, who relocated to Montana from the increasingly urbanized areas of Ohio and Pennsylvania.

Montana's Hispanic population is concentrated around the Billings area in south-central Montana, where many of Montana's Mexican-Americans have been in the state for generations. Great Falls has the highest percentage of African-Americans in its population, although Billings has more African American residents than Great Falls.

The Chinese in Montana, while a low percentage today, have historically been an important presence. About 2000–3000 Chinese miners were in the mining areas of Montana by 1870, and 2500 in 1890. However, public opinion grew increasingly negative toward them in the 1890s and nearly half of the state's Asian population left the state by 1900. Today, there is a significant Hmong population centered in the vicinity of Missoula. Montanans who claim Filipino ancestry amount to almost 3,000, making them currently the largest Asian American group in the state.

English is the official language in the state of Montana, as it is in many U.S. states. According to the 2000 U.S. Census, 94.8 percent of the population aged 5 and older speak English at home. Spanish is the language most commonly spoken at home other than English. There were about 13,040 Spanish-language speakers in the state (1.4 percent of the population) in 2011. There were also 15,438 (1.7 percent of the state population) speakers of Indo-European languages other than English or Spanish, 10,154 (1.1 percent) speakers of a Native American language, and 4,052 (0.4 percent) speakers of an Asian or Pacific Islander language. Other languages spoken in Montana (as of 2013) include Assiniboine (about 150 speakers in the Montana and Canada), Blackfoot (about 100 speakers), Cheyenne (about 1,700 speakers), Plains Cree (about 100 speakers), Crow (about 3,000 speakers), Dakota (about 18,800 speakers in Minnesota, Montana, Nebraska, North Dakota, and South Dakota), German Hutterite (about 5,600 speakers), Gros Ventre (about 10 speakers), Kalispel-Pend d'Oreille (about 64 speakers), Kutenai (about 6 speakers), and Lakota (about 6,000 speakers in Minnesota, Montana, Nebraska, North Dakota, South Dakota). The United States Department of Education estimated in 2009 that 5,274 students in Montana spoke a language at home other than English. These included a Native American language (64 percent), German (4 percent), Spanish (3 percent), Russian (1 percent), and Chinese (less than 0.5 percent).

According to the Pew Forum, the religious affiliations of the people of Montana are as follows: Protestant 47%, Catholic 23%, LDS (Mormon) 5%, Jehovah's Witness 2%, Buddhist 1%, Jewish 0.5%, Muslim 0.5%, Hindu 0.5% and Non-Religious at 20%.

The largest denominations in Montana as of 2010 were the Catholic Church with 127,612 adherents, The Church of Jesus Christ of Latter-day Saints with 46,484 adherents, Evangelical Lutheran Church in America with 38,665 adherents, and non-denominational Evangelical Protestant with 27,370 adherents.

Approximately 66,000 people of Native American heritage live in Montana. Stemming from multiple treaties and federal legislation, including the Indian Appropriations Act (1851), the Dawes Act (1887), and the Indian Reorganization Act (1934), seven Indian reservations, encompassing eleven federally recognized tribal nations, were created in Montana. A twelfth nation, the Little Shell Chippewa is a "landless" people headquartered in Great Falls; it is recognized by the state of Montana but not by the U.S. government. The Blackfeet nation is headquartered on the Blackfeet Indian Reservation (1851) in Browning, Crow on the Crow Indian Reservation (1868) in Crow Agency, Confederated Salish and Kootenai and Pend d'Oreille on the Flathead Indian Reservation (1855) in Pablo, Northern Cheyenne on the Northern Cheyenne Indian Reservation (1884) at Lame Deer, Assiniboine and Gros Ventre on the Fort Belknap Indian Reservation (1888) in Fort Belknap Agency, Assiniboine and Sioux on the Fort Peck Indian Reservation (1888) at Poplar, and Chippewa-Cree on the Rocky Boy's Indian Reservation (1916) near Box Elder. Approximately 63% of all Native people live off the reservations, concentrated in the larger Montana cities, with the largest concentration of urban Indians in Great Falls. The state also has a small Métis population, and 1990 census data indicated that people from as many as 275 different tribes lived in Montana.

Montana's Constitution specifically reads that "the state recognizes the distinct and unique cultural heritage of the American Indians and is committed in its educational goals to the preservation of their cultural integrity." It is the only state in the U.S. with such a constitutional mandate. The Indian Education for All Act (IEFA) was passed in 1999 to provide funding for this mandate and ensure implementation. It mandates that all schools teach American Indian history, culture, and heritage from preschool through college. For kindergarten through 12th-grade students, an "Indian Education for All" curriculum from the Montana Office of Public Instruction is available free to all schools. The state was sued in 2004 because of lack of funding, and the state has increased its support of the program. South Dakota passed similar legislation in 2007, and Wisconsin was working to strengthen its own program based on this model – and the current practices of Montana's schools. Each Indian reservation in the state has a fully accredited tribal colleges. The University of Montana "was the first to establish dual admission agreements with all of the tribal colleges and as such it was the first institution in the nation to actively facilitate student transfer from the tribal colleges"

"Note: Births in table don't add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number."


 The Bureau of Economic Analysis estimates that Montana's total state product in 2014 was $44.3 billion. per capita personal income in 2014 was $40,601, 35th in the nation.
Montana is a relative hub of beer microbrewing, ranking third in the nation in number of craft breweries per capita in 2011. There are significant industries for lumber and mineral extraction; the state's resources include gold, coal, silver, talc, and vermiculite. Ecotaxes on resource extraction are numerous. A 1974 state severance tax on coal (which varied from 20 to 30 percent) was upheld by the Supreme Court of the United States in "Commonwealth Edison Co. v. Montana", 453 U.S. 609 (1981).

Tourism is also important to the economy with over ten million visitors a year to Glacier National Park, Flathead Lake, the Missouri River headwaters, the site of the Battle of Little Bighorn and three of the five entrances to Yellowstone National Park.

Montana's personal income tax contains 7 brackets, with rates ranging from 1 percent to 6.9 percent. Montana has no sales tax. In Montana, household goods are exempt from property taxes. However, property taxes are assessed on livestock, farm machinery, heavy equipment, automobiles, trucks, and business equipment. The amount of property tax owed is not determined solely by the property's value. The property's value is multiplied by a tax rate, set by the Montana Legislature, to determine its taxable value. The taxable value is then multiplied by the mill levy established by various taxing jurisdictions—city and county government, school districts and others.

As of June 2015, the state's unemployment rate is 3.9 percent.

The Montana Territory was formed on April 26, 1864, when the U.S. passed the Organic Act. Schools started forming in the area before it was officially a territory as families started settling into the area. The first schools were subscription schools that typically held in the teacher's home. The first formal school on record was at Fort Owen in Bitterroot valley in 1862. The students were Indian children and the children of Fort Owen employees. The first school term started in early winter and only lasted until February 28. Classes were taught by Mr. Robinson. Another early subscription school was started by Thomas Dimsdale in Virginia City in 1863. In this school students were charged $1.75 per week. The Montana Territorial Legislative Assembly had its inaugural meeting in 1864. The first legislature authorized counties to levy taxes for schools, which set the foundations for public schooling. Madison County was the first to take advantage of the newly authorized taxes and it formed fhe first public school in Virginia City in 1886. The first school year was scheduled to begin in January 1866, but severe weather postponed its opening until March. The first school year ran through the summer and didn't end until August 17. One of the first teachers at the school was Sarah Raymond. She was a 25-year-old woman who had traveled to Virginia City via wagon train in 1865. To become a certified teacher, Raymond took a test in her home and paid a $6 fee in gold dust to obtain a teaching certificate. With the help of an assistant teacher, Mrs. Farley, Raymond was responsible for teaching 50 to 60 students each day out of the 81 students enrolled at the school. Sarah Raymond was paid at a rate of $125 per month, and Mrs. Farley was paid $75 per month. There were no textbooks used in the school. In their place was an assortment of books brought in by various emigrants. Sarah quit teaching the following year, but would later become the Madison County superintendent of schools.

Many well-known artists, photographers and authors have documented the land, culture and people of Montana in the last 100 years. Painter and sculptor Charles Marion Russell, known as "the cowboy artist" created more than 2,000 paintings of cowboys, Native Americans, and landscapes set in the Western United States and in Alberta, Canada. The C. M. Russell Museum Complex located in Great Falls, Montana, houses more than 2,000 Russell artworks, personal objects, and artifacts.

Evelyn Cameron, a naturalist and photographer from Terry documented early 20th century life on the Montana prairie, taking startlingly clear pictures of everything around her: cowboys, sheepherders, weddings, river crossings, freight wagons, people working, badlands, eagles, coyotes and wolves.

Many notable Montana authors have documented or been inspired by life in Montana in both fiction and non-fiction works. Pulitzer Prize winner Wallace Earle Stegner from Great Falls was often called "The Dean of Western Writers". James Willard Schultz ("Apikuni") from Browning is most noted for his prolific stories about Blackfeet life and his contributions to the naming of prominent features in Glacier National Park.

Montana hosts numerous arts and cultural festivals and events every year. Major events include:

There are no major league sports franchises in Montana due to the state's relatively small and dispersed population, but a number of minor league teams play in the state. Baseball is the minor-league sport with the longest heritage in the state, and Montana is currently home to four Minor League Baseball teams, all members of the Pioneer League: the Billings Mustangs, Great Falls Voyagers, Helena Brewers, and Missoula Osprey.

All of Montana's four-year colleges and universities field intercollegiate sports teams. The two largest schools, the University of Montana and Montana State University, are members of the Big Sky Conference and have enjoyed a strong athletic rivalry since the early twentieth century. Six of Montana's smaller four-year schools are members of the Frontier Conference. One is a member of the Great Northwest Athletic Conference.

A variety of sports are offered at Montana high schools. Montana allows the smallest—"Class C"—high schools to utilize six-man football teams, dramatized in the independent 2002 film, "The Slaughter Rule".

There are junior ice hockey teams in Montana, four of which are affiliated with the North American 3 Hockey League: the Bozeman Icedogs, Great Falls Americans, Helena Bighorns, and Missoula Jr. Bruins.


Montanans have been a part of several major sporting achievements:

Montana provides year-round outdoor recreation opportunities for residents and visitors. Hiking, fishing, hunting, watercraft recreation, camping, golf, cycling, horseback riding, and skiing are popular activities.

Montana has been a destination for its world-class trout fisheries since the 1930s. Fly fishing for several species of native and introduced trout in rivers and lakes is popular for both residents and tourists throughout the state. Montana is the home of the Federation of Fly Fishers and hosts many of the organizations annual conclaves. The state has robust recreational lake trout and kokanee salmon fisheries in the west, walleye can be found in many parts of the state, while northern pike, smallmouth and largemouth bass fisheries as well as catfish and paddlefish can be found in the waters of eastern Montana. Robert Redford's 1992 film of Norman Mclean's novel, "A River Runs Through It", was filmed in Montana and brought national attention to fly fishing and the state.

Montana is home to the Rocky Mountain Elk Foundation and has a historic big game hunting tradition. There are fall bow and general hunting seasons for elk, pronghorn antelope, whitetail deer and mule deer. A random draw grants a limited number of permits for moose, mountain goats and bighorn sheep. There is a spring hunting season for black bear and in most years, limited hunting of bison that leave Yellowstone National Park is allowed. Current law allows both hunting and trapping of a specific number of wolves and mountain lions. Trapping of assorted fur bearing animals is allowed in certain seasons and many opportunities exist for migratory waterfowl and upland bird hunting.

Both downhill skiing and cross-country skiing are popular in Montana, which has 15 developed downhill ski areas open to the public, including:

Big Sky Resort and Whitefish Mountain Resort are destination resorts, while the remaining areas do not have overnight lodging at the ski area, though several host restaurants and other amenities. 

Montana also has millions of acres open to cross-country skiing on nine of its national forests plus in Glacier National Park. In addition to cross-country trails at most of the downhill ski areas, there are also 13 private cross-country skiing resorts. Yellowstone National Park also allows cross-country skiing.

Snowmobiling is popular in Montana which boasts over 4000 miles of trails and frozen lakes available in winter. There are 24 areas where snowmobile trails are maintained, most also offering ungroomed trails. West Yellowstone offers a large selection of trails and is the primary starting point for snowmobile trips into Yellowstone National Park, where "oversnow" vehicle use is strictly limited, usually to guided tours, and regulations are in considerable flux.

Snow coach tours are offered at Big Sky, Whitefish, West Yellowstone and into Yellowstone National Park. Equestrian skijoring has a niche in Montana, which hosts the World Skijoring Championships in Whitefish as part of the annual Whitefish Winter Carnival.

Montana does not have a Trauma I hospital, but does have Trauma II hospitals in Missoula, Billings, and Great Falls. In 2013 "AARP The Magazine" named the Billings Clinic one of the safest hospitals in the United States.

Montana is ranked as the least obese state in the U.S., at 19.6%, according to the 2014 Gallup Poll.
As of 2010, Missoula is the 166th largest media market in the United States as ranked by Nielsen Media Research, while Billings is 170th, Great Falls is 190th, the Butte-Bozeman area 191st, and Helena is 206th. There are 25 television stations in Montana, representing each major U.S. network. As of August 2013, there are 527 FCC-licensed FM radio stations broadcast in Montana, with 114 such AM stations.

During the age of the Copper Kings, each Montana copper company had its own newspaper. This changed in 1959 when Lee Enterprises bought several Montana newspapers. Montana's largest circulating daily city newspapers are the "Billings Gazette" (circulation 39,405), "Great Falls Tribune" (26,733), and "Missoulian" (25,439).

Railroads have been an important method of transportation in Montana since the 1880s. Historically, the state was traversed by the main lines of three east-west transcontinental routes: the Milwaukee Road, the Great Northern, and the Northern Pacific. Today, the BNSF Railway is the state's largest railroad, its main transcontinental route incorporating the former Great Northern main line across the state. Montana RailLink, a privately held Class II railroad, operates former Northern Pacific trackage in western Montana.

In addition, Amtrak's "Empire Builder" train runs through the north of the state, stopping in Libby, Whitefish, West Glacier, Essex, East Glacier Park, Browning, Cut Bank, Shelby, Havre, Malta, Glasgow, and Wolf Point.

Bozeman Yellowstone International Airport is the busiest airport in the state of Montana, surpassing Billings Logan International Airport in the spring of 2013. Montana's other major Airports include Billings Logan International Airport, Missoula International Airport, Great Falls International Airport, Glacier Park International Airport, Helena Regional Airport, Bert Mooney Airport and Yellowstone Airport. Eight smaller communities have airports designated for commercial service under the Essential Air Service program.

Historically, U.S. Route 10 was the primary east-west highway route across Montana, connecting the major cities in the southern half of the state. Still the state's most important east-west travel corridor, the route is today served by Interstate 90 and Interstate 94 which roughly follow the same route as the Northern Pacific. U.S. Routes 2 and 12 and Montana Highway 200 also traverse the entire state from east to west.

Montana's only north-south Interstate Highway is Interstate 15. Other major north-south highways include U.S. Routes 87, 89, 93 and 191.

Montana and South Dakota are the only states to share a land border which is not traversed by a paved road. Highway 212, the primary paved route between the two, passes through the northeast corner of Wyoming between Montana and South Dakota.

Montana is governed by a constitution. The first constitution was drafted by a constitutional convention in 1889, in preparation for statehood. Ninety percent of its language came from an 1884 constitution which was never acted upon by Congress for national political reasons. The 1889 constitution mimicked the structure of the United States Constitution, as well as outlining almost the same civil and political rights for citizens. However, the 1889 Montana constitution significantly restricted the power of state government, the legislature was much more powerful than the executive branch, and the jurisdiction of the District Courts very specifically described. Montana voters amended the 1889 constitution 37 times between 1889 and 1972. In 1914, Montana granted women the vote. In 1916, Montana became the first state to elect a woman, Progressive Republican Jeannette Rankin, to Congress.

In 1971, Montana voters approved the call for a state constitutional convention. A new constitution was drafted, which made the legislative and executive branches much more equal in power and which was much less prescriptive in outlining powers, duties, and jurisdictions. The draft included an expanded, more progressive list of civil and political rights, extended these rights to children for the first time, transferred administration of property taxes to the counties from the state, implemented new water rights, eliminated sovereign immunity, and gave the legislature greater power to spend tax revenues. The constitution was narrowly approved, 116,415 to 113,883, and declared ratified on June 20, 1972. Three issues which the constitutional convention were unable to resolve were submitted to voters simultaneously with the proposed constitution. Voters approved the legalization of gambling, a bicameral legislature, and retention of the death penalty.

The 1972 constitution has been amended 31 times as of 2015. Major amendments include establishment of a reclamation trust (funded by taxes on natural resource extraction) to restore mined land (1974); restoration of sovereign immunity, when such immunity has been approved by a two-thirds vote in each house (1974); establishment of a 90-day biennial (rather than annual) legislative session (1974); establishment of a coal tax trust fund, funded by a tax on coal extraction (1976); conversion of the mandatory decennial review of county government into a voluntary one, to be approaved or disallowed by residents in each county (1978); conversion of the provision of public assistance from a mandatory civil right to a non-fundamental legislative prerogative (1988); a new constitutional right to hunt and fish (2004); a prohibition on gay marriage (2004); and a prohibition on new taxes on the sale or transfer of real property (2010). In 1992, voters approved a constitutional amendment implementing term limits for certain statewide elected executive branch offices (governor, lieutenant governor, secretary of state, state auditor, attorney general, superintendent of public instruction) and for members of the Montana Legislature. Extensive new constitutional rights for victims of crime were approved in 2016.

The 1972 constitution requires that voters determine every 20 years whether to hold a new constitutional convention. Voters turned down a new convention in 1990 (84 percent no) and again in 2010 (58.6 percent no).

Montana has three branches of state government: Legislative, executive, and judicial. The executive branch is headed by an elected governor. The current Governor is Steve Bullock, a Democrat elected in 2012. There are nine other statewide elected offices in the executive branch as well: Lieutenant Governor, Attorney General, Secretary of State, State Auditor (who also serves as Commissioner of Securities and Insurance), and Superintendent of Public Instruction. There are five Public Service Commissioners, who are elected on a regional basis. (The Public Service Commission's jurisdiction is statewide.)

There are 18 departments and offices which make up the executive branch: Administration; Agriculture; Auditor (securities and insurance); Commerce; Corrections; Environmental Quality; Fish, Wildlife & Parks; Justice; Labor and Industry; Livestock; Military Affairs; Natural Resources and Conservation; Public Health and Human Services; Revenue; State; and Transportation. Elementary and secondary education are overseen by the Office of Public Instruction (led by the elected Superintendent of Public Instruction), in cooperation with the governor-appointed Board of Public Education. Higher education is overseen by a governor-appointed Board of Regents, which in turn appoints a Commissioner of Higher Education. The Office of the Commissioner of Higher Education acts in an executive capacity on behalf of the regents, and oversees the state-run Montana University System.

Independent state agencies, not located within a department or office, include the Montana Arts Council, Montana Board of Crime Control, Montana Historical Society, Montana Public Employees Retirement Administration, Commissioner of Political Practices, the Montana Lottery, Office of the State Public Defender, Public Service Commission, the Montana School for the Deaf and Blind, the Montana State Fund (which operates the state's unemployment insurance, worker compensation, and self-insurance operations), the Montana State Library, and the Montana Teachers Retirement System.

Montana is an Alcoholic beverage control state. It is an equitable distribution and no-fault divorce state. It is one of five states to have no sales tax.

The Montana Legislature is bicameral, and consists of the 50-member Montana Senate and the 100-member Montana House of Representatives. The legislature meets in the Montana State Capitol in Helena in odd-numbered years for 90 days, beginning the first weekday of the year. The deadline for a legislator to introduce a general bill is the 40th legislative day. The deadline for a legislator to introduce an appropriations, revenue, or referenda bill is the 62nd legislative day. Senators serve four-year terms, while Representatives serve two-year terms. All members are limited to serving no more than eight years in a single 16-year period.

The Courts of Montana are established by the Constitution of Montana. The constitution requires the establishment of a Montana Supreme Court and Montana District Courts, and permits the legislature to establish Justice Courts, City Courts, Municipal Courts, and other inferior courts such as the legislature sees fit to establish.

The Montana Supreme Court is the court of last resort in the Montana court system. The constitution of 1889 provided for the election of no fewer than three Supreme Court justices, and one Chief Justice. Each court member served a six-year term. The legislature increased the number of justices to five in 1919. The 1972 constitution lengthened the term of office to eight years, and established the minimum number of justices at five. It allowed the legislature to increase the number of justices by two, which the legislature did in 1979. The Montana Supreme Court has the authority to declare acts of the legislature and executive unconstitutional under either the Montana or U.S. constitutions. Its decisions may be appealed directly to the U.S. Supreme Court. The Clerk of the Supreme Court is also an elected position, and serves a six-year term. Neither justices nor the clerk are term limited.

Montana District Courts are the courts of general jurisdiction in Montana. There are no intermediate appellate courts. District Courts have jurisdiction primarily over most civil cases, cases involving a monetary claim against the state, felony criminal cases, probate, and cases at law and in equity. When so authorized by the legislature, actions of executive branch agencies may be appealed directly to a District Court. The District Courts also have "de novo" appellate jurisdiction from inferior courts (city courts, justice courts, and municipal courts), and oversee naturalization proceedings. District Court judges are elected, and serve six-year terms. They are not term limited. There are 22 judicial districts in Montana, served by 56 District Courts and 46 District Court judges. The District Courts suffer from excessive workload, and the legislature has struggled to find a solution to the problem.

Montana Youth Courts were established by the Montana Youth Court Act of 1974. They are overseen by District Court judges. They consist of a chief probation officer, one or more juvenile probation officers, and support staff. Youth Courts have jurisdiction over misdemeanor and felony acts committed by those charged as a juvenile under the law. There is a Youth Court in every judicial district, and decisions of the Youth Court are appealable directly to the Montana Supreme Court.

The Montana Worker's Compensation Court was established by the Montana Workers' Compensation Act in 1975. There is a single Workers' Compensation Court. It has a single judge, appointed by the governor. The Worker's Compensation Court has statewide jurisdiction and holds trials in Billings, Great Falls, Helena, Kalispell, and Missoula. The court hears cases arising under the Montana Workers' Compensation Act, and is the court of original jurisdiction for reviews of orders and regulations issued by the Montana Department of Labor and Industry. Decisions of the court are appealable directly to the Montana Supreme Court.

The Montana Water Court was established by the Montana Water Court Act of 1979. The Water Court consists of a Chief Water Judge and four District Water Judges (Lower Missouri River Basin, Upper Missouri River Basin, Yellowstone River Basin, and Clark Fork River Basin). The court employs 12 permanent special masters. The Montana Judicial Nomination Commission develops short lists of nominees for all five Water Judges, who are then appointed by the Chief Justice of the Montana Supreme Court (subject to confirmation by the Montana Senate). The Water Court adjudicates water rights claims under the Montana Water Use Act of 1973, and has statewide jurisdiction. District Courts have the authority to enforce decisions of the Water Court, but only the Montana Supreme Court has the authority to review decisions of the Water Court.

From 1889 to 1909, elections for judicial office in Montana were partisan. Beginning in 1909, these elections became nonpartisan. The Montana Supreme Court struck down the nonpartisan law in 1911 on technical grounds, but a new law was enacted in 1935 which barred political parties from endorsing, making contributions to, or making expenditures on behalf of or against judicial candidates. In 2012, the U.S. Supreme Court struck down Montana's judicial nonpartisan election law in Although candidates must remain nonpartisan, spending by partisan entities is now permitted. Spending on state supreme court races exponentially increased to $1.6 million in 2014, and to more than $1.6 million in 2016 (both new records).

The U.S. Constitution provides each state with two Senators. Montana's two U.S. senators are Jon Tester (Democrat), last reelected in 2012, and Steve Daines (Republican), first elected in 2014. The U.S. Constitution provides each state with a single Representative, with additional representatives apportioned based on population. From statehood in 1889 until 1913, Montana was represented in the United States House of Representatives by a single representative, elected at-large. Montana received a second representative in 1913, following the 1910 census and reapportionment. Both members, however, were still elected at-large. Beginning in 1919, Montana moved to district, rather than at-large, elections for its two House members. This created Montana's 1st congressional district in the west and Montana's 2nd congressional district in the east. In the reapportionment following the 1990 census, Montana lost one of its House seats. The remaining seat was again elected at-large. Greg Gianforte is the current officeholder.

Montana's Senate district is the fourth largest by area, behind Alaska, Texas, and California. The most notorious of Montana's early Senators was William A. Clark, a "Copper King" and one of the 50 richest Americans ever. He is well known for having bribed his way into the U.S. Senate. Among Montana's most historically prominent Senators are Thomas J. Walsh (serving from 1913 to 1933), who was President-elect Franklin D. Roosevelt's choice for Attorney General when he died; Burton K. Wheeler (serving from 1923 to 1947), an oft-mentioned presidential candidate and strong supporter of isolationism; Mike Mansfield, the longest-serving Senate Majority Leader in U.S. history; Max Baucus (served 1978 to 2014), longest-serving U.S. Senator in Montana history, and the senator who shepherded the Patient Protection and Affordable Care Act through the Senate in 2010; and Lee Metcalf (served 1961 to 1978), a pioneer of the environmental movement.

Montana's House district is currently the largest congressional district in the United States by population, with just over 1,023,000 constituents. It is currently the second largest House district by area, after Alaska's at-large congressional district. Of Montana's House delegates, Jeannette Rankin was the first woman to hold national office in the United States when she was elected to the U.S. House of Representatives in 1916. Also notable is Representative (later Senator) Thomas H. Carter, the first Catholic to serve as chairman of the Republican National Committee (from 1892 to 1896).

Federal courts located in Montana include the United States District Court for the District of Montana and the United States Bankruptcy Court for the District of Montana. Three former Montana politicians have been named judges on the U.S. District Court: Charles Nelson Pray (who served in the U.S. House of Representatives from 1907 to 1913), James Franklin Battin (who served in the U.S. House of Representatives from 1961 to 1969), and Paul G. Hatfield (who served as an appointed U.S. Senator in 1978). Brian Morris, who served as an Associate Justice of the Montana Supreme Court from 2005 to 2013, currently served as a judge on the court.

Elections in the state have been competitive, with the Democrats usually holding an edge, thanks to the support among unionized miners and railroad workers. Large-scale battles revolved around the giant Anaconda Copper company, based in Butte and controlled by Rockefeller interests, until it closed in the 1970s. Until 1959, the company owned five of the state's six largest newspapers.

Historically, Montana is a swing state of cross-ticket voters who tend to fill elected offices with individuals from both parties. Through the mid-20th century, the state had a tradition of "sending the liberals to Washington and the conservatives to Helena." Between 1988 and 2006, the pattern flipped, with voters more likely to elect conservatives to federal offices. There have also been long-term shifts of party control. From 1968 through 1988, the state was dominated by the Democratic Party, with Democratic governors for a 20-year period, and a Democratic majority of both the national congressional delegation and during many sessions of the state legislature. This pattern shifted, beginning with the 1988 election, when Montana elected a Republican governor for the first time since 1964 and sent a Republican to the U.S. Senate for the first time since 1948. This shift continued with the reapportionment of the state's legislative districts that took effect in 1994, when the Republican Party took control of both chambers of the state legislature, consolidating a Republican party dominance that lasted until the 2004 reapportionment produced more swing districts and a brief period of Democratic legislative majorities in the mid-2000s.

In more recent presidential elections, Montana has voted for the Republican candidate in all but two elections from 1952 to the present. The state last supported a Democrat for president in 1992, when Bill Clinton won a plurality victory. Overall, since 1889 the state has voted for Democratic governors 60 percent of the time and Republican presidents 40 percent of the time. In the 2008 presidential election, Montana was considered a swing state and was ultimately won by Republican John McCain, albeit by a narrow margin of two percent.

At the state level, the pattern of split-ticket voting and divided government holds. Democrats currently hold one of the state's U.S. Senate seats, as well as one of the five statewide offices (Governor). The lone congressional district has been Republican since 1996 and in 2014 Steve Daines won one of the state's Senate seats for the GOP. The Legislative branch had split party control between the house and senate most years between 2004 and 2010, when the mid-term elections returned both branches to Republican control. The state Senate is, as of 2017, controlled by the Republicans 32 to 18, and the State House of Representatives at 59 to 41. Historically, Republicans are strongest in the east, while Democrats are strongest in the west.

Montana currently has only one representative in the U.S. House, having lost its second district in the 1990 census reapportionment. Montana's single congressional district holds the largest population of any district in the country, which means its one member in the House of Representatives represents more people than any other member of the U.S. House (see List of U.S. states by population). Montana's population grew at about the national average during the 2000s, but it failed to regain its second seat in 2010. Like all other states, Montana has two senators.

Montana has 56 counties with the United States Census Bureau stating Montana's contains 364 "places", broken down into 129 incorporated places and 235 census-designated places. Incorporated places consist of 52 cities, 75 towns, and two consolidated city-counties. Montana has one city, Billings, with a population over 100,000; and two cities with populations over 50,000, Missoula and Great Falls. These three communities are considered the centers of Montana's three Metropolitan Statistical Areas.

The state also has five Micropolitan Statistical Areas centered on Bozeman, Butte, Helena, Kalispell and Havre. These communities, excluding Havre, are colloquially known as the "big 7" Montana cities, as they are consistently the seven largest communities in Montana, with a significant population difference when these communities are compared to those that are 8th or lower on the list. According to the 2010 U.S. Census, the population of Montana's seven most populous cities, in rank order, are Billings, Missoula, Great Falls, Bozeman, Butte, Helena and Kalispell. Based on 2013 census numbers, they collectively contain 35 percent of Montana's population. and the counties containing these communities hold 62 percent of the state's population. The geographic center of population of Montana is located in sparsely populated Meagher County, in the town of White Sulphur Springs.

Montana's motto, "Oro y Plata", Spanish for "Gold and Silver", recognizing the significant role of mining, was first adopted in 1865, when Montana was still a territory. A state seal with a miner's pick and shovel above the motto, surrounded by the mountains and the Great Falls of the Missouri River, was adopted during the first meeting of the territorial legislature in 1864–65. The design was only slightly modified after Montana became a state and adopted it as the Great Seal of the State of Montana, enacted by the legislature in 1893. The state flower, the Bitterroot, was adopted in 1895 with the support of a group called the Floral Emblem Association, which formed after Montana's Women's Christian Temperance Union adopted the bitterroot as the organization's state flower. All other symbols were adopted throughout the 20th century, save for Montana's newest symbol, the state butterfly, the mourning cloak, adopted in 2001, and the state lullaby, "Montana Lullaby", adopted in 2007.

The state song was not composed until 21 years after statehood, when a musical troupe led by Joseph E. Howard stopped in Butte in September 1910. A former member of the troupe who lived in Butte buttonholed Howard at an after-show party, asking him to compose a song about Montana and got another partygoer, the city editor for the "Butte Miner" newspaper, Charles C. Cohan, to help. The two men worked up a basic melody and lyrics in about a half-hour for the entertainment of party guests, then finished the song later that evening, with an arrangement worked up the following day. Upon arriving in Helena, Howard's troupe performed 12 encores of the new song to an enthusiastic audience and the governor proclaimed it the state song on the spot, though formal legislative recognition did not occur until 1945. Montana is one of only three states to have a "state ballad", "Montana Melody", chosen by the legislature in 1983. Montana was the first state to also adopt a State Lullaby.

Montana schoolchildren played a significant role in selecting several state symbols. The state tree, the ponderosa pine, was selected by Montana schoolchildren as the preferred state tree by an overwhelming majority in a referendum held in 1908. However, the legislature did not designate a state tree until 1949, when the Montana Federation of Garden Clubs, with the support of the state forester, lobbied for formal recognition. Schoolchildren also chose the western meadowlark as the state bird, in a 1930 vote, and the legislature acted to endorse this decision in 1931. Similarly, the secretary of state sponsored a children's vote in 1981 to choose a state animal, and after 74 animals were nominated, the grizzly bear won over the elk by a 2–1 margin. The students of Livingston started a statewide school petition drive plus lobbied the governor and the state legislature to name the "Maiasaura" as the state fossil in 1985.

Various community civic groups also played a role in selecting the state grass and the state gemstones. When broadcaster Norma Ashby discovered there was no state fish, she initiated a drive via her television show, "Today in Montana", and an informal citizen's election to select a state fish resulted in a win for the blackspotted cutthroat trout after hot competition from the Arctic grayling. The legislature in turn adopted this recommendation by a wide margin.




</doc>
<doc id="19980" url="https://en.wikipedia.org/wiki?curid=19980" title="Machine translation">
Machine translation

Machine translation, sometimes referred to by the abbreviation MT (not to be confused with computer-aided translation, machine-aided human translation (MAHT) or interactive translation) is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.

On a basic level, MT performs simple substitution of words in one language for words in another, but that alone usually cannot produce a good translation of a text because recognition of whole phrases and their closest counterparts in the target language is needed. Solving this problem with corpus statistical, and neural techniques is a rapidly growing field that is leading to better translations, handling differences in linguistic typology, translation of idioms, and the isolation of anomalies.

Current machine translation software often allows for customization by domain or profession (such as weather reports), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text.

Improved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports).

The progress and potential of machine translation have been debated much through its history. Since the 1950s, a number of scholars have questioned the possibility of achieving fully automatic machine translation of high quality. Some critics claim that there are in-principle obstacles to automating the translation process.

The idea of machine translation may be traced back to the 17th century. In 1629, René Descartes proposed a universal language, with equivalent ideas in different tongues sharing one symbol. The field of "machine translation" appeared in Warren Weaver's Memorandum on Translation (1949). The first researcher in the field, Yehosha Bar-Hillel, began his research at MIT (1951). A Georgetown University MT research team followed (1951) with a public demonstration of its Georgetown-IBM experiment system in 1954. MT research programs popped up in Japan and Russia (1955), and the first MT conference was held in London (1956). Researchers continued to join the field as the Association for Machine Translation and Computational Linguistics was formed in the U.S. (1962) and the National Academy of Sciences formed the Automatic Language Processing Advisory Committee (ALPAC) to study MT (1964). Real progress was much slower, however, and after the ALPAC report (1966), which found that the ten-year-long research had failed to fulfill expectations, funding was greatly reduced. According to a 1972 report by the Director of Defense Research and Engineering (DDR&E), the feasibility of large-scale MT was reestablished by the success of the Logos MT system in translating military manuals into Vietnamese during that conflict.

The French Textile Institute also used MT to translate abstracts from and into French, English, German and Spanish (1970); Brigham Young University started a project to translate Mormon texts by automated translation (1971); and Xerox used SYSTRAN to translate technical manuals (1978). Beginning in the late 1980s, as computational power increased and became less expensive, more interest was shown in statistical models for machine translation. Various MT companies were launched, including Trados (1984), which was the first to develop and market translation memory technology (1989). The first commercial MT system for Russian / English / German-Ukrainian was developed at Kharkov State University (1991).

MT on the web started with SYSTRAN Offering free translation of small texts (1996), followed by AltaVista Babelfish, which racked up 500,000 requests a day (1997). Franz-Josef Och (the future head of Translation Development AT Google) won DARPA's speed MT competition (2003). More innovations during this time included MOSES, the open-source statistical MT engine (2007), a text/SMS translation service for mobiles in Japan (2008), and a mobile phone with built-in speech-to-speech translation functionality for English, Japanese and Chinese (2009). Recently, Google announced that Google Translate translates roughly enough text to fill 1 million books in one day (2012).

The idea of using digital computers for translation of natural languages was proposed as early as 1946 by A. D. Booth and possibly others. Warren Weaver wrote an important memorandum "Translation" in 1949. The Georgetown experiment was by no means the first such application, and a demonstration was made in 1954 on the APEXC machine at Birkbeck College (University of London) of a rudimentary translation of English into French. Several papers on the topic were published at the time, and even articles in popular journals (see for example "Wireless World", Sept. 1955, Cleave and Zacharov). A similar application, also pioneered at Birkbeck College at the time, was reading and composing Braille texts by computer.

The human translation process may be described as:

Behind this ostensibly simple procedure lies a complex cognitive operation. To decode the meaning of the source text in its entirety, the translator must interpret and analyse all the features of the text, a process that requires in-depth knowledge of the grammar, semantics, syntax, idioms, etc., of the source language, as well as the culture of its speakers. The translator needs the same in-depth knowledge to re-encode the meaning in the target language.

Therein lies the challenge in machine translation: how to program a computer that will "understand" a text as a person does, and that will "create" a new text in the target language that sounds as if it has been written by a person.

In its most general application, this is beyond current technology. Though it works much faster, no automated translation program or procedure, with no human participation, can produce output even close to the quality a human translator can produce. What it can do, however, is provide a general, though imperfect, approximation of the original text, getting the "gist" of it (a process called "gisting"). This is sufficient for many purposes, including making best use of the finite and expensive time of a human translator, reserved for those cases in which total accuracy is indispensable.

This problem may be approached in a number of ways, through the evolution of which accuracy has improved.

Machine translation can use a method based on linguistic rules, which means that words will be translated in a linguistic way – the most suitable (orally speaking) words of the target language will replace the ones in the source language.

It is often argued that the success of machine translation requires the problem of natural language understanding to be solved first.

Generally, rule-based methods parse a text, usually creating an intermediary, symbolic representation, from which the text in the target language is generated. According to the nature of the intermediary representation, an approach is described as interlingual machine translation or transfer-based machine translation. These methods require extensive lexicons with morphological, syntactic, and semantic information, and large sets of rules.

Given enough data, machine translation programs often work well enough for a native speaker of one language to get the approximate meaning of what is written by the other native speaker. The difficulty is getting enough data of the right kind to support the particular method. For example, the large multilingual corpus of data needed for statistical methods to work is not necessary for the grammar-based methods. But then, the grammar methods need a skilled linguist to carefully design the grammar that they use.

To translate between closely related languages, the technique referred to as rule-based machine translation may be used.

The rule-based machine translation paradigm includes transfer-based machine translation, interlingual machine translation and dictionary-based machine translation paradigms. This type of translation is used mostly in the creation of dictionaries and grammar programs. Unlike other methods, RBMT involves more information about the linguistics of the source and target languages, using the morphological and syntactic rules and semantic analysis of both languages. The basic approach involves linking the structure of the input sentence with the structure of the output sentence using a parser and an analyzer for the source language, a generator for the target language, and a transfer lexicon for the actual translation. RBMT's biggest downfall is that everything must be made explicit: orthographical variation and erroneous input must be made part of the source language analyser in order to cope with it, and lexical selection rules must be written for all instances of ambiguity. Adapting to new domains in itself is not that hard, as the core grammar is the same across domains, and the domain-specific adjustment is limited to lexical selection adjustment.

Transfer-based machine translation is similar to interlingual machine translation in that it creates a translation from an intermediate representation that simulates the meaning of the original sentence. Unlike interlingual MT, it depends partially on the language pair involved in the translation.

Interlingual machine translation is one instance of rule-based machine-translation approaches. In this approach, the source language, i.e. the text to be translated, is transformed into an interlingual language, i.e. a "language neutral" representation that is independent of any language. The target language is then generated out of the interlingua. One of the major advantages of this system is that the interlingua becomes more valuable as the number of target languages it can be turned into increases. However, the only interlingual machine translation system that has been made operational at the commercial level is the KANT system (Nyberg and Mitamura, 1992), which is designed to translate Caterpillar Technical English (CTE) into other languages.

Machine translation can use a method based on dictionary entries, which means that the words will be translated as they are by a dictionary.

Statistical machine translation tries to generate translations using statistical methods based on bilingual text corpora, such as the Canadian Hansard corpus, the English-French record of the Canadian parliament and EUROPARL, the record of the European Parliament. Where such corpora are available, good results can be achieved translating similar texts, but such corpora are still rare for many language pairs. The first statistical machine translation software was CANDIDE from IBM. Google used SYSTRAN for several years, but switched to a statistical translation method in October 2007. In 2005, Google improved its internal translation capabilities by using approximately 200 billion words from United Nations materials to train their system; translation accuracy improved. Google Translate and similar statistical translation programs work by detecting patterns in hundreds of millions of documents that have previously been translated by humans and making intelligent guesses based on the findings. Generally, the more human-translated documents available in a given language, the more likely it is that the translation will be of good quality. Newer approaches into Statistical Machine translation such as METIS II and PRESEMT use minimal corpus size and instead focus on derivation of syntactic structure through pattern recognition. With further development, this may allow statistical machine translation to operate off of a monolingual text corpus. SMT's biggest downfall includes it being dependent upon huge amounts of parallel texts, its problems with morphology-rich languages (especially with translating "into" such languages), and its inability to correct singleton errors.

Example-based machine translation (EBMT) approach was proposed by Makoto Nagao in 1984. Example-based machine translation is based on the idea of analogy. In this approach, the corpus that is used is one that contains texts that have already been translated. Given a sentence that is to be translated, sentences from this corpus are selected that contain similar sub-sentential components. The similar sentences are then used to translate the sub-sentential components of the original sentence into the target language, and these phrases are put together to form a complete translation.

Hybrid machine translation (HMT) leverages the strengths of statistical and rule-based translation methodologies. Several MT organizations (such as Omniscien Technologies (formerly Asia Online), LinguaSys, Systran, and Polytechnic University of Valencia) claim a hybrid approach that uses both rules and statistics. The approaches differ in a number of ways:
More recently, with the advent of Neural MT, a new version of hybrid machine translation is emerging that combines the benefits of rules, statistical and neural machine translation. The approach allows benefitting from pre- and post-processing in a rule guided workflow as well as benefitting from NMT and SMT. The downside is the inherent complexity which makes the approach suitable only for specific use cases. One of the proponents of this approach for complex use cases is Omniscien Technologies.

A deep learning based approach to MT, neural machine translation has made rapid progress in recent years, and Google has announced its translation services are now using this technology in preference to its previous statistical methods. Other providers including Pangeanic, KantanMT, Omniscien Technologies and SDL have announced the deployment of neural machine translation technology in 2017 as well.

Word-sense disambiguation concerns finding a suitable translation when a word can have more than one meaning. The problem was first raised in the 1950s by Yehoshua Bar-Hillel. He pointed out that without a "universal encyclopedia", a machine would never be able to distinguish between the two meanings of a word. Today there are numerous approaches designed to overcome this problem. They can be approximately divided into "shallow" approaches and "deep" approaches.

Shallow approaches assume no knowledge of the text. They simply apply statistical methods to the words surrounding the ambiguous word. Deep approaches presume a comprehensive knowledge of the word. So far, shallow approaches have been more successful.

Claude Piron, a long-time translator for the United Nations and the World Health Organization, wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved:

The ideal deep approach would require the translation software to do all the research necessary for this kind of disambiguation on its own; but this would require a higher degree of AI than has yet been attained. A shallow approach which simply guessed at the sense of the ambiguous English phrase that Piron mentions (based, perhaps, on which kind of prisoner-of-war camp is more often mentioned in a given corpus) would have a reasonable chance of guessing wrong fairly often. A shallow approach that involves "ask the user about each ambiguity" would, by Piron's estimate, only automate about 25% of a professional translator's job, leaving the harder 75% still to be done by a human.

One of the major pitfalls of MT is its inability to translate non-standard language with the same accuracy as standard language. Heuristic or statistical based MT takes input from various sources in standard form of a language. Rule-based translation, by nature, does not include common non-standard usages. This causes errors in translation from a vernacular source or into colloquial language. Limitations on translation from casual speech present issues in the use of machine translation in mobile devices.

Name entities, in narrow sense, refer to concrete or abstract entities in the real world including people, organizations, companies, places etc. It also refers to expressing of time, space, quantity such as 1 July 2011, $79.99 and so on.

Named entities occur in the text being analyzed in statistical machine translation. The initial difficulty that arises in dealing with named entities is simply identifying them in the text. Consider the list of names common in a particular language to illustrate this – the most common names are different for each language and also are constantly changing. If named entities cannot be recognized by the machine translator, they may be erroneously translated as common nouns, which would most likely not affect the BLEU rating of the translation but would change the text's human readability. It is also possible that, when not identified, named entities will be omitted from the output translation, which would also have implications for the text's readability and message.

Another way to deal with named entities is to use transliteration instead of translation, meaning that you find the letters in the target language that most closely correspond to the name in the source language. There have been attempts to incorporate this into machine translation by adding a transliteration step into the translation procedure. However, these attempts still have their problems and have even been cited as worsening the quality of translation. Named entities were still identified incorrectly, with words not being transliterated when they should or being transliterated when they shouldn't. For example, for "Southern California" the first word should be translated directly, while the second word should be transliterated. However, machines would often transliterate both because they treated them as one entity. Words like these are hard for machine translators, even those with a transliteration component, to process.

The lack of attention to the issue of named entity translation has been recognized as potentially stemming from a lack of resources to devote to the task in addition to the complexity of creating a good system for named entity translation. One approach to named entity translation has been to transliterate, and not translate, those words. A second is to create a "do-not-translate" list, which has the same end goal – transliteration as opposed to translation. Both of these approaches still rely on the correct identification of named entities, however.

A third approach to successful named entity translation is a class-based model. In this method, named entities are replaced with a token to represent the class they belong to. For example, "Ted" and "Erica" would both be replaced with "person" class token. In this way the statistical distribution and use of person names in general can be analyzed instead of looking at the distributions of "Ted" and "Erica" individually. A problem that the class based model solves is that the probability of a given name in a specific language will not affect the assigned probability of a translation. A study by Stanford on improving this area of translation gives the examples that different probabilities will be assigned to "David is going for a walk" and "Ankit is going for a walk" for English as a target language due to the different number of occurrences for each name in the training data. A frustrating outcome of the same study by Stanford (and other attempts to improve named recognition translation) is that many times, a decrease in the BLEU scores for translation will result from the inclusion of methods for named entity translation.

Some work has been done in the utilization of multiparallel corpora, that is a body of text that has been translated into 3 or more languages. Using these methods, a text that has been translated into 2 or more languages may be utilized in combination to provide a more accurate translation into a third language compared with if just one of those source languages were used alone.

An ontology is a formal representation of knowledge which includes the concepts (such as objects, processes etc.) in a domain and some relations between them. If the stored information is of linguistic nature, one can speak of a lexicon.
In NLP, ontologies can be used as a source of knowledge for machine translation systems. With access to a large knowledge base, systems can be enabled to resolve many (especially lexical) ambiguities on their own.
In the following classic examples, as humans, we are able to interpret the prepositional phrase according to the context because we use our world knowledge, stored in our lexicons:
A machine translation system initially would not be able to differentiate between the meanings because syntax does not change. With a large enough ontology as a source of knowledge however, the possible interpretations of ambiguous words in a specific context can be reduced.
Other areas of usage for ontologies within NLP include information retrieval, information extraction and text summarization.

The ontology generated for the PANGLOSS knowledge-based machine translation system in 1993 may serve as an example of how an ontology for NLP purposes can be compiled:

While no system provides the holy grail of fully automatic high-quality machine translation of unrestricted text, many fully automated systems produce reasonable output. The quality of machine translation is substantially improved if the domain is restricted and controlled.

Despite their inherent limitations, MT programs are used around the world. Probably the largest institutional user is the European Commission. The MOLTO project, for example, coordinated by the University of Gothenburg, received more than 2.375 million euros project support from the EU to create a reliable translation tool that covers a majority of the EU languages. The further development of MT systems comes at a time when budget cuts in human translation may increase the EU's dependency on reliable MT programs. The European Commission contributed 3.072 million euros (via its ISA programme) for the creation of MT@EC, a statistical machine translation program tailored to the administrative needs of the EU, to replace a previous rule-based machine translation system.

Google has claimed that promising results were obtained using a proprietary statistical machine translation engine. The statistical translation engine used in the Google language tools for Arabic <-> English and Chinese <-> English had an overall score of 0.4281 over the runner-up IBM's BLEU-4 score of 0.3954 (Summer 2006) in tests conducted by the National Institute for Standards and Technology.

With the recent focus on terrorism, the military sources in the United States have been investing significant amounts of money in natural language engineering. "In-Q-Tel" (a venture capital fund, largely funded by the US Intelligence Community, to stimulate new technologies through private sector entrepreneurs) brought up companies like Language Weaver. Currently the military community is interested in translation and processing of languages like Arabic, Pashto, and Dari. Within these languages, the focus is on key phrases and quick communication between military members and civilians through the use of mobile phone apps. The Information Processing Technology Office in DARPA hosts programs like TIDES and Babylon translator. US Air Force has awarded a $1 million contract to develop a language translation technology.

The notable rise of social networking on the web in recent years has created yet another niche for the application of machine translation software – in utilities such as Facebook, or instant messaging clients such as Skype, GoogleTalk, MSN Messenger, etc. – allowing users speaking different languages to communicate with each other. Machine translation applications have also been released for most mobile devices, including mobile telephones, pocket PCs, PDAs, etc. Due to their portability, such instruments have come to be designated as mobile translation tools enabling mobile business networking between partners speaking different languages, or facilitating both foreign language learning and unaccompanied traveling to foreign countries without the need of the intermediation of a human translator.

Despite being labelled as an unworthy competitor to human translation in 1966 by the Automated Language Processing Advisory Committee put together by the United States government, the quality of machine translation has now been improved to such levels that its application in online collaboration and in the medical field are being investigated. In the Ishida and Matsubara lab of Kyoto University, methods of improving the accuracy of machine translation as a support tool for inter-cultural collaboration in today's globalized society are being studied. The application of this technology in medical settings where human translators are absent is another topic of research however difficulties arise due to the importance of accurate translations in medical diagnoses.

There are many factors that affect how machine translation systems are evaluated. These factors include the intended use of the translation, the nature of the machine translation software, and the nature of the translation process.

Different programs may work well for different purposes. For example, statistical machine translation (SMT) typically outperforms example-based machine translation (EBMT), but researchers found that when evaluating English to French translation, EBMT performs better. The same concept applies for technical documents, which can be more easily translated by SMT because of their formal language.

In certain applications, however, e.g., product descriptions written in a controlled language, a dictionary-based machine-translation system has produced satisfactory translations that require no human intervention save for quality inspection.

There are various means for evaluating the output quality of machine translation systems. The oldest is the use of human judges to assess a translation's quality. Even though human evaluation is time-consuming, it is still the most reliable method to compare different systems such as rule-based and statistical systems. Automated means of evaluation include BLEU, NIST, METEOR, and LEPOR.

Relying exclusively on unedited machine translation ignores the fact that communication in human language is context-embedded and that it takes a person to comprehend the context of the original text with a reasonable degree of probability. It is certainly true that even purely human-generated translations are prone to error. Therefore, to ensure that a machine-generated translation will be useful to a human being and that publishable-quality translation is achieved, such translations must be reviewed and edited by a human. The late Claude Piron wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved. Such research is a necessary prelude to the pre-editing necessary in order to provide input for machine-translation software such that the output will not be meaningless.

In addition to disambiguation problems, decreased accuracy can occur due to varying levels of training data for machine translating programs. Both example-based and statistical machine translation rely on a vast array of real example sentences as a base for translation, and when too many or too few sentences are analyzed accuracy is jeopardized. Researchers found that when a program is trained on 203,529 sentence pairings, accuracy actually decreases. The optimal level of training data seems to be just over 100,000 sentences, possibly because as training data increases, the number of possible sentences increases, making it harder to find an exact translation match.

Although there have been concerns about machine translation's accuracy, Dr. Ana Nino of the University of Manchester has researched some of the advantages in utilizing machine translation in the classroom. One such pedagogical method is called using "MT as a Bad Model." MT as a Bad Model forces the language learner to identify inconsistencies or incorrect aspects of a translation; in turn, the individual will (hopefully) possess a better grasp of the language. Dr. Nino cites that this teaching tool was implemented in the late 1980s. At the end of various semesters, Dr. Nino was able to obtain survey results from students who had used MT as a Bad Model (as well as other models.) Overwhelmingly, students felt that they had observed improved comprehension, lexical retrieval, and increased confidence in their target language.

In the early 2000s, options for machine translation between spoken and signed languages were severely limited. It was a common belief that deaf individuals could use traditional translators. However, stress, intonation, pitch, and timing are conveyed much differently in spoken languages compared to signed languages. Therefore, a deaf individual may misinterpret or become confused about the meaning of written text that is based on a spoken language.

Researchers Zhao, et al. (2000), developed a prototype called TEAM (translation from English to ASL by machine) that completed English to American Sign Language (ASL) translations. The program would first analyze the syntactic, grammatical, and morphological aspects of the English text. Following this step, the program accessed a sign synthesizer, which acted as a dictionary for ASL. This synthesizer housed the process one must follow to complete ASL signs, as well as the meanings of these signs. Once the entire text is analyzed and the signs necessary to complete the translation are located in the synthesizer, a computer generated human appeared and would use ASL to sign the English text to the user.

Only works that are original are subject to copyright protection, so some scholars claim that machine translation results are not entitled to copyright protection because MT does not involve creativity. The copyright at issue is for a derivative work; the author of the original work in the original language does not lose his rights when a work is translated: a translator must have permission to publish a translation.





</doc>
<doc id="19983" url="https://en.wikipedia.org/wiki?curid=19983" title="Central moment">
Central moment

In probability theory and statistics, a central moment is a moment of a probability distribution of a random variable about the random variable's mean; that is, it is the expected value of a specified integer power of the deviation of the random variable from the mean. The various moments form one set of values by which the properties of a probability distribution can be usefully characterised. Central moments are used in preference to ordinary moments, computed in terms of deviations from the mean instead of from zero, because the higher-order central moments relate only to the spread and shape of the distribution, rather than also to its location.

Sets of central moments can be defined for both univariate and multivariate distributions.

The "n"th moment about the mean (or "n"th central moment) of a real-valued random variable "X" is the quantity μ := E[("X" − E["X"])], where E is the expectation operator. For a continuous univariate probability distribution with probability density function "f"("x"), the "n"th moment about the mean μ is

For random variables that have no mean, such as the Cauchy distribution, central moments are not defined.

The first few central moments have intuitive interpretations:

The "n"th central moment is translation-invariant, i.e. for any random variable "X" and any constant "c", we have

For all "n", the "n"th central moment is homogeneous of degree "n":

"Only" for "n" such that n equals 1, 2, or 3 do we have an additivity property for random variables "X" and "Y" that are independent:

A related functional that shares the translation-invariance and homogeneity properties with the "n"th central moment, but continues to have this additivity property even when "n" ≥ 4 is the "n"th cumulant κ("X"). For "n" = 1, the "n"th cumulant is just the expected value; for "n" = either 2 or 3, the "n"th cumulant is just the "n"th central moment; for "n" ≥ 4, the "n"th cumulant is an "n"th-degree monic polynomial in the first "n" moments (about zero), and is also a (simpler) "n"th-degree polynomial in the first "n" central moments.

Sometimes it is convenient to convert moments about the origin to moments about the mean. The general equation for converting the "n"th-order moment about the origin to the moment about the mean is

where μ is the mean of the distribution, and the moment about the origin is given by

For the cases "n" = 2, 3, 4 — which are of most interest because of the relations to variance, skewness, and kurtosis, respectively — this formula becomes (noting that formula_7 and formula_8):,

... and so on, following Pascal's triangle, i.e.

because formula_14

The following sum is a stochastic variable having a compound distribution

where the formula_16 are mutually independent random variables sharing the same common distribution and formula_17 a random integer variable independent of the formula_18 with its own distribution. The moments of formula_19 are obtained as 

where formula_21 is defined as zero for formula_22.

In a symmetric distribution (one that is unaffected by being reflected about its mean), all odd central moments equal zero, because in the formula for the "n"th moment, each term involving a value of "X" less than the mean by a certain amount exactly cancels out the term involving a value of "X" greater than the mean by the same amount.

For a continuous bivariate probability distribution with probability density function "f"("x","y") the ("j","k") moment about the mean μ = (μ, μ) is



</doc>
<doc id="19986" url="https://en.wikipedia.org/wiki?curid=19986" title="Murad I">
Murad I

Murad I (; (nicknamed Hüdavendigâr, from Persian: خداوندگار, "Khodāvandgār", "the devotee of God" – but meaning "sovereign" in this context); 29 June 1326 – 15 June 1389) was the Ottoman Sultan from 1362 to 1389. He was a son of Orhan and the Valide Nilüfer Hatun.

Murad I conquered Adrianople, renamed it to Edirne, and in 1363 made it the new capital of the Ottoman Sultanate. Then he further expanded the Ottoman realm in Southeast Europe by bringing most of the Balkans under Ottoman rule, and forced the princes of northern Serbia and Bulgaria as well as the Byzantine emperor John V Palaiologos to pay him tribute. Murad I administratively divided his sultanate into the two provinces of Anatolia (Asia Minor) and Rumelia (the Balkans). Murad's death against the Serbs would cause the Ottomans to halt their expansion into the territory temporarily and focus their attention once more on the ailing Byzantine Empire.

Murad fought against the powerful beylik of Karaman in Anatolia and against the Serbs, Albanians, Bulgarians and Hungarians in Europe. In particular, a Serb expedition to expel the Turks from Adrianople led by the Serbian brothers King Vukašin and Despot Uglješa, was defeated on September 26, 1371, by Murad's capable second lieutenant Lala Şâhin Paşa, the first governor ("beylerbey") of Rumeli. In 1385, Sofia fell to the Ottomans. In 1386 Prince Lazar Hrebeljanović defeated an Ottoman force at the Battle of Pločnik. The Ottoman army suffered heavy casualties, and was unable to capture Niš on the way back.

In 1389, Murad's army defeated the Serbian Army and its allies under the leadership of Lazar at the Battle of Kosovo.
There are different accounts from different sources about when and how Murad I was assassinated. The contemporary sources mainly noted that the battle took place and that both Prince Lazar and the Sultan lost their lives in the battle. The existing evidence of the additional stories and speculations as to how Murad I died were disseminated and recorded in the 15th century and later, decades after the actual event. One Western source states that during first hours of the battle, Murad I was assassinated by Serbian nobleman and knight Miloš Obilić by knife. Most Ottoman chroniclers (including Dimitrie Cantemir) state that he was assassinated after the finish of the battle while going around the battlefield. Others state that he was assassinated in the evening after the battle at his tent by the assassin who was admitted to ask a special favour. His older son Bayezid, who was in charge of the left wing of the Ottoman forces, took charge after that. His other son, Yakub Bey, who was in charge of the other wing, was called to the Sultan's command center tent by Bayezid, but when Yakub Bey arrived he was strangled, leaving Bayezid as the sole claimant to the throne.

In a letter from the Florentine senate (written by Coluccio Salutati) to the King Tvrtko I of Bosnia, dated 20 October 1389, Murad I's (and Jakub Bey's) killing was described. A party of twelve Serbian lords slashed their way through the Ottoman lines defending Murad I. One of them, allegedly Miloš Obilić, had managed to get through to the Sultan's tent and kill him with sword stabs to the throat and belly.

Sultan Murad's internal organs were buried in Kosovo field and remains to this day on a corner of the battlefield in a location called "Meshed-i Hudavendigar" which has gained a religious significance by the local Muslims. It has been vandalized between 1999-2006 and renovated recently. His other remains were carried to Bursa, his Anatolian capital city, and were buried in a tomb at the complex built in his name.

He established the sultanate by building up a society and government in the newly conquered city of Adrianople (Edirne in Turkish) and by expanding the realm in Europe, bringing most of the Balkans under Ottoman rule and forcing the Byzantine emperor to pay him tribute. It was Murad who established the former Osmanli tribe into an sultanate. He established the title of sultan in 1383 and the corps of the "janissaries" and the "devşirme" recruiting system. He also organised the government of the "Divan", the system of timars and timar-holders (timariots) and the military judge, the "kazasker". He also established the two provinces of Anadolu (Anatolia) and Rumeli (Europe).

He was the son of Orhan and the Valide Hatun Nilüfer Hatun, daughter of the Prince of Yarhisar, who was of ethnic Greek descent






Notes:

References:


</doc>
<doc id="19987" url="https://en.wikipedia.org/wiki?curid=19987" title="Mehmed I">
Mehmed I

Mehmed I (1379 – 26 May 1421), also known as Mehmed Çelebi (, "the noble-born") or Kirişci (from Greek "Kyritzes", "lord's son"), was the Ottoman Sultan from 1413 to 1421. The fourth son of Sultan Bayezid I and Devlet Hatun, he fought with his brothers over control of the Ottoman realm in the Ottoman Interregnum (1402–1413). Starting from the province of Rûm he managed to bring first Anatolia and then the European territories (Rumelia) under his control, reuniting the Ottoman state by 1413, and ruling it until his death in 1421.

Mehmed was born in 1386 or 1387 as the fourth son of Sultan Bayezid I () and one of his consorts, the slave girl Devlet Hatun. Following Ottoman custom, when he reached adolescence in 1399, he was sent to gain experience as provincial governor over the Rûm Eyalet (central northern Anatolia), recently conquered from its Eretnid rulers.

On 20 July 1402, his father Bayezid was defeated in the Battle of Ankara by the Turko-Mongol conqueror and ruler Timur. The brothers (with the exception of Mustafa, who was captured and taken along with Bayezid to Samarkand) were rescued from the battlefield, Mehmed being saved by Bayezid Pasha, who took him to his hometown of Amasya. Mehmed later made Bayezid Pasha his grand vizier (1413–1421).

The early Ottoman Empire had no regulated succession, and according to Turkish tradition, every son could succeed his father. Of Mehmed's brothers, the eldest, Ertuğrul, had died in 1400, while the next in line, Mustafa, was a prisoner of Timur. Leaving aside the underage siblings, this left four princes—Mehmed, Süleyman, İsa, and Musa, to contend over control of the remaining Ottoman territories in the civil war known as the "Ottoman Interregnum". In modern historiography, these princes are usually called by the title "Çelebi", but in contemporary sources, the title is reserved for Mehmed and Musa. The Byzantine sources translated the title as "Kyritzes" (Κυριτζής), which was in turn adopted into Turkish as "kirişçi", sometimes misinterpreted as "güreşçi", "the wrestler".

After winning the Interregnum, Mehmed crowned himself sultan in the Thracian city of Edirne that lay in the European part of the empire (the area dividing the Anatolian and European sides of the empire, Constantinople and the surrounding region, was still held by the Byzantine Empire), becoming Mehmed I. He consolidated his power, made Edirne the most important of the dual capitals, and conquered parts of Albania, the Jandarid emirate, and the Armenian Kingdom of Cilicia from the Mamelukes. Taking his many achievements into consideration, Mehmed is widely known as the "second founder" of the Ottoman Sultanate.

Soon after Mehmed began his reign, his brother Mustafa Çelebi, who had originally been captured along with their father Bayezid I during the Battle of Ankara and held captive in Samarkand, hiding in Anatolia during the Interregnum, reemerged and asked Mehmed to partition the empire with him. Mehmed refused and met Mustafa's forces in battle, easily defeating them. Mustafa escaped to the Byzantine city of Thessaloniki, but after an agreement with Mehmed, the Byzantine emperor Manuel II Palaiologos exiled Mustafa to the island of Lemnos.

However, Mehmed still faced some problems, first being the problem of his nephew Orhan, who Mehmed perceived as a threat to his rule, much like his late brothers had been. There was allegedly a plot involving him by Manuel II Palaiologos, who tried to use Orhan against Sultan Mehmed; however, the sultan found out about the plot and had Orhan blinded for betrayal, according to a common Byzantine practice.

Furthermore, as a result of the Battle of Ankara and other civil wars, the population of the empire had become unstable and traumatized. A very powerful social and religious movement arose in the empire and became disruptive. The movement was led by Sheikh Bedreddin (1359–1420), a famous Muslim Sufi and charismatic theologian. He was an eminent Ulema, born of a Greek mother and a Muslim father in Simavna (Kyprinos) southwest of Edirne (formerly Adrianople). Mehmed's brother Musa had made Bedreddin his "qadi of the army," or the supreme judge. Bedreddin created a populist religious movement in the Ottoman Sultanate, "subversive conclusions promoting the suppression of social differences between rich and poor as well as the barriers between different forms of monotheism." Successfully developing a popular social revolution and syncretism of the various religions and sects of the empire, Bedreddin's movement began in the European side of the empire and underwent further expansion in western Anatolia.

In 1416, Sheikh Bedreddin started his rebellion against the throne. After a four-year struggle, he was finally captured by Mehmed's grand vizier Bayezid Pasha and hanged in the city off Serres, a city in modern-day Greece, in 1420.

The reign of Mehmed I as sultan of the re-united empire lasted only eight years before his death, but he had also been the most powerful brother contending for the throne and "de facto" ruler of most of the empire for nearly the whole preceding period of 11 years of the Ottoman Interregnum that passed between his father's captivity at Ankara and his own final victory over his brother Musa Çelebi at the Battle of Çamurlu.
He was buried in Bursa, in a mausoleum erected by himself near the celebrated mosque which he built there, and which, because of its decorations of green glazed tiles, is called the Green Mosque. Mehmed I also completed another mosque in Bursa, which his grandfather Murad I had commenced but which had been neglected during the reign of Bayezid. Mehmed founded in the vicinity of his own Green Mosque and mausoleum two other characteristic institutions, one a school and one a refectory for the poor, both of which he endowed with royal munificence.






</doc>
<doc id="19988" url="https://en.wikipedia.org/wiki?curid=19988" title="Murad II">
Murad II

Murad II's reign was marked by the long war he fought against the Christian feudal lords of the Balkans and the Turkish beyliks in Anatolia, a conflict that lasted 25 years. He was brought up in Amasya, and ascended the throne on the death of his father Mehmed I. His mother was Valide Sultan Emine Hatun (daughter of Suleyman Bey, ruler of Dulkadirids), his father's third consort. Their marriage served as an alliance between the Ottomans and this buffer state, and produced a son, Mehmed II, who would go on to successfully conquer the Byzantine Empire's capital, Constantinople, in 1453.

Murad was born in June 1404 to Sultan Mehmed I and his wife Emine Hatun, and he spent his early childhood in Amasya. In 1410, Murad came along with his father to the Ottoman capital, Edirne. After his father ascended to the Ottoman throne, he made Murad governor of the Amasya Sanjak. Murad remained at Amasya until the death of Mehmed I in 1421. He was solemnly recognized as sultan of the Ottoman Sultanate at sixteen years of age, girded with the sabre of Osman at Bursa, and the troops and officers of the state willingly paid homage to him as their sovereign.

Murad's reign was troubled by insurrection early on. The Byzantine Emperor, Manuel II, released the 'pretender' Mustafa Çelebi (known as Düzmece Mustafa) from confinement and acknowledged him as the legitimate heir to the throne of Bayezid I (1389–1402). The Byzantine Emperor had first secured a stipulation that Mustafa should, if successful, repay him for his liberation by giving up a large number of important cities. The pretender was landed by the Byzantine galleys in the European dominion of the sultan and for a time made rapid progress. Many Turkish soldiers joined him, and he defeated and killed the veteran general Beyazid Pasha, whom Murad had sent to fight him. Mustafa defeated Murad's army and declared himself Sultan of Adrianople (modern Edirne). He then crossed the Dardanelles to Asia with a large army but Murad out-manoeuvered Mustafa. Mustafa's force passed over in large numbers to Murad II. Mustafa took refuge in the city of Gallipoli, but the sultan, who was greatly aided by a Genoese commander named Adorno, besieged him there and stormed the place. Mustafa was taken and put to death by the sultan, who then turned his arms against the Roman emperor and declared his resolution to punish the Palaiologos for their unprovoked enmity by the capture of Constantinople.

Murad II then formed a new army called Azap in 1421 and marched through the Byzantine Empire and laid siege to Constantinople. While Murad was besieging the city, the Byzantines, in league with some independent Turkish Anatolian states, sent the sultan's younger brother Küçük Mustafa (who was only 13 years old) to rebel against the sultan and besiege Bursa. Murad had to abandon the siege of Constantinople in order to deal with his rebellious brother. He caught Prince Mustafa and executed him. The Anatolian states that had been constantly plotting against him — Aydinids, Germiyanids, Menteshe and Teke — were annexed and henceforth became part of the Ottoman Sultanate.

Murad II then declared war against Venice, the Karamanid Emirate, Serbia and Hungary. The Karamanids were defeated in 1428 and Venice withdrew in 1432 following the defeat at the second Siege of Thessalonica in 1430. In the 1430s Murad captured vast territories in the Balkans and succeeded in annexing Serbia in 1439. In 1441 the Holy Roman Empire and Poland joined the Serbian-Hungarian coalition. Murad II won the Battle of Varna in 1444 against János Hunyadi.

Murad II relinquished his throne in 1444 to his son Mehmed II, but a Janissary revolt in the Empire forced him to return.

In 1448 he defeated the Christian coalition at the Second Battle of Kosovo (the first one took place in 1389). When the Balkan front was secured, Murad II turned east to defeat Timur's son, Shah Rokh, and the emirates of Karamanid and Çorum-Amasya. In 1450 Murad II led his army into Albania and unsuccessfully besieged the Castle of Kruje in an effort to defeat the resistance led by Skanderbeg. In the winter of 1450–1451, Murad II fell ill, and died in Edirne. He was succeeded by his son Mehmed II (1451–81).

When Murad II ascended to the throne, he sought to regain the lost Ottoman territories that had reverted to autonomy following his grandfather Bayezid I’s defeat at the Battle of Ankara in 1402 at the hands of Timur Lang. He needed the support of both the public and the nobles “who would enable him to exercise his rule”, and utilized the old and potent Islamic trope of Ghazi King.

In order to gain popular, international support for his conquests, Murad II modeled himself after the legendary Ghazi kings of old. The Ottomans already presented themselves as ghazis, painting their origins as rising from the ghazas of Osman, the founder of the dynasty. For them, ghaza was the noble championing of Islam and justice against non-Muslims and Muslims alike, if they were cruel; for example, Bayezid I labeled Timur Lang, also a Muslim, an apostate prior to the Battle of Ankara because of the violence his troops had committed upon innocent civilians and because “all you do is to break promises and vows, shed blood, and violate the honor of women.” Murad II only had to capitalize on this dynastic inheritance of doing ghaza, which he did by actively crafting the public image of Ghazi Sultan.

After his accession, there was a flurry of translating and compiling activity where old Persian, Arab, and Anatolian epics were translated into Turkish so Murad II could uncover the ghazi king legends. He drew from the noble behavior of the nameless Caliphs in the "Battalname", an epic about a fictional Arab warrior who fought against the Byzantines, and modelled his actions on theirs. He was careful to embody the simplicity, piety, and noble sense of justice that was part of the Ghazi King persona.

For example, the Caliph in "Battalname" saw the battle turning in his enemy’s favor, and got down from his horse and prayed, after which the battle ended in a victory for him. In the Battle of Varna in 1444, Murad II saw the Hungarians gaining the upper hand, and he got down from his horse and prayed just like the Caliph, and soon after, the tide turned in the Ottoman’s favor and the Hungarian king Wladyslaw was killed. Similarly, the Caliph in the epic roused his warriors by saying “Those of you who die will be martyrs. Those of you who kill will be ghazis”; before the Battle of Varna, Murad II repeated these words to his army, saying “Those of us who kill will be ghazis; those of us who die will be martyrs.” In another instance, since the Ghazi King is meant to be a just and fair, when Murad took Thessalonica in the Balkans, he took care to keep the troops in check and prevented widespread looting. Finally, just as the fictional Caliph’s ghazas were immortalized in "Battalname", Murad II’s battles and victories were also compiled and given the title "The Ghazas of Sultan Murad" ("Gazavat- i Sultan Murad)".

Murad II successfully painted himself as a simple soldier who did not partake in royal excesses, and as a noble ghazi sultan who sought to consolidate Muslim power against non-Muslims such as the Venetians and Hungarians. Through this self-presentation, he got the support of the Muslim population of not only the Ottoman territories, for both himself and his extensive, expensive campaigns, but also the greater Muslim populations in the Dar-al-Islam – such as the Mamluks and the Muslim Delhi Sultanates of India. Murad II was basically presenting himself not only as “a ghazi king who fights caffres [nonmuslims], but also serves as protector and master of lesser ghazis.”

Murad II had four known wives:
Murad had five sons:

Murad had four daughters:

Murad II is portrayed by İlker Kurt in 2012 film "Fetih 1453". He was also portrayed by Vahram Papazian in the Albanian movie "The Great Warrior Skanderbeg" in 1953.





</doc>
<doc id="19989" url="https://en.wikipedia.org/wiki?curid=19989" title="Murad III">
Murad III

Murad III (Ottoman Turkish: مراد ثالث "Murād-i sālis", Turkish: "III.Murat") (4 July 1546 – 15/16 January 1595) was the Sultan of the Ottoman Empire from 1574 until his death in 1595.

Born in Manisa on 4 July 1546, Şehzade Murad was the oldest son of Sultan Selim II and Afife Nurbanu Sultan. After his ceremonial circumcision in 1557, Murad was appointed "sancakbeyi" of Akşehir by Suleiman I (his grandfather) in 1558. At the age of 18 he was appointed "sancakbeyi" of Saruhan. Suleiman died when Murad was 22, and his father became the new sultan. Selim II broke with tradition by sending only his oldest son out of the palace to govern a province, and Murad was sent to Manisa.

Selim died in 1574 and was succeeded by Murad, who began his reign by having his five younger brothers strangled. His authority was undermined by harem influences – more specifically, those of his mother and later of his favorite wife Safiye Sultan. Under Selim II power had only been maintained by the genius of the powerful Grand Vizier, Mehmed Sokollu, who remained in office until his assassination in October 1579. During Murad's reign the northern borders with the Habsburg Monarchy were defended by the Bosnian governor Hasan Predojević. The reign of Murad III was marked by exhausting wars on the empire's western and eastern fronts. The Ottomans also suffered defeats in battles such as the Battle of Sisak.

The Ottomans had been at peace with the neighbouring rivalling Safavid Empire since 1555, per the Treaty of Amasya, that for some time had settled border disputes. But in 1577 Murad declared war, starting the Ottoman–Safavid War (1578–90), seeking to take advantage of the chaos in the Safavid court after the death of Shah Tahmasp I. Murad was influenced by viziers Lala Kara Mustafa Pasha and Sinan Pasha and disregarded the opposing counsel of Grand Vizier Sokollu. The war would drag on for 12 years, ending with the Treaty of Constantinople (1590), which resulted in temporary significant territorial gains for the Ottomans.

Murad's reign was a time of financial stress for the Ottoman state. To keep up with changing military techniques, the Ottomans trained infantrymen in the use of firearms, paying them directly from the treasury. By 1580 an influx of silver from the New World had caused high inflation and social unrest, especially among Janissaries and government officials who were paid in debased currency. Deprivation from the resulting rebellions, coupled with the pressure of over-population, was especially felt in Anatolia. Competition for positions within the government grew fierce, leading to bribery and corruption. Ottoman and Habsburg sources accuse Murad himself of accepting enormous bribes, including 20,000 ducats from a statesman in exchange for the governorship of Tripoli and Tunisia, thus outbidding a rival who had tried bribing the Grand Vizier.

Numerous envoys and letters were exchanged between Elizabeth I and Sultan Murad III. In one correspondence, Murad entertained the notion that Islam and Protestantism had "much more in common than either did with Roman Catholicism, as both rejected the worship of idols", and argued for an alliance between England and the Ottoman Empire. To the dismay of Catholic Europe, England exported tin and lead (for cannon-casting) and ammunitions to the Ottoman Empire, and Elizabeth seriously discussed joint military operations with Murad III during the outbreak of war with Spain in 1585, as Francis Walsingham was lobbying for a direct Ottoman military involvement against the common Spanish enemy. This diplomacy would be continued under Murad's successor Mehmed III, by both the sultan and Safiye Sultan alike.

Following the example of his father Selim II, Murad was the second Ottoman sultan who never went on campaign during his reign, instead spending it entirely in Constantinople. During the final years of his reign, he did not even leave Topkapı Palace. For two consecutive years he did not attend the Friday procession to the imperial mosque—an unprecedented breaking of custom. The Ottoman historian Mustafa Selaniki wrote that whenever Murad planned to go out to Friday prayer, he changed his mind after hearing of alleged plots by the Janissaries to dethrone him once he left the palace. Murad withdrew from his subjects and spent the majority of his reign keeping to the company of few people and abiding by a daily routine structured by the five daily Islamic prayers. Murad's personal physician Domenico Hierosolimitano described a typical day in the life of the sultan:

Murad's sedentary lifestyle and lack of participation in military campaigns earned him the disapproval of Mustafa Ali and Mustafa Selaniki, the major Ottoman historians who lived during his reign. Their negative portrayals of Murad influenced later historians. Both historians also accused Murad of sexual excess. Before becoming sultan, Murad had been loyal to Safiye Sultan, his Venetian-born concubine who had given him a son, Mehmed, and two daughters. His monogamy was disapproved of by his mother Nurbanu, who worried that Murad needed more sons to succeed him in case Mehmed died young. She also worried about Safiye's influence over her son and the Ottoman dynasty. Five or six years after his accession to the throne, Murad was given a pair of concubines by his sister Ismihan. Upon attempting sexual intercourse with them, he proved impotent. "The arrow [of Murad], [despite] keeping with his created nature, for many times [and] for many days has been unable to reach at the target of union and pleasure," wrote Mustafa Ali. Nurbanu accused Safiyye and her retainers of causing Murad's impotence with witchcraft. Several of Safiye's servants were tortured by eunuchs in order to discover a culprit. Court physicians, working under Nurbanu's orders, eventually prepared a successful cure, but a side effect was a drastic increase in sexual appetite—by the time Murad died, he was said to have fathered over a hundred children. Nineteen of these were executed by Mehmed III when he became sultan.

Influential ladies of his court included his mother Nurbanu Sultan, his sister Ismihan Sultan, wife of grand vizier Sokollu Mehmed Pasha, and musahibes (favourites) mistress of the housekeeper Canfeda Hatun, mistress of financial affairs Raziye Hatun, and the poet Hubbi Hatun.

Murad took great interest in the arts, particularly miniatures and books. He actively supported the court Society of Miniaturists, commissioning several volumes including the "Siyer-i Nebi", the most heavily illustrated biographical work on the life of the Islamic prophet Muhammad, the "Book of Skills", the "Book of Festivities" and the "Book of Victories". He had two large alabaster urns transported from Pergamon and placed on two sides of the nave in the Hagia Sophia in Constantinople and a large wax candle dressed in tin which was donated by him to the Rila monastery in Bulgaria is on display in the monastery museum.

Murad also furnished the content of "Kitabü’l-Menamat" ("The Book of Dreams"), addressed to Murad's spiritual advisory, Şüca Dede. A collection of first person accounts, it tells of Murad's spiritual experiences as a Sufi disciple. Compiled from thousands of letters Murad wrote describing his dream visions, it presents a hagiographic self-portrait. Murad dreams of various activities, including being stripped naked by his father and having to sit on his lap, single-handedly killing 12,000 infidels in battle, walking on water, ascending to heaven, and producing milk from his fingers. He frequently encounters the Prophet Muhammed, and in one dream sits in the Prophet's lap and kisses his mouth.

In another letter addressed to Şüca Dede, Murad wrote "I wish that the True Reality/God, "Celle ve Ala", had not created this poor servant as the descendant of the Ottomans so that I would not hear this and that, and would not worry. I wish I were of unknown pedigree. Then, I would have one single task, and could ignore the whole world."

The diplomatic edition of these dream letters have been recently published by Ozgen Felek in Turkish.

Murad died from what is assumed to be natural causes in the Topkapı Palace and was buried in tomb next to the Hagia Sofia. In the mausoleum are 54 sarcophagus of the sultan, his wives and children that are also buried there. He is also responsible for changing the burial customs of the sultans' mothers. Murad had his mother Nurbanu buried next to her husband Selim II, making her the first concubine to share a sultan's tomb.

Murad's wives are:
Murad had at least twenty three sons:

Murad had twenty eight daughters, of whom 
sixteen died of plague in 1597. The rest, who were married, included the following:

Orhan Pamuk's historical novel "Benim Adım Kırmızı" ("My Name is Red", 1998) takes place at the court of Murad III, during nine snowy winter days of 1591, which the writer uses in order to convey the tension between East and West.

The Harem Midwife by Roberta Rich - a historical fiction set in Constantinople (1578) which follows Hannah, a midwife, who tends to many of the women in Sultan Murad III's harem.


[aged 48]


</doc>
<doc id="19990" url="https://en.wikipedia.org/wiki?curid=19990" title="Mehmed III">
Mehmed III

Mehmed III (, "Meḥmed-i sālis"; ; 26 May 1566–21 December 1603) was Sultan of the Ottoman Empire from 1595 until his death in 1603.

Mehmed was born at the Manisa Palace in 1566, during the reign of his great-grandfather, Suleiman the Magnificent. He was the son of Şehzade Murad (later Murad III), himself the son of Şehzade Selim (later Selim II), who was the son of Sultan Suleiman and Hürrem Sultan. His mother was Safiye Sultan, an Albanian from the Dukagjin highlands. His great-grandfather died the year he was born and his grandfather became the new Sultan, Selim II. His grandfather Selim II died when Mehmed was eight, and Mehmed's father, Murad III, became Sultan in 1574. Mehmed thus became Crown Prince until his father's death in 1595, when he was 28 years old.

Mehmed III remains notorious even in Ottoman history for having nineteen of his brothers and half-brothers executed to secure power. They were all strangled by his deaf-mutes.

Mehmed III was an idle ruler, leaving government to his mother Safiye Sultan, the valide sultan. His first major problem was the rivalry between two of his viziers, Serdar Ferhad Pasha and Koca Sinan Pasha, and their supporters. His mother and her son-in-law Damat Ibrahim Pasha supported Koca Sinan Pasha, and prevented Mehmed III from taking control of the issue himself. The issue grew to cause major disturbances by janissaries. On 7 July 1595, Mehmed III finally sacked Serdar Ferhad Pasha from the position of Grand Vizier due to his failure in Wallachia and replaced him with Sinan.

The major event of his reign was the Austro-Ottoman War in Hungary (1593–1606). Ottoman defeats in the war caused Mehmed III to take personal command of the army, the first sultan to do so since Suleiman I in 1566. Accompanied by the Sultan, the Ottomans conquered Eger in 1596. Upon hearing of the Habsburg army's approach, Mehmed wanted to dismiss the army and return to Istanbul. However, the Ottomans eventually decided to face the enemy and defeated the Habsburg and Transylvanian forces at the Battle of Keresztes (known in Turkish as the Battle of Haçova), during which the Sultan had to be dissuaded from fleeing the field halfway through the battle. Upon returning to Istanbul in victory, Mehmed told his Vezirs that he would campaign again. The next year the Venetian Bailo in Istanbul noted, "the doctors declared that the Sultan cannot leave for war on account of his bad health, produced by excesses of eating and drinking".

In reward for his services at the war, Cigalazade Yusuf Sinan Pasha was made Grand Vizier in 1596. However, with pressure from the court and his mother, Mehmed reinstated Damat Ibrahim Pasha to this position shortly afterwards.

However, the victory at the Battle of Keresztes was soon set back by some important losses, including the loss of Győr () to the Austrians and the defeat of the Ottoman forces led by Hafız Ahmet Pasha by the Wallachian forces under Michael the Brave in Nikopol in 1599. In 1600, Ottoman forces under Tiryaki Hasan Pasha captured Nagykanizsa after a 40-day siege and later successfully held it against a much greater attacking force in the Siege of Nagykanizsa.

Another major event of his reign was the Jelali revolts in Anatolia. Karayazıcı Abdülhalim, a former Ottoman official, captured the city of Urfa and declared himself sultan in 1600. The rumors of his claim to the throne spread to Constantinople and Mehmed ordered the rebels to be treated harshly to dispel the rumors, among these was the execution of Hüseyin Pasha, whom Karayazıcı Abdülhalim styled as Grand Vizier. In 1601, Abdülhalim fled to the vicinity of Samsun after being defeated by the forces under Sokulluzade Hasan Pasha, the governor of Baghdad. However, his brother, Deli Hasan, killed Sokulluzade Hasan Pasha and defeated troops under the command of Hadım Hüsrev Pasha. He then marched on to Kütahya, captured and burned the city.

In 1599, the fourth year of Mehmed III's reign, Queen Elizabeth I sent a convoy of gifts to the Ottoman court. These gifts were originally intended for the sultan's predecessor, Murad III, who had died before they had arrived. Included in these gifts was a large jewel-studded clockwork organ that was assembled on the slope of the Royal Private Garden by a team of engineers including Thomas Dallam. The organ took many weeks to complete and featured dancing sculptures such as a flock of blackbirds that sung and shook their wings at the end of the music. The musical clock organ was destroyed by the succeeding Sultan Ahmed I. Also among the English gifts was a ceremonial coach, accompanied by a letter from the Queen to Mehmed's mother, Safiye Sultan. These gifts were intended to cement relations between the two countries, building on the trade agreement signed in 1581 that gave English merchants priority in the Ottoman region. Under the looming threat of Spanish military presence, England was eager to secure an alliance with the Ottomans, the two nations together having the capability to divide the power. Elizabeth's gifts arrived in a large 27-gun merchantman ship that Mehmed personally inspected, a clear display of English maritime strength that would prompt him to build up his fleet over the following years of his reign. The Anglo-Ottoman alliance would never be consummated, however, as relations between the nations grew stagnant due to anti-European sentiments reaped from the worsening Austro-Ottoman War and the deaths of Safiye Sultan's interpreter and the pro-English chief Hasan Pasha.

Mehmed died on 22 December 1603 at the age of 37. According to one source, the cause of his death was the distress caused by the death of his son, Şehzade Mahmud. According to another source, he died either of plague or of stroke. He was buried in Hagia Sophia Mosque. He was succeeded by his son Ahmed I as the new sultan.

None of Mehmed's consorts are listed as "haseki sultan" in Ottoman palace archives. Known consorts were:


[aged 37]


</doc>
<doc id="19991" url="https://en.wikipedia.org/wiki?curid=19991" title="Mustafa I">
Mustafa I

Mustafa I (24 June 1591 – 20 January 1639) (), called Mustafa the Saint (Veli Mustafa) during his second reign and often called Mustafa the Mad (Deli Mustafa) by modern historians, was the son of Mehmed III and was the Sultan of the Ottoman Empire from 1617 to 1618 and from 1622 to 1623.

Mustafa was born in the Manisa Palace, as the younger brother of Ahmed I (1603–1617). His mother was Halime Sultan, an Abkhazian lady.

Before 1603 it was customary for an Ottoman Sultan to have his brothers executed shortly after he gained the throne (Mustafa's father Mehmed III had executed 19 of his own brothers). But when the thirteen-year-old Ahmed I was enthroned in 1603, he spared the life of the twelve-year-old Mustafa.

A factor in Mustafa's survival is the influence of Kösem Sultan (Ahmed's favorite consort), who may have wished to preempt the succession of Osman, Ahmed’s first-born son from another concubine. If Osman became Sultan, he would likely try to execute his half-brothers, the sons of Ahmed and Kösem. (This scenario later became a reality when Osman II executed his brother Mehmed in 1621.) However, the reports of foreign ambassadors suggest that Ahmed actually liked his brother. 

Until Ahmed's death in 1617, Mustafa lived in the Old Palace, along with his mother, and grandmother Safiye Sultan.

Ahmed's death created a dilemma never before experienced by the Ottoman Empire. Multiple princes were now eligible for the Sultanate, and all of them lived in Topkapı Palace. A court faction headed by the Şeyhülislam Esad Efendi and Sofu Mehmed Pasha (who represented the Grand Vizier when he was away from Constantinople) decided to enthrone Mustafa instead of Ahmed's son Osman. Sofu Mehmed argued that Osman was too young to be enthroned without causing adverse comment among the populace. The Chief Black Eunuch Mustafa Agha objected, citing Mustafa's mental problems, but he was overruled. Mustafa's rise created a new succession principle of seniority that would last until the end of the Empire. It was the first time an Ottoman Sultan was succeeded by his brother instead of his son. His mother Halime Sultan became the Valide Sultan as well as a regent and wielded great power. Due to Mustafa's mental conditions, she acted as a regent and exercised power more directly. 

It was hoped that regular social contact would improve Mustafa's mental health, but his behavior remained eccentric. He pulled off the turbans of his viziers and yanked their beards. Others observed him throwing coins to birds and fish. The Ottoman historian İbrahim Peçevi wrote "this situation was seen by all men of state and the people, and they understood that he was psychologically disturbed."

Mustafa was never more than a tool of court cliques at the Topkapı Palace. In 1618, after a short rule, another palace faction deposed him in favour of his young nephew Osman II (1618–1622), and Mustafa was sent back to the Old Palace. The conflict between the Janissaries and Osman II presented him with a second chance. After a Janissary rebellion led to the deposition and assassination of Osman II in 1622, Mustafa was restored to the throne and held it for another year.

Nevertheless, according to Baki Tezcan, there is not enough evidence to properly establish that Mustafa was mentally imbalanced when he came to the throne. Mustafa "made a number of excursions to the arsenal and the navy docks, examining various sorts of arms and taking an active interest in the munitions supply of the army and the navy." One of the dispatches of Baron de Sancy, the French ambassador, "suggested that Mustafa was interested in leading the Safavid campaign himself and was entertaining the idea of wintering in Konya for that purpose." 

Moreover, one contemporary observer provides an explanation of the coup which does not mention the incapacity of Mustafa. Baron de Sancy ascribes the deposition to a political conspiracy between the grand admiral Ali Pasha and Chief Black Eunuch Mustafa Agha, who were angered by the former's removal from office upon Sultan Mustafa's accession. They may have circulated rumors of the sultan's mental instability subsequent to the coup in order to legitimize it.

He commenced his reign by executing all those who had taken any share in the murder of Sultan Osman. Hoca Ömer Efendi, the chief of the rebels, the kızlar Agha Suleiman Agha, the vizier Dilaver Pasha, the Kaim-makam Ahmed Pasha, the defterdar Baki Pasha, the segban-bashi Nasuh Agha, and the general of the janissaries Ali Agha, were cut into pieces.

The epithet "Veli" (meaning "saint") was used in reference to him during his reign.

His mental condition unimproved, Mustafa was a puppet controlled by his mother and brother-in-law, the grand vizier Kara Davud Pasha. He believed that Osman II was still alive and was seen searching for him throughout the palace, knocking on doors and crying out to his nephew to relieve him from the burden of sovereignty. "The present emperor being a fool" (according to English Ambassador Sir Thomas Roe), he was compared unfavorably with his predecessor. In fact, it was his mother Halime Sultan the de facto-co-ruler as Valide Sultan of the Ottoman empire. 

Political instability was generated by conflict between the Janissaries and the sipahis (Ottoman cavalry), followed by the Abaza rebellion, which occurred when the governor-general of Erzurum, Abaza Mehmed Pasha, decided to march to Istanbul to avenge the murder of Osman II. The regime tried to end the conflict by executing Kara Davud Pasha, but Abaza Mehmed continued his advance. Clerics and the new Grand Vizier (Kemankeş Kara Ali Pasha) prevailed upon Mustafa's mother to allow the deposition of her son. She agreed, on condition that Mustafa's life would be spared.

The 11-year-old Murad IV, son of Ahmed I and Kösem, was enthroned on 10 September 1623. In return for her consent to his deposition, the request of Mustafa's mother that he be spared execution was granted. Mustafa was sent along with his mother to the Eski (old) Palace.

He died at the Eski (old) Palace, Constantinople on 20 January 1639, and is buried in the courtyard of the Haghia Sophia.


<BR>


</doc>
<doc id="19992" url="https://en.wikipedia.org/wiki?curid=19992" title="Murad IV">
Murad IV

Murad IV (, "Murād-ı Rābiʿ"; 26/27 July 1612 – 8 February 1640) was the Sultan of the Ottoman Empire from 1623 to 1640, known both for restoring the authority of the state and for the brutality of his methods. Murad IV was born in Istanbul, the son of Sultan Ahmed I (r. 1603–17) and the ethnic Greek Kösem Sultan. Brought to power by a palace conspiracy in 1623, he succeeded his uncle Mustafa I (r. 1617–18, 1622–23). He was only 11 when he took the throne. His reign is most notable for the Ottoman–Safavid War (1623–39), of which the outcome would permanently part the Caucasus between the two Imperial powers for around two centuries, while it also roughly laid the foundation for the current Turkey–Iran–Iraq borders.

Murad IV was for a long time under the control of his relatives and during his early years as Sultan, his mother, Kösem Sultan, essentially ruled through him. The Empire fell into anarchy; the Safavid Empire invaded Iraq almost immediately, Northern Anatolia erupted in revolts, and in 1631 the Janissaries stormed the palace and killed the Grand Vizier, among others. Murad IV feared suffering the fate of his elder brother, Osman II (1618–22), and decided to assert his power.

At the age of 16 in 1628, he had his brother-in-law (his sister Fatma Sultan's husband, who was also the former governor of Egypt), Kara Mustafa Pasha, executed for a claimed action "against the law of God".

Murad IV tried to quell the corruption that had grown during the reigns of previous Sultans, and that had not been checked while his mother was ruling through proxy.

Murad IV also banned alcohol, tobacco, and coffee in Istanbul. He ordered execution for breaking this ban. He would reportedly patrol the streets and the lowest taverns of Istanbul in civilian clothes at night, policing the enforcement of his command by casting off his disguise on the spot and beheading the offender with his own hands. Rivaling the exploits of Selim the Grim, he would sit in a kiosk by the water near his Seraglio Palace and shoot arrows at any passerby or boatman who rowed too close to his imperial compound, seemingly for sport. He restored the judicial regulations by very strict punishments, including execution, he once strangled a grand vizier for the reason that the official had beaten his mother-in-law.

Murad IV's reign is most notable for the Ottoman–Safavid War (1623–39) against Persia (today Iran) in which Ottoman forces managed to conquer Azerbaijan, occupying Tabriz, Hamadan, and capturing Baghdad in 1638. The Treaty of Zuhab that followed the war generally reconfirmed the borders as agreed by the Peace of Amasya, with Eastern Armenia, Eastern Georgia, Azerbaijan, and Dagestan staying Persian, while Western Armenia, and Western Georgia stayed Ottoman. Mesopotamia was irrevocably lost for the Persians. The borders fixed as a result of the war, are more or less the same as the present border line between Turkey, Iraq and Iran.

During the siege of Baghdad in 1638, the city held out for forty days but was compelled to surrender.

Murad IV himself commanded the Ottoman army in the last years of the war.

While he was encamped in Baghdad, Murad IV is known to have met ambassadors of the Mughal Emperor Shah Jahan, Mir Zafar and Mir Baraka, who presented 1000 pieces of finely embroidered cloth and even armor. Murad IV gave them the finest weapons, saddles and Kaftans and ordered his forces to accompany the Mughals to the port of Basra, where they set sail to Thatta and finally Surat.

Murad IV put emphasis on architecture and in his period many monuments were erected. The Baghdad Kiosk, built in 1635, and the Revan Kiosk, built in 1638 in Yerevan, were both built in the local styles. Some of the others include the Kavak Sarayı pavilion; the Meydanı Mosque; the Bayram Pasha Dervish Lodge, Tomb, Fountain, and Primary School; and the Şerafettin Mosque in Konya.

Very little is known about the concubines of Murad IV, principally because he did not leave sons who survived his death to reach the throne, but many historians consider Ayşe Sultan as his only consort until the very end of Murad's seventeen-year reign, when a second Haseki, possibly Sanevber, appeared. It is very possible that Murad had only a single concubine until the advent of the second, or that he had a number of concubines but singled out only two as Haseki,

Murad had three daughters:


Murad IV died from cirrhosis in Istanbul at the age of 27 in 1640.

Rumours had circulated that on his deathbed, Murad IV ordered the execution of his mentally disabled brother, Ibrahim (reigned 1640–48), which would have meant the end of the Ottoman line. However, the order was not carried out.

In the TV series "", Murad IV is portrayed by Cağan Efe Ak as a child, and Metin Akdülger as a sultan.




</doc>
<doc id="19994" url="https://en.wikipedia.org/wiki?curid=19994" title="Masamune Shirow">
Masamune Shirow

Born in the Hyōgo Prefecture capital city of Kobe, he studied oil painting at Osaka University of Arts. While in college, he developed an interest in manga, which led him to create his own complete work, "Black Magic", which was published in the manga dōjinshi "Atlas". His work caught the eye of Seishinsha President Harumichi Aoki, who offered to publish him.

The result was best-selling manga "Appleseed", a full volume of densely plotted drama taking place in an ambiguous future. The story was a sensation, and won the 1986 Seiun Award for Best Manga. After a professional reprint of "Black Magic" and a second volume of "Appleseed", he released "Dominion" in 1986. Two more volumes of "Appleseed" followed before he began work on "Ghost in the Shell".

In 2007, he collaborated again with Production I.G to co-create the original concept for the anime television series "Shinreigari/Ghost Hound", Production I.G's 20th year anniversary project. A further original collaboration with Production I.G began airing in April 2008, titled "Real Drive".


A substantial amount of Shirow's work has been released in art book or poster book format. The following is an incomplete list.


"Galgrease" (published in "Uppers Magazine", 2002) is the collected name of several erotic manga and poster books by Shirow. The name comes from the fact that the women depicted often look "greased".

The first series of "Galgrease" booklets included four issues each in the following settings:


The second series included another run of 12 booklets in the following worlds:


After each regular series, there were one or more bonus poster books that revisited the existing characters and settings.


Main source:














</doc>
<doc id="19995" url="https://en.wikipedia.org/wiki?curid=19995" title="Musical saw">
Musical saw

A musical saw, also called a singing saw, is a hand saw used as a musical instrument. Capable of continuous glissando (portamento), the sound creates an ethereal tone, very similar to the theremin. The musical saw is classified as a plaque friction idiophone with direct friction (132.22) under the Hornbostel-Sachs system of musical instrument classification.

The saw is generally played seated with the handle squeezed between the legs, and the far end held with one hand. Some sawists play standing, either with the handle between the knees and the blade sticking out in front of them. The saw is usually played with the serrated edge, or "teeth", facing the body, though some players face them away. Some saw players file down the teeth which makes no discernable difference to the sound. Many—especially professional—saw players use a handle, called a Cheat, at the tip of the saw for easier bending and higher virtuosity.

To sound a note, a sawist first bends the blade into an S-curve. The parts of the blade that are curved are damped from vibration, and do not sound. At the center of the S-curve a section of the blade remains relatively flat. This section, the "sweet spot", can vibrate across the width of the blade, producing a distinct pitch: the wider the section of blade, the lower the sound. Sound is usually created by drawing a bow across the back edge of the saw at the sweet spot, or sometimes by striking the sweet spot with a mallet. 

The sawist controls the pitch by adjusting the S-curve, making the sweet spot travel up the blade (toward a thinner width) for a higher pitch, or toward the handle for a lower pitch. Harmonics can be created by playing at varying distances on either side of the sweet spot. Sawists can add vibrato by shaking one of their legs or by wobbling the hand that holds the tip of the blade. Once a sound is produced, it will sustain for quite a while, and can be carried through several notes of a phrase.

On occasion the musical saw is called for in orchestral music, but orchestral percussionists are seldom also sawists. If a note outside of the saw's range is called for, an electric guitar with a slide can be substituted.

Sawists often use standard wood-cutting saws, although special musical saws are also made. As compared with wood-cutting saws, the blades of musical saws are generally wider, for range, and longer, for finer control. They do not have set or sharpened teeth, and may have grain running parallel to the back edge of the saw, rather than parallel to the teeth. Some musical saws are made with thinner metal, to increase flexibility, while others are made thicker, for a richer tone, longer sustain, and stronger harmonics. 

A typical musical saw is wide at the handle end and wide at the tip. Such a saw will generally produce about two octaves, regardless of length. A bass saw may be over at the handle and produce about two-and-a-half octaves. There are also musical saws with 3–4 octaves range, and new improvements have resulted in as much as 5 octaves note range. Two-person saws, also called "misery whips", can also be played, though with less virtuosity, and they produce an octave or less of range.

Most sawists use cello or violin bows, using violin rosin, but some may use improvised home-made bows, such as a wooden dowel.

Musical saws have been produced for over a century, primarily in the United States, but also in Scandinavia, Germany, France (Lame sonore) and Asia.

In the early 1900s, there were at least ten companies in the United States manufacturing musical saws. These saws ranged from the familiar steel variety to gold-plated masterpieces worth hundreds of dollars. However, with the start of World War II the demand for metals made the manufacture of saws too expensive and many of these companies went out of business. By the year 2000, only three companies in the United States—Mussehl & Westphal, Charlie Blacklock, and Wentworth—were making saws. In 2012, a company called Index Drums started producing a saw that had a built-in transducer in the handle, called the "JackSaw".

Outside the United States, makers of musical saws include Bahco, makers of the limited edition Stradivarius, Alexis in France, Feldmann and Stövesandt in Germany, Music Blade in Greece and Thomas Flinn & Company in the United Kingdom, based in Sheffield, who produce three different sized musical saws, as well as accessories.

The International Musical Saw Association (IMSA) produces an annual International Musical Saw Festival (including a "Saw-Off" competition) every August in Santa Cruz and Felton, California. An International Musical Saw Festival is held every other summer in New York City, produced by Natalia Paruz. Paruz also produced a musical saw festival in Israel. There are also annual saw festivals in Japan and China.

A Guinness World Record for the largest musical-saw ensemble was established July 18, 2009, at the annual NYC Musical Saw Festival. Organized by Paruz, 53 musical saw players performed together.

In 2011 a World Championship took place in Jelenia Góra/Poland. Winners: 1. Gladys Hulot (France), 2. Katharina Micada (Germany), 3. Tom Fink (Germany).

This is a list of people notable for playing the musical saw. 


German actress and singer Marlene Dietrich, who lived and worked in the United States for a long time, is probably the most well known musical saw player. When she studied the violin for one year in Weimar in her early twenties, her musical skills were already evident. Some years later she learned to play the musical saw while she was shooting the film "Café Electric" in Vienna in 1927. Her colleague, the Bavarian actor and musician Igo Sym told her how to play. In the shooting breaks and at weekends both performed romantic duets, he at the piano and she at the musical saw. 

Sym gave his saw to her as a farewell gift. The following words are engraved on the saw: "Now Suidy is gone / the sun d’ont [sic!] / shine… / Igo / Vienna 1927"
She took the saw with her, when she left for Hollywood in 1929 and played there in the following years at film sets and Hollywood parties.
When she participated at the United Service Organizations (USO) shows for the US troops in 1944, she also played on the saw. Some of these shows were broadcast on radio, so there exist two rare recordings of her saw playing, embedded in entertaining interviews. 1. Aloha Oe 2. other song


Beginning from the early 1920s composers of both contemporary and popular music wrote for the musical saw.
Probably the first was Dmitri Shostakovich. He included the musical saw, e.g., in the film music for "The New Babylon" (1929), in "The Nose" (1928), and in "Lady Macbeth of the Mtsensk District" (1934).
Shostakovich and other composers of his time used the term "Flexaton" to mark the musical saw. "Flexaton" just means "to flex a tone"—the saw is flexed to change the pitch. Unfortunately, there exists another instrument called Flexatone, so there has been confusion for a long time. Aram Khachaturian, who knew Shostakovich's music included the musical saw in his Piano Concerto (1936) in the second movement. Another composer was the Swiss Arthur Honegger, who included the saw in his opera "Antigone" in 1924 . 
The Romanian composer George Enescu used the musical saw at the end of the second act of his opera "Œdipe" (1931) to show in an extensive glissando—which begins with the mezzo-soprano and is continued by the saw—the death and ascension of the sphinx killed by Oedipus.
The Italian composer Giacinto Scelsi wrote a part for the saw in his quarter-tone piece "Quattro pezzi per orchestra" (1959). German composer Hans Werner Henze took the saw to characterize the mean hero of his tragical opera "Elegy for young lovers" (1961).
Other composers were Krysztof Penderecki with "Fluorescences" (1961), "De natura sonoris Nr. 2" (1971) and the opera "Ubu Rex" (1990), Bernd Alois Zimmermann with "Stille und Umkehr" (1970), George Crumb with "Ancient voices of children" (1970), John Corigliano with "The Mannheim Rocket" (2001).
Chaya Czernowin used the saw in her opera "PNIMA...Ins Innere" (2000) to represent the character of the grandfather, who is traumatized by the Holocaust. 
There are Further Leif Segerstam, Hans Zender (orchestration of "5 préludes" by Claude Debussy), Franz Schreker (opera "Christophorus") and Oscar Strasnoy (opera "Le bal").
Russian composer Lera Auerbach wrote for the saw in her ballet "The Little Mermaid" (2005), in her symphonic poem "Dreams and Whispers of Poseidon" (2005), in her oratorio "Requiem Dresden – Ode to Peace" (2012), in her Piano Concerto No.1 (2015), in her comic oratorio "The Infant Minstrel and His Peculiar Menagerie" (2016) and in her violin concerto Nr.4 "NyX – Fractured dreams" (2017).

Canadian composer Robert Minden has written extensively for the musical saw. Michael A. Levine composed "Divination By Mirrors" for musical saw soloist and two string ensembles tuned a quarter tone apart, taking advantage of the saws ability to play in both tunings.

Other composers for chamber music with musical saw are Jonathan Rutherford ("An Intake of Breath"), Dana Wilson ("Whispers from Another Time"), Heinrich Gattermeyer ("Elegie für Singende Säge, Cembalo (oder Klavier)", Vito Zuraj ("Musica di camera" (2001)) and Britta-Maria Bernhard ("Tranquillo")




</doc>
<doc id="19996" url="https://en.wikipedia.org/wiki?curid=19996" title="MIDI">
MIDI

MIDI (; short for Musical Instrument Digital Interface) is a technical standard that describes a communications protocol, digital interface, and electrical connectors that connect a wide variety of electronic musical instruments, computers, and related music and audio devices. A single MIDI link can carry up to sixteen channels of information, each of which can be routed to a separate device.

MIDI carries event messages that specify notation, pitch, velocity, vibrato, panning, and clock signals (which set tempo). For example, a MIDI keyboard or other controller might trigger a sound module to generate sound produced by a keyboard amplifier. MIDI data can be transferred via midi cable, or recorded to a sequencer to be edited or played back. 

A file format that stores and exchanges the data is also defined. Advantages of MIDI include small file size, ease of modification and manipulation and a wide choice of electronic instruments and synthesizer or digitally-sampled sounds. 

Prior to the development of MIDI, electronic musical instruments from different manufacturers could generally not communicate with each other. With MIDI, any MIDI-compatible keyboard (or other controller device) can be connected to any other MIDI-compatible sequencer, sound module, drum machine, synthesizer, or computer, even if they are made by different manufacturers.

MIDI technology was standardized in 1983 by a panel of music industry representatives, and is maintained by the MIDI Manufacturers Association (MMA). All official MIDI standards are jointly developed and published by the MMA in Los Angeles, and the MIDI Committee of the Association of Musical Electronics Industry (AMEI) in Tokyo. In 2016, the MMA established the MIDI Association (TMA) to support a global community of people who work, play, or create with MIDI.

In the early 1980s, there was no standardized means of synchronizing electronic musical instruments manufactured by different companies. Manufacturers had their own proprietary standards to synchronize instruments, such as CV/gate and Digital Control Bus (DCB).

Roland founder Ikutaro Kakehashi felt the lack of standardization was limiting the growth of the industry. In June 1981, he proposed developing a standard to Oberheim Electronics founder Tom Oberheim, who discussed it with Sequential Circuits president Dave Smith. In October 1981, Kakehashi, Oberheim and Smith discussed the idea with representatives from Yamaha, Korg and Kawai, using Roland's DCB as a basis. 

Smith and Sequential Circuits engineer Chet Wood devised a universal synthesizer interface to allow communication between equipment from different manufacturers. Smith proposed this standard at the Audio Engineering Society show in November 1981. The standard was discussed and modified by representatives of Roland, Yamaha, Korg, Kawai, and Sequential Circuits. The result, Musical Instrument Digital Interface (MIDI), was announced by Robert Moog, founder of the Moog Music synthesizer company, in the October 1982 issue of "Keyboard".

At the 1983 Winter NAMM Show, Smith demonstrated a MIDI connection between Prophet 600 and Roland JP-6 synthesizers. The MIDI specification was published in August 1983. The MIDI standard was unveiled by Kakehashi and Smith, who later received Technical Grammy Awards in 2013 for their work.

The first MIDI synthesizers were the Roland Jupiter-6 and the Prophet 600, both released in 1982. 1983 saw the release of the first MIDI drum machine, the Roland TR-909, and the first MIDI sequencer, the Roland MSQ-700. The first computers to support MIDI were the NEC PC-88 and PC-98 in 1982, and the MSX (Yamaha CX5M) released in 1983.

MIDI's appeal was originally limited to professional musicians and record producers who wanted to use electronic instruments in the production of popular music. The standard allowed different instruments to communicate with each other and with computers, and this spurred a rapid expansion of the sales and production of electronic instruments and music software. This interoperability allowed one device to be controlled from another, which reduced the amount of hardware musicians needed. MIDI's introduction coincided with the dawn of the personal computer era and the introductions of samplers and digital synthesizers. The creative possibilities brought about by MIDI technology are credited for helping revive the music industry in the 1980s.

MIDI introduced capabilities that transformed the way many musicians work. MIDI sequencing made it possible for a user with no notation skills to build complex arrangements. A musical act with as few as one or two members, each operating multiple MIDI-enabled devices, can deliver a performance similar to that of a larger group of musicians. The expense of hiring outside musicians for a project could be reduced or eliminated, and complex productions could be realized on a system as small as a synthesizer with integrated keyboard and sequencer. 

MIDI also helped establish home recording. By performing preproduction in a home environment, an artist can reduce recording costs by arriving at a recording studio with a song that is already partially completed and worked out. Educational technology enabled by MIDI has transformed music education.

MIDI was invented so that electronic or digital musical instruments could communicate with each other and so that one instrument can control another. For example, a MIDI-compatible sequencer can trigger beats produced by a drum sound module. Analog synthesizers that have no digital component and were built prior to MIDI's development can be retrofit with kits that convert MIDI messages into analog control voltages. When a note is played on a MIDI instrument, it generates a digital signal that can be used to trigger a note on another instrument. The capability for remote control allows full-sized instruments to be replaced with smaller sound modules, and allows musicians to combine instruments to achieve a fuller sound, or to create combinations of synthesized instrument sounds, such as acoustic piano and strings. MIDI also enables other instrument parameters (volume, effects, etc.) to be controlled remotely.

Synthesizers and samplers contain various tools for shaping an electronic or digital sound. Filters adjust timbre (bass and treble), and envelopes automate the way a sound evolves over time after a note is triggered. The frequency of a filter and the envelope attack, or the time it takes for a sound to reach its maximum level, are examples of synthesizer parameters, and can be controlled remotely through MIDI. Effects devices have different parameters, such as delay feedback or reverb time. When a MIDI continuous controller number (CCN) is assigned to one of these parameters, the device responds to any messages it receives that are identified by that number. Controls such as knobs, switches, and pedals can be used to send these messages. A set of adjusted parameters can be saved to a device's internal memory as a "patch", and these patches can be remotely selected by MIDI program changes. The MIDI standard allows selection of 128 different programs, but devices can provide more by arranging their patches into banks of 128 programs each, and combining a program change message with a bank select message.

MIDI events can be sequenced with computer software, or in specialized hardware music workstations. Many digital audio workstations (DAWs) are specifically designed to work with MIDI as an integral component. MIDI piano rolls have been developed in many DAWs so that the recorded MIDI messages can be extensively modified. These tools allow composers to audition and edit their work much more quickly and efficiently than did older solutions, such as multitrack recording.

Because MIDI is a set of commands that create sound, MIDI sequences can be manipulated in ways that prerecorded audio cannot. It is possible to change the key, instrumentation or tempo of a MIDI arrangement, and to reorder its individual sections. The ability to compose ideas and quickly hear them played back enables composers to experiment. Algorithmic composition programs provide computer-generated performances that can be used as song ideas or accompaniment.

Some composers may take advantage of MIDI 1.0 and General MIDI (GM) technology to allow musical data files to be shared among various electronic instruments by using a standard, portable set of commands and parameters. The data composed via the sequenced MIDI recordings can be saved as a Standard MIDI File (SMF), digitally distributed, and reproduced by any computer or electronic instrument that also adheres to the same MIDI, GM, and SMF standards. MIDI data files are much smaller than recorded audio files.

At the time of MIDI's introduction, the computing industry was mainly devoted to mainframe computers, and personal computers were not commonly owned. The personal computer market stabilized at the same time that MIDI appeared, and computers became a viable option for music production. It was not until the advent of MIDI in 1983 that general-purpose computers started to play a role in mainstream music production.

In the years immediately after the 1983 ratification of the MIDI specification, MIDI features were adapted to several early computer platforms. NEC's PC-88 and PC-98 began supporting MIDI as early as 1982. Yamaha modules introduced MIDI support and sequencing to the MSX in 1983.

The spread of MIDI on personal computers was largely facilitated by Roland Corporation's MPU-401, released in 1984, as the first MIDI-equipped PC sound card, capable of MIDI sound processing and sequencing. After Roland sold MPU sound chips to other sound card manufacturers, it established a universal standard MIDI-to-PC interface. The widespread adoption of MIDI led to computer-based MIDI software being developed. Soon after, a number of platforms began supporting MIDI, including the Apple II+, IIe and Macintosh, Commodore 64 and Amiga, Atari ST, Acorn Archimedes, and PC DOS. The Macintosh was a favorite among US musicians, as it was marketed at a competitive price, and it took several years for PC systems to catch up with its efficiency and graphical interface.

The Atari ST was favored in Europe, where Macintoshes were more expensive. The Apple IIGS used a digital sound chip designed for the Ensoniq Mirage synthesizer, and later models used a custom sound system and upgraded processors, which drove other companies to improve their own offerings. The Atari ST was favored for its MIDI ports that were built directly into the computer. Most music software in MIDI's first decade was published for either the Apple or the Atari. By the time of Windows 3.0's 1990 release, PCs had caught up in processing power and had acquired a graphical interface, and software titles began to see release on multiple platforms.

The Standard MIDI File (SMF) is a file format that provides a standardized way for music sequences to be saved, transported, and opened in other systems. The compact size of these files led to their widespread use in computers, mobile phone ringtones, webpage authoring and musical greeting cards. These files are intended for universal use, and include such information as note values, timing and track names. Lyrics may be included as metadata, and can be displayed by karaoke machines. The SMF specification was developed and is maintained by the MMA.

SMFs are created as an export format of software sequencers or hardware workstations. They organize MIDI messages into one or more parallel tracks, and timestamp the events so that they can be played back in sequence. A header contains the arrangement's track count, tempo and which of three SMF formats the file is in. A type 0 file contains the entire performance, merged onto a single track, while type 1 files may contain any number of tracks that are performed in synchrony. Type 2 files are rarely used and store multiple arrangements, with each arrangement having its own track and intended to be played in sequence.
Microsoft Windows bundles SMFs together with Downloadable Sounds (DLS) in a Resource Interchange File Format (RIFF) wrapper, as RMID files with a codice_1 extension. RIFF-RMID has been deprecated in favor of Extensible Music Files (XMF).

A MIDI file is not a recording of actual audio. Rather, it is a set of instructions (e.g., for pitch, rhythm and other elements), and can use a thousand times less disk space than the equivalent recorded audio. This made MIDI file arrangements an attractive way to share music, before the advent of broadband internet access and multi-gigabyte hard drives. Licensed MIDI files on floppy disks were commonly available in stores in Europe and Japan during the 1990s. The major drawback to this is the wide variation in quality of users' audio cards, and in the actual audio contained as samples or synthesized sound in the card that the MIDI data only refers to symbolically. There is no standardization of how symbols are expressed. Even a sound card that contains high-quality sampled sounds can have inconsistent quality from one sampled instrument to another, while different model cards have no guarantee of consistent sound of the same instrument. Early budget-priced cards, such as the AdLib and the Sound Blaster and its compatibles, used a stripped-down version of Yamaha's frequency modulation synthesis (FM synthesis) technology played back through low-quality digital-to-analog converters. The low-fidelity reproduction of these ubiquitous cards was often assumed to somehow be a property of MIDI itself. This created a perception of MIDI as low-quality audio, while in reality MIDI itself contains no sound, and the quality of its playback depends entirely on the quality of the sound-producing device (and of samples in the device).

The main advantage of the personal computer in a MIDI system is that it can serve a number of different purposes, depending on the software that is loaded. Multitasking allows simultaneous operation of programs that may be able to share data with each other.

Sequencing software provides a number of benefits to a composer or arranger. It allows recorded MIDI to be manipulated using standard computer editing features such as cut, copy and paste and drag and drop. Keyboard shortcuts can be used to streamline workflow, and editing functions are often selectable via MIDI commands. The sequencer allows each channel to be set to play a different sound, and gives a graphical overview of the arrangement. A variety of editing tools are made available, including a notation display that can be used to create printed parts for musicians. Tools such as looping, quantization, randomization, and transposition simplify the arranging process.

Beat creation is simplified, and groove templates can be used to duplicate another track's rhythmic feel. Realistic expression can be added through the manipulation of real-time controllers. Mixing can be performed, and MIDI can be synchronized with recorded audio and video tracks. Work can be saved, and transported between different computers or studios.

Sequencers may take alternate forms, such as drum pattern editors that allow users to create beats by clicking on pattern grids, and loop sequencers such as ACID Pro, which allow MIDI to be combined with prerecorded audio loops whose tempos and keys are matched to each other. Cue list sequencing is used to trigger dialogue, sound effect, and music cues in stage and broadcast production.

With MIDI, notes played on a keyboard can automatically be transcribed to sheet music. Scorewriting software typically lacks advanced sequencing tools, and is optimized for the creation of a neat, professional printout designed for live instrumentalists. These programs provide support for dynamics and expression markings, chord and lyric display, and complex score styles. Software is available that can print scores in braille.

SmartScore software can produce MIDI files from scanned sheet music. Other notation programs include Finale, Encore, Sibelius and MuseScore.

Patch editors allow users to program their equipment through the computer interface. These became essential with the appearance of complex synthesizers such as the Yamaha FS1R, which contained several thousand programmable parameters, but had an interface that consisted of fifteen tiny buttons, four knobs and a small LCD. Digital instruments typically discourage users from experimentation, due to their lack of the feedback and direct control that switches and knobs would provide, but patch editors give owners of hardware instruments and effects devices the same editing functionality that is available to users of software synthesizers. Some editors are designed for a specific instrument or effects device, while other, "universal" editors support a variety of equipment, and ideally can control the parameters of every device in a setup through the use of System Exclusive commands.

Patch librarians have the specialized function of organizing the sounds in a collection of equipment, and allow transmission of entire banks of sounds between an instrument and a computer. This allows the user to augment the device's limited patch storage with a computer's much greater disk capacity, and to share custom patches with other owners of the same instrument. Universal editor/librarians that combine the two functions were once common, and included Opcode Systems' Galaxy and eMagic's SoundDiver. These programs have been largely abandoned with the trend toward computer-based synthesis, although Mark of the Unicorn's (MOTU)'s Unisyn and Sound Quest's Midi Quest remain available. Native Instruments' Kore was an effort to bring the editor/librarian concept into the age of software instruments.

Programs that can dynamically generate accompaniment tracks are called "auto-accompaniment" programs. These create a full band arrangement in a style that the user selects, and send the result to a MIDI sound generating device for playback. The generated tracks can be used as educational or practice tools, as accompaniment for live performances, or as a songwriting aid.

Computers can use software to generate sounds, which are then passed through a digital-to-analog converter (DAC) to a power amplifier and loudspeaker system. The number of sounds that can be played simultaneously (the polyphony) is dependent on the power of the computer's CPU, as are the sample rate and bit depth of playback, which directly affect the quality of the sound. Synthesizers implemented in software are subject to timing issues that are not present with hardware instruments, whose dedicated operating systems are not subject to interruption from background tasks as desktop operating systems are. These timing issues can cause synchronization problems, and clicks and pops when sample playback is interrupted. Software synthesizers also exhibit a noticeable delay known as latency in their sound generation, because computers use an audio buffer that delays playback and disrupts MIDI timing.

Software synthesis' roots go back as far as the 1950s, when Max Mathews of Bell Labs wrote the MUSIC-N programming language, which was capable of non-real-time sound generation. The first synthesizer to run directly on a host computer's CPU was Reality, by Dave Smith's Seer Systems, which achieved a low latency through tight driver integration, and therefore could run only on Creative Labs soundcards. Some systems use dedicated hardware to reduce the load on the host CPU, as with Symbolic Sound Corporation's Kyma System, and the Creamware/Sonic Core Pulsar/SCOPE systems, which power an entire recording studio's worth of instruments, effect units, and mixers.

The ability to construct full MIDI arrangements entirely in computer software allows a composer to render a finalized result directly as an audio file.

Early PC games were distributed on floppy disks, and the small size of MIDI files made them a viable means of providing soundtracks. Games of the DOS and early Windows eras typically required compatibility with either Ad Lib or Sound Blaster audio cards. These cards used FM synthesis, which generates sound through modulation of sine waves. John Chowning, the technique's pioneer, theorized that the technology would be capable of accurate recreation of any sound if enough sine waves were used, but budget computer audio cards performed FM synthesis with only two sine waves. Combined with the cards' 8-bit audio, this resulted in a sound described as "artificial" and "primitive".

Wavetable daughterboards that were later available provided audio samples that could be used in place of the FM sound. These were expensive, but often used the sounds from respected MIDI instruments such as the E-mu Proteus. The computer industry moved in the mid-1990s toward wavetable-based soundcards with 16-bit playback, but standardized on a 2MB ROM, a space too small in which to fit good-quality samples of 128 instruments plus drum kits. Some manufacturers used 12-bit samples, and padded those to 16 bits.

MIDI has been adopted as a control protocol in a number of non-musical applications. MIDI Show Control uses MIDI commands to direct stage lighting systems and to trigger cued events in theatrical productions. VJs and turntablists use it to cue clips, and to synchronize equipment, and recording systems use it for synchronization and automation. Apple Motion allows control of animation parameters through MIDI. The 1987 first-person shooter game "MIDI Maze" and the 1990 Atari ST computer puzzle game "Oxyd" used MIDI to network computers together, and kits are available that allow MIDI control over home lighting and appliances.

Despite its association with music devices, MIDI can control any electronic or digital device that can read and process a MIDI command. The receiving device or object would require a General MIDI processor, however in this instance, the program changes would trigger a function on that device rather than notes from a MIDI instrument's controller. Each function can be set to a timer (also controlled by MIDI) or other condition or trigger determined by the device's creator.

The cables terminate in a 180° five-pin DIN connector. Standard applications use only three of the five conductors: a ground wire, and a balanced pair of conductors that carry a +5 volt signal. This connector configuration can only carry messages in one direction, so a second cable is necessary for two-way communication. Some proprietary applications, such as phantom-powered footswitch controllers, use the spare pins for direct current (DC) power transmission.

Opto-isolators keep MIDI devices electrically separated from their connectors, which prevents the occurrence of ground loops and protects equipment from voltage spikes. There is no error detection capability in MIDI, so the maximum cable length is set at 15 meters (50 feet) to limit interference.

Most devices do not copy messages from their input to their output port. A third type of port, the "thru" port, emits a copy of everything received at the input port, allowing data to be forwarded to another instrument in a "daisy chain" arrangement. Not all devices contain thru ports, and devices that lack the ability to generate MIDI data, such as effects units and sound modules, may not include out ports.

Each device in a daisy chain adds delay to the system. This is avoided with a MIDI thru box, which contains several outputs that provide an exact copy of the box's input signal. A MIDI merger is able to combine the input from multiple devices into a single stream, and allows multiple controllers to be connected to a single device. A MIDI switcher allows switching between multiple devices, and eliminates the need to physically repatch cables. MIDI patch bays combine all of these functions. They contain multiple inputs and outputs, and allow any combination of input channels to be routed to any combination of output channels. Routing setups can be created using computer software, stored in memory, and selected by MIDI program change commands. This enables the devices to function as standalone MIDI routers in situations where no computer is present. MIDI patch bays also clean up any skewing of MIDI data bits that occurs at the input stage.

MIDI data processors are used for utility tasks and special effects. These include MIDI filters, which remove unwanted MIDI data from the stream, and MIDI delays, effects that send a repeated copy of the input data at a set time.

A computer MIDI interface's main function is to match clock speeds between the MIDI device and the computer. Some computer sound cards include a standard MIDI connector, whereas others connect by any of various means that include the D-subminiature DA-15 game port, USB, FireWire, Ethernet or a proprietary connection. The increasing use of USB connectors in the 2000s has led to the availability of MIDI-to-USB data interfaces that can transfer MIDI channels to USB-equipped computers. Some MIDI keyboard controllers are equipped with USB jacks, and can be plugged into computers that run music software.

MIDI's serial transmission leads to timing problems. A three-byte MIDI message requires nearly 1 millisecond for transmission. Because MIDI is serial, it can only send one event at a time. If an event is sent on two channels at once, the event on the higher-numbered channel cannot transmit until the first one is finished, and so is delayed by 1ms. If an event is sent on all channels at the same time, the highest-numbered channel's transmission is delayed by as much as 16ms. This contributed to the rise of MIDI interfaces with multiple in- and out-ports, because timing improves when events are spread between multiple ports as opposed to multiple channels on the same port. The term "MIDI slop" refers to audible timing errors that result when MIDI transmission is delayed.

There are two types of MIDI controllers: performance controllers that generate notes and are used to perform music, and controllers that may not send notes, but transmit other types of real-time events. Many devices are some combination of the two types.

Keyboards are by far the most common type of MIDI controller. MIDI was designed with keyboards in mind, and any controller that is not a keyboard is considered an "alternative" controller. This was seen as a limitation by composers who were not interested in keyboard-based music, but the standard proved flexible, and MIDI compatibility was introduced to other types of controllers, including guitars, stringed and wind instruments, drums and specialized and experimental controllers. Other controllers include drum controllers and wind controllers, which can emulate the playing of drum kit and wind instruments, respectively.

Software synthesizers offer great power and versatility, but some players feel that division of attention between a MIDI keyboard and a computer keyboard and mouse robs some of the immediacy from the playing experience. Devices dedicated to real-time MIDI control provide an ergonomic benefit, and can provide a greater sense of connection with the instrument than an interface that is accessed through a mouse or a push-button digital menu. Controllers may be general-purpose devices that are designed to work with a variety of equipment, or they may be designed to work with a specific piece of software. Examples of the latter include Akai's APC40 controller for Ableton Live, and Korg's MS-20ic controller that is a reproduction of their MS-20 analog synthesizer. The MS-20ic controller includes patch cables that can be used to control signal routing in their virtual reproduction of the MS-20 synthesizer, and can also control third-party devices.

A MIDI instrument contains ports to send and receive MIDI signals, a CPU to process those signals, an interface that allows user programming, audio circuitry to generate sound, and controllers. The operating system and factory sounds are often stored in a Read-only memory (ROM) unit.

A MIDI instrument can also be a stand-alone module (without a piano style keyboard) consisting of a General MIDI soundboard (GM, GS and XG), onboard editing, including transposing/pitch changes, MIDI instrument changes and adjusting volume, pan, reverb levels and other MIDI controllers. Typically, the MIDI Module includes a large screen, so the user can view information for the currently selected function. Features can include scrolling lyrics, usually embedded in a MIDI file or karaoke MIDI, playlists, song library and editing screens. Some MIDI Modules include a Harmonizer and the ability to playback and transpose MP3 audio files.

Synthesizers may employ any of a variety of sound generation techniques. They may include an integrated keyboard, or may exist as "sound modules" or "expanders" that generate sounds when triggered by an external controller, such as a MIDI keyboard. Sound modules are typically designed to be mounted in a 19-inch rack. Manufacturers commonly produce a synthesizer in both standalone and rack-mounted versions, and often offer the keyboard version in a variety of sizes.

A sampler can record and digitize audio, store it in random-access memory (RAM), and play it back. Samplers typically allow a user to edit a sample and save it to a hard disk, apply effects to it, and shape it with the same tools that synthesizers use. They also may be available in either keyboard or rack-mounted form. Instruments that generate sounds through sample playback, but have no recording capabilities, are known as "ROMplers".

Samplers did not become established as viable MIDI instruments as quickly as synthesizers did, due to the expense of memory and processing power at the time. The first low-cost MIDI sampler was the Ensoniq Mirage, introduced in 1984. MIDI samplers are typically limited by displays that are too small to use to edit sampled waveforms, although some can be connected to a computer monitor.

Drum machines typically are sample playback devices that specialize in drum and percussion sounds. They commonly contain a sequencer that allows the creation of drum patterns, and allows them to be arranged into a song. There often are multiple audio outputs, so that each sound or group of sounds can be routed to a separate output. The individual drum voices may be playable from another MIDI instrument, or from a sequencer.

Sequencer technology predates MIDI. Analog sequencers use CV/Gate signals to control pre-MIDI analog synthesizers. MIDI sequencers typically are operated by transport features modeled after those of tape decks. They are capable of recording MIDI performances, and arranging them into individual tracks along a multitrack recording concept. Music workstations combine controller keyboards with an internal sound generator and a sequencer. These can be used to build complete arrangements and play them back using their own internal sounds, and function as self-contained music production studios. They commonly include file storage and transfer capabilities.

Audio effects units that are frequently used in stage and recording, such as reverbs, delays and choruses, can be remotely adjusted via MIDI signals. Some units allow only a limited number of parameters to be controlled this way, but most respond to program change messages. The Eventide H3000 Ultra-harmonizer is an example of a unit that allows such extensive MIDI control that it is playable as a synthesizer.

MIDI messages are made up of 8-bit "word"s (commonly called "bytes") that are transmitted serially at a rate of 31.25 kbit/s. This rate was chosen because it is an exact division of 1 MHz, the operational speed of many early microprocessors. The first bit of each word identifies whether the word is a status byte or a data byte, and is followed by seven bits of information. A start bit and a stop bit are added to each byte for framing purposes, so a MIDI byte requires ten bits for transmission.

A MIDI link can carry sixteen independent channels of information. The channels are numbered 1–16, but their actual corresponding binary encoding is 0–15. A device can be configured to only listen to specific channels and to ignore the messages sent on other channels ("Omni Off" mode), or it can listen to all channels, effectively ignoring the channel address ("Omni On"). An individual device may be monophonic (the start of a new "note-on" MIDI command implies the termination of the previous note), or polyphonic (multiple notes may be sounding at once, until the polyphony limit of the instrument is reached, or the notes reach the end of their decay envelope, or explicit "note-off" MIDI commands are received). Receiving devices can typically be set to all four combinations of "omni off/on" versus "mono/poly" modes.

A MIDI message is an instruction that controls some aspect of the receiving device. A MIDI message consists of a status byte, which indicates the type of the message, followed by up to two data bytes that contain the parameters. MIDI messages can be "channel messages" sent on only one of the 16 channels and monitored only by devices on that channel, or "system messages" that all devices receive. Each receiving device ignores data not relevant to its function. There are five types of message: Channel Voice, Channel Mode, System Common, System Real-Time, and System Exclusive.

Channel Voice messages transmit real-time performance data over a single channel. Examples include "note-on" messages which contain a MIDI note number that specifies the note's pitch, a velocity value that indicates how forcefully the note was played, and the channel number; "note-off" messages that end a note; program change messages that change a device's patch; and control changes that allow adjustment of an instrument's parameters. Channel Mode messages include the Omni/mono/poly mode on and off messages, as well as messages to reset all controllers to their default state or to send "note-off" messages for all notes. System messages do not include channel numbers, and are received by every device in the MIDI chain. MIDI time code is an example of a System Common message. System Real-Time messages provide for synchronization, and include MIDI clock and Active Sensing.

System Exclusive (SysEx) messages are a major reason for the flexibility and longevity of the MIDI standard. Manufacturers use them to create proprietary messages that control their equipment more thoroughly than standard MIDI messages could. SysEx messages are addressed to a specific device in a system. Each manufacturer has a unique identifier that is included in its SysEx messages, which helps ensure that only the targeted device responds to the message, and that all others ignore it. Many instruments also include a SysEx ID setting, so a controller can address two devices of the same model independently. SysEx messages can include functionality beyond what the MIDI standard provides. They target a specific instrument, and are ignored by all other devices on the system.

Devices typically do not respond to every type of message defined by the MIDI specification. The MIDI implementation chart was standardized by the MMA as a way for users to see what specific capabilities an instrument has, and how it responds to messages. A specific MIDI Implementation Chart is usually published for each MIDI device within the device documentation.

The MIDI specification for the electrical interface is based on a fully isolated current loop. The MIDI out port nominally sources a +5 volt source through a 220 ohm resistor out through pin 4 on the MIDI out DIN connector, in on pin 4 of the receiving device's MIDI in DIN connector, through a 220 ohm protection resistor and the LED of an opto-isolator. The current then returns via pin 5 on the MIDI in port to the originating device's MIDI out port pin 5, again with a 220 ohm resistor in the path, giving a nominal current of about 5 milliamperes. Despite the cable's appearance, there is no conductive path between the two MIDI devices, only an optically isolated one. Properly designed MIDI devices are relatively immune to ground loops and similar interference. The data rate on this system is 31,250 bits per second, logic 0 being current on.

The MIDI specification provides for a ground "wire" and a braid or foil shield, connected on pin 2, protecting the two signal-carrying conductors on pins 4 and 5. Although the MIDI cable is supposed to connect pin 2 and the braid or foil shield to chassis ground, it should do so only at the MIDI out port; the MIDI in port should leave pin 2 unconnected and isolated. Some large manufacturers of MIDI devices use modified MIDI in-only DIN 5-pin sockets with the metallic conductors intentionally omitted at pin positions 1, 2, and 3 so that the maximum voltage isolation is obtained.

MIDI's flexibility and widespread adoption have led to many refinements of the standard, and have enabled its application to purposes beyond those for which it was originally intended.

MIDI allows selection of an instrument's sounds through program change messages, but there is no guarantee that any two instruments have the same sound at a given program location. Program #0 may be a piano on one instrument, or a flute on another. The General MIDI (GM) standard was established in 1991, and provides a standardized sound bank that allows a Standard MIDI File created on one device to sound similar when played back on another. GM specifies a bank of 128 sounds arranged into 16 families of eight related instruments, and assigns a specific program number to each instrument. Percussion instruments are placed on channel 10, and a specific MIDI note value is mapped to each percussion sound. GM-compliant devices must offer 24-note polyphony. Any given program change selects the same instrument sound on any GM-compatible instrument.

The GM standard eliminates variation in note mapping. Some manufacturers had disagreed over what note number should represent middle C, but GM specifies that note number 69 plays A440, which in turn fixes middle C as note number 60. GM-compatible devices are required to respond to velocity, aftertouch, and pitch bend, to be set to specified default values at startup, and to support certain controller numbers such as for sustain pedal, and Registered Parameter Numbers. A simplified version of GM, called "GM Lite", is used in mobile phones and other devices with limited processing power.

A general opinion quickly formed that the GM's 128-instrument sound set was not large enough. Roland's General Standard, or GS, system included additional sounds, drumkits and effects, provided a "bank select" command that could be used to access them, and used MIDI Non-Registered Parameter Numbers (NRPNs) to access its new features. Yamaha's Extended General MIDI, or XG, followed in 1994. XG similarly offered extra sounds, drumkits and effects, but used standard controllers instead of NRPNs for editing, and increased polyphony to 32 voices. Both standards feature backward compatibility with the GM specification, but are not compatible with each other. Neither standard has been adopted beyond its creator, but both are commonly supported by music software titles.

Member companies of Japan's AMEI developed the General MIDI Level 2 specification in 1999. GM2 maintains backward compatibility with GM, but increases polyphony to 32 voices, standardizes several controller numbers such as for sostenuto and soft pedal ("una corda"), RPNs and Universal System Exclusive Messages, and incorporates the MIDI Tuning Standard. GM2 is the basis of the instrument selection mechanism in Scalable Polyphony MIDI (SP-MIDI), a MIDI variant for low power devices that allows the device's polyphony to scale according to its processing power.

Most MIDI synthesizers use equal temperament tuning. The MIDI tuning standard (MTS), ratified in 1992, allows alternate tunings. MTS allows microtunings that can be loaded from a bank of up to 128 patches, and allows real-time adjustment of note pitches. Manufacturers are not required to support the standard. Those who do are not required to implement all of its features.

A sequencer can drive a MIDI system with its internal clock, but when a system contains multiple sequencers, they must synchronize to a common clock. MIDI Time Code (MTC), developed by Digidesign, implements SysEx messages that have been developed specifically for timing purposes, and is able to translate to and from the SMPTE time code standard. MIDI Clock is based on tempo, but SMPTE time code is based on frames per second, and is independent of tempo. MTC, like SMPTE code, includes position information, and can adjust itself if a timing pulse is lost. MIDI interfaces such as Mark of the Unicorn's MIDI Timepiece can convert SMPTE code to MTC.

MIDI Machine Control (MMC) consists of a set of SysEx commands that operate the transport controls of hardware recording devices. MMC lets a sequencer send "Start", "Stop", and "Record" commands to a connected tape deck or hard disk recording system, and to fast-forward or rewind the device so that it starts playback at the same point as the sequencer. No synchronization data is involved, although the devices may synchronize through MTC.

MIDI Show Control (MSC) is a set of SysEx commands for sequencing and remotely cueing show control devices such as lighting, music and sound playback, and motion control systems. Applications include stage productions, museum exhibits, recording studio control systems, and amusement park attractions.

One solution to MIDI timing problems is to mark MIDI events with the times they are to be played, and store them in a buffer in the MIDI interface ahead of time. Sending data beforehand reduces the likelihood that a busy passage can send a large amount of information that overwhelms the transmission link. Once stored in the interface, the information is no longer subject to timing issues associated with USB jitter and computer operating system interrupts, and can be transmitted with a high degree of accuracy. MIDI timestamping only works when both hardware and software support it. MOTU's MTS, eMagic's AMT, and Steinberg's Midex 8 had implementations that were incompatible with each other, and required users to own software and hardware manufactured by the same company to work. Timestamping is built into FireWire MIDI interfaces, Mac OS X Core Audio, and Linux ALSA Sequencer.

An unforeseen capability of SysEx messages was their use for transporting audio samples between instruments. This led to the development of the sample dump standard (SDS), which established a new SysEx format for sample transmission. The SDS was later augmented with a pair of commands that allow the transmission of information about sample loop points, without requiring that the entire sample be transmitted.

The Downloadable Sounds (DLS) specification, ratified in 1997, allows mobile devices and computer sound cards to expand their wave tables with downloadable sound sets. The DLS Level 2 Specification followed in 2006, and defined a standardized synthesizer architecture. The Mobile DLS standard calls for DLS banks to be combined with SP-MIDI, as self-contained Mobile XMF files.

In addition to the original 31.25 kbit/s current-loop transported on 5-pin DIN, other connectors have been used for the same electrical data, and transmission of MIDI streams in different forms over USB, IEEE 1394 a.k.a. FireWire, and Ethernet is now common. Some samplers and hard drive recorders can also pass MIDI data between each other over SCSI.

Members of the USB-IF in 1999 developed a standard for MIDI over USB, the "Universal Serial Bus Device Class Definition for MIDI Devices" MIDI over USB has become increasingly common as other interfaces that had been used for MIDI connections (serial, joystick, etc.) disappeared from personal computers. Linux, Microsoft Windows, Macintosh OS X, and Apple iOS operating systems include standard class drivers to support devices that use the "Universal Serial Bus Device Class Definition for MIDI Devices". Some manufacturers choose to implement a MIDI interface over USB that is designed to operate differently from the class specification, using custom drivers.

Apple Computer developed the FireWire interface during the 1990s. It began to appear on digital video cameras toward the end of the decade, and on G3 Macintosh models in 1999. It was created for use with multimedia applications. Unlike USB, FireWire uses intelligent controllers that can manage their own transmission without attention from the main CPU. As with standard MIDI devices, FireWire devices can communicate with each other with no computer present.

The Octave-Plateau Voyetra-8 synthesizer was an early MIDI implementation using XLR3 connectors in place of the 5-pin DIN. It was released in the pre-MIDI years and later retrofitted with a MIDI interface but keeping its XLR connector.

As computer-based studio setups became common, MIDI devices that could connect directly to a computer became available. These typically used the 8-pin mini-DIN connector that was used by Apple for serial and printer ports prior to the introduction of the Blue & White G3 models. MIDI interfaces intended for use as the centerpiece of a studio, such as the Mark of the Unicorn MIDI Time Piece, were made possible by a "fast" transmission mode that could take advantage of these serial ports' ability to operate at 20 times the standard MIDI speed. Mini-DIN ports were built into some late-1990s MIDI instruments, and enabled such devices to be connected directly to a computer. Some devices connected via PCs' DB-25 parallel port, or through the joystick port found in many PC sound cards.

Yamaha introduced the mLAN protocol in 1999. It was conceived as a Local Area Network for musical instruments using FireWire as the transport, and was designed to carry multiple MIDI channels together with multichannel digital audio, data file transfers, and time code. mLan was used in a number of Yamaha products, notably digital mixing consoles and the Motif synthesizer, and in third-party products such as the PreSonus FIREstation and the Korg Triton Studio. No new mLan products have been released since 2007.

Computer network implementations of MIDI provide network routing capabilities, and the high-bandwidth channel that earlier alternatives to MIDI, such as ZIPI, were intended to bring. Proprietary implementations have existed since the 1980s, some of which use fiber optic cables for transmission. The Internet Engineering Task Force's RTP-MIDI open specification has gained industry support. Apple has supported this protocol from Mac OS X 10.4 onwards, and a Windows driver based on Apple's implementation exists for Windows XP and newer versions.

Systems for wireless MIDI transmission have been available since the 1980s. Several commercially available transmitters allow wireless transmission of MIDI and OSC signals over Wi-Fi and Bluetooth. iOS devices are able to function as MIDI control surfaces, using Wi-Fi and OSC. An XBee radio can be used to build a wireless MIDI transceiver as a do-it-yourself project. Android devices are able to function as full MIDI control surfaces using several different protocols over Wi-Fi and Bluetooth.

Some devices use standard TRS audio minijack connectors for MIDI data, including the Korg Electribe 2 and the Arturia Beatstep Pro. Both come with adaptors that break out to standard 5-pin DIN connectors.

A new protocol, tentatively called "HD Protocol" or "High-Definition Protocol", has been discussed since 2005, when it was announced as "HD-MIDI". This new standard offers full backward compatibility with MIDI 1.0 and supports higher-speed transports, plug-and-play device discovery and enumeration, and greater data range and resolution. It increases the numbers of channels and controllers, and simplifies messages. HD Protocol supports entirely new kinds of events, such as a Note Update message and Direct Pitch in the Note message, which are aimed at guitar controllers. Proposed physical layer transports include Ethernet-based protocols such as RTP MIDI and Audio Video Bridging. The HD Protocol and a User Datagram Protocol (UDP)-based transport are under review by MMA's High-Definition Protocol Working Group (HDWG), which includes representatives from all sizes and types of companies.

Prototype devices based on the draft standard have been shown privately at NAMM using wired and wireless connections, however it is uncertain if and when the industry will release products that use the new protocol. As of 2015, the HD Protocol specifications are nearing completion and MMA develops the policies on licensing and product certification.

MIDI Polyphonic Expression (MPE) is a method of using MIDI that enables pitch bend, and other dimensions of expressive control, to be adjusted continuously for individual notes. Instruments like the Continuum Fingerboard, Linnstrument, and Seaboard let users control pitch, timbre, and other nuances for individual notes within chords. A growing number of soft synths and effects are also compatible with MPE (such as Equator, UVI Falcon, and Sandman Pro), as well as a few hardware synths (such as Modal Electronics 002, Futuresonus Parva, and Modor NF-1). MPE works by assigning each note to its own MIDI channel so that particular messages can be applied to each note individually.




</doc>
<doc id="19999" url="https://en.wikipedia.org/wiki?curid=19999" title="Microcode">
Microcode

Microcode is a computer hardware technique that imposes an interpreter between the CPU hardware and the programmer-visible instruction set architecture of the computer. As such, the microcode is a layer of hardware-level instructions that implement higher-level machine code instructions or internal state machine sequencing in many digital processing elements. Microcode is used in general-purpose central processing units, although in current desktop CPUs it is only a fallback path for cases that the faster hardwired control unit cannot handle.

Microcode typically resides in special high-speed memory and translates machine instructions, state machine data or other input into sequences of detailed circuit-level operations. It separates the machine instructions from the underlying electronics so that instructions can be designed and altered more freely. It also facilitates the building of complex multi-step instructions, while reducing the complexity of computer circuits. Writing microcode is often called microprogramming and the microcode in a particular processor implementation is sometimes called a microprogram.

More extensive microcoding allows small and simple microarchitectures to emulate more powerful architectures with wider word length, more execution units and so on, which is a relatively simple way to achieve software compatibility between different products in a processor family.

Some hardware vendors, especially IBM, use the term "microcode" as a synonym for "firmware". In that way, all code within a device is termed "microcode" regardless of it being microcode or machine code; for example, hard disk drives are said to have their microcode updated, though they typically contain both microcode and firmware.

The lowest layer in a computer's software stack is traditionally raw binary machine code instructions for the processor. Microcode sits one level below this. To avoid confusion, each microprogram-related element is differentiated by the "micro" prefix: microinstruction, microassembler, microprogrammer, microarchitecture, etc.

Engineers normally write the microcode during the design phase of a processor, storing it in a read-only memory (ROM) or programmable logic array (PLA) structure, or in a combination of both. However, machines also exist that have some or all microcode stored in SRAM or flash memory. This is traditionally denoted as "writeable control store" in the context of computers, which can be either read-only or read-write memory. In the latter case, the CPU initialization process loads microcode into the control store from another storage medium, with the possibility of altering the microcode to correct bugs in the instruction set, or to implement new machine instructions.

Complex digital processors may also employ more than one (possibly microcode-based) control unit in order to delegate sub-tasks that must be performed essentially asynchronously in parallel. A high-level programmer, or even an assembly programmer, does not normally see or change microcode. Unlike machine code, which often retains some backward compatibility among different processors in a family, microcode only runs on the exact electronic circuitry for which it is designed, as it constitutes an inherent part of the particular processor design itself.

Microprograms consist of series of microinstructions, which control the CPU at a very fundamental level of hardware circuitry. For example, a single typical "horizontal" microinstruction might specify the following operations:

To simultaneously control all processor's features in one cycle, the microinstruction is often wider than 50 bits; e.g., 128 bits on a 360/85 with an emulator feature. Microprograms are carefully designed and optimized for the fastest possible execution, as a slow microprogram would result in a slow machine instruction and degraded performance for related application programs that use such instructions.

Microcode was originally developed as a simpler method of developing the control logic for a computer. Initially, CPU instruction sets were hardwired. Each step needed to fetch, decode, and execute the machine instructions (including any operand address calculations, reads, and writes) was controlled directly by combinational logic and rather minimal sequential state machine circuitry. While very efficient, the need for powerful instruction sets with multi-step addressing and complex operations ("see below") made such hard-wired processors difficult to design and debug; highly encoded and varied-length instructions can contribute to this as well, especially when very irregular encodings are used.

Microcode simplified the job by allowing much of the processor's behaviour and programming model to be defined via microprogram routines rather than by dedicated circuitry. Even late in the design process, microcode could easily be changed, whereas hard-wired CPU designs were very cumbersome to change. Thus, this greatly facilitated CPU design.

From the 1940s to the late 1970s, a large portion of programming was done in assembly language; higher-level instructions mean greater programmer productivity, so an important advantage of microcode was the relative ease by which powerful machine instructions can be defined. The ultimate extension of this are "Directly Executable High Level Language" designs, in which each statement of a high-level language such as PL/I is entirely and directly executed by microcode, without compilation. The IBM Future Systems project and Data General Fountainhead Processor are examples of this. During the 1970s, CPU speeds grew more quickly than memory speeds and numerous techniques such as memory block transfer, memory pre-fetch and multi-level caches were used to alleviate this. High-level machine instructions, made possible by microcode, helped further, as fewer more complex machine instructions require less memory bandwidth. For example, an operation on a character string can be done as a single machine instruction, thus avoiding multiple instruction fetches.

Architectures with instruction sets implemented by complex microprograms included the IBM System/360 and Digital Equipment Corporation VAX. The approach of increasingly complex microcode-implemented instruction sets was later called CISC. An alternate approach, used in many microprocessors, is to use PLAs or ROMs (instead of combinational logic) mainly for instruction decoding, and let a simple state machine (without much, or any, microcode) do most of the sequencing. The MOS Technology 6502 is an example of a microprocessor using a PLA for instruction decode and sequencing. The PLA is visible in photomicrographs of the chip, and its operation can be seen in the transistor-level simulation.

Microprogramming is still used in modern CPU designs. In some cases, after the microcode is debugged in simulation, logic functions are substituted for the control store. Logic functions are often faster and less expensive than the equivalent microprogram memory.

A processor's microprograms operate on a more primitive, totally different, and much more hardware-oriented architecture than the assembly instructions visible to normal programmers. In coordination with the hardware, the microcode implements the programmer-visible architecture. The underlying hardware need not have a fixed relationship to the visible architecture. This makes it easier to implement a given instruction set architecture on a wide variety of underlying hardware micro-architectures.

The IBM System/360 has a 32-bit architecture with 16 general-purpose registers, but most of the System/360 implementations actually use hardware that implemented a much simpler underlying microarchitecture; for example, the System/360 Model 30 has 8-bit data paths to the arithmetic logic unit (ALU) and main memory and implemented the general-purpose registers in a special unit of higher-speed core memory, and the System/360 Model 40 has 8-bit data paths to the ALU and 16-bit data paths to main memory and also implemented the general-purpose registers in a special unit of higher-speed core memory. The Model 50 has full 32-bit data paths and implements the general-purpose registers in a special unit of higher-speed core memory. The Model 65 through the Model 195 have larger data paths and implement the general-purpose registers in faster transistor circuits. In this way, microprogramming enabled IBM to design many System/360 models with substantially different hardware and spanning a wide range of cost and performance, while making them all architecturally compatible. This dramatically reduces the number of unique system software programs that must be written for each model.

A similar approach was used by Digital Equipment Corporation (DEC) in their VAX family of computers. As a result, different VAX processors use different microarchitectures, yet the programmer-visible architecture does not change.

Microprogramming also reduces the cost of field changes to correct defects (bugs) in the processor; a bug can often be fixed by replacing a portion of the microprogram rather than by changes being made to hardware logic and wiring.

In 1947, the design of the MIT Whirlwind introduced the concept of a control store as a way to simplify computer design and move beyond "ad hoc" methods. The control store is a diode matrix: a two-dimensional lattice, where one dimension accepts "control time pulses" from the CPU's internal clock, and the other connects to control signals on gates and other circuits. A "pulse distributor" takes the pulses generated by the CPU clock and breaks them up into eight separate time pulses, each of which activates a different row of the lattice. When the row is activated, it activates the control signals connected to it.

Described another way, the signals transmitted by the control store are being played much like a player piano roll. That is, they are controlled by a sequence of very wide words constructed of bits, and they are "played" sequentially. In a control store, however, the "song" is short and repeated continuously.

In 1951, Maurice Wilkes enhanced this concept by adding "conditional execution", a concept akin to a conditional in computer software. His initial implementation consisted of a pair of matrices: the first one generated signals in the manner of the Whirlwind control store, while the second matrix selected which row of signals (the microprogram instruction word, so to speak) to invoke on the next cycle. Conditionals were implemented by providing a way that a single line in the control store could choose from alternatives in the second matrix. This made the control signals conditional on the detected internal signal. Wilkes coined the term microprogramming to describe this feature and distinguish it from a simple control store.



Each microinstruction in a microprogram provides the bits that control the functional elements that internally compose a CPU. The advantage over a hard-wired CPU is that internal CPU control becomes a specialized form of a computer program. Microcode thus transforms a complex electronic design challenge (the control of a CPU) into a less complex programming challenge. To take advantage of this, a CPU is divided into several parts:


There may also be a memory address register and a memory data register, used to access the main computer storage. Together, these elements form an "execution unit". Most modern CPUs have several execution units. Even simple computers usually have one unit to read and write memory, and another to execute user code. These elements could often be brought together as a single chip. This chip comes in a fixed width that would form a "slice" through the execution unit. These are known as "bit slice" chips. The AMD Am2900 family is one of the best known examples of bit slice elements. The parts of the execution units and the execution units themselves are interconnected by a bundle of wires called a bus.

Programmers develop microprograms, using basic software tools. A microassembler allows a programmer to define the table of bits symbolically. Because of its close relationship to the underlying architecture, "microcode has several properties that make it difficult to generate using a compiler." A simulator program is intended to execute the bits in the same way as the electronics, and allows much more freedom to debug the microprogram. After the microprogram is finalized, and extensively tested, it is sometimes used as the input to a computer program that constructs logic to produce the same data. This program is similar to those used to optimize a programmable logic array. Even without fully optimal logic, heuristically optimized logic can vastly reduce the number of transistors from the number required for a ROM control store. This reduces the cost of producing, and the electricity consumed by, a CPU.

Microcode can be characterized as "horizontal" or "vertical", referring primarily to whether each microinstruction controls CPU elements with little or no decoding (horizontal microcode) or requires extensive decoding by combinatorial logic before doing so (vertical microcode). Consequently, each horizontal microinstruction is wider (contains more bits) and occupies more storage space than a vertical microinstruction.

"Horizontal microcode has several discrete micro-operations that are combined in a single microinstruction for simultaneous operation." Horizontal microcode is typically contained in a fairly wide control store; it is not uncommon for each word to be 108 bits or more. On each tick of a sequencer clock a microcode word is read, decoded, and used to control the functional elements that make up the CPU.

In a typical implementation a horizontal microprogram word comprises fairly tightly defined groups of bits. For example, one simple arrangement might be:

For this type of micromachine to implement a JUMP instruction with the address following the opcode, the microcode might require two clock ticks. The engineer designing it would write microassembler source code looking something like this:

For each tick it is common to find that only some portions of the CPU are used, with the remaining groups of bits in the microinstruction being no-ops. With careful design of hardware and microcode, this property can be exploited to parallelise operations that use different areas of the CPU; for example, in the case above, the ALU is not required during the first tick, so it could potentially be used to complete an earlier arithmetic instruction.

In vertical microcode, each microinstruction is significantly encoded that is, the bit fields generally pass through intermediate combinatory logic that, in turn, generates the actual control and sequencing signals for internal CPU elements (ALU, registers, etc.). This is in contrast with horizontal microcode, in which the bit fields themselves either directly produce the control and sequencing signals or are only minimally encoded. Consequently, vertical microcode requires smaller instruction lengths and less storage, but requires more time to decode, resulting in a slower CPU clock.

Some vertical microcode is just the assembly language of a simple conventional computer that is emulating a more complex computer. Some processors, such as DEC Alpha processors and the CMOS microprocessors on later IBM System/390 mainframes and z/Architecture mainframes, have PALcode (the term used on Alpha processors) or millicode (the term used on IBM mainframe microprocessors). This is a form of machine code, with access to special registers and other hardware resources not available to regular machine code, used to implement some instructions and other functions, such as page table walks on Alpha processors.

Another form of vertical microcode has two fields:
The "field select" selects which part of the CPU will be controlled by this word of the control store. The "field value" actually controls that part of the CPU. With this type of microcode, a designer explicitly chooses to make a slower CPU to save money by reducing the unused bits in the control store; however, the reduced complexity may increase the CPU's clock frequency, which lessens the effect of an increased number of cycles per instruction.

As transistors became cheaper, horizontal microcode came to dominate the design of CPUs using microcode, with vertical microcode being used less often.

When both vertical and horizontal microcode are used, the horizontal microcode may be referred to as "nanocode" or "picocode".

A few computers were built using "writable microcode". In this design, rather than storing the microcode in ROM or hard-wired logic, the microcode is stored in a RAM called a "writable control store" or "WCS". Such a computer is sometimes called a "writable instruction set computer" or "WISC".

Many experimental prototype computers use writable control stores; there are also commercial machines that use writable microcode, such as the Burroughs Small Systems, early Xerox workstations, the DEC VAX 8800 ("Nautilus") family, the Symbolics L- and G-machines, a number of IBM System/360 and System/370 implementations, some DEC PDP-10 machines, and the Data General Eclipse MV/8000.

Many more machines offer user-programmable writable control stores as an option, including the HP 2100, DEC PDP-11/60 and Varian Data Machines V-70 series minicomputers. The IBM System/370 includes a facility called "Initial-Microprogram Load" ("IML" or "IMPL") that can be invoked from the console, as part of "power-on reset" ("POR") or from another processor in a tightly coupled multiprocessor complex.

Some commercial machines, for example IBM 360/85, have both a read-only storage and a writable control store for microcode.

WCS offers several advantages including the ease of patching the microprogram and, for certain hardware generations, faster access than ROMs can provide. User-programmable WCS allows the user to optimize the machine for specific purposes.

Starting with the Pentium Pro in 1995, several Intel x86 CPUs have writable microcode. This, for example, has allowed bugs in the Intel Core 2 and Intel Xeon microcodes to be fixed by patching their microprograms, rather than requiring the entire chips to be replaced. A second prominent example is the set of microcode patches that Intel offered for improving the capabilities of processor architectures of up to nearly 10 years in age in the progress of counter fighting the serious Spectre and Meltdown security dangers as went really public in start of 2018. A microcode update can be installed by Linux, FreeBSD, Microsoft Windows, or the motherboard BIOS.

The design trend toward heavily microcoded processors with complex instructions began in the early 1960s and continued until roughly the mid-1980s. At that point the RISC design philosophy started becoming more prominent.

A CPU that uses microcode generally takes several clock cycles to execute a single instruction, one clock cycle for each step in the microprogram for that instruction. Some CISC processors include instructions that can take a very long time to execute. Such variations interfere with both interrupt latency and, what is far more important in modern systems, pipelining.

When designing a new processor, a hardwired control RISC has the following advantages over microcoded CISC:


There are counterpoints as well:

Many RISC and VLIW processors are designed to execute every instruction (as long as it is in the cache) in a single cycle. This is very similar to the way CPUs with microcode execute one microinstruction per cycle. VLIW processors have instructions that behave similarly to very wide horizontal microcode, although typically without such fine-grained control over the hardware as provided by microcode. RISC instructions are sometimes similar to the narrow vertical microcode.

Microcoding has been popular in application-specific processors such as network processors, microcontrollers, digital signal processors, channel controllers, disk controllers, network interface controllers, graphics processing units, and in other hardware.

Modern CISC implementations, such as the x86 family, decode instructions into dynamically buffered micro-operations ("μops") with an instruction encoding similar to RISC or traditional microcode. A hardwired instruction decode unit directly emits μops for common x86 instructions, but falls back to a more traditional microcode ROM for more complex or rarely used instructions.

For example, an x86 might look up μops from microcode to handle complex multistep operations such as loop or string instructions, floating point unit transcendental functions or unusual values such as denormal numbers, and special purpose instructions such as CPUID.




</doc>
<doc id="20003" url="https://en.wikipedia.org/wiki?curid=20003" title="Multitier architecture">
Multitier architecture

In software engineering, multitier architecture (often referred to as "n"-tier architecture) or multilayered architecture is a client–server architecture in which presentation, application processing, and data management functions are physically separated. The most widespread use of multitier architecture is the three-tier architecture.

"N"-tier application architecture provides a model by which developers can create flexible and reusable applications. By segregating an application into tiers, developers acquire the option of modifying or adding a specific layer, instead of reworking the entire application. A three-tier architecture is typically composed of a "presentation" tier, a "domain logic" tier, and a "data storage" tier.

While the concepts of layer and tier are often used interchangeably, one fairly common point of view is that there is indeed a difference. This view holds that a "layer" is a logical structuring mechanism for the elements that make up the software solution, while a "tier" is a physical structuring mechanism for the system infrastructure. For example, a three-layer solution could easily be deployed on a single tier, such as a personal workstation.

The "Layers" architectural pattern has been described in various publications.

In a logical multilayered architecture for an information system with an object-oriented design, the following four are the most common:


The book "Domain Driven Design" describes some common uses for the above four layers, although its primary focus is the domain layer.

If the application architecture has no explicit distinction between the business layer and the presentation layer (i.e., the presentation layer is considered part of the business layer), then a traditional client-server (two-tier) model has been implemented.

The more usual convention is that the application layer (or service layer) is considered a sublayer of the business layer, typically encapsulating the API definition surfacing the supported business functionality. The application/business layers can, in fact, be further subdivided to emphasize additional sublayers of distinct responsibility. For example, if the Model View Presenter pattern is used, the presenter sublayer might be used as an additional layer between the user interface layer and the business/application layer (as represented by the model sublayer).

Some also identify a separate layer called the business infrastructure layer (BI), located between the business layer(s) and the infrastructure layer(s). It's also sometimes called the "low-level business layer" or the "business services layer". This layer is very general and can be used in several application tiers (e.g. a CurrencyConverter).

The infrastructure layer can be partitioned into different levels (high-level or low-level technical services). Developers often focus on the persistence (data access) capabilities of the infrastructure layer and therefore only talk about the persistence layer or the data access layer (instead of an infrastructure layer or technical services layer). In other words, the other kind of technical services are not always explicitly thought of as part of any particular layer.

A layer is on top of another, because it depends on it. Every layer can exist without the layers above it, and requires the layers below it to function. Another common view is that layers do not always strictly depend on only the adjacent layer below. For example, in a relaxed layered system (as opposed to a strict layered system) a layer can also depend on all the layers below it.

Three-tier architecture is a client–server software architecture pattern in which the user interface (presentation), functional process logic ("business rules"), computer data storage and data access are developed and maintained as independent modules, most often on separate platforms. It was developed by John J. Donovan in Open Environment Corporation (OEC), a tools company he founded in Cambridge, Massachusetts.

Apart from the usual advantages of modular software with well-defined interfaces, the three-tier architecture is intended to allow any of the three tiers to be upgraded or replaced independently in response to changes in requirements or technology. For example, a change of operating system in the "presentation tier" would only affect the user interface code.

Typically, the user interface runs on a desktop PC or workstation and uses a standard graphical user interface, functional process logic that may consist of one or more separate modules running on a workstation or application server, and an RDBMS on a database server or mainframe that contains the computer data storage logic. The middle tier may be multitiered itself (in which case the overall architecture is called an ""n"-tier architecture").

Three-tier architecture:

In the web development field, three-tier is often used to refer to websites, commonly electronic commerce websites, which are built using three tiers:

Data transfer between tiers is part of the architecture. Protocols involved may include one or more of SNMP, CORBA, Java RMI, .NET Remoting, Windows Communication Foundation, sockets, UDP, web services or other standard or proprietary protocols. Often middleware is used to connect the separate tiers. Separate tiers often (but not necessarily) run on separate physical servers, and each tier may itself run on a cluster.

The end-to-end traceability of data flows through "n"-tier systems is a challenging task which becomes more important when systems increase in complexity. The Application Response Measurement defines concepts and APIs for measuring performance and correlating transactions between tiers. 
Generally, the term "tiers" is used to describe physical distribution of components of a system on separate servers, computers, or networks (processing nodes). A three-tier architecture then will have three processing nodes. The term "layers" refer to a logical grouping of components which may or may not be physically located on one processing node.




</doc>
<doc id="20016" url="https://en.wikipedia.org/wiki?curid=20016" title="Myrinet">
Myrinet

Myrinet, ANSI/VITA 26-1998, is a high-speed local area networking system designed by the company Myricom to be used as an interconnect between multiple machines to form computer clusters. 

Myrinet was promoted as having lower protocol overhead than standards such as Ethernet, and therefore better throughput, less interference, and lower latency while using the host CPU. Although it can be used as a traditional networking system, Myrinet is often used directly by programs that "know" about it, thereby bypassing a call into the operating system.

Myrinet physically consists of two fibre optic cables, upstream and downstream, connected to the host computers with a single connector. Machines are connected via low-overhead routers and switches, as opposed to connecting one machine directly to another. Myrinet includes a number of fault-tolerance features, mostly backed by the switches. These include flow control, error control, and "heartbeat" monitoring on every link. The "fourth-generation" Myrinet, called Myri-10G, supported a 10 Gbit/s data rate and can use 10 Gigabit Ethernet on PHY, the physical layer (cables, connectors, distances, signaling). Myri-10G started shipping at the end of 2005.

Myrinet was approved in 1998 by the American National Standards Institute for use on the VMEbus as ANSI/VITA 26-1998.

Myrinet is a lightweight protocol with little overhead that allows it to operate with throughput close to the basic signaling speed of the physical layer. For supercomputing, the low latency of Myrinet is even more important than its throughput performance, since, according to Amdahl's law, a high-performance parallel system tends to be bottlenecked by its slowest sequential process, which in all but the most embarrassingly parallel supercomputer workloads is often the latency of message transmission across the network.

According to Myricom, 141 (28.2%) of the June 2005 TOP500 supercomputers used Myrinet technology. In the November 2005 TOP500, the number of supercomputers using Myrinet was down to 101 computers, or 20.2%, in November 2006, 79 (15.8%), and by November 2007, 18 (3.6%), a long way behind gigabit Ethernet at 54% and InfiniBand at 24.2%.

In the June 2014 TOP500 list, the number of supercomputers using Myrinet interconnect was 1 (0.2%).

In November, 2013, the assets of Myricom (including the Myrinet technology) were acquired by CSP Inc. In 2016, it was reported that Google had also offered to buy the company.




</doc>
<doc id="20017" url="https://en.wikipedia.org/wiki?curid=20017" title="Musique concrète">
Musique concrète

Musique concrète (, meaning "concrete music") is a form of musique expérimentale (experimental music ) that exploits acousmatic listening, meaning sound identities can often be intentionally obscured or appear unconnected to their source cause. It can feature sounds derived from recordings of musical instruments, the human voice, and the natural environment as well as those created using synthesizers and computer-based digital signal processing. Compositions in this idiom are not restricted to the normal musical rules of melody, harmony, rhythm, metre, and so on. Originally contrasted with "pure" "elektronische Musik" (based solely on the production and manipulation of electronically produced sounds rather than recorded sounds), the theoretical basis of "musique concrète" as a compositional practice was developed by Pierre Schaeffer, beginning in the early 1940s. From the late 1960s onward, and particularly in France, the term acousmatic music ("musique acousmatique") started to be used in reference to fixed media compositions that utilized both musique concrète based techniques and live sound spatialisation.

In 1928 music critic André Cœuroy wrote in his book "Panorama of Contemporary Music" that "perhaps the time is not far off when a composer will be able to represent through recording, music specifically composed for the gramophone" . In the same period the American composer Henry Cowell, in referring to the projects of Nikolai Lopatnikoff, believed that "there was a wide field open for the composition of music for phonographic discs." This sentiment was echoed further in 1930 by Igor Stravinsky, when he stated in the revue "Kultur und Schallplatte" that "there will be a greater interest in creating music in a way that will be peculiar to the gramophone record." The following year, 1931, Boris de Schloezer also expressed the opinion that one could write for the gramophone or for the wireless just as one can for the piano or the violin . Shortly after, German art theorist Rudolf Arnheim discussed the effects of microphonic recording in an essay entitled "Radio", published in 1936. In it the idea of a creative role for the recording medium was introduced and Arnheim stated that: "The rediscovery of the musicality of sound in noise and in language, and the reunification of music, noise and language in order to obtain a unity of material: that is one of the chief artistic tasks of radio" .

In 1942 French composer and theoretician Pierre Schaeffer began his exploration of radiophony when he joined Jacques Copeau and his pupils in the foundation of the Studio d'Essai de la Radiodiffusion nationale. The studio originally functioned as a center for the Resistance movement in French radio, which in August 1944 was responsible for the first broadcasts in liberated Paris. It was here that Schaeffer began to experiment with creative radiophonic techniques using the sound technologies of the time .

The development of Schaeffer's practice was informed by encounters with voice actors, and microphone usage and radiophonic art played an important part in inspiring and consolidating Schaeffer's conception of sound-based composition . Another important influence on Schaeffer's practice was cinema, and the techniques of recording and montage, which were originally associated with cinematographic practice, came to "serve as the substrate of musique concrète." Marc Battier notes that, prior to Schaeffer, Jean Epstein drew attention to the manner in which sound recording revealed what was hidden in the act of basic acoustic listening. Epstein's reference to this "phenomenon of an epiphanic being", which appears through the transduction of sound, proved influential on Schaeffer's concept of reduced listening. Schaeffer would explicitly cite Jean Epstein with reference to his use of extra-musical sound material. Epstein had already imagined that "through the transposition of natural sounds, it becomes possible to create chords and dissonances, melodies and symphonies of noise, which are a new and specifically cinematographic music" .

Perhaps earlier than Schaeffer conducting his preliminary experiments into sound manipulation (assuming these were later than 1944, and not as early as the foundation of the Studio d'Essai in 1942) was the activity of Egyptian composer Halim El-Dabh. As a student in Cairo in the early to mid-1940s he began experimenting with "tape music" using a cumbersome wire recorder. He recorded the sounds of an ancient "zaar" ceremony and at the Middle East Radio studios processed the material using reverberation, echo, voltage controls, and re-recording. The resulting tape-based composition, entitled "The Expression of Zaar", was presented in 1944 at an art gallery event in Cairo. El-Dabh has described his initial activities as an attempt to unlock "the inner sound" of the recordings. While his early compositional work was not widely known outside of Egypt at the time, El-Dabh would eventually gain recognition for his influential work at the Columbia-Princeton Electronic Music Center in the late 1950s .

Following Schaeffer's work with Studio d'Essai at Radiodiffusion Nationale during the early 1940s he was credited with originating the theory and practice of "musique concrète." The Studio d'Essai was renamed Club d'Essai de la Radiodiffusion-Télévision Française in 1946 and in the same year Schaeffer discussed, in writing, the question surrounding the transformation of time perceived through recording. The essay evidenced knowledge of sound manipulation techniques he would further exploit compositionally. In 1948 Schaeffer formally initiated "research in to noises" at the Club d'Essai and on 5 October 1948 the results of his initial experimentation were premiered at a concert given in Paris . Five works for phonograph (known collectively as "Cinq études de bruits"—Five Studies of Noises) including "Etude violette" ("Study in Purple") and "Etude aux chemins de fer" (Study of the Railroads), were presented.

By 1949 Schaeffer's compositional work was known publicly as "musique concrète" . Schaeffer stated: "when I proposed the term 'musique concrète,' I intended … to point out an opposition with the way musical work usually goes. Instead of notating musical ideas on paper with the symbols of solfege and entrusting their realization to well-known instruments, the question was to collect concrete sounds, wherever they came from, and to abstract the musical values they were potentially containing" . According to Pierre Henry, "musique concrète was not a study of timbre, it is focused on envelopes, forms. It must be presented by means of non-traditional characteristics, you see … one might say that the origin of this music is also found in the interest in 'plastifying' music, of rendering it plastic like sculpture…musique concrète, in my opinion … led to a manner of composing, indeed, a new mental framework of composing" . Schaeffer had developed an aesthetic that was centred upon the use of sound as a primary compositional resource. The aesthetic also emphasised the importance of play ("jeu") in the practice of sound based composition. Schaeffer's use of the word "jeu", from the verb "jouer", carries the same double meaning as the English verb play: 'to enjoy oneself by interacting with one's surroundings', as well as 'to operate a musical instrument' .

By 1951 the work of Schaeffer, composer-percussionist Pierre Henry, and sound engineer Jacques Poullin had received official recognition and The Groupe de Recherches de Musique Concrète, Club d 'Essai de la Radiodiffusion-Télévision Française was established at RTF in Paris, the ancestor of the ORTF . At RTF the GRMC established the first purpose-built electroacoustic music studio. It quickly attracted many who either were or were later to become notable composers, including Olivier Messiaen, Pierre Boulez, Jean Barraqué, Karlheinz Stockhausen, Edgard Varèse, Iannis Xenakis, Michel Philippot, and Arthur Honegger. Compositional output from 1951 to 1953 comprised "Étude I" (1951) and "Étude II" (1951) by Boulez, "Timbres-durées" (1952) by Messiaen, "Konkrete Etüde" (1952) by Stockhausen, "Le microphone bien tempéré" (1952) and "La voile d'Orphée" (1953) by Henry, "Étude I" (1953) by Philippot, "Étude" (1953) by Barraqué, the mixed pieces "Toute la lyre" (1951) and "Orphée 53" (1953) by Schaeffer/Henry, and the film music "Masquerage" (1952) by Schaeffer and "Astrologie" (1953) by Henry. In 1954 Varèse and Honegger visited to work on the tape parts of "Déserts" and "La rivière endormie" .

In the early and mid 1950s Schaeffer's commitments to RTF included official missions which often required extended absences from the studios. This led him to invest Philippe Arthuys with responsibility for the GRMC in his absence, with Pierre Henry operating as Director of Works. Pierre Henry's composing talent developed greatly during this period at the GRMC and he worked with experimental filmmakers such as Max de Haas, Jean Grémillon, Enrico Fulchignoni, and Jean Rouch, and with choreographers including Dick Sanders and Maurice Béjart . Schaeffer returned to run the group at the end of 1957, and immediately stated his disapproval of the direction the GRMC had taken. A proposal was then made to "renew completely the spirit, the methods and the personnel of the Group, with a view to undertake research and to offer a much needed welcome to young composers" .

Following the emergence of differences within the GRMC Pierre Henry, Philippe Arthuys, and several of their colleagues, resigned in April 1958. Schaeffer created a new collective, called Groupe de Recherches Musicales (GRM) and set about recruiting new members including Luc Ferrari, Beatriz Ferreyra, François-Bernard Mâche, Iannis Xenakis, Bernard Parmegiani, and Mireille Chamass-Kyrou. Later arrivals included Ivo Malec, Philippe Carson, Romuald Vandelle, Edgardo Canton and François Bayle .

GRM was one of several theoretical and experimental groups working under the umbrella of the Schaeffer-led Service de la Recherche at ORTF (1960–74). Together with the GRM, three other groups existed: the Groupe de Recherches Image GRI, the Groupe de Recherches Technologiques GRT and the Groupe de Recherches which became the Groupe d'Etudes Critiques . Communication was the one theme that unified the various groups, all of which were devoted to production and creation. In terms of the question "who says what to whom?" Schaeffer added "how?", thereby creating a platform for research into audiovisual communication and mass media, audible phenomena and music in general (including non-Western musics) (Beatriz Ferreyra, new preface to Schaeffer and Reibel 1967, reedition of 1998, 9). At the GRM the theoretical teaching remained based on practice and could be summed up in the catch phrase "do and listen" .

Schaeffer kept up a practice established with the GRMC of delegating the functions (though not the title) of Group Director to colleagues. Since 1961 GRM has had six Group Directors: Michel Philippot (1960–61), Luc Ferrari (1962–63), Bernard Baschet and François Vercken (1964–66). From the beginning of 1966, François Bayle took over the direction for the duration of thirty-one years, to 1997. He was then replaced by Daniel Teruggi .

The group continued to refine Schaeffer's ideas and strengthened the concept of "musique acousmatique" . Schaeffer had borrowed the term acousmatic from Pythagoras and defined it as: ""Acousmatic, adjective: referring to a sound that one hears without seeing the causes behind it"" . In 1966 Schaeffer published the book "Traité des objets musicaux" (Treatise on Musical Objects) which represented the culmination of some 20 years of research in the field of "musique concrète". In conjunction with this publication, a set of sound recordings was produced, entitled "Le solfège de l'objet sonore" (Music Theory of the Acoustic Object), to provide examples of concepts dealt with in the treatise.

The development of musique concrète was facilitated by the emergence of new music technology in post-war Europe. Access to microphones, phonographs, and later magnetic tape recorders (created in 1939 and acquired by the Schaeffer's Groupe de Recherche de Musique Concrète (Research Group on Concrete Music) in 1952), facilitated by an association with the French national broadcasting organization, at that time the Radiodiffusion-Télévision Française, gave Schaeffer and his colleagues an opportunity to experiment with recording technology and tape manipulation.

In 1948, a typical radio studio consisted of a series of shellac record players, a shellac record recorder, a mixing desk with rotating potentiometers, mechanical reverberation, filters, and microphones. This technology made a number of limited operations available to a composer (, ):

The application of the above technologies in the creation of musique concrète led to the development of a number of sound manipulation techniques including (, ):


The first tape recorders started arriving at ORTF in 1949; however, their functioning was much less reliable than the shellac players, to the point that the "Symphonie pour un homme seul", which was composed in 1950–51, was mainly composed with records, even if the tape recorder was available . In 1950, when the machines finally functioned correctly, the techniques of musique concrète were expanded. A range of new sound manipulation practices were explored using improved media manipulation methods and operations such as speed variation. A completely new possibility of organising sounds appears with tape editing, which permits tape to be spliced and arranged with an extraordinary new precision. The "axe-cut junctions" were replaced with micrometric junctions and a whole new technique of production, less dependency on performance skills, could be developed. Tape editing brought a new technique called "micro-editing", in which very tiny fragments of sound, representing milliseconds of time, were edited together, thus creating completely new sounds or structures .

During the GRMC period from 1951–1958 time Schaeffer and Jacques Poullin developed a number of novel sound creation tools including a three-track tape recorder, a machine with ten playback heads to replay tape loops in echo (the morphophone), a keyboard-controlled machine to replay tape loops at twenty-four preset speeds (the keyboard, chromatic, or Tolana phonogène), a slide-controlled machine to replay tape loops at a continuously variable range of speeds (the handle, continuous, or Sareg phonogène), and a device to distribute an encoded track across four loudspeakers, including one hanging from the centre of the ceiling (the potentiomètre d'espace) .

Speed variation was a powerful tool for sound design applications. It had been identified that transformations brought about by varying playback speed lead to modification in the character of the sound material:
The phonogène was a machine capable of modifying sound structure significantly and it provided composers with a means to adapt sound to meet specific compositional contexts. The initial phonogènes were manufactured in 1953 by two subcontractors: the chromatic phonogène by a company called Tolana, and the sliding version by the SAREG Company . A third version was developed later at ORTF. An outline of the unique capabilities of the various phonogènes can be seen here:


This original tape recorder was one of the first machines permitting the simultaneous listening of several synchronised sources. Until 1958 musique concrète, radio and the studio machines were monophonic. The three-head tape recorder superposed three magnetic tapes that were dragged by a common motor, each tape having an independent spool. The objective was to keep the three tapes synchronised from a common starting point. Works could then be conceived polyphonically, and thus each head conveyed a part of the information and was listened to through a dedicated loudspeaker. It was an ancestor of the multi-track player (four then eight tracks) that appeared in the 1960s. "Timbres Durées" by Olivier Messiaen with the technical assistance of Pierre Henry was the first work composed for this tape recorder in 1952. A rapid rhythmic polyphony was distributed over the three channels .
This machine was conceived to build complex forms through repetition, and accumulation of events through delays, filtering and feedback. It consisted of a large rotating disk, 50 cm in diameter, on which was stuck a tape with its magnetic side facing outward. A series of twelve movable magnetic heads (one each recording head and erasing head, and ten playback heads) were positioned around the disk, in contact with the tape. A sound up to four seconds long could be recorded on the looped tape and the ten playback heads would then read the information with different delays, according to their (adjustable) positions around the disk. A separate amplifier and band-pass filter for each head could modify the spectrum of the sound, and additional feedback loops could transmit the information to the recording head. The resulting repetitions of a sound occurred at different time intervals, and could be filtered or modified through feedback. This system was also easily capable of producing artificial reverberation or continuous sounds .

At the premiere of Pierre Schaeffer's "Symphonie pour un homme seul" in 1951, a system that was designed for the spatial control of sound was tested. It was called a "relief desk" ("pupitre de relief", but also referred to as "pupitre d'espace" or "potentiomètre d'espace") and was intended to control the dynamic level of music played from several shellac players. This created a stereophonic effect by controlling the positioning of a monophonic sound source . One of five tracks, provided by a purpose-built tape machine, was controlled by the performer and the other four tracks each supplied a single loudspeaker. This provided a mixture of live and preset sound positions . The placement of loudspeakers in the performance space included two loudspeakers at the front right and left of the audience, one placed at the rear, and in the centre of the space a loudspeaker was placed in a high position above the audience. The sounds could therefore be moved around the audience, rather than just across the front stage. On stage, the control system allowed a performer to position a sound either to the left or right, above or behind the audience, simply by moving a small, hand held transmitter coil towards or away from four somewhat larger receiver coils arranged around the performer in a manner reflecting the loudspeaker positions . A contemporary eyewitness described the "potentiomètre d'espace" in normal use:

One found one's self sitting in a small studio which was equipped with four loudspeakers—two in front of one—right and left; one behind one and a fourth suspended above. In the front center were four large loops and an "executant" moving a small magnetic unit through the air. The four loops controlled the four speakers, and while all four were giving off sounds all the time, the distance of the unit from the loops determined the volume of sound sent out from each.<br>The music thus came to one at varying intensity from various parts of the room, and this "spatial projection" gave new sense to the rather abstract sequence of sound originally recorded. The central concept underlying this method was the notion that music should be controlled during public presentation in order to create a performance situation; an attitude that has stayed with acousmatic music to the present day .

After the longstanding rivalry with the "electronic music" of the Cologne studio had subsided, in 1970 the GRM finally created an electronic studio using tools developed by the physicist Enrico Chiarucci, called the Studio 54, which featured the "Coupigny modular synthesiser" and a Moog synthesiser . The Coupigny synthesiser, named for its designer François Coupigny, director of the Group for Technical Research (Battier 2007, 200), and the Studio 54 mixing desk had a major influence on the evolution of GRM and from the point of their introduction on they brought a new quality to the music . The mixing desk and synthesiser were combined in one unit and were created specifically for the creation of musique concrète.

The design of the desk was influenced by trade union rules at French National Radio that required technicians and production staff to have clearly defined duties. The solitary practice of musique concrète composition did not suit a system that involved three operators: one in charge of the machines, a second controlling the mixing desk, and third to provide guidance to the others. Because of this the synthesiser and desk were combined and organised in a manner that allowed it to be used easily by a composer. Independently of the mixing tracks (twenty-four in total), it had a coupled connection patch that permitted the organisation of the machines within the studio. It also had a number of remote controls for operating tape recorders. The system was easily adaptable to any context, particularly that of interfacing with external equipment .

Before the late 1960s the musique concrète produced at GRM had largely been based on the recording and manipulation of sounds, but synthesised sounds had featured in a number of works prior to the introduction of the Coupigny. Pierre Henry had used oscillators to produce sounds as early as 1955. But a synthesiser with parametrical control was something Pierre Schaeffer was against, since it favoured the preconception of music and therefore deviated from Schaeffer's principal of "making through listening" . Because of Schaeffer's concerns the Coupigny synthesiser was conceived as a sound-event generator with parameters controlled globally, without a means to define values as precisely as some other synthesisers of the day .

The development of the machine was constrained by several factors. It needed to be modular and the modules had to be easily interconnected (so that the synthesiser would have more modules than slots and it would have an easy-to-use patch). It also needed to include all the major functions of a modular synthesiser including oscillators, noise-generators, filters, ring-modulators, but an intermodulation facility was viewed as the primary requirement; to enable complex synthesis processes such as frequency modulation, amplitude modulation, and modulation via an external source. No keyboard was attached to the synthesiser and instead a specific and somewhat complex envelope generator was used to shape sound. This synthesiser was well-adapted to the production of continuous and complex sounds using intermodulation techniques such as cross-synthesis and frequency modulation but was less effective in generating precisely defined frequencies and triggering specific sounds .

The Coupigny synthesiser also served as the model for a smaller, portable unit, which has been used down to the present day .

In 1966 composer and technician François Bayle was placed in charge of the Groupe de Recherches Musicales and in 1975, GRM was integrated with the new Institut national de l'audiovisuel (INA – Audiovisual National Institute) with Bayle as its head. In taking the lead on work that began in the early 1950s, with Jacques Poullin's potentiomètre d'espace, a system designed to move monophonic sound sources across four speakers, Bayle and the engineer Jean-Claude Lallemand created an orchestra of loudspeakers ("un orchestre de haut-parleurs") known as the Acousmonium in 1974 . An inaugural concert took place on 14 February 1974 at the Espace Pierre Cardin in Paris with a presentation of Bayle's "Expérience acoustique" .

The Acousmonium is a specialised sound reinforcement system consisting of between 50 and 100 loudspeakers, depending on the character of the concert, of varying shape and size. The system was designed specifically for the concert presentation of musique-concrète-based works but with the added enhancement of sound spatialisation. Loudspeakers are placed both on stage and at positions throughout the performance space and a mixing console is used to manipulate the placement of acousmatic material across the speaker array, using a performative technique known as "sound diffusion" . Bayle has commented that the purpose of the Acousmonium is to ""substitute a momentary classical disposition of sound making, which diffuses the sound from the circumference towards the centre of the hall, by a group of sound projectors which form an 'orchestration' of the acoustic image"" .

As of 2010, the Acousmonium was still performing, with 64 speakers, 35 amplifiers, and 2 consoles .






</doc>
<doc id="20018" url="https://en.wikipedia.org/wiki?curid=20018" title="Metric space">
Metric space

In mathematics, a metric space is a set for which distances between all members of the set are defined. Those distances, taken together, are called a metric on the set. A metric on a space induces topological properties like open and closed sets, which lead to the study of more abstract topological spaces.

The most familiar metric space is 3-dimensional Euclidean space. In fact, a "metric" is the generalization of the Euclidean metric arising from the four long-known properties of the Euclidean distance. The Euclidean metric defines the distance between two points as the length of the straight line segment connecting them. Other metric spaces occur for example in elliptic geometry and hyperbolic geometry, where distance on a sphere measured by angle is a metric, and the hyperboloid model of hyperbolic geometry is used by special relativity as a metric space of velocities.

In 1906 Maurice Fréchet introduced metric spaces in his work "Sur quelques points du calcul fonctionnel". However the name is due to Felix Hausdorff.

A metric space is an ordered pair formula_1 where formula_2 is a set and formula_3 is a metric on formula_2, i.e., a function

such that for any formula_6, the following holds:

The first condition follows from the other three. Since for any formula_7:

The function formula_3 is also called "distance function" or simply "distance". Often, formula_3 is omitted and one just writes formula_2 for a metric space if it is clear from the context what metric is used.

Ignoring mathematical details, for any system of roads and terrains the distance between two locations can be defined as the length of the shortest route connecting those locations. To be a metric there shouldn't be any one-way roads. The triangle inequality expresses the fact that detours aren't shortcuts. If the distance between two points is zero, the two points are indistinguishable from one-another. Many of the examples below can be seen as concrete versions of this general idea.


Every metric space is a topological space in a natural manner, and therefore all definitions and theorems about general topological spaces also apply to all metric spaces.

About any point formula_14 in a metric space formula_2 we define the open ball of radius formula_77 (where formula_78 is a real number) about formula_14 as the set
These open balls form the base for a topology on "M", making it a topological space.

Explicitly, a subset formula_81 of formula_2 is called open if for every formula_14 in formula_81 there exists an formula_77 such that formula_86 is contained in formula_81. The complement of an open set is called closed. A neighborhood of the point formula_14 is any subset of formula_2 that contains an open ball about formula_14 as a subset.

A topological space which can arise in this way from a metric space is called a metrizable space; see the article on metrization theorems for further details.

A sequence (formula_91) in a metric space formula_2 is said to converge to the limit formula_93 iff for every formula_94, there exists a natural number "N" such that formula_95 for all formula_96. Equivalently, one can use the general definition of convergence available in all topological spaces.

A subset formula_68 of the metric space formula_2 is closed iff every sequence in formula_68 that converges to a limit in formula_2 has its limit in formula_68.

A metric space formula_2 is said to be complete if every Cauchy sequence converges in formula_2. That is to say: if formula_104 as both formula_73 and formula_72 independently go to infinity, then there is some formula_107 with formula_108.

Every Euclidean space is complete, as is every closed subset of a complete space. The rational numbers, using the absolute value metric formula_109, are not complete.

Every metric space has a unique (up to isometry) completion, which is a complete space that contains the given space as a dense subset. For example, the real numbers are the completion of the rationals.

If formula_30 is a complete subset of the metric space formula_2, then formula_30 is closed in formula_2. Indeed, a space is complete iff it is closed in any containing metric space.

Every complete metric space is a Baire space.

A metric space "M" is called bounded if there exists some number "r", such that "d"("x","y") ≤ "r" for all "x" and "y" in "M". The smallest possible such "r" is called the diameter of "M". The space "M" is called precompact or totally bounded if for every "r" > 0 there exist finitely many open balls of radius "r" whose union covers "M". Since the set of the centres of these balls is finite, it has finite diameter, from which it follows (using the triangle inequality) that every totally bounded space is bounded. The converse does not hold, since any infinite set can be given the discrete metric (one of the examples above) under which it is bounded and yet not totally bounded.

Note that in the context of intervals in the space of real numbers and occasionally regions in a Euclidean space formula_114 a bounded set is referred to as "a finite interval" or "finite region". However boundedness should not in general be confused with "finite", which refers to the number of elements, not to how far the set extends; finiteness implies boundedness, but not conversely. Also note that an unbounded subset of formula_114 may have a finite volume.

A metric space "M" is compact if every sequence in "M" has a subsequence that converges to a point in "M". This is known as sequential compactness and, in metric spaces (but not in general topological spaces), is equivalent to the topological notions of countable compactness and compactness defined via open covers.

Examples of compact metric spaces include the closed interval [0,1] with the absolute value metric, all metric spaces with finitely many points, and the Cantor set. Every closed subset of a compact space is itself compact.

A metric space is compact iff it is complete and totally bounded. This is known as the Heine–Borel theorem. Note that compactness depends only on the topology, while boundedness depends on the metric.

Lebesgue's number lemma states that for every open cover of a compact metric space "M", there exists a "Lebesgue number" δ such that every subset of "M" of diameter < δ is contained in some member of the cover.

Every compact metric space is second countable, and is a continuous image of the Cantor set. (The latter result is due to Pavel Alexandrov and Urysohn.)

A metric space is said to be locally compact if every point has a compact neighborhood. Euclidean spaces are locally compact, but infinite-dimensional Banach spaces are not.

A space is proper if every closed ball {"y" : "d"("x","y") ≤ "r"} is compact. Proper spaces are locally compact, but the converse is not true in general.

A metric space formula_2 is connected if the only subsets that are both open and closed are the empty set and formula_2 itself.

A metric space formula_2 is path connected if for any two points formula_7 there exists a continuous map formula_120 with formula_121 and formula_122.
Every path connected space is connected, but the converse is not true in general.

There are also local versions of these definitions: locally connected spaces and locally path connected spaces.

Simply connected spaces are those that, in a certain sense, do not have "holes".

A metric space is separable space if it has a countable dense subset. Typical examples are the real numbers or any Euclidean space. For metric spaces (but not for general topological spaces) separability is equivalent to second-countability and also to the Lindelöf property.

If formula_30 is a nonempty metric space and formula_124 then formula_125 is called a "pointed metric space", and formula_126 is called a "distinguished point". Note that a pointed metric space is just a nonempty metric space with attention drawn to its distinguished point, and that any nonempty metric space can be viewed as a pointed metric space. The distinguished point is sometimes denoted formula_23 due to its similar behavior to zero in certain contexts.

Suppose ("M","d") and ("M","d") are two metric spaces.

The map "f":"M"→"M" is continuous
if it has one (and therefore all) of the following equivalent properties:

Moreover, "f" is continuous if and only if it is continuous on every compact subset of "M".

The image of every compact set under a continuous function is compact, and the image of every connected set under a continuous function is connected.

The map "ƒ" : "M" → "M" is uniformly continuous if for every "ε" > 0 there exists "δ" > 0 such that

Every uniformly continuous map "ƒ" : "M" → "M" is continuous. The converse is true if "M" is compact (Heine–Cantor theorem).

Uniformly continuous maps turn Cauchy sequences in "M" into Cauchy sequences in "M". For continuous maps this is generally wrong; for example, a continuous map
from the open interval (0,1) "onto" the real line turns some Cauchy sequences into unbounded sequences.

Given a real number "K" > 0, the map "ƒ" : "M" → "M" is "K"-Lipschitz continuous if

Every Lipschitz-continuous map is uniformly continuous, but the converse is not true in general.

If "K" < 1, then "ƒ" is called a contraction. Suppose "M" = "M" and "M" is complete. If "ƒ" is a contraction, then "ƒ" admits a unique fixed point (Banach fixed point theorem). If "M" is compact, the condition can be weakened a bit: "ƒ" admits a unique fixed point if

The map "f":"M"→"M" is an isometry if
Isometries are always injective; the image of a compact or complete set under an isometry is compact or complete, respectively. However, if the isometry is not surjective, then the image of a closed (or open) set need not be closed (or open).

The map "f" : "M" → "M" is a quasi-isometry if there exist constants "A" ≥ 1 and "B" ≥ 0 such that

and a constant "C" ≥ 0 such that every point in "M" has a distance at most "C" from some point in the image "f"("M").

Note that a quasi-isometry is not required to be continuous. Quasi-isometries compare the "large-scale structure" of metric spaces; they find use in geometric group theory in relation to the word metric.

Given two metric spaces ("M", "d") and ("M", "d"):


Metric spaces are paracompact Hausdorff spaces and hence normal (indeed they are perfectly normal). An important consequence is that every metric space admits partitions of unity and that every continuous real-valued function defined on a closed subset of a metric space can be extended to a continuous map on the whole space (Tietze extension theorem). It is also true that every real-valued Lipschitz-continuous map defined on a subset of a metric space can be extended to a Lipschitz-continuous map on the whole space.

Metric spaces are first countable since one can use balls with rational radius as a neighborhood base.

The metric topology on a metric space "M" is the coarsest topology on "M" relative to which the metric "d" is a continuous map from the product of "M" with itself to the non-negative real numbers.

A simple way to construct a function separating a point from a closed set (as required for a completely regular space) is to consider the distance between the point and the set. If ("M","d") is a metric space, "S" is a subset of "M" and "x" is a point of "M", we define the distance from "x" to "S" as

Then "d"("x", "S") = 0 if and only if "x" belongs to the closure of "S". Furthermore, we have the following generalization of the triangle inequality:
which in particular shows that the map formula_137 is continuous.

Given two subsets "S" and "T" of "M", we define their Hausdorff distance to be

In general, the Hausdorff distance "d"("S","T") can be infinite. Two sets are close to each other in the Hausdorff distance if every element of either set is close to some element of the other set.

The Hausdorff distance "d" turns the set "K"("M") of all non-empty compact subsets of "M" into a metric space. One can show that "K"("M") is complete if "M" is complete.

One can then define the Gromov–Hausdorff distance between any two metric spaces by considering the minimal Hausdorff distance of isometrically embedded versions of the two spaces. Using this distance, the class of all (isometry classes of) compact metric spaces becomes a metric space in its own right.

If formula_140 are metric spaces, and "N" is the Euclidean norm on "R", then formula_141 is a metric space, where the product metric is defined by

and the induced topology agrees with the product topology. By the equivalence of norms in finite dimensions, an equivalent metric is obtained if "N" is the taxicab norm, a p-norm, the max norm, or any other norm which is non-decreasing as the coordinates of a positive "n"-tuple increase (yielding the triangle inequality).

Similarly, a countable product of metric spaces can be obtained using the following metric

An uncountable product of metric spaces need not be metrizable. For example, formula_144 is not first-countable and thus isn't metrizable.

In the case of a single space formula_1, the distance map formula_146 (from the definition) is uniformly continuous with respect to any of the above product metrics formula_147, and in particular is continuous with respect to the product topology of formula_148.

If "M" is a metric space with metric "d", and "~" is an equivalence relation on "M", then we can endow the quotient set "M/~" with the following (pseudo)metric. Given two equivalence classes ["x"] and ["y"], we define

where the infimum is taken over all finite sequences formula_150 and formula_151 with formula_152, formula_153, formula_154. In general this will only define a pseudometric, i.e. formula_155 does not necessarily imply that formula_156. However, for nice equivalence relations (e.g., those given by gluing together polyhedra along faces), it is a metric.

The quotient metric "d" is characterized by the following universal property. If formula_157 is a metric map between metric spaces (that is, formula_158 for all "x", "y") satisfying "f"("x")="f"("y") whenever formula_159 then the induced function formula_160, given by formula_161, is a metric map formula_162

A topological space is sequential if and only if it is a quotient of a metric space.


The ordered set formula_164 can be seen as a category by requesting exactly one morphism formula_165 if formula_166 and none otherwise. By using formula_167 as the tensor product and formula_23 as the identity, it becomes a monoidal category formula_169.
Every metric space formula_1 can now be viewed as a category formula_171 enriched over formula_169:

See the paper by F.W. Lawvere listed below.


This is reprinted (with author commentary) at Reprints in Theory and Applications of Categories
Also (with an author commentary) in Enriched categories in the logic of geometry and analysis. Repr. Theory Appl. Categ. No. 1 (2002), 1–37.



</doc>
<doc id="20021" url="https://en.wikipedia.org/wiki?curid=20021" title="Marine biology">
Marine biology

Marine biology is the scientific study of marine life, organisms in the sea. Given that in biology many phyla, families and genera have some species that live in the sea and others that live on land, marine biology classifies species based on the environment rather than on taxonomy.

A large proportion of all life on Earth lives in the ocean. The exact size of this "large proportion" is unknown, since many ocean species are still to be discovered. The ocean is a complex three-dimensional world covering approximately 71% of the Earth's surface. The habitats studied in marine biology include everything from the tiny layers of surface water in which organisms and abiotic items may be trapped in surface tension between the ocean and atmosphere, to the depths of the oceanic trenches, sometimes 10,000 meters or more beneath the surface of the ocean. Specific habitats include coral reefs, kelp forests, seagrass meadows, the surrounds of seamounts and thermal vents, tidepools, muddy, sandy and rocky bottoms, and the open ocean (pelagic) zone, where solid objects are rare and the surface of the water is the only visible boundary. The organisms studied range from microscopic phytoplankton and zooplankton to huge cetaceans (whales) in length. Marine ecology is the study of how marine organisms interact with each other and the environment.

Marine life is a vast resource, providing food, medicine, and raw materials, in addition to helping to support recreation and tourism all over the world. At a fundamental level, marine life helps determine the very nature of our planet. Marine organisms contribute significantly to the oxygen cycle, and are involved in the regulation of the Earth's climate. Shorelines are in part shaped and protected by marine life, and some marine organisms even help create new land.

Many species are economically important to humans, including both finfish and shellfish. It is also becoming understood that the well-being of marine organisms and other organisms are linked in fundamental ways. The human body of knowledge regarding the relationship between life in the sea and important cycles is rapidly growing, with new discoveries being made nearly every day. These cycles include those of matter (such as the carbon cycle) and of air (such as Earth's respiration, and movement of energy through ecosystems including the ocean). Large areas beneath the ocean surface still remain effectively unexplored.

The study of marine biology dates back to Aristotle (384–322 BC), who made many observations of life in the sea around Lesbos, laying the foundation for many future discoveries. In 1768, Samuel Gottlieb Gmelin (1744–1774) published the "Historia Fucorum", the first work dedicated to marine algae and the first book on marine biology to use the then new binomial nomenclature of Linnaeus. It included elaborate illustrations of seaweed and marine algae on folded leaves. The British naturalist Edward Forbes (1815–1854) is generally regarded as the founder of the science of marine biology. The pace of oceanographic and marine biology studies quickly accelerated during the course of the 19th century.

The observations made in the first studies of marine biology fueled the age of discovery and exploration that followed. During this time, a vast amount of knowledge was gained about the life that exists in the oceans of the world. Many voyages contributed significantly to this pool of knowledge. Among the most significant were the voyages of where Charles Darwin came up with his theories of evolution and on the formation of coral reefs. Another important expedition was undertaken by HMS "Challenger", where findings were made of unexpectedly high species diversity among fauna stimulating much theorizing by population ecologists on how such varieties of life could be maintained in what was thought to be such a hostile environment. This era was important for the history of marine biology but naturalists were still limited in their studies because they lacked technology that would allow them to adequately examine species that lived in deep parts of the oceans.

The creation of marine laboratories was important because it allowed marine biologists to conduct research and process their specimens from expeditions. The oldest marine laboratory in the world, Station biologique de Roscoff, was established in France in 1872. In the United States, the Scripps Institution of Oceanography dates back to 1903, while the prominent Woods Hole Oceanographic Institute was founded in 1930. The development of technology such as sound navigation ranging, scuba diving gear, submersibles and remotely operated vehicles allowed marine biologists to discover and explore life in deep oceans that was once thought to not exist.

As inhabitants of the largest environment on Earth, microbial marine systems drive changes in every global system. Microbes are responsible for virtually all the photosynthesis that occurs in the ocean, as well as the cycling of carbon, nitrogen, phosphorus and other nutrients and trace elements.

Microscopic life undersea is incredibly diverse and still poorly understood. For example, the role of viruses in marine ecosystems is barely being explored even in the beginning of the 21st century.

The role of phytoplankton is better understood due to their critical position as the most numerous primary producers on Earth. Phytoplankton are categorized into cyanobacteria (also called blue-green algae/bacteria), various types of algae (red, green, brown, and yellow-green), diatoms, dinoflagellates, euglenoids, coccolithophorids, cryptomonads, chrysophytes, chlorophytes, prasinophytes, and silicoflagellates.

Zooplankton tend to be somewhat larger, and not all are microscopic. Many Protozoa are zooplankton, including dinoflagellates, zooflagellates, foraminiferans, and radiolarians. Some of these (such as dinoflagellates) are also phytoplankton; the distinction between plants and animals often breaks down in very small organisms. Other zooplankton include cnidarians, ctenophores, chaetognaths, molluscs, arthropods, urochordates, and annelids such as polychaetes. Many larger animals begin their life as zooplankton before they become large enough to take their familiar forms. Two examples are fish larvae and sea stars (also called starfish).

Microscopic algae and plants provide important habitats for life, sometimes acting as hiding places for larval forms of larger fish and foraging places for invertebrates.

Algal life is widespread and very diverse under the ocean. Microscopic photosynthetic algae contribute a larger proportion of the world's photosynthetic output than all the terrestrial forests combined. Most of the niche occupied by sub plants on land is actually occupied by macroscopic algae in the ocean, such as "Sargassum" and kelp, which are commonly known as seaweeds that create kelp forests.

Plants that survive in the sea are often found in shallow waters, such as the seagrasses (examples of which are eelgrass, "Zostera", and turtle grass, "Thalassia"). These plants have adapted to the high salinity of the ocean environment. The intertidal zone is also a good place to find plant life in the sea, where mangroves or cordgrass or beach grass might grow. 

As on land, invertebrates make up a huge portion of all life in the sea. Invertebrate sea life includes Cnidaria such as jellyfish and sea anemones; Ctenophora; sea worms including the phyla Platyhelminthes, Nemertea, Annelida, Sipuncula, Echiura, Chaetognatha, and Phoronida; Mollusca including shellfish, squid, octopus; Arthropoda including Chelicerata and Crustacea; Porifera; Bryozoa; Echinodermata including starfish; and Urochordata including sea squirts or tunicates. Invertebrates have no backbone. There are over a million species.

Over 1500 species of fungi are known from marine environments. These are parasitic on marine algae or animals, or are saprobes on algae, corals, protozoan cysts, sea grasses, wood and other substrata, and can also be found in sea foam. Spores of many species have special appendages which facilitate attachment to the substratum. A very diverse range of unusual secondary metabolites is produced by marine fungi.

A reported 33,400 species of fish, including bony and cartilaginous fish, had been described by 2016, more than all other vertebrates combined. About 60% of fish species live in saltwater.

Reptiles which inhabit or frequent the sea include sea turtles, sea snakes, terrapins, the marine iguana, and the saltwater crocodile. Most extant marine reptiles, except for some sea snakes, are oviparous and need to return to land to lay their eggs. Thus most species, excepting sea turtles, spend most of their lives on or near land rather than in the ocean. Despite their marine adaptations, most sea snakes prefer shallow waters nearby land, around islands, especially waters that are somewhat sheltered, as well as near estuaries. Some extinct marine reptiles, such as ichthyosaurs, evolved to be viviparous and had no requirement to return to land.

Birds adapted to living in the marine environment are often called seabirds. Examples include albatross, penguins, gannets, and auks. Although they spend most of their lives in the ocean, species such as gulls can often be found thousands of miles inland.

There are five main types of marine mammals, namely cetaceans (toothed whales and baleen whales); sirenians such as manatees; pinnipeds including seals and the walrus; sea otters; and the 
polar bear. All are air-breathing, and while some such as the sperm whale can dive for prolonged periods, all must return to the surface to breathe.

Marine habitats can be divided into coastal and open ocean habitats. Coastal habitats are found in the area that extends from the shoreline to the edge of the continental shelf. Most marine life is found in coastal habitats, even though the shelf area occupies only seven percent of the total ocean area. Open ocean habitats are found in the deep ocean beyond the edge of the continental shelf. Alternatively, marine habitats can be divided into pelagic and demersal habitats. Pelagic habitats are found near the surface or in the open water column, away from the bottom of the ocean and affected by ocean currents, while demersal habitats are near or on the bottom. Marine habitats can be modified by their inhabitants. Some marine organisms, like corals, kelp and sea grasses, are ecosystem engineers which reshape the marine environment to the point where they create further habitat for other organisms.

Intertidal zones, the areas that are close to the shore, are constantly being exposed and covered by the ocean's tides. A huge array of life can be found within this zone. Shore habitats span from the upper intertidal zones to the area where land vegetation takes prominence. It can be underwater anywhere from daily to very infrequently. Many species here are scavengers, living off of sea life that is washed up on the shore. Many land animals also make much use of the shore and intertidal habitats. A subgroup of organisms in this habitat bores and grinds exposed rock through the process of bioerosion.

Estuaries are also near shore and influenced by the tides. An estuary is a partially enclosed coastal body of water with one or more rivers or streams flowing into it and with a free connection to the open sea. Estuaries form a transition zone between freshwater river environments and saltwater maritime environments. They are subject both to marine influences—such as tides, waves, and the influx of saline water—and to riverine influences—such as flows of fresh water and sediment. The shifting flows of both sea water and fresh water provide high levels of nutrients both in the water column and in sediment, making estuaries among the most productive natural habitats in the world.

Reefs comprise some of the densest and most diverse habitats in the world. The best-known types of reefs are tropical coral reefs which exist in most tropical waters; however, reefs can also exist in cold water. Reefs are built up by corals and other calcium-depositing animals, usually on top of a rocky outcrop on the ocean floor. Reefs can also grow on other surfaces, which has made it possible to create artificial reefs. Coral reefs also support a huge community of life, including the corals themselves, their symbiotic zooxanthellae, tropical fish and many other organisms.

Much attention in marine biology is focused on coral reefs and the El Niño weather phenomenon. In 1998, coral reefs experienced the most severe mass bleaching events on record, when vast expanses of reefs across the world died because sea surface temperatures rose well above normal. Some reefs are recovering, but scientists say that between 50% and 70% of the world's coral reefs are now endangered and predict that global warming could exacerbate this trend.

The open ocean is relatively unproductive because of a lack of nutrients, yet because it is so vast, in total it produces the most primary productivity. The open ocean is separated into different zones, and the different zones each have different ecologies. Zones which vary according to their depth include the epipelagic, mesopelagic, bathypelagic, abyssopelagic, and hadopelagic zones. Zones which vary by the amount of light they receive include the photic and aphotic zones. Much of the aphotic zone's energy is supplied by the open ocean in the form of detritus.

The deepest recorded oceanic trench measured to date is the Mariana Trench, near the Philippines, in the Pacific Ocean at . At such depths, water pressure is extreme and there is no sunlight, but some life still exists. A white flatfish, a shrimp and a jellyfish were seen by the American crew of the bathyscaphe "Trieste" when it dove to the bottom in 1960. In general, the deep sea is considered to start at the aphotic zone, the point where sunlight loses its power of transference through the water. Many life forms that live at these depths have the ability to create their own light known as bio-luminescence. Marine life also flourishes around seamounts that rise from the depths, where fish and other sea life congregate to spawn and feed. Hydrothermal vents along the mid-ocean ridge spreading centers act as oases, as do their opposites, cold seeps. Such places support unique biomes and many new microbes and other lifeforms have been discovered at these locations .

The marine ecosystem is large, and thus there are many sub-fields of marine biology. Most involve studying specializations of particular animal groups, such as phycology, invertebrate zoology and ichthyology. Other subfields study the physical effects of continual immersion in sea water and the ocean in general, adaptation to a salty environment, and the effects of changing various oceanic properties on marine life. A subfield of marine biology studies the relationships between oceans and ocean life, and global warming and environmental issues (such as carbon dioxide displacement). Recent marine biotechnology has focused largely on marine biomolecules, especially proteins, that may have uses in medicine or engineering. Marine environments are the home to many exotic biological materials that may inspire biomimetic materials.

Marine biology is a branch of biology. It is closely linked to oceanography and may be regarded as a sub-field of marine science. It also encompasses many ideas from ecology. Fisheries science and marine conservation can be considered partial offshoots of marine biology (as well as environmental studies). Marine Chemistry, Physical oceanography and Atmospheric sciences are closely related to this field.

An active research topic in marine biology is to discover and map the life cycles of various species and where they spend their time. Technologies that aid in this discovery include pop-up satellite archival tags, acoustic tags, and a variety of other data loggers. Marine biologists study how the ocean currents, tides and many other oceanic factors affect ocean life forms, including their growth, distribution and well-being. This has only recently become technically feasible with advances in GPS and newer underwater visual devices.

Most ocean life breeds in specific places, nests or not in others, spends time as juveniles in still others, and in maturity in yet others. Scientists know little about where many species spend different parts of their life cycles especially in the infant and juvenile years. For example, it is still largely unknown where juvenile sea turtles and some year-1 sharks travel. Recent advances in underwater tracking devices are illuminating what we know about marine organisms that live at great Ocean depths. The information that pop-up satellite archival tags give aids in certain time of the year fishing closures and development of a marine protected area. This data is important to both scientists and fishermen because they are discovering that by restricting commercial fishing in one small area they can have a large impact in maintaining a healthy fish population in a much larger area.




</doc>
<doc id="20023" url="https://en.wikipedia.org/wiki?curid=20023" title="Microkernel">
Microkernel

In computer science, a microkernel (also known as μ-kernel) is the near-minimum amount of software that can provide the mechanisms needed to implement an operating system (OS). These mechanisms include low-level address space management, thread management, and inter-process communication (IPC).

If the hardware provides multiple rings or CPU modes, the microkernel may be the only software executing at the most privileged level, which is generally referred to as supervisor or kernel mode. Traditional operating system functions, such as device drivers, protocol stacks and file systems, are typically removed from the microkernel itself and are instead run in user space.

In terms of the source code size, as a general rule microkernels tend to be smaller than monolithic kernels. The MINIX 3 microkernel, for example, has approximately 12,000 lines of code.

Microkernels trace their roots back to Danish computer pioneer Per Brinch Hansen and his tenure in Danish computer company Regnecentralen where he led software development efforts for the RC 4000 computer.
In 1967, Regnecentralen was installing a RC 4000 prototype in a Polish fertilizer plant in Puławy. The computer used a small real-time operating system tailored for the needs of the plant. Brinch Hansen and his team became concerned with the lack of generality and reusability of the RC 4000 system. They feared that each installation would require a different operating system so they started to investigate novel and more general ways of creating software for the RC 4000.
In 1969, their effort resulted in the completion of the RC 4000 Multiprogramming System. Its nucleus provided inter-process communication based on message-passing for up to 23 unprivileged processes, out of which 8 at a time were protected from one another. It further implemented scheduling of time slices of programs executed in parallel, initiation and control of program execution at the request of other running programs, and initiation of data transfers to or from peripherals. Besides these elementary mechanisms, it had no built-in strategy for program execution and resource allocation. This strategy was to be implemented by a hierarchy of running programs in which parent processes had complete control over child processes and acted as their operating systems.

Following Brinch Hansen's work, microkernels have been developed since the 1970s The term microkernel itself appeared no later than 1981. Microkernels were meant as a response to changes in the computer world, and to several challenges adapting existing "mono-kernels" to these new systems. New device drivers, protocol stacks, file systems and other low-level systems were being developed all the time. This code was normally located in the monolithic kernel, and thus required considerable work and careful code management to work on. Microkernels were developed with the idea that all of these services would be implemented as user-space programs, like any other, allowing them to be worked on monolithically and started and stopped like any other program. This would not only allow these services to be more easily worked on, but also separated the kernel code to allow it to be finely tuned without worrying about unintended side effects. Moreover, it would allow entirely new operating systems to be "built up" on a common core, aiding OS research.

Microkernels were a very hot topic in the 1980s when the first usable local area networks were being introduced. The same mechanisms that allowed the kernel to be distributed into user space also allowed the system to be distributed across network links. The first microkernels, notably Mach, proved to have disappointing performance, but the inherent advantages appeared so great that it was a major line of research into the late 1990s. However, during this time the speed of computers grew greatly in relation to networking systems, and the disadvantages in performance came to overwhelm the advantages in development terms. Many attempts were made to adapt the existing systems to have better performance, but the overhead was always considerable and most of these efforts required the user-space programs to be moved back into the kernel. By 2000, most large-scale (Mach-like) efforts had ended, although Apple's macOS, released in 2001, uses a hybrid kernel called XNU, which combines a heavily modified (hybrid) OSFMK 7.3 kernel with code from BSD UNIX, and this kernel is also used in iOS, tvOS, and watchOS. , the Mach-based GNU Hurd is also functional and included in testing versions of Arch Linux and Debian.

Although major work on microkernels had largely ended, experimenters continued development. It has since been shown that many of the performance problems of earlier designs were not a fundamental limitation of the concept, but instead due to the designer's desire to use single-purpose systems to implement as many of these services as possible. Using a more pragmatic approach to the problem, including assembly code and relying on the processor to enforce concepts normally supported in software led to a new series of microkernels with dramatically improved performance.

Microkernels are closely related to exokernels.
They also have much in common with hypervisors,
but the latter make no claim to minimality and are specialized to supporting virtual machines; indeed, the L4 microkernel frequently finds use in a hypervisor capacity.

Early operating system kernels were rather small, partly because computer memory was limited. As the capability of computers grew, the number of devices the kernel had to control also grew. Throughout the early history of Unix, kernels were generally small, even though they contained various device drivers and file system implementations. When address spaces increased from 16 to 32 bits, kernel design was no longer constrained by the hardware architecture, and kernels began to grow larger.

The Berkeley Software Distribution (BSD) of Unix began the era of larger kernels. In addition to operating a basic system consisting of the CPU, disks and printers, BSD added a complete TCP/IP networking system and a number of "virtual" devices that allowed the existing programs to work 'invisibly' over the network. This growth continued for many years, resulting in kernels with millions of lines of source code. As a result of this growth, kernels were prone to bugs and became increasingly difficult to maintain.

The microkernel was intended to address this growth of kernels and the difficulties that resulted. In theory, the microkernel design allows for easier management of code due to its division into user space services. This also allows for increased security and stability resulting from the reduced amount of code running in kernel mode. For example, if a networking service crashed due to buffer overflow, only the networking service's memory would be corrupted, leaving the rest of the system still functional.

Inter-process communication (IPC) is any mechanism which allows separate processes to communicate with each other, usually by sending messages. Shared memory is strictly speaking also an inter-process communication mechanism, but the abbreviation IPC usually only refers to message passing, and it is the latter that is particularly relevant to microkernels. IPC allows the operating system to be built from a number of small programs called servers, which are used by other programs on the system, invoked via IPC. Most or all support for peripheral hardware is handled in this fashion, with servers for device drivers, network protocol stacks, file systems, graphics, etc.

IPC can be synchronous or asynchronous. Asynchronous IPC is analogous to network communication: the sender dispatches a message and continues executing. The receiver checks (polls) for the availability of the message by attempting a receive, or is alerted to it via some notification mechanism. Asynchronous IPC requires that the kernel maintains buffers and queues for messages, and deals with buffer overflows; it also requires double copying of messages (sender to kernel and kernel to receiver). In synchronous IPC, the first party (sender or receiver) blocks until the other party is ready to perform the IPC. It does not require buffering or multiple copies, but the implicit rendezvous can make programming tricky. Most programmers prefer asynchronous send and synchronous receive.

First-generation microkernels typically supported synchronous as well as asynchronous IPC, and suffered from poor IPC performance. Jochen Liedtke assumed the design and implementation of the IPC mechanisms to be the underlying reason for this poor performance. In his L4 microkernel he pioneered methods that lowered IPC costs by an order of magnitude. These include an IPC system call that supports a send as well as a receive operation, making all IPC synchronous, and passing as much data as possible in registers. Furthermore, Liedtke introduced the concept of the "direct process switch", where during an IPC execution an (incomplete) context switch is performed from the sender directly to the receiver. If, as in L4, part or all of the message is passed in registers, this transfers the in-register part of the message without any copying at all. Furthermore, the overhead of invoking the scheduler is avoided; this is especially beneficial in the common case where IPC is used in an RPC-type fashion by a client invoking a server. Another optimization, called "lazy scheduling", avoids traversing scheduling queues during IPC by leaving threads that block during IPC in the ready queue. Once the scheduler is invoked, it moves such threads to the appropriate waiting queue. As in many cases a thread gets unblocked before the next scheduler invocation, this approach saves significant work. Similar approaches have since been adopted by QNX and MINIX 3.

In a series of experiments, Chen and Bershad compared memory cycles per instruction (MCPI) of monolithic Ultrix with those of microkernel Mach combined with a 4.3BSD Unix server running in user space. Their results explained Mach's poorer performance by higher MCPI and demonstrated that IPC alone is not responsible for much of the system overhead, suggesting that optimizations focused exclusively on IPC will have limited impact. Liedtke later refined Chen and Bershad's results by making an observation that the bulk of the difference between Ultrix and Mach MCPI was caused by capacity cache-misses and concluding that drastically reducing the cache working set of a microkernel will solve the problem.

In a client-server system, most communication is essentially synchronous, even if using asynchronous primitives, as the typical operation is a client invoking a server and then waiting for a reply. As it also lends itself to more efficient implementation, most microkernels generally followed L4's lead and only provided a synchronous IPC primitive. Asynchronous IPC could be implemented on top by using helper threads. However, experience has shown that the utility of synchronous IPC is dubious: synchronous IPC forces a multi-threaded design onto otherwise simple systems, with the resulting synchronization complexities. Moreover, an RPC-like server invocation sequentializes client and server, which should be avoided if they are running on separate cores. Versions of L4 deployed in commercial products have therefore found it necessary to add an asynchronous notification mechanism to better support asynchronous communication. This signal-like mechanism does not carry data and therefore does not require buffering by the kernel. By having two forms of IPC, they have nonetheless violated the principle of minimality. Other versions of L4 have switched to asynchronous IPC completely.

As synchronous IPC blocks the first party until the other is ready, unrestricted use could easily lead to deadlocks. Furthermore, a client could easily mount a denial-of-service attack on a server by sending a request and never attempting to receive the reply. Therefore, synchronous IPC must provide a means to prevent indefinite blocking. Many microkernels provide timeouts on IPC calls, which limit the blocking time. In practice, choosing sensible timeout values is difficult, and systems almost inevitably use infinite timeouts for clients and zero timeouts for servers. As a consequence, the trend is towards not providing arbitrary timeouts, but only a flag which indicates that the IPC should fail immediately if the partner is not ready. This approach effectively provides a choice of the two timeout values of zero and infinity. Recent versions of L4 and MINIX have gone down this path (older versions of L4 used timeouts, as does QNX).

Microkernel servers are essentially daemon programs like any others, except that the kernel grants some of them privileges to interact with parts of physical memory that are otherwise off limits to most programs. This allows some servers, particularly device drivers, to interact directly with hardware.

A basic set of servers for a general-purpose microkernel includes file system servers, device driver servers, networking servers, display servers, and user interface device servers. This set of servers (drawn from QNX) provides roughly the set of services offered by a Unix monolithic kernel. The necessary servers are started at system startup and provide services, such as file, network, and device access, to ordinary application programs. With such servers running in the environment of a user application, server development is similar to ordinary application development, rather than the build-and-boot process needed for kernel development.

Additionally, many "crashes" can be corrected by simply stopping and restarting the server. However, part of the system state is lost with the failing server, hence this approach requires applications to cope with failure. A good example is a server responsible for TCP/IP connections: If this server is restarted, applications will experience a "lost" connection, a normal occurrence in a networked system. For other services, failure is less expected and may require changes to application code. For QNX, restart capability is offered as the QNX High Availability Toolkit.

To make all servers restartable, some microkernels have concentrated on adding various database-like methods such as transactions, replication and checkpointing to preserve essential state across single server restarts. An example is ChorusOS, which was made for high-availability applications in the telecommunications world. Chorus included features to allow any "properly written" server to be restarted at any time, with clients using those servers being paused while the server brought itself back into its original state. However, such kernel features are incompatible with the minimality principle, and are thus not provided in modern microkernels, which instead rely on appropriate user-level protocols.

Device drivers frequently perform direct memory access (DMA), and therefore can write to arbitrary locations of physical memory, including various kernel data structures. Such drivers must therefore be trusted. It is a common misconception that this means that they must be part of the kernel. In fact, a driver is not inherently more or less trustworthy by being part of the kernel.

While running a device driver in user space does not necessarily reduce the damage a misbehaving driver can cause, in practice it is beneficial for system stability in the presence of buggy (rather than malicious) drivers: memory-access violations by the driver code itself (as opposed to the device) may still be caught by the memory-management hardware. Furthermore, many devices are not DMA-capable, their drivers can be made untrusted by running them in user space. Recently, an increasing number of computers feature IOMMUs, many of which can be used to restrict a device's access to physical memory. This also allows user-mode drivers to become untrusted.

User-mode drivers actually predate microkernels. The Michigan Terminal System (MTS), in 1967, supported user space drivers (including its file system support), the first operating system to be designed with that capability.
Historically, drivers were less of a problem, as the number of devices was small and trusted anyway, so having them in the kernel simplified the design and avoided potential performance problems. This led to the traditional driver-in-the-kernel style of Unix, Linux, and Windows NT.
With the proliferation of various kinds of peripherals, the amount of driver code escalated and in modern operating systems dominates the kernel in code size.

As a microkernel must allow building arbitrary operating system services on top, it must provide some core functionality. At a minimum, this includes:

This minimal design was pioneered by Brinch Hansen's Nucleus and the hypervisor of IBM's VM. It has since been formalised in Liedtke's "minimality principle":
A concept is tolerated inside the microkernel only if moving it outside the kernel, i.e., permitting competing implementations, would prevent the implementation of the system's required functionality.
Everything else can be done in a usermode program, although device drivers implemented as user programs may on some processor architectures require special privileges to access I/O hardware.

Related to the minimality principle, and equally important for microkernel design, is the separation of mechanism and policy, it is what enables the construction of arbitrary systems on top of a minimal kernel. Any policy built into the kernel cannot be overwritten at user level and therefore limits the generality of the microkernel.
Policy implemented in user-level servers can be changed by replacing the servers (or letting the application choose between competing servers offering similar services).

For efficiency, most microkernels contain schedulers and manage timers, in violation of the minimality principle and the principle of policy-mechanism separation.

Start up (booting) of a microkernel-based system requires device drivers, which are not part of the kernel. Typically this means that they are packaged with the kernel in the boot image, and the kernel supports a bootstrap protocol that defines how the drivers are located and started; this is the traditional bootstrap procedure of L4 microkernels. Some microkernels simplify this by placing some key drivers inside the kernel (in violation of the minimality principle), LynxOS and the original Minix are examples. Some even include a file system in the kernel to simplify booting. A microkernel-based system may boot via multiboot compatible boot loader. Such systems usually load statically-linked servers to make an initial bootstrap or mount an OS image to continue bootstrapping.

A key component of a microkernel is a good IPC system and virtual-memory-manager design that allows implementing page-fault handling and swapping in usermode servers in a safe way. Since all services are performed by usermode programs, efficient means of communication between programs are essential, far more so than in monolithic kernels. The design of the IPC system makes or breaks a microkernel. To be effective, the IPC system must not only have low overhead, but also interact well with CPU scheduling.

On most mainstream processors, obtaining a service is inherently more expensive in a microkernel-based system than a monolithic system. In the monolithic system, the service is obtained by a single system call, which requires two "mode switches" (changes of the processor's ring or CPU mode). In the microkernel-based system, the service is obtained by sending an IPC message to a server, and obtaining the result in another IPC message from the server. This requires a context switch if the drivers are implemented as processes, or a function call if they are implemented as procedures. In addition, passing actual data to the server and back may incur extra copying overhead, while in a monolithic system the kernel can directly access the data in the client's buffers.

Performance is therefore a potential issue in microkernel systems. Indeed, the experience of first-generation microkernels such as Mach and ChorusOS showed that systems based on them performed very poorly. However, Jochen Liedtke showed that Mach's performance problems were the result of poor design and implementation, specifically Mach's excessive cache footprint.
Liedtke demonstrated with his own L4 microkernel that through careful design and implementation, and especially by following the minimality principle, IPC costs could be reduced by more than an order of magnitude compared to Mach. L4's IPC performance is still unbeaten across a range of architectures.

While these results demonstrate that the poor performance of systems based on first-generation microkernels is not representative for second-generation kernels such as L4, this constitutes no proof that microkernel-based systems can be built with good performance. It has been shown that a monolithic Linux server ported to L4 exhibits only a few percent overhead over native Linux.
However, such a single-server system exhibits few, if any, of the advantages microkernels are supposed to provide by structuring operating system functionality into separate servers.

A number of commercial multi-server systems exist, in particular the real-time systems QNX and Integrity. No comprehensive comparison of performance relative to monolithic systems has been published for those multiserver systems. Furthermore, performance does not seem to be the overriding concern for those commercial systems, which instead emphasize reliably quick interrupt handling response times (QNX) and simplicity for the sake of robustness. An attempt to build a high-performance multiserver operating system was the IBM Sawmill Linux project.
However, this project was never completed.

It has been shown in the meantime that user-level device drivers can come close to the performance of in-kernel drivers even for such high-throughput, high-interrupt devices as Gigabit Ethernet. This seems to imply that high-performance multi-server systems are possible.

The security benefits of microkernels have been frequently discussed. In the context of security the minimality principle of microkernels is, some have argued, a direct consequence of the principle of least privilege, according to which all code should have only the privileges needed to provide required functionality. Minimality requires that a system's trusted computing base (TCB) should be kept minimal. As the kernel (the code that executes in the privileged mode of the hardware) has unvetted access to any data and can thus violate its integrity or confidentiality, the kernel is always part of the TCB. Minimizing it is natural in a security-driven design.

Consequently, microkernel designs have been used for systems designed for high-security applications, including KeyKOS, EROS and military systems. In fact common criteria (CC) at the highest assurance level (Evaluation Assurance Level (EAL) 7) has an explicit requirement that the target of evaluation be "simple", an acknowledgment of the practical impossibility of establishing true trustworthiness for a complex system. Unfortunately, again, the term "simple" is misleading and ill-defined. At least the Department of Defense Trusted Computer System Evaluation Criteria introduced somewhat more precise verbiage at the B3/A1 classes:

Recent work on microkernels has been focusing on formal specifications of the kernel API, and formal proofs of the API's security properties and implementation correctness. The first example of this is a mathematical proof of the confinement mechanisms in EROS, based on a simplified model of the EROS API. More recently, a comprehensive set of machine-checked proofs has been performed of the properties of the protection model of , a version of L4.

This has led to what is referred to as "third-generation microkernels",
characterised by a security-oriented API with resource access controlled by capabilities, virtualization as a first-class concern, novel approaches to kernel resource management,
and a design goal of suitability for formal analysis, besides the usual goal of high performance. Examples are Coyotos, , Nova,
Redox and Fiasco.OC.

In the case of seL4, complete formal verification of the implementation has been achieved, i.e. a mathematical proof that the kernel's implementation is consistent with its formal specification. This provides a guarantee that the properties proved about the API actually hold for the real kernel, a degree of assurance which goes beyond even CC EAL7. It was followed by proofs of security-enforcement properties of the API, and a proof demonstrating that the executable binary code is a correct translation of the C implementation, taking the compiler out of the TCB. Taken together, these proofs establish an end-to-end proof of security properties of the kernel.

The term "nanokernel" or "picokernel" historically referred to:

There is also at least one case where the term nanokernel is used to refer not to a small kernel, but one that supports a nanosecond clock resolution.




</doc>
<doc id="20024" url="https://en.wikipedia.org/wiki?curid=20024" title="Mach">
Mach

Mach may refer to:









</doc>
<doc id="20025" url="https://en.wikipedia.org/wiki?curid=20025" title="Multihull">
Multihull

A multihull is a ship, vessel, craft or boat with more than one hull. Multihull ships (vessels, craft, boats) include multiple types, sizes and applications. Hulls range from two to five, of uniform or diverse types and arrangements

Catamarans are the most common type, with two symmetric hulls. They are used as racing, sailing, tourist and fishing boats. About 70% of fast passenger and car-passenger ferries are catamarans. About 300 semi-submersible drilling and auxiliary platforms operate at sea.

Some ships with outriggers are built, including the experimental ship "Tritone" (UK) and the first and second sister-ships of the series of Littoral Combat Ships (US). About 70 small waterplan area ships whose hulls have a smaller cross section at the waterplane than below the surface exist.

Multihull ships can be classified by the number of hulls, by their arrangement and by their shapes and sizes.

The first multihull vessels were Austronesian canoes. They hollowed out logs to make canoes and stabilized them by attaching outriggers to prevent them from capsizing. This led to the proa, catamaran, and trimaran plus various outriggers throughout the Pacific.

Individual hulls are connected by an above-water structure called the "platform", bridgedeck, crossarms, wingdeck, or by akas. The structure can be watertight partially or fully, or can consist of separate connections.

The distance between hulls is called the "transverse clearance" and can be measured between center planes or between the inner boards.Overall beam is the width of the vessel. LOA is length over all. The distance between the design waterplane and the bottom of the above-water platform ("wet deck") is called the "vertical clearance", wingdeck height, or wingdeck clearance. ..

Outriggers or amas are outboard hulls for stability. . 

An outrigger/aka is a small side hull attached to the main load-carrying hull by two or more struts/akas.

"Proas" have one hull and one outrigger.

A ship with one hull of conventional shape and two small side hulls (outriggers) is called an "outrigger ship". Smaller vessels-less that 100 feet LOA with three hulls are "trimaran"s in English language publications and two hulled vessels are catamarans.

Three terms from the Malay and Micronesian language group describe hull components. The terms originated as descriptions of the proa. "Catamarans and trimarans share the same terminology."

Semantically, the catamaran is a pair of "Vaka" held together by "Aka", whereas the trimaran is a central "Vaka", with "Ama" on each side, attached by "Aka".

Sometimes, the term "catamaran" is applied to any ship or boat consisting of two hulls. However, other twin-hull types include" duplus" and "trisec"

The history of commercial catamarans began in 17th century England. Separate attempts at steam-powered catamarans were carried out by the middle of the 20th century. However, success required better materials and more developed hydrodynamic technologies. During the second half of the 20th century catamaran designs flourished.

The term "trimaran" is often used for any triple-hull ships or boats. More precisely, the term "trimaran" is used for a ship or boat with three identical hulls of traditional shape. The other triple-hull ships are "outrigger "and "tricore".

The trimaran has the widest range of interactions of wave systems generated by hulls at speed. The interactions can be favorable or unfavorable, depending on relative hull arrangement and speed. No authentic trimarans exist. Model test results and corresponding simulations provide estimates on the power of the full-scale ships. The calculations show possible advantages in a defined band of relative speeds.

A new type of super-fast vessel, the "wave-piercing" trimaran (WPT) is known as an air-born unloaded (up to 25% of displacement) vessel, that can achieve twice the speed with a relative power.

The term "quadrimaran" is used for four-hulled vessels.

The term "pentamaran" is used for five-hulled vessels. The M80 Stiletto is a pentamaran.

Hulls with beam designs that are narrower at the water surface (waterplane) than below can be classified as hulls with decreased or small waterplane area. More often the term "small waterplane area hull" means a hull with an underwater "gondola" and strut(s) that connect the gondola to the above-water platform. Any ship employing such hulls qualifies.
Any twin-hull SWA ship is called a "small waterplane area twin hull" ("SWATH)". A SWATH with one long strut on each hull is called a "duplus" (named after the first drilling ship of this type). The duplus is the most common type of SWA ships built.

A "trisec" is a SWATH ship with two struts on each gondola. A trisec can have a minimal waterplane area and minimal motions in waves, resulting in more effective motion control.

A triple-hull SWA ship is called a "tricore", regardless of the number of struts. The term applies to ship with three identical SWA hulls. There are no built tricores, but towing tests of models show the possibility of sufficient advantage from a power point of view in defining band of the relative speeds.

Outrigger ships can employ SWA main hull and/or outriggers .

Many traits differentiate multihulls from monohulls along several axes.

Multihulls have a broader variety of hull and payload geometries. They have a relatively large beam, deck area (upper and inner), above-water capacity, shallower draft (allowing operation in shallower water) and generally less load/payload capacity..

The Austronesians discovered that round logs tied together into a raft don't roll, or capsize as easily as a single log. Hollowing out the logs increased buoyancy (increasing payload) while preserving stability. However, this requires a lot of work and it has increased drag and weight.

Separating two logs by a pair of cross-members called akas (crossarm) achieved the increased stability at lower weight and less effort. Covering the intervening distance with a platform provides stability similar to a raft. Multihulls are midway between pontoon boats and rafts, design wise.

Multihulls feature reduced roll and yaw (equivalent pitch motion) with transverse stability (depends on transverse clearance) comparable to or greater than longitudinal stability. This reduced motion reduces seasickness and allows for more efficient solar energy collection and radar operation. They offer more effective motion mitigation systems (SWA ships), reduced wave resistance and towing resistance by controlling hull aspect ratio (for twin-hulls) and optimizing the interference between each hull's wave systems. .

The inertia of a (heavier) monohull will drive it temporarily if the wind drops, while a (lighter) multihull has less inertia, it can still maneuver well and coast for a time. Monohulls can push through waves that a multihull passes over and some lighter multihulls "ghost" well under sail as they respond easily to light air. Multihulls are more prone towards "hobby horsing" especially when lightly loaded and of short overall length.

Other cultures stabilized their watercraft by filling the bottom with rocks and other ballast. The practice can be traced back to the Romans, Phoenicians, Vikings and others. Modern ocean liners carry tons of ballast. Naval architects insure that the center of gravity of their designs remains substantially below the metacenter. The low centre of gravity acts as a counterweight as the craft rotates around its centre of buoyancy, creating a restorative force as the craft deviates from its vertical position.

Multihulls feature greater seaworthiness vs monohulls with the same displacement. Most production multihulls are officially rated as unsinkable. Watertight above-water platforms with sections protected by water-tight bulkheads can prevent sinking if the hulls fail. Catamarans may have increased reliability because the engines are on/in separate hulls. However, capsized monohulls may right themselves, pulled by the ballast while capsized multihulls remain inverted. Multihulls with their reduced weight and shallow draft make them unsuitable for breaking ice.

Multihulls are substantially faster than monohulls of comparable size, in part because of their reduced weight, reduced draft , lower beam to length ratio, and finer entries. Monohulls/multihulls can be designed to give very low wake at some speeds.

Multihulls wider beam requires a wider area for construction and dry dock and a wider haul-out ramp.

Common multihull sailboats and small craft include proas, catamarans and trimarans.

The added space and stability are valued amenities for small boat users but lack of attention to loading places the vessel deeper in the water if overloaded, can be more sluggish to respond to directional change, and possibly dangerous in a seaway.

Multihull powerboats, usually catamarans are used for racing and transportation. Speed and open cabin space are the main factors for choosing a cruising multihull. 

Multihulls are popular for racing, especially in Europe, New Zealand and Australia, and are somewhat popular for cruising in the Caribbean and South Pacific. They appear less frequently in the United States, partially because their increased beam require wider dock/slips. Multihulls are very popular in the trailerable size for competition. Until the 1980s most multihull sailboats (except for beach cats) were built either by their owners or by boat builders. Since then companies have been selling mass-produced boats.

Small sailing catamarans are also called beach catamarans. The Malibu Outrigger is one of the first beach launched multihull sailboat (1950). The most recognized racing classes are the Hobie Cat 16, Formula 18 cats, A-cats, the current Olympic Nacra 17, the former Olympic multihull Tornado and New Zealand's Weta trimaran.

Power catamarans are becoming more common in Caribbean and Mediterranean international charter fleets.

Mega or super catamarans are those over 60 feet in length. These often receive substantial customization following the request of the owner. One builder is New Zealand's Pachoud Yachts.

Builders include Corsair Marine (mid-sized trimarans) and Privilege (large catamarans). The Seawind, Perry and Lightwave. The largest manufacturer of large multihulls is Fontaine-Pajot in France.

Powerboats range from small single pilot Formula 1s to large multi-engined or gas turbined power boats that are used in off-shore racing and employ 2 to 4 pilots.

Pioneers of multihull design include James Wharram (UK), Derek Kelsall (UK), Lock Crowther (Aust), Hedly Nicol (Aust), Malcolm Tennant (NZ), Jim Brown (USA), Arthur Piver (USA), Chris White (US), Ian Farrier (NZ) and LOMOcean (NZ).

In 1978, 101 years after catamarans like "Amaryllis" were banned from yacht racing they returned the sport. This started with the victory of the trimaran "Olympus Photo", skippered by Mike Birch in the first Route du Rhum. Thereafter, no open ocean race was won by a monohull. Winning times dropped by 70%, since 1978. Olympus Photo's 23-day 6 hr 58' 35" success dropped to Gitana 11's 7d 17h 19'6", in 2006. Around 2016 the first large wind driven foil-borne racing catamarans were built. They are only on foils and T foiled rudders at higher speeds.




</doc>
<doc id="20029" url="https://en.wikipedia.org/wiki?curid=20029" title="Multics Relational Data Store">
Multics Relational Data Store

The Multics Relational Data Store, or MRDS for short, was the first commercial relational database management system. It was written in PL/1 by Honeywell for the Multics operating system and first sold in June 1976. Unlike the SQL systems that emerged in the late 1970s and early 80's, MRDS used a command language only for basic data manipulation, equivalent to the codice_1 or codice_2 statements in SQL. Other operations, like creating a new database, or general file management, required the use of a separate command program.



</doc>
<doc id="20032" url="https://en.wikipedia.org/wiki?curid=20032" title="Mike Oldfield">
Mike Oldfield

Michael Gordon Oldfield (born 15 May 1953) is an English musician and composer. His work blends progressive rock with world, folk, classical, electronic, ambient, and new-age music. His biggest commercial success is the 1973 album "Tubular Bells"which launched Virgin Records and became a hit in America after its opening was used as the theme for the film "The Exorcist". He recorded the 1983 hit single "Moonlight Shadow" and a rendition of the Christmas piece "In Dulci Jubilo".

Oldfield has released more than 20 albums with the most recent being a sequel to his 1975 album "Ommadawn" titled "Return to Ommadawn", released on 20 January 2017.

Oldfield's parents were Raymond Oldfield, a general practitioner, and Maureen Liston, an Irish nurse. His older sister Sally and older brother Terry are also successful musicians and have appeared on several of Mike's albums. He also had a younger brother, David, who had Down syndrome and who died in infancy.

Oldfield was born in the Battle Hospital in Reading and attended St Joseph's Convent School, Highlands Junior School, St Edward's Preparatory School, and Presentation College in Reading. The family lived in Western Elms Avenue, Reading. When he was 13, he moved with his parents to Harold Wood in Essex and attended Hornchurch Grammar School, where, having already begun his career in music, he took one GCE examination, in English.

Having taught himself to play the guitar, Oldfield's career began in his early teenage years, playing acoustic guitar in local folk clubs in Reading. At this time, he had already written two 15-minute instrumental pieces in which he would "go through all sorts of moods", precursors to his landmark 1970s compositions. In his early teens, Oldfield was involved in a beat group playing The Shadows-style music (he has often cited Hank Marvin as a major influence, and would later cover The Shadows' song "Wonderful Land"). In 1967, Oldfield and his sister Sally formed the folk duo The Sallyangie and, after exposure in the local folk scene, were signed to Transatlantic Records. An album, "Children of the Sun", was issued in 1968. After The Sallyangie disbanded, Mike formed another duo called Barefoot with his brother, which took him back to rock music.

In 1970, Oldfield joined The Whole Worldformer Soft Machine vocalist Kevin Ayers's backing groupplaying bass and occasionally lead guitar. He and Ayers shared a flat for a time at the northern end of the Seven Sisters Road in London. Oldfield is featured on two Ayers albums, "Whatevershebringswesing" and "Shooting at the Moon". The band also included keyboardist and composer David Bedford, who quickly befriended Oldfield, encouraged him in his composition of an early version of "Tubular Bells" and later arranged and conducted an orchestral version of the "Tubular Bells" album. Oldfield was also the reserve guitarist for the musical "Hair" and played with Alex Harvey.

Having recorded sections of this early version of "Tubular Bells" as demo pieces, Oldfield attempted to persuade record labels to take on the "Tubular Bells" project. Nothing came of his efforts until September 1971, when as a session musician and bass guitarist for the Arthur Louis Band, he attended recording sessions at The Manor Studio near Kidlington, Oxfordshire, owned by a young Richard Branson and run by engineers Tom Newman and Simon Heyworth. Branson already had several business ventures and was about to start his own record label, Virgin Records, together with Simon Draper. Newman and Heyworth heard some of Oldfield's demo music and took it to Branson and Draper, who eventually gave Oldfield one week's worth of recording time at The Manor. During this week, he completed "Part One" of "Tubular Bells"; "Part Two" was compiled over a number of months.

"Tubular Bells" is Oldfield's most famous work. The instrumental composition was recorded in 1972 and released on 25 May 1973 as the inaugural album of Richard Branson and Simon Draper's label Virgin Records. Oldfield played more than twenty different instruments in the multi-layered recording, and its style moved through diverse musical genres. Its 2,630,000 UK sales puts it at No. 34 on the list of the best-selling albums in the country. The title track became a top 10 hit single in the US after the opening was used in "The Exorcist" film in 1973. It is today considered to be a forerunner of the new-age music movement.

In 1974, Oldfield played guitar on the critically acclaimed album "Rock Bottom" by Robert Wyatt.

In late 1974, his follow-up LP, "Hergest Ridge", was No. 1 in the UK for three weeks before being dethroned by "Tubular Bells". Although "Hergest Ridge" was released over a year after "Tubular Bells", it reached No. 1 first. "Tubular Bells" spent 11 weeks (10 of them consecutive) at No. 2 before its one week at the top. Like "Tubular Bells", "Hergest Ridge" is a two-movement instrumental piece, this time evoking scenes from Oldfield's Herefordshire country retreat. It was followed in 1975 by the pioneering world music piece "Ommadawn" released after the death of his mother Maureen.

In 1975, Oldfield recorded a version of the Christmas piece "In Dulci Jubilo" which charted at No. 4 in the UK.

In 1975, Oldfield received a Grammy award for Best Instrumental Composition in "Tubular Bells – Theme from "The Exorcist"".

In 1976, Oldfield and his sister joined his friend and band member Pekka Pohjola to play on his album "Mathematician's Air Display", which was released in 1977. The album was recorded and edited at Oldfield's Througham Slad Manor in Gloucestershire by Oldfield and Paul Lindsay. Oldfield's 1976 rendition of "Portsmouth" remains his best-performing single on the UK Singles Chart, reaching No. 3.

In 1978, "Incantations" introduced more diverse choral performances from Sally Oldfield, Maddy Prior, and the Queen's College Girls Choir. Around the time of "Incantations", Oldfield underwent a controversial self-assertiveness therapy course known as Exegesis, which had a significant effect on his personality, making him more confident and out-going. Possibly as a result, the formerly reclusive musician staged a major Tour of Europe to promote the album, chronicled in his live album "Exposed", much of which was recorded at the National Exhibition Centre near Birmingham.

In 1979, Oldfield's music was used as the musical score for "The Space Movie", a Virgin movie that celebrated the tenth anniversary of the Apollo 11 mission. Also in 1979, he recorded a version of the signature tune for the BBC children's television programme "Blue Peter", which was used by the show for 10 years. In 1981, Oldfield was asked to compose a piece for the Royal Wedding of Charles, Prince of Wales, and Lady Diana Spencer, titled "Royal Wedding Anthem".

The early 1980s saw Oldfield make the transition to mainstream pop music, beginning with the inclusion of shorter instrumental tracks and contemporary cover versions on "Platinum" and "QE2" (the latter named after the ocean liner). Soon afterwards, he turned to songwriting, with a string of collaborations featuring various lead vocalists alongside his characteristic searing guitar solos. The best known of these is "Moonlight Shadow", his 1983 hit with Maggie Reilly. The most successful Oldfield composition on the US pop charts during this period was Hall & Oates's cover of Oldfield's "Family Man" for their 1982 album "HO". Released as the album's third single, it hit the Top 10 during the spring of 1983 and was a hugely popular MTV music video.

Oldfield later turned to film and video, writing the score for Roland Joffé's acclaimed film "The Killing Fields" and producing substantial video footage for his album "Islands". "Islands" continued what Oldfield had been doing on the past couple of albums, with an instrumental piece on one side and rock/pop singles on the other. Of these, "Islands", sung by Bonnie Tyler and "Magic Touch", with vocals by Max Bacon (in the US version) and Glasgow vocalist Jim Price (Southside Jimmy) in the rest of the world, were the major hits. In the US "Magic Touch" reached the top 10 on the Billboard album rock charts in 1988. During the 1980s, Oldfield's then-wife, Norwegian singer Anita Hegerland, contributed vocals to many songs including "Pictures in the Dark".

"Earth Moving" was released in July 1989 and was a moderate success. The album was the first to consist solely of rock/pop songs, several of which were released as singles: "Innocent" and "Holy" in Europe, and "Hostage" in the US for album rock stations. This was a time of much friction with his record label. Virgin Records insisted that Oldfield use the title "Tubular Bells 2" for his next instrumental album. Oldfield's rebellious response was "Amarok", an hour-long work featuring rapidly changing themes, unpredictable bursts of noise and a hidden Morse code insult, stating "Fuck off RB", allegedly directed at Richard Branson. It was not a commercial success. His last album for the Virgin label was "Heaven's Open", released under the name 'Michael Oldfield'. The album, notable for being the first time Oldfield had contributed all the lead vocals himself, consisted of songs and the rapidly changing instrumental piece "Music from the Balcony". However the rift with Virgin was healed some years later. In 2013 Oldfield invited Sir Richard to preside over the opening of the new school hall at St.Andrew's International School of The Bahamas, where two of Oldfield's children were pupils. This was the occasion of the debut of "Tubular Bells for Schools", a piano solo adaptation of Oldfield's work.

The first thing Oldfield did when arriving at his new label, Warner Bros., was to write and release "Tubular Bells II", the sequel to his first record on Virgin, in what appeared to be a final insult to his former label. It was premiered at a live concert at Edinburgh Castle. He continued to embrace new musical styles, with "The Songs of Distant Earth" (based on Arthur C. Clarke's novel of the same name) exhibiting a softer new-age sound. In 1994, he also had an asteroid, 5656 Oldfield, named after him.

In 1995, Oldfield continued to embrace new musical styles by producing the Celtic-themed album "Voyager". In 1992, Oldfield met Luar na Lubre, a Galician Celtic-folk band (from A Coruña, Spain). The band's popularity grew after Oldfield covered their song "O son do ar" ("The sound of the air") on his "Voyager" album.

In 1998, Oldfield produced the third "Tubular Bells" album (also premiered at a concert, this time in Horse Guards Parade, London), drawing on the dance music scene at his then new home on the island of Ibiza. This album was inspired by themes from "Tubular Bells", but differed in lacking a clear two-part layout.

During 1999, Oldfield released two albums. The first, "Guitars", used guitars as the source for all the sounds on the album, including percussion. The second, "The Millennium Bell", consisted of pastiches of a number of styles of music that represented various historical periods over the past millennium. The work was performed live in Berlin for the city's millennium celebrations in 1999–2000.

He added to his repertoire the MusicVR project, combining his music with a virtual reality-based computer game. His first work on this project is "Tr3s Lunas" launched in 2002, a virtual game where the player can interact with a world full of new music. This project appeared as a double CD, one with the music, and the other with the game.

In 2003, Oldfield released "Tubular Bells 2003", a re-recording of the original "Tubular Bells", on CD, and DVD-Audio. This was done to "fix" many "imperfections" in the original due to the recording technologies of the early 1970s and limitations in time that he could spend in the recording studio. It celebrated the 30th anniversary of "Tubular Bells", Oldfield's 50th birthday and his marriage to Fanny in the same year. At around the same time Virgin released an SACD version containing both the original stereo album and the 1975 quadraphonic mix by Phil Newell. In the 2003 version, the original voice of the 'Master of Ceremonies' (Viv Stanshall) was replaced with the voice of John Cleese, Stanshall having died in the interim.

On 12 April 2004 Oldfield launched his next virtual reality project, "Maestro", which contains music from the "Tubular Bells 2003" album and some new chillout melodies. The games have since been made available free of charge on Tubular.net. A double album, "Light + Shade", was released on Mercury Records in 2005, with whom Mike had recently signed a three-album deal. The two discs contain music of contrasting moods, one relaxed ("Light") and the other more edgy and moody ("Shade"). Oldfield headlined the pan-European Night of the Proms tour, consisting of 21 concerts in 2006 and 2007.

His autobiography "Changeling" was published in May 2007 by Virgin Books. In March 2008 Oldfield released his first classical album, "Music of the Spheres"; Karl Jenkins assisted with the orchestration. In the first week of release the album topped the UK Classical chart and reached number 9 on the main UK Album Chart. A single "Spheres", featuring a demo version of pieces from the album, was released digitally. The album was nominated for a Classical Brit Award, the NS&I Best Album of 2009.

In 2008, when Oldfield's original 35-year deal with Virgin Records ended, the rights to "Tubular Bells" and his other Virgin releases were returned to him, and were then transferred to Mercury Records. Mercury issued a press release on 15 April 2009, noting that Oldfield's Virgin albums would be re-released, starting 8 June 2009. These releases include special features from the archives. a further seven albums have been reissued and compilation albums have been released such as "Two Sides".

In March 2010, "Music Week" reported that publishing company Stage Three Music had acquired a 50% stake in the songs of Oldfield's entire recorded output in a seven-figure deal.

In 2008, Oldfield contributed an exclusive song ("Song for Survival") to a charity album called "Songs for Survival", in support of the Survival International. Oldfield's daughter, Molly, played a large part in the project. In 2010 lyricist Don Black said in an interview with "Music Week" that he had been working with Oldfield. In 2012, Oldfield was featured on Terry Oldfield's "Journey into Space" album and on a track called "Islanders" by German producer Torsten Stenzel's York project. In 2013 Oldfield and York released a remix album titled "Tubular Beats".

At the 2012 Summer Olympics opening ceremony, Oldfield performed renditions of "Tubular Bells", "Far Above the Clouds" and "In Dulci Jubilo" during a segment about the National Health Service. This track appears on the "Isles of Wonder" album which contains music from the Danny Boyle-directed show.

In October 2013, the BBC broadcast "Tubular Bells: The Mike Oldfield Story", an hour-long appreciation of Oldfield's life and musical career, filmed on location at his home recording studio in Nassau.

Oldfield's latest rock-themed album of songs, titled "Man on the Rocks", was released on 3 March 2014 by Virgin EMI. The album was produced by Steve Lipson. The album marks a return of Oldfield to a Virgin branded label, through the merger of Mercury Records UK and Virgin Records after Universal Music's purchase of EMI. The track "Nuclear" was used for the E3 trailer of "".

Interviewed by Steve Wright in May 2015 for his BBC Radio 2 show, Oldfield said that he was currently working on a "prequel to "Tubular Bells"" which was being recorded using analogue equipment as much as possible. He suggested that the album might only be released on vinyl. The project is in its infancy and would follow his current reissue campaign. Oldfield suggested that it would be released "in a couple of years".

On 16 October 2015 Oldfield tweeted, via his official Twitter account ""I am continuing to work on ideas for "A New Ommadawn" for the last week or so to see if [...] the idea actually works."" On 8 May 2016, Oldfield announced via his Facebook group page that the new "Ommadawn" project with the tentative title of "Return to Ommadawn" is finished, and he is awaiting a release date from the record company. He also suggested that he may soon be starting work on a possible fourth "Tubular Bells" album.

Oldfield's latest album, "Return to Ommadawn" was released on 20 January 2017 and reached #4 in the UK Album Chart. On the 29 January 2017 Oldfield again hinted at a "Tubular Bells 4" album via his official Facebook fan page; he uploaded photos of new equipment and a new Fender Telecaster guitar with the caption ""New sounds for TB4!"" "The Daily Mail" also reported that Oldfield is working on a fourth "Tubular Bells" album.

Although Oldfield considers himself primarily a guitarist, he is also one of popular music's most skilled and diverse multi-instrumentalists. His 1970s recordings were characterised by a very broad variety of instrumentation predominantly played by himself, plus assorted guitar sound treatments to suggest other instrumental timbres (such as the bagpipe, mandolin, "Glorfindel" and varispeed guitars on the original "Tubular Bells").
During the 1980s Oldfield became expert in the use of digital synthesizer and sequencers (notably the Fairlight CMI) which began to dominate the sound of his recordings: from the late 1990s onwards, he became a keen user of software synthesizers. He has, however, regularly returned to projects emphasising detailed, manually played and part-acoustic instrumentation (such as 1990's "Amarok", 1996's "Voyager" and 1999's "Guitars").

Oldfield has played over forty distinct and different instruments on record, including:


While generally preferring the sound of guest vocalists, Oldfield has frequently sung both lead and backup parts for his songs and compositions. He has also contributed experimental vocal effects such as fake choirs and the notorious "Piltdown Man" impression on "Tubular Bells".

Although recognised as an extremely skilled guitarist, Oldfield is self-deprecating about his other instrumental skills, citing them as having been developed out of necessity to perform and record the music he composes. He has been particularly dismissive of his violin-playing and singing abilities.

Over the years, Oldfield has used a range of guitars. Among the more notable of these are:

Oldfield used a modified Roland GP8 effects processor in conjunction with his PRS Artist to get many of his heavily overdriven guitar sounds from the "Earth Moving" album onwards. Oldfield has also been using guitar synthesizers since the mid-1980s, using a 1980s Roland GR-300/G-808 type system, then a 1990s Roland GK2 equipped red PRS Custom 24 (sold in 2006) with a Roland VG8, and most recently a Line 6 Variax.

Oldfield has an unusual playing style, using fingers and long right-hand fingernails and different ways of creating vibrato: a "very fast side-to-side vibrato" and "violinist's vibrato". Oldfield has stated that his playing style originates from his musical roots playing folk music and the bass guitar.

Over the years, Oldfield has owned and used a vast number of synthesizers and other keyboard instruments. In the 1980s, he composed the score for the film "The Killing Fields" on a Fairlight CMI. Some examples of keyboard and synthesised instruments which Oldfield has made use of include Sequential Circuits Prophet-5s (notably on "Platinum" and "The Killing Fields"), Roland JV-1080/JV-2080 units (1990s), a Korg M1 (as seen in the "Innocent" video), a Clavia Nord Lead and Steinway pianos. In recent years, he has also made use of software synthesis products, such as Native Instruments.

Oldfield has occasionally sung himself on his records and live performances, sometimes using a vocoder as a resource. It is not unusual for him to collaborate with diverse singers and to hold auditions before deciding the most appropriate for a particular song or album. Featured lead vocalists who have collaborated with him include:

Oldfield has self-recorded and produced many of his albums, and played the majority of the featured instruments, largely at his home studios. In the 1990s and 2000s he mainly used DAWs such as Apple Logic, Avid Pro Tools and Steinberg Nuendo as recording suites. For composing classical music Oldfield has been quoted as using the software notation program Sibelius running on Apple Macintoshes. He also used the FL Studio DAW on his 2005 double album "Light + Shade". Among the mixing consoles Oldfield has owned are an AMS Neve Capricorn 33238, a Harrison Series X, and a Euphonix System 5-MC.

Oldfield and his siblings were raised as Roman Catholics, their mother's faith. In his early life, Oldfield used drugs including LSD, whose effects on his mental health he discussed in his autobiography. In the early 1990s, he underwent a course on mental health problems and subsequently set up a foundation called Tonic, which sponsored people to have counselling and therapy. The trustee was the Professor of Psychiatry at Guy's Hospital, London.

In the late 1970s, Oldfield briefly married Diana D'Aubigny (the sister of the Exegesis group leader), but this lasted just a few weeks. Oldfield has had seven children with his partners. In the early 1980s, he had three children with Sally Cooper: Molly, Dougal (who died aged 33 in May 2015) and Luke. In the late 1980s, he had two children (Greta and Noah) with Norwegian singer Anita Hegerland. In the 2000s, he married Fanny Vandekerckhove (born 1977), whom he met during his time in Ibiza; they have two sons together (Jake and Eugene). Oldfield and Fanny separated in 2013.

Oldfield is a motorcycle fan and has five bikes. These include a BMW R1200GS, a Suzuki GSX-R750, a Suzuki GSX-R1000, and a Yamaha R1. He says that some of his inspiration for composing comes from riding them. Throughout his life Oldfield has also had a passion for aircraft and building model aircraft. Since 1980, he has been a licensed pilot and has flown fixed wing aircraft (the first of which was a Beechcraft Sierra) and helicopters (including the Agusta Bell 47G, which featured on the sleeve of his cover version of the ABBA song "Arrival" as a pastiche of their album artwork). He is also interested in cars and has owned a Ferrari and a Bentley which was a gift from Richard Branson as an incentive for him to give his first live performance of "Tubular Bells". He has endorsed the Mercedes-Benz S-Class in the Mercedes UK magazine. Oldfield also considers himself to be a Trekkie (fan of the popular science fiction television series "Star Trek"). He noted in an interview in 2008 that he had two boats.

In 2007, Oldfield caused a (very) minor stir in the British press by criticising Britain for being too controlling and protective, specifically concentrating on the smoking ban which England and Wales had introduced that year. Oldfield then moved from his South Gloucestershire home to Palma de Mallorca, Spain and then to Monaco. He has lived outside the UK in the past, including in Los Angeles and Ibiza in the 1990s and, for tax reasons, Switzerland in the mid-1980s. In 2009, he moved to the Bahamas and put his home in Mallorca up for sale.

Oldfield has had more than 30 charting albums and 25 charting singles on the British charts and many more around the world.

Studio albums







</doc>
<doc id="20034" url="https://en.wikipedia.org/wiki?curid=20034" title="Mutual recursion">
Mutual recursion

In mathematics and computer science, mutual recursion is a form of recursion where two mathematical or computational objects, such as functions or data types, are defined in terms of each other. Mutual recursion is very common in functional programming and in some problem domains, such as recursive descent parsers, where the data types are naturally mutually recursive.

The most important basic example of a data type that can be defined by mutual recursion is a tree, which can be defined mutually recursively in terms of a forest (a list of trees). Symbolically:
A forest "f" consists of a list of trees, while a tree "t" consists of a pair of a value "v" and a forest "f" (its children). This definition is elegant and easy to work with abstractly (such as when proving theorems about properties of trees), as it expresses a tree in simple terms: a list of one type, and a pair of two types. Further, it matches many algorithms on trees, which consist of doing one thing with the value, and another thing with the children.

This mutually recursive definition can be converted to a singly recursive definition by inlining the definition of a forest:
A tree "t" consists of a pair of a value "v" and a list of trees (its children). This definition is more compact, but somewhat messier: a tree consists of a pair of one type and a list of another, which require disentangling to prove results about.

In Standard ML, the tree and forest data types can be mutually recursively defined as follows, allowing empty trees:
Just as algorithms on recursive data types can naturally be given by recursive functions, algorithms on mutually recursive data structures can be naturally given by mutually recursive functions. Common examples include algorithms on trees, and recursive descent parsers. As with direct recursion, tail call optimization is necessary if the recursion depth is large or unbounded, such as using mutual recursion for multitasking. Note that tail call optimization in general (when the function called is not the same as the original function, as in tail-recursive calls) may be more difficult to implement than the special case of tail-recursive call optimization, and thus efficient implementation of mutual tail recursion may be absent from languages that only optimize tail-recursive calls. In languages such as Pascal that require declaration before use, mutually recursive functions require forward declaration, as a forward reference cannot be avoided when defining them.

As with directly recursive functions, a wrapper function may be useful, with the mutually recursive functions defined as nested functions within its scope if this is supported. This is particularly useful for sharing state across a set of functions without having to pass parameters between them.

A standard example of mutual recursion, which is admittedly artificial, determines whether a non-negative number is even or odd by defining two separate functions that call each other, decrementing each time. In C:
These functions are based on the observation that the question "is 4 even?" is equivalent to "is 3 odd?", which is in turn equivalent to "is 2 even?", and so on down to 0. This example is mutual single recursion, and could easily be replaced by iteration. In this example, the mutually recursive calls are tail calls, and tail call optimization would be necessary to execute in constant stack space. In C, this would take "O"("n") stack space, unless rewritten to use jumps instead of calls.

As a more general class of examples, an algorithm on a tree can be decomposed into its behavior on a value and its behavior on children, and can be split up into two mutually recursive functions, one specifying the behavior on a tree, calling the forest function for the forest of children, and one specifying the behavior on a forest, calling the tree function for the tree in the forest. In Python:

In this case the tree function calls the forest function by single recursion, but the forest function calls the tree function by multiple recursion.

Using the Standard ML data type above, the size of a tree (number of nodes) can be computed via the following mutually recursive functions:
A more detailed example in Scheme, counting the leaves of a tree:
These examples reduce easily to a single recursive function by inlining the forest function in the tree function, which is commonly done in practice: directly recursive functions that operate on trees sequentially process the value of the node and recurse on the children within one function, rather than dividing these into two separate functions.

A more complicated example is given by recursive descent parsers, which can be naturally implemented by having one function for each production rule of a grammar, which then mutually recurse; this will in general be multiple recursion, as production rules generally combine multiple parts. This can also be done without mutual recursion, for example by still having separate functions for each production rule, but having them called by a single controller function, or by putting all the grammar in a single function.

Mutual recursion can also implement a finite-state machine, with one function for each state, and single recursion in changing state; this requires tail call optimization if the number of state changes is large or unbounded. This can be used as a simple form of cooperative multitasking. A similar approach to multitasking is to instead use coroutines which call each other, where rather than terminating by calling another routine, one coroutine yields to another but does not terminate, and then resumes execution when it is yielded back to. This allows individual coroutines to hold state, without it needing to be passed by parameters or stored in shared variables.

There are also some algorithms which naturally have two phases, such as minimax (min and max), and these can be implemented by having each phase in a separate function with mutual recursion, though they can also be combined into a single function with direct recursion.

In mathematics, the Hofstadter Female and Male sequences are an example of a pair of integer sequences defined in a mutually recursive manner.

Fractals can be computed (up to a given resolution) by recursive functions. This can sometimes be done more elegantly via mutually recursive functions; the Sierpiński curve is a good example.

Mutual recursion is very common in the functional programming style, and is often used for programs written in LISP, Scheme, ML, and similar languages. In languages such as Prolog, mutual recursion is almost unavoidable.

Some programming styles discourage mutual recursion, claiming that it can be confusing to distinguish the conditions which will return an answer from the conditions that would allow the code to run forever without producing an answer. Peter Norvig points to a design pattern which discourages the use entirely, stating:
Mutual recursion is also known as indirect recursion, by contrast with direct recursion, where a single function calls itself directly. This is simply a difference of emphasis, not a different notion: "indirect recursion" emphasises an individual function, while "mutual recursion" emphasises the set of functions, and does not single out an individual function. For example, if "f" calls itself, that is direct recursion. If instead "f" calls "g" and then "g" calls "f," which in turn calls "g" again, from the point of view of "f" alone, "f" is indirectly recursing, while from the point of view of "g" alone, "g" is indirectly recursing, while from the point of view of both, "f" and "g" are mutually recursing on each other. Similarly a set of three or more functions that call each other can be called a set of mutually recursive functions.

Mathematically, a set of mutually recursive functions are primitive recursive, which can be proven by course-of-values recursion, building a single function "F" that lists the values of the individual recursive function in order: formula_1 and rewriting the mutual recursion as a primitive recursion.

Any mutual recursion between two procedures can be converted to direct recursion by inlining the code of one procedure into the other. If there is only one site where one procedure calls the other, this is straightforward, though if there are several it can involve code duplication. In terms of the call stack, two mutually recursive procedures yield a stack ABABAB..., and inlining B into A yields the direct recursion (AB)(AB)(AB)...

Alternately, any number of procedures can be merged into a single procedure that takes as argument a variant record (or algebraic data type) representing the selection of a procedure and its arguments; the merged procedure then dispatches on its argument to execute the corresponding code and uses direct recursion to call self as appropriate. This can be seen as a limited application of defunctionalization. This translation may be useful when any of the mutually recursive procedures can be called by outside code, so there is no obvious case for inlining one procedure into the other. Such code then needs to be modified so that procedure calls are performed by bundling arguments into a variant record as described; alternately, wrapper procedures may be used for this task.





</doc>
<doc id="20036" url="https://en.wikipedia.org/wiki?curid=20036" title="Metasyntactic variable">
Metasyntactic variable

A metasyntactic variable is a specific word or set of words identified as a placeholder in computer science and specifically computer programming. These words are commonly found in source code and are intended to be modified or substituted to be applicable to the specific usage before compilation (translation to an executable). The words foo and bar are good examples as they are used in over 330 Internet Engineering Task Force Requests for Comments, which are documents explaining foundational internet technologies like HTTP (websites), TCP/IP, and email protocols.

By mathematical analogy, a metasyntactic variable is a word that is a variable for other words, just as in algebra letters are used as variables for numbers.

Metasyntactic variables are used to name entities such as variables, functions, and commands whose exact identity is unimportant and serve only to demonstrate a concept, which is useful for teaching programming.

Due to English being the foundation-language, or lingua franca, of most computer programming languages these variables are commonly seen even in programs and examples of programs written for other spoken-language audiences.

The typical names may depend however on the subculture that has developed around a given programming language.

Metasyntactic variables used commonly across all programming languages include "foobar", "foo", "bar", "baz", "qux", "quux", "quuz", "corge", "grault", "garply", "waldo", "fred", "plugh", "xyzzy", and "thud". "Wibble", "wobble", "wubble", and "flob" are also used in the UK.

A complete reference can be found in a MIT Press book titled: The Hacker's Dictionary.

spam, ham, and eggs are the principal metasyntactic variables used in the Python programming language. This is a reference to the comedy sketch "Spam" by Monty Python, the eponym of the language.

In Japanese, the words "hoge" (ほげ) and "piyo" (ぴよ) are commonly used, with other common words and variants being "fuga" (ふが), "hogera" (ほげら), and "hogehoge" (ほげほげ). Note that "-ra" is a pluralizing ending in Japanese, and reduplication is also used for pluralizing. The origin of "hoge" as a metasyntactic variable is not known, but it is believed to date to the early 1980s.

In the following example the function name foo and the variable name bar are both metasyntactic variables. Lines beginning with // are comments.

Spam, ham, and eggs are the principal metasyntactic variables used in the Python programming language. This is a reference to the famous comedy sketch, "Spam", by Monty Python, the eponym of the language.
In the following example spam, ham, and eggs are metasyntactic variables and lines beginning with # are comments.

In the following example the baz, foo, and bar are metasyntactic variables and lines beginning with # are comments.

Both the IETF RFCs and computer programming languages are rendered in plain text, making it necessary to distinguish metasyntactic variables by a naming convention, since it would not be obvious from context. 

Plain text example:

RFC 772 (cited in RFC 3092) contains for instance:

Another point reflected in the above example is the convention that a metavariable is to be uniformly substituted with the same instance in all its appearances in a given schema. This is in contrast with nonterminal symbols in formal grammars where the nonterminals on the right of a production can be substituted by different instances.

This section includes bits of code which show how metasyntactic variables are used in teaching computer programming concepts.

Function prototypes with different argument passing mechanisms:
Example showing the function overloading capabilities of the C++ language



</doc>
<doc id="20038" url="https://en.wikipedia.org/wiki?curid=20038" title="Mondegreen">
Mondegreen

A mondegreen is a mishearing or misinterpretation of a phrase as a result of near-homophony, in a way that gives it a new meaning. Mondegreens are most often created by a person listening to a poem or a song; the listener, being unable to clearly hear a lyric, substitutes words that sound similar and make some kind of sense. American writer Sylvia Wright coined the term in 1954, writing about how as a girl she had misheard the lyric "...and laid him on the green" in a Scottish ballad as "...and Lady Mondegreen".

"Mondegreen" was included in the 2000 edition of the "Random House Webster's College Dictionary", and in the "Oxford English Dictionary" in 2002. Merriam-Webster's "Collegiate Dictionary" added the word in 2008. Examples in other languages include those cited by Fyodor Dostoyevsky, in the Hebrew song "Háva Nagíla" ("Let's Be Happy"), and in Bollywood films.

A closely related category is a Hobson-Jobson, where a word from a foreign language is homophonically translated into one's own language, e.g. "cockroach" from Spanish "cucaracha". For misheard lyrics this phenomenon is called "soramimi", a Japanese term for homophonic translation of song lyrics. An unintentionally incorrect use of similar-sounding words or phrases, resulting in a changed meaning, is a malapropism. If there is a connection in meaning, it can be called an eggcorn. If a person stubbornly continues to mispronounce a word or phrase after being corrected, that person has committed a mumpsimus.

In a 1954 essay in "Harper's Magazine", Wright described how, as a young girl, she misheard the last line of the first stanza from the 17th-century ballad "The Bonnie Earl o' Moray". She wrote:

The actual fourth line is "And "laid him on the green"." Wright explained the need for a new term:
Her essay had already described the bonny Earl holding the beautiful Lady Mondegreen's hand, both bleeding profusely but faithful unto death. She disputed:

Other examples Wright suggested are:

People are more likely to notice what they expect than things not part of their everyday experiences; this is known as confirmation bias; they may mistake an unfamiliar stimulus for a familiar and more plausible version. For example, to consider a well-known mondegreen in the song "Purple Haze", one would be more likely to hear Jimi Hendrix singing that he is about to "kiss this guy" than that he is about to "kiss the sky". Similarly, if a lyric uses words or phrases that the listener is unfamiliar with, they may be misheard as using more familiar terms.

The creation of mondegreens may be driven in part by cognitive dissonance, as the listener finds it psychologically uncomfortable to listen to a song and not make out the words. Steven Connor suggests that mondegreens are the result of the brain's constant attempts to make sense of the world by making assumptions to fill in the gaps when it cannot clearly determine what it is hearing. Connor sees mondegreens as the "wrenchings of nonsense into sense".

On the other hand, Steven Pinker has observed that mondegreen mishearings tend to be "less" plausible than the original lyrics, and that once a listener has "locked in" to a particular misheard interpretation of a song's lyrics, it can remain unquestioned, even when that plausibility becomes strained. Pinker gives the example of a student "stubbornly" mishearing the chorus to "Venus" ("I'm your Venus") as "I'm your penis," and being surprised that the song was allowed on the radio. The phenomenon may, in some cases, be triggered by people hearing "what they want to hear", as in the case of the song "Louie Louie": parents heard obscenities in the Kingsmen recording where none existed.

James Gleick claims that the mondegreen is a distinctly modern phenomenon. Without the improved communication and language standardization brought about by radio, he believes there would have been no way to recognize and discuss this shared experience. Just as mondegreens transform songs based on experience, a folk song learned by repetition is often transformed over time when sung by people in a region where some of the song's references have become obscure. A classic example is "The Golden Vanity", which contains the line "As she sailed upon the lowland sea". English immigrants carried the song to Appalachia, where singers, not knowing what the term "lowland sea" refers to, transformed it over generations from "lowland" to "lonesome".

The top three mondegreens submitted regularly to mondegreen expert Jon Carroll are:


Both Creedence's John Fogerty and Hendrix eventually acknowledged these mishearings by deliberately singing the "mondegreen" versions of their songs in concert.

The national anthem of the United States is highly susceptible (especially for young grade-school students) to the creation of mondegreens, two in the first line. Francis Scott Key's "Star-Spangled Banner" begins with the line: "O say can you see, by the dawn's early light." This has been accidentally and deliberately misinterpreted as "Jose, can you see", another example of the Hobson-Jobson effect, countless times. The second half of the line has been misheard as well, as "by the donzerly light" or other variants. This has led to many people believing that "donzerly" is an actual word.

"Blinded by the Light", a cover of a Bruce Springsteen song by Manfred Mann's Earth Band, contains what has been called "probably the most misheard lyric of all time". The phrase "revved up like a deuce", altered from Springsteen's original "cut loose like a deuce," both lyrics referring to the hot rodders slang "deuce" (short for deuce coupé) for a 1932 Ford coupé, is frequently misheard as "wrapped up like a douche". Springsteen himself has joked about the phenomenon, claiming that it was not until Manfred Mann rewrote the song to be about a "feminine hygiene product" that the song became popular.

A 2010 survey in Britain found that the most commonly misheard lyric was "Call me when you try to wake her" in R.E.M.'s "The Sidewinder Sleeps Tonite," which was misheard as "Calling Jamaica" or "Calling Cheryl Baker" (in the United Kingdom). Other misheard lyrics reported in the survey included "See that girl, watch her scream, kicking the dancing queen," from the ABBA song "Dancing Queen" ("See that girl, watch that scene, diggin' the dancing queen"). In the United Kingdom, the BBC received complaints after Billy Ocean had performed his song "When the Going Gets Tough, the Tough Get Going" on the TV show "Top of the Pops", because some viewers misheard the lyrics as "go and get stuffed."

Rap and hip hop lyrics may be particularly susceptible to being misheard because they do not necessarily follow standard pronunciations. The delivery of rap lyrics relies heavily upon an often regional pronunciation or non-traditional accenting of words and their phonemes to adhere to the artist's stylizations and the lyrics's written structure. This issue is exemplified in controversies over alleged transcription errors in Yale University Press's 2010 "Anthology of Rap." For example, in the 2015 song "Post To Be" by Omarion ft. Chris Brown & Jhené Aiko, Aiko's lyric "I might have that nigga sailing his soul for me" was widely misheard as "selling his soul" but it was really a play on the title of Aiko's mixtape "Sailing Soul(s)".

Sometimes, the modified version of a lyric becomes standard, as is the case with "The Twelve Days of Christmas". The original has "four colly birds" ("colly" means "black"; cf. "A Midsummer Night's Dream": "Brief as the lightning in the collied night."); sometime around the turn of the twentieth century, these became "calling" birds, which is the lyric used in the 1909 Frederic Austin version.

A number of misheard lyrics have been recorded, turning a mondegreen into a real title. The song "Sea Lion Woman", recorded in 1939 by Christine and Katherine Shipp, was performed by Nina Simone under the title "See Line Woman". According to the liner notes from the compilation "A Treasury of Library of Congress Field Recordings", the actual title of this playground song might also be "See [the] Lyin' Woman" or "C-Line Woman". Jack Lawrence's misinterpretation of the French phrase "pauvre Jean" ("poor John") as the identically pronounced "pauvres gens" ("poor people") led to the translation of "La Goualante du pauvre Jean" ("The Ballad of Poor John") as "The Poor People of Paris", a hit song in 1956.

"A Monk Swimming" by author Malachy McCourt is so titled because of a childhood mishearing of a phrase from the Catholic rosary prayer, Hail Mary. "Amongst women" became "a monk swimmin'".

The title and plot of the short sci-fi story "Come You Nigh: Kay Shuns" ("Com-mu-ni-ca-tions") by Lawrence A. Perkins, in "Analog Science Fiction and Fact" magazine (April 1970), deals with securing interplanetary radio communications by encoding them with mondegreens.

In Beverly Cleary's children's book "Ramona the Pest", Ramona mishears "by the dawn's early light" as "by the dawnzer lee light" and concludes that a "dawnzer" must be a kind of lamp.

"Olive, the Other Reindeer" is a 1997 children's book by Vivian Walsh, which borrows its title from a mondegreen of the line "all of the other reindeer" in the song "Rudolph the Red-Nosed Reindeer". The book was adapted into an animated Christmas special in 1999.

A monologue of mondegreens appears in the 1971 film "Carnal Knowledge". The camera focuses on actress Candice Bergen laughing as she recounts various phrases that fooled her as a child, including "Round John Virgin" (instead of '"Round yon virgin...") and "Gladly, the cross-eyed bear".

The enigmatic title of the 2013 film "Ain't Them Bodies Saints" is actually a misheard lyric from a folk song; director David Lowery decided to use it because it evoked the "classical, regional" feel of 1970s rural Texas.

"Mondegreens" was the name of a segment on the Australian music quiz show "Spicks and Specks" (ABC TV).

Mondegreens have been used in many television advertising campaigns, including:

The traditional game Chinese whispers ("Telephone" in the United States) involves mishearing a whispered sentence to produce a mondegreen.

Among schoolchildren in the U.S., daily rote recitation of the Pledge of Allegiance has long provided opportunities for the genesis of mondegreens.

In Dutch, mondegreens are popularly referred to as "Mama appelsap" ("Mommy applejuice"), from the Michael Jackson song "Wanna Be Startin' Somethin'" which features the lyrics "Mama-se mama-sa ma-ma-coo-sa", and was once misheard as "Mama say mama sa mam[a]appelsap". The Dutch radio station 3FM had a show "Superrradio" (originally "Timur Open Radio") run by Timur Perlin and Ramon with an item in which listeners were encouraged to send in mondegreens under the name "Mama appelsap". The segment was popular for years. 

The title of the film "La Vie en rose" depicting the life of Édith Piaf can be mistaken for "L'Avion rose" (The pink airplane).

The French word "lapalissade", designating a gross truism or platitude, is derived from the name of Jacques II de Chabannes, Seigneur de La Palice, because of a misread mondegreen in a mourning song written just after his heroic death. Reading an "f" as a long "s" (ſ), ""s'il n'était pas mort, il ferait encore envie"" ("if he were not dead, he would still arouse envy") becomes ""il serait encore en vie"" ("he would still be alive"). This truism remains as the first and most well-known "lapalissade" in French.

The title of the 1983 French novel "Le Thé au harem d'Archi Ahmed" ("Tea in the Harem of Archi Ahmed") by Mehdi Charef (and the 1985 movie of the same name) is based on the main character mishearing "le théorème d'Archimède" ("the theorem of Archimedes") in his mathematics class.

A classic example in French is similar to the "Lady Mondegreen" anecdote: in his 1962 collection of children's quotes "La Foire aux cancres", the humorist Jean-Charles refers to a misunderstood lyric of "La Marseillaise" (the French national anthem): "Entendez-vous ... mugir ces féroces soldats" (Do you hear those savage soldiers roar?) is heard as "...Séféro, ce soldat" (that soldier Séféro).

Ghil'ad Zuckermann cites the Hebrew example "mukhrakhím liyót saméakh" ("we must be happy", with a grammar mistake) instead of (the high-register) "úru 'akhím belév saméakh" ("wake up, brothers, with a happy heart"), from the well-known song "Háva Nagíla" ("Let's be happy").
The Israeli site dedicated to Hebrew mondegreens has coined the term "avatiach" (Hebrew for watermelon) for "mondegreen", named for a common mishearing of Shlomo Artzi's award-winning 1970 song "Ahavtia" ("I loved her", using a form uncommon in spoken Hebrew).

A paper in phonology cites memoirs of poet Antoni Słonimski, who confessed that in the recited poem "Konrad Wallenrod" he used to hear "zwierz Alpuhary" ("a beast of Alpujarras") rather than "z wież Alpuhary" ("from the towers of Alpujarras").

The most well-known mondegreen in Brazil is in the song "Noite do Prazer" (Night of Pleasure) by Claudio Zoli: the line "Na madrugada a vitrola rolando um blues, tocando B. B. King sem parar" (At dawn the phonograph playing a blues, playing B. B. King nonstop), is often misheard as "Na madrugada a vitrola rolando um blues, trocando de biquini sem parar" (at dawn the phonograph playing a blues, [people are] exchanging bikini nonstop).

Russian author Fyodor Dostoyevsky, in 1875, cited a line from Fyodor Glinka's song "Troika" (1825) "колокольчик, дар Валдая" ("the bell, gift of Valday") claiming that it is usually understood as "колокольчик, дарвалдая" ("the bell "darvaldaying""—supposedly an onomatopoeia of ringing).

In Turkey, the political Democratic Party changed its logo in 2007 to a white horse in front of a red background because many voters mispronounced its Turkish name "Demokrat", instead saying "demir kırat" ("iron white-horse").

A reverse mondegreen is the intentional production, in speech or writing, of words or phrases that seem to be gibberish but disguise meaning. A prominent example is "Mairzy Doats", a 1943 novelty song by Milton Drake, Al Hoffman, and Jerry Livingston. The lyrics are a reverse mondegreen, made up of oronyms, so pronounced (and written) as to challenge the listener (or reader) to interpret them: 

The clue to the meaning is contained in the bridge:

With that clue, the listener can figure out that the last line is "a kid'll eat ivy, too; wouldn't you?"

Other examples include:

Two authors have written books of supposed foreign-language poetry that are actually mondegreens of nursery rhymes in English. Luis van Rooten's pseudo-French "" includes critical, historical, and interpretive apparatus, as does John Hulme's "Mörder Guss Reims", attributed to a fictitious German poet. Both titles sound like the phrase "Mother Goose Rhymes". Both works can also be considered soramimi, which produces different meanings when interpreted in another language. Wolfgang Amadeus Mozart produced a similar effect in his canon "Difficile Lectu" (written c. 1786-87, when he was 30 or 31), which, though ostensibly in Latin, is actually an opportunity for scatological humor in both German and Italian.

Some performers and writers have used deliberate mondegreens to create double entendres. The phrase "if you see Kay" (F-U-C-K) has been employed many times, notably as a line from James Joyce's 1922 novel "Ulysses" and in many songs, including by blues pianist Memphis Slim in 1963, R. Stevie Moore in 1977, April Wine on its 1982 album "Power Play", the Poster Children via their "Daisy Chain Reaction" in 1991, Turbonegro in 2005, Aerosmith in "Devil's Got a New Disguise" in 2006, and The Script in their 2008 song "If You See Kay". Britney Spears did the same thing with the song "If U Seek Amy". A similar effect was created in Hindi in the 2011 Bollywood movie "Delhi Belly" in the song "Bhaag D.K. Bose". While "D. K. Bose" appears to be a person's name, it is sung repeatedly in the chorus to form the deliberate mondegreen ""bhosadi ke"" (Hindi: भोसडी के), a Hindi expletive.

"Mondegreen" is a song by Yeasayer on their 2010 album, "Odd Blood". The lyrics are intentionally obscure (for instance, "Everybody sugar in my bed" and "Perhaps the pollen in the air turns us into a stapler") and spoken hastily to encourage the mondegreen effect.

Notes
Citations



</doc>
<doc id="20039" url="https://en.wikipedia.org/wiki?curid=20039" title="Merge sort">
Merge sort

In computer science, merge sort (also commonly spelled mergesort) is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the implementation preserves the input order of equal elements in the sorted output. Merge sort is a divide and conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up mergesort appeared in a report by Goldstine and von Neumann as early as 1948.

Conceptually, a merge sort works as follows:

Example C-like code using indices for top down merge sort algorithm that recursively splits the list (called "runs" in this example) into sublists until sublist size is 1, then merges those sublists to produce a sorted list. The copy back step is avoided with alternating the direction of the merge with each level of recursion.

Example C-like code using indices for bottom up merge sort algorithm which treats the list as an array of "n" sublists (called "runs" in this example) of size 1, and iteratively merges sub-lists back and forth between two buffers:

Pseudocode for top down merge sort algorithm which recursively divides the input list into smaller sublists until the sublists are trivially sorted, and then merges the sublists while returning up the call chain.

In this example, the function merges the left and right sublists.

Pseudocode for bottom up merge sort algorithm which uses a small fixed size array of references to nodes, where array[i] is either a reference to a list of size 2 or 0. "node" is a reference or pointer to a node. The merge() function would be similar to the one shown in the top down merge lists example, it merges two already sorted lists, and handles empty lists. In this case, merge() would use "node" for its input parameters and return value.

A natural merge sort is similar to a bottom up merge sort except that any naturally occurring runs (sorted sequences) in the input are exploited. Both monotonic and bitonic (alternating up/down) runs may be exploited, with lists (or equivalently tapes or files) being convenient data structures (used as FIFO queues or LIFO stacks). In the bottom up merge sort, the starting point assumes each run is one item long. In practice, random input data will have many short runs that just happen to be sorted. In the typical case, the natural merge sort may not need as many passes because there are fewer runs to merge. In the best case, the input is already sorted (i.e., is one run), so the natural merge sort need only make one pass through the data. In many practical cases, long natural runs are present, and for that reason natural merge sort is exploited as the key component of Timsort. Example:

Tournament replacement selection sorts are used to gather the initial runs for external sorting algorithms.

In sorting "n" objects, merge sort has an average and worst-case performance of O("n" log "n"). If the running time of merge sort for a list of length "n" is "T"("n"), then the recurrence "T"("n") = 2"T"("n"/2) + "n" follows from the definition of the algorithm (apply the algorithm to two lists of half the size of the original list, and add the "n" steps taken to merge the resulting two lists). The closed form follows from the master theorem for divide-and-conquer recurrences.

In the worst case, the number of comparisons merge sort makes is equal to or slightly smaller than ("n" ⌈lg "n"⌉ - 2 + 1), which is between ("n" lg "n" - "n" + 1) and ("n" lg "n" + "n" + O(lg "n")).

For large "n" and a randomly ordered input list, merge sort's expected (average) number of comparisons approaches "α"·"n" fewer than the worst case where formula_1

In the "worst" case, merge sort does about 39% fewer comparisons than quicksort does in the "average" case. In terms of moves, merge sort's worst case complexity is O("n" log "n")—the same complexity as quicksort's best case, and merge sort's best case takes about half as many iterations as the worst case.

Merge sort is more efficient than quicksort for some types of lists if the data to be sorted can only be efficiently accessed sequentially, and is thus popular in languages such as Lisp, where sequentially accessed data structures are very common. Unlike some (efficient) implementations of quicksort, merge sort is a stable sort.

Merge sort's most common implementation does not sort in place; therefore, the memory size of the input must be allocated for the sorted output to be stored in (see below for versions that need only "n"/2 extra spaces).

Variants of merge sort are primarily concerned with reducing the space complexity and the cost of copying.

A simple alternative for reducing the space overhead to "n"/2 is to maintain "left" and "right" as a combined structure, copy only the "left" part of "m" into temporary space, and to direct the "merge" routine to place the merged output into "m". With this version it is better to allocate the temporary space outside the "merge" routine, so that only one allocation is needed. The excessive copying mentioned previously is also mitigated, since the last pair of lines before the "return result" statement (function " merge "in the pseudo code above) become superfluous.

One drawback of merge sort, when implemented on arrays, is its working memory requirement. Several in-place variants have been suggested:

An alternative to reduce the copying into multiple lists is to associate a new field of information with each key (the elements in "m" are called keys). This field will be used to link the keys and any associated information together in a sorted list (a key and its related information is called a record). Then the merging of the sorted lists proceeds by changing the link values; no records need to be moved at all. A field which contains only a link will generally be smaller than an entire record so less space will also be used. This is a standard sorting technique, not restricted to merge sort.

An external merge sort is practical to run using disk or tape drives when the data to be sorted is too large to fit into memory. External sorting explains how merge sort is implemented with disk drives. A typical tape drive sort uses four tape drives. All I/O is sequential (except for rewinds at the end of each pass). A minimal implementation can get by with just 2 record buffers and a few program variables.

Naming the four tape drives as A, B, C, D, with the original data on A, and using only 2 record buffers, the algorithm is similar to Bottom-up implementation, using pairs of tape drives instead of arrays in memory. The basic algorithm can be described as follows:


Instead of starting with very short runs, usually a hybrid algorithm is used, where the initial pass will read many records into memory, do an internal sort to create a long run, and then distribute those long runs onto the output set. The step avoids many early passes. For example, an internal sort of 1024 records will save 9 passes. The internal sort is often large because it has such a benefit. In fact, there are techniques that can make the initial runs longer than the available internal memory.

A more sophisticated merge sort that optimizes tape (and disk) drive usage is the polyphase merge sort.

On modern computers, locality of reference can be of paramount importance in software optimization, because multilevel memory hierarchies are used. Cache-aware versions of the merge sort algorithm, whose operations have been specifically chosen to minimize the movement of pages in and out of a machine's memory cache, have been proposed. For example, the algorithm stops partitioning subarrays when subarrays of size S are reached, where S is the number of data items fitting into a CPU's cache. Each of these subarrays is sorted with an in-place sorting algorithm such as insertion sort, to discourage memory swaps, and normal merge sort is then completed in the standard recursive fashion. This algorithm has demonstrated better performance on machines that benefit from cache optimization. 

Also, many applications of external sorting use a form of merge sorting where the input get split up to a higher number of sublists, ideally to a number for which merging them still makes the currently processed set of pages fit into main memory.

Merge sort parallelizes well due to use of the divide-and-conquer method. Several parallel variants are discussed in the third edition of Cormen, Leiserson, Rivest, and Stein's "Introduction to Algorithms". The first of these can be very easily expressed in a pseudocode with fork and join keywords:

This algorithm is a trivial modification from the serial version, and its speedup is not impressive: when executed on an infinite number of processors, it runs in time, which is only a improvement on the serial version. A better result can be obtained by using a parallelized merge algorithm, which gives parallelism , meaning that this type of parallel merge sort runs in

time if enough processors are available. Such a sort can perform well in practice when combined with a fast stable sequential sort, such as insertion sort, and a fast sequential merge as a base case for merging small arrays.

Merge sort was one of the first sorting algorithms where optimal speed up was achieved, with Richard Cole using a clever subsampling algorithm to ensure merge. Other sophisticated parallel sorting algorithms can achieve the same or better time bounds with a lower constant. For example, in 1991 David Powers described a parallelized quicksort (and a related radix sort) that can operate in "O"(log "n") time on a CRCW parallel random-access machine (PRAM) with "n" processors by performing partitioning implicitly. Powers further shows that a pipelined version of Batcher's Bitonic Mergesort at "O"((log "n")) time on a butterfly sorting network is in practice actually faster than his "O"(log "n") sorts on a PRAM, and he provides detailed discussion of the hidden overheads in comparison, radix and parallel sorting.

Although heapsort has the same time bounds as merge sort, it requires only Θ(1) auxiliary space instead of merge sort's Θ("n"). On typical modern architectures, efficient quicksort implementations generally outperform mergesort for sorting RAM-based arrays. On the other hand, merge sort is a stable sort and is more efficient at handling slow-to-access sequential media. Merge sort is often the best choice for sorting a linked list: in this situation it is relatively easy to implement a merge sort in such a way that it requires only Θ(1) extra space, and the slow random-access performance of a linked list makes some other algorithms (such as quicksort) perform poorly, and others (such as heapsort) completely impossible.

As of Perl 5.8, merge sort is its default sorting algorithm (it was quicksort in previous versions of Perl). In Java, the Arrays.sort() methods use merge sort or a tuned quicksort depending on the datatypes and for implementation efficiency switch to insertion sort when fewer than seven array elements are being sorted. The Linux kernel uses merge sort for its linked lists. Python uses Timsort, another tuned hybrid of merge sort and insertion sort, that has become the standard sort algorithm in Java SE 7 (for arrays of non-primitive types), on the Android platform, and in GNU Octave.




</doc>
<doc id="20040" url="https://en.wikipedia.org/wiki?curid=20040" title="Maule Air">
Maule Air

Maule Air, Inc. is a manufacturer of light, single-engined, short take-off and landing (STOL) aircraft, based in Moultrie, Georgia, USA. The company delivered 2,500 aircraft in its first 50 years of business.

Belford D. Maule (1911–1995) designed his first aircraft, the M-1 starting at age 19. He founded the company Mechanical Products Co. in Jackson, Michigan to market his own starter design. In 1941 the B.D. Maule Co. was founded, and Maule produced tailwheels and fabric testers. In 1953 he began design work, and started aircraft production with the "Bee-Dee" M-4 in 1957.

The company is a family-owned enterprise. Its owner, June Maule, widow of B. D. Maule, remained directly involved with factory production until her death in 2009 at the age of 92.

The aircraft produced by Maule Air are tube-and-fabric designs and are popular with bush pilots, thanks to their very low stall speed, their tundra tires and oleo strut landing gear. Most Maules are built with tailwheel or amphibious configurations, although the newer MXT models have tricycle gear.



</doc>
<doc id="20041" url="https://en.wikipedia.org/wiki?curid=20041" title="Shoma Morita">
Shoma Morita

Morita Masatake (1874-1938), also read as Morita Shoma (森田 正馬), was a contemporary of Sigmund Freud and the founder of Morita therapy, a branch of clinical psychology strongly influenced by Zen Buddhism. In his capacity as the head of psychiatry for a large Tokyo hospital, Morita began developing his methods while working with sufferers of "shinkeishitsu", or anxiety disorders with a hypochondriac base.

According to Morita, how a person feels is important as a and as an indicator for the present moment, but is uncontrollable: we don't create feelings, feelings happen to us. Since feelings do not cause our behavior, we can coexist with unpleasant feelings while still taking constructive action.

The essence of Morita's method maybe summarized in three rules: Accept all your feelings, know your purpose(s), and do what needs to be done. When once asked what shy people should do, Morita replied, "Sweat."


Perhaps most notable of Morita's foreign followers is David K. Reynolds. Dr. Reynolds synthesized parts of Morita therapy along with the practice of Naikan into Constructive Living, an educational method intended for English-speaking Westerners. Constructive Living has since become extremely popular in Japan. Fritz Perls spent a week in a Morita Hospital in Japan.


</doc>
<doc id="20042" url="https://en.wikipedia.org/wiki?curid=20042" title="Montezuma">
Montezuma

Montezuma, Moctezuma, Moteczoma, Motecuhzoma, Moteuczomah, and Mwatazuma are variant spellings of the same word and may refer to:













</doc>
<doc id="20048" url="https://en.wikipedia.org/wiki?curid=20048" title="Mooney">
Mooney

Mooney is a family name, which is probably predominantly derived from the Irish Ó Maonaigh. It can also be spelled Moony, Moonie, Mainey, Mauney, Meaney and Meeney depending on the dialectic pronunciation that was Anglicised.

The origin of the different Moony or Mooney families is lost in antiquity. The name is derived from "maoin" a Gaelic word meaning "wealth" or "treasure of treasure", hence when O'Maonaigh was anglicised to Mooney it meant "the descendant of the wealthy one."

According to Irish lore, the Mooney family comes from one of the largest and most noble Irish lines. They are said to be descendants of the ancient Irish King Heremon, who, along with his brother Herber, conquered Ireland. Heremon slew his brother shortly after their invasion, took the throne for himself, and fathered a line of kings of Ireland that include Malachi II, and King Niall of the Nine Hostages.

Baptismal records, parish records, ancient land grants, the Annals of the Four Masters, and books by O'Hart, McLysaght, and O'Brien were all used in researching the history of the Mooney family name. These varied and often ancient records indicate that distant septs of the name arose in several places throughout Ireland. The most known and most numerous sept came from the county of Offaly. The members of this sept were from Chieftain Monach, son of Ailill Mor, Lord of Ulster, who was descended from the Kings of Connacht. These family members gave their name to town lands called Ballymooney both in that county and in the neighbouring county of Leix.




</doc>
<doc id="20050" url="https://en.wikipedia.org/wiki?curid=20050" title="Minnesota Twins">
Minnesota Twins

The Minnesota Twins are an American professional baseball team based in Minneapolis, Minnesota. The Twins compete in Major League Baseball (MLB) as a member club of the American League (AL) Central division. The team is named after the Twin Cities area comprising Minneapolis and St. Paul. They played in Metropolitan Stadium from 1961 to 1981 and the Hubert H. Humphrey Metrodome from 1982 to 2009. They played their inaugural game at the newly completed Target Field on April 12, 2010.

The team was founded in Washington, D.C. in as one of the eight original teams of the American League, named the Washington Senators or Washington Nationals. Although the Washington team endured long bouts of mediocrity (immortalized in the 1955 Broadway musical "Damn Yankees"), they had a period of prolonged success in the 1920s and 1930s, led by Baseball Hall of Fame members Bucky Harris, Goose Goslin, Sam Rice, Heinie Manush, Joe Cronin, and Walter Johnson. Manager Clark Griffith joined the team in 1912 and became the team's owner in 1920. The franchise remained under Griffith family ownership until 1984.

In 1960, Major League Baseball granted the city of Minneapolis an expansion team. Washington owner Calvin Griffith, Clark's nephew and adopted son, requested that he be allowed to move his team to Minneapolis-St. Paul and instead give Washington the expansion team. Upon league approval, the team moved to Minnesota after the 1960 season, setting up shop in Metropolitan Stadium, while Washington fielded a brand new "Washington Senators" (which later became the Texas Rangers prior to the 1972 season).

Success came quickly to the team in Minnesota. Sluggers Harmon Killebrew and Bob Allison, who had already been stars in Washington, were joined by Tony Oliva and Zoilo Versalles, and later second baseman Rod Carew and pitchers Jim Kaat and Jim Perry, winning the American League pennant in 1965. A second wave of success came in the late 1980s and early 1990s under manager Tom Kelly, led by Kent Hrbek, Bert Blyleven, Frank Viola, and Kirby Puckett, winning the franchise's second and third World Series (and first and second in Minnesota).

Through the 2017 season, the franchise has won three World Series championships (1924, 1987, and 1991), and has fielded 18 American League batting champions. The team has hosted five All-Star Games (1937, 1956, 1965, 1985 and 2014).

The Washington Senators spent the first decade of their existence finishing near the bottom of the American League standings. Their fortunes began to improve with the arrival of 19-year-old pitcher, Walter Johnson, in 1907. Johnson blossomed in 1911 with 25 victories, although the Senators still finished the season in seventh place. In 1912, the Senators improved dramatically, as their pitching staff led the league in team earned run average and in strikeouts. Johnson won 33 games while teammate Bob Groom added another 24 wins to help the Senators finish the season in second place. The Senators continued to perform respectably in 1913 with Johnson posting a career-high 35 victories, as the team once again finished in second place. The Senators then fell into another period of decline for the next decade. After a string of mediocre seasons, a rejuvenated Johnson rebounded in 1924 to win 23 games with the help of his catcher, Muddy Ruel, as the Senators won the American League pennant for the first time in the history of the franchise.

The Senators faced John McGraw's heavily favored New York Giants in the 1924 World Series. The two teams traded wins back and forth until the series reached the seventh and deciding game. The Senators trailed the Giants 3 to 1 in the eighth inning of Game 7, when Bucky Harris hit a routine ground ball to third which hit a pebble and took a bad hop over Giants third baseman Freddie Lindstrom. Two runners scored on the play, tying the score at three. An aging Walter Johnson then came in to pitch the ninth inning, and held the Giants scoreless into extra innings. In the bottom of the twelfth inning with Ruel at bat, he hit a high, foul ball directly over home plate. The Giants' catcher, Hank Gowdy, dropped his protective mask to field the ball but, failing to toss the mask aside, stumbled over it and dropped the ball, thus giving Ruel another chance to bat. On the next pitch, Ruel hit a double and proceeded to score the winning run when Earl McNeely hit a ground ball that took another bad hop over Lindstrom's head. This would mark the only World Series triumph for the franchise during their 60-year tenure in Washington.

The following season they repeated as American League champions but ultimately lost the 1925 World Series to the Pittsburgh Pirates. After Walter Johnson's retirement in 1927, he was hired as manager of the Senators. After enduring a few losing seasons, the team returned to contention in 1930. In 1933, Senators owner Clark Griffith returned to the formula that worked for him nine years prior: 26-year-old shortstop Joe Cronin became player-manager. The Senators posted a 99–53 record and cruised to the pennant seven games ahead of the New York Yankees, but in the 1933 World Series the Giants exacted their revenge winning in five games. Following the loss, the Senators sank all the way to seventh place in 1934 and attendance began to fall. Despite the return of Harris as manager from 1935–42 and again from 1950–54, Washington was mostly a losing ball club for the next 25 years contending for the pennant only during World War II. Washington came to be known as "first in war, first in peace, and last in the American League", with their hard luck being crucial to the plot of the musical and film "Damn Yankees". Cecil Travis, Buddy Myer (1935 A.L. batting champion), Roy Sievers, Mickey Vernon (batting champion in 1946 and 1953), and Eddie Yost were notable Senators players whose careers were spent in obscurity due to the team's lack of success. In 1954, the Senators signed future Hall of Fame member Harmon Killebrew. By 1959 he was the Senators’ regular third baseman and led the league with 42 home runs earning him a starting spot on the American League All-Star team.
After Griffith's death in 1955, his nephew and adopted son Calvin took over the team presidency. Calvin sold Griffith Stadium to the city of Washington and leased it back leading to speculation that the team was planning to move as the Boston Braves, St. Louis Browns and Philadelphia Athletics had all done in the early 1950s. By 1957, after an early flirtation with San Francisco (where the New York Giants would eventually move after that season ended), Griffith began courting Minneapolis–St. Paul, a prolonged process that resulted in his rejecting the Twin Cities' first offer before agreeing to relocate. The American League opposed the move at first, but in 1960 a deal was reached: The Senators would move and would be replaced with an expansion Senators team for 1961. Thus, the old Washington Senators became the Minnesota Twins.

The Washington franchise was known as both "Senators" and "Nationals" at various times, and sometimes at the same time. In 1905, the team changed its official name to the "Washington Nationals." The name "Nationals" appeared on uniforms for only two seasons, and was then replaced with the "W" logo for the next 52 years. The media often shortened the nickname to "Nats." Many fans and newspapers (especially out-of-town papers) persisted in using the "Senators" nickname, because of potential confusion caused by an American League team using the "Nationals" name. Over time, "Nationals" faded as a nickname, and "Senators" became dominant. Baseball guides listed the club's nickname as "Nationals or Senators", acknowledging the dual-nickname situation.

The team name was officially changed to Washington Senators around the time of Clark Griffith's death. It was not until 1959 that the word "Senators" first appeared on team shirts. "Nats" continued to be used by space-saving headline writers, even for the 1961 expansion team, which was never officially known as "Nationals."

The current "Nationals" and "Nats" names were revived in 2005, when the Montreal Expos relocated to Washington to become the Nationals.

The name "Twins" was derived from the popular name of the region, the Twin Cities (Minneapolis and St. Paul). The NBA's Minneapolis Lakers had relocated to Los Angeles in 1960 due to poor attendance which was believed to have been caused in part by the reluctance of fans in St. Paul to support the team. Griffith was determined not to alienate fans in either city by naming the team after one city or the other, so his desire was to name the team the "Twin Cities Twins", however MLB objected. Griffith therefore named the team the "Minnesota Twins". However, the team was allowed to keep its original "TC" (for Twin Cities) insignia for its caps. The team's logo shows two men, one in a Minneapolis Millers uniform and one in a St. Paul Saints uniform, shaking hands across the Mississippi River within an outline of the state of Minnesota. The "TC" remained on the Twins' caps until 1987, when they adopted new uniforms. By this time, the team felt it was established enough to put an "M" on its cap without having St. Paul fans think it stood for Minneapolis. The "TC" logo was moved to a sleeve on the jerseys, and occasionally appeared as an alternate cap design. Both the "TC" and "Minnie & Paul" logos remain the team's primary insignia. As of 2010, the "TC" logo has been reinstated on the cap as their logo.

The Twins were eagerly greeted in Minnesota when they arrived in 1961. They brought a nucleus of talented players: Harmon Killebrew, Bob Allison, Camilo Pascual, Zoilo Versalles, Jim Kaat, Earl Battey, and Lenny Green. Tony Oliva, who would go on to win American League batting championships in 1964, 1965 and 1971, made his major league debut in 1962. That year, the Twins won 91 games, the most by the franchise since 1933. Behind Mudcat Grant's 21 victories, Versalles' A.L. MVP season and Oliva's batting title, the Twins won 102 games and the American League Pennant in 1965, but they were defeated in the World Series by the Los Angeles Dodgers in seven games (behind the Series MVP, Sandy Koufax, who compiled a 2–1 record, including winning the seventh game).

Heading into the final weekend of the 1967 season, when Rod Carew was named the A.L. Rookie of the Year, the Twins, Boston Red Sox, Chicago White Sox, and Detroit Tigers all had a shot at clinching the American League championship. The Twins and the Red Sox started the weekend tied for 1st place and played against each other in Boston for the final three games of the season. The Red Sox won two out of the three games, seizing their first pennant since 1946 with a 92–70 record. The Twins and Tigers both finished one game back, with 91–71 records, while the White Sox finished three games back, at 89–73. In 1969, the new manager of the Twins, Billy Martin, pushed aggressive base running all-around, and Carew set the all-time Major League record by stealing home seven times in addition to winning the first of seven A.L. batting championships. With Killebrew slugging 49 homers and winning the AL MVP Award, these 1969 Twins won the very first American League Western Division Championship, but they lost three straight games to the Baltimore Orioles, winners of 109 games, in the first American League Championship Series. The Orioles would go on to be upset by the New York Mets in the World Series. Martin was fired after the season, in part due to an August fight in Detroit with 20-game winner Dave Boswell and outfielder Bob Allison, in an alley outside the Lindell A.C. bar. Bill Rigney led the Twins to a repeat division title in 1970, behind the star pitching of Jim Perry (24-12), the A.L. Cy Young Award winner, while the Orioles again won the Eastern Division Championship behind the star pitching of Jim Palmer. Once again, the Orioles won the A.L. Championship Series in a three-game sweep, and this time they would win the World Series.

After winning the division again in 1970, the team entered an eight-year dry spell, finishing around the .500 mark. Killebrew departed after 1974. Owner Calvin Griffith faced financial difficulty with the start of free agency, costing the Twins the services of Lyman Bostock and Larry Hisle, who left as free agents after the 1977 season, and Carew, who was traded after the 1978 season. In 1975, Carew won his fourth consecutive AL batting title, having already joined Ty Cobb as the only players to lead the major leagues in batting average for three consecutive seasons. In , Carew batted .388, which was the highest in baseball since Boston's Ted Williams hit .406 in ; he won the 1977 AL MVP Award. He won another batting title in 1978, hitting .333.

In 1982, the Twins moved into the Hubert H. Humphrey Metrodome, which they shared with the Minnesota Vikings. After a 16-54 start, the Twins were on the verge on becoming the worst team in MLB history. They turned the season around somewhat, but still lost 102 games which is the second-worst record in Twins history (beaten only by the 2016 team, who lost 103 games), despite the .301 average, 23 homers and 92 RBI from rookie Kent Hrbek. In 1984, Griffith sold the Twins to multi-billionaire banker/financier Carl Pohlad. The Metrodome hosted the 1985 Major League Baseball All-Star Game. After several losing seasons, the 1987 team, led by Hrbek, Gary Gaetti, Frank Viola (A.L. Cy Young winner in 1988), Bert Blyleven, Jeff Reardon, Tom Brunansky, Dan Gladden, and rising star Kirby Puckett, returned to the World Series after defeating the favored Detroit Tigers in the ALCS, 4 games to 1. Tom Kelly managed the Twins to World Series victories over the St. Louis Cardinals in 1987 and the Atlanta Braves in 1991. The 1988 Twins were the first team in American League history to draw more than 3 million fans. On July 17, 1990, the Twins became the only team in major league history to pull off two triple plays in the same game. Twins' pitcher and Minnesota native Jack Morris was the star of the series in 1991, going 2–0 in his three starts with a 1.17 ERA. 1991 also marked the first time that any team that finished in last place in their division would advance to the World Series the following season; both the Twins and the Braves did this in 1991. Contributors to the 1991 Twins' improvement from 74 wins to 95 included Chuck Knoblauch, the A.L. Rookie of the Year; Scott Erickson, 20-game winner; new closer Rick Aguilera and new designated hitter Chili Davis.

The World Series in 1991 is regarded by many as one of the classics of all time. In this Series, four games were won during the teams' final at-bat, and three of these were in extra innings. The Atlanta Braves won all three of their games in Atlanta, and the Twins won all four of their games in Minnesota. The sixth game was a legendary one for Puckett, who tripled in a run, made a sensational leaping catch against the wall, and finally in the 11th inning hit the game-winning home run. The seventh game was tied 0–0 after the regulation nine innings, and marked only the second time that the seventh game of the World Series had ever gone into extra innings. The Twins won on a walk-off RBI single by Gene Larkin in the bottom of the 10th inning, after Morris had pitched ten shutout innings against the Braves. The seventh game of the 1991 World Series is widely regarded as one of the greatest games in the history of professional baseball.

After a winning season in 1992 but falling short of Oakland in the division, the Twins fell into a years-long stretch of mediocrity, posting a losing record each season for the next eight: 71–91 in 1993, 50–63 in 1994, 56–88 in 1995, 78–84 in 1996, 68–94 in 1997, 70–92 in 1998, 63–97 in 1999 and 69–93 in 2000. From 1994 to 1997, a long sequence of retirements and injuries hurt the team badly, and Tom Kelly spent the remainder of his managerial career attempting to rebuild the Twins. In 1997, owner Carl Pohlad almost sold the Twins to North Carolina businessman Don Beaver, who would have moved the team to the Piedmont Triad area.

Puckett after the 1995 season was forced to retire at age 35 due to loss of vision in one eye from a central retinal vein occlusion. The 1989 A.L. batting champion, he retired as the Twins' all-time leader in career hits, runs, doubles, and total bases. At the time of his retirement, his .318 career batting average was the highest by any right-handed American League batter since Joe DiMaggio. Puckett was the fourth baseball player during the 20th century to record 1,000 hits in his first five full calendar years in Major League Baseball, and was the second to record 2,000 hits during his first 10 full calendar years. He was elected to the Baseball Hall of Fame in 2001, his first year of eligibility.

The Twins dominated the Central Division in the first decade of the new century, winning the division in six of those ten years ('02, '03, '04, '06, '09 and '10), and nearly winning it in '08 as well. From 2001 to 2006, the Twins compiled the longest streak of consecutive winning seasons since moving to Minnesota.

Threatened with closure by league contraction, the 2002 team battled back to reach the American League Championship Series before being eliminated 4–1 by that year's World Series champion Anaheim Angels. The Twins have not won a playoff series since the 2002 series against the Athletics, this despite the team winning several division championships in the decade. 

In 2006, the Twins won the division on the last day of the regular season (the only day all season they held sole possession of first place) but lost to the Oakland Athletics in the ALDS. Ozzie Guillén coined a nickname for this squad, calling the Twins "little piranhas". The Twins players embraced the label, and in response, the Twins Front office started a "Piranha Night", with piranha finger puppets given out to the first 10,000 fans. Scoreboard operators sometimes played an animated sequence of piranhas munching under that caption in situations where the Twins were scoring runs playing "small ball", and the stadium vendors sold T-shirts and hats advertising "The Little Piranhas".

The Twins also had the AL MVP in Justin Morneau, the AL batting champion in Joe Mauer, and the AL Cy Young Award winner in Johan Santana.

In 2008, the Twins finished the regular season tied with the White Sox on top of the AL Central, forcing a one-game playoff in Chicago to determine the division champion. The Twins lost that game and missed the playoffs. The game location was determined by rule of a coin flip that was conducted in mid-September. This rule was changed for the start of the 2009 season, making the site for any tiebreaker game to be determined by the winner of the regular season head-to-head record between the teams involved.

After a year where the Twins played .500 baseball for most of the season, the team won 17 of their last 21 games to tie the Detroit Tigers for the lead in the Central Division. The Twins were able to use the play-in game rule to their advantage when they won the AL Central at the end of the regular season by way of a 6–5 tiebreaker game that concluded with a 12th-inning walk-off hit by Alexi Casilla to right field, that scored Carlos Gómez. However, they failed to advance to the American League Championship Series as they lost the American League Divisional Series in three straight games to the eventual World Series champion New York Yankees. That year, Joe Mauer became only the second catcher in 33 years to win the AL MVP award. Iván Rodríguez won for the Texas Rangers in 1999, previous to that, the last catcher to win an AL MVP was the New York Yankees Thurman Munson in 1976.

In their inaugural season played at Target Field, the Twins finished the regular season with a record of 94-68, clinching the AL Central Division title for the 6th time in 9 years under manager Ron Gardenhire. New regular players included rookie Danny Valencia at third base, designated hitter Jim Thome, closer Matt Capps, infielder J. J. Hardy, and infielder Orlando Hudson. In relief pitching roles were late additions Brian Fuentes and Randy Flores. On July 7, the team suffered a major blow when Justin Morneau sustained a concussion, which knocked him out for the rest of the season. In the divisional series, the Twins lost to the Yankees in a three-game sweep for the second consecutive year. Following the season, Ron Gardenhire received AL Manager of the Year honors after finishing as a runner up in several prior years.

After repeating as AL Central champions in 2010, the Twins entered 2011 with no players on the disabled list, and the team seemed poised for another strong season. During the off-season, the team signed Japanese shortstop Tsuyoshi Nishioka to fill a hole in the middle infield, re-signed Jim Thome, who was in pursuit of career home run number 600, and also re-signed Carl Pavano. However, the season was largely derailed by an extensive list of injuries. Nishioka's broken leg in a collision at second base led the way and was followed by DL stints from Kevin Slowey, Joe Mauer, Jason Repko, Thome, Delmon Young (two stints on the DL), José Mijares, Glen Perkins, Joe Nathan, Francisco Liriano, Jason Kubel, Denard Span (two stints), Justin Morneau, Scott Baker, and Alexi Casilla. The team's low point was arguably on May 1 when the team started 7 players who were batting below .235 in a game against Kansas City. From that day forward, the Twins made a strong push to get as close as five games back of the division lead by the All-Star break. However, the team struggled down the stretch and fell back out of contention. The team failed to reach the playoffs for the first time since 2008 and experienced their first losing season in four years. Despite an AL-worst 63-99 record, the team drew over 3 million fans for the second consecutive year.

Michael Cuddyer served as the Twins representative at the All-Star game, his first appearance. Bert Blyleven's number was retired during the season and he was also inducted into the Baseball Hall of Fame during the month of July. On August 10, Nathan recorded his 255th save, passing Rick Aguilera for first place on the franchise's all-time saves list. On August 15, Thome hit 599th and 600th home run at Comerica Park to become the eighth player in Major League history to hit 600 home runs, joining Babe Ruth, Willie Mays, Hank Aaron, Barry Bonds, Sammy Sosa, Ken Griffey, Jr., and Alex Rodriguez.

The team started the 2012 season with a league worst 10-24 record. In late May and early June, the team embarked on a hot streak, winning ten out of thirteen games. By mid July, the team found themselves only 10 games out of the division lead. On July 16, the Twins defeated the Baltimore Orioles 19-7, the most runs scored in the short history of Target Field. By the end of August, the Twins were more than 20 games below .500, and last in the American League. On August 29, it was announced that the Twins would host the 2014 All-Star Game. In 2013, the Twins finished in 4th place in the AL Central, with a record of 66-96. In 2014, the team finished with a 70-92 record, last in the division and accumulated the second fewest wins in the American League. As a result, Ron Gardenhire was fired on September 29, 2014. On November 3, 2014 Paul Molitor was announced by the team as the 13th manager in Twins history.

In 2015, the team had a winning season (83-79), following four consecutive seasons of 90 or more losses.

In 2016, the Minnesota Twins finished last in the AL Central, with a 59-103 record. Brian Dozier set his career high in home runs with 43, which was tied for second in baseball, and leading all 2nd basemen. Tyler Duffey led all Twins starters with 9 wins throughout the season, while fellow reliever Brandon Kintzler led the team with 17 saves. Rising stars Miguel Sano, Max Kepler, and Byron Buxton combined to have 263 total hits, 52 home runs, 167 RBIs, and a batting average of .232 throughout the season. The Twins signed star Korean slugger Byung Ho Park to a 4-year/$12 million contract, where he hit a .191 batting average, with 12 home runs, and 24 RBIs before being sent down to Rochester for the remainder of the season.

The quirks of the Hubert H. Humphrey Metrodome, including the turf floor and the white roof, gave the Twins a significant home-field advantage that played into their winning the World Series in both 1987 and 1991, at least in the opinion of their opponents, as the Twins went 12–1 in postseason home games during those two seasons. These were the first two World Series in professional baseball history in which a team won the championship by winning all four home games. (The feat has since been repeated once, by the Arizona Diamondbacks in 2001.) Nevertheless, the Twins argued that the Metrodome was obsolete and that the lack of a dedicated baseball-only ballpark limited team revenue and made it difficult to sustain a top-notch, competitive team (the Twins had been sharing tenancy in stadiums with the NFL's Minnesota Vikings since 1961). The team was rumored to contemplate moving to such places as New Jersey, Las Vegas, Portland, Oregon, the Greensboro/Winston-Salem, North Carolina, area, and elsewhere in search of a more financially competitive market. In 2002, the team was nearly disbanded when Major League Baseball selected the Twins and the Montreal Expos (now the Washington Nationals franchise) for elimination due to their financial weakness relative to other franchises in the league. The impetus for league contraction diminished after a court decision forced the Twins to play out their lease on the Metrodome. However, Twins owner Carl Pohlad continued his efforts to relocate, pursuing litigation against the Metropolitan Stadium Commission and obtaining a state court ruling that his team was not obligated to play in the Metrodome after the 2006 season. This cleared the way for the Twins to either be relocated or disbanded prior to the 2007 season if a new deal was not reached.

In response to the threatened loss of the Twins, the Minnesota private and public sector negotiated and approved a financing package for a replacement stadium— a baseball-only outdoor, natural turf ballpark in the Warehouse District of downtown Minneapolis— owned by a new entity known as the Minnesota Ballpark Authority. Target Field was constructed at a cost of $544.4 million (including site acquisition and infrastructure), utilizing the proceeds of a $392 million public bond offering based on a 0.15% sales tax in Hennepin County and private financing of $185 million provided by the Pohlad family. As part of the deal, the Twins also signed a 30-year lease of the new stadium, effectively guaranteeing the continuation of the team in Minnesota for a long time to come. Construction of the new field began in 2007, and was completed in December 2009, in time for the 2010 season. Commissioner Bud Selig, who earlier had threatened to disband the team, observed that without the new stadium the Twins could not have committed to sign their star player, catcher Joe Mauer, to an 8-year, $184 million contract extension. The first regular season game in Target Field was played against the Boston Red Sox on April 12, 2010, with Mauer driving in two runs and going 3-for-5 to help the Twins defeat the Red Sox, 5–2.

On May 18, 2011, Target Field was named "The Best Place To Shop" by Street and Smith's "SportsBusiness Journal" at the magazine's 2011 Sports Business Awards Ceremony in New York City. It was also named "The Best Sports Stadium in North America" by "ESPN The Magazine" in a ranking that included over 120 different stadiums, ballparks and arenas from around North America.

In July 2014, Target Field hosted the 85th Major League Baseball All-Star Game and the Home Run Derby.

Minnesota Twins all-time roster: A complete list of players who played in at least one game for the Twins franchise.

Molitor and Winfield, St. Paul natives and University of Minnesota graduates, came to the team late in their careers and were warmly received as "hometown heroes", but were elected to the Hall on the basis of their tenures with other teams. Both swatted their 3,000th hit with the Twins.

Cronin, Goslin, Griffith, Harris, Johnson, Killebrew and Wynn are listed on the Washington Hall of Stars display at Nationals Park (previously they were listed at Robert F. Kennedy Stadium). So are Ossie Bluege, George Case, Joe Judge, George Selkirk, Roy Sievers, Cecil Travis, Mickey Vernon and Eddie Yost.


The Metrodome's upper deck in center and right fields was partly covered by a curtain containing banners of various titles won, and retired numbers. There was no acknowledgment of the Twins' prior championships in Washington and several Senator Hall of Famers, such as Walter Johnson, played in the days prior to numbers being used on uniforms. However Killebrew played seven seasons as a Senator, including two full seasons as a regular prior to the move to Minnesota in 1961.

Prior to the addition of the banners, the Twins acknowledged their retired numbers on the Metrodome's outfield fence. Harmon Killebrew's #3 was the first to be displayed, as it was the only one the team had retired when they moved in. It was joined by Rod Carew's #29 in 1987, Tony Oliva's #6 in 1991, Kent Hrbek's #14 in 1995, and Kirby Puckett's #34 in 1997 before the Twins began hanging the banners to reduce capacity. The championships, meanwhile were marked on the "Baggie" in right field.

The numbers that have been retired hang within Target Field in front of the tower that serves as the Twins' executive offices in left field foul territory. The championships banners have been replaced by small pennants that fly on masts at the back of the left field upper deck. Those pennants, along with the flags flying in the plaza behind right field, serve as a visual cue for the players, suggesting the wind direction and speed.

Jackie Robinson's number, 42, was retired by Major League Baseball on April 15, 1997 and formally honored by the Twins on May 23, 1997. Robinson's number was positioned to the left of the Twins numbers in both venues.

In 2007, the Twins took the rights to the broadcasts in-house and created the Twins Radio Network (TRN). With that new network in place the Twins secured a new Metro Affiliate flagship radio station in KSTP (AM 1500). It replaced WCCO (AM 830), which held broadcast rights for the Twins since the team moved to Minneapolis in 1961. For 2013, the Twins moved to FM radio on KTWN-FM "96.3 K-Twin", which is owned by the Pohlad family. The original radio voices of the Twins in 1961 were Ray Scott, Halsey Hall and Bob Wolff. After the first season, Herb Carneal replaced Wolff. Twins TV and radio broadcasts were originally sponsored by the Hamm's Brewing Company. In 2009, Treasure Island Resort & Casino became the first ever naming rights partner for the Twins Radio Network, making the commercial name of TRN the Treasure Island Baseball Network. In 2017, it was announced that WCCO would become the flagship station the Twins again starting in 2018, thus returning the team back to its original station after 11 years.

Cory Provus is the current radio play by play announcer, taking over in 2012 for longtime Twins voice John Gordon who retired following the 2011 season. Former Twins OF Dan Gladden serves as color commentator.

TRN broadcasts are originated from the studios at Minnesota News Network and Minnesota Farm Networks. Kris Atteberry hosts the pre-game show, the "Lineup Card" and the "Post-game Download" from those studios except when filling in for Provus or Gladden when they are on vacation.

On April 1, 2007, Herb Carneal, the radio voice of the Twins for all but one year of their existence, died at his home in Minnetonka, Minnesota after a long battle with a list of illnesses. Carneal is in the broadcasters wing of the Baseball Hall of Fame.

The television rights are held by Fox Sports North with Dick Bremer as the play-by-play announcer and former Twin, 2011 National Baseball Hall of Fame inductee, Bert Blyleven as color analyst. They are sometimes joined by Roy Smalley and Jack Morris.

Bob Casey was the Twins first public-address announcer starting in 1961 and continuing until his death in 2005. He was well known for his unique delivery and his signature announcements of "No smoking in the Metrodome, either go outside or quit!" (or "go back to Boston", etc.), "Batting 3rd, the center-fielder, No. 34, Kirby Puckett!!!" and asking fans not to 'throw anything or anybody' onto the field.


Fans wave a "Homer Hanky" to rally the team during play-offs and other crucial games. The Homer Hanky was created by Terrie Robbins of the Star Tribune newspaper in the Twin Cities in 1987. It was her idea to originally give away 60,000 inaugural Homer Hankies. That year, over 2.3 million Homer Hankies were distributed.

The party atmosphere of the Twins clubhouse after a win is well-known, the team's players unwinding with loud rock music (usually the choice of the winning pitcher) and video games.

The club has several hazing rituals, such as requiring the most junior relief pitcher on the team to carry water and snacks to the bullpen in a brightly colored small child's backpack (Barbie in 2005, SpongeBob SquarePants in 2006, Hello Kitty in 2007, Disney Princess and Tinkerbell in 2009, Chewbacca and Darth Vader in 2010), and many of its players, both past and present, are notorious pranksters. For example, Bert Blyleven earned the nickname "The Frying Dutchman" for his ability to pull the "hotfoot" – which entails crawling under the bench in the dugout and lighting a teammate's shoelaces on fire.




</doc>
<doc id="20051" url="https://en.wikipedia.org/wiki?curid=20051" title="Mach number">
Mach number

In fluid dynamics, the Mach number (M or Ma) (; ) is a dimensionless quantity representing the ratio of flow velocity past a boundary to the local speed of sound.

where:

By definition, at Mach1, the local flow velocity is equal to the speed of sound. At Mach0.65, is 65% of the speed of sound (subsonic), and, at Mach1.35, is 35% faster than the speed of sound (supersonic).

The local speed of sound, and thereby the Mach number, depends on the condition of the surrounding medium, in particular the temperature. The Mach number is primarily used to determine the approximation with which a flow can be treated as an incompressible flow. The medium can be a gas or a liquid. The boundary can be traveling in the medium, or it can be stationary while the medium flows along it, or they can both be moving, with different velocities: what matters is their relative velocity with respect to each other. The boundary can be the boundary of an object immersed in the medium, or of a channel such as a nozzle, diffusers or wind tunnels channeling the medium. As the Mach number is defined as the ratio of two speeds, it is a dimensionless number. If  < 0.2–0.3 and the flow is quasi-steady and isothermal, compressibility effects will be small and simplified incompressible flow equations can be used.

The Mach number is named after Austrian physicist and philosopher Ernst Mach, and is a designation proposed by aeronautical engineer Jakob Ackeret. As the Mach number is a dimensionless quantity rather than a unit of measure, with Mach, the number comes "after" the unit; the second Mach number is "Mach2" instead of "2Mach" (or Machs). This is somewhat reminiscent of the early modern ocean sounding unit "mark" (a synonym for fathom), which was also unit-first, and may have influenced the use of the term Mach. In the decade preceding faster-than-sound human flight, aeronautical engineers referred to the speed of sound as "Mach's number", never "Mach 1".

Mach number is useful because the fluid behaves in a similar manner at a given Mach number, regardless of other variables. As modeled in the International Standard Atmosphere, dry air at mean sea level, standard temperature of , the speed of sound is . The speed of sound is not a constant; in a gas, it increases as the absolute temperature increases, and since atmospheric temperature generally decreases with increasing altitude between sea level and , the speed of sound also decreases. For example, the standard atmosphere model lapses temperature to at altitude, with a corresponding speed of sound (Mach1) of , 86.7% of the sea level value.

While the terms "subsonic" and "supersonic", in the purest sense, refer to speeds below and above the local speed of sound respectively, aerodynamicists often use the same terms to talk about particular ranges of Mach values. This occurs because of the presence of a "transonic regime" around M = 1 where approximations of the Navier-Stokes equations used for subsonic design actually no longer apply; the simplest explanation is that the flow locally begins to exceed M = 1 even though the freestream Mach number is below this value.

Meanwhile, the "supersonic regime" is usually used to talk about the set of Mach numbers for which linearised theory may be used, where for example the (air) flow is not chemically reacting, and where heat-transfer between air and vehicle may be reasonably neglected in calculations.

In the following table, the "regimes" or "ranges of Mach values" are referred to, and not the "pure" meanings of the words "subsonic" and "supersonic".

Generally, NASA defines "high" hypersonic as any Mach number from 10 to 25, and re-entry speeds as anything greater than Mach 25. Aircraft operating in this regime include the Space Shuttle and various space planes in development.

Flight can be roughly classified in six categories:

For comparison: the required speed for low Earth orbit is approximately 7.5 km/s = Mach 25.4 in air at high altitudes.

At transonic speeds, the flow field around the object includes both sub- and supersonic parts. The transonic period begins when first zones of M > 1 flow appear around the object. In case of an airfoil (such as an aircraft's wing), this typically happens above the wing. Supersonic flow can decelerate back to subsonic only in a normal shock; this typically happens before the trailing edge. (Fig.1a)

As the speed increases, the zone of M > 1 flow increases towards both leading and trailing edges. As M = 1 is reached and passed, the normal shock reaches the trailing edge and becomes a weak oblique shock: the flow decelerates over the shock, but remains supersonic. A normal shock is created ahead of the object, and the only subsonic zone in the flow field is a small area around the object's leading edge. (Fig.1b)

Fig. 1. "Mach number in transonic airflow around an airfoil; M < 1 (a) and M > 1 (b)."

When an aircraft exceeds Mach 1 (i.e. the sound barrier), a large pressure difference is created just in front of the aircraft. This abrupt pressure difference, called a shock wave, spreads backward and outward from the aircraft in a cone shape (a so-called Mach cone). It is this shock wave that causes the sonic boom heard as a fast moving aircraft travels overhead. A person inside the aircraft will not hear this. The higher the speed, the more narrow the cone; at just over M = 1 it is hardly a cone at all, but closer to a slightly concave plane.

At fully supersonic speed, the shock wave starts to take its cone shape and flow is either completely supersonic, or (in case of a blunt object), only a very small subsonic flow area remains between the object's nose and the shock wave it creates ahead of itself. (In the case of a sharp object, there is no air between the nose and the shock wave: the shock wave starts from the nose.)

As the Mach number increases, so does the strength of the shock wave and the Mach cone becomes increasingly narrow. As the fluid flow crosses the shock wave, its speed is reduced and temperature, pressure, and density increase. The stronger the shock, the greater the changes. At high enough Mach numbers the temperature increases so much over the shock that ionization and dissociation of gas molecules behind the shock wave begin. Such flows are called hypersonic.

It is clear that any object traveling at hypersonic speeds will likewise be exposed to the same extreme temperatures as the gas behind the nose shock wave, and hence choice of heat-resistant materials becomes important.

As a flow in a channel becomes supersonic, one significant change takes place. The conservation of mass flow rate leads one to expect that contracting the flow channel would increase the flow speed (i.e. making the channel narrower results in faster air flow) and at subsonic speeds this holds true. However, once the flow becomes supersonic, the relationship of flow area and speed is reversed: expanding the channel actually increases the speed.

The obvious result is that in order to accelerate a flow to supersonic, one needs a convergent-divergent nozzle, where the converging section accelerates the flow to sonic speeds, and the diverging section continues the acceleration. Such nozzles are called de Laval nozzles and in extreme cases they are able to reach hypersonic speeds ( at 20 °C).

An aircraft Machmeter or electronic flight information system (EFIS) can display Mach number derived from stagnation pressure (pitot tube) and static pressure.

The Mach number at which an aircraft is flying can be calculated by

where:

Note that the dynamic pressure can be found as:

Assuming air to be an ideal gas, the formula to compute Mach number in a subsonic compressible flow is derived from Bernoulli's equation for M < 1:

where:

The formula to compute Mach number in a supersonic compressible flow is derived from the Rayleigh supersonic pitot equation:

Mach number is a function of temperature and true airspeed.
Aircraft flight instruments, however, operate using pressure differential to compute Mach number, not temperature.

Assuming air to be an ideal gas, the formula to compute Mach number in a subsonic compressible flow is found from Bernoulli's equation for (above):

The formula to compute Mach number in a supersonic compressible flow can be found from the Rayleigh supersonic pitot equation (above) using parameters for air:

where:

As can be seen, M appears on both sides of the equation, and for practical purposes a root-finding algorithm must be used for a numerical solution. It is first determined whether M is indeed greater than 1.0 by calculating M from the subsonic equation. If M is greater than 1.0 at that point, then the value of M from the subsonic equation is used as the initial condition for fixed point iteration of the supersonic equation, which usually converges in just a few iterations. Alternatively, Newton's method can also be used.




</doc>
<doc id="20053" url="https://en.wikipedia.org/wiki?curid=20053" title="March 8">
March 8





</doc>
<doc id="20054" url="https://en.wikipedia.org/wiki?curid=20054" title="March 9">
March 9




</doc>
<doc id="20055" url="https://en.wikipedia.org/wiki?curid=20055" title="Moving Picture Experts Group">
Moving Picture Experts Group

The Moving Picture Experts Group (MPEG) is a working group of authorities that was formed by ISO and IEC to set standards for audio and video compression and transmission. It was established in 1988 by the initiative of Hiroshi Yasuda (Nippon Telegraph and Telephone) and Leonardo Chiariglione, group Chair since its inception. The first MPEG meeting was in May 1988 in Ottawa, Canada. As of late 2005, MPEG has grown to include approximately 350 members per meeting from various industries, universities, and research institutions. MPEG's official designation is ISO/IEC JTC 1/SC 29/WG 11 – "Coding of moving pictures and audio" (ISO/IEC Joint Technical Committee 1, Subcommittee 29, Working Group 11).

ISO/IEC JTC1/SC29/WG11 – "Coding of moving pictures and audio" has following Sub Groups (SG):

Joint Video Team (JVT) is joint project between ITU-T SG16/Q.6 (Study Group 16 / Question 6) – VCEG (Video Coding Experts Group) and ISO/IEC JTC1/SC29/WG11 – MPEG for the development of new video coding recommendation and international standard. It was formed in 2001 and its main result has been H.264/MPEG-4 AVC (MPEG-4 Part 10).

Joint Collaborative Team on Video Coding (JCT-VC) is a group of video coding experts from ITU-T Study Group 16 (VCEG) and ISO/IEC JTC 1/SC 29/WG 11 (MPEG). It was created in 2010 to develop High Efficiency Video Coding, a new generation video coding standard that further reduces (by 50%) the data rate required for high quality video coding, as compared to the current ITU-T H.264 / ISO/IEC 14496-10 standard. JCT-VC is co-chaired by Jens-Rainer Ohm and Gary Sullivan.

The MPEG standards consist of different "Parts". Each "part" covers a certain aspect of the whole specification. The standards also specify "Profiles" and "Levels". "Profiles" are intended to define a set of tools that are available, and "Levels" define the range of appropriate values for the properties associated with them. Some of the approved MPEG standards were revised by later amendments and/or new editions. MPEG has standardized the following compression formats and ancillary standards:

MPEG-4 has been chosen as the compression scheme for over-the-air in Brazil (ISDB-TB), based on original digital television from Japan (ISDB-T).

In addition, the following standards, while not sequential advances to the video encoding standard as with MPEG-1 through MPEG-4, are referred to by similar notation:


Moreover, more recently than other standards above, MPEG has started following international standards; each of the standards holds multiple MPEG technologies for a way of application. (For example, MPEG-A includes a number of technologies on multimedia application format.)


A standard published by ISO/IEC is the last stage of a long process that starts with the proposal of new work within a committee. Here are some abbreviations used for marking a standard with its status:


Other abbreviations:

A proposal of work (New Proposal) is approved at Subcommittee and then at the Technical Committee level (SC29 and JTC1 respectively – in the case of MPEG). When the scope of new work is sufficiently clarified, MPEG usually makes open requests for proposals – known as "Call for proposals". The first document that is produced for audio and video coding standards is called a Verification Model (VM). In the case of MPEG-1 and MPEG-2 this was called Simulation and Test Model, respectively. When a sufficient confidence in the stability of the standard under development is reached, a Working Draft (WD) is produced. This is in the form of a standard but is kept internal to MPEG for revision. When a WD is sufficiently solid, becomes Committee Draft (CD) (usually at the planned time). It is then sent to National Bodies (NB) for ballot. The CD becomes Final Committee Draft (FCD) if the number of positive votes is above the quorum. After a review and comments issued by NBs, FCD is again submitted to NBs for the second ballot. If the FCD is approved, it becomes Final Draft International Standard (FDIS). ISO then holds a ballot with National Bodies, where no technical changes are allowed (yes/no ballot). If approved, the document becomes International Standard (IS).

ISO/IEC Directives allow also the so-called "Fast-track procedure". In this procedure a document is submitted directly for approval as a draft International Standard (DIS) to the ISO member bodies or as a final draft International Standard (FDIS) if the document was developed by an international standardizing body recognized by the ISO Council.




</doc>
<doc id="20056" url="https://en.wikipedia.org/wiki?curid=20056" title="MPEG-1">
MPEG-1

MPEG-1 is a standard for lossy compression of video and audio. It is designed to compress VHS-quality raw digital video and CD audio down to 1.5 Mbit/s (26:1 and 6:1 compression ratios respectively) without excessive quality loss, making video CDs, digital cable/satellite TV and digital audio broadcasting (DAB) possible.

Today, MPEG-1 has become the most widely compatible lossy audio/video format in the world, and is used in a large number of products and technologies. Perhaps the best-known part of the MPEG-1 standard is the MP3 audio format it introduced.

The MPEG-1 standard is published as ISO/IEC 11172 – Information technology—Coding of moving pictures and associated audio for digital storage media at up to about 1.5 Mbit/s. 
The standard consists of the following five "Parts":

Modeled on the successful collaborative approach and the compression technologies developed by the Joint Photographic Experts Group and CCITT's Experts Group on Telephony (creators of the JPEG image compression standard and the H.261 standard for video conferencing respectively), the Moving Picture Experts Group (MPEG) working group was established in January 1988. MPEG was formed to address the need for standard video and audio formats, and to build on H.261 to get better quality through the use of more complex encoding methods. It was established in 1988 by the initiative of Hiroshi Yasuda (Nippon Telegraph and Telephone) and Leonardo Chiariglione.

Development of the MPEG-1 standard began in May 1988. Fourteen video and fourteen audio codec proposals were submitted by individual companies and institutions for evaluation. The codecs were extensively tested for computational complexity and subjective (human perceived) quality, at data rates of 1.5 Mbit/s. This specific bitrate was chosen for transmission over T-1/E-1 lines and as the approximate data rate of audio CDs. The codecs that excelled in this testing were utilized as the basis for the standard and refined further, with additional features and other improvements being incorporated in the process.

After 20 meetings of the full group in various cities around the world, and 4½ years of development and testing, the final standard (for parts 1–3) was approved in early November 1992 and published a few months later. The reported completion date of the MPEG-1 standard varies greatly: a largely complete draft standard was produced in September 1990, and from that point on, only minor changes were introduced. The draft standard was publicly available for purchase. The standard was finished with the 6 November 1992 meeting. The Berkeley Plateau Multimedia Research Group developed an MPEG-1 decoder in November 1992. In July 1990, before the first draft of the MPEG-1 standard had even been written, work began on a second standard, MPEG-2, intended to extend MPEG-1 technology to provide full broadcast-quality video (as per CCIR 601) at high bitrates (3–15  Mbit/s) and support for interlaced video. Due in part to the similarity between the two codecs, the MPEG-2 standard includes full backwards compatibility with MPEG-1 video, so any MPEG-2 decoder can play MPEG-1 videos.

Notably, the MPEG-1 standard very strictly defines the bitstream, and decoder function, but does not define how MPEG-1 encoding is to be performed, although a reference implementation is provided in ISO/IEC-11172-5. This means that MPEG-1 coding efficiency can drastically vary depending on the encoder used, and generally means that newer encoders perform significantly better than their predecessors. The first three parts (Systems, Video and Audio) of ISO/IEC 11172 were published in August 1993.

All widely known patent searches suggest that, due to its age, MPEG-1 video and Layer I/II audio is no longer covered by any patents and can thus be used without obtaining a licence or paying any fees. The ISO patent database lists one patent for ISO 11172, US 4,472,747, which expired in 2003. The near-complete draft of the MPEG-1 standard was publicly available as ISO CD 11172 by December 6, 1991. Neither the July 2008 Kuro5hin article "Patent Status of MPEG-1, H.261 and MPEG-2", nor an August 2008 thread on the gstreamer-devel mailing list were able to list a single unexpired MPEG-1 video and Layer I/II audio patent. A May 2009 discussion on the whatwg mailing list mentioned US 5,214,678 patent as possibly covering MPEG audio layer II. Filed in 1990 and published in 1993, this patent is now expired.

A full MPEG-1 decoder and encoder, with "Layer 3 audio", could not be implemented royalty free since there were companies that required patent fees for implementations of MPEG-1 Layer 3 Audio as discussed in the MP3 article. All patents in the world connected to MP3 expired 30 December 2017, which makes this format totally free for use. Despite this as early as on 23 April 2017 Fraunhofer IIS stopped charging for Technicolor's mp3 licensing program for certain mp3 related patents and software.


Part 1 of the MPEG-1 standard covers "systems", and is defined in ISO/IEC-11172-1.

MPEG-1 Systems specifies the logical layout and methods used to store the encoded audio, video, and other data into a standard bitstream, and to maintain synchronization between the different contents. This file format is specifically designed for storage on media, and transmission over communication channels, that are considered relatively reliable. Only limited error protection is defined by the standard, and small errors in the bitstream may cause noticeable defects.

This structure was later named an MPEG program stream: "The MPEG-1 Systems design is essentially identical to the MPEG-2 Program Stream structure." This terminology is more popular, precise (differentiates it from an MPEG transport stream) and will be used here.

Elementary Streams (ES) are the raw bitstreams of MPEG-1 audio and video encoded data (output from an encoder). These files can be distributed on their own, such as is the case with MP3 files.

Packetized Elementary Streams ("PES") are elementary streams packetized into packets of variable lengths, i.e., divided ES into independent chunks where cyclic redundancy check (CRC) checksum was added to each packet for error detection.

System Clock Reference (SCR) is a timing value stored in a 33-bit header of each PES, at a frequency/precision of 90 kHz, with an extra 9-bit extension that stores additional timing data with a precision of 27 MHz. These are inserted by the encoder, derived from the system time clock (STC). Simultaneously encoded audio and video streams will not have identical SCR values, however, due to buffering, encoding, jitter, and other delay.

Program Streams (PS) are concerned with combining multiple packetized elementary streams (usually just one audio and video PES) into a single stream, ensuring simultaneous delivery, and maintaining synchronization. The PS structure is known as a multiplex, or a container format.

Presentation time stamps (PTS) exist in PS to correct the inevitable disparity between audio and video SCR values (time-base correction). 90 kHz PTS values in the PS header tell the decoder which video SCR values match which audio SCR values. PTS determines when to display a portion of an MPEG program, and is also used by the decoder to determine when data can be discarded from the buffer. Either video or audio will be delayed by the decoder until the corresponding segment of the other arrives and can be decoded.

PTS handling can be problematic. Decoders must accept multiple "program streams" that have been concatenated (joined sequentially). This causes PTS values in the middle of the video to reset to zero, which then begin incrementing again. Such PTS wraparound disparities can cause timing issues that must be specially handled by the decoder.

Decoding Time Stamps (DTS), additionally, are required because of B-frames. With B-frames in the video stream, adjacent frames have to be encoded and decoded out-of-order (re-ordered frames). DTS is quite similar to PTS, but instead of just handling sequential frames, it contains the proper time-stamps to tell the decoder when to decode and display the next B-frame (types of frames explained below), ahead of its anchor (P- or I-) frame. Without B-frames in the video, PTS and DTS values are identical.

To generate the PS, the multiplexer will interleave the (two or more) packetized elementary streams. This is done so the packets of the simultaneous streams can be transferred over the same channel and are guaranteed to both arrive at the decoder at precisely the same time. This is a case of time-division multiplexing.

Determining how much data from each stream should be in each interleaved segment (the size of the interleave) is complicated, yet an important requirement. Improper interleaving will result in buffer underflows or overflows, as the receiver gets more of one stream than it can store (e.g. audio), before it gets enough data to decode the other simultaneous stream (e.g. video). The MPEG Video Buffering Verifier (VBV) assists in determining if a multiplexed PS can be decoded by a device with a specified data throughput rate and buffer size. This offers feedback to the muxer and the encoder, so that they can change the mux size or adjust bitrates as needed for compliance.

Part 2 of the MPEG-1 standard covers video and is defined in ISO/IEC-11172-2. The design was heavily influenced by H.261.

MPEG-1 Video exploits perceptual compression methods to significantly reduce the data rate required by a video stream. It reduces or completely discards information in certain frequencies and areas of the picture that the human eye has limited ability to fully perceive. It also exploits temporal (over time) and spatial (across a picture) redundancy common in video to achieve better data compression than would be possible otherwise. (See: Video compression)

Before encoding video to MPEG-1, the color-space is transformed to Y'CbCr (Y'=Luma, Cb=Chroma Blue, Cr=Chroma Red). Luma (brightness, resolution) is stored separately from chroma (color, hue, phase) and even further separated into red and blue components. The chroma is also subsampled to , meaning it is reduced by one half vertically and one half horizontally, to just one quarter the resolution of the video.
This software algorithm also has analogies in hardware, such as the output from a Bayer pattern filter, common in digital colour cameras.

Because the human eye is much more sensitive to small changes in brightness (the Y component) than in color (the Cr and Cb components), chroma subsampling is a very effective way to reduce the amount of video data that needs to be compressed. On videos with fine detail (high spatial complexity) this can manifest as chroma aliasing artifacts. Compared to other digital compression artifacts, this issue seems to be very rarely a source of annoyance.

Because of subsampling, Y'CbCr video must always be stored using even dimensions (divisible by 2), otherwise chroma mismatch ("ghosts") will occur, and it will appear as if the color is ahead of, or behind the rest of the video, much like a shadow.

Y'CbCr is often inaccurately called YUV which is only used in the domain of analog video signals. Similarly, the terms luminance and chrominance are often used instead of the (more accurate) terms luma and chroma.

MPEG-1 supports resolutions up to 4095×4095 (12-bits), and bitrates up to 100 Mbit/s.

MPEG-1 videos are most commonly seen using Source Input Format (SIF) resolution: 352x240, 352x288, or 320x240. These low resolutions, combined with a bitrate less than 1.5 Mbit/s, make up what is known as a constrained parameters bitstream (CPB), later renamed the "Low Level" (LL) profile in MPEG-2. This is the minimum video specifications any decoder should be able to handle, to be considered MPEG-1 compliant. This was selected to provide a good balance between quality and performance, allowing the use of reasonably inexpensive hardware of the time.

MPEG-1 has several frame/picture types that serve different purposes. The most important, yet simplest, is I-frame.

I-frame is an abbreviation for Intra-frame, so-called because they can be decoded independently of any other frames. They may also be known as I-pictures, or keyframes due to their somewhat similar function to the key frames used in animation. I-frames can be considered effectively identical to baseline JPEG images.

High-speed seeking through an MPEG-1 video is only possible to the nearest I-frame. When cutting a video it is not possible to start playback of a segment of video before the first I-frame in the segment (at least not without computationally intensive re-encoding). For this reason, I-frame-only MPEG videos are used in editing applications.

I-frame only compression is very fast, but produces very large file sizes: a factor of 3× (or more) larger than normally encoded MPEG-1 video, depending on how temporally complex a specific video is. I-frame only MPEG-1 video is very similar to MJPEG video. So much so that very high-speed and theoretically lossless (in reality, there are rounding errors) conversion can be made from one format to the other, provided a couple of restrictions (color space and quantization matrix) are followed in the creation of the bitstream.

The length between I-frames is known as the group of pictures (GOP) size. MPEG-1 most commonly uses a GOP size of 15-18. i.e. 1 I-frame for every 14-17 non-I-frames (some combination of P- and B- frames). With more intelligent encoders, GOP size is dynamically chosen, up to some pre-selected maximum limit.

Limits are placed on the maximum number of frames between I-frames due to decoding complexing, decoder buffer size, recovery time after data errors, seeking ability, and accumulation of IDCT errors in low-precision implementations most common in hardware decoders (See: IEEE-1180).

P-frame is an abbreviation for Predicted-frame. They may also be called forward-predicted frames, or inter-frames (B-frames are also inter-frames).

P-frames exist to improve compression by exploiting the temporal (over time) redundancy in a video. P-frames store only the "difference" in image from the frame (either an I-frame or P-frame) immediately preceding it (this reference frame is also called the "anchor frame").

The difference between a P-frame and its anchor frame is calculated using "motion vectors" on each "macroblock" of the frame (see below). Such motion vector data will be embedded in the P-frame for use by the decoder.

A P-frame can contain any number of intra-coded blocks, in addition to any forward-predicted blocks.

If a video drastically changes from one frame to the next (such as a cut), it is more efficient to encode it as an I-frame.

B-frame stands for bidirectional-frame. They may also be known as backwards-predicted frames or B-pictures. B-frames are quite similar to P-frames, except they can make predictions using both the previous and future frames (i.e. two anchor frames).

It is therefore necessary for the player to first decode the next I- or P- anchor frame sequentially after the B-frame, before the B-frame can be decoded and displayed. This means decoding B-frames requires larger data buffers and causes an increased delay on both decoding and during encoding. This also necessitates the decoding time stamps (DTS) feature in the container/system stream (see above). As such, B-frames have long been subject of much controversy, they are often avoided in videos, and are sometimes not fully supported by hardware decoders.

No other frames are predicted from a B-frame. Because of this, a very low bitrate B-frame can be inserted, where needed, to help control the bitrate. If this was done with a P-frame, future P-frames would be predicted from it and would lower the quality of the entire sequence. However, similarly, the future P-frame must still encode all the changes between it and the previous I- or P- anchor frame. B-frames can also be beneficial in videos where the background behind an object is being revealed over several frames, or in fading transitions, such as scene changes.

A B-frame can contain any number of intra-coded blocks and forward-predicted blocks, in addition to backwards-predicted, or bidirectionally predicted blocks.

MPEG-1 has a unique frame type not found in later video standards. D-frames or DC-pictures are independent images (intra-frames) that have been encoded using DC transform coefficients only (AC coefficients are removed when encoding D-frames—see DCT below) and hence are very low quality. D-frames are never referenced by I-, P- or B- frames. D-frames are only used for fast previews of video, for instance when seeking through a video at high speed.

Given moderately higher-performance decoding equipment, fast preview can be accomplished by decoding I-frames instead of D-frames. This provides higher quality previews, since I-frames contain AC coefficients as well as DC coefficients. If the encoder can assume that rapid I-frame decoding capability is available in decoders, it can save bits by not sending D-frames (thus improving compression of the video content). For this reason, D-frames are seldom actually used in MPEG-1 video encoding, and the D-frame feature has not been included in any later video coding standards.

MPEG-1 operates on video in a series of 8x8 blocks for quantization. However, because chroma (color) is subsampled by a factor of 4, each pair of (red and blue) chroma blocks corresponds to 4 different luma blocks. This set of 6 blocks, with a resolution of 16x16, is called a macroblock.

A macroblock is the smallest independent unit of (color) video. Motion vectors (see below) operate solely at the macroblock level.

If the height or width of the video are not exact multiples of 16, full rows and full columns of macroblocks must still be encoded and decoded to fill out the picture (though the extra decoded pixels are not displayed).

To decrease the amount of temporal redundancy in a video, only blocks that change are updated, (up to the maximum GOP size). This is known as conditional replenishment. However, this is not very effective by itself. Movement of the objects, and/or the camera may result in large portions of the frame needing to be updated, even though only the position of the previously encoded objects has changed. Through motion estimation the encoder can compensate for this movement and remove a large amount of redundant information.

The encoder compares the current frame with adjacent parts of the video from the anchor frame (previous I- or P- frame) in a diamond pattern, up to a (encoder-specific) predefined radius limit from the area of the current macroblock. If a match is found, only the direction and distance (i.e. the "vector" of the "motion") from the previous video area to the current macroblock need to be encoded into the inter-frame (P- or B- frame). The reverse of this process, performed by the decoder to reconstruct the picture, is called motion compensation.

A predicted macroblock rarely matches the current picture perfectly, however. The differences between the estimated matching area, and the real frame/macroblock is called the prediction error. The larger the error, the more data must be additionally encoded in the frame. For efficient video compression, it is very important that the encoder is capable of effectively and precisely performing motion estimation.

Motion vectors record the "distance" between two areas on screen based on the number of pixels (called pels). MPEG-1 video uses a motion vector (MV) precision of one half of one pixel, or half-pel. The finer the precision of the MVs, the more accurate the match is likely to be, and the more efficient the compression. There are trade-offs to higher precision, however. Finer MVs result in larger data size, as larger numbers must be stored in the frame for every single MV, increased coding complexity as increasing levels of interpolation on the macroblock are required for both the encoder and decoder, and diminishing returns (minimal gains) with higher precision MVs. Half-pel was chosen as the ideal trade-off. (See: qpel)

Because neighboring macroblocks are likely to have very similar motion vectors, this redundant information can be compressed quite effectively by being stored DPCM-encoded. Only the (smaller) amount of difference between the MVs for each macroblock needs to be stored in the final bitstream.

P-frames have one motion vector per macroblock, relative to the previous anchor frame. B-frames, however, can use two motion vectors; one from the previous anchor frame, and one from the future anchor frame.

Partial macroblocks, and black borders/bars encoded into the video that do not fall exactly on a macroblock boundary, cause havoc with motion prediction. The block padding/border information prevents the macroblock from closely matching with any other area of the video, and so, significantly larger prediction error information must be encoded for every one of the several dozen partial macroblocks along the screen border. DCT encoding and quantization (see below) also isn't nearly as effective when there is large/sharp picture contrast in a block.

An even more serious problem exists with macroblocks that contain significant, random, "edge noise", where the picture transitions to (typically) black. All the above problems also apply to edge noise. In addition, the added randomness is simply impossible to compress significantly. All of these effects will lower the quality (or increase the bitrate) of the video substantially.

Each 8x8 block is encoded by first applying a "forward" discrete cosine transform (FDCT) and then a quantization process. The FDCT process (by itself) is theoretically lossless, and can be reversed by applying an "Inverse" DCT (IDCT) to reproduce the original values (in the absence of any quantization and rounding errors). In reality, there are some (sometimes large) rounding errors introduced both by quantization in the encoder (as described in the next section) and by IDCT approximation error in the decoder. The minimum allowed accuracy of a decoder IDCT approximation is defined by ISO/IEC 23002-1. (Prior to 2006, it was specified by IEEE 1180-1990.)

The FDCT process converts the 8x8 block of uncompressed pixel values (brightness or color difference values) into an 8x8 indexed array of "frequency coefficient" values. One of these is the (statistically high in variance) DC coefficient, which represents the average value of the entire 8x8 block. The other 63 coefficients are the statistically smaller AC coefficients, which are positive or negative values each representing sinusoidal deviations from the flat block value represented by the "DC coefficient".

An example of an encoded 8x8 FDCT block: 

Since the DC coefficient value is statistically correlated from one block to the next, it is compressed using DPCM encoding. Only the (smaller) amount of difference between each DC value and the value of the DC coefficient in the block to its left needs to be represented in the final bitstream.

Additionally, the frequency conversion performed by applying the DCT provides a statistical decorrelation function to efficiently concentrate the signal into fewer high-amplitude values prior to applying quantization (see below).

Quantization (of digital data) is, essentially, the process of reducing the accuracy of a signal, by dividing it into some larger step size (i.e. finding the nearest multiple, and discarding the remainder/modulus).

The frame-level quantizer is a number from 0 to 31 (although encoders will usually omit/disable some of the extreme values) which determines how much information will be removed from a given frame. The frame-level quantizer is either dynamically selected by the encoder to maintain a certain user-specified bitrate, or (much less commonly) directly specified by the user.

Contrary to popular belief, a fixed frame-level quantizer (set by the user) does not deliver a constant level of quality. Instead, it is an arbitrary metric that will provide a somewhat varying level of quality, depending on the contents of each frame. Given two files of identical sizes, the one encoded at an average bitrate should look better than the one encoded with a fixed quantizer (variable bitrate). Constant quantizer encoding can be used, however, to accurately determine the minimum and maximum bitrates possible for encoding a given video.

A quantization matrix is a string of 64-numbers (0-255) which tells the encoder how relatively important or unimportant each piece of visual information is. Each number in the matrix corresponds to a certain frequency component of the video image.

An example quantization matrix:

Quantization is performed by taking each of the 64 "frequency" values of the DCT block, dividing them by the frame-level quantizer, then dividing them by their corresponding values in the quantization matrix. Finally, the result is rounded down. This significantly reduces, or completely eliminates, the information in some frequency components of the picture. Typically, high frequency information is less visually important, and so high frequencies are much more "strongly quantized" (drastically reduced). MPEG-1 actually uses two separate quantization matrices, one for intra-blocks (I-blocks) and one for inter-block (P- and B- blocks) so quantization of different block types can be done independently, and so, more effectively.

This quantization process usually reduces a significant number of the "AC coefficients" to zero, (known as sparse data) which can then be more efficiently compressed by entropy coding (lossless compression) in the next step.

An example quantized DCT block:

Quantization eliminates a large amount of data, and is the main lossy processing step in MPEG-1 video encoding. This is also the primary source of most MPEG-1 video compression artifacts, like blockiness, color banding, noise, ringing, discoloration, et al. This happens when video is encoded with an insufficient bitrate, and the encoder is therefore forced to use high frame-level quantizers ("strong quantization") through much of the video.

Several steps in the encoding of MPEG-1 video are lossless, meaning they will be reversed upon decoding, to produce exactly the same (original) values. Since these lossless data compression steps don't add noise into, or otherwise change the contents (unlike quantization), it is sometimes referred to as noiseless coding. Since lossless compression aims to remove as much redundancy as possible, it is known as entropy coding in the field of information theory.

The coefficients of quantized DCT blocks tend to zero towards the bottom-right. Maximum compression can be achieved by a zig-zag scanning of the DCT block starting from the top left and using Run-length encoding techniques.

The DC coefficients and motion vectors are DPCM-encoded.

Run-length encoding (RLE) is a very simple method of compressing repetition. A sequential string of characters, no matter how long, can be replaced with a few bytes, noting the value that repeats, and how many times. For example, if someone were to say "five nines", you would know they mean the number: 99999.

RLE is particularly effective after quantization, as a significant number of the AC coefficients are now zero (called sparse data), and can be represented with just a couple of bytes. This is stored in a special 2-dimensional Huffman table that codes the run-length and the run-ending character.

Huffman Coding is a very popular method of entropy coding, and used in MPEG-1 video to reduce the data size. The data is analyzed to find strings that repeat often. Those strings are then put into a special table, with the most frequently repeating data assigned the shortest code. This keeps the data as small as possible with this form of compression. Once the table is constructed, those strings in the data are replaced with their (much smaller) codes, which reference the appropriate entry in the table. The decoder simply reverses this process to produce the original data.

This is the final step in the video encoding process, so the result of Huffman coding is known as the MPEG-1 video "bitstream."

I-frames store complete frame info within the frame and are therefore suited for random access. P-frames provide compression using motion vectors relative to the previous frame ( I or P ). B-frames provide maximum compression but require the previous as well as next frame for computation. Therefore, processing of B-frames requires more buffer on the decoded side. A configuration of the Group of Pictures (GOP) should be selected based on these factors. I-frame only sequences give least compression, but are useful for random access, FF/FR and editability. I- and P-frame sequences give moderate compression but add a certain degree of random access, FF/FR functionality. I-, P- and B-frame sequences give very high compression but also increase the coding/decoding delay significantly. Such configurations are therefore not suited for video-telephony or video-conferencing applications.

The typical data rate of an I-frame is 1 bit per pixel while that of a P-frame is 0.1 bit per pixel and for a B-frame, 0.015 bit per pixel.

Part 3 of the MPEG-1 standard covers audio and is defined in ISO/IEC-11172-3.

MPEG-1 Audio utilizes psychoacoustics to significantly reduce the data rate required by an audio stream. It reduces or completely discards certain parts of the audio that it deduces that the human ear can't "hear", either because they are in frequencies where the ear has limited sensitivity, or are "masked" by other (typically louder) sounds.

Channel Encoding:

MPEG-1 Audio is divided into 3 layers. Each higher layer is more computationally complex, and generally more efficient at lower bitrates than the previous. The layers are semi backwards compatible as higher layers reuse technologies implemented by the lower layers. A "Full" Layer II decoder can also play Layer I audio, but not Layer III audio, although not all higher level players are "full".

MPEG-1 Layer I is nothing more than a simplified version of Layer II. Layer I uses a smaller 384-sample frame size for very low delay, and finer resolution. This is advantageous for applications like teleconferencing, studio editing, etc. It has lower complexity than Layer II to facilitate real-time encoding on the hardware available circa 1990.

Layer I saw limited adoption in its time, and most notably was used on Philips' defunct Digital Compact Cassette at a bitrate of 384 kbit/s. With the substantial performance improvements in digital processing since its introduction, Layer I quickly became unnecessary and obsolete.

Layer I audio files typically use the extension .mp1 or sometimes .m1a

MPEG-1 Layer II (MP2—often incorrectly called MUSICAM) is a lossy audio format designed to provide high quality at about 192 kbit/s for stereo sound. Decoding MP2 audio is computationally simple, relative to MP3, AAC, etc.

MPEG-1 Layer II was derived from the MUSICAM ("Masking pattern adapted Universal Subband Integrated Coding And Multiplexing") audio codec, developed by Centre commun d'études de télévision et télécommunications (CCETT), Philips, and Institut für Rundfunktechnik (IRT/CNET) as part of the EUREKA 147 pan-European inter-governmental research and development initiative for the development of digital audio broadcasting.

Most key features of MPEG-1 Audio were directly inherited from MUSICAM, including the filter bank, time-domain processing, audio frame sizes, etc. However, improvements were made, and the actual MUSICAM algorithm was not used in the final MPEG-1 Layer II audio standard. The widespread usage of the term MUSICAM to refer to Layer II is entirely incorrect and discouraged for both technical and legal reasons.

Layer II/MP2 is a time-domain encoder. It uses a low-delay 32 sub-band polyphased filter bank for time-frequency mapping; having overlapping ranges (i.e. polyphased) to prevent aliasing. The psychoacoustic model is based on the principles of auditory masking, simultaneous masking effects, and the absolute threshold of hearing (ATH). The size of a Layer II frame is fixed at 1152-samples (coefficients).

Time domain refers to how analysis and quantization is performed on short, discrete samples/chunks of the audio waveform. This offers low delay as only a small number of samples are analyzed before encoding, as opposed to frequency domain encoding (like MP3) which must analyze many times more samples before it can decide how to transform and output encoded audio. This also offers higher performance on complex, random and transient impulses (such as percussive instruments, and applause), offering avoidance of artifacts like pre-echo.

The 32 sub-band filter bank returns 32 amplitude coefficients, one for each equal-sized frequency band/segment of the audio, which is about 700 Hz wide (depending on the audio's sampling frequency). The encoder then utilizes the psychoacoustic model to determine which sub-bands contain audio information that is less important, and so, where quantization will be inaudible, or at least much less noticeable.
The psychoacoustic model is applied using a 1024-point Fast Fourier Transform (FFT). Of the 1152 samples per frame, 64 samples at the top and bottom of the frequency range are ignored for this analysis. They are presumably not significant enough to change the result. The psychoacoustic model uses an empirically determined masking model to determine which sub-bands contribute more to the masking threshold, and how much quantization noise each can contain without being perceived. Any sounds below the absolute threshold of hearing (ATH) are completely discarded. The available bits are then assigned to each sub-band accordingly.

Typically, sub-bands are less important if they contain quieter sounds (smaller coefficient) than a neighboring (i.e. similar frequency) sub-band with louder sounds (larger coefficient). Also, "noise" components typically have a more significant masking effect than "tonal" components.

Less significant sub-bands are reduced in accuracy by quantization. This basically involves compressing the frequency range (amplitude of the coefficient), i.e. raising the noise floor. Then computing an amplification factor, for the decoder to use to re-expand each sub-band to the proper frequency range.

Layer II can also optionally use intensity stereo coding, a form of joint stereo. This means that the frequencies above 6 kHz of both channels are combined/down-mixed into one single (mono) channel, but the "side channel" information on the relative intensity (volume, amplitude) of each channel is preserved and encoded into the bitstream separately. On playback, the single channel is played through left and right speakers, with the intensity information applied to each channel to give the illusion of stereo sound. This perceptual trick is known as stereo irrelevancy. This can allow further reduction of the audio bitrate without much perceivable loss of fidelity, but is generally not used with higher bitrates as it does not provide very high quality (transparent) audio.

Subjective audio testing by experts, in the most critical conditions ever implemented, has shown MP2 to offer transparent audio compression at 256 kbit/s for 16-bit 44.1 kHz CD audio using the earliest reference implementation (more recent encoders should presumably perform even better). That (approximately) 1:6 compression ratio for CD audio is particularly impressive because it is quite close to the estimated upper limit of perceptual entropy, at just over 1:8. Achieving much higher compression is simply not possible without discarding some perceptible information.

MP2 remains a favoured lossy audio coding standard due to its particularly high audio coding performances on important audio material such as castanet, symphonic orchestra, male and female voices and particularly complex and high energy transients (impulses) like percussive sounds: triangle, glockenspiel and audience applause. More recent testing has shown that MPEG Multichannel (based on MP2), despite being compromised by an inferior matrixed mode (for the sake of backwards compatibility) rates just slightly lower than much more recent audio codecs, such as Dolby Digital (AC-3) and Advanced Audio Coding (AAC) (mostly within the margin of error—and substantially superior in some cases, such as audience applause). This is one reason that MP2 audio continues to be used extensively. The MPEG-2 AAC Stereo verification tests reached a vastly different conclusion, however, showing AAC to provide superior performance to MP2 at half the bitrate. The reason for this disparity with both earlier and later tests is not clear, but strangely, a sample of applause is notably absent from the latter test.

Layer II audio files typically use the extension .mp2 or sometimes .m2a

MPEG-1 Layer III (MP3) is a lossy audio format designed to provide acceptable quality at about 64 kbit/s for monaural audio over single-channel (BRI) ISDN links, and 128 kbit/s for stereo sound.

Layer III/MP3 was derived from the "Adaptive Spectral Perceptual Entropy Coding" (ASPEC) codec developed by Fraunhofer as part of the EUREKA 147 pan-European inter-governmental research and development initiative for the development of digital audio broadcasting. ASPEC was adapted to fit in with the Layer II/MUSICAM model (frame size, filter bank, FFT, etc.), to become Layer III.

ASPEC was itself based on "Multiple adaptive Spectral audio Coding" (MSC) by E. F. Schroeder, "Optimum Coding in the Frequency domain" (OCF) the doctoral thesis by Karlheinz Brandenburg at the University of Erlangen-Nuremberg, "Perceptual Transform Coding" (PXFM) by J. D. Johnston at AT&T Bell Labs, and "Transform coding of audio signals" by Y. Mahieux and J. Petit at Institut für Rundfunktechnik (IRT/CNET).

MP3 is a frequency-domain audio transform encoder. Even though it utilizes some of the lower layer functions, MP3 is quite different from Layer II/MP2.

MP3 works on 1152 samples like Layer II, but needs to take multiple frames for analysis before frequency-domain (MDCT) processing and quantization can be effective. It outputs a variable number of samples, using a bit buffer to enable this variable bitrate (VBR) encoding while maintaining 1152 sample size output frames. This causes a significantly longer delay before output, which has caused MP3 to be considered unsuitable for studio applications where editing or other processing needs to take place.

MP3 does not benefit from the 32 sub-band polyphased filter bank, instead just using an 18-point MDCT transformation on each output to split the data into 576 frequency components, and processing it in the frequency domain. This extra granularity allows MP3 to have a much finer psychoacoustic model, and more carefully apply appropriate quantization to each band, providing much better low-bitrate performance.

Frequency-domain processing imposes some limitations as well, causing a factor of 12 or 36 × worse temporal resolution than Layer II. This causes quantization artifacts, due to transient sounds like percussive events and other high-frequency events that spread over a larger window. This results in audible smearing and pre-echo. MP3 uses pre-echo detection routines, and VBR encoding, which allows it to temporarily increase the bitrate during difficult passages, in an attempt to reduce this effect. It is also able to switch between the normal 36 sample quantization window, and instead using 3× short 12 sample windows instead, to reduce the temporal (time) length of quantization artifacts. And yet in choosing a fairly small window size to make MP3's temporal response adequate enough to avoid the most serious artifacts, MP3 becomes much less efficient in frequency domain compression of stationary, tonal components.

Being forced to use a "hybrid" time domain (filter bank) /frequency domain (MDCT) model to fit in with Layer II simply wastes processing time and compromises quality by introducing aliasing artifacts. MP3 has an aliasing cancellation stage specifically to mask this problem, but which instead produces frequency domain energy which must be encoded in the audio. This is pushed to the top of the frequency range, where most people have limited hearing, in hopes the distortion it causes will be less audible.

Layer II's 1024 point FFT doesn't entirely cover all samples, and would omit several entire MP3 sub-bands, where quantization factors must be determined. MP3 instead uses two passes of FFT analysis for spectral estimation, to calculate the global and individual masking thresholds. This allows it to cover all 1152 samples. Of the two, it utilizes the global masking threshold level from the more critical pass, with the most difficult audio.

In addition to Layer II's intensity encoded joint stereo, MP3 can use middle/side (mid/side, m/s, MS, matrixed) joint stereo. With mid/side stereo, certain frequency ranges of both channels are merged into a single (middle, mid, L+R) mono channel, while the sound difference between the left and right channels is stored as a separate (side, L-R) channel. Unlike intensity stereo, this process does not discard any audio information. When combined with quantization, however, it can exaggerate artifacts.

If the difference between the left and right channels is small, the side channel will be small, which will offer as much as a 50% bitrate savings, and associated quality improvement. If the difference between left and right is large, standard (discrete, left/right) stereo encoding may be preferred, as mid/side joint stereo will not provide any benefits. An MP3 encoder can switch between m/s stereo and full stereo on a frame-by-frame basis.

Unlike Layers I/II, MP3 uses variable-length Huffman coding (after perceptual) to further reduce the bitrate, without any further quality loss.

These technical limitations inherently prevent MP3 from providing critically transparent quality at any bitrate. This makes Layer II sound quality actually superior to MP3 audio, when it is used at a high enough bitrate to avoid noticeable artifacts. The term "transparent" often gets misused, however. The quality of MP3 (and other codecs) is sometimes called "transparent," even at impossibly low bitrates, when what is really meant is "good quality on average/non-critical material," or perhaps "exhibiting only non-annoying artifacts."

MP3's more fine-grained and selective quantization does prove notably superior to Layer II/MP2 at lower-bitrates, however. It is able to provide nearly equivalent audio quality to Layer II, at a 15% lower bitrate (approximately). 128 kbit/s is considered the "sweet spot" for MP3; meaning it provides generally acceptable quality stereo sound on most music, and there are diminishing quality improvements from increasing the bitrate further. MP3 is also regarded as exhibiting artifacts that are less annoying than Layer II, when both are used at bitrates that are too low to possibly provide faithful reproduction.

Layer III audio files use the extension .mp3.
The MPEG-2 standard includes several extensions to MPEG-1 Audio. These are known as MPEG-2 BC – backwards compatible with MPEG-1 Audio. MPEG-2 Audio is defined in ISO/IEC 13818-3


These sampling rates are exactly half that of those originally defined for MPEG-1 Audio. They were introduced to maintain higher quality sound when encoding audio at lower-bitrates. The even-lower bitrates were introduced because tests showed that MPEG-1 Audio could provide higher quality than any existing (circa 1994) very low bitrate (i.e. speech) audio codecs.

Part 4 of the MPEG-1 standard covers conformance testing, and is defined in ISO/IEC-11172-4.

Conformance: Procedures for testing conformance.

Provides two sets of guidelines and reference bitstreams for testing the conformance of MPEG-1 audio and video decoders, as well as the bitstreams produced by an encoder.

Part 5 of the MPEG-1 standard includes reference software, and is defined in ISO/IEC TR 11172-5.

Simulation: Reference software.

C reference code for encoding and decoding of audio and video, as well as multiplexing and demultiplexing.

This includes the "ISO Dist10" audio encoder code, which LAME and TooLAME were originally based upon.

.mpg is one of a number of file extensions for MPEG-1 or MPEG-2 audio and video compression. MPEG-1 Part 2 video is rare nowadays, and this extension typically refers to an MPEG program stream (defined in MPEG-1 and MPEG-2) or MPEG transport stream (defined in MPEG-2). Other suffixes such as .m2ts also exists specifying the precise container, in this case MPEG-2 TS, but this has little relevance to MPEG-1 media.

.mp3 is the most common extension for files containing MPEG-1 Layer 3 audio. An MP3 file is typically an uncontained stream of raw audio; the conventional way to tag MP3 files is by writing data to "garbage" segments of each frame, which preserve the media information but are discarded by the player. This is similar in many respects to how raw .AAC files are tagged (but this is less supported nowadays, e.g. iTunes).

Note that although it would apply, .mpg does not normally append raw AAC or AAC in MPEG-2 Part 7 Containers. The .aac extension normally denotes these audio files.





</doc>
<doc id="20057" url="https://en.wikipedia.org/wiki?curid=20057" title="Mumia Abu-Jamal">
Mumia Abu-Jamal

Mumia Abu-Jamal (born Wesley Cook; April 24, 1954) is a political activist and journalist who was convicted of murder and sentenced to death in 1982 for the 1981 murder of Philadelphia police officer Daniel Faulkner. After numerous appeals, his sentence was commuted in 2011 to life imprisonment without parole.

A supporter of the MOVE Organization, Abu-Jamal was also a member of the Black Panther Party until October 1970 but left the party and became a radio reporter, eventually becoming president of the Philadelphia Association of Black Journalists. 

Some activists and human rights groups have criticized the quality of Abu-Jamal's trial; some have claimed that he is innocent, and many opposed his death sentence. The Faulkner family, public authorities, police organizations, and conservative groups have maintained that Abu-Jamal's trial was fair, his guilt undeniable, and his death sentence appropriate. He was described in 2001 as "perhaps the world's best known death-row inmate" by "The New York Times." During his imprisonment Abu-Jamal has published books and commentaries on social and political issues, including "Live from Death Row" (1995).

He was born Wesley Cook in Philadelphia, Pennsylvania, where he grew up. He has a younger brother named William. They attended local public schools.

In 1968, a high school teacher, a Kenyan instructing a class on African cultures, encouraged the students to take African or Arabic names for classroom use and named him "Mumia". According to Abu-Jamal, "Mumia" means "Prince" and was the name of a Kenyan anti-colonial African nationalist who fought against the British before Kenyan independence.

Before joining MOVE, Abu-Jamal was a reporter covering the organization.

When he joined MOVE, he said he had done so because of the love of the people in the organization. Thinking back on it later, he said he "was probably enraged as well".

Cook adopted the surname Abu-Jamal ("father of Jamal" in Arabic) after the birth of his son Jamal on July 18, 1971. His first marriage at age 19, to Jamal's mother Biba, was short-lived. Their daughter, Lateefa, was born shortly after the wedding. The couple divorced.

Abu-Jamal married his second wife, Marilyn (known as "Peachie"), in 1977. Their son, Mazi, was born in early 1978. By 1981, Abu-Jamal was living with his third (and current) wife, Wadiya.

In his own writings, Abu-Jamal describes his adolescent experience of being "kicked ... into the Black Panther Party" after suffering a beating from "white racists" and a policeman for his efforts to disrupt a George Wallace for President rally in 1968. From the age of 14, he helped form the Philadelphia branch of the Black Panther Party with Defense Captain Reggie Schell, and other Panthers. He was appointed as the chapter's "Lieutenant of Information," responsible for writing information and news communications. In an interview in the early years, Abu-Jamal quoted Mao Zedong, saying that "political power grows out of the barrel of a gun". That same year, he dropped out of Benjamin Franklin High School and began living at the branch's headquarters.

He spent late 1969 in New York City and early 1970 in Oakland, living and working with BPP colleagues in those cities; the party was founded in Oakland. He was a party member from May 1969 until October 1970. During this period, he was subject to Federal Bureau of Investigation COINTELPRO surveillance, with which the Philadelphia police cooperated, from then until about 1974. The FBI was working to infiltrate black radical groups and to disrupt them by creating internal dissension.

After leaving the Panthers, Abu-Jamal returned to his former high school. He was suspended for distributing literature calling for "black revolutionary student power". He led unsuccessful protests to change the school name to Malcolm X High. After attaining his GED, he studied briefly at Goddard College in rural Vermont.

By 1975 Abu-Jamal was pursuing a vocation in radio newscasting, first at Temple University's WRTI and then at commercial enterprises. In 1975, he was employed at radio station WHAT and he became host of a weekly feature program at WCAU-FM in 1978. He was also employed for brief periods at radio station WPEN, and became active in the local chapter of the Marijuana Users Association of America.

From 1979 to 1981 he worked at National Public Radio (NPR) affiliate WHYY; he was asked to resign, as management believed he did not maintain a sufficiently objective approach in his presentation of news. As a radio journalist, Abu-Jamal was renowned for identifying with and covering the MOVE anarcho-primitivist commune in Philadelphia's Powelton Village neighborhood. He reported on the 1979–80 trial of certain of its members (the "MOVE Nine"), who were convicted of the murder of police officer James Ramp. Abu-Jamal had several high-profile interviews, including with Julius Erving, Bob Marley and Alex Haley. He was elected president of the Philadelphia Association of Black Journalists.

In December 1981, Abu-Jamal was working as a taxicab driver in Philadelphia two nights a week to supplement his income. He had been working part-time as a reporter for WDAS, then an African-American-oriented and minority-owned radio station.

At 3:55 am on December 9, 1981, in Philadelphia, close to the intersection at 13th and Locust streets, Philadelphia Police Department officer Daniel Faulkner conducted a traffic stop on a vehicle belonging to and driven by William Cook, Abu-Jamal's younger brother. Faulkner and Cook became engaged in a physical confrontation. During the traffic stop, Abu-Jamal who was driving his cab in the vicinity, observed the altercation, parked, and ran across the street toward Cook's car. Faulkner was shot from behind and in the face. He shot Abu-Jamal in the stomach. Faulkner died at the scene from the gunshot to his head.

Police arrived and arrested Abu-Jamal, who was found wearing a shoulder holster. His revolver, which had five spent cartridges, was beside him. Abu-Jamal was taken directly from the scene of the shooting to Thomas Jefferson University Hospital, where he received treatment for his wound. He was then taken to Police
Headquarters where he was charged and held for trial in the first-degree murder of Officer Faulkner.

The prosecution presented four witnesses to the court. Robert Chobert, a cab driver who testified he was parked behind Faulkner, identified Abu-Jamal as the shooter. Cynthia White, a prostitute, testified that Abu-Jamal emerged from a nearby parking lot and shot Faulkner. Michael Scanlan, a motorist, testified that from two car lengths away, he saw a man, matching Abu-Jamal's description, run across the street from a parking lot and shoot Faulkner. Albert Magilton, a pedestrian who did not see the shooting, testified to seeing Faulkner pull over Cook's car. As Abu-Jamal started to cross the street toward them, Magilton turned away and lost sight of what happened next.

The prosecution presented two witnesses who were at the hospital after the shootings. Hospital security guard Priscilla Durham and police officer Garry Bell testified that Abu-Jamal confessed in the hospital by saying, "I shot the motherfucker, and I hope the motherfucker dies."

A .38 caliber Charter Arms revolver, belonging to Abu-Jamal, with five spent cartridges was retrieved beside him at the scene. He was wearing a shoulder holster. Anthony Paul, the Supervisor of the Philadelphia Police Department's firearms identification unit, testified at trial that the cartridge cases and rifling characteristics of the weapon were consistent with bullet fragments taken from Faulkner's body. Tests to confirm that Abu-Jamal had handled and fired the weapon were not performed. Contact with arresting police and other surfaces at the scene could have compromised the forensic value of such tests.

The defense maintained that Abu-Jamal was innocent and that the prosecution witnesses were unreliable. The defense presented nine character witnesses, including poet Sonia Sanchez, who testified that Abu-Jamal was "viewed by the black community as a creative, articulate, peaceful, genial man". Another defense witness, Dessie Hightower, testified that he saw a man running along the street shortly after the shooting, although he did not see the actual shooting. His testimony contributed to the development of a "running man theory", based on the possibility that a "running man" may have been the shooter. Veronica Jones also testified for the defense, but she did not see anyone running. Other potential defense witnesses refused to appear in court. Abu-Jamal did not testify in his own defense, nor did his brother, William Cook, who told investigators at the crime scene: "I ain't got nothing to do with this".

After three hours of deliberations, the jury presented a unanimous guilty verdict.

In the sentencing phase of the trial, Abu-Jamal read to the jury from a prepared statement. He was cross-examined about issues relevant to the assessment of his character by Joseph McGill, the prosecuting attorney.

In his statement, Abu-Jamal criticized his attorney as a "legal trained lawyer" who was imposed on him against his will and who "knew he was inadequate to the task and chose to follow the directions of this black-robed conspirator, Albert Sabo, even if it meant ignoring my directions." He claimed that his rights had been "deceitfully stolen" from him by Sabo, particularly focusing on the denial of his request to receive defense assistance from non-attorney John Africa and being prevented from proceeding "pro se". He quoted remarks of John Africa, and said:
Abu-Jamal was sentenced to death by the unanimous decision of the jury. Amnesty International has objected to the introduction by the prosecution of statements from his youth at the time of sentencing. It also protested the politicization of the trial, noting a documented recent history in Philadelphia of police abuse and corruption including fabricated evidence and use of excessive force. Amnesty International concluded "that the proceedings used to convict and sentence Mumia Abu-Jamal to death were in violation of minimum international standards that govern fair trial procedures and the use of the death penalty".

Direct appeal of his conviction was considered and denied by the Supreme Court of Pennsylvania on March 6, 1989, subsequently denying rehearing. The Supreme Court of the United States denied his petition for writ of "certiorari" on October 1, 1990, and denied his petition for rehearing twice up to June 10, 1991.

On June 1, 1995, Abu-Jamal's death warrant was signed by Pennsylvania Governor Tom Ridge. Its execution was suspended while Abu-Jamal pursued state post-conviction review. At the post-conviction review hearings, new witnesses were called. William "Dales" Singletary testified that he saw the shooting and that the gunman was the passenger in Cook's car. Singletary's account contained discrepancies which rendered it "not credible" in the opinion of the court. William Harmon, a convicted fraudster, testified that Faulkner's murderer fled in a car that pulled up to the crime scene, and could not have been Abu-Jamal. Robert Harkins testified that he had witnessed a man stand over Faulkner as the latter lay wounded on the ground, shoot Faulkner at point-blank in the face, and then "walked and sat down on the curb".

The six judges of the Supreme Court of Pennsylvania ruled unanimously that all issues raised by Abu-Jamal, including the claim of ineffective assistance of counsel, were without merit. The Supreme Court of the United States denied a petition for "certiorari" against that decision on October 4, 1999, enabling Ridge to sign a second death warrant on October 13, 1999. Its execution in turn was stayed as Abu-Jamal commenced his pursuit of federal "habeas corpus" review.

In 1999, Arnold Beverly claimed that he and an unnamed assailant, not Mumia Abu-Jamal, shot Daniel Faulkner as part of a contract killing because Faulkner was interfering with graft and payoff to corrupt police. The Beverly affidavit became an item of division for Mumia's defense team. Some thought it usable and others rejected Beverly's story as "not credible".

Private investigator George Newman claimed in 2001 that Chobert had recanted his testimony. Commentators also noted that police and news photographs of the crime scene did not show Chobert's taxi, and that Cynthia White, the only witness at the trial to testify to seeing the taxi, had previously provided crime scene descriptions that omitted it. Cynthia White was declared to be dead by the state of New Jersey in 1992, but Pamela Jenkins claimed that she saw White alive as late as 1997. Mumia supporters often claim that White was a police informant and that she falsified her testimony against Abu-Jamal.

Priscilla Durham's step-brother, Kenneth Pate, who was imprisoned with Abu-Jamal on other charges, has since claimed that Durham admitted she had not heard the "hospital confession" to which she had testified. The hospital doctors said that Abu-Jamal was "on the verge of fainting" when brought in and they did not hear any confession. In 2008, the Supreme Court of Pennsylvania rejected a further request from Abu-Jamal for a hearing into claims that the trial witnesses perjured themselves, on the grounds that he had waited too long before filing the appeal.

On March 26, 2012 the Supreme Court of Pennsylvania rejected his appeal for retrial that asserted, based on a 2009 report by the National Academy of Sciences, that forensic evidence presented by the prosecution and accepted into evidence in the original trial was unreliable. It was reported to be the former death row inmate's last legal appeal.

On April 30, 2018, it was ruled that Abu-Jamal would not be immediately granted another appeal and that the proceedings had to continue until August of that year. Philadelphia District Attorney (DA) Larry Krasner also could not find any document supporting the defense's claim that former Pennsylvania Supreme Court Chief justice and Philadelphia DA Ronald D. Castille showed bias in his judgement.

Abu-Jamal did not make any public statements about Faulkner's murder until May 2001. In his version of events, he claimed that he was sitting in his cab across the street when he heard shouting, then saw a police vehicle, and heard the sound of gunshots. Upon seeing his brother appearing disoriented across the street, Abu-Jamal ran to him from the parking lot and was shot by a police officer. Abu-Jamal's brother William Cook, who had been stopped by the police officer, did not testify or make any statement until April 29, 2001. At that time he said that he had not seen who had shot Faulkner.

Judge William H. Yohn, Jr. of the United States District Court for the Eastern District of Pennsylvania upheld the conviction but vacated the sentence of death on December 18, 2001, citing irregularities in the original process of sentencing. Particularly, he said that

He ruled that it was unconstitutional to require that a jury be unanimous in its finding of circumstances mitigating against determining a sentence of death. Eliot Grossman and Marlene Kamish, attorneys for Abu-Jamal, criticized the ruling on the grounds that it denied the possibility of a "trial de novo," at which they could introduce evidence that their client had been framed. Prosecutors also criticized the ruling; Officer Faulkner's widow Maureen described Abu-Jamal as a "remorseless, hate-filled killer" who would "be permitted to enjoy the pleasures that come from simply being alive" on the basis of the judgment. Both parties appealed.

On December 6, 2005, the Third Circuit Court of Appeals admitted four issues for appeal of the ruling of the District Court: 

The Third Circuit Court heard oral arguments in the appeals on May 17, 2007, at the United States Courthouse in Philadelphia. The appeal panel consisted of Chief Judge Anthony Joseph Scirica, Judge Thomas Ambro, and Judge Robert Cowen. The Commonwealth of Pennsylvania sought to reinstate the sentence of death, on the basis that Yohn's ruling was flawed, as he should have deferred to the Pennsylvania Supreme Court which had already ruled on the issue of sentencing, and the "Batson" claim was invalid because Abu-Jamal made no complaints during the original jury selection. Although Abu-Jamal's jury was racially mixed, with 2 blacks and 10 whites at the time of his unanimous conviction, his counsel told the Third Circuit Court that Abu-Jamal did not get a fair trial because the jury was racially biased, misinformed, and the judge was a racist. He noted that the prosecution used eleven out of fourteen peremptory challenges to eliminate prospective black jurors. Terri Maurer-Carter, a former Philadelphia court stenographer, claimed in a 2001 affidavit nearly 20 years after the trial that she overheard Judge Sabo say "Yeah, and I'm going to help them fry the nigger" in the course of a conversation with three people present regarding Abu-Jamal's case. Sabo denied having made any such comment.

On March 27, 2008, the three-judge panel issued a majority 2–1 opinion upholding Yohn's 2001 opinion but rejecting the bias and "Batson" claims, with Judge Ambro dissenting on the "Batson" issue. On July 22, 2008, Abu-Jamal's formal petition seeking reconsideration of the decision by the full Third Circuit panel of 12 judges was denied. On April 6, 2009, the United States Supreme Court also refused to hear Abu-Jamal's appeal, allowing his conviction to stand.

On January 19, 2010, the Supreme Court ordered the appeals court to reconsider its decision to rescind the death penalty, with the same three-judge panel convening in Philadelphia on November 9, 2010, to hear oral argument. On April 26, 2011, the Third Circuit Court of Appeals reaffirmed its prior decision to vacate the death sentence on the grounds that the jury instructions and verdict form were ambiguous and confusing. The Supreme Court declined to hear the case in October.

On December 7, 2011, District Attorney of Philadelphia R. Seth Williams announced that prosecutors, with the support of the victim's family, would no longer seek the death penalty for Abu-Jamal. Faulkner's widow said that she did not want to relive the trauma of another trial. She understood that it would be extremely difficult to present the case against Abu-Jamal again, after the passage of 30 years and the deaths of several key witnesses. Williams, the prosecutor, said that Abu-Jamal would spend the rest of his life in prison without the possibility of parole, a sentence that was reaffirmed by the Superior Court of Pennsylvania on July 9, 2013. After the press conference, Maureen Faulkner made a statement condemning Abu-Jamal:

In 1991 Abu-Jamal published an essay in the "Yale Law Journal", on the death penalty and his death row experience. In May 1994, Abu-Jamal was engaged by National Public Radio's "All Things Considered" program to deliver a series of monthly three-minute commentaries on crime and punishment. The broadcast plans and commercial arrangement were canceled following condemnations from, among others, the Fraternal Order of Police and U.S. Senator Bob Dole (Kansas Republican Party). Abu-Jamal sued NPR for not airing his work, but a federal judge dismissed the suit. His commentaries later were published in May 1995 as part of "Live from Death Row."

In 1999, Abu-Jamal was invited to record a keynote address for the graduating class at Evergreen State College in Washington State. The event was protested by some. In 2000, he recorded a commencement address for Antioch College. The now defunct New College of California School of Law presented him with an honorary degree "for his struggle to resist the death penalty." On October 5, 2014, he gave the commencement speech at Goddard College, via playback of a recording. As before, the choice of Abu-Jamal was controversial.

With occasional interruptions due to prison disciplinary actions, Abu-Jamal has for many years been a regular commentator on an online broadcast, sponsored by Prison Radio. He also is published as a regular columnist for "Junge Welt," a Marxist newspaper in Germany. For almost a decade, Abu-Jamal taught introductory courses in Georgist economics to other prisoners around the world by correspondence.

His publications include "Death Blossoms: Reflections from a Prisoner of Conscience", in which he explores religious themes; "All Things Censored", a political critique examining issues of crime and punishment; "Live From Death Row," a diary of life on Pennsylvania's death row; and "We Want Freedom: A Life in the Black Panther Party," a history of the Black Panthers drawing on autobiographical material.

In 1995, Abu-Jamal was punished with solitary confinement for engaging in entrepreneurship contrary to prison regulations. Subsequent to the airing of the 1996 HBO documentary "," which included footage from visitation interviews conducted with him, the Pennsylvania Department of Corrections acted to ban outsiders from using any recording equipment in state prisons. In litigation before the U.S. Court of Appeals, in 1998 he successfully established his right to write for financial gain in prison. The same litigation also established that the Pennsylvania Department of Corrections had illegally opened his mail in an attempt to establish whether he was writing for financial gain.

When, for a brief time in August 1999, he began delivering his radio commentaries live on the Pacifica Network's "Democracy Now!" weekday radio newsmagazine, prison staff severed the connecting wires of his telephone from their mounting in mid-performance. He was later allowed to resume his broadcasts, and hundreds of his broadcasts have been aired on Pacifica Radio.

At the end of January 2012, Abu-Jamal was released into general prison population at State Correctional Institution – Mahanoy. He went into diabetic shock on March 30, 2015 and has been diagnosed with active hepatitis C. In August 2015 his attorneys filed suit in the U.S. District Court for the Middle District of Pennsylvania, alleging that he has not received appropriate medical care for his serious health conditions.

Labor unions, politicians, advocates, educators, the NAACP Legal Defense and Educational Fund, and human rights advocacy organizations such as Human Rights Watch and Amnesty International have expressed concern about the impartiality of the trial of Abu-Jamal. Amnesty International neither takes a position on the guilt or innocence of Abu-Jamal nor classifies him as a political prisoner. They are opposed by the family of Daniel Faulkner, the Commonwealth of Pennsylvania, the City of Philadelphia, politicians, and the Fraternal Order of Police. In August 1999, the Fraternal Order of Police called for an economic boycott against all individuals and organizations that support Abu-Jamal.

Abu-Jamal has been made an honorary citizen of about 25 cities around the world, including Montreal, Palermo, and Paris.

In 2001, he received the sixth biennial Erich Mühsam Prize, named after an anarcho-communist essayist, which recognizes activism in line with that of its namesake. In October 2002, he was made an honorary member of the German political organization Society of People Persecuted by the Nazi Regime – Federation of Anti-Fascists (VVN-BdA)

On April 29, 2006, a newly paved road in the Parisian suburb of Saint-Denis was named Rue Mumia Abu-Jamal in his honor. In protest of the street-naming, U.S. Congressman Michael Fitzpatrick and Senator Rick Santorum, both members of the Republican Party of Pennsylvania, introduced resolutions in both Houses of Congress condemning the decision. The House of Representatives voted 368–31 in favor of Fitzpatrick's resolution. In December 2006, the 25th anniversary of the murder, the executive committee of the Republican Party for the 59th Ward of the City of Philadelphia—covering approximately Germantown, Philadelphia—filed two criminal complaints in the French legal system against the city of Paris and the city of Saint-Denis, accusing the municipalities of "glorifying" Abu-Jamal and alleging the offense "apology or denial of crime" in respect of their actions.
In 2007, the widow of Officer Faulkner coauthored a book with Philadelphia radio journalist Michael Smerconish titled "Murdered by Mumia: A Life Sentence of Pain, Loss, and Injustice." The book was part memoir of Faulkner's widow, part discussion in which they chronicled Abu-Jamal's trial and discussed evidence for his conviction, and part discussion on supporting the death penalty.

In early 2014, President Barack Obama nominated Debo Adegbile, a former lawyer for the NAACP Legal Defense Fund who worked on Abu-Jamal's case, to head the civil rights division of the Justice Department, but the nomination was rejected by the U.S. Senate on a bipartisan basis because of Adegbile's prior work on Abu-Jamal’s case.

In April 10, 2015 Marylin Zuniga, a teacher at Forest Street Elementary School in Orange, New Jersey, was suspended without pay after asking her students to write letters to Abu-Jamal, who fell ill in prison due to complications from diabetes, without approval from the school or parents. Some parents and police leaders denounced her actions. On the other hand, community members, parents, teachers, and professors expressed their support and condemned Zuniga's suspension. Scholars and educators nationwide including Noam Chomsky, Chris Hedges and Cornel West among others signed a letter calling for her immediate reinstatement. On May 13, 2015 The Orange Preparatory Academy board voted to dismiss Marylin Zuniga after hearing from her and several of her supporters.



Video

Supporter websites

Opponent websites


</doc>
<doc id="20059" url="https://en.wikipedia.org/wiki?curid=20059" title="Multiplicative function">
Multiplicative function

In number theory, a multiplicative function is an arithmetic function "f"("n") of a positive integer "n" with the property that "f"(1) = 1 and whenever
"a" and "b" are coprime, then

An arithmetic function "f"("n") is said to be completely multiplicative (or totally multiplicative) if "f"(1) = 1 and "f"("ab") = "f"("a")"f"("b") holds "for all" positive integers "a" and "b", even when they are not coprime.

Some multiplicative functions are defined to make formulas easier to write:


Other examples of multiplicative functions include many functions of importance in number theory, such as:


An example of a non-multiplicative function is the arithmetic function "r"("n") - the number of representations of "n" as a sum of squares of two integers, positive, negative, or zero, where in counting the number of ways, reversal of order is allowed. For example:

and therefore "r"(1) = 4 ≠ 1. This shows that the function is not multiplicative. However, "r"("n")/4 is multiplicative.

In the On-Line Encyclopedia of Integer Sequences, sequences of values of a multiplicative function have the keyword "mult".

See arithmetic function for some other examples of non-multiplicative functions.

A multiplicative function is completely determined by its values at the powers of prime numbers, a consequence of the fundamental theorem of arithmetic. Thus, if "n" is a product of powers of distinct primes, say "n" = "p" "q" ..., then 
"f"("n") = "f"("p") "f"("q") ...

This property of multiplicative functions significantly reduces the need for computation, as in the following examples for "n" = 144 = 2 · 3:

Similarly, we have:

In general, if "f"("n") is a multiplicative function and "a", "b" are any two positive integers, then

Every completely multiplicative function is a homomorphism of monoids and is completely determined by its restriction to the prime numbers.

If "f" and "g" are two multiplicative functions, one defines a new multiplicative function "f" * "g", the "Dirichlet convolution" of "f" and "g", by
where the sum extends over all positive divisors "d" of "n". 
With this operation, the set of all multiplicative functions turns into an abelian group; the identity element is "ε". Convolution is commutative, associative, and distributive over addition.

Relations among the multiplicative functions discussed above include:


The Dirichlet convolution can be defined for general arithmetic functions, and yields a ring structure, the Dirichlet ring. 

The Dirichlet convolution of two multiplicative functions is again multiplicative. A proof of this fact is given by the following expansion for relatively prime formula_11: 

More examples are shown in the article on Dirichlet series.

Let "A" = , the polynomial ring over the finite field with "q" elements. "A" is a principal ideal domain and therefore "A" is a unique factorization domain.

A complex-valued function formula_17 on "A" is called multiplicative if formula_18 whenever "f" and "g" are relatively prime.

Let "h" be a polynomial arithmetic function (i.e. a function on set of monic polynomials over "A"). Its corresponding Dirichlet series is defined to be

where for formula_20 set formula_21 if formula_22 and formula_23 otherwise.

The polynomial zeta function is then

Similar to the situation in , every Dirichlet series of a multiplicative function "h" has a product representation (Euler product):

where the product runs over all monic irreducible polynomials "P". For example, the product representation of the zeta function is as for the integers:

Unlike the classical zeta function, formula_27 is a simple rational function:

In a similar way, If "f" and "g" are two polynomial arithmetic functions, one defines "f" * "g", the "Dirichlet convolution" of "f" and "g", by

where the sum is over all monic divisors "d" of "m", or equivalently over all pairs ("a", "b") of monic polynomials whose product is "m". The identity formula_30 still holds.





</doc>
<doc id="20060" url="https://en.wikipedia.org/wiki?curid=20060" title="MPEG-2">
MPEG-2

MPEG-2 (a.k.a. H.222/H.262 as defined by the ITU) is a standard for "the generic coding of moving pictures and associated audio information". It describes a combination of lossy video compression and lossy audio data compression methods, which permit storage and transmission of movies using currently available storage media and transmission bandwidth. While MPEG-2 is not as efficient as newer standards such as H.264/AVC and H.265/HEVC, backwards compatibility with existing hardware and software means it is still widely used, for example in over-the-air digital television broadcasting and in the DVD-Video standard.

MPEG-2 is widely used as the format of digital television signals that are broadcast by terrestrial (over-the-air), cable, and direct broadcast satellite TV systems. It also specifies the format of movies and other programs that are distributed on DVD and similar discs. TV stations, TV receivers, DVD players, and other equipment are often designed to this standard. MPEG-2 was the second of several standards developed by the Moving Pictures Expert Group (MPEG) and is an international standard (ISO/IEC 13818). Parts 1 and 2 of MPEG-2 were developed in a collaboration with ITU-T, and they have a respective catalog number in the ITU-T Recommendation Series.

While MPEG-2 is the core of most digital television and DVD formats, it does not completely specify them. Regional institutions can adapt it to their needs by restricting and augmenting aspects of the standard. See Video profiles and levels.

MPEG-2 includes a Systems section, part 1, that defines two distinct, but related, container formats. One is the "transport stream", a data packet format designed to transmit one data packet in four ATM data packets for streaming digital video and audio over fixed or mobile transmission mediums, where the beginning and the end of the stream may not be identified, such as radio frequency, cable and linear recording mediums, examples of which include ATSC/DVB/ISDB/SBTVD broadcasting, and HDV recording on tape. The other is the "program stream", an extended version of the container format with less overhead than "transport stream". "Program stream" is designed for random access storage mediums such as hard disk drives, optical discs and flash memory.

"Transport stream" file formats include M2TS, which is used on Blu-ray discs, AVCHD on re-writable DVDs and HDV on compact flash cards. "Program stream" files include VOB on DVDs and Enhanced VOB on the short lived HD DVD. The standard MPEG-2 "transport stream" contains packets of 188 bytes. M2TS prepends each packet with 4 bytes containing a 2-bit copy permission indicator and 30-bit timestamp.

MPEG-2 Systems is formally known as ISO/IEC 13818-1 and as ITU-T Rec. H.222.0. ISO authorized the "SMPTE Registration Authority, LLC" as the registration authority for MPEG-2 format identifiers. The registration descriptor of MPEG-2 transport is provided by ISO/IEC 13818-1 in order to enable users of the standard to unambiguously carry data when its format is not necessarily a recognized international standard. This provision will permit the MPEG-2 transport standard to carry all types of data while providing for a method of unambiguous identification of the characteristics of the underlying private data.

The Video section, part 2 of MPEG-2, is similar to the previous MPEG-1 standard, but also provides support for interlaced video, the format used by analog broadcast TV systems. MPEG-2 video is not optimized for low bit-rates, especially less than 1 Mbit/s at standard definition resolutions. All standards-compliant MPEG-2 Video decoders are fully capable of playing back MPEG-1 Video streams conforming to the Constrained Parameters Bitstream syntax. MPEG-2/Video is formally known as ISO/IEC 13818-2 and as ITU-T Rec. H.262.

With some enhancements, MPEG-2 Video and Systems are also used in some HDTV transmission systems, and is the standard format for over-the-air ATSC digital television.

MPEG-2 introduces new audio encoding methods compared to MPEG-1:

The MPEG-2 Audio section, defined in Part 3 (ISO/IEC 13818-3) of the standard, enhances MPEG-1's audio by allowing the coding of audio programs with more than two channels, up to 5.1 multichannel. This method is backwards-compatible (also known as MPEG-2 BC), allowing MPEG-1 audio decoders to decode the two main stereo components of the presentation. MPEG-2 part 3 also defined additional bit rates and sample rates for MPEG-1 Audio Layer I, II and III.

MPEG-2 BC (backward compatible with MPEG-1 audio formats)

Part 7 (ISO/IEC 13818-7) of the MPEG-2 standard specifies a rather different, non-backwards-compatible audio format (also known as MPEG-2 NBC). Part 7 is referred to as MPEG-2 AAC. AAC is more efficient than the previous MPEG audio standards, and is in some ways less complicated than its predecessor, MPEG-1 Audio, Layer 3, in that it does not have the hybrid filter bank. It supports from 1 to 48 channels at sampling rates of 8 to 96 kHz, with multichannel, multilingual, and multiprogram capabilities. Advanced Audio is also defined in Part 3 of the MPEG-4 standard.

MPEG-2 NBC (Non-Backward Compatible)

MPEG-2 standards are published as parts of ISO/IEC 13818. Each part covers a certain aspect of the whole specification.

MPEG-2 evolved out of the shortcomings of MPEG-1.

MPEG-1's known weaknesses:

Sakae Okubo of NTT was also the ITU-T coordinator for developing the H.262/MPEG-2 Part 2 video coding standard and the requirements chairman in MPEG for the MPEG-2 set of standards.

.mpg, .mpeg, .m2v, .mp2, .mp3 are some of a number of filename extensions used for MPEG-1 or MPEG-2 audio and video file formats.

The DVD-Video standard uses MPEG-2 video, but imposes some restrictions:



HDV is a format for recording and playback of high-definition MPEG-2 video on a DV cassette tape.

MOD and TOD are recording formats for use in consumer digital file-based camcorders.

XDCAM is a professional file-based video recording format.

Application-specific restrictions on MPEG-2 video in the DVB standard:

Allowed resolutions for SDTV:
For HDTV:

The ATSC A/53 standard used in the United States, uses MPEG-2 video at the Main Profile @ High Level (MP@HL), with additional restrictions such as the maximum bitrate of 19.4 Mbit/s for broadcast television and 38.8 Mbit/s for cable television, 4:2:0 chroma subsampling format, and mandatory colorimetry information.

ATSC allows the following video resolutions, aspect ratios, and frame/field rates:

ATSC standard A/63 defines additional resolutions and aspect rates for 50 Hz (PAL) signal.

The ATSC specification and MPEG-2 allow the use of progressive frames, even within an interlaced video sequence. For example, a station that transmits 1080i60 video sequence can use a coding method where those 60 fields are coded with 24 progressive frames and metadata instructs the decoder to interlace them and perform 3:2 pulldown before display. This allows broadcasters to switch between 60 Hz interlaced (news, soap operas) and 24 Hz progressive (prime-time) content without ending the MPEG-2 sequence and introducing a several seconds of delay as the TV switches formats. This is the reason why 1080p30 and 1080p24 sequences allowed by the ATSC specification are not used in practice.

The 1080-line formats are encoded with 1920 × 1088 pixel luma matrices and 960 × 540 chroma matrices, but the last 8 lines are discarded by the MPEG-2 decoding and display process.

ATSC A/72 is the newest revision of ATSC standards for digital television, which allows the use of H.264/AVC video coding format and 1080p60 signal.

MPEG-2 audio was a contender for the ATSC standard during the DTV "Grand Alliance" shootout, but lost out to Dolby AC-3.

Technical features of MPEG-2 in ATSC are also valid for ISDB-T, except that in the main TS has aggregated a second program for mobile devices compressed in MPEG-4 H.264 AVC for video and AAC-LC for audio, mainly known as 1seg.

Commercial Blu-ray discs encode the first 10 second long "FBI anti-piracy warning" in MPEG-2 regardless of the rest of the disc's encoding. The feature film can also be in MPEG-2, which was common on early Blu-ray releases, but recent releases most often use H.264 or VC-1.

All MPEG-2 patents is officially expired and can be used freely.

MPEG LA, a private patent licensing organization, has acquired rights from over 20 corporations and one university to license a patent pool of approximately 640 worldwide patents, which it claims are the "essential" to use of MPEG-2 technology, although many of the patents have since expired. Where software patentability is upheld, the use of MPEG-2 requires the payment of licensing fees to the patent holders. Other patents are licensed by Audio MPEG, Inc. The development of the standard itself took less time than the patent negotiations. Patent pooling between essential and peripheral patent holders in the MPEG-2 pool is the subject of a study by the University of Wisconsin. Over half of the patents expired in 2012.

According to the MPEG-2 licensing agreement any use of MPEG-2 technology is subject to royalties. MPEG-2 encoders are subject to a royalty of $2.00 per unit, decoders are subject to a royalty of $2.00 per unit, and royalty-based sales of encoders and decoders are subject to different rules and $2.50 per unit. Also, any packaged medium (DVDs/Data Streams) is subject to licence fees according to length of recording/broadcast. 
A criticism of the MPEG-2 patent pool is that even though the number of patents will decrease from 1,048 to 416 by June 2013 the license fee has not decreased with the expiration rate of MPEG-2 patents. Since January 1, 2010, the MPEG-2 patent pool has remained at $2 for a decoding license and $2 for an encoding license. By 2015 more than 90% of the MPEG-2 patents will have expired but as long as there are one or more active patents in the MPEG-2 patent pool in either the country of manufacture or the country of sale the MPEG-2 license agreement requires that licensees pay a license fee that does not change based on the number of patents that have expired.

The last United States patent expired on .




</doc>
<doc id="20061" url="https://en.wikipedia.org/wiki?curid=20061" title="MPEG-3">
MPEG-3

MPEG-3 is the designation for a group of audio and video coding standards agreed upon by the Moving Picture Experts Group (MPEG) designed to handle HDTV signals at 1080p in the range of 20 to 40 megabits per second. MPEG-3 was launched as an effort to address the need of an HDTV standard while work on MPEG-2 was underway, but it was soon discovered that MPEG-2, at high data rates, would accommodate HDTV. Thus, in 1992 HDTV was included as a separate profile in the MPEG-2 standard and MPEG-3 was rolled into MPEG-2.



</doc>
<doc id="20062" url="https://en.wikipedia.org/wiki?curid=20062" title="Meditation">
Meditation

Meditation can be defined as a practice where an individual uses a technique, such as focusing their mind on a particular object, thought or activity, to achieve a mentally clear and emotionally calm state.

Meditation has been practiced since antiquity in numerous religious traditions and beliefs. Since the 19th century, it has spread from its origins to other cultures where it is commonly practiced in private and business life.

Meditation may be used with the aim of reducing stress, anxiety, depression, and pain, and increasing peace, perception and wellbeing. Meditation is under research to define its possible health (psychological, neurological, and cardiovascular) and other effects.

The English "meditation" is derived from the Latin "meditatio", from a verb "meditari", meaning "to think, contemplate, devise, ponder".

In the Old Testament, "hāgâ" (Hebrew: "הגה") means to sigh or murmur, and also, to meditate. When the Hebrew Bible was translated into Greek, "hāgâ" became the Greek "melete". The Latin Bible then translated "hāgâ"/"melete" into "meditatio".
The use of the term "meditatio" as part of a formal, stepwise process of meditation goes back to the 12th-century monk Guigo II.

In addition within specific contexts more precise meanings are not uncommonly given the word "meditation". For example, "meditation" is sometimes the translation of "meditatio" in Latin. "Meditatio" is the second of four steps of "Lectio Divina", an ancient form of Christian prayer. "Meditation" also refers to the seventh of the eight limbs of Yoga in Patanjali's "Yoga Sutras", a step called "dhyāna" in Sanskrit. Meditation refers to a mental or spiritual "state" that may be attained by such practices, and also refers to the "practice" of that state.

Apart from its historical usage, the term "meditation" was introduced as a translation for Eastern spiritual practices, referred to as "dhyāna" in Buddhism and in Hinduism, which comes from the Sanskrit root "dhyai", meaning to contemplate or meditate. The term "meditation" in English may also refer to practices from Islamic Sufism, or other traditions such as Jewish Kabbalah and Christian Hesychasm. An edited book about "meditation" published in 2003, for example, included chapter contributions by authors describing Hindu, Buddhist, Taoist, Jewish, Christian, and Islamic traditions. Scholars have noted that "the term 'meditation' as it has entered contemporary usage" is parallel to the term "contemplation" in Christianity, but in many cases, practices similar to modern forms of meditation were simply called "prayer". Christian, Judaic, and Islamic forms of meditation are typically devotional, scriptural or thematic, while Asian forms of meditation are often more purely technical.

In popular usage, the word "meditation" and the phrase "meditative practice" are often used imprecisely to designate broadly similar practices, or sets of practices, that are found across many cultures and traditions.

A 2009 study of views common to seven experts trained in diverse but empirically highly studied (clinical or Eastern-derived) forms of meditation identified ""three main criteria... as essential to any meditation practice: the use of a defined technique, logic relaxation, and a self-induced state/mode. Other criteria deemed important <nowiki>[but not essential]</nowiki> involve a state of psychophysical relaxation, the use of a self-focus skill or anchor, the presence of a state of suspension of logical thought processes, a religious/spiritual/philosophical context, or a state of mental silence."" However, the study cautioned, ""It is plausible that meditation is best thought of as a natural category of techniques best captured by 'family resemblances'... or by the related 'prototype' model of concepts.""

The table shows several other definitions of meditation that have been used by influential modern reviews of research on meditation across multiple traditions.

In modern psychological research, meditation has been defined and characterized in a variety of ways; many of these emphasize the role of attention. Scientific reviews have proposed that researchers attempt to more clearly define the type of meditation being practiced in order that the results of their studies be made clearer.

Definitions in the Oxford and Cambridge living dictionaries are "to focus one's mind for a period of time" and "the act of giving your attention to only one thing."

Some of the difficulty in precisely defining meditation has been the need to recognize the particularities of the many various traditions. There may be differences between the theories of one tradition of meditation as to what it means to practice meditation. The differences between the various traditions themselves, which have grown up a great distance apart from each other, may be even starker. Taylor noted that to refer only to meditation from a particular faith (e.g., "Hindu" or "Buddhist")

Ornstein noted that ""Most techniques of meditation do not exist as solitary practices but are only artificially separable from an entire system of practice and belief."" This means that, for instance, while monks engage in meditation as a part of their everyday lives, they also engage the codified rules and live together in monasteries in specific cultural settings that go along with their meditative practices. These meditative practices sometimes have similarities (often noticed by Westerners), for instance concentration on the breath is practiced in Zen, Tibetan and Theravadan contexts, and these similarities or "typologies" are noted here.

This article mainly focuses on meditation in the broad sense of a type of technique, found in various forms in many cultures, by which the practitioner attempts to get beyond the reflexive, "thinking" mind (sometimes called "discursive thinking" or "logic") This may be to achieve a deeper, more devout, or more relaxed state. The terms "meditative practice" and "meditation" are mostly used here in this broad sense. However, usage may vary somewhat by context – readers should be aware that in quotations, or in discussions of particular traditions, more specialized meanings of "meditation" may sometimes be used (with meanings made clear by context whenever possible).

In the West, meditation techniques have sometimes been thought of in two broad categories: focused (or concentrative) meditation and open monitoring (or mindfulness) meditation. 

"One style, Focused Attention (FA) meditation, entails the voluntary focusing of attention on a chosen object, breathing, image, or words. The other style, Open Monitoring (OM) meditation, involves non-reactive monitoring of the content of experience from moment to moment."
"Direction of mental attention... A practitioner can focus intensively on one particular object (so-called "concentrative meditation"), on all mental events that enter the field of awareness (so-called "mindfulness meditation"), or both specific focal points and the field of awareness."
These include paying attention to the breath, to an idea or feeling (such as mettā (loving-kindness)), or to a mantra (such as in transcendental meditation), and single point meditation.

These include mindfulness, shikantaza and other awareness states.

Some practices use both techniques, including vipassana (which uses anapanasati as a preparation), samatha/calm-abiding, and Headspace.

In these methods, ""the practitioner is fully alert, aware, and in control of their faculties but does not experience any unwanted thought activity."" This is in contrast to the common meditative approaches of being detached from, and non-judgmental of, thoughts, but not of aiming for thoughts to cease. In the meditation practice of the Sahaja yoga spiritual movement, the focus is on thoughts ceasing. Clear light yoga also aims at a state of no mental content, as does the no thought (wu nian) state taught by Huineng, and the teaching of Yaoshan Weiyan.

One proposal is that transcendental meditation and possibly other techniques be grouped as an 'automatic self-transcending' set of techniques.

Other typologies include dividing meditation into concentrative, generative, receptive and reflective practices.

Evidence from neuroimaging studies suggests that the categories of meditation, defined by how they direct attention, appear to generate different brainwave patterns. Evidence also suggests that using different focus objects during meditation may generate different brainwave patterns.

The transcendental meditation technique recommends practice of 20 minutes twice per day. Some techniques suggest less time, especially when starting meditation, and Richie Davidson has quoted research saying benefits can be achieved with a practice of only 8 minutes per day. Some meditators practice for much longer, particularly when on a course or retreat. Some meditators find practice best in the hours before dawn.

Whilst positions such as the full-lotus, half-lotus, Burmese, Seiza, and kneeling positions are popular in Buddhism, Jainism and Hinduism, other postures such as sitting, supine (lying), and standing are also used. Meditation is also sometimes done while walking, known as kinhin, or while doing a simple task mindfully, known as samu.

Some ancient religions of the world have a tradition of using prayer beads as tools in devotional meditation. Most prayer beads and Christian rosaries consist of pearls or beads linked together by a thread. The Roman Catholic rosary is a string of beads containing five sets with ten small beads. The Hindu japa mala has 108 beads (the figure 108 in itself having spiritual significance, as well as those used in Jainism and Buddhist prayer beads. Each bead is counted once as a person recites a mantra until the person has gone all the way around the mala. The Muslim misbaha has 99 beads.

Richie Davidson has expressed the view that having a narrative can help maintenance of daily practice. For instance he himself prostrates to the teachings, and meditates "not primarily for my benefit, but for the benefit of others."

In Jainism, meditation has been a core spiritual practice, one that Jains believe people have undertaken since the teaching of the Tirthankara, Rishabha. All the twenty-four Tirthankaras practiced deep meditation and attained enlightenment. They are all shown in meditative postures in the images or idols. Mahavira practiced deep meditation for twelve years and attained enlightenment. The Acaranga Sutra dating to 500 BCE, addresses the meditation system of Jainism in detail. Acharya Bhadrabahu of the 4th century BCE practiced deep "Mahaprana" meditation for twelve years. Kundakunda of 1st century BCE, opened new dimensions of meditation in Jain tradition through his books "Samayasāra", "Pravachansar" and others. The 8th century Jain philosopher Haribhadra also contributed to the development of Jain yoga through his Yogadṛṣṭisamuccaya, which compares and analyzes various systems of yoga, including Hindu, Buddhist and Jain systems.

Jain meditation and spiritual practices system were referred to as salvation-path. It has three important parts called the "Ratnatraya" "Three Jewels": right perception and faith, right knowledge and right conduct. Meditation in Jainism aims at realizing the self, attaining salvation, take the soul to complete freedom. It aims to reach and to remain in the pure state of soul which is believed to be pure consciousness, beyond any attachment or aversion. The practitioner strives to be just a knower-seer (Gyata-Drashta). Jain meditation can be broadly categorized to "Dharmya Dhyana" and "Shukla Dhyana".

There exists a number of meditation techniques such as "pindāstha-dhyāna, padāstha-dhyāna, rūpāstha-dhyāna, rūpātita-dhyāna, savīrya-dhyāna", etc. In "padāstha dhyāna" one focuses on Mantra. A Mantra could be either a combination of core letters or words on deity or themes. There is a rich tradition of Mantra in Jainism. All Jain followers irrespective of their sect, whether Digambara or Svetambara, practice mantra. Mantra chanting is an important part of daily lives of Jain monks and followers. Mantra chanting can be done either loudly or silently in mind. Yogasana and "Pranayama" has been an important practice undertaken since ages. Pranayama – breathing exercises – are performed to strengthen the five "Pranas" or vital energy. Yogasana and "Pranayama" balances the functioning of neuro-endocrine system of body and helps in achieving good physical, mental and emotional health.

Contemplation is a very old and important meditation technique. The practitioner meditates deeply on subtle facts. In "agnya vichāya", one contemplates on seven facts – life and non-life, the inflow, bondage, stoppage and removal of "karmas", and the final accomplishment of liberation. In "apaya vichāya", one contemplates on the incorrect insights one indulges, which eventually develops right insight. In "vipaka vichāya", one reflects on the eight causes or basic types of "karma". In "sansathan vichāya", one thinks about the vastness of the universe and the loneliness of the soul.

Acharya Mahapragya formulated Preksha meditation in the 1970s and presented a well-organised system of meditation. Asana and "Pranayama", meditation, contemplation, mantra and therapy are its integral parts. Numerous Preksha meditation centers came into existence around the world and numerous meditations camps are being organized to impart training in it.

Buddhist meditation refers to the meditative practices associated with the religion and philosophy of Buddhism. Core meditation techniques have been preserved in ancient Buddhist texts and have proliferated and diversified through teacher-student transmissions. Buddhists pursue meditation as part of the path toward awakening and nirvana. The closest words for meditation in the classical languages of Buddhism are "bhāvanā", "jhāna"/"dhyāna", and "vipassana".

Buddhist meditation techniques have become increasingly popular in the wider world, with many non-Buddhists taking them up for a variety of reasons. There is considerable homogeneity across meditative practices – such as breath meditation and various recollections ("anussati") – that are used across Buddhist schools, as well as significant diversity. In the Theravāda tradition alone, there are over fifty methods for developing mindfulness and forty for developing concentration, while in the Tibetan tradition there are thousands of visualization meditations. Most classical and contemporary Buddhist meditation guides are school-specific.

The Buddha is said to have identified two paramount mental qualities that arise from wholesome meditative practice:

According to Buddhist theory, through the meditative development of serenity, one is able to weaken the obscuring hindrances and bring the mind to a collected, pliant and still state (samadhi). This quality of mind then supports the development of insight and wisdom (Prajñā) which is the quality of mind that can "clearly see" ("vi-passana") the nature of phenomena. According to the Buddhist tradition, all phenomena are to be seen as impermanent, suffering, not-self and empty. When this happens, one develops dispassion ("viraga") for all phenomena, including all negative qualities and hindrances and lets them go. It is through the release of the hindrances and ending of craving through the meditative development of insight that one gains liberation.

In the modern era, Buddhist meditation saw increasing popularity due to the influence of Buddhist modernism and the lay meditation based Vipassana movement. The spread of Buddhist meditation to the Western world paralleled the spread of Buddhism in the West. Buddhist meditation has also influenced Western Psychology, especially through the work of Jon Kabat-Zinn who founded the Mindfulness-Based Stress Reduction (MBSR) in 1979. The modernized concept of mindfulness (based on the Buddhist term "sati") and related meditative practices has in turn led to several mindfulness based therapies.

There are many schools and styles of meditation within Hinduism.

In pre-modern and traditional Hindu religions, "Yoga" and "Dhyana" are done to realize union of one's eternal self or soul, one's ātman. In some Hindu traditions, such as Advaita Vedanta this is equated with the omnipresent and non-dual Brahman. In others, such as the dualistic the Yoga school and Samkhya, the Self is referred to as Purusha, a pure consciousness which is separate from matter. Depending on the tradition, this liberative event is referred to as moksha, vimukti or kaivalya.

The earliest clear references to meditation in Hindu literature are in the middle Upanishads and the Mahabharata, the latter of which includes the Bhagavad Gita. According to Gavin Flood, the earlier Brihadaranyaka Upanishad refers to meditation when it states that "having become calm and concentrated, one perceives the self ("ātman") within oneself".

One of the most influential texts of classical Hindu Yoga is Patañjali's Yoga sutras (c. 400 CE), a text associated with Yoga and Samkhya, which outlines eight limbs leading to kaivalya ("aloneness"). These are ethical discipline (yamas), rules (niyamas), physical postures (āsanas), breath control (prāṇāyama), withdrawal from the senses (pratyāhāra), one-pointedness of mind (dhāraṇā), meditation (dhyāna), and finally samādhi.

Later developments in Hind meditation include the compilation of Hatha Yoga (forceful yoga) compendiums like the Hatha Yoga Pradipika, the development of Bhakti yoga as a major form of meditation and Tantra. Another important Hindu yoga text is the Yoga Yajnavalkya, which makes use of Hatha Yoga and Vedanta Philosophy.

In the sixth chapter of Bhāvārthadipikā commentary on the Bhagavad-Gita by Sri Jñāneśvar (Dnyaneshwar) meditation in yoga is described as a state caused by the spontaneous awakening of the sacred energy Kundalini (not Prana or Chi), which creates a connection of the individual soul Ātman with universal Spirit - Paramātman.

Meditation is used in modern Hindu religious movements.

In Sikhism, simran (meditation) and good deeds are both necessary to achieve the devotee's Spiritual goals; without good deeds meditation is futile. When Sikhs meditate, they aim to feel God's presence and immerge in the divine light. It is only God's divine will or order that allows a devotee to desire to begin to meditate. Guru Nanak in the Japji Sahib daily Sikh scripture explains:Visits to temples, penance, compassion and charity gain you but a sesame seed of credit. It is hearkening to His Name, accepting and adoring Him that obtains emancipation by bathing in the shrine of soul. All virtues are Yours, O Lord! I have none; Without good deeds one can't even meditate.Nām Japnā involves focusing one's attention on the names or great attributes of God.

Taoist or Daoist meditation has a long history, and has developed various techniques including concentration, visualization, "qi" cultivation, contemplation, and mindfulness meditations. Traditional Daoist meditative practices were influenced by Chinese Buddhism beginning around the 5th century, and later had influence upon Traditional Chinese medicine and the Chinese martial arts.

Livia Kohn distinguishes three basic types of Daoist meditation: "concentrative", "insight", and "visualization". "Ding" 定 (literally means "decide; settle; stabilize") refers to "deep concentration", "intent contemplation", or "perfect absorption". "Guan" 觀 (lit. "watch; observe; view") meditation seeks to merge and attain unity with the Dao. It was developed by Tang Dynasty (618–907) Daoist masters based upon the "Tiantai" Buddhist practice of "Vipassanā" "insight" or "wisdom" meditation. "Cun" 存 (lit. "exist; be present; survive") has a sense of "to cause to exist; to make present" in the meditation techniques popularized by the Daoist Shangqing and Lingbao Schools. A meditator visualizes or actualizes solar and lunar essences, lights, and deities within his/her body, which supposedly results in health and longevity, even "xian" 仙/仚/僊, "immortality".

The (late 4th century BCE) "Guanzi" essay "Neiye" "Inward training" is the oldest received writing on the subject of "qi" cultivation and breath-control meditation techniques. For instance, "When you enlarge your mind and let go of it, when you relax your vital breath and expand it, when your body is calm and unmoving: And you can maintain the One and discard the myriad disturbances. ... This is called "revolving the vital breath": Your thoughts and deeds seem heavenly."

The (c. 3rd century BCE) Daoist "Zhuangzi" records "zuowang" or "sitting forgetting" meditation. Confucius asked his disciple Yan Hui to explain what "sit and forget" means: "I slough off my limbs and trunk, dim my intelligence, depart from my form, leave knowledge behind, and become identical with the Transformational Thoroughfare."

Daoist meditation practices are central to Chinese martial arts (and some Japanese martial arts), especially the "qi"-related "neijia" "internal martial arts". Some well-known examples are "daoyin" "guiding and pulling", "qigong" "life-energy exercises", "neigong" "internal exercises", "neidan" "internal alchemy", and "taijiquan" "great ultimate boxing", which is thought of as moving meditation. One common explanation contrasts "movement in stillness" referring to energetic visualization of "qi" circulation in "qigong" and "zuochan" "seated meditation", versus "stillness in movement" referring to a state of meditative calm in "taijiquan" forms.

In the teachings of the Bahá'í Faith, meditation along with prayer are both primary tools for spiritual development and mainly refer to one's reflection on the words of God. While prayer and meditation are linked, where meditation happens generally in a prayerful attitude, prayer is seen specifically as turning toward God, and meditation is seen as a communion with one's self where one focuses on the divine.

The Bahá'í teachings note that the purpose of meditation is to strengthen one's understanding of the words of God, and to make one's soul more susceptible to their potentially transformative power, more receptive to the need for both prayer and meditation to bring about and maintain a spiritual communion with God.

Bahá'u'lláh, the founder of the religion, never specified any particular form of meditation, and thus each person is free to choose their own form. However, he specifically did state that Bahá'ís should read a passage of the Bahá'í writings twice a day, once in the morning, and once in the evening, and meditate on it. He also encouraged people to reflect on one's actions and worth at the end of each day. During the Nineteen Day Fast, a period of the year during which Bahá'ís adhere to a sunrise-to-sunset fast, they meditate and pray to reinvigorate their spiritual forces.

There is evidence that Judaism has had meditative practices that go back thousands of years. For instance, in the Torah, the patriarch Isaac is described as going ""לשוח"" ("lasuach") in the field—a term understood by all commentators as some type of meditative practice (Genesis 24:63).

Similarly, there are indications throughout the Tanakh (the Hebrew Bible) that meditation was used by the prophets. In the Old Testament, there are two Hebrew words for meditation: "hāgâ" (), which means "to sigh" or "murmur", but also "to meditate", and "sîḥâ" (), which means "to muse", or "rehearse in one's mind".

Some meditative traditions have been encouraged in the school of Judaism known as Kabbalah, and some Jews have described Kabbalah as an inherently meditative field of study. Aryeh Kaplan has argued that, for the Kabbalist, the ultimate purpose of meditative practice is to understand and cleave to the Divine. Classic methods include the mental visualisation of the supernal realms the soul navigates through to achieve certain ends. One of the best known types of meditation in early Jewish mysticism was the work of the Merkabah, from the root /R-K-B/ meaning "chariot" (of God).

Meditation has been of interest to a wide variety of modern Jews. In modern Jewish practice, one of the best known meditative practices is called ""hitbodedut"" ("התבודדות", alternatively transliterated as "hisbodedus"), and is explained in Kabbalistic, Hasidic, and Mussar writings, especially the Hasidic method of Rabbi Nachman of Breslav. The word derives from the Hebrew word "boded" (בודד), meaning the state of being alone. Another Hasidic system is the Habad method of "hisbonenus", related to the Sephirah of "Binah", Hebrew for understanding. This practice is the analytical reflective process of making oneself understand a mystical concept well, that follows and internalises its study in Hasidic writings.

The Musar Movement, founded by Rabbi Israel Salanter in the middle of the nineteenth-century, emphasized meditative practices of introspection and visualization that could help to improve moral character.

Jewish Buddhists have adopted Buddhist styles of meditation.

Christian meditation is a term for a form of prayer in which a structured attempt is made to get in touch with and deliberately reflect upon the revelations of God. The word meditation comes from the Latin word "meditari", which means to concentrate. Christian meditation is the process of deliberately focusing on specific thoughts (e.g. a biblical scene involving Jesus and the Virgin Mary) and reflecting on their meaning in the context of the love of God.

The Rosary is a devotion for the meditation of the mysteries of Jesus and Mary.“The gentle repetition of its prayers makes it an excellent means to moving into deeper meditation. It gives us an opportunity to open ourselves to God’s word, to refine our interior gaze by turning our minds to the life of Christ. The first principle is that meditation is learned through practice. Many people who practice rosary meditation begin very simply and gradually develop a more sophisticated meditation. The meditator learns to hear an interior voice, the voice of God”.

Christian meditation contrasts with Eastern forms of meditation as radically as the portrayal of God the Father in the Bible contrasts with depictions of Krishna or Brahman in Indian teachings. Unlike Eastern meditations, most styles of Christian meditations do not rely on the repeated use of mantras, and yet are also intended to stimulate thought and deepen meaning. Christian meditation aims to heighten the personal relationship based on the love of God that marks Christian communion.

In "Aspects of Christian meditation", the Catholic Church warned of potential incompatibilities in mixing Christian and Eastern styles of meditation. In 2003, in "A Christian reflection on the New Age" the Vatican announced that the "Church avoids any concept that is close to those of the New Age".

Christian meditation is sometimes taken to mean the middle level in a broad three stage characterization of prayer: it then involves more reflection than first level vocal prayer, but is more structured than the multiple layers of contemplation in Christianity.

In Frankfurt, Germany in 2007 the Centre for Christian Meditation and Spirituality in the Holy Cross Church, Frankfurt-Bornheim was founded by the Roman Catholic Diocese of Limburg. In and by the centre different kinds of church services are offered like for example with elements such as expressionist dance, moreover days of exercises of Christian mysticism, contemplative prayer, meditative singing, meditation courses, Zen-meditation courses, days of reflection, spiritual exercises and retreats

Early studies on states of consciousness conducted by Roland Fischer found evidence of mystical experience description in the writings of Saint Teresa of Avila. In her autobiography she describes that, at the peak of a praying experience "... the soul neither hears nor sees nor feels. While it lasts, none of the senses perceives or knows what is taking place". This corresponds to the fourth stage described by Saint Teresa, "Devotion of Ecstasy", where the consciousness of being in the body disappears, as an effect of deep transcendent meditation in prayer.

Salah is a mandatory act of devotion performed by Muslims five times per day. The body goes through sets of different postures, as the mind attains a level of concentration called "khushu"'.

A second optional type of meditation, called dhikr, meaning remembering and mentioning God, is interpreted in different meditative techniques in Sufism or Islamic mysticism. This became one of the essential elements of Sufism as it was systematized traditionally. It is juxtaposed with "fikr" (thinking) which leads to knowledge. By the 12th century, the practice of Sufism included specific meditative techniques, and its followers practiced breathing controls and the repetition of holy words.

Numerous Sufi traditions place emphasis upon a meditative procedure which comes from the cognitive aspect to one of the two principal approaches to be found in the Buddhist traditions: that of the concentration technique, involving high-intensity and sharply focused introspection. In the Oveyssi-Shahmaghsoudi Sufi order, for example, this is particularly evident, where muraqaba takes the form of tamarkoz, the latter being a Persian term that means "concentration".

"Tafakkur" or "tadabbur" in Sufism literally means "reflection upon the universe": this is considered to permit access to a form of cognitive and emotional development that can emanate only from the higher level, i.e. from God. The sensation of receiving divine inspiration awakens and liberates both heart and intellect, permitting such inner growth that the apparently mundane actually takes on the quality of the infinite. Muslim teachings embrace life as a test of one's submission to God.

Religions and religious movements which use magic, such as Wicca, Thelema, Neopaganism, occultism etc., often require their adherents to meditate as a preliminary to the magical work. This is because magic is often thought to require a particular state of mind in order to make contact with spirits, or because one has to visualize one's goal or otherwise keep intent focused for a long period during the ritual in order to see the desired outcome. Meditation practice in these religions usually revolves around visualization, absorbing energy from the universe or higher self, directing one's internal energy, and inducing various trance states. Meditation and magic practice often overlap in these religions as meditation is often seen as merely a stepping stone to supernatural power, and the meditation sessions may be peppered with various chants and spells.

Mantra meditation, with the use of a japa mala and especially with focus on the Hare Krishna maha-mantra, is a central practice of the Gaudiya Vaishnava faith tradition and the International Society for Krishna Consciousness (ISKCON), also known as the Hare Krishna movement. Other popular New Religious Movements include the Ramakrishna Mission, Vedanta Society, Divine Light Mission, Chinmaya Mission, Osho, Sahaja Yoga, Transcendental Meditation, Oneness University, Brahma Kumaris and Vihangam Yoga.

New Age meditations are often influenced by Eastern philosophy, mysticism, yoga, Hinduism and Buddhism, yet may contain some degree of Western influence. In the West, meditation found its mainstream roots through the social revolution of the 1960s and 1970s, when many of the youth of the day rebelled against traditional religion as a reaction against what some perceived as the failure of Christianity to provide spiritual and ethical guidance.
New Age meditation as practised by the early hippies is regarded for its techniques of blanking out the mind and releasing oneself from conscious thinking. This is often aided by repetitive chanting of a mantra, or focusing on an object. New Age meditation evolved into a range of purposes and practices, from serenity and balance to access to other realms of consciousness to the concentration of energy in group meditation to the supreme goal of samadhi, as in the ancient yogic practice of meditation.

Over the past 20 years, mindfulness and mindfulness-based programs have been used to assist people, whether they be clinically sick or healthy. Jon Kabat-Zinn, who founded the Mindfulness-Based Stress Reduction program in 1979, has defined mindfulness as 'moment to moment non-judgmental awareness.' Several methods are used during time set aside specifically for mindfulness meditation, such as body scan techniques or letting thought arise and pass, and also during our daily lives, such as being aware of the taste and texture of the food that we eat. Some studies offer evidence that mindfulness practices are beneficial for the brain's self-regulation by increasing activity in the anterior cingulate cortex. A shift from using the right prefrontal cortex is claimed to be associated with a trend away from depression and anxiety, and towards happiness, relaxation, and emotional balance.

As stated by the National Center for Complementary and Alternative Medicine, a US government entity within the National Institutes of Health that advocates various forms of Alternative Medicine, ""Meditation may be practiced for many reasons, such as to increase calmness and physical relaxation, to improve psychological balance, to cope with illness, or to enhance overall health and well-being.""

Meditation techniques have also been used by Western theories of counseling and psychotherapy. Relaxation training works toward achieving mental and muscle relaxation to reduce daily stresses. 
Sahaja (mental silence) meditators scored above control group for emotional well-being and mental health measures on SF-36 ratings.

Jacobson's Progressive Muscle Relaxation was developed by American physician Edmund Jacobson in the early 1920s. In this practice one tenses and then relaxes muscle groups in a sequential pattern whilst concentrating on how they feel. The method has been seen to help people with many conditions, especially extreme anxiety.
Jacobson is credited with developing the initial progressive relaxation procedure. These techniques are used in conjunction with other behavioral techniques. Originally used with systematic desensitization, relaxation techniques are now used with other clinical problems. Meditation, hypnosis and biofeedback-induced relaxation are a few of the techniques used with relaxation training.

One of the eight essential phases of EMDR (developed by Francine Shapiro), bringing adequate closure to the end of each session, also entails the use of relaxation techniques, including meditation. Multimodal therapy, a technically eclectic approach to behavioral therapy, also employs the use of meditation as a technique used in individual therapy.
From the point of view of psychology and physiology, meditation can induce an altered state of consciousness. Such altered states of consciousness may correspond to altered neuro-physiologic states.

Today, there are many different types of meditation practiced in western culture. Mindful breathing, progressive muscle relaxation, and loving kindness meditations for instance have been found to provide cognitive benefits such as relaxation and decentering. With training in meditation, depressive rumination can be decreased and overall peace of mind can flourish. Different techniques have shown to work better for different people.

Herbert Benson of Harvard Medical School conducted a series of clinical tests on meditators from various disciplines, including the Transcendental Meditation technique and Tibetan Buddhism. In 1975, Benson published a book titled "The Relaxation Response" where he outlined his own version of meditation for relaxation. Also in the 1970s, the American psychologist Patricia Carrington developed a similar technique called Clinically Standardized Meditation (CSM). In Norway, another sound-based method called Acem Meditation developed a psychology of meditation and has been the subject of several scientific studies.

Biofeedback has been used by many researchers since the 1950s in an effort to enter deeper states of mind.

The history of meditation is intimately bound up with the religious context within which it was practiced. Some authors have even suggested the hypothesis that the emergence of the capacity for focused attention, an element of many methods of meditation, may have contributed to the latest phases of human biological evolution. Some of the earliest references to meditation are found in the Hindu Vedas of India. Wilson translates the most famous Vedic mantra "Gayatri" as: "We meditate on that desirable light of the divine Savitri, who influences our pious rites" (Rigveda : Mandala-3, Sukta-62, Rcha-10). Around the 6th to 5th centuries BCE, other forms of meditation developed via Confucianism and Taoism in China as well as Hinduism, Jainism, and early Buddhism in Nepal and India.

In the Roman Empire, by 20 BCE Philo of Alexandria had written on some form of "spiritual exercises" involving attention (prosoche) and concentration and by the 3rd century Plotinus had developed meditative techniques.

The Pāli Canon, which dates to 1st century BCE considers Buddhist meditation as a step towards liberation. By the time Buddhism was spreading in China, the "Vimalakirti Sutra" which dates to 100 CE included a number of passages on meditation, clearly pointing to Zen (known as Chan in China, Thiền in Vietnam, and Seon in Korea). The Silk Road transmission of Buddhism introduced meditation to other Asian countries, and in 653 the first meditation hall was opened in Singapore. Returning from China around 1227, Dōgen wrote the instructions for zazen.

The Islamic practice of Dhikr had involved the repetition of the 99 Names of God since the 8th or 9th century. By the 12th century, the practice of Sufism included specific meditative techniques, and its followers practiced breathing controls and the repetition of holy words. Interactions with Indians, Nepalese or the Sufis may have influenced the Eastern Christian meditation approach to hesychasm, but this can not be proved. Between the 10th and 14th centuries, hesychasm was developed, particularly on Mount Athos in Greece, and involves the repetition of the Jesus prayer.

Western Christian meditation contrasts with most other approaches in that it does not involve the repetition of any phrase or action and requires no specific posture. Western Christian meditation progressed from the 6th century practice of Bible reading among Benedictine monks called Lectio Divina, i.e. divine reading. Its four formal steps as a "ladder" were defined by the monk Guigo II in the 12th century with the Latin terms "lectio", "meditatio", "oratio", and "contemplatio" (i.e. read, ponder, pray, contemplate). Western Christian meditation was further developed by saints such as Ignatius of Loyola and Teresa of Avila in the 16th century.
Secular forms of meditation were introduced in India in the 1950s as a modern form of Hindu meditative techniques and arrived in Australia in the late 1950s and, the United States and Europe in the 1960s. Rather than focusing on spiritual growth, secular meditation emphasizes stress reduction, relaxation and self-improvement. Both spiritual and secular forms of meditation have been subjects of scientific analyses. Research on meditation began in 1931, with scientific research increasing dramatically during the 1970s and 1980s. Since the beginning of the '70s more than a thousand studies of meditation in English have been reported. However, after 60 years of scientific study, the exact mechanism at work in meditation remains unclear.

Methods of meditation have been cross-culturally disseminated at various times throughout history, such as Buddhism going to East Asia, and Sufi practices going to many Islamic societies. Of special relevance to the modern world is the dissemination of meditative practices since the late 19th century, accompanying increased travel and communication among cultures worldwide. Most prominent has been the transmission of numerous Asian-derived practices to the West. In addition, interest in some Western-based meditative practices has also been revived, and these have been disseminated to a limited extent to Asian countries. Also evident is some extent of influence over Enlightenment thinking through Denis Diderot's "Encyclopédie", although he states, "I find that a meditation practitioner is often quite useless and that a contemplation practitioner is always insane".

Ideas about Eastern meditation had begun "seeping into American popular culture even before the American Revolution through the various sects of European occult Christianity", and such ideas "came pouring in [to America] during the era of the transcendentalists, especially between the 1840s and the 1880s." The following decades saw further spread of these ideas to America:

The World Parliament of Religions, held in Chicago in 1893, was the landmark event that increased Western awareness of meditation. This was the first time that Western audiences on American soil received Asian spiritual teachings from Asians themselves. Thereafter, Swami Vivekananda... [founded] various Vedanta ashrams... Anagarika Dharmapala lectured at Harvard on Theravada Buddhist meditation in 1904; Abdul Baha ... [toured] the US teaching the principles of Bahai, and Soyen Shaku toured in 1907 teaching Zen...

More recently, in the 1960s, another surge in Western interest in meditative practices began. Observers have suggested many types of explanations for this interest in Eastern meditation and revived Western contemplation. Thomas Keating, a founder of Contemplative Outreach, wrote that "the rush to the East is a symptom of what is lacking in the West. There is a deep spiritual hunger that is not being satisfied in the West." Daniel Goleman, a scholar of meditation, suggested that the shift in interest from "established religions" to meditative practices "is caused by the scarcity of the personal experience of these [meditation-derived] transcendental states – the living spirit at the common core of all religions."

Another suggested contributing factor is the rise of communist political power in Asia, which "set the stage for an influx of Asian spiritual teachers to the West", oftentimes as refugees.

A 2010 review of the literature on spirituality and performance in organizations found an increase in corporate meditation programs.

As of 2016 around a quarter of U.S. employers were using stress reduction initiatives. The goal was to help reduce stress and improve reactions to stress. Aetna now offers its program to its customers. Google also implements mindfulness, offering more than a dozen meditation courses, with the most prominent one, "Search Inside Yourself", having been implemented since 2007. General Mills offers the Mindful Leadership Program Series, a course which uses a combination of mindfulness meditation, yoga and dialog with the intention of developing the mind's capacity to pay attention.

Research on the processes and effects of meditation is a subfield of neurological research. Modern scientific techniques, such as fMRI and EEG, were used to observe neurological responses during meditation. Since the 1950s, hundreds of studies on meditation have been conducted, though the overall methological quality of meditation research is poor, yielding unreliable results.

Since the 1970s, clinical psychology and psychiatry have developed meditation techniques for numerous psychological conditions. Mindfulness practice is employed in psychology to alleviate mental and physical conditions, such as reducing depression, stress, and anxiety. Mindfulness is also used in the treatment of drug addiction. Studies demonstrate that meditation has a moderate effect to reduce pain. There is insufficient evidence for any effect of meditation on positive mood, attention, eating habits, sleep, or body weight.

A 2017 systematic review and meta-analysis of the effects of meditation on empathy, compassion, and prosocial behaviors found that meditation practices had small to medium effects on self-reported and observable outcomes, concluding that such practices can "improve positive prosocial emotions and behaviors".

Preliminary studies showed a potential relationship between meditation and job performance, resulting from cognitive and social effects.

Concerns have been raised on the quality of much meditation research, including the particular characteristics of individuals who tend to participate.

Many major traditions in which meditation is practiced, such as Buddhism and Hinduism, advise members not to consume intoxicants, while others, such as the Rastafarian movements and Native American Church, view drugs as integral to their religious lifestyle.

The fifth of the five precepts of the Pancasila, the ethical code in the Theravada and Mahayana Buddhist traditions, states that adherents must: "abstain from fermented and distilled beverages that cause heedlessness."

On the other hand, the ingestion of psychoactives has been a central feature in the rituals of many religions, in order to produce altered states of consciousness. In several traditional shamanistic ceremonies, drugs are used as agents of ritual. In the Rastafari movement, cannabis is believed to be a gift from Jah and a sacred herb to be used regularly, while alcohol is considered to debase man. Native Americans use peyote, as part of religious ceremony, continuing today. In India, the soma drink has a long history of use alongside prayer and sacrifice, and is mentioned in the Vedas.

During the 1960s and 70s, both eastern meditation traditions and psychedelics, such as LSD, became popular in America, and it was suggested that LSD use and meditation were both means to the same spiritual/existential end. Many practitioners of eastern traditions rejected this idea, including many who had tried LSD themselves. In "The Master Game", Robert S de Ropp writes that the "door to full consciousness" can be glimpsed with the aid of substances, but to "pass beyond the door" requires yoga and meditation. Other authors, such as Rick Strassman, believe that the relationship between religious experiences reached by way of meditation and through the use of psychedelic drugs deserves further exploration.

The 2012 US National Health Interview Survey (NHIS) (n = 34,525), found 8.0% of US adults used meditation, with lifetime and 12-month prevalence of meditation use of 5.2% and 4.1% respectively. In the 2017 survey meditation use among workers was 9.9% (up from 8.0% in 2002).

Meditation is regarded by some as encouraging narcissistic and self-obsessed mindsets which can be unhealthy.

There has been some reporting of cases where meditating correlated with negative experiences for the meditator.




</doc>
<doc id="20063" url="https://en.wikipedia.org/wiki?curid=20063" title="MPEG-4">
MPEG-4

MPEG-4 is a method of defining compression of audio and visual (AV) digital data. It was introduced in late 1998 and designated a standard for a group of audio and video coding formats and related technology agreed upon by the ISO/IEC Moving Picture Experts Group (MPEG) (ISO/IEC JTC1/SC29/WG11) under the formal standard ISO/IEC 14496 – "Coding of audio-visual objects". Uses of MPEG-4 include compression of AV data for web (streaming media) and CD distribution, voice (telephone, videophone) and broadcast television applications.

MPEG-4 absorbs many of the features of MPEG-1 and MPEG-2 and other related standards, adding new features such as (extended) VRML support for 3D rendering, object-oriented composite files (including audio, video and VRML objects), support for externally specified Digital Rights Management and various types of interactivity. AAC (Advanced Audio Coding) was standardized as an adjunct to MPEG-2 (as Part 7) before MPEG-4 was issued.

MPEG-4 is still an evolving standard and is divided into a number of parts. Companies promoting MPEG-4 compatibility do not always clearly state which "part" level compatibility they are referring to. The key parts to be aware of are MPEG-4 Part 2 (including Advanced Simple Profile, used by codecs such as DivX, Xvid, Nero Digital and 3ivx and by QuickTime 6) and MPEG-4 part 10 (MPEG-4 AVC/H.264 or Advanced Video Coding, used by the x264 encoder, Nero Digital AVC, QuickTime 7, and high-definition video media like Blu-ray Disc).

Most of the features included in MPEG-4 are left to individual developers to decide whether or not to implement. This means that there are probably no complete implementations of the entire MPEG-4 set of standards. To deal with this, the standard includes the concept of "profiles" and "levels", allowing a specific set of capabilities to be defined in a manner appropriate for a subset of applications.

Initially, MPEG-4 was aimed primarily at low bit-rate video communications; however, its scope as a multimedia coding standard was later expanded. MPEG-4 is efficient across a variety of bit-rates ranging from a few kilobits per second to tens of megabits per second. MPEG-4 provides the following functions:

MPEG-4 provides a series of technologies for developers, for various service-providers and for end users:


The MPEG-4 format can perform various functions, among which might be the following:


MPEG-4 provides a large and rich set of tools for encoding.
Subsets of the MPEG-4 tool sets have been provided for use in specific applications.
These subsets, called 'Profiles', limit the size of the tool set a decoder is required to implement. In order to restrict computational complexity, one or more 'Levels' are set for each Profile. A Profile and Level combination allows:

MPEG-4 consists of several standards—termed "parts"—including the following (each part covers a certain aspect of the whole specification):

Profiles are also defined within the individual "parts", so an implementation of a part is ordinarily not an implementation of an entire part.

MPEG-1, MPEG-2, MPEG-7 and MPEG-21 are other suites of MPEG standards.

The low profile levels are part of the MPEG-4 video encoding/decoding constraints and are compatible with the older ITU H.261 standard, also compatible with former analog TV standards for broadcast and records (such as NTSC or PAL video). The ASP profile in its highest level is suitable for most usual DVD medias and players or for many online video sites, but not for Blu-ray records or online HD video contents.

More advanced profiles for HD media have been defined later in the AVC profile, which is functionally identical to the ITU H.264 standard but are now also integrated in MPEG-4 Part 10 (see H.264/MPEG-4 AVC for the list of defined levels in this AVC profile).

MPEG-4 contains patented technologies, the use of which requires licensing in countries that acknowledge software algorithm patents. Over two dozen companies claim to have patents covering MPEG-4. MPEG LA licenses patents required for MPEG-4 Part 2 Visual from a wide range of companies (audio is licensed separately) and lists all of its licensors and licensees on the site. New licenses for MPEG-4 System patents are under development and no new licenses are being offered while holders of its old MPEG-4 Systems license are still covered under the terms of that license for the patents listed (MPEG LA – Patent List).

AT&T is trying to sue companies such as Apple Inc. over alleged MPEG-4 patent infringement. The terms of Apple's QuickTime 7 license for users describes in paragraph 14 the terms under Apple's existing MPEG-4 System Patent Portfolio license from MPEG LA.




</doc>
<doc id="20064" url="https://en.wikipedia.org/wiki?curid=20064" title="Maritime archaeology">
Maritime archaeology

Maritime archaeology (also known as marine archaeology) is a discipline within archaeology as a whole that specifically studies human interaction with the sea, lakes and rivers through the study of associated physical remains, be they vessels, shore-side facilities, port-related structures, cargoes, human remains and submerged landscapes. A specialty within maritime archaeology is nautical archaeology, which studies ship construction and use.

As with archaeology as a whole, maritime archaeology can be practised within the historical, industrial, or prehistoric periods. An associated discipline, and again one that lies within archaeology itself, is underwater archaeology, which studies the past through any submerged remains be they of maritime interest or not. An example from the prehistoric era would be the remains of submerged settlements or deposits now lying under water despite having been dry land when sea levels were lower. The study of submerged aircraft lost in lakes, rivers or in the sea is an example from the historical, industrial or modern era. Many specialist sub-disciplines within the broader maritime and underwater archaeological categories have emerged in recent years.

Maritime archaeological sites often result from shipwrecks or sometimes seismic activity, and thus represent a moment in time rather than a slow deposition of material accumulated over a period of years, as is the case with port-related structures (such as piers, wharves, docks and jetties) where objects are lost or thrown off structures over extended periods of time. This fact has led to shipwrecks often being described in the media and in popular accounts as 'time capsules'.

Archaeological material in the sea or in other underwater environments is typically subject to different factors than artifacts on land. However, as with terrestrial archaeology, what survives to be investigated by modern archaeologists can often be a tiny fraction of the material originally deposited. A feature of maritime archaeology is that despite all the material that is lost, there are occasional rare examples of substantial survival, from which a great deal can be learned, due to the difficulties often experienced in accessing the sites.

There are those in the archaeology community who see maritime archaeology as a separate discipline with its own concerns (such as shipwrecks) and requiring the specialized skills of the underwater archaeologist. Others value an integrated approach, stressing that nautical activity has economic and social links to communities on land and that archaeology is archaeology no matter where the study is conducted. All that is required is the mastering of skills specific to the environment in which the work occurs.

Before the industrial era, travel by water was often easier than over land. As a result, marine channels, navigable rivers and sea crossings formed the trade routes of historic and ancient civilisations. For example, the Mediterranean Sea was known to the Romans as the inner sea because the Roman empire spread around its coasts. The historic record as well as the remains of harbours, ships and cargoes, testify to the volume of trade that crossed it. Later, nations with a strong maritime culture such as the United Kingdom, the Netherlands, Denmark, Portugal and Spain were able to establish colonies on other continents. Wars were fought at sea over the control of important resources. The material cultural remains that are discovered by maritime archaeologists along former trade routes can be combined with historical documents and material cultural remains found on land to understand the economic, social and political environment of the past. Of late maritime archaeologists have been examining the submerged cultural remains of China, India, Korea and other Asian nations.

There are significant differences in the survival of archaeological material depending on whether a site is wet or dry, on the nature of the chemical environment, on the presence of biological organisms and on the dynamic forces present. Thus rocky coastlines, especially in shallow water, are typically inimical to the survival of artifacts, which can be dispersed, smashed or ground by the effect of currents and surf, possibly (but not always) leaving an artifact pattern but little if any wreck structure.

Saltwater is particularly inimical to iron artefacts including metal shipwrecks, and sea organisms will readily consume organic material such as wooden shipwrecks. On the other hand, out of all the thousands of potential archaeological sites destroyed or grossly eroded by such natural processes, occasionally sites survive with exceptional preservation of a related collection of artifacts. An example of such a collection is . Survival in this instance is largely due to the remains being buried in sediment

Of the many examples where the sea bed provides an extremely hostile environment for submerged evidence of history, one of the most notable, , though a relatively young wreck and in deep water so calcium-starved that concretion does not occur, appears strong and relatively intact, though indications are that it has already incurred irreversible degradation of her steel and iron hull. As such degradation inevitably continues, data will be forever lost, objects' context will be destroyed and the bulk of the wreck will over centuries completely deteriorate on the floor of the Atlantic Ocean. Comparative evidence shows that all iron and steel ships, especially those in a highly oxygenated environment, continue to degrade and will continue to do so until only their engines and other machinery project much above the sea-floor. Where it remains even after the passage of time, the iron or steel hull is often fragile with no remaining metal within the layer of concretion and corrosion products. , having been found in the 1970s, was subjected to a program of attempted "in situ" preservation, for example, but deterioration of the vessel progressed at such a rate that the rescue of her turret was undertaken lest nothing be saved from the wreck.

Some wrecks, lost to natural obstacles to navigation, are at risk of being smashed by subsequent wrecks sunk by the same hazard, or are deliberately destroyed because they present a hazard to navigation. Even in deep water, commercial activities such as pipe-laying operations and deep sea trawling can place a wreck at risk. Such a wreck is the "Mardi Gras" shipwreck sunk in the Gulf of Mexico in of water. The shipwreck lay forgotten at the bottom of the sea until it was discovered in 2002 by an oilfield inspection crew working for the Okeanos Gas Gathering Company (OGGC). Large pipelines can crush sites and render some of their remnants inaccessible as pipe is dropped from the ocean surface to the substrate thousands of feet below. Trawl nets snag and tear superstructures and separate artifacts from their context.
The wrecks, and other archaeological sites that have been preserved have generally survived because the dynamic nature of the sea bed can result in artifacts becoming rapidly buried in sediments. These sediments then provide an anaerobic environment which protects from further degradation. Wet environments, whether on land in the form of peat bogs and wells, or underwater are particularly important for the survival of organic material, such as wood, leather, fabric and horn. Cold and absence of light also aid survival of artifacts, because there is little energy available for either organic activity or chemical reactions. Salt water provides for greater organic activity than freshwater, and in particular, the shipworm, terredo navalis, lives only in salt water, so some of the best preservation in the absence of sediments has been found in the cold, dark waters of the Great Lakes in North America and in the (low salinity) Baltic Sea (where "Vasa" was preserved).

While the land surface is continuously reused by societies, the sea bed was largely inaccessible until the advent of submarines, scuba equipment and remotely operated underwater vehicles (ROVs) in the twentieth century. Salvagers have operated in much earlier times, but much of the material was beyond the reach of anyone. Thus "Mary Rose" was subject to salvage from the sixteenth century and later, but a very large amount of material, buried in the sediments, remained to be found by maritime archaeologists of the twentieth century.

While preservation in situ is not assured, material that has survived underwater and is then recovered to land is typically in an unstable state and can only be preserved using highly specialised conservation processes. While the wooden structure of "Mary Rose" and the individual artifacts have been undergoing conservation since their recovery, provides an example of a relatively recent (metal) wreck for which extensive conservation has been necessary to preserve the hull. While the hull remains intact, its machinery remains inoperable. The engine of that was recovered in 1985 from a saline environment after over a century underwater is presently considered somewhat anomalous, in that after two decades of treatment it can now be turned over by hand.

A challenge for the modern archaeologist is to consider whether "in-situ" preservation, or recovery and conservation on land is the preferable option; or to face the fact that preservation in any form, other than as an archaeological record is not feasible. A site that has been discovered has typically been subjected to disturbance of the very factors that caused its survival in the first place, for example, when a covering of sediment has been removed by storms or the action of man. Active monitoring and deliberate protection may mitigate further rapid destruction making "in situ" preservation an option, but long-term survival can never be guaranteed. For very many sites, the costs are too great for either active measures to ensure "in situ" preservation or to provide for satisfactory conservation on recovery. Even the cost of proper and complete archaeological investigation may be too great to enable this to occur within a timescale that ensures that an archaeological record is made before data is inevitably lost.

Maritime archaeology studies prehistorical objects and sites that are, because of changes in climate and geology, now underwater.

Bodies of water, fresh and saline, have been important sources of food for people for as long as we have existed. It should be no surprise that ancient villages were located at the water's edge. Since the last ice age sea level has risen as much as .

Therefore, a great deal of the record of human activity throughout the Ice Age is now to be found under water.

The flooding of the area now known as the Black Sea (when a land bridge, where the Bosporus is now, collapsed under the pressure of rising water in the Mediterranean Sea) submerged a great deal of human activity that had been gathered round what had been an enormous, fresh-water lake.

Significant cave art sites off the coast of western Europe such as the Grotto Cosquer can be reached only by diving, because the cave entrances are underwater, though the upper portions of the caves themselves are not flooded.

Throughout history, seismic events have at times caused submergence of human settlements. The remains of such catastrophes exist all over the world, and sites such as Alexandria and Port Royal now form important archaeological sites. As with shipwrecks, archaeological research can follow multiple themes, including evidence of the final catastrophe, the structures and landscape before the catastrophe and the culture and economy of which it formed a part. Unlike the wrecking of a ship, the destruction of a town by a seismic event can take place over many years and there may be evidence for several phases of damage, sometimes with rebuilding in between.

Not all maritime sites are underwater. There are many structures at the margin of land and water that provide evidence of the human societies of the past. Some are deliberately created for access - such as bridges and walkways. Other structures remain from exploitation of resources, such as dams and fish traps. Nautical remains include early harbours and places where ships were built or repaired. At the end of their life, ships were often beached. Valuable or easily accessed timber has often been salvaged leaving just a few frames and bottom planking.

Archaeological sites can also be found on the foreshore today that would have been on dry land when they were constructed. An example of such a site is Seahenge, a Bronze Age timber circle.

The archaeology of shipwrecks can be divided into a three-tier hierarchy, of which the first tier considers the wrecking process itself: how does a ship break up, how does a ship sink to the bottom, and how do the remains of the ship, cargo and the surrounding environment evolve over time? The second tier studies the ship as a machine, both in itself and in a military or economic system. The third tier consists of the archaeology of maritime cultures, in which nautical technology, naval warfare, trade and shipboard societies are studied. Some consider this to be the most important tier. Ships and boats are not necessarily wrecked: some are deliberately abandoned, scuttled or beached. Many such abandoned vessels have been extensively salvaged.

The earliest boats discovered date from the Bronze Age and are constructed of hollowed out logs or sewn planks. Vessels have been discovered where they have been preserved in sediments underwater or in waterlogged land sites, such as the discovery of a canoe near St Botolphs. Examples of sewn-plank boats include those found at North Ferriby and the Dover Bronze Age Boat which is now displayed at Dover Museum. These may be an evolution from boats made of sewn hides, but it is highly unlikely that hide boats could have survived.

Ships wrecked in the sea have probably not survived, although remains of cargo (particularly bronze material) have been discovered, such as those at the Salcombe B site. A close collection of artefacts on the sea bed may imply that artefacts were from a ship, even if there are no remains of the actual vessel.

Late Bronze Age ships, such as the Uluburun Shipwreck have been discovered in the Mediterranean, constructed of edge joined planks. This shipbuilding technology continued through the classical period.

In the Mediterranean area, maritime archaeologists have investigated several ancient cultures. Notable early Iron Age shipwrecks include two Phoenician ships of c. 750 BC that foundered off Gaza with cargoes of wine in amphoras. The crew of the U.S. Navy deep submergence research submarine NR-1 discovered the sites in 1997. In 1999 a team led by Robert Ballard and Harvard University archaeology Professor Lawrence Stager investigated the wrecks.

Extensive research has been carried out on the Mediterranean and Aegean coastlines of Turkey. Complete excavations have been performed on several wrecks from the Classical, Hellenistic, Byzantine, and Ottoman periods.

Maritime archaeological studies in Italy illuminate the naval and maritime activities of the Etruscans, Greek colonists, and Romans. After the 2nd century BC, the Roman fleet ruled the Mediterranean and actively suppressed piracy. During this Pax Romana, seaborne trade increased significantly throughout the region. Though sailing was the safest, fastest, and most efficient method of transportation in the ancient world, some fractional percentage of voyages ended in shipwreck. With the significantly increased sea traffic during the Roman era came a corresponding increase in shipwrecks. These wrecks and their cargo remains offer glimpses through time of the economy, culture, and politics of the ancient world. Particularly useful to archaeologists are studies of amphoras, the ceramic shipping containers used in the Mediterranean region from the 15th century BC through the Medieval period.

In addition to many discoveries in the sea, some wrecks have been examined in lakes. Most notable are Caligula's pleasure barges in Lake Nemi, Italy. The Nemi ships and other shipwreck sites occasionally yield objects of unique artistic value. For instance, the Antikythera wreck contained a staggering collection of marble and bronze statues including the Antikythera Youth. Discovered in 1900 by Greek sponge divers, the ship probably sank in the 1st century BC and may have been dispatched by the Roman general, Sulla, to carry booty back to Rome. The sponge divers also recovered from the wreck the famous Antikythera mechanism, believed to be an astronomical calculator. Further examples of fabulous works of art recovered from the sea floor are the two "bronzi" found in Riace (Calabria), Italy. In the cases of Antikythera and Riace, however, the artifacts were recovered without the direct participation of maritime archaeologists.

Recent studies in the Sarno river (near Pompeii) show other interesting elements of ancient life. The Sarno projects suggests that on the Tyrrhenian shore there were little towns with palafittes, similar to ancient Venice. In the same area, the submerged town of Puteoli (Pozzuoli, close to Naples) contains the "portus Julius" created by Marcus Vipsanius Agrippa in 37 BC, later sunk due to bradyseism.

The sea floor elsewhere in the Mediterranean holds countless archaeological sites. In Israel, Herod the Great's port at Caesarea Maritima has been extensively studied. Other finds are consistent with some passages of the Bible (like the so-called Jesus boat, which appears to have been in use during the first century AD).

Maritime archaeology in Australia commenced in the 1970s with the advent of Jeremy Green due to concerns expressed by academics and politicians with the rampant destruction of the Dutch and British East India ships lost on the west coast. As Commonwealth legislation was enacted and enforced after 1976 and as States enacted their own legislation the sub-discipline spread throughout Australia concentrating initially on shipwrecks due to on-going funding by both the States and the Commonwealth under their shipwreck legislation. Studies now include as an element of underwater archaeology, as a whole, the study of submerged indigenous sites. Nautical Archaeology, (the specialised study of boat and ship construction) is also practised in the region. Often the sites or relics studied in Australia as in the rest of the world are not inundated. The study of historic submerged aircraft, better known as a sub-discipline of aviation archaeology, underwater aviation archaeology is also practised in the region. In some states maritime and underwater archaeology is practised out of Museums and in others out of cultural heritage management units and all practitioners operate under the aegis of the Australasian Institute for Maritime Archaeology (AIMA).









</doc>
<doc id="20069" url="https://en.wikipedia.org/wiki?curid=20069" title="Morihei Ueshiba">
Morihei Ueshiba

The son of a landowner from Tanabe, Ueshiba studied a number of martial arts in his youth, and served in the Japanese Army during the Russo-Japanese War. After being discharged in 1907, he moved to Hokkaidō as the head of a pioneer settlement; here he met and studied with Takeda Sōkaku, the founder of Daitō-ryū Aiki-jūjutsu. On leaving Hokkaido in 1919, Ueshiba joined the Ōmoto-kyō movement, a Shinto sect, in Ayabe, where he served as a martial arts instructor and opened his first dojo. He accompanied the head of the Ōmoto-kyō group, Onisaburo Deguchi, on an expedition to Mongolia in 1924, where they were captured by Chinese troops and returned to Japan. The following year, he had a profound spiritual experience, stating that, "a golden spirit sprang up from the ground, veiled my body, and changed my body into a golden one." After this experience, his martial arts skill appeared to be greatly increased.

Ueshiba moved to Tokyo in 1926, where he set up the Aikikai Hombu Dojo. By now he was comparatively famous in martial arts circles, and taught at this dojo and others around Japan, including in several military academies. In the aftermath of World War II the Hombu dojo was temporarily closed, but Ueshiba had by this point left Tokyo and retired to Iwama, and he continued training at the dojo he had set up there. From the end of the war until the 1960s, he worked to promote aikido throughout Japan and abroad. He died from liver cancer in 1969.

After Ueshiba's death, aikido continued to be promulgated by his students (many of whom became noted martial artists in their own right). It is now practiced around the world.

Morihei Ueshiba was born in Nishinotani village (now part of the city of Tanabe), Wakayama Prefecture, Japan, on December 14, 1883, the fourth child (and only son) born to Yoroku Ueshiba and his wife Yuki.

The young Ueshiba was raised in a somewhat privileged setting. His father Yoroku was a wealthy gentleman farmer and minor politician, being an elected member of the Nishinotani village council for 22 consecutive years. His mother Yuki was from the Itokawa clan, a prominent local family who could trace their lineage back to the Heian period. Ueshiba was a rather weak, sickly child and bookish in his inclinations. At a young age his father encouraged him to take up sumo wrestling and swimming and entertained him with stories of his great-grandfather Kichiemon, who was considered a very strong samurai in his era. The need for such strength was further emphasized when the young Ueshiba witnessed his father being attacked by followers of a competing politician.

A major influence on Ueshiba's early education was his elementary schoolteacher Tasaburo Nasu, who was a Shinto priest and who introduced Ueshiba to the religion. At the age of six Ueshiba was sent to study at the Jizōderu Temple, but had little interest in the rote learning of Confucian education. However, his schoolmaster Mitsujo Fujimoto was also a priest of Shingon Buddhism, and taught the young Ueshiba some of the esoteric chants and ritual observances of the sect, which Ueshiba found intriguing. His interest in Buddhism was sufficiently great that his mother considered enrolling him in the priesthood, but his father Yoroku vetoed the idea. Ueshiba went to Tanage Higher Elementary School and then to Tanabe Prefectural Middle School, but left formal education in his early teens, enrolling instead at a private abacus academy, the Yoshida Institute, to study accountancy. On graduating from the academy, he worked at a local tax office for a few months, but the job did not suit him and in 1901 he left for Tokyo, funded by his father. Ueshiba Trading, the stationery business which he opened there, was short-lived; unhappy with life in the capital, he returned to Tanabe less than a year later after suffering a bout of beri-beri. Shortly thereafter he married his childhood acquaintance Hatsu Itokawa.

In 1903, Ueshiba was called up for military service. He failed the initial physical examination, being shorter than the regulation . To overcome this, he stretched his spine by attaching heavy weights to his legs and suspending himself from tree branches; when he re-took the physical exam he had increased his height by the necessary half-inch to pass. He was assigned to the Osaka Fourth Division, 37th Regiment, and was promoted to corporal of the 61st Wakayama regiment by the following year; after serving on the front lines during the Russo-Japanese War he was promoted to sergeant. He was discharged in 1907, and again returned to his father's farm in Tanabe. Here he befriended the writer and philosopher Minakata Kumagusu, becoming involved with Minakata's opposition to the Meiji government's Shrine Consolidation Policy. He and his wife had their first child, a daughter named Matsuko, in 1911.

Ueshiba studied several martial arts during his early life, and was renowned for his physical strength during his youth. During his sojourn in Tokyo he studied Kitō-ryū jujutsu under Takisaburo Tobari, and briefly enrolled in a school teaching Shinkage-ryū. His training in Gotō-ha Yagyū-ryu under Masakatsu Nakai was sporadic due to his military service, although he was granted a diploma in the art within a few years. In 1901 he received some instruction from Tozawa Tokusaburōin in Tenjin Shin'yō-ryū jujutsu and he studied judo with Kiyoichi Takagi in Tanabe in 1911, after his father had a dojo built on the family compound to encourage his son's training. In 1907, after his return from the war, he was also presented with a certificate of enlightenment ("shingon inkyo") by his childhood teacher Mitsujo Fujimoto.

In the early part of the 20th century, the prefectural government of Hokkaidō, Japan's northernmost island, were offering various grants and incentives for mainland Japanese groups willing to relocate there. At the time, Hokkaidō was still largely unsettled by the Japanese, being occupied primarily by the indigenous Ainu. In 1910, Ueshiba travelled to Hokkaidō in the company of his acquaintance Denzaburo Kurahashi, who had lived on the northern island before. His intent was to scout out a propitious location for a new settlement, and he found the site at Shirataki suitable for his plans. Despite the hardships he suffered on this journey (which included getting lost in snowstorms several times and an incident in which he nearly drowned in a freezing river), Ueshiba returned to Tanabe filled with enthusiasm for the project, and began recruiting families to join him. He became the leader of the Kishū Settlement Group, a collective of eighty-five pioneers who intended to settle in the Shirataki district and live as farmers; the group founded the village of Yubetsu (later Shirataki village) in August, 1912. Much of the funding for this project came from Ueshiba's father and his brothers-in-law Zenzo and Koshiro Inoue. Zenzo's son Noriaki was also a member of the settlement group.

Poor soil conditions and bad weather led to crop failures during the first three years of the project, but the group still managed to cultivate mint and farm livestock. The burgeoning timber industry provided a boost to the settlement's economy, and by 1918 there were over 500 families residing there. A fire in 1917 razed the entire village, leading to the departure of around twenty families. Ueshiba was attending a meeting over railway construction around 50 miles away, but on learning of the fire travelled back the entire distance on foot. He was elected to the village council that year, and took a prominent role in leading the reconstruction efforts. In the summer of 1918, Hatsu gave birth to their first son, Takemori.

The young Ueshiba met Takeda Sōkaku, the founder of Daitō-ryū Aiki-jūjutsu, at the Hisada Inn in Engaru, in March 1915. Ueshiba was deeply impressed with Takeda's martial art, and despite being on an important mission for his village at the time, abandoned his journey to spend the next month studying with Takeda. He requested formal instruction and began studying Takeda's style of jūjutsu in earnest, going so far as to construct a dojo at his home and inviting his new teacher to be a permanent house guest. He received a "kyōju dairi" certificate, a teaching license, for the system from Takeda in 1922, when Takeda visited him in Ayabe. Takeda also gave him a Yagyū Shinkage-ryū sword transmission scroll. Ueshiba then became a representative of Daitō-ryū, toured with Takeda as a teaching assistant and taught the system to others. The relationship between Ueshiba and Takeda was a complicated one. Ueshiba was an extremely dedicated student, dutifully attending to his teacher's needs and displaying great respect. However, Takeda overshadowed him throughout his early martial arts career, and Ueshiba's own students recorded the need to address what they referred to as "the Takeda problem".

In November 1919, Ueshiba learned that his father Yoroku was ill, and was not expected to survive. Leaving most of his possessions to Takeda, Ueshiba left Shirataki with the apparent intention of returning to Tanabe to visit his ailing parent. En route he made a detour to Ayabe, near Kyoto, intending to visit Onisaburo Deguchi, the spiritual leader of the Ōmoto-kyō religion (Ueshiba's nephew Noriaki Inoue had already joined the religion and may have recommended it to his uncle). Ueshiba stayed at the Ōmoto-kyō headquarters for several days, and met with Deguchi, who told him that, "There is nothing to worry about with your father". On his return to Tanabe, Ueshiba found that Yoroku had died. Criticised by family and friends for arriving too late to see his father, Ueshiba went into the mountains with a sword and practised solo sword exercises for several days; this almost led to his arrest when the police were informed of a sword-wielding madman on the loose.

Within a few months, Ueshiba was back in Ayabe, having decided to become a full-time student of Ōmoto-kyō. In 1920 he moved his entire family, including his mother, to the Ōmoto compound; at the same time he also purchased enough rice to feed himself and his family for several years. That same year, Deguchi asked Ueshiba to become the group's martial arts instructor, and a dojo—the first of several that Ueshiba was to lead—was constructed on the centre's grounds. Ueshiba also taught Takeda's Daitō-ryū in neighbouring Hyōgo Prefecture during this period. His second son, Kuniharu, was born in 1920 in Ayabe, but died from illness the same year, along with three-year-old Takemori.

Takeda visited Ueshiba in Ayabe to provide instruction, although he was not a follower of Ōmoto and did not get along with Deguchi, which led to a cooling of the relationship between him and Ueshiba. Ueshiba continued to teach his martial art under the name "Daitō-ryū Aiki-jūjutsu", at the behest of his teacher. However, Deguchi encouraged Ueshiba to create his own style of martial arts, "Ueshiba-ryū", and sent many Ōmoto followers to study at the dojo. He also brought Ueshiba into the highest levels of the group's bureaucracy, making Ueshiba his executive assistant and putting him in charge of the Showa Seinenkai (Ōmoto-kyō's national youth organisation) and the Ōmoto Shobotai, a volunteer fire service.

His close relationship with Deguchi introduced Ueshiba to various members of Japan's far-right; members of the ultranationalist group the Sakurakai would hold meetings at Ueshiba's dojo, and he developed a friendship with the philosopher Shūmei Ōkawa during this period, as well as meeting with Nisshō Inoue and Kozaburō Tachibana. Deguchi also offered Ueshiba's services as a bodyguard to Kingoro Hashimoto, the Sakurakai's founder. Ueshiba's commitment to the goal of world peace, stressed by many biographers, must be viewed in the light of these relationships and his Ōmoto-kyō beliefs. His association with the extreme right-wing is understandable when one considers that Ōmoto-kyō's view of world peace was of a benevolent dictatorship by the Emperor of Japan, with other nations being subjugated under Japanese rule.

In 1921, in an event known as the , the Japanese authorities raided the compound, destroying the main buildings on the site and arresting Deguchi on charges of lèse-majesté. Ueshiba's dojo was undamaged and over the following two years he worked closely with Deguchi to reconstruct the group's centre, becoming heavily involved in farming work and serving as the group's "Caretaker of Forms", a role which placed him in charge of overseeing Ōmoto's move towards self-sufficiency. His son Kisshomaru was born in the summer of 1921.

Three years later, in 1924, Deguchi led a small group of Ōmoto-kyō disciples, including Ueshiba, on a journey to Mongolia at the invitation of retired naval captain Yutaro Yano and his associates within the ultra-nationalist Black Dragon Society. Deguchi's intent was to establish a new religious kingdom in Mongolia, and to this end he had distributed propaganda suggesting that he was the reincarnation of Genghis Khan. Allied with the Mongolian bandit Lu Zhankui, Deguchi's group were arrested in Tongliao by the Chinese authorities—fortunately for Ueshiba, whilst Lu and his men were executed by firing squad, the Japanese group were released into the custody of the Japanese consul. They were returned under guard to Japan, where Deguchi was imprisoned for breaking the terms of his bail. During this expedition Ueshiba was given the Chinese alias Wang Shou-gao, rendered in Japanese as "Moritaka" – he was reportedly very taken with this name and continued to use it intermittently for the rest of his life.

After returning to Ayabe, Ueshiba began a regimen of spiritual training, regularly retreating to the mountains or performing "misogi" in the Nachi Falls. As his prowess as a martial artist increased, his fame began to spread. He was challenged by many established martial artists, some of whom later became his students after being defeated by him. In the autumn of 1925 he was asked to give a demonstration of his art in Tokyo, at the behest of Admiral Isamu Takeshita; one of the spectators was Yamamoto Gonnohyōe, who requested that Ueshiba stay in the capital to instruct the Imperial Guard in his martial art. After a couple of weeks, however, Ueshiba took issue with several government officials who voiced concerns about his connections to Deguchi; he cancelled the training and returned to Ayabe.

In 1926 Takeshita invited Ueshiba to visit Tokyo again. Ueshiba relented and returned to the capital, but while residing there was stricken with a serious illness. Deguchi visited his ailing student and, concerned for his health, commanded Ueshiba to return to Ayabe. The appeal of returning increased after Ueshiba was questioned by the police following his meeting with Deguchi; the authorities were keeping the Ōmoto-kyō leader under close surveillance. Angered at the treatment he had received, Ueshiba went back to Ayabe again. Six months later, this time with Deguchi's blessing, he and his family moved permanently to Tokyo. This move allowed Ueshiba to teach politicians, high-ranking military personnel, and members of the Imperial household; suddenly he was no longer an obscure provincial martial artist, but a sensei to some of Japan's most important citizens. Arriving in October 1927, the Ueshiba family set up home in the Shirokane district. The building proved too small to house the growing number of aikido students, and so the Ueshibas moved to larger premises, first in Mita district, then in Takanawa, and finally to a purpose-built hall in Shinjuku. This last location, originally named the Kobukan (), would eventually become the Aikikai Hombu Dojo. During its construction, Ueshiba rented a property nearby, where he was visited by Kanō Jigorō, the founder of judo.

During this period, Ueshiba was invited to teach at a number of military institutes, due to his close personal relationships with key figures in the military (among them Sadao Araki, the Japanese Minister of War). He accepted an invitation from Admiral Sankichi Takahashi to be the martial arts instructor at the Imperial Japanese Naval Academy, and also taught at the Nakano Spy School, although aikido was later judged to be too technical for the students there and karate was adopted instead. He also became a visiting instructor at the Imperial Japanese Army Academy after being challenged by (and defeating) General Makoto Miura, another student of Takeda Sōkaku's Daitō-ryū. Takeda himself met Ueshiba for the last time around 1935, while Ueshiba was teaching at the Osaka headquarters of the "Asahi Shimbun" newspaper. Frustrated by the appearance of his teacher, who was openly critical of Ueshiba's martial arts and who appeared intent on taking over the classes there, Ueshiba left Osaka during the night, bowing to the residence in which Takeda was staying and thereafter avoiding all contact with him. Between 1940 and 1942 he made several visits to Manchukuo (Japanese occupied Manchuria) where he was the principal martial arts instructor at Kenkoku University. Whilst in Manchuria, he met and defeated the sumo wrestler Tenryū Saburō during a demonstration.

The "Second Ōmoto Incident" in 1935 saw another government crackdown on Deguchi's sect, in which the Ayabe compound was destroyed and most of the group's leaders imprisoned. Although he had relocated to Tokyo, Ueshiba had retained links with the Ōmoto-kyō group (he had in fact helped Deguchi to establish a paramilitary branch of the sect only three years earlier) and expected to be arrested as one of its senior members. However, he had a good relationship with the local police commissioner Kenji Tomita and the chief of police Gīchi Morita, both of whom had been his students. As a result, although he was taken in for interrogation, he was released without charge on Morita's authority.

In 1932, Ueshiba's daughter Matsuko was married to the swordsman Kiyoshi Nakakura, who was adopted as Ueshiba's heir under the name Morihiro Ueshiba. The marriage ended after a few years, and Nakakura left the family in 1937. Ueshiba later designated his son Kisshomaru as the heir to his martial art.

The 1930s saw Japan's invasion of mainland Asia and increased military activity in Europe. Ueshiba was concerned about the prospect of war, and became involved in a number of efforts to try and forestall the conflict that would eventually become World War II. He was part of a group, along with Shūmei Ōkawa and several wealthy Japanese backers, that tried to broker a deal with Harry Chandler to export aviation fuel from the United States to Japan (in contravention of the ), although this effort ultimately failed. In 1941 Ueshiba also undertook a secret diplomatic mission to China at the behest of Prince Fumimaro Konoe. The intended goal was a meeting with Chiang Kai-shek to establish peace talks, but Ueshiba was unable to meet with the Chinese leader, arriving too late to fulfil his mission.

From 1935 onwards, Ueshiba had been purchasing land in Iwama in Ibaraki Prefecture, and by the early 1940s had acquired around of farmland there. In 1942, disenchanted with the war-mongering and political manoeuvring in the capital, he left Tokyo and moved to Iwama permanently, settling in a small farmer's cottage. Here he founded the Aiki Shuren Dojo, also known as the Iwama dojo, and the Aiki Shrine, a devotional shrine to the "Great Spirit of Aiki". During this time he travelled extensively in Japan, particularly in the Kansai region, teaching his aikido. Despite the prohibition on the teaching of martial arts after World War II, Ueshiba and his students continued to practice in secret at the Iwama dojo; the Hombu dojo in Tokyo was in any case being used as a refugee centre for citizens displaced by the severe firebombing. It was during this period that Ueshiba met and befriended Koun Nakanishi, an expert in kotodama. The study of kotodama was to become one of Ueshiba's passions in later life, and Nakanishi's work inspired Ueshiba's concept of "takemusu aiki".

The rural nature of his new home in Iwama allowed Ueshiba to concentrate on the second great passion of his life: farming. He had been born into a farming family and spent much of his life cultivating the land, from his settlement days in Hokkaidō to his work in Ayabe trying to make the Ōmoto-kyō compound self-sufficient. He viewed farming as a logical complement to martial arts; both were physically demanding and required single-minded dedication. Not only did his farming activities provide a useful cover for martial arts training under the government's restrictions, it also provided food for Ueshiba, his students and other local families at a time when food shortages were commonplace.

The government prohibition (on aikido, at least) was lifted in 1948 with the creation of the Aiki Foundation, established by the Japanese Ministry of Education with permission from the Occupation forces. The Hombu dojo re-opened the following year. After the war Ueshiba effectively retired from aikido. He delegated most of the work of running the Hombu dojo and the Aiki Federation to his son Kisshomaru, and instead chose to spend much of his time in prayer, meditation, calligraphy and farming. He still travelled extensively to promote aikido, even visiting Hawaii in 1961. He also appeared in a television documentary on aikido: NTV's "The Master of Aikido", broadcast in January 1960. Ueshiba maintained links with the Japanese nationalist movement even in later life; his student Kanshu Sunadomari reported that Ueshiba temporarily sheltered Mikami Taku, one of the naval officers involved in the May 15 Incident, at Iwama.

In 1969, Ueshiba became ill. He led his last training session on March 10, and was taken to hospital where he was diagnosed with cancer of the liver. He died suddenly on April 26, 1969. His body was buried at Kōzan-ji, and he was given the posthumous Buddhist title "Aiki-in Moritake En'yū Daidōshi" (); parts of his hair were enshrined at Ayabe, Iwama and Kumano. Two months later, his wife Hatsu ( "Ueshiba Hatsu", née "Itokawa Hatsu"; 1881–1969) also died.

Aikido—usually translated as the "Way of Unifying Spirit" or the "Way of Spiritual Harmony"—is a fighting system that focuses on throws, pins and joint locks together with some striking techniques. It emphasises protecting the opponent and promotes spiritual and social development.

The technical curriculum of aikido was derived from the teachings of Takeda Sōkaku; the basic techniques of aikido stem from his Daitō-ryū system. In the earlier years of his teaching, from the 1920s to the mid-1930s, Ueshiba taught the Daitō-ryū Aiki-jūjutsu system; his early students' documents bear the term Daitō-ryū. Indeed, Ueshiba trained one of the future highest grade earners in Daitō-ryū, Takuma Hisa, in the art before Takeda took charge of Hisa's training.

The early form of training under Ueshiba was noticeably different from later forms of aikido. It had a larger curriculum, increased use of strikes to vital points ("atemi") and a greater use of weapons. The schools of aikido developed by Ueshiba's students from the pre-war period tend to reflect the harder style of the early training. These students included Kenji Tomiki (who founded the Shodokan Aikido sometimes called Tomiki-ryū), Noriaki Inoue (who founded Shin'ei Taidō), Minoru Mochizuki (who founded Yoseikan Budo) and Gozo Shioda (who founded Yoshinkan Aikido). Many of these styles are therefore considered "pre-war styles", although some of these teachers continued to train with Ueshiba in the years after World War II.
During his lifetime, Ueshiba had three spiritual experiences that impacted greatly on his understanding of the martial arts. The first occurred in 1925, after Ueshiba had defeated a naval officer's "bokken" (wooden katana) attacks unarmed and without hurting the officer. Ueshiba then walked to his garden, where he had the following realisation:
His second experience occurred in 1940 when engaged in the ritual purification process of "misogi".
His third experience was in 1942 during the worst fighting of World War II, when Ueshiba had a vision of the "Great Spirit of Peace".
After these events, Ueshiba seemed to slowly grow away from Takeda, and he began to change his art. These changes are reflected in the differing names with which he referred to his system, first as "aiki-jūjutsu", then Ueshiba-ryū, Asahi-ryū, and "aiki budō". In 1942, when Ueshiba's group joined the Dai Nippon Butoku Kai, the martial art that Ueshiba developed finally came to be known as aikido.

As Ueshiba grew older, more skilled, and more spiritual in his outlook, his art also changed and became softer and more gentle. Martial techniques became less important, and more focus was given to the control of ki. In his own expression of the art there was a greater emphasis on what is referred to as "kokyū-nage", or "breath throws" which are soft and blending, utilizing the opponent's movement in order to throw them. Ueshiba regularly practiced cold water "misogi", as well as other spiritual and religious rites, and viewed his studies of aikido as part of this spiritual training.
Over the years, Ueshiba trained a large number of students, many of whom later became famous teachers in their own right and developed their own styles of aikido. Some of them were "uchi-deshi", live-in students. Ueshiba placed many demands on his "uchi-deshi", expecting them to attend him at all times, act as training partners (even in the middle of the night), arrange his travel plans, massage and bathe him, and assist with household chores.

There were roughly four generations of students, comprising the pre-war students (training 1921–1935), students who trained during the Second World War (c.1936–1945), the post-war students in Iwama (c.1946–1955) and the students who trained with Ueshiba during his final years (c.1956–c.1969). As a result of Ueshiba's martial development throughout his life, students from each of these generations tend to have markedly different approaches to aikido. These variations are compounded by the fact that few students trained with Ueshiba for a protracted period; only Yoichiro Inoue, Kenji Tomiki, Gozo Shioda, Morihiro Saito and Tsutomu Yukawa studied directly under Ueshiba for more than five or six years. After the war, Ueshiba and the Hombu Dojo dispatched some of their students to various other countries, resulting in aikido spreading around the world.




</doc>
<doc id="20070" url="https://en.wikipedia.org/wiki?curid=20070" title="Memory address register">
Memory address register

In a computer, the Memory Address Register (MAR) is the CPU register that either stores the memory address from which data will be fetched from the CPU, or the address to which data will be sent and stored.

In other words, MAR holds the memory location of data that needs to be accessed. When reading from memory, data addressed by MAR is fed into the MDR (memory data register) and then used by the CPU.
When writing to memory, the CPU writes data from MDR to the memory location whose address is stored in MAR. MAR which is found inside the CPU goes either to the RAM(Random Access Memory) or Cache.

The Memory Address Register is half of a minimal interface between a microprogram and computer storage. The other half is a memory data register.

In general, MAR is a parallel load register that contains the next memory address to be manipulated. For example, the next address to be read or written.


</doc>
<doc id="20072" url="https://en.wikipedia.org/wiki?curid=20072" title="Microassembler">
Microassembler

A microassembler is a computer program that helps prepare a microprogram, called "firmware", to control the low level operation of a computer in much the same way an assembler helps prepare higher level code for a processor. The difference is that the microprogram is usually only developed by the processor manufacturer and works intimately with the computer hardware. On a microprogrammed computer the microprogram implements the operations of the instruction set in which any normal program (including both application programs and operating systems) is written. The use of a microprogram allows the manufacturer to fix certain mistakes, including working around hardware design errors, without modifying the hardware. Another means of employing microassembler-generated microprograms is in allowing the same hardware to run different instruction sets. After it is assembled, the microprogram is then loaded to a control store to become part of the logic of a CPU's control unit.

Some microassemblers are more generalized and are not targeted at a single computer architecture. For example, through the use of macro-assembler-like capabilities, Digital Equipment Corporation used their "MICRO2" microassembler for a very wide range of computer architectures and implementations.

If a given computer implementation supports a writeable control store, the microassembler is usually provided to customers as a means of writing customized microcode.

In the process of microcode assembly it is helpful to verify the microprogram with emulation tools before distribution. Nowadays, microcoding has experienced a revival, since it is possible to correct and optimize the firmware of processing units already manufactured or sold, in order to adapt to specific operating systems or to fix hardware bugs. However, a commonly usable microassembler for today's CPUs is not available to manipulate the microcode. Unfortunately, knowledge of a processor's microcode is usually considered proprietary information so it is difficult to obtain information about how to modify it.



</doc>
<doc id="20074" url="https://en.wikipedia.org/wiki?curid=20074" title="Machine pistol">
Machine pistol

A machine pistol is typically a handgun-style machine gun, capable of fully automatic or burst fire. The term is a calque of the German word "Maschinenpistole".

During World War I, the Austrians introduced the world's first machine pistol the Steyr Repetierpistole M1912/P16. The Germans also experimented with machine pistols, by converting various types of semi-automatic pistols to full-auto, leading to the development of the first practical submachine gun. During World War II, machine pistol development was more or less ignored as the major powers were focused on mass-producing submachine guns. After the war, machine pistols development was limited and only a handful of manufacturers would develop new designs, with varying degrees of success. Originally, issued to primarily German artillery crews who needed a self-defense weapon, lighter than a rifle but more effective than a standard pistol. Today, machine pistols are considered a special purpose weapon with limited utility, and difficult for all but the best shooters to control.

During World War I, a machine pistol version of the Steyr M1912 called the Repetierpistole M1912/P16 was produced. It used a 16-round fixed magazine loaded via 8 round stripper clips, a detachable shoulder stock and a rather large exposed semi-auto/full-auto selector on the right side of the frame above the trigger (down = semi & up = full). It fires the 9×23mm Steyr cartridge, with a full-auto rate-of-fire of about 800 to 1000 rounds-per-minute. It weighed about 2.6 pounds. Introduced in 1916, it is considered the world first machine pistol, only 960 M1912/P16 were made.
The Germans also experimented with machine pistols, by converting various types of semi-automatic pistols to full-auto, such as Luger P08 "Artillery Pistol" with its long barrel, detachable stock and 32-round drum magazine. It was issued to primarily German artillery crews who needed a self-defence weapon, lighter than a rifle but more effective than a standard pistol. It fired the newly developed 9mm Parabellum pistol cartridge, which was designed for low recoil without sacrificing penetration and stopping power. These machine pistols proved to be quite effective in close range trench warfare and led the Germans to develop the 9mm Parabellum Bergmann MP-18, the first practical submachine gun.

The Mauser C96 was introduced in 1896, it was one of the first commercially successful and practical semi-automatic pistols. During World War I, the Germans experimented with machine pistols by converting both 7.63mm Mauser and 9mm Parabellum semi-automatic C96 pistols to full-auto. In the late 1920s, Spanish gunmakers introduced select fire copies of the C96 with 20 round detachable magazines. In the early 1930s, Mauser engineers followed suit, and introduced the Model 1932 or Model 712 "Schnellfeuer" variant, which included 20 round detachable magazine and a select fire mechanism allowing fully automatic fire at a rate of 1000 rounds/minute.

During World War II machine pistol development was more or less ignored as the major powers were focused on mass-producing submachine guns. With one exception, the 9mm Parabellum Browning Hi-Power machine pistol. The artillery version with its adjustable tangent rear-sight, shoulder-stock, 13 round magazine and later 20 round magazine was routinely converted to full-auto-only. In German service, it was used mainly by Waffen-SS and Fallschirmjäger personnel along with Mauser M1932 Schnellfeuer machine pistol.

The 9×18mm Makarov Stechkin automatic pistol (APS) is a Russian selective-fire machine pistol introduced into the Russian army in 1951. Like the other common Russian army pistol of this era, the Makarov, the Stechkin uses a simple unlocked blow-back mechanism and the double action trigger. In addition, the Stechkin APS has an automatic fire mode, which is selected using the safety lever. In burst or automatic fire, the pistol should be fitted with the wooden shoulder stock; otherwise, the weapon quickly becomes uncontrollable. The Stechkin was intended as a side arm for artillery soldiers and tank crews. In practice, it earned a strong following in the ranks of political and criminal police, special forces and the like. Many KGB and GRU operatives favored the Stechkin for its firepower and 20 round magazine.
The Škorpion vz. 61 is a Czechoslovak 7.65 mm or .32 ACP machine pistol developed in 1959 and produced from 1961 to 1979. Although it was developed for use with security and special forces, the weapon was also accepted into service with the Czechoslovak Army, as a personal sidearm for lower-ranking army staff, vehicle drivers, armoured vehicle personnel and special forces. The Skorpion's lower powered .32 ACP cartridge, coupled with a rate-of-fire limiting device housed in the grip (which allows a reasonable rate of 850 RPM with a relatively light bolt), also makes it easier to control in full-auto than the more common 9mm Parabellum designs. Currently the weapon is in use with the armed forces of several countries as a sidearm. The Škorpion was also licence-built in Yugoslavia, designated M84.

The Beretta M951R, was based on the 9mm Parabellum Beretta M1951 pistol and produced during the 1960s in response to a request made by the Italian special forces. The primary difference between the M951R and the original M1951 lied in the fire selector lever mounted on the right side of the weapon’s frame, enabling either semi-automatic or continuous fire - labelled "SEM" and "AUT", respectively. Additionally, the weapon has a heavier slide, a folding wooden forward grip, the barrel was extended, and so was the magazine, increasing capacity to 10 rounds.

The MAC-10 and MAC-11 were 1970s blowback designed weapons with the magazine in the pistol grip and a fire selector switch. The .45 ACP MAC-10 had a 1,145 round-per-minute rate of fire, and the 9×19mm version 1090 rounds-per-minute. The MAC-11 could fire 1200 rounds-per-minute with its .380 ACP cartridges. These guns were designed by Gordon Ingram and Military Armament Corporation in the US. The weapons used in special ops and clandestine applications in Vietnam and by Brazilian anti-terrorist units. It could be fitted with a silencer using its threaded barrel. While some sources call the MAC-10 and MAC-11 machine pistols, the guns are also referred to as compact submachine guns.
Since it is difficult to control machine pistols when they are fired in full automatic mode, in the 1970s, some manufacturers developed an "intermittent-fire" setting that fires a burst of three shots instead of a full-automatic, such as the Heckler & Koch VP70. It is a 9mm Parabellum, 18-round, double action only, select-fire capable polymer frame pistol. It was the first polymer-framed pistol, predating the Glock 17. The stock incorporates a semi-auto/three-round-burst selector. It will only fire a 3RB with the stock attached. Cyclic rounds per minute for the three-round bursts is 2200 rpm. Despite the VP70's potential, it was never adopted by the Bundeswehr.
In 1976, a shorten machine pistol version of the 9mm Parabellum Heckler & Koch MP5 was introduced; the MP5K (K from the German word "Kurz" = "short") was designed for close quarters battle use by clandestine operations and special services. The MP5K does not have a shoulder stock, and the bolt and receiver were shortened at the rear. The resultant lighter bolt led to a higher rate of fire than the standard MP5. The barrel, cocking handle and its cover were shortened and a vertical foregrip was used to replace the standard handguard. The barrel ends at the base of the front sight, which prevents the use of any sort of muzzle device.

The Stechkin APS made a comeback in the late 1970s, when Russian Spetsnaz special forces units in Afghanistan used the suppressor-equipped APB variant for clandestine missions in enemy territory, such as during the Soviet war in Afghanistan.
The 9mm Parabellum Micro Uzi is a scaled-down version of the Uzi SMG, first introduced in 1983. It is 460 mm (18.11 inches) long with the stock extended, and just 250 mm (9.84 inches) long with the stock folded. Its barrel length is 117 mm and its muzzle velocity is 350 m/s. Used by the Israeli Isayeret and the US Secret Service, Micro-Uzis are available in open-bolt or closed-bolt versions. The weapon has an additional tungsten weight on the bolt to slow the rate of fire, which would otherwise make such a lightweight weapon uncontrollable.
The 9mm Parabellum Glock 18 is a select-fire variant of the Glock 17, developed in 1986 at the request of the Austrian counter-terrorist unit EKO Cobra. This machine pistol has a lever-type fire-control selector switch, installed on the left side, at the rear of the slide, serrated portion (selector lever in the bottom position for continuous fire, top setting for single fire). The firearm is typically used with an extended 33-round capacity magazine and may be fired with or with out a shoulder stock. The pistol's rate of fire in fully automatic mode is approximately 1100–1200 rounds/min.

Introduced in 1992, the Steyr TMP (Taktische Maschinenpistole/Tactical Machine Pistol) is a select-fire 9×19mm Parabellum machine pistol manufactured by Steyr Mannlicher of Austria. The magazines come in 15-, 20-, 25-, or 30-round detachable box types. A suppressor can also be fitted.

Also introduced in 1992, the 9mm Parabellum CZ 75 AUTOMATIC is the full-auto version of the CZ75. It has a longer barrel with three vent ports. This machine pistol has a horizontal rail in front of the trigger guard through which a spare 16 or 20 round magazine can be attached and be used as a fore-grip for better control during full automatic firing.
During the 1990s, the Russian Stechkin APS was once again put into service, as a weapon for VIP bodyguards and for anti-terrorist hostage rescue teams that needed the capability for full automatic fire in emergencies.

Developed in the 1990s and 2000s the personal defense weapon, a compact submachine gun-like firearm which can fire armor-piercing, higher-powered ammunition began to replace the machine pistol as a self-defence side arm for artillery crews, tank crews, and helicopter pilots. Introduced in 2001, the Heckler & Koch MP7 is often called a machine pistol. The MP7 uses a short stroke piston gas system as used on H&K's G36 and HK416 assault rifles, in place of a blow-back system traditionally seen on machine pistols. The MP7 uses 20-, 30- and 40-round magazines and fires 4.6×30mm ammunition which can penetrate soft body armor. Due to the heavy use of polymers in its construction, the MP7 is much lighter than older designs, only with an empty 20 round magazine.

The dividing line between machine pistols and compact submachine guns is hard to draw. The term "submachine gun" usually refers to magazine-fed, fully automatic carbines designed to fire pistol cartridges, while the term "machine pistol" usually refers to a fully automatic handgun based weapons. However, many weapons fall into both categories.

The CZ-Scorpion, is often called a submachine gun. However, it is small enough to be carried in a pistol holster and so is also often referred to as a machine pistol. The MAC-10, MAC-11 and the compact versions of the Uzi series have been placed in both classes. The Steyr TMP (Tactical Machine Pistol) is also called as a compact submachine gun. Likewise, the German Heckler & Koch MP5K also falls in both categories. Personal Defense Weapons (PDWs) such as the Heckler & Koch MP7 are also often called machine pistols.

Machine pistols are considered a special purpose weapon with limited utility. Due to their small size, machine pistols are difficult for all but the best shooters to control. As a result, most machine pistols are fitted with an unwieldy detachable shoulder stock. Some, such as the Heckler & Koch VP70, will only fire in semi-automatic unless the stock is attached, because the select-fire mechanism is incorporated into the stock. The VP70 also introduced a three-round-burst limiter to improve controllability. The Beretta 93R not only uses a detachable shoulder stock and a three-round-burst limiter, but also a folding forward hand-grip to improve controllability in full auto. The MAC-10 and MAC-11 use suppressors to reduce muzzle climb, while other designs use a combination of burst limiters, forward hand-grips, ported barrels and muzzle brakes.

Gunsite, a US firearms training facility, decided against teaching machine pistol firing when it was founded in 1976. Facility experts believed that it is "a slob's weapon, useful only by half-trained or poorly motivated troops"; they claimed that the machine pistol "hits no harder than a pistol and is no more portable than a rifle." Nevertheless, even the critics from Gunsite concede that the machine pistol is useful for a few situations, such as boarding an enemy boat in low light or when repelling boarders in a naval situation. In the 1970s, International Association of Police Chiefs weapons researcher David Steele criticized the MAC-10's accuracy when he described the MAC series as "fit only for combat in a phone booth".

Walt Rauch notes that "... despite the 50 to 70 years of bad press that has accrued to the concept of shooting a hand-held machine pistol", in which critics contend that the weapon will "spray bullets indiscriminately all over the area", he believes that the 2000s-era models such as the Glock 18 are controllable and accurate in full-auto shooting. Leroy Thompson states that "...machine pistols were reasonably good for use from within a vehicle or for issue to VIP [bodyguard] drivers to give them a marginally more effective weapon during an evacuation under fire". However, he also stated that machine pistols are "...(h)ard to control in full-auto fire", which means that there is nothing that a machine pistol "...can do that other weapons available today can't do more efficiently."




</doc>
<doc id="20076" url="https://en.wikipedia.org/wiki?curid=20076" title="Martin Luther King Jr.">
Martin Luther King Jr.

Martin Luther King Jr. (January 15, 1929 – April 4, 1968) was an American Baptist minister and activist who became the most visible spokesperson and leader in the civil rights movement from 1954 until his death in 1968. King is best known for advancing civil rights through nonviolence and civil disobedience, tactics his Christian beliefs and the nonviolent activism of Mahatma Gandhi helped inspire.

King led the 1955 Montgomery bus boycott and in 1957 became the first president of the Southern Christian Leadership Conference (SCLC). With the SCLC, he led an unsuccessful 1962 struggle against segregation in Albany, Georgia, and helped organize the nonviolent 1963 protests in Birmingham, Alabama. He also helped organize the 1963 March on Washington, where he delivered his famous "I Have a Dream" speech.

On October 14, 1964, King won the Nobel Peace Prize for combating racial inequality through nonviolent resistance. In 1965, he helped organize the Selma to Montgomery marches. The following year, he and the SCLC took the movement north to Chicago to work on segregated housing. In his final years, he expanded his focus to include opposition towards poverty and the Vietnam War. He alienated many of his liberal allies with a 1967 speech titled "". J. Edgar Hoover considered him a radical and made him an object of the FBI's COINTELPRO from 1963 on. FBI agents investigated him for possible communist ties, recorded his extramarital liaisons and reported on them to government officials, and on one occasion mailed King a threatening anonymous letter, which he interpreted as an attempt to make him commit suicide.

In 1968, King was planning a national occupation of Washington, D.C., to be called the Poor People's Campaign, when he was assassinated on April 4 in Memphis, Tennessee. His death was followed by riots in many U.S. cities. Allegations that James Earl Ray, the man convicted of killing King, had been framed or acted in concert with government agents persisted for decades after the shooting.

King was posthumously awarded the Presidential Medal of Freedom and the Congressional Gold Medal. Martin Luther King Jr. Day was established as a holiday in numerous cities and states beginning in 1971, and as a U.S. federal holiday in 1986. Hundreds of streets in the U.S. have been renamed in his honor, and a county in Washington State was also rededicated for him. The Martin Luther King Jr. Memorial on the National Mall in Washington, D.C., was dedicated in 2011.

King was born on January 15, 1929, in Atlanta, Georgia, to the Reverend Martin Luther King Sr. and Alberta Williams King. King's legal name at birth was Michael King, and his father was also born Michael King, but, after a period of gradual transition on the elder King's part, he changed both his and his son's names in 1934. The elder King would later state that "Michael" was a mistake by the attending physician to his son's birth, and the younger King's birth certificate was altered to read "Martin Luther King Jr." in 1957. King's parents were both African-American, and he also had Irish ancestry through his paternal great-grandfather.

King was a middle child, between older sister Christine King Farris and younger brother A.D. King. King sang with his church choir at the 1939 Atlanta premiere of the movie "Gone with the Wind", and he enjoyed singing and music. His mother was an accomplished organist and choir leader who took him to various churches to sing, and he received attention for singing "I Want to Be More and More Like Jesus". King later became a member of the junior choir in his church.

King said that his father regularly whipped him until he was fifteen; a neighbor reported hearing the elder King telling his son "he would make something of him even if he had to beat him to death." King saw his father's proud and fearless protests against segregation, such as King Sr. refusing to listen to a traffic policeman after being referred to as "boy," or stalking out of a store with his son when being told by a shoe clerk that they would have to "move to the rear" of the store to be served.

When King was a child, he befriended a white boy whose father owned a business near his family's home. When the boys were six, they started school: King had to attend a school for African Americans and the other boy went to one for whites (public schools were among the facilities segregated by state law). King lost his friend because the child's father no longer wanted the boys to play together.

King suffered from depression through much of his life. In his adolescent years, he initially felt resentment against whites due to the "racial humiliation" that he, his family, and his neighbors often had to endure in the segregated South. At the age of 12, shortly after his maternal grandmother died, King blamed himself and jumped out of a second-story window, but survived.

King was initially skeptical of many of Christianity's claims. At the age of 13, he denied the bodily resurrection of Jesus during Sunday school. From this point, he stated, "doubts began to spring forth unrelentingly." However, he later concluded that the Bible has "many profound truths which one cannot escape" and decided to enter the seminary.

Growing up in Atlanta, King attended Booker T. Washington High School. He became known for his public speaking ability and was part of the school's debate team. When King was thirteen in 1942, he became the youngest assistant manager of a newspaper delivery station for the "Atlanta Journal". During his junior year, he won first prize in an oratorical contest sponsored by the Negro Elks Club in Dublin, Georgia. On the ride home to Atlanta by bus, he and his teacher were ordered by the driver to stand so that white passengers could sit down. King initially refused but complied after his teacher told him that he would be breaking the law if he did not submit. During this incident, King said that he was "the angriest I have ever been in my life." An outstanding student, he skipped both the ninth and the twelfth grades of high school.

During King's junior year in high school, Morehouse College—a respected historically black college—announced that it would accept any high school juniors who could pass its entrance exam. At that time, many students had abandoned further studies to enlist in World War II. Due to this, Morehouse was eager to fill its classrooms. At the age of 15, King passed the exam and entered Morehouse. The summer before his last year at Morehouse, in 1947, the 18-year-old King chose to enter the ministry. He had concluded that the church offered the most assuring way to answer "an inner urge to serve humanity." King's "inner urge" had begun developing, and he made peace with the Baptist Church, as he believed he would be a "rational" minister with sermons that were "a respectful force for ideas, even social protest."

In 1948, King graduated at age 19 from Morehouse with a B.A. in sociology. He then enrolled in Crozer Theological Seminary in Chester, Pennsylvania, from which he graduated with a B.Div. degree in 1951. King's father fully supported his decision to continue his education and made arrangements for King to work with J. Pius Barbour, a family friend who pastored at Calvary Baptist Church in Chester. King became known as one of the "Sons of Calvary", an honor he shared with William Augustus Jones, Jr. and Samuel D. Proctor who went on to become well-known preachers in the black church.

While attending Crozer, King was joined by Walter McCall, a former classmate at Morehouse. At Crozer, King was elected president of the student body. The African-American students of Crozer for the most part conducted their social activity on Edwards Street. King became fond of the street because a classmate had an aunt who prepared collard greens for them, which they both relished.

King once reproved another student for keeping beer in his room, saying they had shared responsibility as African Americans to bear "the burdens of the Negro race." For a time, he was interested in Walter Rauschenbusch's "social gospel." In his third year at Crozer, King became romantically involved with the white daughter of an immigrant German woman who worked as a cook in the cafeteria. The daughter had been involved with a professor prior to her relationship with King. King planned to marry her, but friends advised against it, saying that an interracial marriage would provoke animosity from both blacks and whites, potentially damaging his chances of ever pastoring a church in the South. King tearfully told a friend that he could not endure his mother's pain over the marriage and broke the relationship off six months later. He continued to have lingering feelings toward the women he left; one friend was quoted as saying, "He never recovered."

King married Coretta Scott on June 18, 1953, on the lawn of her parents' house in her hometown of Heiberger, Alabama. They became the parents of four children: Yolanda King (1955–2007), Martin Luther King III (b. 1957), Dexter Scott King (b. 1961), and Bernice King (b. 1963). During their marriage, King limited Coretta's role in the civil rights movement, expecting her to be a housewife and mother.

At age 25 in 1954, King was called as pastor of the Dexter Avenue Baptist Church in Montgomery, Alabama.

King began doctoral studies in systematic theology at Boston University and received his Ph.D. degree on June 5, 1955, with a dissertation (initially supervised by Edgar S. Brightman and, upon the latter's death, by Lotan Harold DeWolf) titled "A Comparison of the Conceptions of God in the Thinking of Paul Tillich and Henry Nelson Wieman." While pursuing doctoral studies, King worked as an assistant minister at Boston's historic Twelfth Baptist Church with Rev. William Hunter Hester. Hester was an old friend of King's father, and was an important influence on King.

Decades later, an academic inquiry in October 1991 concluded that portions of his dissertation had been plagiarized and he had acted improperly. However, its finding, the committee said that 'no thought should be given to the revocation of Dr. King's doctoral degree,' an action that the panel said would serve no purpose." The committee also found that the dissertation still "makes an intelligent contribution to scholarship." A letter is now attached to the copy of King's dissertation held in the university library, noting that numerous passages were included without the appropriate quotations and citations of sources. Significant debate exists on how to interpret King's plagiarism.

In March 1955, Claudette Colvin—a fifteen-year-old black schoolgirl in Montgomery—refused to give up her bus seat to a white man in violation of Jim Crow laws, local laws in the Southern United States that enforced racial segregation. King was on the committee from the Birmingham African-American community that looked into the case; E. D. Nixon and Clifford Durr decided to wait for a better case to pursue because the incident involved a minor.

Nine months later on December 1, 1955, a similar incident occurred when Rosa Parks was arrested for refusing to give up her seat on a city bus. The two incidents led to the Montgomery bus boycott, which was urged and planned by Nixon and led by King. The boycott lasted for 385 days, and the situation became so tense that King's house was bombed. King was arrested during this campaign, which concluded with a United States District Court ruling in "Browder v. Gayle" that ended racial segregation on all Montgomery public buses. King's role in the bus boycott transformed him into a national figure and the best-known spokesman of the civil rights movement.

In 1957, King, Ralph Abernathy, Fred Shuttlesworth, Joseph Lowery, and other civil rights activists founded the Southern Christian Leadership Conference (SCLC). The group was created to harness the moral authority and organizing power of black churches to conduct nonviolent protests in the service of civil rights reform. The group was inspired by the crusades of evangelist Billy Graham, who befriended King after he attended a 1957 Graham crusade in New York City. King led the SCLC until his death. The SCLC's 1957 Prayer Pilgrimage for Freedom was the first time King addressed a national audience. Other civil rights leaders involved in the SCLC with King included: James Bevel, Allen Johnson, Curtis W. Harris, Walter E. Fauntroy, C. T. Vivian, Andrew Young, The Freedom Singers, Charles Evers, Cleveland Robinson, Randolph Blackwell, Annie Bell Robinson Devine, Charles Kenzie Steele, Alfred Daniel Williams King, Benjamin Hooks, Aaron Henry and Bayard Rustin.

On September 20, 1958, King was signing copies of his book "Stride Toward Freedom" in Blumstein's department store in Harlem when he narrowly escaped death. Izola Curry—a mentally ill black woman who thought that King was conspiring against her with communists—stabbed him in the chest with a letter opener. King underwent emergency surgery with three doctors: Aubre de Lambert Maynard, Emil Naclerio and John W. V. Cordice; he remained hospitalized for several weeks. Curry was later found mentally incompetent to stand trial. In 1959, he published a short book called "The Measure of A Man", which contained his sermons "What is Man?" and "The Dimensions of a Complete Life." The sermons argued for man's need for God's love and criticized the racial injustices of Western civilization.

Harry Wachtel joined King's legal advisor Clarence B. Jones in defending four ministers of the SCLC in the libel case "New York Times Co. v. Sullivan"; the case was litigated in reference to the newspaper advertisement "Heed Their Rising Voices". Wachtel founded a tax-exempt fund to cover the expenses of the suit and to assist the nonviolent civil rights movement through a more effective means of fundraising. This organization was named the "Gandhi Society for Human Rights." King served as honorary president for the group. He was displeased with the pace that President Kennedy was using to address the issue of segregation. In 1962, King and the Gandhi Society produced a document that called on the President to follow in the footsteps of Abraham Lincoln and issue an executive order to deliver a blow for civil rights as a kind of Second Emancipation Proclamation. Kennedy did not execute the order.
The FBI was under written directive from Attorney General Robert F. Kennedy when it began tapping King's telephone line in the fall of 1963.
Kennedy was concerned that public allegations of communists in the SCLC would derail the administration's civil rights initiatives. He warned King to discontinue these associations and later felt compelled to issue the written directive that authorized the FBI to wiretap King and other SCLC leaders. FBI Director J. Edgar Hoover feared the civil rights movement and investigated the allegations of communist infiltration. When no evidence emerged to support this, the FBI used the incidental details caught on tape over the next five years in attempts to force King out of his leadership position, in the COINTELPRO program.

King believed that organized, nonviolent protest against the system of southern segregation known as Jim Crow laws would lead to extensive media coverage of the struggle for black equality and voting rights. Journalistic accounts and televised footage of the daily deprivation and indignities suffered by Southern blacks, and of segregationist violence and harassment of civil rights workers and marchers, produced a wave of sympathetic public opinion that convinced the majority of Americans that the civil rights movement was the most important issue in American politics in the early 1960s.

King organized and led marches for blacks' right to vote, desegregation, labor rights, and other basic civil rights. Most of these rights were successfully enacted into the law of the United States with the passage of the Civil Rights Act of 1964 and the 1965 Voting Rights Act.

King and the SCLC put into practice many of the principles of the Christian Left and applied the tactics of nonviolent protest with great success by strategically choosing the method of protest and the places in which protests were carried out. There were often dramatic stand-offs with segregationist authorities, who sometimes turned violent.

King was criticized by many groups during the course of his participation in the civil rights movement. This included opposition by more militant blacks such as Nation of Islam member Malcolm X. Stokely Carmichael was a separatist and disagreed with King's plea for racial integration because he considered it an insult to a uniquely African-American culture. Omali Yeshitela urged Africans to remember the history of violent European colonization and how power was not secured by Europeans through integration, but by violence and force.

The Albany Movement was a desegregation coalition formed in Albany, Georgia, in November 1961. In December, King and the SCLC became involved. The movement mobilized thousands of citizens for a broad-front nonviolent attack on every aspect of segregation within the city and attracted nationwide attention. When King first visited on December 15, 1961, he "had planned to stay a day or so and return home after giving counsel."
The following day he was swept up in a mass arrest of peaceful demonstrators, and he declined bail until the city made concessions. According to King, "that agreement was dishonored and violated by the city" after he left town.

King returned in July 1962 and was given the option of forty-five days in jail or a $178 fine (); he chose jail. Three days into his sentence, Police Chief Laurie Pritchett discreetly arranged for King's fine to be paid and ordered his release. "We had witnessed persons being kicked off lunch counter stools ... ejected from churches ... and thrown into jail ... But for the first time, we witnessed being kicked out of jail." It was later acknowledged by the King Center that Billy Graham was the one who bailed King out of jail during this time.

After nearly a year of intense activism with few tangible results, the movement began to deteriorate. King requested a halt to all demonstrations and a "Day of Penance" to promote nonviolence and maintain the moral high ground. Divisions within the black community and the canny, low-key response by local government defeated efforts. Though the Albany effort proved a key lesson in tactics for King and the national civil rights movement, the national media was highly critical of King's role in the defeat, and the SCLC's lack of results contributed to a growing gulf between the organization and the more radical SNCC. After Albany, King sought to choose engagements for the SCLC in which he could control the circumstances, rather than entering into pre-existing situations.

In April 1963, the SCLC began a campaign against racial segregation and economic injustice in Birmingham, Alabama. The campaign used nonviolent but intentionally confrontational tactics, developed in part by Rev. Wyatt Tee Walker. Black people in Birmingham, organizing with the SCLC, occupied public spaces with marches and sit-ins, openly violating laws that they considered unjust.

King's intent was to provoke mass arrests and "create a situation so crisis-packed that it will inevitably open the door to negotiation." The campaign's early volunteers did not succeed in shutting down the city, or in drawing media attention to the police's actions. Over the concerns of an uncertain King, SCLC strategist James Bevel changed the course of the campaign by recruiting children and young adults to join in the demonstrations.
"Newsweek" called this strategy a Children's Crusade.

During the protests, the Birmingham Police Department, led by Eugene "Bull" Connor, used high-pressure water jets and police dogs against protesters, including children. Footage of the police response was broadcast on national television news and dominated the nation's attention, shocking many white Americans and consolidating black Americans behind the movement. Not all of the demonstrators were peaceful, despite the avowed intentions of the SCLC. In some cases, bystanders attacked the police, who responded with force. King and the SCLC were criticized for putting children in harm's way. But the campaign was a success: Connor lost his job, the "Jim Crow" signs came down, and public places became more open to blacks. King's reputation improved immensely.

King was arrested and jailed early in the campaign—his 13th arrest out of 29. From his cell, he composed the now-famous Letter from Birmingham Jail that responds to calls on the movement to pursue legal channels for social change. King argues that the crisis of racism is too urgent, and the current system too entrenched: "We know through painful experience that freedom is never voluntarily given by the oppressor; it must be demanded by the oppressed." He points out that the Boston Tea Party, a celebrated act of rebellion in the American colonies, was illegal civil disobedience, and that, conversely, "everything Adolf Hitler did in Germany was 'legal'." King also expresses his frustration with white moderates and clergymen too timid to oppose an unjust system:

I have almost reached the regrettable conclusion that the Negro's great stumbling block in his stride toward freedom is not the White Citizen's Councilor or the Ku Klux Klanner, but the white moderate, who is more devoted to "order" than to justice; who prefers a negative peace which is the absence of tension to a positive peace which is the presence of justice; who constantly says: "I agree with you in the goal you seek, but I cannot agree with your methods of direct action"; who paternalistic-ally believes he can set the timetable for another man's freedom; who lives by a mythical concept of time and who constantly advises the Negro to wait for a "more convenient season."

In March 1964, King and the SCLC joined forces with Robert Hayling's then-controversial movement in St. Augustine, Florida. Hayling's group had been affiliated with the NAACP but was forced out of the organization for advocating armed self-defense alongside nonviolent tactics. However, the pacifist SCLC accepted them. King and the SCLC worked to bring white Northern activists to St. Augustine, including a delegation of rabbis and the 72-year-old mother of the governor of Massachusetts, all of whom were arrested.
During June, the movement marched nightly through the city, "often facing counter demonstrations by the Klan, and provoking violence that garnered national media attention." Hundreds of the marchers were arrested and jailed. During the course of this movement, the Civil Rights Act of 1964 was passed.

In December 1964, King and the SCLC joined forces with the Student Nonviolent Coordinating Committee (SNCC) in Selma, Alabama, where the SNCC had been working on voter registration for several months. 
A local judge issued an injunction that barred any gathering of three or more people affiliated with the SNCC, SCLC, DCVL, or any of 41 named civil rights leaders. This injunction temporarily halted civil rights activity until King defied it by speaking at Brown Chapel on January 2, 1965. During the 1965 march to Montgomery, Alabama, violence by state police and others against the peaceful marchers resulted in much publicity, which made Alabama's racism visible nationwide.

On February 6, 1964, King delivered the inaugural speech of a lecture series initiated at the New School called "The American Race Crisis." No audio record of his speech has been found, but in August 2013, almost 50 years later, the school discovered an audiotape with 15 minutes of a question-and-answer session that followed King's address. In these remarks, King referred to a conversation he had recently had with Jawaharlal Nehru in which he compared the sad condition of many African Americans to that of India's untouchables.

King, representing the SCLC, was among the leaders of the "Big Six" civil rights organizations who were instrumental in the organization of the March on Washington for Jobs and Freedom, which took place on August 28, 1963. The other leaders and organizations comprising the Big Six were Roy Wilkins from the National Association for the Advancement of Colored People; Whitney Young, National Urban League; A. Philip Randolph, Brotherhood of Sleeping Car Porters; John Lewis, SNCC; and James L. Farmer Jr., of the Congress of Racial Equality.

Bayard Rustin's open homosexuality, support of democratic socialism, and his former ties to the Communist Party USA caused many white and African-American leaders to demand King distance himself from Rustin, which King agreed to do. However, he did collaborate in the 1963 March on Washington, for which Rustin was the primary logistical and strategic organizer.
For King, this role was another which courted controversy, since he was one of the key figures who acceded to the wishes of United States President John F. Kennedy in changing the focus of the march.

Kennedy initially opposed the march outright, because he was concerned it would negatively impact the drive for passage of civil rights legislation. However, the organizers were firm that the march would proceed. With the march going forward, the Kennedys decided it was important to work to ensure its success. President Kennedy was concerned the turnout would be less than 100,000. Therefore, he enlisted the aid of additional church leaders and Walter Reuther, president of the United Automobile Workers, to help mobilize demonstrators for the cause.
The march originally was conceived as an event to dramatize the desperate condition of blacks in the southern U.S. and an opportunity to place organizers' concerns and grievances squarely before the seat of power in the nation's capital. Organizers intended to denounce the federal government for its failure to safeguard the civil rights and physical safety of civil rights workers and blacks. The group acquiesced to presidential pressure and influence, and the event ultimately took on a far less strident tone.
As a result, some civil rights activists felt it presented an inaccurate, sanitized pageant of racial harmony; Malcolm X called it the "Farce on Washington", and the Nation of Islam forbade its members from attending the march.

The march made specific demands: an end to racial segregation in public schools; meaningful civil rights legislation, including a law prohibiting racial discrimination in employment; protection of civil rights workers from police brutality; a $2 minimum wage for all workers (); and self-government for Washington, D.C., then governed by congressional committee.
Despite tensions, the march was a resounding success. More than a quarter of a million people of diverse ethnicities attended the event, sprawling from the steps of the Lincoln Memorial onto the National Mall and around the reflecting pool. At the time, it was the largest gathering of protesters in Washington, D.C.'s history.

King delivered a 17-minute speech, later known as "I Have a Dream". In the speech's most famous passage—in which he departed from his prepared text, possibly at the prompting of Mahalia Jackson, who shouted behind him, "Tell them about the dream!"—King said:

"I Have a Dream" came to be regarded as one of the finest speeches in the history of American oratory.
The March, and especially King's speech, helped put civil rights at the top of the agenda of reformers in the United States and facilitated passage of the Civil Rights Act of 1964.

The original typewritten copy of the speech, including King's handwritten notes on it, was discovered in 1984 to be in the hands of George Raveling, the first African-American basketball coach of the University of Iowa. In 1963, Raveling, then 26, was standing near the podium, and immediately after the oration, impulsively asked King if he could have his copy of the speech. He got it.

Acting on James Bevel's call for a march from Selma to Montgomery, King, Bevel, and the SCLC, in partial collaboration with SNCC, attempted to organize the march to the state's capital. The first attempt to march on March 7, 1965, was aborted because of mob and police violence against the demonstrators. This day has become known as Bloody Sunday and was a major turning point in the effort to gain public support for the civil rights movement. It was the clearest demonstration up to that time of the dramatic potential of King's nonviolence strategy. King, however, was not present.

On March 5, King met with officials in the Johnson Administration in order to request an injunction against any prosecution of the demonstrators. He did not attend the march due to church duties, but he later wrote, "If I had any idea that the state troopers would use the kind of brutality they did, I would have felt compelled to give up my church duties altogether to lead the line." Footage of police brutality against the protesters was broadcast extensively and aroused national public outrage.

King next attempted to organize a march for March 9. The SCLC petitioned for an injunction in federal court against the State of Alabama; this was denied and the judge issued an order blocking the march until after a hearing. Nonetheless, King led marchers on March 9 to the Edmund Pettus Bridge in Selma, then held a short prayer session before turning the marchers around and asking them to disperse so as not to violate the court order. The unexpected ending of this second march aroused the surprise and anger of many within the local movement. The march finally went ahead fully on March 25, 1965.
At the conclusion of the march on the steps of the state capitol, King delivered a speech that became known as "How Long, Not Long." In it, King stated that equal rights for African Americans could not be far away, "because the arc of the moral universe is long, but it bends toward justice" and "you shall reap what you sow".

In 1966, after several successes in the south, King, Bevel, and others in the civil rights organizations took the movement to the North, with Chicago as their first destination. King and Ralph Abernathy, both from the middle class, moved into a building at 1550 S. Hamlin Avenue, in the slums of North Lawndale
on Chicago's West Side, as an educational experience and to demonstrate their support and empathy for the poor.

The SCLC formed a coalition with CCCO, Coordinating Council of Community Organizations, an organization founded by Albert Raby, and the combined organizations' efforts were fostered under the aegis of the Chicago Freedom Movement.
During that spring, several white couple/black couple tests of real estate offices uncovered racial steering: discriminatory processing of housing requests by couples who were exact matches in income, background, number of children, and other attributes. Several larger marches were planned and executed: in Bogan, Belmont Cragin, Jefferson Park, Evergreen Park (a suburb southwest of Chicago), Gage Park, Marquette Park, and others.

King later stated and Abernathy wrote that the movement received a worse reception in Chicago than in the South. Marches, especially the one through Marquette Park on August 5, 1966, were met by thrown bottles and screaming throngs. Rioting seemed very possible.
King's beliefs militated against his staging a violent event, and he negotiated an agreement with Mayor Richard J. Daley to cancel a march in order to avoid the violence that he feared would result.
King was hit by a brick during one march but continued to lead marches in the face of personal danger.

When King and his allies returned to the South, they left Jesse Jackson, a seminary student who had previously joined the movement in the South, in charge of their organization.
Jackson continued their struggle for civil rights by organizing the Operation Breadbasket movement that targeted chain stores that did not deal fairly with blacks.

A 1967 CIA document declassified in 2017 downplayed King's role in the "black militant situation" in Chicago, with a source stating that King "sought at least constructive, positive projects."

King was long opposed to American involvement in the Vietnam War, but at first avoided the topic in public speeches in order to avoid the interference with civil rights goals that criticism of President Johnson's policies might have created. At the urging of SCLC's former Director of Direct Action and now the head of the Spring Mobilization Committee to End the War in Vietnam, James Bevel, King eventually agreed to publicly oppose the war as opposition was growing among the American public.

During an April 4, 1967, appearance at the New York City Riverside Church—exactly one year before his death—King delivered a speech titled "."
He spoke strongly against the U.S.'s role in the war, arguing that the U.S. was in Vietnam "to occupy it as an American colony" and calling the U.S. government "the greatest purveyor of violence in the world today." He also connected the war with economic injustice, arguing that the country needed serious moral change:

King also opposed the Vietnam War because it took money and resources that could have been spent on social welfare at home. The United States Congress was spending more and more on the military and less and less on anti-poverty programs at the same time. He summed up this aspect by saying, "A nation that continues year after year to spend more money on military defense than on programs of social uplift is approaching spiritual death." He stated that North Vietnam "did not begin to send in any large number of supplies or men until American forces had arrived in the tens of thousands", and accused the U.S. of having killed a million Vietnamese, "mostly children."
King also criticized American opposition to North Vietnam's land reforms.

King's opposition cost him significant support among white allies, including President Johnson, Billy Graham, union leaders and powerful publishers.
"The press is being stacked against me", King said,
complaining of what he described as a double standard that applauded his nonviolence at home, but deplored it when applied "toward little brown Vietnamese children."
"Life" magazine called the speech "demagogic slander that sounded like a script for Radio Hanoi", and "The Washington Post" declared that King had "diminished his usefulness to his cause, his country, his people."
The "Beyond Vietnam" speech reflected King's evolving political advocacy in his later years, which paralleled the teachings of the progressive Highlander Research and Education Center, with which he was affiliated.
King began to speak of the need for fundamental changes in the political and economic life of the nation, and more frequently expressed his opposition to the war and his desire to see a redistribution of resources to correct racial and economic injustice.
He guarded his language in public to avoid being linked to communism by his enemies, but in private he sometimes spoke of his support for democratic socialism.

In a 1952 letter to Coretta Scott, he said: "I imagine you already know that I am much more socialistic in my economic theory than capitalistic ..." In one speech, he stated that "something is wrong with capitalism" and claimed, "There must be a better distribution of wealth, and maybe America must move toward a democratic socialism."
King had read Marx while at Morehouse, but while he rejected "traditional capitalism", he also rejected communism because of its "materialistic interpretation of history" that denied religion, its "ethical relativism", and its "political totalitarianism."

King also stated in "Beyond Vietnam" that "true compassion is more than flinging a coin to a beggar ... it comes to see that an edifice which produces beggars needs restructuring." King quoted a United States official who said that from Vietnam to Latin America, the country was "on the wrong side of a world revolution." King condemned America's "alliance with the landed gentry of Latin America", and said that the U.S. should support "the shirtless and barefoot people" in the Third World rather than suppressing their attempts at revolution.

King's stance on Vietnam encouraged Allard K. Lowenstein, William Sloane Coffin and Norman Thomas, with the support of anti-war Democrats, to attempt to persuade King to run against President Johnson in the 1968 United States presidential election. King contemplated but ultimately decided against the proposal on the grounds that he felt uneasy with politics and considered himself better suited for his morally unambiguous role as an activist.

On April 15, 1967, King participated and spoke at an anti-war march from Manhattan's Central Park to the United Nations. The march was organized by the Spring Mobilization Committee to End the War in Vietnam and initiated by its chairman, James Bevel. At the U.N. King also brought up issues of civil rights and the draft.

Seeing an opportunity to unite civil rights activists and anti-war activists, Bevel convinced King to become even more active in the anti-war effort. Despite his growing public opposition towards the Vietnam War, King was also not fond of the hippie culture which developed from the anti-war movement. In his 1967 Massey Lecture, King stated:

On January 13, 1968 (the day after President Johnson's State of the Union Address), King called for a large march on Washington against "one of history's most cruel and senseless wars."

In 1968, King and the SCLC organized the "Poor People's Campaign" to address issues of economic justice. King traveled the country to assemble "a multiracial army of the poor" that would march on Washington to engage in nonviolent civil disobedience at the Capitol until Congress created an "economic bill of rights" for poor Americans.

The campaign was preceded by King's final book, "" which laid out his view of how to address social issues and poverty. King quoted from Henry George and George's book, "Progress and Poverty", particularly in support of a guaranteed basic income. The campaign culminated in a march on Washington, D.C., demanding economic aid to the poorest communities of the United States.

King and the SCLC called on the government to invest in rebuilding America's cities. He felt that Congress had shown "hostility to the poor" by spending "military funds with alacrity and generosity." He contrasted this with the situation faced by poor Americans, claiming that Congress had merely provided "poverty funds with miserliness." His vision was for change that was more revolutionary than mere reform: he cited systematic flaws of "racism, poverty, militarism and materialism", and argued that "reconstruction of society itself is the real issue to be faced."

The Poor People's Campaign was controversial even within the civil rights movement. Rustin resigned from the march, stating that the goals of the campaign were too broad, that its demands were unrealizable, and that he thought that these campaigns would accelerate the backlash and repression on the poor and the black.

The plan to set up a shantytown in Washington, D.C., was carried out soon after the April 4 assassination. Criticism of King's plan was subdued in the wake of his death, and the SCLC received an unprecedented wave of donations for the purpose of carrying it out. The campaign officially began in Memphis, on May 2, at the hotel where King was murdered.

Thousands of demonstrators arrived on the National Mall and established a camp they called "Resurrection City." They stayed for six weeks.

On March 29, 1968, King went to Memphis, Tennessee, in support of the black sanitary public works employees, who were represented by AFSCME Local 1733. The workers had been on strike since March 12 for higher wages and better treatment. In one incident, black street repairmen received pay for two hours when they were sent home because of bad weather, but white employees were paid for the full day.

On April 3, King addressed a rally and delivered his "I've Been to the Mountaintop" address at Mason Temple, the world headquarters of the Church of God in Christ. King's flight to Memphis had been delayed by a bomb threat against his plane.
In the prophetic peroration of the last speech of his life, in reference to the bomb threat, King said the following:

King was booked in Room 306 at the Lorraine Motel (owned by Walter Bailey) in Memphis. Abernathy, who was present at the assassination, testified to the United States House Select Committee on Assassinations that King and his entourage stayed at Room 306 so often that it was known as the "King-Abernathy suite."
According to Jesse Jackson, who was present, King's last words on the balcony before his assassination were spoken to musician Ben Branch, who was scheduled to perform that night at an event King was attending: "Ben, make sure you play 'Take My Hand, Precious Lord' in the meeting tonight. Play it real pretty."

King was fatally shot by James Earl Ray at 6:01 p.m., April 4, 1968, as he stood on the motel's second-floor balcony. The bullet entered through his right cheek, smashing his jaw, then traveled down his spinal cord before lodging in his shoulder.
Abernathy heard the shot from inside the motel room and ran to the balcony to find King on the floor.
Jackson stated after the shooting that he cradled King's head as King lay on the balcony, but this account was disputed by other colleagues of King; Jackson later changed his statement to say that he had "reached out" for King.

After emergency chest surgery, King died at St. Joseph's Hospital at 7:05 p.m.
According to biographer Taylor Branch, King's autopsy revealed that though only 39 years old, he "had the heart of a 60 year old", which Branch attributed to the stress of 13 years in the civil rights movement.

The assassination led to a nationwide wave of race riots in Washington, D.C., Chicago, Baltimore, Louisville, Kansas City, and dozens of other cities. Presidential candidate Robert F. Kennedy was on his way to Indianapolis for a campaign rally when he was informed of King's death. He gave a short, improvised speech to the gathering of supporters informing them of the tragedy and urging them to continue King's ideal of nonviolence. The following day, he delivered a prepared response in Cleveland. James Farmer Jr., and other civil rights leaders also called for non-violent action, while the more militant Stokely Carmichael called for a more forceful response. The city of Memphis quickly settled the strike on terms favorable to the sanitation workers.

President Lyndon B. Johnson declared April 7 a national day of mourning for the civil rights leader.
Vice President Hubert Humphrey attended King's funeral on behalf of the President, as there were fears that Johnson's presence might incite protests and perhaps violence.
At his widow's request, King's last sermon at Ebenezer Baptist Church was played at the funeral,
a recording of his "Drum Major" sermon, given on February 4, 1968. In that sermon, King made a request that at his funeral no mention of his awards and honors be made, but that it be said that he tried to "feed the hungry", "clothe the naked", "be right on the [Vietnam] war question", and "love and serve humanity."

His good friend Mahalia Jackson sang his favorite hymn, "Take My Hand, Precious Lord", at the funeral.

Two months after King's death, James Earl Ray—who was on the loose from a previous prison escape—was captured at London Heathrow Airport while trying to leave England on a false Canadian passport. He was using the alias Ramon George Sneyd on his way to white-ruled Rhodesia.
Ray was quickly extradited to Tennessee and charged with King's murder. He confessed to the assassination on March 10, 1969, though he recanted this confession three days later.
On the advice of his attorney Percy Foreman, Ray pleaded guilty to avoid a trial conviction and thus the possibility of receiving the death penalty. He was sentenced to a 99-year prison term.
Ray later claimed a man he met in Montreal, Quebec, with the alias "Raoul" was involved and that the assassination was the result of a conspiracy.
He spent the remainder of his life attempting, unsuccessfully, to withdraw his guilty plea and secure the trial he never had. Ray died in 1998 at age 70.

Ray's lawyers maintained he was a scapegoat similar to the way that John F. Kennedy's assassin Lee Harvey Oswald is seen by conspiracy theorists.
Supporters of this assertion said that Ray's confession was given under pressure and that he had been threatened with the death penalty.
They admitted that Ray was a thief and burglar, but claimed that he had no record of committing violent crimes with a weapon. However, prison records in different U.S. cities have shown that he was incarcerated on numerous occasions for charges of armed robbery. In a 2008 interview with CNN, Jerry Ray, the younger brother of James Earl Ray, claimed that James was smart and was sometimes able to get away with armed robbery. Jerry Ray said that he had assisted his brother on one such robbery. "I never been with nobody as bold as he is," Jerry said. "He just walked in and put that gun on somebody, it was just like it's an everyday thing."

Those suspecting a conspiracy in the assassination point to the two successive ballistics tests which proved that a rifle similar to Ray's Remington Gamemaster had been the murder weapon. Those tests did not implicate Ray's specific rifle.
Witnesses near King at the moment of his death said that the shot came from another location. They said that it came from behind thick shrubbery near the boarding house—which had been cut away in the days following the assassination—and not from the boarding house window. However, Ray's fingerprints were found on various objects (a rifle, a pair of binoculars, articles of clothing, a newspaper) that were left in the bathroom where it was determined the gunfire came from. An examination of the rifle containing Ray's fingerprints also determined that at least one shot was fired from the firearm at the time of the assassination.

In 1997, King's son Dexter Scott King met with Ray, and publicly supported Ray's efforts to obtain a new trial.

Two years later, King's widow Coretta Scott King and the couple's children won a wrongful death claim against Loyd Jowers and "other unknown co-conspirators." Jowers claimed to have received $100,000 to arrange King's assassination. The jury of six whites and six blacks found in favor of the King family, finding Jowers to be complicit in a conspiracy against King and that government agencies were party to the assassination.
William F. Pepper represented the King family in the trial.

In 2000, the U.S. Department of Justice completed the investigation into Jowers' claims but did not find evidence to support allegations about conspiracy. The investigation report recommended no further investigation unless some new reliable facts are presented. A sister of Jowers admitted that he had fabricated the story so he could make $300,000 from selling the story, and she in turn corroborated his story in order to get some money to pay her income tax.

In 2002, "The New York Times" reported that a church minister, Rev. Ronald Denton Wilson, claimed his father, Henry Clay Wilson—not James Earl Ray—assassinated King. He stated, "It wasn't a racist thing; he thought Martin Luther King was connected with communism, and he wanted to get him out of the way." Wilson provided no evidence to back up his claims.

King researchers David Garrow and Gerald Posner disagreed with William F. Pepper's claims that the government killed King.
In 2003, Pepper published a book about the long investigation and trial, as well as his representation of James Earl Ray in his bid for a trial, laying out the evidence and criticizing other accounts.
King's friend and colleague, James Bevel, also disputed the argument that Ray acted alone, stating, "There is no way a ten-cent white boy could develop a plan to kill a million-dollar black man."
In 2004, Jesse Jackson stated:

King's main legacy was to secure progress on civil rights in the U.S. Just days after King's assassination, Congress passed the Civil Rights Act of 1968.
Title VIII of the Act, commonly known as the Fair Housing Act, prohibited discrimination in housing and housing-related transactions on the basis of race, religion, or national origin (later expanded to include sex, familial status, and disability). This legislation was seen as a tribute to King's struggle in his final years to combat residential discrimination in the U.S.

Internationally, King's legacy includes influences on the Black Consciousness Movement and civil rights movement in South Africa.
King's work was cited by and served as an inspiration for South African leader Albert Lutuli, who fought for racial justice in his country and was later awarded the Nobel Prize. The day following King's assassination, school teacher Jane Elliott conducted her first "Blue Eyes/Brown Eyes" exercise with her class of elementary school students in Riceville, Iowa. Her purpose was to help them understand King's death as it related to racism, something they little understood as they lived in a predominantly white community.

King has become a national icon in the history of American liberalism and American progressivism. King also influenced Irish politician and activist John Hume. Hume, the former leader of the Social Democratic and Labour Party, cited King's legacy as quintessential to the Northern Irish civil rights movement and the signing of the Good Friday Agreement, calling him "one of my great heroes of the century."

King's wife Coretta Scott King followed in her husband's footsteps and was active in matters of social justice and civil rights until her death in 2006. The same year that Martin Luther King was assassinated, she established the King Center in Atlanta, Georgia, dedicated to preserving his legacy and the work of championing nonviolent conflict resolution and tolerance worldwide.
Their son, Dexter King, serves as the center's chairman.
Daughter Yolanda King, who died in 2007, was a motivational speaker, author and founder of Higher Ground Productions, an organization specializing in diversity training.

Even within the King family, members disagree about his religious and political views about gay, lesbian, bisexual and transgender people. King's widow Coretta publicly said that she believed her husband would have supported gay rights. However, his youngest child, Bernice King, has said publicly that he would have been opposed to gay marriage.

On February 4, 1968, at the Ebenezer Baptist Church, in speaking about how he wished to be remembered after his death, King stated:

Beginning in 1971, cities such as St. Louis, Missouri, and states established annual holidays to honor King. At the White House Rose Garden on November 2, 1983, President Ronald Reagan signed a bill creating a federal holiday to honor King. Observed for the first time on January 20, 1986, it is called Martin Luther King Jr. Day. Following President George H. W. Bush's 1992 proclamation, the holiday is observed on the third Monday of January each year, near the time of King's birthday.
On January 17, 2000, for the first time, Martin Luther King Jr. Day was officially observed in all fifty U.S. states.
Arizona (1992), New Hampshire (1999) and Utah (2000) were the last three states to recognize the holiday. Utah previously celebrated the holiday at the same time but under the name Human Rights Day.

King is remembered as a martyr by the Episcopal Church in the United States of America with an annual feast day on the anniversary of his death, April 4. The Evangelical Lutheran Church in America commemorates King liturgically on the anniversary of his birth, January 15.

In the United Kingdom, The Northumbria and Newcastle Universities Martin Luther King Peace Committee exists to honor King's legacy, as represented by his final visit to the UK to receive an honorary degree from Newcastle University in 1967. The Peace Committee operates out of the chaplaincies of the city's two universities, Northumbria and Newcastle, both of which remain centres for the study of Martin Luther King and the US civil rights movement. Inspired by King's vision, it undertakes a range of activities across the UK as it seeks to "build cultures of peace."

In 2017, Newcastle University unveiled a bronze statue of King to celebrate the 50th anniversary of his honorary doctorate ceremony. The Students Union also voted to rename their bar 'Luthers'.

As a Christian minister, King's main influence was Jesus Christ and the Christian gospels, which he would almost always quote in his religious meetings, speeches at church, and in public discourses. King's faith was strongly based in Jesus' commandment of loving your neighbor as yourself, loving God above all, and loving your enemies, praying for them and blessing them. His nonviolent thought was also based in the injunction to "turn the other cheek" in the Sermon on the Mount, and Jesus' teaching of putting the sword back into its place (Matthew 26:52). In his famous Letter from Birmingham Jail, King urged action consistent with what he describes as Jesus' "extremist" love, and also quoted numerous other Christian pacifist authors, which was very usual for him. In another sermon, he stated:

In his speech "I've Been to the Mountaintop", he stated that he just wanted to do God's will.

Veteran African-American civil rights activist Bayard Rustin was King's first regular advisor on nonviolence. King was also advised by the white activists Harris Wofford and Glenn Smiley. Rustin and Smiley came from the Christian pacifist tradition, and Wofford and Rustin both studied Gandhi's teachings. Rustin had applied nonviolence with the Journey of Reconciliation campaign in the 1940s, and Wofford had been promoting Gandhism to Southern blacks since the early 1950s.

King had initially known little about Gandhi and rarely used the term "nonviolence" during his early years of activism in the early 1950s. King initially believed in and practiced self-defense, even obtaining guns in his household as a means of defense against possible attackers. The pacifists guided King by showing him the alternative of nonviolent resistance, arguing that this would be a better means to accomplish his goals of civil rights than self-defense. King then vowed to no longer personally use arms.

In the aftermath of the boycott, King wrote "Stride Toward Freedom", which included the chapter "Pilgrimage to Nonviolence." King outlined his understanding of nonviolence, which seeks to win an opponent to friendship, rather than to humiliate or defeat him. The chapter draws from an address by Wofford, with Rustin and Stanley Levison also providing guidance and ghostwriting.

King was inspired by Mahatma Gandhi and his success with nonviolent activism, and as a theology student, King described Gandhi as being one of the "individuals who greatly reveal the working of the Spirit of God". King had "for a long time ... wanted to take a trip to India." With assistance from Harris Wofford, the American Friends Service Committee, and other supporters, he was able to fund the journey in April 1959. The trip to India affected King, deepening his understanding of nonviolent resistance and his commitment to America's struggle for civil rights. In a radio address made during his final evening in India, King reflected, "Since being in India, I am more convinced than ever before that the method of nonviolent resistance is the most potent weapon available to oppressed people in their struggle for justice and human dignity."

King's admiration of Gandhi's nonviolence did not diminish in later years. He went so far as to hold up his example when receiving the Nobel Peace Prize in 1964, hailing the "successful precedent" of using nonviolence "in a magnificent way by Mohandas K. Gandhi to challenge the might of the British Empire ... He struggled only with the weapons of truth, soul force, non-injury and courage."

Another influence for King's nonviolent method was Henry David Thoreau's essay "On Civil Disobedience" and its theme of refusing to cooperate with an evil system. He also was greatly influenced by the works of Protestant theologians Reinhold Niebuhr and Paul Tillich, and said that Walter Rauschenbusch's "Christianity and the Social Crisis" left an "indelible imprint" on his thinking by giving him a theological grounding for his social concerns. King was moved by Rauschenbusch's vision of Christians spreading social unrest in "perpetual but friendly conflict" with the state, simultaneously critiquing it and calling it to act as an instrument of justice. He was apparently unaware of the American tradition of Christian pacifism exemplified by Adin Ballou and William Lloyd Garrison King frequently referred to Jesus' Sermon on the Mount as central for his work. King also sometimes used the concept of "agape" (brotherly Christian love). However, after 1960, he ceased employing it in his writings.

Even after renouncing his personal use of guns, King had a complex relationship with the phenomenon of self-defense in the movement. He publicly discouraged it as a widespread practice, but acknowledged that it was sometimes necessary. Throughout his career King was frequently protected by other civil rights activists who carried arms, such as Colonel Stone Johnson, Robert Hayling, and the Deacons for Defense and Justice.

As the leader of the SCLC, King maintained a policy of not publicly endorsing a U.S. political party or candidate: "I feel someone must remain in the position of non-alignment, so that he can look objectively at both parties and be the conscience of both—not the servant or master of either."
In a 1958 interview, he expressed his view that neither party was perfect, saying, "I don't think the Republican party is a party full of the almighty God nor is the Democratic party. They both have weaknesses ... And I'm not inextricably bound to either party."
King did praise Democratic Senator Paul Douglas of Illinois as being the "greatest of all senators" because of his fierce advocacy for civil rights causes over the years.

King critiqued both parties' performance on promoting racial equality: 

Although King never publicly supported a political party or candidate for president, in a letter to a civil rights supporter in October 1956 he said that he was undecided as to whether he would vote for Adlai Stevenson or Dwight Eisenhower, but that "In the past I always voted the Democratic ticket."
In his autobiography, King says that in 1960 he privately voted for Democratic candidate John F. Kennedy: "I felt that Kennedy would make the best president. I never came out with an endorsement. My father did, but I never made one." King adds that he likely would have made an exception to his non-endorsement policy for a second Kennedy term, saying "Had President Kennedy lived, I would probably have endorsed him in 1964."

In 1964, King urged his supporters "and all people of goodwill" to vote against Republican Senator Barry Goldwater for president, saying that his election "would be a tragedy, and certainly suicidal almost, for the nation and the world."

King supported the ideals of democratic socialism, although he was reluctant to speak directly of this support due to the anti-communist sentiment being projected throughout the United States at the time, and the association of socialism with communism. King believed that capitalism could not adequately provide the basic necessities of many American people, particularly the African-American community.

King stated that black Americans, as well as other disadvantaged Americans, should be compensated for historical wrongs. In an interview conducted for "Playboy" in 1965, he said that granting black Americans only equality could not realistically close the economic gap between them and whites. King said that he did not seek a full restitution of wages lost to slavery, which he believed impossible, but proposed a government compensatory program of $50 billion over ten years to all disadvantaged groups.

He posited that "the money spent would be more than amply justified by the benefits that would accrue to the nation through a spectacular decline in school dropouts, family breakups, crime rates, illegitimacy, swollen relief rolls, rioting and other social evils." He presented this idea as an application of the common law regarding settlement of unpaid labor, but clarified that he felt that the money should not be spent exclusively on blacks. He stated, "It should benefit the disadvantaged of "all" races."

On being awarded the Planned Parenthood Federation of America's Margaret Sanger Award on May 5, 1966, King said:

FBI director J. Edgar Hoover personally ordered surveillance of King, with the intent to undermine his power as a civil rights leader.
According to the Church Committee, a 1975 investigation by the U.S. Congress, "From December 1963 until his death in 1968, Martin Luther King Jr. was the target of an intensive campaign by the Federal Bureau of Investigation to 'neutralize' him as an effective civil rights leader."

In the fall of 1963, the FBI received authorization from Attorney General Robert F. Kennedy to proceed with wiretapping of King's phone lines. The Bureau informed President John F. Kennedy. He and his brother unsuccessfully tried to persuade King to dissociate himself from Stanley Levison, a New York lawyer who had been involved with Communist Party USA. Although Robert Kennedy only gave written approval for limited wiretapping of King's telephone lines "on a trial basis, for a month or so", Hoover extended the clearance so his men were "unshackled" to look for evidence in any areas of King's life they deemed worthy.

The Bureau placed wiretaps on the home and office phone lines of Levison and King, and bugged King's rooms in hotels as he traveled across the country.
In 1967, Hoover listed the SCLC as a black nationalist hate group, with the instructions: "No opportunity should be missed to exploit through counterintelligence techniques the organizational and personal conflicts of the leaderships of the groups ... to insure the targeted group is disrupted, ridiculed, or discredited."

In a secret operation code-named "Minaret", the National Security Agency (NSA) monitored the communications of leading Americans, including King, who criticized the U.S. war in Vietnam. A review by the NSA itself concluded that Minaret was "disreputable if not outright illegal."

For years, Hoover had been suspicious about potential influence of communists in social movements such as labor unions and civil rights.
Hoover directed the FBI to track King in 1957, and the SCLC as it was established (it did not have a full-time executive director until 1960). The investigations were largely superficial until 1962, when the FBI learned that one of King's most trusted advisers was New York City lawyer Stanley Levison.

The FBI feared Levison was working as an "agent of influence" over King, in spite of its own reports in 1963 that Levison had left the Party and was no longer associated in business dealings with them. Another King lieutenant, Hunter Pitts O'Dell, was also linked to the Communist Party by sworn testimony before the House Un-American Activities Committee (HUAC).
However, by 1976 the FBI had acknowledged that it had not obtained any evidence that King himself or the SCLC were actually involved with any communist organizations.

For his part, King adamantly denied having any connections to communism. In a 1965 "Playboy" interview, he stated that "there are as many Communists in this freedom movement as there are Eskimos in Florida." He argued that Hoover was "following the path of appeasement of political powers in the South" and that his concern for communist infiltration of the civil rights movement was meant to "aid and abet the salacious claims of southern racists and the extreme right-wing elements."
Hoover did not believe King's pledge of innocence and replied by saying that King was "the most notorious liar in the country."
After King gave his "I Have A Dream" speech during the March on Washington on August 28, 1963, the FBI described King as "the most dangerous and effective Negro leader in the country." It alleged that he was "knowingly, willingly and regularly cooperating with and taking guidance from communists."

The attempt to prove that King was a communist was related to the feeling of many segregationists that blacks in the South were happy with their lot but had been stirred up by "communists" and "outside agitators."
However, the 1950s and '60s civil rights movement arose from activism within the black community dating back to before World War I. King said that "the Negro revolution is a genuine revolution, born from the same womb that produces all massive social upheavals—the womb of intolerable conditions and unendurable situations."

CIA files declassified in 2017 revealed that the agency was investigating possible links between King and Communism after a Washington Post article dated November 4, 1964 claimed he was invited to the Soviet Union and that Ralph Abernathy, spokesman for subject, refused to comment on the source of the invitation.

Having concluded that King was dangerous due to communist infiltration, the FBI attempted to discredit King through revelations regarding his private life. FBI surveillance of King, some of it since made public, attempted to demonstrate that he also engaged in numerous extramarital affairs. Lyndon Johnson once said that King was a "hypocritical preacher."

In his 1989 autobiography "And the Walls Came Tumbling Down", Ralph Abernathy stated that King had a "weakness for women", although they "all understood and believed in the biblical prohibition against sex outside of marriage. It was just that he had a particularly difficult time with that temptation." In a later interview, Abernathy said that he only wrote the term "womanizing", that he did not specifically say King had extramarital sex and that the infidelities King had were emotional rather than sexual.

Abernathy criticized the media for sensationalizing the statements he wrote about King's affairs, such as the allegation that he admitted in his book that King had a sexual affair the night before he was assassinated. In his original wording, Abernathy had claimed he saw King coming out of his room with a lady when he awoke the next morning and later claimed that "he may have been in there discussing and debating and trying to get her to go along with the movement, I don't know."

In his 1986 book "Bearing the Cross", David Garrow wrote about a number of extramarital affairs, including one woman King saw almost daily. According to Garrow, "that relationship ... increasingly became the emotional centerpiece of King's life, but it did not eliminate the incidental couplings ... of King's travels." He alleged that King explained his extramarital affairs as "a form of anxiety reduction." Garrow asserted that King's supposed promiscuity caused him "painful and at times overwhelming guilt." King's wife Coretta appeared to have accepted his affairs with equanimity, saying once that "all that other business just doesn't have a place in the very high level relationship we enjoyed." Shortly after "Bearing the Cross" was released, civil rights author Howell Raines gave the book a positive review but opined that Garrow's allegations about King's sex life were "sensational" and stated that Garrow was "amassing facts rather than analyzing them."

The FBI distributed reports regarding such affairs to the executive branch, friendly reporters, potential coalition partners and funding sources of the SCLC, and King's family. The bureau also sent anonymous letters to King threatening to reveal information if he did not cease his civil rights work. The FBI–King suicide letter sent to King just before he received the Nobel Peace Prize read, in part:

The American public, the church organizations that have been helping—Protestants, Catholics and Jews will know you for what you are—an evil beast. So will others who have backed you. You are done. King, there is only one thing left for you to do. You know what it is. You have just 34 days in which to do (this exact number has been selected for a specific reason, it has definite practical significant ). You are done. There is but one way out for you. You better take it before your filthy fraudulent self is bared to the nation.

The letter was accompanied by a tape recording—excerpted from FBI wiretaps—of several of King's extramarital liaisons. King interpreted this package as an attempt to drive him to suicide, although William Sullivan, head of the Domestic Intelligence Division at the time, argued that it may have only been intended to "convince Dr. King to resign from the SCLC." King refused to give in to the FBI's threats.

In 1977, Judge John Lewis Smith Jr. ordered all known copies of the recorded audiotapes and written transcripts resulting from the FBI's electronic surveillance of King between 1963 and 1968 to be held in the National Archives and sealed from public access until 2027.

A fire station was located across from the Lorraine Motel, next to the boarding house in which James Earl Ray was staying. Police officers were stationed in the fire station to keep King under surveillance.
Agents were watching King at the time he was shot.
Immediately following the shooting, officers rushed out of the station to the motel. Marrell McCollough, an undercover police officer, was the first person to administer first aid to King.
The antagonism between King and the FBI, the lack of an all points bulletin to find the killer, and the police presence nearby led to speculation that the FBI was involved in the assassination.

King was awarded at least fifty honorary degrees from colleges and universities.
On October 14, 1964, King became the youngest winner of the Nobel Peace Prize, which was awarded to him for leading nonviolent resistance to racial prejudice in the U.S.
In 1965, he was awarded the American Liberties Medallion by the American Jewish Committee for his "exceptional advancement of the principles of human liberty."
In his acceptance remarks, King said, "Freedom is one thing. You have it all or you are not free."

In 1957, he was awarded the Spingarn Medal from the NAACP. Two years later, he won the Anisfield-Wolf Book Award for his book "Stride Toward Freedom: The Montgomery Story".
In 1966, the Planned Parenthood Federation of America awarded King the Margaret Sanger Award for "his courageous resistance to bigotry and his lifelong dedication to the advancement of social justice and human dignity."
Also in 1966, King was elected as a fellow of the American Academy of Arts and Sciences. In November 1967 he made a 24-hour trip to the United Kingdom to receive an honorary degree from Newcastle University, being the first African-American to be so honoured by Newcastle. In a moving impromptu acceptance speech, he said

There are three urgent and indeed great problems that we face not only in the United States of America but all over the world today. That is the problem of racism, the problem of poverty and the problem of war.

In 1971 he was posthumously awarded a Grammy Award for Best Spoken Word Album for his "Why I Oppose the War in Vietnam".

In 1977, the Presidential Medal of Freedom was posthumously awarded to King by President Jimmy Carter. The citation read:

Martin Luther King Jr. was the conscience of his generation. He gazed upon the great wall of segregation and saw that the power of love could bring it down. From the pain and exhaustion of his fight to fulfill the promises of our founding fathers for our humblest citizens, he wrung his eloquent statement of his dream for America. He made our nation stronger because he made it better. His dream sustains us yet.

King and his wife were also awarded the Congressional Gold Medal in 2004.

King was second in Gallup's List of Most Widely Admired People of the 20th Century.
In 1963, he was named "Time" Person of the Year, and in 2000, he was voted sixth in an online "Person of the Century" poll by the same magazine. King placed third in the Greatest American contest conducted by the Discovery Channel and AOL.

On April 20, 2016, Treasury Secretary Jacob Lew announced that the $5, $10, and $20 bills would all undergo redesign prior to 2020. Lew said that while Lincoln would remain on the obverse of the $5 bill, the reverse would be redesigned to depict various historical events that had occurred at the Lincoln Memorial. Among the planned designs are images from King's "I Have a Dream" speech and the 1939 concert by opera singer Marian Anderson.





Speeches and interviews


</doc>
<doc id="20077" url="https://en.wikipedia.org/wiki?curid=20077" title="MLK (disambiguation)">
MLK (disambiguation)

MLK are the initials of Martin Luther King Jr. (1929–1968).

MLK or mlk may also refer to:





</doc>
<doc id="20079" url="https://en.wikipedia.org/wiki?curid=20079" title="Martin Luther King (disambiguation)">
Martin Luther King (disambiguation)

Martin Luther King Jr. (1929–1968) was a minister and Civil Rights activist.

Martin Luther King may also refer to:



</doc>
<doc id="20080" url="https://en.wikipedia.org/wiki?curid=20080" title="Marino Marini (sculptor)">
Marino Marini (sculptor)

Marino Marini (27 February 1901 – 6 August 1980) was an Italian sculptor.

He attended the Accademia di Belle Arti in Florence in 1917. Although he never abandoned painting, Marini devoted himself primarily to sculpture from about 1922. From this time his work was influenced by Etruscan art and the sculpture of Arturo Martini. Marini succeeded Martini as professor at the Scuola d’Arte di Villa Reale in Monza, near Milan, in 1929, a position he retained until 1940.

During this period, Marini traveled frequently to Paris, where he associated with Massimo Campigli, Giorgio de Chirico, Alberto Magnelli, and Filippo Tibertelli de Pisis. In 1936 he moved to Tenero-Locarno, in Ticino Canton, Switzerland; during the following few years the artist often visited Zürich and Basel, where he became a friend of Alberto Giacometti, Germaine Richier, and Fritz Wotruba. In 1936, he received the Prize of the Quadriennale of Rome. In 1938, he married Mercedes Pedrazzini.
He accepted a professorship in sculpture at the Accademia di Belle Arti di Brera, Milan, in 1940.

In 1943, he went into exile in Switzerland, exhibiting in Basel, Bern, and Zurich.
In 1946, the artist settled permanently in Milan.

He is buried at Cimitero Comunale of Pistoia, Toscana, Italy.

He participated in the 'Twentieth-Century Italian Art' show at the Museum of Modern Art in New York City in 1944. Curt Valentin began exhibiting Marini’s work at his Buchholz Gallery in New York in 1950, on which occasion the sculptor visited the city and met Jean Arp, Max Beckmann, Alexander Calder, Lyonel Feininger, and Jacques Lipchitz. On his return to Europe, he stopped in London, where the Hanover Gallery had organized a solo show of his work, and there met Henry Moore. In 1951 a Marini exhibition traveled from the Kestner-Gesellschaft Hannover to the Kunstverein in Hamburg and the Haus der Kunst of Munich. He was awarded the Grand Prize for Sculpture at the Venice Biennale in 1952 and the Feltrinelli Prize at the Accademia dei Lincei in Rome in 1954. One of his monumental sculptures was installed in The Hague in 1959.

Retrospectives of Marini’s work took place at the Kunsthaus Zürich in 1962 and at the Palazzo Venezia in Rome in 1966. His paintings were exhibited for the first time at Toninelli Arte Moderna in Milan in 1963–64. In 1973 a permanent installation of his work opened at the Galleria d’Arte Moderna in Milan, and in 1978 a Marini show was presented at the National Museum of Modern Art in Tokyo.

There is a museum dedicated to his work in Florence (in the former church of San Pancrazio).
His work may also be found in museums such as the Civic Gallery of Modern Art in Milan, the Tate Collection, "The Angel of the City" at the Peggy Guggenheim Collection, Venice, the Norton Simon Museum, Museum de Fundatie and the Hirshhorn Museum and Sculpture Garden in Washington, D.C.

Marini developed several themes in sculpture: equestrian, Pomonas (nudes), portraits, and circus figures. He drew on traditions of Etruscan and Northern European sculpture in developing these themes. His aim was to develop mythical images by interpreting classical themes in light of modern concerns and techniques.

Marini is particularly famous for his series of stylised equestrian statues, which feature a man with outstretched arms on a horse. The evolution of the horse and rider as a subject in Marini's works reflects the artist's response to the changing context of the modern world. This theme appeared in his work in 1936. At first the proportions of horse and rider are slender and both are "poised, formal, and calm." By the next year the horse is depicted rearing and the rider gesturing. By 1940 the forms are simpler and more archaic in spirit; the proportions squatter. 

After World War II, in the late 1940s, the horse is planted, immobile, with neck extended, ears pinned back, mouth open. An example, in the Peggy Guggenheim Collection in Venice, is "The Angel of the City," depicting "affirmation and charged strength associated explicitly with sexual potency." In later works, the rider is, increasingly, oblivious of his mount, "involved in his own visions or anxieties." In the artist's final work, the rider is unseated as the horse falls to the ground in an "apocalyptic image of lost control" which parallels Marini’s growing despair for the future of the world.




</doc>
<doc id="20087" url="https://en.wikipedia.org/wiki?curid=20087" title="Modular arithmetic">
Modular arithmetic

In mathematics, modular arithmetic is a system of arithmetic for integers, where numbers "wrap around" upon reaching a certain value—the modulus (plural moduli). The modern approach to modular arithmetic was developed by Carl Friedrich Gauss in his book "Disquisitiones Arithmeticae", published in 1801.

A familiar use of modular arithmetic is in the 12-hour clock, in which the day is divided into two 12-hour periods. If the time is 7:00 now, then 8 hours later it will be 3:00. Usual addition would suggest that the later time should be , but this is not the answer because clock time "wraps around" every 12 hours. Because the hour number starts over after it reaches 12, this is arithmetic "modulo" 12. According to the definition below, 12 is congruent not only to 12 itself, but also to 0, so the time called "12:00" could also be called "0:00", since 12 is congruent to 0 modulo 12.

Modular arithmetic can be handled mathematically by introducing a congruence relation on the integers that is compatible with the operations on integers: addition, subtraction, and multiplication. For a positive integer , two numbers and are said to be ", if their difference is an integer multiple of (that is, if there is an integer such that ). This congruence relation is typically considered when and are integers, and is denoted 
(some authors use instead of ; in this case, if the parentheses are omitted, this generally means that "mod" denotes the modulo operation, that is, that ).

The number is called the " of the congruence. 

The congruence relation may be rewritten as
explicitly showing its relationship with Euclidean division. However, need not be the remainder of the division of by More precisely, what the statement asserts is that and have the same remainder when divided by . That is,
where is the common remainder. Subtracting these two expressions, we recover the previous relation:
by setting 

For example,

because , which is a multiple of 12, or, equivalently, because both 38 and 14 have the same remainder 2 when divided by 12.

The same rule holds for negative values:

A remark on the notation: Because it is common to consider several congruence relations for different moduli at the same time, the modulus is incorporated in the notation. In spite of the ternary notation, the congruence relation for a given modulus is binary. This would have been clearer if the notation had been used, instead of the common traditional notation.

The congruence relation satisfies all the conditions of an equivalence relation:

If and or if then:

If , then it is false, in general, that . However, one has:

For cancellation of common terms, we have the following rules:


The modular multiplicative inverse is defined by the following rules:

In particular, if is a prime number then is coprime with for every such that . Thus, a multiplicative inverse exists for all that are not congruent to zero modulo .

Some of the more advanced properties of congruence relations are the following:

Like any congruence relation, congruence modulo is an equivalence relation, and the equivalence class of the integer , denoted by , is the set . This set, consisting of the integers congruent to  modulo , is called the congruence class or residue class or simply residue of the integer , modulo . When the modulus is known from the context, that residue may also be denoted .

Each residue class modulo may be represented by any one of its members, although we usually represent each residue class by the smallest nonnegative integer which belongs to that class (since this is the proper remainder which results from division). Any two members of different residue classes modulo are incongruent modulo . Furthermore, every integer belongs to one and only one residue class modulo .

The set of integers is called the least residue system modulo . Any set of integers, no two of which are congruent modulo , is called a complete residue system modulo .

It is clear that the least residue system is a complete residue system, and that a complete residue system is simply a set containing precisely one representative of each residue class modulo . The least residue system modulo 4 is {0, 1, 2, 3}. Some other complete residue systems modulo 4 are:


Some sets which are "not" complete residue systems modulo 4 are:


Any set of integers that are relatively prime to and that are mutually incongruent modulo , where denotes Euler's totient function, is called a reduced residue system modulo . The example above, {5,15} is an example of a reduced residue system modulo 4.

The set of all congruence classes of the integers for a modulus is called the ring of integers modulo , and is denoted formula_9, formula_10, or formula_11. The notation formula_11 is, however, not recommended because it can be confused with the set of -adic integers. The ring formula_9 is fundamental to various branches of mathematics (see Applications below).

The set is defined for "n" > 0 as:

We define addition, subtraction, and multiplication on formula_9 by the following rules:


The verification that this is a proper definition uses the properties given before.

In this way, formula_9 becomes a commutative ring. For example, in the ring formula_22, we have
as in the arithmetic for the 24-hour clock.

We use the notation formula_9 because this is the quotient ring of formula_16 by the ideal formula_26 containing all integers divisible by , where formula_27 is the singleton set . Thus formula_9 is a field when formula_26 is a maximal ideal, that is, when is prime.

This can also be constructed from the group formula_30 under the addition operation alone. The residue class is the group coset of in the quotient group formula_9, a cyclic group.

Rather than excluding the special case , it is more useful to include formula_32 (which, as mentioned before, is isomorphic to the ring formula_16 of integers), for example, when discussing the characteristic of a ring.

The ring of integers modulo is a finite field if and only if is prime (this ensures every nonzero element has a multiplicative inverse). If formula_34 is a prime power with "k" > 1, there exists a unique (up to isomorphism) finite field formula_35 with elements, but this is "not" formula_36, which fails to be a field because it has zero-divisors.

We denote the multiplicative subgroup of the modular integers by formula_37. This consists of formula_38 for "a" coprime to "n", which are precisely the classes possessing a multiplicative inverse. This forms a commutative group under multiplication, with order formula_39.

In theoretical mathematics, modular arithmetic is one of the foundations of number theory, touching on almost every aspect of its study, and it is also used extensively in group theory, ring theory, knot theory, and abstract algebra. In applied mathematics, it is used in computer algebra, cryptography, computer science, chemistry and the visual and musical arts.

A very practical application is to calculate checksums within serial number identifiers. For example, International Standard Book Number (ISBN) uses modulo 11 (if issued before 1 January, 2007) or modulo 10 (if issued on or after 1 January, 2007) arithmetic for error detection. Likewise, International Bank Account Numbers (IBANs), for example, make use of modulo 97 arithmetic to spot user input errors in bank account numbers. In chemistry, the last digit of the CAS registry number (a unique identifying number for each chemical compound) is a check digit, which is calculated by taking the last digit of the first two parts of the CAS registry number times 1, the previous digit times 2, the previous digit times 3 etc., adding all these up and computing the sum modulo 10.

In cryptography, modular arithmetic directly underpins public key systems such as RSA and Diffie–Hellman, and provides finite fields which underlie elliptic curves, and is used in a variety of symmetric key algorithms including Advanced Encryption Standard (AES), International Data Encryption Algorithm (IDEA), and RC4. RSA and Diffie–Hellman use modular exponentiation.

In computer algebra, modular arithmetic is commonly used to limit the size of integer coefficients in intermediate calculations and data. It is used in polynomial factorization, a problem for which all known efficient algorithms use modular arithmetic. It is used by the most efficient implementations of polynomial greatest common divisor, exact linear algebra and Gröbner basis algorithms over the integers and the rational numbers.

In computer science, modular arithmetic is often applied in bitwise operations and other operations involving fixed-width, cyclic data structures. The modulo operation, as implemented in many programming languages and calculators, is an application of modular arithmetic that is often used in this context. XOR is the sum of 2 bits, modulo 2.

In music, arithmetic modulo 12 is used in the consideration of the system of twelve-tone equal temperament, where octave and enharmonic equivalency occurs (that is, pitches in a 1∶2 or 2∶1 ratio are equivalent, and C-sharp is considered the same as D-flat).

The method of casting out nines offers a quick check of decimal arithmetic computations performed by hand. It is based on modular arithmetic modulo 9, and specifically on the crucial property that 10 ≡ 1 (mod 9).

Arithmetic modulo 7 is used in algorithms that determine the day of the week for a given date. In particular, Zeller's congruence and the Doomsday algorithm make heavy use of modulo-7 arithmetic.

More generally, modular arithmetic also has application in disciplines such as law (see for example, apportionment), economics, (see for example, game theory) and other areas of the social sciences, where proportional division and allocation of resources plays a central part of the analysis.

Since modular arithmetic has such a wide range of applications, it is important to know how hard it is to solve a system of congruences. A linear system of congruences can be solved in polynomial time with a form of Gaussian elimination, for details see linear congruence theorem. Algorithms, such as Montgomery reduction, also exist to allow simple arithmetic operations, such as multiplication and exponentiation modulo , to be performed efficiently on large numbers.

Some operations, like finding a discrete logarithm or a quadratic congruence appear to be as hard as integer factorization and thus are a starting point for cryptographic algorithms and encryption. These problems might be NP-intermediate.

Solving a system of non-linear modular arithmetic equations is NP-complete.

Below are three reasonably fast C functions, two for performing modular multiplication and one for modular exponentiation on unsigned integers not larger than 63 bits, without overflow of the transient operations.

An algorithmic way to compute formula_40:
uint64_t mul_mod(uint64_t a, uint64_t b, uint64_t m)

On computer architectures where an extended precision format with at least 64 bits of mantissa is available (such as the long double type of most x86 C compilers), the following routine is faster than any algorithmic solution, by employing the trick that, by hardware, floating-point multiplication results in the most significant bits of the product kept, while integer multiplication results in the least significant bits kept:
uint64_t mul_mod(uint64_t a, uint64_t b, uint64_t m)

Below is a C function for performing modular exponentiation, that uses the function implemented above.

An algorithmic way to compute formula_41:
uint64_t pow_mod(uint64_t a, uint64_t b, uint64_t m)

However, for all above routines to work, must not exceed 63 bits.




</doc>
<doc id="20088" url="https://en.wikipedia.org/wiki?curid=20088" title="Myriad">
Myriad

A myriad (from Ancient Greek ) is technically the number ten thousand; in that sense, the term is used almost exclusively in translations from Greek, Latin, or Chinese, or when talking about ancient Greek numbers. More generally, a myriad may be an indefinitely large number of things.

The Aegean numerals of the Minoan and Mycenaean civilizations included a single unit to denote tens of thousands. It was written with a symbol composed of a circle with four dashes 𐄫.

In Classical Greek numerals, a myriad was written as a capital mu: Μ, as lower case letters did not exist in Ancient Greece. To distinguish this numeral from letters, it was sometimes given an overbar: . Multiples were written above this sign, so that for example would equal 4,582×10,000 or 45,820,000. The etymology of the word "myriad" itself is uncertain: it has been variously connected to PIE "*meu-" ("damp") in reference to the waves of the sea and to Greek "myrmex" (, "ant") in reference to their swarms.

The largest number named in Ancient Greek was the myriad myriad (written ) or hundred million. In his "Sand Reckoner", Archimedes of Syracuse used this quantity as the basis for a numeration system of large powers of ten, which he used to count grains of sand.

In English, "myriad" is most commonly used to mean "some large but unspecified number". It may be either an adjective or a noun: both "there are myriad people outside" and "there is a myriad of people outside" are in use. (There are small differences: the former could imply that it is a "diverse" group of people; the latter does not usually but could possibly indicate a group of exactly ten thousand.) The "Merriam-Webster Dictionary" notes that confusion over the use of myriad as a noun "seems to reflect a mistaken belief that the word was originally and is still properly only an adjective ... however, the noun is in fact the older form, dating to the 16th century. The noun "myriad" has appeared in the works of such writers as Milton (plural 'myriads') and Thoreau ('a myriad of'), and it continues to occur frequently in reputable English."

"Myriad" is also infrequently used in English as the specific number 10,000. Owing to the possible confusion with the generic meaning of "large quantity", however, this is generally restricted to translation of other languages like ancient Greek, Chinese, and Hindi where numbers may be grouped into sets of 10,000 (myriads). Such use permits the translator to remain closer to the original text and avoid repeated and unwieldy mentions of "tens of thousands": for example, "the original number of the crews supplied by the several nations I find to have been twenty-four myriads" and "What is the distance between one bridge and another? Twelve myriads of parasangs".

Most European languages include variations of "myriad" with similar meanings to the English word.

Additionally, the prefix "myria-" indicating multiplication times ten thousand (×10), was part of the original metric system adopted by France in 1795. Although it was not retained after the 11th CGPM conference in 1960, "myriameter" is sometimes still encountered as a translation of the Scandinavian mile (Swedish & Norwegian: "mil") of , or in some classifications of wavelengths as the adjective "myriametric". The "myriagramme" (10 kg) was a French approximation of the avoirdupois "quartier" of and the "myriaton" appears in Isaac Asimov's "Foundation" novel trilogy.

In Modern Greek, the word "myriad" is rarely used to denote 10,000, but a million is "ekatommyrio" (, "lit." 'hundred myriad') and a thousand million is "disekatommyrio" (, "lit." 'twice hundred myriad').

In East Asia, the traditional numeral systems of China, Korea, and Japan are all decimal-based but grouped into ten thousands rather than thousands. The character for myriad is in traditional script and in simplified form in both mainland China and Japan. The pronunciation varies within China and abroad: "wàn" (Mandarin), "wan" (Hakka), "bān" (Minnan), "maan" (Cantonese), "man" (Japanese and Korean), and "vạn" (Vietnamese). Vietnam is peculiar within the Sinosphere in largely rejecting Chinese numerals in favor of its own: "vạn" is less common than the native "mười nghìn" ("ten thousand") and its numerals are grouped in threes.

Because of this grouping into fours, higher orders of numbers are provided by the powers of 10,000 rather than 1,000: In China, 10,000 was in ancient texts but is now called and sometimes written as 1,0000,0000; 10,000 is 1,0000,0000,0000 or ; 10,000 is 1,0000,0000,0000,0000 or ; and so on. Conversely, Chinese, Japanese, and Korean generally do not have native words for powers of one thousand: what is called "one million" in English is "100" (100 myriad) in the Sinosphere, and "one billion" in English is "" (ten myllion) or "" (ten myriad myriad) in the Sinosphere. Unusually, Vietnam employs its former translation of , "một triệu", to mean 1,000,000 rather than the Chinese figure. Similarly, the PRC government has adapted the word to mean the scientific prefix mega-, but transliterations are used instead for giga-, tera-, and other larger prefixes. This has caused confusion in areas closely related to the PRC such as Hong Kong and Macau, where is still largely used to mean 10,000.

In the programming world, binary literals are frequently expressed in groups of four resembling myriads, due to 4 being a power of 2 and to map with hexadecimal notation. For example, the number 128 in binary would be expressed as codice_1, with a space as the delimiter.

As of C++ 14, binary literals can be entered in the form codice_2, using an apostrophe as the delimiter instead of a space.



</doc>
<doc id="20089" url="https://en.wikipedia.org/wiki?curid=20089" title="Mohamed Al-Fayed">
Mohamed Al-Fayed

Mohamed Al-Fayed (, ; born 27 January 1929) is an Egyptian business magnate. Fayed's business interests include ownership of Hôtel Ritz Paris and formerly Harrods Department Store, Knightsbridge. Al-Fayed sold his ownership of Fulham F.C. to Shahid Khan in 2013. 

In his personal life, Fayed had a son, Dodi, from his first marriage to Samira Khashoggi. Dodi died in a car crash in Paris with Diana, Princess of Wales on 31 August 1997. Fayed married Finnish socialite and former model Heini Wathén in 1985, with whom he has four children: Jasmine, Karim, Camilla, and Omar. In 2013, Fayed's wealth was estimated at US$1.4 billion, making him the 1,031st-richest person in the world in 2013.

He was born Mohamed Fayed on 27 January 1929, in Bakos, Alexandria, Egypt, the eldest son of an Egyptian primary school teacher. Fayed has five siblings: Ali, Ashraf, Salah, Soaad, and Safia. Ali and Salah have been his business colleagues. 

He was married for two years, from 1954 to 1956, to Samira Khashoggi. Fayed worked for his wife's brother, Saudi Arabian arms dealer and businessman Adnan Khashoggi.

Some time in the early 1970s, he began using "Al-Fayed" rather than "Fayed". His brothers Ali and Salah began to follow suit at the time of their acquisition of the House of Fraser in the 1980s, though by the late 1980s, both had reverted to calling themselves simply "Fayed". Some have assumed that Fayed's addition of "Al-" to his name was to imply aristocratic origins, like "de" in French or "von" in German, though "Al-" does not have the same social connotations in Arabic. This assumption led to "Private Eye" nicknaming him the "Phoney Pharaoh".

Fayed and his brothers founded a shipping company in Egypt before moving its headquarters to Genoa, Italy with additional offices in London. Around 1964 Fayed entered a close relationship with Haitian leader François Duvalier, known as 'Papa Doc' Duvalier, and became interested in the construction of a Fayed-Duvalier oil refinery in Haiti. He also associated with the geologist George de Mohrenschildt. Fayed terminated his stay in Haiti six months later when a sample of "crude oil" provided by Haitian associates proved to be low-grade molasses.

It was then that Fayed moved to England where he lived in central London. In the mid 1960s, Fayed met the ruler of Dubai, Sheikh Mohammed bin Rashid Al Maktoum who entrusted Fayed with helping transform Dubai, where he set up IMS (International Marine Services) in 1968. Fayed introduced British companies like the Costain Group (of which he became a director and 30 percent shareholder), Bernard Sunley & Sons and Taylor Woodrow to the Emirate to carry out the required construction work. He also became a financial adviser to the then Sultan of Brunei Omar Ali Saifuddien III, in 1966.

He briefly joined the board of the mining conglomerate Lonrho in 1975 but left after a disagreement. In 1979, Fayed bought The Ritz hotel in Paris, France for US$30 million.

In 1984, Fayed and his brothers purchased a 30 percent stake in House of Fraser, a group that included the famous London store Harrods, from Roland 'Tiny' Rowland, the head of Lonrho. In 1985, he and his brothers bought the remaining 70 percent of House of Fraser for £615m. Rowland claimed the Fayed brothers had lied about their background and wealth and put pressure on the government to investigate them. A Department of Trade and Industry (DTI) inquiry into the Fayeds was launched. The DTI's subsequent report was critical, but no action was taken against the Fayeds, and while many believed the contents of the report, others felt it was politically motivated.

In 1998, Rowland accused Fayed of stealing papers and jewels from his Harrods safe deposit box. Fayed was arrested, but the charges were dropped. Rowland died in 1998. Fayed settled the dispute with a payment to his widow; he also sued the Metropolitan Police for false arrest in 2002, but lost the case.

In 1994, House of Fraser went public, but Fayed retained private ownership of Harrods. He relaunched the humour publication "Punch" in 1996 but it folded again in 2002. Al Fayed unsuccessfully applied for British citizenship twice – once in 1994 and once in 1999. It was suggested that the feud with Rowland contributed to Fayed's being refused British citizenship the first time.

In 1994, in what became known as the cash-for-questions affair, Fayed revealed the names of MPs he had paid to ask questions in parliament on his behalf, but who had failed to declare their fees. It saw the Conservative MPs Neil Hamilton and Tim Smith leave the government in disgrace, and a Committee on Standards in Public Life established to prevent such corruption occurring again. Fayed also revealed that the cabinet minister Jonathan Aitken had stayed for free at the Ritz Hotel in Paris at the same time as a group of Saudi arms dealers leading to Aitken's subsequent unsuccessful libel case and imprisonment for perjury. During this period, from 1988 to February 1998, Al-Fayed's spokesman was Michael Cole, a former BBC journalist, although Cole's PR work for Al-Fayed did not cease in 1998.

Hamilton lost a subsequent libel action against Al-Fayed in December 1999 and a subsequent appeal against the verdict in December 2000. The former MP has always denied that he was paid by Al-Fayed for asking questions in parliament. Hamilton's libel action related to a Channel 4 "Dispatches" documentary broadcast on 16 January 1997 in which Al-Fayed made claims that the MP had received up to £110,000 in cash and received other gratuities for asking parliamentary questions. Hamilton's basis for his appeal was that the original verdict was invalid because Al-Fayed had paid £10,000 for documents stolen from the dustbins of Hamilton's legal representatives by Benjamin Pell, also known as 'Benjy the Binman'.

In 2003, Fayed moved from Surrey, UK to Switzerland, alleging a breach in an agreement with Inland Revenue. In 2005, he moved back to Britain, saying that he "regards Britain as home". He moored a yacht in Monaco called the "Sokar" prior to selling it in 2014.

After previously denying that Harrods was for sale, Harrods was sold to Qatar Holdings, the sovereign wealth fund of the emirate of Qatar, on 10 May 2010. A fortnight previously, Fayed had stated that "People approach us from Kuwait, Saudi Arabia, Qatar. Fair enough. But I put two fingers up to them. It is not for sale. This is not Marks and Spencer or Sainsbury's. It is a special place that gives people pleasure. There is only one Mecca."

Harrods was sold for £1.5 billion. Fayed later revealed in an interview that he decided to sell Harrods following the difficulty in getting his dividend approved by the trustee of the Harrods pension fund. Fayed said "I'm here every day, I can't take my profit because I have to take a permission of those bloody idiots. I say is this right? Is this logic? Somebody like me? I run a business and I need to take bloody fucking trustee's permission to take my profit". Fayed was appointed honorary chairman of Harrods, a position he was scheduled to hold for at least six months.

In 1972, Fayed purchased the Balnagown estate in Easter Ross, Northern Scotland. From an initial twelve acres, Al-Fayed has since built the estate up to sixty-five thousand acres. Al-Fayed has invested more than £20 million in the estate, restored the 14th century pink Balnagown Castle, and created a tourist accommodation business. The Highlands of Scotland tourist board awarded Al-Fayed the "Freedom of the Highlands" in 2002, in recognition of his "outstanding contribution and commitment to the highlands."

As an Egyptian with links to Scotland, Al-Fayed was intrigued enough to fund a 2008 reprint of the 15th-century chronicle "Scotichronicon" by Walter Bower. The "Scotichronicon" describes how Scota, a sister of the Egyptian Pharaoh Tutankhamen, fled her family and landed in Scotland, bringing with her the Stone of Scone. According to the chronicle, Scotland was later named in her honour. The tale is disputed by modern historians. Al-Fayed later declared that "The Scots are originally Egyptians and that's the truth."

In 2009, Al-Fayed revealed that he was a supporter of Scottish independence from the United Kingdom, announcing to the Scots that "It's time for you to waken up and detach yourselves from the English and their terrible politicians...whatever help is needed for Scotland to regain its independence, I will provide it...when you Scots regain your freedom, I am ready to be your president."

Fayed set up the Al Fayed Charitable Foundation in 1987 aiming to help children with life-limiting conditions and children living in poverty. The charity works mainly with charities and hospices for disabled and neglected children in the UK, Thailand and Mongolia. 

Some of the charities with which it works include Francis House Hospice in Manchester, Great Ormond Street Hospital and ChildLine. In 1998, Al Fayed bought Princess Diana's old boarding school in Kent and helped found the New School at West Heath for traumatised children there.

In 2011, Mohamed Al-Fayed's daughter Camilla, who has worked as an ambassador for the charity for eight years, opened the newly refurbished Zoe’s Place baby hospice in West Derby, Liverpool.

Al-Fayed bought the freehold of West London professional football club Fulham F.C. for £6.25 million in 1997. The club was purchased via Bill Muddyman's Muddyman Group. His long-term aim was that Fulham would become a FA Premier League side within five years. In 2001, Fulham took the First Division (now Football League Championship) under manager Jean Tigana, winning 100 points and scoring over 100 goals in the season. This meant that Al-Fayed had achieved his objective of Fulham becoming a Premier League club a year ahead of schedule. By 2002, Fulham were competing in European football, winning the Intertoto Cup and challenging in the UEFA Cup. Fulham reached the final of the 2009–10 UEFA Europa League and continued to play in the Premier League throughout Al-Fayed's tenure as owner (which ended in 2013).

Fulham temporarily left Craven Cottage while it was being upgraded to meet modern safety standards. There were fears that Fulham would not return to the Cottage after it was revealed that Al-Fayed had sold the first right to build on the ground to a property development firm.

Fulham lost a legal case against former manager Tigana in 2004 after Al-Fayed had wrongly alleged that Tigana had overpaid more than £7m for new players and had negotiated transfers in secret. In 2009 Al-Fayed revealed that he was in favour of a wage cap for footballers, and criticised the management of The Football Association and Premier League as "run by donkeys who don't understand business, who are dazzled by money."

A statue of the American entertainer Michael Jackson was unveiled by Al-Fayed in April 2011 at Craven Cottage In 1999 Jackson had attended a league game against Wigan Athletic at the stadium. Following criticisms of the statue, Al-Fayed said "If some stupid fans don't understand and appreciate such a gift this guy gave to the world they can go to hell. I don't want them to be fans." The statue was removed by the club's new owners in 2013; Al-Fayed blamed the club's subsequent relegation from the Premier League on the 'bad luck' brought by its removal. Al-Fayed then donated the statue to the National Football Museum.

Under Al-Fayed Fulham F.C. was owned by Mafco Holdings, based in the tax haven of Bermuda. Mafco Holdings is owned by Al-Fayed and his family. By 2011, Al-Fayed had loaned Fulham F.C. £187 million in interest free loans. In July 2013, it was announced that Al-Fayed had sold the club to Pakistani American businessman Shahid Khan, who owns the NFL's Jacksonville Jaguars.

Lady Diana Spencer was born in 1961, and married the heir to the British throne, Charles, Prince of Wales, in 1981, becoming the Princess of Wales. Diana was an international celebrity and a frequent visitor to Harrods in the 1980s. Al-Fayed and Dodi first met Diana and Charles when they were introduced at a polo tournament in July 1986, that had been sponsored by Harrods.

Diana and Charles divorced in 1996. Diana was hosted by Al-Fayed in the south of France in the summer of 1997, with her two sons, the Princes William and Harry. For the holiday, Fayed bought a 195 ft yacht, the "Jonikal" (later renamed the "Sokar").
Dodi and Diana later began a private cruise on the "Jonikal" and paparazzi photographs of the couple in an embrace were published. Diana's friend, the journalist Richard Kay, confirmed that Diana was involved in "her first serious romance" since her divorce.

Dodi and Diana went on a second private cruise on the "Jonikal" in the third week of August, and returned from Sardinia to Paris on 30 August. The couple privately dined at the Ritz later that day, after the behaviour of the press caused them to cancel a restaurant reservation, they then planned to spend the night at Dodi's apartment near the Arc de Triomphe. In an attempt to deceive the paparazzi, a decoy car left the front of the hotel, while Diana and Dodi departed at speed in a Mercedes-Benz W140 driven by chauffeur Henri Paul from the rear of the hotel. Five minutes later, the car crashed in the Pont de l'Alma tunnel, killing Paul and Dodi. Diana died later in hospital. Fayed arrived in Paris the next day and viewed Dodi's body, which was returned to Britain for an Islamic funeral.

From February 1998, Al-Fayed maintained that the crash was a result of a conspiracy, and later contended that the crash was orchestrated by MI6 on the instructions of Prince Philip, Duke of Edinburgh. His claims that the crash was a result of a conspiracy were dismissed by a French judicial investigation, but Fayed appealed against this verdict. A libel action was brought against Al-Fayed by Neil Hamilton (see above). During the questioning of one day of the hearing, which referred to Al-Fayed's conspiracy theories, writer Martyn Gregory and a journalist from ITN counted 70 instances of Al-Fayed saying "I don't know" or "I can't remember" to questions relating to the tragedy.

The British Operation Paget, a Metropolitan police inquiry that concluded in 2006, also found no evidence of a conspiracy. To Operation Paget, Al-Fayed made 175 "conspiracy claims".

An inquest headed by Lord Justice Scott Baker into the deaths of Diana and Dodi began at the Royal Courts of Justice, London, on 2 October 2007 and lasted for six months. It was a continuation of the original inquest that had begun in 2004.

At the Scott Baker inquest Fayed accused the Duke of Edinburgh, the Prince of Wales, Lady Sarah McCorquodale, her sister, and numerous others, of plotting to kill the Princess of Wales. Their motive, he claimed, was that they could not tolerate the idea of the Princess marrying a Muslim.

Al-Fayed first claimed that the Princess was pregnant to the "Daily Express" in May 2001, and that he was the only person who had been told of this news. Witnesses at the inquest who said the Princess was not pregnant, and could not have been, were part of the conspiracy according to Al-Fayed. Fayed's testimony at the inquest was roundly condemned in the press as being farcical. Members of the British Government's Intelligence and Security Committee accused Fayed of turning the inquest into a 'circus' and called for it to be ended maturely. Lawyers representing Al-Fayed later accepted at the inquest that there was no direct evidence that either the Duke of Edinburgh or MI6 had been involved in any murder conspiracy involving Diana or Dodi. A few days before Al-Fayed's appearance, John Macnamara, a former senior detective at Scotland Yard and Al-Fayed's investigator for five years from 1997, was forced to admit on 14 February 2008 that he had no evidence to suggest foul play, except for the assertions Al-Fayed had made to him. His admissions also related to the lack of evidence for Al-Fayed's claims about the alleged pregnancy of the Princess and the couple's supposed engagement.

The jury verdict, given on 7 April 2008, was that Diana and Dodi had been "unlawfully killed" through the grossly negligent driving of chauffeur Henri Paul, who was intoxicated, and the pursuing vehicles.

Lawyers for Al-Fayed also accepted that there was no evidence to support the assertion that Diana was illegally embalmed in order to cover up a pregnancy, a "pregnancy" that they accepted, could not be established by any medical evidence. They also accepted that there was no evidence to support the assertion the French emergency and medical services had played any role in a conspiracy to harm Diana. Following the Baker inquest, Al-Fayed said that he was abandoning his campaign to prove that Diana and Dodi were murdered in a conspiracy, and said that he would accept the verdict of the jury.

Al-Fayed financially supported "Unlawful Killing" (2011), a documentary film accused of presenting his version of events. The film was not formally released as a result of legal problems.

Al-Fayed's business interests include:


Al-Fayed's major business purchases have included:


Al-Fayed has been accused by multiple women of sexual harassment and assault.

Young women applying for employment at Harrods were often submitted to HIV tests and gynacological examinations. These women were then selected to spend the weekend with Al-Fayed in Paris. In her profile of Al-Fayed for "Vanity Fair", Maureen Orth described how according to former employees "Fayed regularly walked the store on the lookout for young, attractive women to work in his office. Those who rebuffed him would often be subjected to crude, humiliating comments about their appearance or dress...A dozen ex-employees I spoke with said that
Fayed would chase secretaries around the office and sometimes try to stuff money down women's blouses".

In 1994, Hermina Da Silva quit her job as a nanny at Al-Fayed's home in Oxted. Da Silva had prepared accusations that she was sexually harassed by Al-Fayed, and she was subsequently arrested by detectives and held overnight in cells following a complaint of theft by an employee of Al-Fayeds. She was later released without charge after officers concluded she had not stolen anything. Al-Fayed eventually settled with her out of court, and she was awarded £12,000.

Al-Fayed was interviewed under caution by the Metropolitan Police after an allegation of sexual assault against a 15-year-old schoolgirl in October 2008. The case was dropped by the Crown Prosecution Service, after they found that there was no realistic chance of conviction due to conflicting statements.

In December 1997, the ITV current affairs programme, "The Big Story" broadcast testimonies from a number of former Harrods employees who spoke of how women were routinely sexually harassed by Al-Fayed.

A December 2017 episode of Channel 4's "Dispatches" programme alleged that Al-Fayed had sexually harassed three Harrods employees, and attempted to "groom" them. One of the women was aged 17 at the time. Cheska Hill-Wood, now in her 40s, waived her right to anonymity to be interviewed for the programme. The programme alleged al-Fayed targeted young employees over a 13 year period.



</doc>
<doc id="20090" url="https://en.wikipedia.org/wiki?curid=20090" title="Marmite">
Marmite

Marmite ( ) is a British food spread currently produced by Unilever. Marmite is made from yeast extract, a by-product of beer brewing. Other similar products include the Australian Vegemite, the Swiss Cenovis, and the German Vitam-R.

Marmite is a sticky, dark brown food paste with a distinctive, powerful flavour, which is extremely salty. This distinctive taste is reflected in the marketing slogan: "Love it or hate it." Such is its prominence in British popular culture that the product's name has entered British English as a metaphor for something that is an acquired taste or tends to polarise opinions.

The image on the front of the jar shows a "marmite" (), a French term for a large, covered earthenware or metal cooking pot. Marmite was originally supplied in earthenware pots, but since the 1920s has been sold in glass jars shaped like the French cooking pot.

A similar spread called Marmite has been manufactured in New Zealand since 1919. This is the only product sold as Marmite in Australasia and the Pacific, whereas elsewhere in the world the British version predominates.

The product that was to become Marmite was invented in the late 19th century when German scientist Justus von Liebig discovered that brewer's yeast could be concentrated, bottled and eaten. In 1902 the Marmite Food Extract Company was formed in Burton upon Trent, Staffordshire, England with Marmite as its main product and Burton as the site of the first factory. The by-product yeast needed for the paste was supplied by Bass Brewery. By 1907, the product had become successful enough to warrant construction of a second factory at Camberwell Green in London.

By 1912, the discovery of vitamins was a boost for Marmite, as the spread is a rich source of the vitamin B complex;
with the vitamin B deficiency beri-beri being common during World War I, the spread became more popular. British troops during World War I were issued Marmite as part of their rations.

In the 1930s, Marmite was used by the English scientist Lucy Wills to successfully treat a form of anaemia in mill workers in Bombay. She later identified folic acid as the active ingredient. Marmite was used to treat malnutrition in Suriya-Mal workers during the 1934–5 malaria epidemic in Sri Lanka. Housewives were encouraged to spread Marmite thinly and to "use it sparingly just now" because of limited rations of the product.

In 1990, Marmite Limited, which had become a subsidiary of Bovril Limited, was bought by CPC International Inc, which changed its name to Best Foods Inc in 1998. Best Foods Inc subsequently merged with Unilever in 2000, and Marmite is now a trademark owned by Unilever..

There are a number of similar yeast products available in other countries; these products are not directly connected to the original Marmite recipe and brand. The Australian product Vegemite is distributed in many countries, and AussieMite is sold in Australia. Other products include OzeMite, which is made by Dick Smith Foods; Cenovis, a Swiss spread; and Vegex, an autolyzed yeast product available in the United States since 1913.

Marmite is traditionally eaten as a savoury spread on bread, toast, savoury biscuits or crackers, and other similar baked products. Owing to its concentrated taste it is often spread thinly with butter or margarine. Marmite can also be made into a savoury hot drink by adding one teaspoon to a mug of hot water much like Oxo and Bovril.

Marmite is paired with cheese, such as in a cheese sandwich, and has been used as an additional flavouring in Mini Cheddars, a cheese-flavoured biscuit snack. Similarly, it is one of Walkers Crisps flavours; is sold as a flavouring on rice cakes; and Marmite Biscuits. Starbucks in the UK has a cheese and Marmite panini on its menu.

Marmite has been used as an ingredient in cocktails, including the Marmite Cocktail and the Marmite Gold Rush.

While the process is secret, the general method for making yeast extract on a commercial scale is to add salt to a suspension of yeast, making the solution hypertonic, which leads to the cells shrivelling up; this triggers "autolysis", in which the yeast self-destructs. The dying yeast cells are then heated to complete their breakdown, and since yeast cells have thick cell walls which would detract from the smoothness of the end product, the husks are sieved out. As with other yeast extracts, Marmite contains free glutamic acids, which are analogous to monosodium glutamate.

Today, the main ingredients of Marmite are glutamic acid-rich yeast extract, with lesser quantities of sodium chloride, vegetable extract, spice extracts and celery extracts, although the precise composition is a trade secret. Vitamin B is not naturally found in yeast extract, but is added to Marmite during manufacture.

Marmite is rich in B vitamins including thiamin (B), riboflavin (B), niacin (B), folic acid (B) and vitamin B. The sodium content of the spread is high and has caused concern, although it is the amount per serving rather than the percentage in bulk Marmite that is relevant. The main ingredient of Marmite is yeast extract, which contains a high concentration of glutamic acid. Marmite is almost gluten free. However, Unilever will not confirm that it contains less than 20 ppm of gluten, the current European standard and the proposed US FDA standard for gluten-free labelling.

Marmite should be avoided if a person takes a MAOI antidepressant, such as phenelzine (Nardil) or tranylcypromine (Parnate), as yeast extracts interact adversely with these types of medications due to their tyramine content.

Marmite today is fortified with added vitamins, resulting in it being banned temporarily in Denmark, which disallows foodstuffs that have been fortified until they have undergone necessary testing. The Danish Veterinary and Food Administration stated in 2015 that Marmite had not been banned in the country, but that fortified foods need to be tested for safety and approved before they can be marketed in the country. In 2014, suppliers applied for a risk assessment to be undertaken after which Marmite became available in Denmark.

Marmite's publicity campaigns initially emphasised the spread's healthy nature, extolling it as "The growing up spread you never grow out of." The first major Marmite advertising campaign began in the 1930s, with characters whose faces incorporated the word "good". Soon afterwards, the rising awareness of vitamins was used in Marmite advertising, with slogans proclaiming that "A small quantity added to the daily diet will ensure you and your family are taking sufficient vitamin B to keep nerves, brain, and digestion in proper working order."

During the 1980s, the spread was advertised with the slogan "My mate, Marmite", chanted in television commercials by an army platoon. The spread had been a standard vitamin supplement for British-based German POWs during the Second World War.

By the 1990s Marmite's distinctive and powerful flavour had earned it as many detractors as it had fans, and it was known for producing a polarised "love/hate" reaction amongst consumers. For many years TV adverts for Marmite featured the song Low Rider by War with the lyrics changed to the phrase "My Mate, Marmite". Marmite launched a "Love it or Hate it" campaign in October 1996, and this resulted in the coining of the phrase "Marmite effect" or "Marmite reaction" for anything which provoked strong and polarised feelings. On 22 April 2010, Unilever threatened legal action against the British National Party for using a jar of Marmite and the "love it or hate it" slogan in their television adverts.

Because of the local product, British Marmite is sold under the name "Our Mate" in Australia and New Zealand. New Zealand Marmite uses the name "NZ-Mite" elsewhere.

In Denmark, food safety legislation dictates that foodstuffs that contain added vitamins can only be sold by retailers which have been licensed by the Veterinary and Food Administration. In May 2011, the company that imports the product to Denmark revealed that it wasn't licensed and had therefore stopped selling the product: this led to widespread but inaccurate reports in the British media that Marmite had been banned by the Danish authorities.

On 24 January 2014, the Canadian Food Inspection Agency was noted, in a CBC story, as moving to stop the sale of Marmite, as well as Vegemite and Ovaltine, in Canada because they were enriched with vitamins and minerals which were not listed in Canadian food regulations. The agency said the products were not a health hazard.
The CFIA later clarified that these specific items had been seized because they were not the versions that are formulated for sale in Canada and which met all Canadian food regulations. Canadian versions of Marmite and the other products would still be permitted to be sold in Canada.

Marmite is manufactured under licence in South Africa by Pioneer Foods in traditional and cheese flavour.

In 2002 a 100th anniversary jar was released.

In February 2007 Marmite produced a limited edition Guinness Marmite of 300,000 250g jars of their yeast extract with 30% Guinness yeast, giving it a noticeable hint of "Guinness" flavour. In January 2008 Champagne Marmite was released for Valentine's Day, with a limited-edition run of 600,000 units initially released exclusively to Selfridges of London. The product had 0.3% champagne added to the recipe, and a modified heart-shaped label with "I love you" in place of the logo.

In 2009, a limited edition Marston's Pedigree Marmite was launched to celebrate the 2009 Ashes Cricket test series.

In March 2010, a super-strong blend called Marmite Extra Old, or XO for short, was launched in the UK. Normal Marmite contains a mix of lager, bitter and ale varieties of yeast, sourced from breweries throughout the UK. However, as lagers have a lighter, sweeter taste, residue from this product is not used in Marmite XO. Only residue from traditional bitters and ales are blended to ensure the stronger taste. Marmite XO is matured for 28 days - four times longer than usual. Continuing the tradition of different coloured caps for special editions, Marmite XO's cap is black.

In April 2012 a special edition jar in commemoration of the Diamond Jubilee of Queen Elizabeth II was released. With the product renamed "Ma'amite," the redesigned label featured a colour scheme based upon the Union Jack; the marmite and spoon logo replaced by a gold crown, and with a red rather than yellow cap. The front label also declares "Made with 100% British Yeast". Coinciding with the 110th anniversary of the brand, production was limited to 300,000 jars. For Christmas 2012 a gold limited edition was launched, containing edible gold-coloured flecks.

Marmite chocolate is also available.

In 2015, Marmite Summer of Love Special Edition featured a flower power themed label. This special edition's blend had a lighter taste made using 100% Lager yeast.


</doc>
<doc id="20092" url="https://en.wikipedia.org/wiki?curid=20092" title="Mabon ap Modron">
Mabon ap Modron

Mabon ap Modron is a prominent figure from Welsh literature and mythology, the son of Modron and a member of Arthur's war band. Both he and his mother were likely deities in origin, descending from a divine mother–son pair. His name is related to the Romano-British god Maponos, whose name means "Great Son"; Modron, in turn, is likely related to the Gaulish goddess Dea Matrona. He is often equated with the Demetian hero Pryderi fab Pwyll, and may be associated with the minor Arthurian character Mabon fab Mellt.

The name "Mabon" is derived from the Common Brittonic and Gaulish deity name "Maponos" "Great Son", from the Proto-Celtic root "*makwo-" "son". Similarly, Modron is derived from the name of the Brittonic and Gaulish deity "Mātronā", meaning "Great Mother", from Proto-Celtic "*mātīr" "mother".

Culhwch's father, King Cilydd, the son of Celyddon, loses his wife Goleuddydd after a difficult childbirth. When he remarries, the young Culhwch rejects his stepmother's attempt to pair him with his new stepsister. Offended, the new queen puts a curse on him so that he can marry no one besides the beautiful Olwen, daughter of the giant Ysbaddaden. Though he has never seen her, Culhwch becomes infatuated with her, but his father warns him that he will never find her without the aid of his famous cousin Arthur. The young man immediately sets off to seek his kinsman. He finds him at his court in Celliwig in Cornwall and asks for support and assistance. Cai is the first knight to volunteer to assist Culhwch in his quest, promising to stand by his side until Olwen is found. A further five knights join them in their mission.

They travel onwards until they come across the "fairest of the castles of the world", and meet Ysbaddaden's shepherd brother, Custennin. They learn that the castle belongs to Ysbaddaden, that he stripped Custennin of his lands and murdered the shepherd's twenty-three children out of cruelty. Custennin sets up a meeting between Culhwch and Olwen, and the maiden agrees to lead Culhwch and his companions to Ysbadadden's castle. Cai pledges to protect the twenty-fourth son, Goreu, with his life.

The knights attack the castle by stealth, killing the nine porters and the nine watchdogs, and enter the giant's hall. Upon their arrival, Ysbaddaden attempts to kill Culhwch with a poison dart, but is outwitted and wounded, first by Bedwyr, then by the enchanter Menw, and finally by Culhwch himself. Eventually, Ysbaddaden relents, and agrees to give Culhwch his daughter on the condition that he completes a number of impossible tasks ("anoethau"), including hunting the Twrch Trwyth and recovering the exalted prisoner, Mabon son of Modron, the only man able to hunt the dog Drudwyn, in turn the only dog who can track the Twrch Trwyth.

Arthur and his men learn that Mabon was stolen from his mother's arms when he was three nights old, and question the world's oldest and wisest animals about his whereabouts, until they are led to the salmon of Llyn Llyw, the oldest animal of them all. The enormous salmon carries Arthur's men Cei and Bedwyr downstream to Mabon's prison in Gloucester; they hear him through the walls, singing a lamentation for his fate. The rest of Arthur's men launch an assault on the front of the prison, while Cei and Bedwyr sneak in the back and rescue Mabon. He subsequently plays a key role in the hunt for the Twrch Trwyth.

One of the earliest direct reference to Mabon can be found in the tenth century poem Pa Gur, in which Arthur recounts the feats and achievements of his knights so as to gain entrance to a fortress guarded by Glewlwyd Gafaelfawr, the eponymous porter. The poem relates that Mabon fab Mydron (a misspelling of Modron) is one of Arthur's followers, and is described as a "servant to Uther Pendragon". A second figure, Mabon fab Mellt, is described as having "stained the grass with blood". He further appears in the medieval tale "The Dream of Rhonabwy", in which he fights alongside Arthur at the Battle of Badon and is described as one of the king's chief advisors.

Mabon is almost certainly related to the continental Arthurian figures Mabonagrain, Mabuz, Nabon le Noir and Maboun.


</doc>
<doc id="20097" url="https://en.wikipedia.org/wiki?curid=20097" title="Microwave">
Microwave

Microwaves are a form of electromagnetic radiation with wavelengths ranging from one meter to one millimeter; with frequencies between and . Different sources define different frequency ranges as microwaves; the above broad definition includes both UHF and EHF (millimeter wave) bands. A more common definition in radio engineering is the range between 1 and 100 GHz (wavelengths between 300 and 3 mm). In all cases, microwaves include the entire SHF band (3 to 30 GHz, or 10 to 1 cm) at minimum. Frequencies in the microwave range are often referred to by their IEEE radar band designations: S, C, X, K, K, or K band, or by similar NATO or EU designations.

The prefix "" in "microwave" is not meant to suggest a wavelength in the micrometer range. It indicates that microwaves are "small", compared to the radio waves used prior to microwave technology, in that they have shorter wavelengths. The boundaries between far infrared, terahertz radiation, microwaves, and ultra-high-frequency radio waves are fairly arbitrary and are used variously between different fields of study.

Microwaves travel by line-of-sight; unlike lower frequency radio waves they do not diffract around hills, follow the earth's surface as ground waves, or reflect from the ionosphere, so terrestrial microwave communication links are limited by the visual horizon to about 40 miles (64 km). At the high end of the band they are absorbed by gases in the atmosphere, limiting practical communication distances to around a kilometer. Microwaves are extremely widely used in modern technology. They are used for point-to-point communication links, wireless networks, microwave radio relay networks, radar, satellite and spacecraft communication, medical diathermy and cancer treatment, remote sensing, radio astronomy, particle accelerators, spectroscopy, industrial heating, collision avoidance systems, garage door openers and keyless entry systems, and for cooking food in microwave ovens.

Microwaves occupy a place in the electromagnetic spectrum with frequency above ordinary radio waves, and below infrared light:

In descriptions of the electromagnetic spectrum, some sources classify microwaves as radio waves, a subset of the radio wave band; while others classify microwaves and radio waves as distinct types of radiation. This is an arbitrary distinction.

Microwaves travel solely by line-of-sight paths; unlike lower frequency radio waves, they do not travel as ground waves which follow the contour of the Earth, or reflect off the ionosphere (skywaves). Although at the low end of the band they can pass through building walls enough for useful reception, usually rights of way cleared to the first Fresnel zone are required. Therefore, on the surface of the Earth, microwave communication links are limited by the visual horizon to about . Microwaves are absorbed by moisture in the atmosphere, and the attenuation increases with frequency, becoming a significant factor (rain fade) at the high end of the band. Beginning at about 40 GHz, atmospheric gases also begin to absorb microwaves, so above this frequency microwave transmission is limited to a few kilometers. A spectral band structure causes absorption peaks at specific frequencies (see graph at right). Above 100 GHz, the absorption of electromagnetic radiation by Earth's atmosphere is so great that it is in effect opaque, until the atmosphere becomes transparent again in the so-called infrared and optical window frequency ranges.

In a microwave beam directed at an angle into the sky, a small amount of the power will be randomly scattered as the beam passes through the troposphere. A sensitive receiver beyond the horizon with a high gain antenna focused on that area of the troposphere can pick up the signal. This technique has been used at frequencies between 0.45 and 5 GHz in tropospheric scatter (troposcatter) communication systems to communicate beyond the horizon, at distances up to 300 km.

The short wavelengths of microwaves allow omnidirectional antennas for portable devices to be made very small, from 1 to 20 centimeters long, so microwave frequencies are widely used for wireless devices such as cell phones, cordless phones, and wireless LANs (Wifi) access for laptops, and Bluetooth earphones. Antennas used include short whip antennas, rubber ducky antennas, sleeve dipoles, patch antennas, and increasingly the printed circuit inverted F antenna (PIFA) used in cell phones.

Their short wavelength also allows narrow beams of microwaves to be produced by conveniently small high gain antennas from a half meter to 5 meters in diameter. Therefore, beams of microwaves are used for point-to-point communication links, and for radar. An advantage of narrow beams is that they don't interfere with nearby equipment using the same frequency, allowing frequency reuse by nearby transmitters. Parabolic ("dish") antennas are the most widely used directive antennas at microwave frequencies, but horn antennas, slot antennas and dielectric lens antennas are also used. Flat microstrip antennas are being increasingly used in consumer devices. Another directive antenna practical at microwave frequencies is the phased array, a computer-controlled array of antennas which produces a beam which can be electronically steered in different directions.

At microwave frequencies, the transmission lines which are used to carry lower frequency radio waves to and from antennas, such as coaxial cable and parallel wire lines, have excessive power losses, so when low attenuation is required microwaves are carried by metal pipes called waveguides. Due to the high cost and maintenance requirements of waveguide runs, in many microwave antennas the output stage of the transmitter or the RF front end of the receiver is located at the antenna.

The term "microwave" also has a more technical meaning in electromagnetics and circuit theory. Apparatus and techniques may be described qualitatively as "microwave" when the frequencies used are high enough that wavelengths of signals are roughly the same as the dimensions of the circuit, so that lumped-element circuit theory is inaccurate, and instead distributed circuit elements and transmission-line theory are more useful methods for design and analysis. As a consequence, practical microwave circuits tend to move away from the discrete resistors, capacitors, and inductors used with lower-frequency radio waves. Open-wire and coaxial transmission lines used at lower frequencies are replaced by waveguides and stripline, and lumped-element tuned circuits are replaced by cavity resonators or resonant stubs. In turn, at even higher frequencies, where the wavelength of the electromagnetic waves becomes small in comparison to the size of the structures used to process them, microwave techniques become inadequate, and the methods of optics are used.

High-power microwave sources use specialized vacuum tubes to generate microwaves. These devices operate on different principles from low-frequency vacuum tubes, using the ballistic motion of electrons in a vacuum under the influence of controlling electric or magnetic fields, and include the magnetron (used in microwave ovens), klystron, traveling-wave tube (TWT), and gyrotron. These devices work in the density modulated mode, rather than the current modulated mode. This means that they work on the basis of clumps of electrons flying ballistically through them, rather than using a continuous stream of electrons.

Low-power microwave sources use solid-state devices such as the field-effect transistor (at least at lower frequencies), tunnel diodes, Gunn diodes, and IMPATT diodes. Low-power sources are available as benchtop instruments, rackmount instruments, embeddable modules and in card-level formats. A maser is a solid state device which amplifies microwaves using similar principles to the laser, which amplifies higher frequency light waves.

All warm objects emit low level microwave black-body radiation, depending on their temperature, so in meteorology and remote sensing microwave radiometers are used to measure the temperature of objects or terrain. The sun and other astronomical radio sources such as Cassiopeia A emit low level microwave radiation which carries information about their makeup, which is studied by radio astronomers using receivers called radio telescopes. The cosmic microwave background radiation (CMBR), for example, is a weak microwave noise filling empty space which is a major source of information on cosmology's Big Bang theory of the origin of the Universe.

Microwave technology is extensively used for point-to-point telecommunications (i.e. non-broadcast uses). Microwaves are especially suitable for this use since they are more easily focused into narrower beams than radio waves, allowing frequency reuse; their comparatively higher frequencies allow broad bandwidth and high data transmission rates, and antenna sizes are smaller than at lower frequencies because antenna size is inversely proportional to transmitted frequency. Microwaves are used in spacecraft communication, and much of the world's data, TV, and telephone communications are transmitted long distances by microwaves between ground stations and communications satellites. Microwaves are also employed in microwave ovens and in radar technology.

Before the advent of fiber-optic transmission, most long-distance telephone calls were carried via networks of microwave radio relay links run by carriers such as AT&T Long Lines. Starting in the early 1950s, frequency division multiplex was used to send up to 5,400 telephone channels on each microwave radio channel, with as many as ten radio channels combined into one antenna for the "hop" to the next site, up to 70 km away.

Wireless LAN protocols, such as Bluetooth and the IEEE 802.11 specifications used for Wi-Fi, also use microwaves in the 2.4 GHz ISM band, although 802.11a uses ISM band and U-NII frequencies in the 5 GHz range. Licensed long-range (up to about 25 km) Wireless Internet Access services have been used for almost a decade in many countries in the 3.5–4.0 GHz range. The FCC recently carved out spectrum for carriers that wish to offer services in this range in the U.S. — with emphasis on 3.65 GHz. Dozens of service providers across the country are securing or have already received licenses from the FCC to operate in this band. The WIMAX service offerings that can be carried on the 3.65 GHz band will give business customers another option for connectivity.

Metropolitan area network (MAN) protocols, such as WiMAX (Worldwide Interoperability for Microwave Access) are based on standards such as IEEE 802.16, designed to operate between 2 and 11 GHz. Commercial implementations are in the 2.3 GHz, 2.5 GHz, 3.5 GHz and 5.8 GHz ranges.

Mobile Broadband Wireless Access (MBWA) protocols based on standards specifications such as IEEE 802.20 or ATIS/ANSI HC-SDMA (such as iBurst) operate between 1.6 and 2.3 GHz to give mobility and in-building penetration characteristics similar to mobile phones but with vastly greater spectral efficiency.

Some mobile phone networks, like GSM, use the low-microwave/high-UHF frequencies around 1.8 and 1.9 GHz in the Americas and elsewhere, respectively. DVB-SH and S-DMB use 1.452 to 1.492 GHz, while proprietary/incompatible satellite radio in the U.S. uses around 2.3 GHz for DARS.

Microwave radio is used in broadcasting and telecommunication transmissions because, due to their short wavelength, highly directional antennas are smaller and therefore more practical than they would be at longer wavelengths (lower frequencies). There is also more bandwidth in the microwave spectrum than in the rest of the radio spectrum; the usable bandwidth below 300 MHz is less than 300 MHz while many GHz can be used above 300 MHz. Typically, microwaves are used in television news to transmit a signal from a remote location to a television station from a specially equipped van. See broadcast auxiliary service (BAS), remote pickup unit (RPU), and studio/transmitter link (STL).

Most satellite communications systems operate in the C, X, K, or K bands of the microwave spectrum. These frequencies allow large bandwidth while avoiding the crowded UHF frequencies and staying below the atmospheric absorption of EHF frequencies. Satellite TV either operates in the C band for the traditional large dish fixed satellite service or K band for direct-broadcast satellite. Military communications run primarily over X or K-band links, with K band being used for Milstar.

Global Navigation Satellite Systems (GNSS) including the Chinese Beidou, the American Global Positioning System (introduced in 1978) and the Russian GLONASS broadcast navigational signals in various bands between about 1.2 GHz and 1.6 GHz.

Radar is a radiolocation technique in which a beam of radio waves emitted by a transmitter bounces off an object and returns to a receiver, allowing the location, range, speed, and other characteristics of the object to be determined. The short wavelength of microwaves causes large reflections from objects the size of motor vehicles, ships and aircraft. Also, at these wavelengths, the high gain antennas such as parabolic antennas which are required to produce the narrow beamwidths needed to accurately locate objects are conveniently small, allowing them to be rapidly turned to scan for objects. Therefore, microwave frequencies are the main frequencies used in radar. Microwave radar is widely used for applications such as air traffic control, weather forecasting, navigation of ships, and speed limit enforcement. Long distance radars use the lower microwave frequencies since at the upper end of the band atmospheric absorption limits the range, but millimeter waves are used for short range radar such as collision avoidance systems.

Microwaves emitted by astronomical radio sources; planets, stars, galaxies, and nebulas are studied in radio astronomy with large dish antennas called radio telescopes. In addition to receiving naturally occurring microwave radiation, radio telescopes have been used in active radar experiments to bounce microwaves off planets in the solar system, to determine the distance to the Moon or map the invisible surface of Venus through cloud cover.

A recently completed microwave radio telescope is the Atacama Large Millimeter Array, located at more than 5,000 meters (16,597 ft) altitude in Chile, observes the universe in the millimetre and submillimetre wavelength ranges. The world's largest ground-based astronomy project to date, it consists of more than 66 dishes and was built in an international collaboration by Europe, North America, East Asia and Chile.

A major recent focus of microwave radio astronomy has been mapping the cosmic microwave background radiation (CMBR) discovered in 1964 by radio astronomers Arno Penzias and Robert Wilson. This faint background radiation, which fills the universe and is almost the same in all directions, is "relic radiation" from the Big Bang, and is one of the few sources of information about conditions in the early universe. Due to the expansion and thus cooling of the Universe, the originally high-energy radiation has been shifted into the microwave region of the radio spectrum. Sufficiently sensitive radio telescopes can detected the CMBR as a faint signal that is not associated with any star, galaxy, or other object.

A microwave oven passes microwave radiation at a frequency near through food, causing dielectric heating primarily by absorption of the energy in water. Microwave ovens became common kitchen appliances in Western countries in the late 1970s, following the development of less expensive cavity magnetrons. Water in the liquid state possesses many molecular interactions that broaden the absorption peak. In the vapor phase, isolated water molecules absorb at around 22 GHz, almost ten times the frequency of the microwave oven.

Microwave heating is used in industrial processes for drying and curing products.

Many semiconductor processing techniques use microwaves to generate plasma for such purposes as reactive ion etching and plasma-enhanced chemical vapor deposition (PECVD).

Microwave frequencies typically ranging from 110 – 140 GHz are used in stellarators and tokamak experimental fusion reactors to help heat the fuel into a plasma state. The upcoming ITER thermonuclear reactor is expected to range from 110–170 GHz and will employ electron cyclotron resonance heating (ECRH).

Microwaves can be used to transmit power over long distances, and post-World War II research was done to examine possibilities. NASA worked in the 1970s and early 1980s to research the possibilities of using solar power satellite (SPS) systems with large solar arrays that would beam power down to the Earth's surface via microwaves.

Less-than-lethal weaponry exists that uses millimeter waves to heat a thin layer of human skin to an intolerable temperature so as to make the targeted person move away. A two-second burst of the 95 GHz focused beam heats the skin to a temperature of at a depth of . The United States Air Force and Marines are currently using this type of active denial system in fixed installations.

Microwave radiation is used in electron paramagnetic resonance (EPR or ESR) spectroscopy, typically in the X-band region (~9 GHz) in conjunction typically with magnetic fields of 0.3 T. This technique provides information on unpaired electrons in chemical systems, such as free radicals or transition metal ions such as Cu(II). Microwave radiation is also used to perform rotational spectroscopy and can be combined with electrochemistry as in microwave enhanced electrochemistry.

Bands of frequencies in the microwave spectrum are designated by letters. Unfortunately, there are several incompatible band designation systems, and even within a system the frequency ranges corresponding to some of the letters vary somewhat between different application fields. The letter system had its origin in World War 2 in a top secret U.S. classification of bands used in radar sets; this is the origin of the oldest letter system, the IEEE radar bands. One set of microwave frequency bands designations by the Radio Society of Great Britain (RSGB), is tabulated below:

P band is sometimes used for K Band. "P" for "previous" was a radar band used in the UK ranging from 250 to 500 MHz and now obsolete per IEEE Std 521.

When radars were first developed at K band during World War II, it was not known that there was a nearby absorption band (due to water vapor and oxygen in the atmosphere). To avoid this problem, the original K band was split into a lower band, K, and upper band, K.

Microwave frequency can be measured by either electronic or mechanical techniques.

Frequency counters or high frequency heterodyne systems can be used. Here the unknown frequency is compared with harmonics of a known lower frequency by use of a low frequency generator, a harmonic generator and a mixer. Accuracy of the measurement is limited by the accuracy and stability of the reference source.

Mechanical methods require a tunable resonator such as an absorption wavemeter, which has a known relation between a physical dimension and frequency.

In a laboratory setting, Lecher lines can be used to directly measure the wavelength on a transmission line made of parallel wires, the frequency can then be calculated. A similar technique is to use a slotted waveguide or slotted coaxial line to directly measure the wavelength. These devices consist of a probe introduced into the line through a longitudinal slot, so that the probe is free to travel up and down the line. Slotted lines are primarily intended for measurement of the voltage standing wave ratio on the line. However, provided a standing wave is present, they may also be used to measure the distance between the nodes, which is equal to half the wavelength. Precision of this method is limited by the determination of the nodal locations.

Microwaves do not contain sufficient energy to chemically change substances by ionization, and so are an example of non-ionizing radiation. The word "radiation" refers to energy radiating from a source and not to radioactivity. It has not been shown conclusively that microwaves (or other non-ionizing electromagnetic radiation) have significant adverse biological effects at low levels. Some, but not all, studies suggest that long-term exposure may have a carcinogenic effect. This is separate from the risks associated with very high-intensity exposure, which can cause heating and burns like any heat source, and not a unique property of microwaves specifically.

During World War II, it was observed that individuals in the radiation path of radar installations experienced clicks and buzzing sounds in response to microwave radiation. This microwave auditory effect was thought to be caused by the microwaves inducing an electric current in the hearing centers of the brain. Research by NASA in the 1970s has shown this to be caused by thermal expansion in parts of the inner ear. In 1955 Dr. James Lovelock was able to reanimate rats frozen at 0 °C using microwave diathermy.

When injury from exposure to microwaves occurs, it usually results from dielectric heating induced in the body. Exposure to microwave radiation can produce cataracts by this mechanism, because the microwave heating denatures proteins in the crystalline lens of the eye (in the same way that heat turns egg whites white and opaque). The lens and cornea of the eye are especially vulnerable because they contain no blood vessels that can carry away heat. Exposure to heavy doses of microwave radiation (as from an oven that has been tampered with to allow operation even with the door open) can produce heat damage in other tissues as well, up to and including serious burns that may not be immediately evident because of the tendency for microwaves to heat deeper tissues with higher moisture content.

Eleanor R. Adair conducted microwave health research by exposing herself, animals and humans to microwave levels that made them feel warm or even start to sweat and feel quite uncomfortable. She found no adverse health effects other than heat.

Microwaves were first generated in the late 1800s in some of the earliest radio experiments by physicists who thought of them as a form of "invisible light". James Clerk Maxwell in his 1873 theory of electromagnetism, now called Maxwell's equations, had predicted the existence of electromagnetic waves and proposed that light was composed of these waves. In 1888, German physicist Heinrich Hertz was the first to demonstrate the existence of radio waves using a primitive spark gap radio transmitter. Hertz and the other early radio researchers were interested in exploring the similarities between radio waves and light waves, to test Maxwell's theory. They concentrated on producing short wavelength radio waves in the UHF and microwave ranges, with which they could duplicate classic optics experiments, using quasioptical components such as prisms and lenses made of paraffin and pitch and wire diffraction gratings, to refract and diffract radio waves like light rays. Hertz produced waves up to 450 MHz; his directional 450 MHz transmitter consisted of a 26 cm brass rod dipole antenna with a spark gap between the ends suspended at the focal line of a parabolic antenna made of a curved zinc sheet. His historic experiments demonstrated that radio waves like light exhibited refraction, diffraction, polarization, interference and standing waves, proving that radio waves and light waves were both forms of Maxwell's electromagnetic waves. 

In 1894, Oliver Lodge and Augusto Righi generated 1.5 and 12 GHz microwaves respectively with small metal ball spark resonators, and Bengali physicist Jagdish Chandra Bose generated 60 GHz microwaves in the millimeter wave region. Bose also used waveguide and horn antennas in his experiments. In 1897 Lord Rayleigh solved the mathematical boundary-value problem of electromagnetic waves propagating through conducting tubes and dielectric rods of arbitrary shape. which gave the modes and cutoff frequency of microwaves propagating through a waveguide. However since microwaves were limited to line of sight paths, the subsequent development of radio communication after 1896 employed lower frequencies, and microwave frequencies were not further explored at this time.

Practical use of microwave frequencies did not occur until the 1940s and 1950s due to a lack of adequate sources, since the triode vacuum tube (valve) electronic oscillator used in radio transmitters could not produce frequencies above a few hundred megahertz due to excessive electron transit time and interelectrode capacitance. By the 1930s, the first low power microwave vacuum tubes had been developed using new principles; the Barkhausen-Kurz tube and the split-anode magnetron. These could generate a few watts of power at frequencies up to a few gigahertz, and were used in the first experiments in communication with microwaves. In 1931 an Anglo-French consortium demonstrated the first experimental microwave relay link, across the English Channel between Dover, UK and Calais, France. The system transmitted telephony, telegraph and facsimile data over bidirectional 1.7 GHz beams with a power of one-half watt, produced by miniature Barkhausen-Kurz tubes at the focus of metal dishes. 

A word was needed to distinguish these new shorter wavelengths, which had previously been lumped into the "short wave" band, which meant all waves shorter than 200 meters. The terms "quasi-optical waves" and "ultrashort waves" were used briefly, but didn't catch on. The first usage of the word "microwave" apparently occurred in 1931.

The development of radar, mainly in secrecy, before and during World War 2, resulted in the technological advances which made microwaves practical. Radar antennas small enough to fit on aircraft which had a narrow enough beamwidth to localize enemy aircraft required wavelengths in the centimeter range. It was found that conventional transmission lines used to carry radio waves had excessive power losses at microwave frequencies, and George Southworth at Bell Labs and Wilmer Barrow at MIT independently invented waveguide in 1936. Barrow invented the horn antenna in 1938 as a means to efficiently radiate microwaves into or out of a waveguide. In a microwave receiver, a nonlinear component was needed that would act as a detector and mixer at these frequencies, as vacuum tubes had too much capacitance. To fill this need researchers resurrected an obsolete technology, the point contact crystal detector (cat whisker detector) which was used as a demodulator in crystal radios around the turn of the century before vacuum tube receivers. The low capacitance of semiconductor junctions allowed them to function at microwave frequencies. The first modern silicon and germanium diodes were developed as microwave detectors in the 1930s, and the principles of semiconductor physics learned during their development led to semiconductor electronics after the war.

The first powerful sources of microwaves were invented at the beginning of World War 2: the klystron tube by Russell and Sigurd Varian at Stanford University in 1937, and the cavity magnetron tube by John Randall and Harry Boot at Birmingham University, UK in 1940. Britain's 1940 decision to share its microwave technology with the US (the Tizard Mission) significantly influenced the outcome of the war. The MIT Radiation Laboratory established secretly at Massachusetts Institute of Technology in 1940 to research radar, produced much of the theoretical knowledge necessary to use microwaves. By 1943, 10 centimeter (3 GHz) radar was in use on British and American warplanes. The first microwave relay systems were developed by the Allied military near the end of the war and used for secure battlefield communication networks in the European theater.

After World War 2, microwaves were rapidly exploited commercially. Due to their high frequency they had a very large information-carrying capacity (bandwidth); a single microwave beam could carry tens of thousands of phone calls. In the 1950s and 60s transcontinental microwave relay networks were built in the US and Europe to exchange telephone calls between cities and distribute television programs. In the new television industry, from the 1940s microwave dishes were used to transmit video feed from mobile production trucks back to the studio, allowing the first remote TV broadcasts. The first communications satellites were launched in the 1960s, which relayed telephone calls and television between widely separated points on Earth using microwave beams. In 1964, Arno Penzias and Robert Woodrow Wilson while investigating noise in a satellite horn antenna at Bell Labs, Holmdel, New Jersey discovered cosmic microwave background radiation. 

Microwave radar became the central technology used in air traffic control, maritime navigation, anti-aircraft defense, ballistic missile detection, and later many other uses. Radar and satellite communication motivated the development of modern microwave antennas; the parabolic antenna (the most common type), cassegrain antenna, lens antenna, slot antenna, and phased array.

The ability of short waves to quickly heat materials and cook food had been investigated in the 1930s by I. F. Mouromtseff at Westinghouse, and at the 1933 Chicago World's Fair demonstrated cooking meals with a 60 MHz radio transmitter. In 1945 Percy Spencer, an engineer working on radar at Raytheon, noticed that the radiation from a magnetron oscillator melted a candy bar in his pocket. He investigated cooking with microwaves and invented the microwave oven, consisting of a magnetron feeding microwaves into a closed metal cavity containing food, which was patented by Raytheon on 8 October 1945. Microwave heating became widely used as an industrial process in industries such as plastics fabrication, and as a medical therapy to kill cancer cells in microwave hyperthermy.

The traveling wave tube (TWT) developed in 1943 by Rudolph Kompfner and John Pierce provided a high-power tunable source of microwaves up to 50 GHz, and became the most widely used microwave tube (besides the ubiquitous magnetron used in microwave ovens). The gyrotron tube family developed in Russia could produce megawatts of power up into millimeter wave frequencies, and is used in industrial heating and plasma research, and to power particle accelerators and nuclear fusion reactors.

The development of semiconductor electronics in the 1950s led to the first solid state microwave devices which worked by a new principle; negative resistance (some of the prewar microwave tubes had also used negative resistance). The feedback oscillator and two-port amplifiers which were used at lower frequencies became unstable at microwave frequencies, and negative resistance oscillators and amplifiers based on one-port devices like diodes worked better. 

The tunnel diode invented in 1957 by Japanese physicist Leo Esaki could produce a few milliwatts of microwave power. Its invention set off a search for better negative resistance semiconductor devices for use as microwave oscillators, resulting in the invention of the IMPATT diode in 1956 by W.T. Read and Ralph L. Johnston and the Gunn diode in 1962 by J. B. Gunn. Diodes are the most widely used microwave sources today. Two low-noise solid state negative resistance microwave amplifiers were developed; the ruby maser invented in 1953 by Charles H. Townes, James P. Gordon, and H. J. Zeiger, and the varactor parametric amplifier developed in 1956 by Marion Hines. These were used for low noise microwave receivers in radio telescopes and satellite ground stations. The maser led to the development of atomic clocks, which keep time using a precise microwave frequency emitted by atoms undergoing an electron transition between two energy levels. Negative resistance amplifier circuits required the invention of new nonreciprocal waveguide components, such as circulators, isolators, and directional couplers. In 1969 Kurokawa derived mathematical conditions for stability in negative resistance circuits which formed the basis of microwave oscillator design.

Prior to the 1970s microwave devices and circuits were bulky and expensive, so microwave frequencies were generally limited to the output stage of transmitters and the RF front end of receivers, and signals were heterodyned to a lower intermediate frequency for processing. The period from the 1970s to the present has seen the development of tiny inexpensive solid state microwave components which can be mounted on circuit boards, allowing circuits to perform significant signal processing at microwave frequencies. This has made possible satellite television, cable television, GPS devices, and modern wireless devices, such as smartphones, Wifi, and Bluetooth which connect to networks using microwaves. 

Microstrip, a type of transmission line usable at microwave frequencies, was invented with printed circuits in the 1950s. The ability to cheaply fabricate a wide range of shapes on printed circuit boards allowed microstrip versions of capacitors, inductors, resonant stubs, splitters, directional couplers, diplexers, filters and antennas to be made, thus allowing compact microwave circuits to be constructed. 
Transistors that operated at microwave frequencies were developed in the 1970s. The semiconductor gallium arsenide (GaAs) has a much higher electron mobility than silicon, so devices fabricated with this material can operate at 4 times the frequency of similar devices of silicon. Beginning in the 1970s GaAs was used to make the first microwave transistors, and it has dominated microwave semiconductors ever since. MESFETs (metal-semiconductor field-effect transistors), fast GaAs field effect transistors using Schottky junctions for the gate, were developed starting in 1968 and have reached cutoff frequencies of 100 GHz, and are now the most widely used active microwave devices. Another family of transistors with a higher frequency limit is the HEMT (high electron mobility transistor), a field effect transistor made with two different semiconductors, AlGaAs and GaAs, using heterojunction technology, and the similar HBT (heterojunction bipolar transistor).

GaAs can be made semi-insulating, allowing it to be used as a substrate on which circuits containing passive components as well as transistors can be fabricated by lithography. By 1976 this led to the first integrated circuits (ICs) which functioned at microwave frequencies, called monolithic microwave integrated circuits (MMIC). The word "monolithic" was added to distinguish these from microstrip PCB circuits, which were called "microwave integrated circuits" (MIC). Since then silicon MMICs have also been developed. Today MMICs have become the workhorses of both analog and digital high frequency electronics, enabling the production of single chip microwave receivers, broadband amplifiers, modems, and microprocessors.



</doc>
<doc id="20101" url="https://en.wikipedia.org/wiki?curid=20101" title="Mary">
Mary

Mary may refer to:











</doc>
<doc id="20108" url="https://en.wikipedia.org/wiki?curid=20108" title="Mick Doohan">
Mick Doohan

Michael "Mick" Sydney Melbourne Doohan, (born 4 June 1965) is an Australian former Grand Prix motorcycle road racing World Champion, who won five consecutive 500 cc World Championships. Only Giacomo Agostini with eight (seven consecutive) and Valentino Rossi with seven (five consecutive) have won more premier class titles.

Originally from the Gold Coast, near Brisbane, Doohan attended St. Joseph's College, Gregory Terrace, Brisbane. He raced in Australian Superbikes in the late 1980s, and also won both races as Superbike World Championship visited Oran Park in as well as the second leg of the Japanese round held earlier in the year. In a break-out season he also won the final Australian motorcycle Grand Prix to be held in the TT format at Mount Panorama before the race became a round of the World Championship the following year and moved to Phillip Island. He is one of the few 500 cc or MotoGP World Champions to have won a Superbike World Championship race.
He made his Grand Prix debut for Honda on an NSR 500 cc two-stroke motorcycle in 1989. Late in the 1990 season Doohan claimed his first victory at the Hungarian Grand Prix on his way to third in the championship. In 1991, he was paired with his fellow Australian Wayne Gardner on a Honda RVF750 superbike and won the Suzuka 8 Hours endurance race. He competed successfully throughout the early 1990s and appeared to be on his way to winning his first world championship when he was seriously injured in a practice crash before the 1992 Dutch TT. He suffered permanent and serious damage to his right leg due to medical complications and, at one stage, faced amputation of the leg. At the time, Doohan was 65 points in the lead of the championship, but could not compete for eight weeks after the crash. After an arduous recovery, he returned to racing for the final two races but could not prevent Yamaha rider Wayne Rainey from winning his third consecutive title (by four points from Doohan). In 1993 he struggled with the healing of his leg and the ability to race the Honda at elite level, stating later that in that year it was all he could do to just keep his ride at Honda. It was also during this time he switched to a left thumb-operated rear brake, as his right foot is no longer able to perform this function.

In 1994 however, he won his first 500 cc World Championship. Thereafter, until 1998, he dominated the class, winning five consecutive 500 cc World Championships. In 1997, his most successful year, Doohan won 12 out of 15 races, finished second in another two, and crashed out of the final race of the season at his home GP while leading by more than six seconds. In June 1996, Doohan was inducted as a Member of the Order of Australia for his contribution to the sport of motor racing. 

Despite up to eight rivals on non-factory HRC Honda motorcycles Doohan's margin of superiority over them was such that in many races Doohan would build a comfortable lead and then ride well within his limits to cruise to victory. Although pure riding skill clearly played a large part in his success, the ability of his chief race engineer, Jeremy Burgess, to perfect the suspension and geometry of a racing motorcycle may have given him an advantage over his rivals. Between 1994 and 1998 the bike was said not to have had many changes, with Honda engineers reportedly becoming frustrated at Doohan's reluctance to try innovations such as electronic shifting (it was only when Rossi came to Honda in 2000 that Honda engineers had their head with Rossi willing to try more innovations).

One notable trait of Doohan's post-crash riding style was the use of a thumb-operated rear brake developed during 1993 owing to the reduced range of motion in his ankle. This was operated by a "nudge" bar similar to a personal water craft throttle, but mounted on the left handlebar. In 1999 Doohan had another accident, this time in a very wet qualifying session for the Spanish Grand Prix. He again broke his leg in several places and subsequently announced his retirement. Jeremy Burgess, Doohan's chief engineer for his entire career, later became Valentino Rossi's chief engineer. After Doohan retired he went to work as a roving adviser to Honda's Grand Prix race effort. At the conclusion of the 2004 season, Doohan and Honda parted company. 

In June 2011, Doohan made an appearance at the Isle of Man TT. Doohan completed a parade lap, and was most enamored by the thrill and spectacle of the Snaefell Mountain Course. He then went on to pay tribute to his former Honda racing teammate, Joey Dunlop.

On 8 August 2006, Doohan appeared in Darwin Magistrates Court to face charges over a weekend fracas at a strip club. He was fined $2,500 after pleading guilty to assaulting a bouncer and failing to leave a licensed premise. No conviction was recorded.

Doohan married Selina Sines, his partner of eleven years, on Tuesday 21 March 2006, on Hamilton Island; the couple have two children.

After his success in Grand Prix motorcycle racing he got a chance to test a Formula One race car, the Williams FW19, at Circuit de Catalunya (in Spain) in April 1998. He found the car difficult to drive and crashed against a guard rail.

Doohan helped design an Intamin Motorbike Launch Roller Coaster, named Mick Doohan's Motocoaster. The ride is located at Dreamworld on the Gold Coast, Queensland.

Doohan was made a Member of the Order of Australia in 1996 and received an Australian Sports Medal in 2000. He was inducted into the Sport Australia Hall of Fame in 2009. The first turn at Phillip Island Grand Prix Circuit is named after him.

In 2009 as part of the Q150 celebrations, Mick Doohan was announced as one of the Q150 Icons of Queensland for his role as a "sports legend".


 


</doc>
