<doc id="22656" url="https://en.wikipedia.org/wiki?curid=22656" title="Operand">
Operand

In mathematics an operand is the object of a mathematical operation, i.e. it is the object or quantity that is operated on.

The following arithmetic expression shows an example of operators and operands:

In the above example, '+' is the symbol for the operation called addition. 

The operand '3' is one of the inputs (quantities) followed by the addition operator, and the operand '6' is the other input necessary for the operation.

The result of the operation is 9. (The number '9' is also called the sum of the augend 3 and the addend 6.)

An operand, then, is also referred to as "one of the inputs (quantities) for an operation".

Operands may be complex, and may consist of expressions also made up of operators with operands.

In the above expression '(3 + 5)' is the first operand for the multiplication operator and '2' the second. The operand '(3 + 5)' is an expression in itself, which contains an addition operator, with the operands '3' and '5'.

Rules of precedence affect which values form operands for which operators:

In the above expression, the multiplication operator has the higher precedence than the addition operator, so the multiplication operator has operands of '5' and '2'. The addition operator has operands of '3' and '5 × 2'.

Depending on the mathematical notation being used the position of an operator in relation to its operand(s) may vary. In everyday usage infix notation is the most common, however other notations also exist, such as the prefix and postfix notations. These alternate notations are most common within computer science.

Below is a comparison of three different notations — all represent an addition of the numbers '1' and '2'

In a mathematical expression, the order of operation is carried out from left to right. Start with the left most value and seek the first operation to be carried out in accordance with the order specified above (i.e., start with parentheses and end with the addition/subtraction group). For example, in the expression

the first operation to be acted upon is any and all expressions found inside a parenthesis. So beginning at the left and moving to the right, find the first (and in this case, the only) parenthesis, that is, (2 + 2). Within the parenthesis itself is found the expression 2. The reader is required to find the value of 2 before going any further. The value of 2 is 4. Having found this value, the remaining expression looks like this:

The next step is to calculate the value of expression inside the parenthesis itself, that is, (2 + 4) = 6. Our expression now looks like this:

Having calculated the parenthetical part of the expression, we start over again beginning with the left most value and move right. The next order of operation (according to the rules) is exponents. Start at the left most value, that is, 4, and scan your eyes to the right and search for the first exponent you come across. The first (and only) expression we come across that is expressed with an exponent is 2. We find the value of 2, which is 4. What we have left is the expression

The next order of operation is multiplication. 4 × 4 is 16. Now our expression looks like this:

The next order of operation according to the rules is division. However, there is no division operator sign (÷) in the expression, 16 − 6. So we move on to the next order of operation, i.e., addition and subtraction, which have the same precedence and are done left to right.

So the correct value for our original expression, 4 × 2 − (2 + 2), is 10. 

It is important to carry out the order of operation in accordance with rules set by convention. If the reader evaluates an expression but does not follow the correct order of operation, the reader will come forth with a different value. The different value will be the incorrect value because the order of operation was not followed. The reader will arrive at the correct value for the expression if and only if each operation is carried out in the proper order.

The number of operands of an operator is called its arity. Based on arity, operators are classified as nullary (no operands), unary (1 operand), binary (2 operands), ternary (3 operands) etc.

In computer programming languages, the definitions of operator and operand are almost the same as in mathematics.

In computing, an operand is the part of a computer instruction which specifies what data is to be manipulated or operated on, while at the same time representing the data itself.
A computer instruction describes an operation such as add or multiply X, while the operand (or operands, as there can be more than one) specify on which X to operate as well as the value of X.

Additionally, in assembly language, an operand is a value (an argument) on which the instruction, named by mnemonic, operates. The operand may be a processor register, a memory address, a literal constant, or a label. A simple example (in the x86 architecture) is

MOV DS, AX

where the value in register operand codice_1 is to be moved (codice_2) into register codice_3. Depending on the instruction, there may be zero, one, two, or more operands.



</doc>
<doc id="22657" url="https://en.wikipedia.org/wiki?curid=22657" title="Order of magnitude">
Order of magnitude

An order of magnitude is an approximate measure of the number of digits that a number has in the commonly-used base-ten number system. It is equal to the logarithm (base 10) rounded to a whole number. For example, the order of magnitude of 1500 is 3, because 1500 = 1.5 × 10.

Differences in order of magnitude can be measured on a base-10 logarithmic scale in “decades” (i.e., factors of ten). Examples of numbers of different magnitudes can be found at Orders of magnitude (numbers).

Generally, the order of magnitude of a number is the smallest power of 10 used to represent that number. To work out the order of magnitude of a number formula_1, the number is first expressed in the following form:
where formula_3. Then, formula_4 represents the order of magnitude of the number. The order of magnitude can be any integer. The table below enumerates the order of magnitude of some numbers in light of this definition:

The geometric mean of formula_5 and formula_6 is formula_7, meaning that a value of exactly formula_5 (i.e., formula_9) represents a geometric "halfway point" within the range of possible values of formula_10.

Some use a simpler definition where <math>0.5, perhaps because the arithmetic mean of formula_5 and formula_12 approaches formula_13 for increasing formula_14. This definition has the effect of lowering the values of formula_4 slightly:

Yet others restrict formula_10 to values where formula_17, making the order of magnitude of a number exactly equal to its exponent part in scientific notation.

Orders of magnitude are used to make approximate comparisons. If numbers differ by one order of magnitude, "x" is "about" ten times different in quantity than "y". If values differ by two orders of magnitude, they differ by a factor of about 100. Two numbers of the same order of magnitude have roughly the same scale: the larger value is less than ten times the smaller value.

The order of magnitude of a number is, intuitively speaking, the number of powers of 10 contained in the number. More precisely, the order of magnitude of a number can be defined in terms of the common logarithm, usually as the integer part of the logarithm, obtained by truncation. For example, the number has a logarithm (in base 10) of 6.602; its order of magnitude is 6. When truncating, a number of this order of magnitude is between 10 and 10. In a similar example, with the phrase "He had a seven-figure income", the order of magnitude is the number of figures minus one, so it is very easily determined without a calculator to 6. An order of magnitude is an approximate position on a logarithmic scale.

An order-of-magnitude estimate of a variable, whose precise value is unknown, is an estimate rounded to the nearest power of ten. For example, an order-of-magnitude estimate for a variable between about 3 billion and 30 billion (such as the human population of the Earth) is 10 billion. To round a number to its nearest order of magnitude, one rounds its logarithm to the nearest integer. Thus , which has a logarithm (in base 10) of 6.602, has 7 as its nearest order of magnitude, because "nearest" implies rounding rather than truncation. For a number written in scientific notation, this logarithmic rounding scale requires rounding up to the next power of ten when the multiplier is greater than the square root of ten (about 3.162). For example, the nearest order of magnitude for is 8, whereas the nearest order of magnitude for is 9. An order-of-magnitude estimate is sometimes also called a zeroth order approximation.

An order-of-magnitude difference between two values is a factor of 10. For example, the mass of the planet Saturn is 95 times that of Earth, so Saturn is "two orders of magnitude" more massive than Earth. Order-of-magnitude differences are called decades when measured on a logarithmic scale.

Other orders of magnitude may be calculated using bases other than 10. The ancient Greeks ranked the nighttime brightness of celestial bodies by 6 levels in which each level was the fifth root of one hundred (about 2.512) as bright as the nearest weaker level of brightness, and thus the brightest level being 5 orders of magnitude brighter than the weakest indicates that it is (100) or a factor of 100 times brighter.

The different decimal numeral systems of the world use a larger base to better envision the size of the number, and have created names for the powers of this larger base. The table shows what number the order of magnitude aim at for base 10 and for base . It can be seen that the order of magnitude is included in the number name in this example, because bi- means 2 and tri- means 3 (these make sense in the long scale only), and the suffix -illion tells that the base is . But the number names billion, trillion themselves (here with other meaning than in the first chapter) are not names of the "orders of" magnitudes, they are names of "magnitudes", that is the "numbers" etc.

SI units in the table at right are used together with SI prefixes, which were devised with mainly base 1000 magnitudes in mind. The IEC standard prefixes with base 1024 were invented for use in electronic technology.

The ancient apparent magnitudes for the brightness of stars uses the base formula_18 and is reversed. The modernized version has however turned into a logarithmic scale with non-integer values.

For extremely large numbers, a generalized order of magnitude can be based on their double logarithm or super-logarithm. Rounding these downward to an integer gives categories between very "round numbers", rounding them to the nearest integer and applying the inverse function gives the "nearest" round number.

The double logarithm yields the categories:
(the first two mentioned, and the extension to the left, may not be very useful, they merely demonstrate how the sequence mathematically continues to the left).

The super-logarithm yields the categories:

The "midpoints" which determine which round number is nearer are in the first case:
and, depending on the interpolation method, in the second case

For extremely small numbers (in the sense of close to zero) neither method is suitable directly, but the generalized order of magnitude of the reciprocal can be considered.

Similar to the logarithmic scale one can have a double logarithmic scale (example provided here) and super-logarithmic scale. The intervals above all have the same length on them, with the "midpoints" actually midway. More generally, a point midway between two points corresponds to the generalised f-mean with "f"("x") the corresponding function log log "x" or slog "x". In the case of log log "x", this mean of two numbers (e.g. 2 and 16 giving 4) does not depend on the base of the logarithm, just like in the case of log "x" (geometric mean, 2 and 8 giving 4), but unlike in the case of log log log "x" (4 and giving 16 if the base is 2, but not otherwise).





</doc>
<doc id="22659" url="https://en.wikipedia.org/wiki?curid=22659" title="Ockham">
Ockham

Occam or Ockham may refer to:




</doc>
<doc id="22660" url="https://en.wikipedia.org/wiki?curid=22660" title="Occam (programming language)">
Occam (programming language)

occam is a programming language which is concurrent and builds on the communicating sequential processes (CSP) process algebra, and shares many of its features. It is named after philosopher William of Ockham for whom Occam's razor is named.

occam is an imperative procedural language (such as Pascal). It was developed by David May and others at Inmos (trademark INMOS), advised by Tony Hoare, as the native programming language for their transputer microprocessors, but implementations for other platforms are available. The most widely known version is occam 2; its programming manual was written by Steven Ericsson-Zenith and others at Inmos.

In the following examples indentation and formatting are critical for parsing the code: expressions are terminated by the end of the line, lists of expressions need to be on the same level of indentation. This feature, named the off-side rule, is also found in other languages such as Haskell and Python.

Communication between processes work through named "channels". One process outputs data to a channel via codice_1 while another one inputs data with codice_2. Input and output cannot proceed until the other end is ready to accept or offer data. (In the "not proceeding" case it is often said that the process "blocks" on the channel. However, the program will neither spin nor poll; thus terms like "wait", "hang" or "yield" may also convey the behaviour; also in the context that it will not "block" other independent processes from running.) Examples (c is a variable):

SEQ introduces a list of expressions that are evaluated sequentially. This is not implicit as it is in most other programming languages. Example:

PAR begins a list of expressions that may be evaluated concurrently. Example:

ALT specifies a list of "guarded" commands. The "guards" are a combination of a boolean condition and an input expression (both optional). Each guard for which the condition is true and the input channel is ready is successful. One of the successful alternatives is selected for execution. Example:

This will read data from channels c1 or c2 (whichever is ready) and pass it into a merged channel. If countN reaches 100, reads from the corresponding channel will be disabled. A request on the status channel is answered by outputting the counts to out.

occam 1 (released 1983) was a preliminary version of the language which borrowed from David May's work on EPL and Tony Hoare's CSP. This supported only the VAR data type, which was an integral type corresponding to the native word length of the target architecture, and arrays of only one dimension.

occam 2 is an extension produced by Inmos Ltd in 1987 that adds floating-point support, functions, multi-dimensional arrays and more data types such as varying sizes of integers (INT16, INT32) and bytes.

With this revision, occam became a language capable of expressing useful programs, whereas occam 1 was more suited to examining algorithms and exploring the new language (however, the occam 1 compiler was written in occam 1, so there is an existence proof that reasonably sized, useful programs could be written in occam 1, despite its limitations).

occam 2.1 was the last of the series of occam language developments contributed by Inmos. Defined in 1994, it was influenced by an earlier proposal for an occam 3 language (also referred to as "occam91" during its early development) created by Geoff Barrett at Inmos in the early 1990s. A revised Reference Manual describing occam 3 was distributed for community comment, but the language was never fully implemented in a compiler.

occam 2.1 introduced several new features to occam 2, including:

For a full list of the changes see Appendix P of the Inmos occam 2.1 Reference Manual.

occam-π is the common name for the occam variant implemented by later versions of the Kent Retargetable occam Compiler (KRoC). The addition of the symbol "π" (pi) to the occam name is an allusion to KRoC occam including several ideas inspired by the pi-calculus. It contains several significant extensions to the occam 2.1 compiler, for example:






</doc>
<doc id="22661" url="https://en.wikipedia.org/wiki?curid=22661" title="October Revolution">
October Revolution

The October Revolution (), officially known in Soviet literature as the Great October Socialist Revolution (, '), and commonly referred to as Red October, the October Uprising, the Bolshevik Revolution, or the Bolshevik Coup, was a revolution in Russia led by the Bolsheviks and Vladimir Lenin that was instrumental in the larger Russian Revolution of 1917. It took place with an armed insurrection in Petrograd on 7 November (25 October, Old Style) 1917.

It followed and capitalized on the February Revolution of the same year, which overthrew the Tsarist autocracy and resulted in a provisional government after a transfer of power proclaimed by Grand Duke Michael, brother of Tsar Nicolas II, who declined to take power after the Tsar stepped down. During this time, urban workers began to organize into councils (Russian: "Soviet") wherein revolutionaries criticized the provisional government and its actions. After the Congress of Soviets, now the governing body, had its second session, it elected members of the Bolsheviks and other leftist groups such as the Left Socialist Revolutionaries to important positions within the new state of affairs. This immediately initiated the establishment of the Russian Socialist Federative Soviet Republic, the world's first self-proclaimed socialist state. On 17 July 1918, the Tsar and his family were executed.

The revolution was led by the Bolsheviks, who used their influence in the Petrograd Soviet to organize the armed forces. Bolshevik Red Guards forces under the Military Revolutionary Committee began the occupation of government buildings on 7 November 1917 (New Style). The following day, the Winter Palace (the seat of the Provisional government located in Petrograd, then capital of Russia) was captured.

The long-awaited Constituent Assembly elections were held on 12 November 1917. In contrast to their majority in the Soviets, the Bolsheviks only won 175 seats in the 715-seat legislative body, coming in second behind the Socialist Revolutionary Party, which won 370 seats, although the SR Party no longer existed as a whole party by that time, as the Left SRs had gone into coalition with the Bolsheviks from October 1917 to March 1918. The Constituent Assembly was to first meet on 28 November 1917, but its convocation was delayed until 5 January 1918 by the Bolsheviks. On its first and only day in session, the Constituent Assembly came into conflict with the Soviets, and it rejected Soviet decrees on peace and land, resulting in the Constituent Assembly being dissolved the next day by order of the Congress of Soviets.

As the revolution was not universally recognized, there followed the struggles of the Russian Civil War (1917–22) and the creation of the Soviet Union in 1922.

At first, the event was referred to as the "October coup" (Октябрьский переворот) or the "Uprising of 3rd", as seen in contemporary documents (for example, in the first editions of Lenin's complete works). In Russian, however, "переворот" has a similar meaning to "revolution" and also means "upheaval" or "overturn", so "coup" is not necessarily the correct translation. With time, the term "October Revolution" (Октябрьская революция) came into use. It is also known as the "November Revolution" having occurred in November according to the Gregorian Calendar (for details, see Soviet calendar).

The February Revolution had toppled Tsar Nicolas II of Russia, and replaced his government with the Russian Provisional Government. However, the provisional government was weak and riven by internal dissension. It continued to wage World War I, which became increasingly unpopular. A nationwide crisis developed in Russia, affecting social, economic, and political relations. Disorder in industry and transport had intensified, and difficulties in obtaining provisions had increased. Gross industrial production in 1917 had decreased by over 36% from what it had been in 1914. In the autumn, as much as 50% of all enterprises were closed down in the Urals, the Donbas, and other industrial centers, leading to mass unemployment. At the same time, the cost of living increased sharply. Real wages fell about 50% from what they had been in 1913. Russia's national debt in October 1917 had risen to 50 billion rubles. Of this, debts to foreign governments constituted more than 11 billion rubles. The country faced the threat of financial bankruptcy.

Throughout June, July, and August 1917, it was common to hear working-class Russians speak about their lack of confidence and misgivings with those in power in the Provisional Government. Factory workers around Russia felt unhappy with the growing shortages of food, supplies, and other materials. They blamed their own managers or foremen and would even attack them in the factories. The workers blamed many rich and influential individuals, such as elites in positions of power, for the overall shortage of food and poor living conditions. Workers labelled these rich and powerful individuals as opponents of the Revolution, and called them words such as "bourgeois, capitalist, and imperialist."

In September and October 1917, there were mass strike actions by the Moscow and Petrograd workers, miners in Donbas, metalworkers in the Urals, oil workers in Baku, textile workers in the Central Industrial Region, and railroad workers on 44 railway lines. In these months alone, more than a million workers took part in strikes. Workers established control over production and distribution in many factories and plants in a social revolution. Workers were able to organize these strikes through factory committees. The factory committees represented the workers and were able to negotiate better working conditions, pay, and hours. Even though workplace conditions may have been increasing in quality, the overall quality of life for workers was not improving. There were still shortages of food and the increased wages workers had obtained did little to provide for their families.

By October 1917, peasant uprisings were common. While the uprisings varied in severity, complete uprisings and seizures of the land were not uncommon. Less robust forms of protest included marches on landowner manors and government offices, as well as withholding and storing grains rather than selling them as a result of the economic crisis. When the Provisional Government sent punitive detachments, it only enraged the peasants. The garrisons in Petrograd, Moscow, and other cities, the Northern and Western fronts, and the sailors of the Baltic Fleet in September declared through their elected representative body Tsentrobalt that they did not recognize the authority of the Provisional Government and would not carry out any of its commands.

Soldiers' wives were key players in unrest in the village. From 1914 to 1917, almost 50% of healthy men were sent to war, and many were killed on the front, resulting in a female occupation of the position of the household head. When government allowances were often late and were not sufficient to match the rising costs of goods, soldiers' wives sent masses of appeals and letters to the government, which largely were left unanswered. Frustration resulted, and these women were influential in inciting "subsistence riots" – also referred to as "hunger riots," "pogroms," or "baba riots". In these riots, citizens seized food and resources from shop owners who they believed to be charging unfair prices. Upon police intervention, protesters responded with "rakes, sticks, rocks and fists".

In a diplomatic note of 1 May, the minister of foreign affairs, Pavel Milyukov, expressed the Provisional Government's desire to continue the war against the Central Powers "to a victorious conclusion", arousing broad indignation. On 1–4 May, about 100,000 workers and soldiers of Petrograd, and after them the workers and soldiers of other cities, led by the Bolsheviks, demonstrated under banners reading "Down with the war!" and "all power to the soviets!" The mass demonstrations resulted in a crisis for the Provisional Government. 1 July saw more demonstrations, as about 500,000 workers and soldiers in Petrograd demonstrated, again demanding "all power to the soviets", "down with the war", and "down with the ten capitalist ministers". The Provisional Government opened an offensive against the Central Powers on 1 July, which soon collapsed. The news of the offensive and its collapse intensified the struggle of the workers and the soldiers. A new crisis in the Provisional Government began on 15 July.

On 16 July, spontaneous demonstrations of workers and soldiers began in Petrograd, demanding that power be turned over to the soviets. The Central Committee of the Russian Social Democratic Labour Party provided leadership to the spontaneous movements. On 17 July, over 500,000 people participated in what was intended to be a peaceful demonstration in Petrograd, the so-called July Days. The Provisional Government, with the support of Socialist-Revolutionary Party-Menshevik leaders of the All-Russian Executive Committee of the Soviets, ordered an armed attack against the demonstrators, killing hundreds.

A period of repression followed. On 5–6 July, attacks were made on the editorial offices and printing presses of "Pravda" and on the Palace of Kshesinskaya, where the Central Committee and the Petrograd Committee of the Bolsheviks were located. On 7 July, the government ordered the arrest and trial of Vladimir Lenin. He was forced to go underground, as he had been under the Tsarist regime. Bolsheviks were arrested, workers were disarmed, and revolutionary military units in Petrograd were disbanded or sent to the war front. On 12 July, the Provisional Government published a law introducing the death penalty at the front. The second coalition government was formed on 24 July, chaired by Alexander Kerensky.

Another problem for the government centered on General Lavr Kornilov, who had been Commander-in-Chief since 18 July. In response to a Bolshevik appeal, Moscow's working class began a protest strike of 400,000 workers. They were supported by strikes and protest rallies by workers in Kiev, Kharkov, Nizhny Novgorod, Ekaterinburg, and other cities.

In what became known as the Kornilov affair, Kornilov directed an army under Aleksandr Krymov to march toward Petrograd to restore order to Russia, with Kerensky's agreement. Details remain sketchy, but Kerensky appeared to become frightened by the possibility the army would stage a coup, and reversed the order. By contrast, historian Richard Pipes has argued that the episode was engineered by Kerensky. On 27 August, feeling betrayed by the government, Kornilov pushed on towards Petrograd. With few troops to spare on the front, Kerensky turned to the Petrograd Soviet for help. Bolsheviks, Mensheviks and Socialist Revolutionaries confronted the army and convinced them to stand down. The Bolsheviks' influence over railroad and telegraph workers also proved vital in stopping the movement of troops. Right-wingers felt betrayed, and the left wing was resurgent.

With Kornilov defeated, the Bolsheviks' popularity in the soviets grew significantly, both in the central and local areas. On 31 August, the Petrograd Soviet of Workers and Soldiers Deputies, and on 5 September, the Moscow Soviet Workers Deputies adopted the Bolshevik resolutions on the question of power. The Bolsheviks won a majority in the Soviets of Briansk, Samara, Saratov, Tsaritsyn, Minsk, Kiev, Tashkent, and other cities.

On 23 October 1917 (5 November new style), the Bolsheviks' Central Committee voted 10–2 for a resolution saying that "an armed uprising is inevitable, and that the time for it is fully ripe". At the Committee meeting, Lenin discussed how the people of Russia had waited long enough for "an armed uprising", and it was the Bolsheviks' time to take power. Lenin expressed his confidence in the success of the planned insurrection. His confidence stemmed from months of Bolshevik buildup of power and successful elections to different committees and councils in major cities such as Petrograd and Moscow.

The Bolsheviks created a revolutionary military committee within the Petrograd soviet, led by the soviet's president, Trotsky. The committee included armed workers, sailors and soldiers, and assured the support or neutrality of the capital's garrison. The committee methodically planned to occupy strategic locations through the city, almost without concealing their preparations: the Provisional Government's president Kerensky was himself aware of them, and some details, leaked by Kamenev and Zinoviev, were published in newspapers.

In the early morning of 24 October (November 6 N.S.), a group of soldiers loyal to Kerensky's government marched on the printing house of the Bolshevik newspaper, "Rabochy put [Worker's Path]", seizing and destroying printing equipment and thousands of newspapers copies. Shortly thereafter the government announced the immediate closure of not only "Rabochy put" but also the left-wing "Soldat" as well as the far-right newspapers "Zhivoe slovo" and "Novaia Rus"'. The editors of these newspapers, as well as any authors seen to be calling for insurrection, were to be prosecuted on criminal charges.

In response, at 9 AM the Military Revolutionary Committee issued a statement denouncing the government's actions. At 10 AM, Bolshevik-aligned soldiers successfully retook the "Rabochy put" printing house. Kerensky responded at approximately 3 PM that afternoon by ordering the raising of all but one of Petrograd's bridges, a tactic used by the government several months earlier in the July Days. What followed was a series of sporadic clashes over control of the bridges between Red Guard militias aligned with the Military Revolutionary Committee and military regiments still loyal to the government. At approximately 5 PM the Military Revolutionary Committee seized the Central Telegraph of Petrograd, giving the Bolsheviks control over communications through the city.

On 25 October (7 November new style) 1917, Bolsheviks led their forces in the uprising in Petrograd (now St. Petersburg, then capital of Russia) against the Kerensky Provisional Government. The event coincided with the arrival of a flotilla of pro-Bolshevik marines, primarily five destroyers and their crews, in St. Petersburg harbor. At Kronstadt, sailors also announced their allegiance to the Bolshevik insurrection. In the early morning, the military-revolutionary committee planned the last of the locations to be assaulted or seized from its heavily guarded and picketed center in Smolny Palace. The Red Guards systematically captured major government facilities, key communication installations and vantage points with little opposition. The Petrograd Garrison and most of the city's military units joined the insurrection against the Provisional Government.

Kerensky and the provisional government were virtually helpless to offer significant resistance. Railways and railway stations had been controlled by Soviet workers and soldiers for days, making rail travel to and from Petrograd impossible for Provisional Government officials. The Provisional Government was also unable to locate any serviceable vehicles. On the morning of the insurrection, Kerensky desperately searched for a means of reaching military forces he hoped would be friendly to the Provisional government outside the city, and ultimately borrowed a Renault car from the American Embassy, which he drove from the Winter Palace alongside a Pierce Arrow. Kerensky was able to evade the pickets going up around the palace and drive to meet approaching soldiers.

As Kerensky left Petrograd, Lenin wrote a proclamation ""To the Citizens of Russia"" stating that the Provisional Government had been overthrown by the Military Revolutionary Committee. The proclamation was sent by telegraph throughout Russia even as the pro-Soviet soldiers were seizing important control centers throughout the city. One of Lenin's intentions was to present members of the Soviet congress, who would assemble that afternoon, with a "fait accompli" and thus forestall further debate on the wisdom or legitimacy of taking power.

A bloodless insurrection occurred with a final assault against the Winter Palace, with 3,000 cadets, officers, cossacks and female soldiers poorly defending the Winter Palace. The Bolsheviks delayed the assault because the revolutionaries could not find functioning artillery. The Bolsheviks also prolonged the assault for fear of violence since the insurrection did not generate violent outbreaks. At 6:15p.m., a large group of artillery cadets abandoned the palace, taking their artillery with them. At 8:00p.m., 200 cossacks left the palace and returned to their barracks. While the cabinet of the provisional government within the palace debated what action to take, the Bolsheviks issued an ultimatum to surrender. Workers and soldiers occupied the last of the telegraph stations, cutting off the cabinet's communications with loyal military forces outside the city. As the night progressed, crowds of insurgents surrounded the palace, and many infiltrated it. While soviet historians and officials tended to depict the event in heroic terms, the insurrection and even the seizure of the Winter Palace happened almost without resistance. At 9:45p.m, the cruiser "Aurora" fired a blank shot from the harbor. Some of the revolutionaries entered the Palace at 10:25p.m. and there was a mass entry 3 hours later. By 2:10a.m on 26 October Bolshevik forces had gained control of the palace. After sporadic gunfire throughout the building, the cabinet of the provisional government had surrendered. The only member who was not arrested was Kerensky himself who had already left the Palace.

Later official accounts of the revolution from the Soviet Union would depict the events in October as being far more dramatic than they actually had been. (See a first-hand account by British General Knox.) This was aided by the historical reenactment, entitled "The Storming of the Winter Palace", which was staged in 1920. This reenactment, watched by 100,000 spectators, provided the model for official films made much later, which showed a huge storming of the Winter Palace and fierce fighting. In reality, the Bolshevik insurgents faced little opposition. The insurrection was timed and organized to hand state power to the Second All-Russian Congress of Soviets of Workers' and Soldiers' Deputies, which began on 25 October (7 November new style). After a single day of revolution, the death toll was low not because Bolsheviks decided not to use artillery fire, but instead because the class struggle was used as the strongest weapon.

Soviet government archives show that parties of Bolshevik operatives sent from the Smolny Institute by Lenin took over all critical centers of power in Petrograd in the early hours of the first night without a significant number of shots fired. This was completed so efficiently that the takeover resembled the changing of the guard. There was not much of a storming of the Winter Palace because the resistance basically did not exist and at 2:10a.m. on 26 October (8 November new style) 1917 the Red Guards took control of the Winter Palace. The Cossacks deserted when the Red Guard approached, and the Cadets and the 140 volunteers of the Women's Battalion surrendered rather than resist the 40,000 strong army. The "Aurora" was commandeered to then fire blanks at the palace in a symbolic act of rejection of the government. The Bolsheviks effectively controlled the almost unoccupied Winter Palace not because of an intense military barrage, but because the back door was left open, allowing the Red Guard to enter. The Provisional Government was arrested and imprisoned in Peter and Paul Fortress after the ministers resigned to fate and surrendered without a fight, and officially overthrown.

Later stories of the heroic "Storming of the Winter Palace" and "defense of the Winter Palace" were later propaganda by Bolshevik publicists. Grandiose paintings depicting the "Women's Battalion" and photo stills taken from Sergei Eisenstein's staged film depicting the "politically correct" version of the October events in Petrograd came to be taken as truth.

With the Petrograd Soviet now in control of government, garrison and proletariat, the Second All Russian Congress of Soviets held its opening session on the day, while Trotsky dismissed the opposing Mensheviks and the Socialist Revolutionaries (SR) from Congress.

Some sources contend that as the leader of Tsentrobalt, Pavlo Dybenko played an enormous role in the revolt. It is said that the ten warships that entered the city with ten thousand Baltic fleet mariners was the force that actually took the power in Petrograd and put down the Provisional Government. The same mariners then dispersed by force the elected parliament of Russia, and used machine-gun fire against protesting demonstrators in Petrograd. About 100 demonstrators were killed, and several hundreds wounded. Dybenko in his memoirs mentioned this event as "several shots in the air". Later, during the first hours after the taking of the Winter Palace, Dybenko personally entered the Ministry of Justice and destroyed there the documents about the financing of the Bolshevik party by Germany. These are disputed by various sources such as Louise Bryant, who claims that news outlets in the West at the time reported that the unfortunate loss of life occurred in Moscow, not Petrograd, and the number was much less than suggested above. As for the "several shots in the air", there is little evidence suggesting otherwise. The alleged action of Dybenko entering the Ministry of Justice to destroy documents as recalled by Savchenko can also be challenged. According to reports, Pavel Dybenko was in Helsingfors organizing the sailors' departures for Petrograd. In the book "Radio October...On the "Krechet" in Helsingfors", radio operator Makarov hands a telegram to Pavel Dybenko with the report of the "Samson" commissar, Grigoriy Borisov: "To Tsentrobalt. Everything is calm in Petrograd. The power is in the hands of the revolutionary committee. You have to immediately get in touch with the front committee of the Northern Army in order to preserve unity of forces and stability."

The Second Congress of Soviets consisted of 670 elected delegates; 300 were Bolshevik and nearly a hundred were Left Socialist-Revolutionaries, who also supported the overthrow of the Alexander Kerensky Government. When the fall of the Winter Palace was announced, the Congress adopted a decree transferring power to the Soviets of Workers', Soldiers' and Peasants' Deputies, thus ratifying the Revolution.

The transfer of power was not without disagreement. The center and Right wings of the Socialist Revolutionaries as well as the Mensheviks believed that Lenin and the Bolsheviks had illegally seized power and they walked out before the resolution was passed. As they exited, they were taunted by Leon Trotsky who told them "You are pitiful isolated individuals; you are bankrupts; your role is played out. Go where you belong from now on — into the dustbin of history!"

The following day, 26 October, the Congress elected a new cabinet of Bolsheviks, pending the convocation of a Constituent Assembly. This basis for the new Soviet government was known as the Council (Soviet) of People's Commissars (Sovnarkom), with Lenin as leader. Lenin allegedly approved of the name, reporting that it "smells of revolution". The cabinet quickly passed the Decree on Peace and the Decree on Land. This new government was also officially called "provisional" until the Assembly was dissolved. Posters were pinned on walls and fences by the Right Socialist Revolutionaries, describing the takeover as a "crime against the motherland and revolution".

On 27 October 1917 (9 November new style), the Mensheviks seized power in Georgia and declared it an independent republic. The Don Cossacks also claimed control of their own government. The biggest Bolshevik strongholds were in the cities, particularly Petrograd, with support much more mixed in rural areas. The peasant dominated Left SR Party was in coalition with the Bolsheviks. There are reports that the Provisional Government had not conceded defeat and are meeting with the army at the Front.

On 28 October 1917,(10 November new style) some posters and newspapers started criticizing the actions of the Bolsheviks and refuted their authority. The Executive Committee of Peasants Soviets "[refuted] with indignation all participation of the organised peasantry in this criminal violation of the will of the working class".
On 29 October 1917, opposition to the Bolsheviks developed into major counter-revolutionary action. Cossacks entered Tsarskoye Selo on outskirts of Petrograd with Kerensky riding on a white horse welcomed by church bells. Kerensky gave an ultimatum to the rifle garrison to lay down weapons, which was promptly refused. They were then fired upon by Kerensky's Cossacks, which resulted in 8 deaths. This turned soldiers in Petrograd against Kerensky because he was just like the Tsarist regime. Kerensky's failure to assume authority over troops was described by John Reed as a 'fatal blunder' that signalled the final death of the government.

On 30 October 1917 (12 November new style), the battle against the anti-Bolsheviks continued. The Red Guard fought against Cossacks at Tsarskoye Selo, with the Cossacks breaking rank and fleeing, leaving their artillery behind.

On 31 October 1917 (13 November new style), the Bolsheviks gained control of Moscow after a week of bitter street-fighting. Artillery had been freely used with an estimated 700 casualties. However, there was still continued support for Kerensky in some of the provinces.

On 1 November 1917 (14 November new style), there was an appeal to anti-Bolsheviks throughout Russia to join the new government of the people, with the Bolsheviks winning even more support from the Russian people.

On 2 November 1917 (15 November new style), there was only minor public anti-Bolshevik sentiment; for example, the newspaper Novaya Zhizn criticised the lack of manpower and organisation of the Bolsheviks to run a party, let alone a government. Lenin confidently claimed that there is "not a shadow of hesitation in the masses of Petrograd, Moscow and the rest of Russia" towards Bolshevik rule.

On 10 November 1917 (23 November new style), the government sought to label its citizens as "citizens of the Russian Republic," and make them equal in all possible respects. This was accomplished by the nullification of all "legal designations of civil equality, such as estates, titles, and ranks.

On 12 November (25 November new style), a Constituent Assembly was elected. In these elections, 26 mandatory delegates were proposed by the Bolshevik Central Committee and 58 were proposed by the Socialist Revolutionaries. Of these mandatory candidates, only one Bolshevik and seven Socialist Revolutionary delegates were women. The outcome of the election gave the majority to the Socialist Revolutionary Party, which no longer existed as a full party by that time, as the Left SR Party was in coalition with the Bolsheviks. The Bolsheviks dissolved the Constituent Assembly in January 1918, when it came into conflict with the Soviets.

On 16 December 1917 (29 December 1917 new style), the government ventured to eliminate hierarchy in the army, removing all titles, ranks, and uniform decorations. The tradition of saluting was also eliminated.

On 20 December 1917 (2 January 1918 new style), the Cheka was created by the decree of Vladimir Lenin. These were the beginnings of the Bolsheviks' consolidation of power over their political opponents. The Red Terror was started in September 1918, following a failed assassination attempt on Lenin's life. The Jacobin Terror was an example for the Soviet Bolsheviks. Leon Trotsky had compared Lenin to Maximilien Robespierre as early as 1904.

The Decree on Land ratified the actions of the peasants who throughout Russia gained private land and redistributed it among themselves. The Bolsheviks viewed themselves as representing an alliance of workers and peasants and memorialized that understanding with the Hammer and Sickle on the flag and coat of arms of the Soviet Union.
Other decrees:

Bolshevik-led attempts to gain power in other parts of the Russian Empire were largely successful in Russia proper — although the fighting in Moscow lasted for two weeks — but they were less successful in ethnically non-Russian parts of the Empire, which had been clamoring for independence since the February Revolution. For example, the Ukrainian Rada, which had declared autonomy on 23 June 1917, created the Ukrainian People's Republic on 20 November, which was supported by the Ukrainian Congress of Soviets. This led to an armed conflict with the Bolshevik government in Petrograd and, eventually, a Ukrainian declaration of independence from Russia on 25 January 1918. In Estonia, two rival governments emerged: the Estonian Provincial Assembly, established in April 1917, proclaimed itself the supreme legal authority of Estonia on 28 November 1917 and issued the Declaration of Independence on 24 February 1918. Soviet Russia recognized the Executive Committee of the Soviets of Estonia as the legal authority in the province, although the Soviets in Estonia controlled only the capital and a few other major towns.
The success of the October Revolution transformed the Russian state into a soviet republic. A coalition of anti-Bolshevik groups attempted to unseat the new government in the Russian Civil War from 1918 to 1922.

In an attempt to intervene in the civil war after the Bolsheviks' separate peace with the Central Powers, the Allied powers (United Kingdom, France, Italy, United States and Japan) occupied parts of the Soviet Union for over two years before finally withdrawing. The United States did not recognize the new Russian government until 1933. The European powers recognized the Soviet Union in the early 1920s and began to engage in business with it after the New Economic Policy (NEP) was implemented.

Few events in historical research have been as conditioned by political influences as the October Revolution. The historiography of the Revolution generally divides into three camps: the Soviet-Marxist view, the Western-Totalitarian view, and the Revisionist view.

Soviet historiography of the October Revolution is intertwined with Soviet historical development. Many of the initial Soviet interpreters of the Revolution were themselves Bolshevik revolutionaries. After the initial wave of revolutionary narratives, Soviet historians worked within "narrow guidelines" defined by the Soviet government. The rigidity of interpretive possibilities reached its height under Joseph Stalin.

Soviet historians of the October Revolution interpreted the Revolution with regard to establishing the legitimacy of Marxist ideology, and also the Bolshevik government. To establish the accuracy of Marxist ideology, Soviet historians generally described the Revolution as the product of class struggle. They maintained that the Revolution was the supreme event in a world history governed by historical laws. The Bolshevik Party is placed at the center of the Revolution, exposing the errors of both the moderate Provisional Government and the spurious "socialist" Mensheviks in the Petrograd Soviet. Guided by Vladimir Lenin's leadership and his firm grasp of scientific Marxist theory, the Party led the "logically predetermined" events of the October Revolution from beginning to end. The events were, according to these historians, logically predetermined because of the socio-economic development of Russia, where the monopoly industrial capitalism alienated the masses. In this view, the Bolshevik party took the leading role in organizing these alienated industrial workers, and thereby established the construction of the first socialist state.

Although Soviet historiography of the October Revolution stayed relatively constant until 1991, it did undergo some changes. Following Stalin's death, historians such as E. N. Burdzhalov and P. V. Volobuev published historical research that deviated significantly from the party line in refining the doctrine that the Bolshevik victory "was predetermined by the state of Russia's socio-economic development". These historians, who constituted the "New Directions Group", posited that the complex nature of the October Revolution "could only be explained by a multi-causal analysis, not by recourse to the mono-causality of monopoly capitalism". For them, the central actor is still the Bolshevik party, but this party triumphed "because it alone could solve the preponderance of 'general democratic' tasks the country faced" (such as the struggle for peace, the exploitation of landlords, and so on.)

Following the turn of the 21st century, some Soviet historians began to implement an "anthropological turn" in their historiographical analysis of the Russian Revolution. This method of analysis focuses on the average person's experience of day-to-day life during the revolution, and pulls the analytical focus away from larger events, notable revolutionaries, and overarching claims about party views. In 2006, S. V. Iarov employed this methodology when he focused on citizen adjustment to the new Soviet system. Iarov explored the dwindling labor protests, evolving forms of debate, and varying forms of politicization as a result of the new Soviet rule from 1917 to 1920. In 2010, O. S. Nagornaia took interest in the personal experiences of Russian prisoners of war taken by Germany, examining Russian soldiers and officers' ability to cooperate and implement varying degrees of autocracy despite being divided by class, political views and race. Other analyses following this "anthropological turn" have explored texts from soldiers and how they used personal war experiences to further their political goals, as well as how individual life-structure and psychology may have shaped major decisions in the civil war that followed the revolution.

During the late Soviet period, the opening of select Soviet archives during glasnost sparked innovative research that broke away from some aspects of Marxism–Leninism, though the key features of the orthodox Soviet view remained intact.

During the Cold War, Western historiography of the October Revolution developed in direct response to the assertions of the Soviet view. The Soviet version of the October Revolution conditioned historical interpretations in the United States and the West. As a result, these Western historians exposed what they believed were flaws in the Soviet view, thereby undermining the Bolsheviks' original legitimacy, as well as the precepts of Marxism.

These Western historians described the revolution as the result of a chain of contingent accidents. Examples of these accidental and contingent factors they say precipitated the Revolution included World War I's timing, chance, and the poor leadership of Tsar Nicholas II as well as liberal and moderate socialists. According to Western historians, it was not popular support, but rather manipulation of the masses, ruthlessness, and the superior structure of the Bolsheviks that enabled it to survive. For these historians, the Bolsheviks' defeat in the Constituent Assembly elections of November–December 1917 demonstrated popular opposition to the Bolsheviks' coup, as did the scale and breadth of the Civil War.

Western historians saw the organization of the Bolshevik party as proto-totalitarian. Their interpretation of the October Revolution as a violent coup organized by a proto-totalitarian party reinforced to them the idea that totalitarianism was an inherent part of Soviet history. For them, Stalinist totalitarianism developed as a natural progression from Leninism and the Bolshevik party's tactics and organization.

The dissolution of the USSR affected historical interpretations of the October Revolution. Since 1991, increasing access to large amounts of Soviet archival materials made it possible to re‑examine the October Revolution. Though both Western and Russian historians now have access to many of these archives, the effect of the dissolution of the USSR can be seen most clearly in the work of historians in the former USSR. While the disintegration essentially helped solidify the Western and Revisionist views, post-USSR Russian historians largely repudiated the former Soviet historical interpretation of the Revolution. As Stephen Kotkin argues, 1991 prompted "a return to political history and the apparent resurrection of totalitarianism, the interpretive view that, in different ways…revisionists sought to bury".

The term "Red October" (Красный Октябрь, Krasnyy Oktyabr) has also been used to describe the events of the month. This name has in turn been lent to a steel factory made notable by the Battle of Stalingrad, a Moscow sweets factory that is well known in Russia, and a fictional Soviet submarine.

"Ten Days That Shook the World", a book written by American journalist John Reed and first published in 1919, gives a firsthand exposition of the events. Reed died in 1920, shortly after the book was finished.

Dmitri Shostakovich wrote his Symphony No. 2 in B major, Op. 14 and subtitled "To October", for the 10th anniversary of the October Revolution. The choral finale of the work, "To October", is set to a text by Alexander Bezymensky, which praises Lenin and the revolution. The Symphony No. 2 was first performed by the Leningrad Philharmonic Orchestra and the Academy Capella Choir under the direction of Nikolai Malko, on 5 November 1927.

Sergei Eisenstein and Grigori Aleksandrov's film "", first released on 20 January 1928 in the USSR and on 2 November 1928 in New York City, describes and glorifies the revolution and was commissioned to commemorate the event.

7 November, the anniversary of the October Revolution, was the official national day of the Soviet Union from 1918 onward and still is a public holiday in Belarus and the breakaway territory of Transnistria.

The October revolution of 1917 also marks the inception of the first communist government in Russia, and thus the first large-scale socialist state in world history. After this Russia became the Russian SFSR and later part of the USSR, which dissolved in late 1991.




</doc>
<doc id="22665" url="https://en.wikipedia.org/wiki?curid=22665" title="Opole Voivodeship">
Opole Voivodeship

Opole Voivodeship, or Opole Province ( , ), is the smallest and least populated voivodeship (province) of Poland. The province's name derives from that of the region's capital and largest city, Opole. It is part of Upper Silesia. A relatively large German minority lives in the voivodeship, with representatives in the Sejm.

Opole Voivodeship is bordered by Lower Silesian Voivodeship to the west, Greater Poland and Łódź Voivodeships to the north, Silesian Voivodeship to the east, and the Czech Republic to the south.

Opole Province's geographic location, economic potential, and its population's level of education make it an attractive business partner for other Polish regions (especially Lower Silesian and Silesian Voivodeships) and for foreign investors. Formed in 1997, the Praděd/Pradziad Euroregion has facilitated economic, cultural and tourist exchanges between the border areas of Poland and the Czech Republic.

Opole Voivodeship was created on January 1, 1999, out of the former Opole Voivodeship and parts of Częstochowa Voivodeship, pursuant to the Polish local government reforms adopted in 1998.
Originally, the government, advised by prominent historians, had wanted to disestablish Opolskie and partition its territory between the more historically Polish regions of Lower Silesia and Silesian Voivodeship (eastern Upper Silesia and western Malopolska. The plan was that Brzeg and Namysłów, as the Western part of the region, were to be transferred to Lower Silesia, while the rest was to become, along with a part of the Częstochowa Voivodeship, an integral part of the new 'Silesian' region. However, the plans resulted in an outcry from the German minority population of Opole Voivodeship, who feared that should their region be abolished, they would lose all hope of regional representation (in the proposed Silesian Region, they would have formed a very small minority among a great number of ethnic Poles). To the surprise of many of the ethnic Germans in Opole however, the local Polish Silesian population and groups of ethnic Poles also rose up to oppose the planned reforms; this came about as a result of an overwhelming feeling of attachment to the voivodeships that were scheduled to be 'redrawn', as well as a fear of 'alienation' should one find themselves residing in a new, unfamiliar region.

The solution came in late 1999, when Olesno was, after 24 years apart, finally reunited with the Opole Voivodeship to form the new legally defined region. A historic moment came in 2006 when the town of Radłów changed its local laws to make German, alongside Polish, the district's second official language; thus becoming the first town in the region to achieve such a feat.

The voivodeship lies in southwestern Poland, the major part on the Silesian Lowland (). To the east, the region touches upon the Silesian Upland (Silesian Uplands, ) with the famous Saint Anne Mountain; the Sudetes range, the Opawskie Mountains, lies to the southwest. The Oder River cuts across the middle of the voivodeship. The northern part of the voivodeship, along the Mała Panew River, is densely forested, while the southern part consists of arable land.

The region has the warmest climate in the country.

Protected areas in Opole Voivodeship include the following three areas designated as Landscape Parks:

Opole Voivodeship is divided into 12 counties (powiats): 1 city county and 11 land counties. These are further divided into 71 gminas.

The counties are listed in the following table (ordering is by decreasing population).

The voivodeship contains 35 cities and towns. These are listed below in descending order of population (according to official figures for 2006):

The Opole Voivodeship is the smallest region in the administrative makeup of the country in terms of both area and population. 

About 15% of the one million inhabitants of this voivodeship are ethnic Germans, which constitutes 90% of all ethnic Germans in Poland. As a result, many areas are officially bilingual in Opolskie, and the German language and culture play a significant role in education in the region. 

The Opole Voivodeship is an industrial as well as an agricultural region. With respect to mineral resources, of major importance are deposits of raw materials for building: limestone (Strzelce Opolskie), marl (near Opole), marble, and basalt. The favourable climate, fertile soils, and high farming culture contribute to the development of agriculture, which is among the most productive in the country.

A total of nineteen industries are represented in the voivodeship. The most important are cement and lime, furniture, food, car manufacturing, and chemical industries. In 1997, the biggest production growth in the area was in companies producing wood and wood products, electrical equipment, machinery and appliances, as well as cellulose and paper products. In 1997, the top company in the region was Zakłady Azotowe S.A. in Kędzierzyn-Koźle, whose income was over PLN 860 million. The voivodship's economy consists of more than 53,000 businesses, mostly small and medium-sized, employing over 332,000 people. Manufacturing companies employ over 89,000 people; 95.7% of all the region's business operate in the private sector.

The Opole Voivodeship is a green region with three large lakes: Turawskie, Nyskie, and Otmuchów (the latter two are connected). The Opawskie Mountains are extremely popular. The region also includes the castle in Brzeg, built during the reign of the Piast dynasty—pearl of the Silesian Renaissance, the Franciscan monastery on top of Saint Anne Mountain, as well as the medieval defence fortifications in Paczków (referred to as the Upper Silesian Carcassonne).

According to the Central Statistical Office of Poland, Opole Voivodeship is most frequently visited by international tourists from countries located in Europe (94.6%). The rank was followed by tourists from Asia, compromising 2.4% of the total international tourist figure, followed by that of North America at 1.8%. The general composition of international tourists visiting the Opole Voivodeship remains unchanged, with 46.2% of tourists heading from Germany. 

International tourists visiting Opole Voivodeship with an overnight stay according to country of permanent residence:

In 2015, a total of c. 90,800 overnight stays were hosted for international tourists, a figure making up 12.4% of the total amount of ovenight stays for Opole Voivodeship. The majority (44.7%) of international overnight stays were hosted in the city of Opole, followed by Kędzierzyn-Koźle County (9.9%) and Nysa County at (9.4%).

The transport route from Germany to Ukraine, the A4, runs through Opole. The region has four border crossings, and direct rail connections to all important Polish cities, as well as to Frankfurt, Munich, Budapest, Kiev, and the Baltic ports.

There are three state-run universities in the region: the Opole University, the Opole University of Technology, and the State Medical College. All of them are based in the voivodeship's capital. Among the region's private schools, the Opole School of Management and Administration has been certified as a degree-granting institution by the Ministry of National Education.

Most popular surnames in Opole Voivodeship:

Opole Voivodeship was also a unit of administrative division and local government in Poland between 1975 and 1998.

Major cities and towns (population in 1995):

This administrative region of the People's Republic of Poland (1950–1975) was created as a result of the partition of Katowice Voivodeship in 1950.




</doc>
<doc id="22666" url="https://en.wikipedia.org/wiki?curid=22666" title="Old Norse">
Old Norse

Old Norse was a North Germanic language that was spoken by inhabitants of Scandinavia and inhabitants of their overseas settlements from about the 9th to the 13th century.

The Proto-Norse language developed into Old Norse by the 8th century, and Old Norse began to develop into the modern North Germanic languages in the mid- to late 14th century, ending the language phase known as Old Norse. These dates, however, are not absolute, since written Old Norse is found well into the 15th century.

Old Norse was divided into three dialects: Old West Norse, Old East Norse, and Old Gutnish. Old West and East Norse formed a dialect continuum, with no clear geographical boundary between them. For example, Old East Norse traits were found in eastern Norway, although Old Norwegian is classified as Old West Norse, and Old West Norse traits were found in western Sweden. Most speakers spoke Old East Norse in what is present day Denmark and Sweden. Old Gutnish, the more obscure dialectal branch, is sometimes included in the Old East Norse dialect due to geographical associations. It developed its own unique features and shared in changes to both other branches.

The 12th century Icelandic "Gray Goose Laws" state that Swedes, Norwegians, Icelanders, and Danes spoke the same language, "dönsk tunga" ("Danish tongue"; speakers of Old East Norse would have said "dansk tunga"). Another term, used especially commonly with reference to West Norse, was "norrænt mál" ("Nordic/Northern speech"). Today Old Norse has developed into the modern North Germanic languages Icelandic, Faroese, Norwegian, Danish, and Swedish, of which Norwegian, Danish and Swedish retain considerable mutual intelligibility.

Old Icelandic was very close to Old Norwegian, and together they formed the Old West Norse dialect, which was also spoken in settlements in Ireland, Scotland, the Isle of Man and northwest England, and in Norse settlements in Normandy. The Old East Norse dialect was spoken in Denmark, Sweden, settlements in Kievan Rus', eastern England, and Danish settlements in Normandy. The Old Gutnish dialect was spoken in Gotland and in various settlements in the East. In the 11th century, Old Norse was the most widely spoken European language, ranging from Vinland in the West to the Volga River in the East. In Kievan Rus', it survived the longest in Veliky Novgorod, probably lasting into the 13th century there. The age of the Swedish-speaking population of Finland is strongly contested, but at latest by the time of the Second Swedish Crusade in the 13th century, Swedish settlement had spread the language into the region.

The modern descendants of the Old West Norse dialect are the West Scandinavian languages of Icelandic, Faroese, Norwegian and the extinct Norn language of Orkney and Shetland; the descendants of the Old East Norse dialect are the East Scandinavian languages of Danish and Swedish. Norwegian is descended from Old West Norse, but over the centuries it has been heavily influenced by East Norse, particularly during the Denmark–Norway union.

Among these, Icelandic and the closely related Faroese have changed the least from Old Norse in the last thousand years, although with Danish rule of the Faroe Islands, Faroese has also been influenced by Danish. Old Norse also had an influence on English dialects and Lowland Scots, which contain many Old Norse loanwords. It also influenced the development of the Norman language, and through it and to a smaller extent, that of modern French.

Various other languages, which are not closely related, have been heavily influenced by Norse, particularly the Norman dialects, Scottish Gaelic and Irish. Russian, Ukrainian, Belarusian, Lithuanian, Finnish, Latvian and Estonian also have a number of Norse loanwords; the words "Rus" and "Russia", according to one theory, may be named after the Rus' people, a Norse tribe; "see Rus (name)", probably from present-day east-central Sweden. The current Finnish and Estonian words for Sweden are "Ruotsi" and "Rootsi", respectively. 

Of the modern languages, Icelandic is the closest to Old Norse. Written modern Icelandic derives from the Old Norse phonemic writing system. Contemporary Icelandic-speakers can read Old Norse, which varies slightly in spelling as well as semantics and word order. However, pronunciation, particularly of the vowel phonemes, has changed at least as much as in the other North Germanic languages.

Faroese retains many similarities but is influenced by Danish, Norwegian, and Gaelic (Scottish and/or Irish). Although Swedish, Danish and the Norwegian languages have diverged the most, they still retain asymmetric mutual intelligibility. Speakers of modern Swedish, Norwegian and Danish can mostly understand each other without studying their neighboring languages, particularly if speaking slowly. The languages are also sufficiently similar in writing that they can mostly be understood across borders. This could be because these languages have been mutually affected by each other, as well as having a similar development influenced by Middle Low German.

The vowel phonemes mostly come in pairs of long and short. The standardized orthography marks the long vowels with an acute accent. In medieval manuscripts, it is often unmarked but sometimes marked with an accent or through gemination. All phonemes have, more or less, the expected phonetic realization.

Old Norse has had nasalized versions of all nine vowel places. These occurred as allophones of the vowels before nasal consonants and in places where a nasal had followed it in an older form of the word, before it was absorbed into a neighboring sound. If the nasal was absorbed by a stressed vowel, it would also lengthen the vowel. These nasalizations also occurred in the other Germanic languages, but were not retained long. They were noted in the First Grammatical Treatise, and otherwise might have remained unknown. The First Grammarian marked these with a dot above the letter. This notation did not catch on, and would soon be obsolete. Nasal and oral vowels probably merged around the 11th century in most of Old East Norse. However, the distinction still holds in Dalecarlian dialects. The dots in the following vowel table separate the oral from nasal phonemes.

Note: The open or open-mid vowels may be transcribed differently:

Sometime around the 13th century, (spelled ǫ) merged with or in all dialects except Old Danish. In Icelandic, all (ǫ) merged with . This can be determined by their distinction within the 12th-century First Grammatical Treatise but not within the early 13th-century Prose Edda. The nasal vowels, also noted in the First Grammatical Treatise, are assumed to have been lost in most dialects by this time (but notably they are retained in Elfdalian). See Old Icelandic for the mergers of (spelled œ) with (spelled æ) and (spelled ę) with (e).

Old Norse has six plosive phonemes. Of these is rare word-initially and and are realized as voiced fricative allophones between vowels, except in compound words (e.g. veðrabati), already in the Proto-Germanic language (e.g. "*b" > [v] between vowels). The phoneme is realized as after an "n" or another "g" and as before and . It is realized as a voiced velar fricative , by some accounts inside words, and by others between vowels (and otherwise as ). The Old East Norse /ʀ/ was an apical consonant whose position isn't precisely known, being reconstructed as a palatal sibilant. It descended from Proto-Germanic and eventually developed into , as it already had done in Old West Norse.

The consonant digraphs "hl", "hr", "hn" occurred word-initially. It is unclear whether they were sequences of two consonants (with the first element realised as or perhaps ), or as single voiceless sonorants , and respectively. In Old Norwegian, Old Danish and later Old Swedish the groups "hl", "hr", "hn" were reduced to plain "l", "r", "n", suggesting that they were most likely realised as voiceless sonorants by Old Norse times.

The pronunciation of "hv" is unclear, and may have been (the Proto-Germanic pronunciation), or . Unlike the other three groups above, it was retained much longer in all dialects, and never developed into a voiceless sonorant in Icelandic, but instead "hardened" to a plosive . This suggests that it was not a voiceless sonorant, but retained stronger frication.

Unlike Proto-Norse, which was written with the Elder Futhark, runic Old Norse was originally written with the Younger Futhark, which only had 16 letters. Because of the limited number of runes, several runes were used for different sounds, and the distinction between long and short vowels wasn't retained in writing. Medieval runes came into use some time later.

As for the Latin alphabet, there was no standardized orthography in use in the Middle Ages. A modified version of the letter wynn called vend was used briefly for the sounds , , and . Long vowels were sometimes marked with acutes, but also sometimes left unmarked or geminated. The standardized Old Norse spelling was created in the 19th century, and is for the most part phonemic. The most notable deviation is that the non-phonemic difference between the voiced and the voiceless dental fricative is marked—the oldest texts as well as runic inscriptions use "þ" exclusively. Long vowels are denoted with acutes. Most other letters are written with the same glyph as the IPA phoneme, except as shown in the table below.

Primary stress in Old Norse falls on the word stem, so that "hyrjar" would be pronounced . In compound words, secondary stress falls on the second stem (e.g. "lærisveinn", ).

Ablaut patterns are groups of vowels which are swapped, or ablauted, in the nucleus of a word. Strong verbs ablaut the lemma's nucleus to derive the past forms of the verb. This parallels English conjugation, where, e.g., the nucleus of "sing" becomes "sang" in the past tense and "sung" in the past participle. Some verbs are derived by ablaut, as the present-in-past verbs do by consequence of being derived from the past tense forms of strong verbs.

Umlaut or mutation is an assimilatory process acting on vowels preceding a vowel or semivowel of a different vowel backness. In the case of i-umlaut and ʀ-umlaut, this entails a fronting of back vowels, with retention of lip rounding. In the case of u-umlaut, this entails labialization of unrounded vowels. Umlaut is phonemic and in many situations grammatically significant as a side effect of losing the Proto-Germanic morphological suffixes whose vowels created the umlaut allophones.

Some , , , , , , , and all were obtained by i-umlaut from , , , , , , , and respectively. Others were formed via ʀ-umlaut from , , , , and .

Some , , , , and all , were obtained by u-umlaut from , , , , and , respectively. See Old Icelandic for information on .

OEN often preserves the original value of the vowel directly preceding runic "ʀ" while OWN receives ʀ-umlaut. Compare runic OEN "glaʀ, haʀi, hrauʀ" with OWN "gler, heri" (later "héri"), "hrøyrr/hreyrr" ("glass", "hare", "pile of rocks").

U-umlaut is more common in Old West Norse in both phonemic and allophonic positions, while it only occurs sparsely in post-runic Old East Norse and even in runic Old East Norse. Compare West Old Norse "fǫður" (accusative of "faðir", 'father'), "vǫrðr" (guardian/caretaker), "ǫrn" (eagle), "jǫrð" ('earth', Modern Icelandic: "jörð"), "mjǫlk" ('milk', Modern Icelandic: "mjólk") with Old Swedish "faður", "varðer", "örn", "jorð" and Modern Swedish "örn", "jord", "mjölk" with the latter two demonstrating the u-umlaut found in Swedish.

This is still a major difference between Swedish and Faroese and Icelandic today. Plurals of neuters do not have u-umlaut at all in Swedish, but in Faroese and Icelandic they do, for example the Faroese and Icelandic plurals of the word "land", "lond" and "lönd" respectively, in contrast to the Swedish plural "land" and numerous other examples. That also applies to almost all feminine nouns, for example the largest feminine noun group, the o-stem nouns (except the Swedish noun "jord" mentioned above), and even i-stem nouns and root nouns, such as Old West Norse "mǫrk" ("mörk" in Icelandic) in comparison with Modern and Old Swedish "mark".

Vowel breaking, or fracture, caused a front vowel to be split into a semivowel-vowel sequence before a back vowel in the following syllable. While West Norse only broke "e", East Norse also broke "i". The change was blocked by a "v", "l", or "r" preceding the potentially-broken vowel.

Some or and or result from breaking of and respectively.

When a noun, pronoun, adjective, or verb has a long vowel or diphthong in the accented syllable and its stem ends in a single "l", "n", or "s", the "r" (or the elder "r"- or "z"-variant "ʀ") in an ending is assimilated. When the accented vowel is short, the ending is dropped.

The nominative of the strong masculine declension and some i-stem feminine nouns uses one such -r (ʀ). "Óðin"-"r" ("Óðin"-"ʀ") becomes "Óðinn" instead of "*Óðinr" ("*Óðinʀ"), but "karl"-"r" ("karl"-"ʀ") remains "karl".

"Blása", to blow, has "blæss" for "you blow" instead of "*blæsr" ("*blæsʀ").

The rule is not hard and fast, with counter-examples such as "vinr", which has the synonym "vin", yet retains the unabsorbed version, and "jǫtunn", where assimilation takes place even though the root vowel, "ǫ", is short.

Words with a final "r" in the word stem, such as "vetr", do not add another -r, as the sounds are already the same. The effect of the dropping usually results in the lack of distinction between some forms of the noun. In the case of "vetr" the dropping renders the nominative and accusative singular and plural identical; the nominative singular and nominative and accusative plural would otherwise have been "*vetrr" ("*vintrʀ"), while the accusative singular would still have been "vetr". This is because the 3rd strong masculine declension, to which it belongs, marks the nominative singular and nominative and accusative plural, but not the accusative singular, with inflectional "ʀ"s.

"I/j" adjacent to "i", "e", their u-umlauts, and "æ" was not possible, nor "u/v" adjacent to "u", "o", their i-umlauts, and "ǫ". At the beginning of words, this manifested as a dropping of the initial "j" or "v". Compare ON "orð, úlfr, ár" with English "word, wolf, year". In inflections, this manifested as the dropping of the inflectional vowels. Thus, "klæði" + "-i" remains "klæði", and "sjáum" in Icelandic progressed to "sjǫ́um" > "sjǫ́m" > "sjám". The "jj" and "ww" of Proto-Germanic became "ggj" and "ggv" respectively in Old Norse, a change known as Holtzmann's law.

An epenthetic vowel became popular by 1200 in Old Danish, 1250 in Old Swedish and Norwegian, and 1300 in Old Icelandic. An unstressed vowel was used which varied by dialect. Old Norwegian exhibited all three: /u/ was used in West Norwegian south of Bergen, as in "aftur", "aftor" (older "aptr"); North of Bergen, /i/ appeared in "aftir", "after"; and East Norwegian used /a/, "after", "aftær".

Old Norse was a moderately inflected language with high levels of nominal and verbal inflection. Most of the fused morphemes are retained in modern Icelandic, especially in regard to noun case declensions, whereas modern Norwegian in comparison has moved towards more analytical word structures.

Old Norse had three grammatical genders – masculine, feminine and neuter. Adjectives or pronouns referring to a noun must mirror the gender of that noun, so that one says, "heill maðr!" but, "heilt barn!" As in other languages, the grammatical gender of an impersonal noun is generally unrelated to an expected natural gender of that noun. While indeed "karl", "man" is masculine, "kona", "woman", is feminine, and "hús", house, is neuter, so also are "hrafn" and "kráka", for "raven" and "crow", masculine and feminine respectively, even in reference to a female raven or a male crow.

All neuter words have identical nominative and accusative forms, and all feminine words have identical nominative and accusative plurals.

The gender of some words' plurals does not agree with that of their singulars, such as "lim" and "mund". Some words, such as "hungr", have multiple genders, evidenced by their determiners being declined in different genders within a given sentence.

Old Norse inherited the Proto-Germanic feature of having neuter as the default gender. This means that when the gender of a noun is unknown, adjectives and pronouns referencing it use the neuter gender forms, rather than the masculine or feminine. Thus, if speaking or writing to a general audience, one would say "velkomit", "well is it come," rather than "velkominn" or "velkomin", "well is [he or she] come," as one does not know whether the person hearing it is going to be male or female.

One generally sees adjectives in their neuter form when used pronominally for this reason. For words more commonly used in this way (rather than to describe a noun) one sees their neuter forms more often than their masculine or feminine. Normally the masculine form would be the most beneficial form of an adjective to learn first, given that the majority of nouns are masculine. In these cases, however, the most practical form to learn first would be the neuter.

Nouns, adjectives and pronouns were declined in four grammatical cases—nominative, accusative, genitive and dative, in singular and plural numbers. Adjectives and pronouns were additionally declined in three grammatical genders. Some pronouns (first and second person) could have dual number in addition to singular and plural. The genitive is used partitively, and quite often in compounds and kennings (e.g.: "Urðarbrunnr", the well of Urðr; "Lokasenna", the gibing of Loki).

There were several classes of nouns within each gender, the following is an example of the "strong" inflectional paradigms:

In addition to these examples there were the numerous "weak" noun paradigms, which had a much higher degree of syncretism between the different cases in its paradigms, i.e. they had fewer forms than the "strong" nouns.

A definite article was realised as a suffix, that retained an independent declension e.g. troll ("a troll") – trollit ("the troll"), hǫll (" a hall") – hǫllin ("the hall"), armr ("an arm") – armrinn ("the arm"). This definite article, however, was a separate word, and did not become attached to the noun before later stages of the Old Norse period.

The earliest inscriptions in Old Norse are runic, from the 8th century. Runes continued to be commonly used until the 15th century and have been recorded to be in use in some form as late as the 19th century in some parts of Sweden. With the conversion to Christianity in the 11th century came the Latin alphabet. The oldest preserved texts in Old Norse in the Latin alphabet date from the middle of the 12th century. Subsequently, Old Norse became the vehicle of a large and varied body of vernacular literature, unique in medieval Europe. Most of the surviving literature was written in Iceland. Best known are the Norse sagas, the Icelanders' sagas and the mythological literature, but there also survives a large body of religious literature, translations into Old Norse of courtly romances, classical mythology, and the Old Testament, as well as instructional material, grammatical treatises and a large body of letters and official documents.

Most of the innovations that appeared in Old Norse spread evenly through the Old Norse area. As a result, the dialects were very similar and considered to be the same language, a language that they sometimes called the Danish tongue ("Dǫnsk tunga"), sometimes Norse language ("Norrœnt mál"), as evidenced in the following two quotes from Heimskringla by Snorri Sturluson:

However, some changes were geographically limited and so created a dialectal difference between Old West Norse and Old East Norse.

As Proto-Norse evolved into Old Norse, in the 8th century, the effects of the umlauts seem to have been very much the same over the whole Old Norse area. But in later dialects of the language a split occurred mainly between west and east as the use of umlauts began to vary. The typical umlauts (for example "fylla" from *"fullijan") were better preserved in the West due to later generalizations in the east where many instances of umlaut were removed (many archaic Eastern texts as well as eastern runic inscriptions however portray the same extent of umlauts as in later Western Old Norse).

All the while, the changes resulting in breaking (for example "hiarta" from *"hertō") were more influential in the East probably once again due to generalizations within the inflectional system. This difference was one of the greatest reasons behind the dialectalization that took place in the 9th and 10th centuries, shaping an Old West Norse dialect in Norway and the Atlantic settlements and an Old East Norse dialect in Denmark and Sweden.

Old West Norse and Old Gutnish did not take part in the monophthongization which changed "æi" ("ei") into "ē", "øy" ("ey") and "au" into "ø̄", nor did certain peripheral dialects of Swedish, as seen in modern Ostrobothnian. Another difference was that Old West Norse lost certain combinations of consonants. The combinations -"mp"-, -"nt"-, and -"nk"- were assimilated into -"pp"-, -"tt"- and -"kk"- in Old West Norse, but this phenomenon was limited in Old East Norse.

Here is a comparison between the two dialects as well as Old Gutnish. It is a transcription from one of the Funbo Runestones (U 990) meaning : "Veðr and Thane and Gunnar raised this stone after Haursi, their father. God help his spirit":

The OEN original text above is transliterated according to traditional scholarly methods, wherein u-umlaut is not regarded in runic Old East Norse. Modern studies have shown that the positions where it applies are the same as for runic Old West Norse. An alternative and probably more accurate transliteration would therefore render the text in OEN as such:
Some past participles and other words underwent i-umlaut in Old West Norse but not in Old East Norse dialects. Examples of that are Icelandic slegið/sleginn and tekið/tekinn, which in Swedish are slagit/slagen and tagit/tagen. This can also be seen in the Icelandic and Norwegian words sterkur and sterk ("strong"), which in Swedish is stark as in Old Swedish. These differences can also be seen in comparison between Norwegian and Swedish.

The combinations -mp-, -nt-, and -nk- mostly merged to -pp-, -tt- and -kk- in Old West Norse at around the 7th century, marking the first distinction between the Eastern and Western dialects. The following table illustrates this:

An early difference between Old West Norse and the other dialects was that Old West Norse had the forms "bú" "dwelling", "kú" "cow" (accusative) and "trú" "faith" whereas Old East Norse had "bó", "kó" and "tró". Old West Norse was also characterized by the preservation of "u"-umlaut, which meant that for example Proto-Norse *"tanþu" "tooth" was pronounced "tǫnn" and not "tann" as in post-runic Old East Norse; OWN "gǫ́s" and runic OEN "gǫ́s", while post-runic OEN "gás" "goose".

The earliest body of text appears in runic inscriptions and in poems composed c. 900 by Þjóðólfr of Hvinir (although the poems are not preserved in contemporary sources, but only in much later manuscripts). The earliest manuscripts are from the period 1150–1200 and concern both legal, religious and historical matters. During the 12th and 13th centuries, Trøndelag and Western Norway were the most important areas of the Norwegian kingdom and they shaped Old West Norse as an archaic language with a rich set of declensions. In the body of text that has come down to us from until c. 1300, Old West Norse had little dialect variation, and Old Icelandic does not diverge much more than the Old Norwegian dialects do from each other.

Old Norwegian differentiated early from Old Icelandic by the loss of the consonant "h" in initial position before "l", "n" and "r", thus whereas Old Icelandic manuscripts might use the form "hnefi" "fist", Old Norwegian manuscripts might use "nefi".

From the late 13th century, Old Icelandic and Old Norwegian started to diverge more. After c. 1350, the Black Death and following social upheavals seem to have accelerated language changes in Norway. From the late 14th century, the language used in Norway is generally referred to as Middle Norwegian.

Old West Norse underwent a lengthening of initial vowels at some point, especially in Norwegian, so that OWN "eta" became "éta," ONW "akr" > "ákr", OIC "ek" > "ék".

In Iceland, initial before was lost. Compare Icelandic "rangur" with Norwegian "vrangr", OEN "vrangʀ". This change is shared with Old Gutnish.

A specifically Icelandic sound, the long, u-umlauted A, spelled Ǫ́ and pronounced , developed circa the early 11th century. It was short-lived, being marked in the Grammatical Treatises and remaining until the end of the 12th century.

Around the 13th century, Œ/Ǿ () merged to Æ (). Thus, pre-13th-century "grœnn" 'green' became modern Icelandic "grænn". The 12th-century Gray Goose Laws manuscripts distinguish the vowels, and so the Codex Regius copy does as well. However, the 13th-century Codex Regius copy of the Poetic Edda probably relied on newer and/or poorer quality sources—demonstrating either difficulty with or total lack of natural distinction, the manuscripts show separation of the two phonemes in some places, but frequently mix up the letters chosen to distinguish them in others.

Towards the end of the 13th century, Ę () merged to E ().

Around the 11th century, Old Norwegian , , and became , , and . It is debatable whether the sequences represented a consonant cluster, , or a devoicing, .

Orthographic evidence suggests that, in a confined dialect of Old Norwegian, /ɔ/ may have been unrounded before /u/, so that u-umlaut was reversed where the "u" had not been eliminated. e.g. "ǫll", "ǫllum" > "ǫll", "allum".

This dialect of Old West Norse was spoken by Icelandic colonies in Greenland. When the colonies died out around the 15th century, the dialect went with it. The phoneme , and some merged to , so that Old Icelandic Þórðr becomes Tortr.

The following text is from "Alexanders saga", an Alexander romance. The manuscript, AM 519 a 4to, is dated c. 1280. The facsimile demonstrates the sigla used by scribes to write Old Norse. Many of these were borrowed from Latin. Without familiarity with these abbreviations, the facsimile will be unreadable to many. In addition, reading the manuscript itself requires familiarity with the letterforms of the native script. The abbreviations are expanded in a version with normalized spelling like the standard normalization system's. Comparing this to the spelling of the same text in Modern Icelandic shows that, while pronunciation has changed greatly, spelling has changed little.

<nowiki>*</nowiki> a printed in uncial. Uncials not encoded separately in Unicode as of this section's writing.

Old East Norse, between 800 and 1100, is called "Runic Swedish" in Sweden and "Runic Danish" in Denmark, but for geographical not linguistic reasons. Any differences between the two were minute at best during the more ancient stages of this dialect group. Changes had a tendency to occur earlier in the Danish region. Even today many Old Danish changes have still not taken place in modern Swedish. Swedish is therefore the more archaic of the two in both the ancient and the modern languages, sometimes by a profound margin but in general, differences are still minute. The language is called "runic" because the body of text appears in runes.

Runic Old East Norse is characteristically archaic in form, especially Swedish (which is still true for modern Swedish compared to Danish). In essence it matches or surpasses the archaicness of post-runic Old West Norse which in its turn is generally more archaic than post-runic Old East Norse. While typically "Eastern" in structure, many later post-runic changes and trademarks of EON had yet to happen.

The phoneme "ʀ", which evolved during the Proto-Norse period from "z", was still clearly separated from "r" in most positions, even when being geminated, while in OWN it had already merged with "r".

Monophthongization of "æi > ē" and "øy, au > ø̄" started in mid-10th-century Denmark. Compare runic OEN: "fæigʀ", "gæiʀʀ", "haugʀ", "møydōmʀ", "diūʀ"; with Post-runic OEN: "fēgher", "gēr", "hø̄gher", "mø̄dōmber", "diūr"; OWN: "feigr", "geirr", "haugr", "meydómr", "dýr"; from PN *faigiaz, *gaizaz, *haugaz, *mawi- + dōmaz (maidendom; virginity), *diuza ((wild) animal).

Feminine o-stems often preserve the plural ending -aʀ while in OWN they more often merge with the feminine i-stems: (runic OEN) "*sōlaʀ", "*hafnaʀ"/"*hamnaʀ", "*vāgaʀ" while OWN "sólir", "hafnir" and "vágir" (modern Swedish "solar", "hamnar", "vågar"; suns, havens, scales; Danish has mainly lost the distinction between the two stems with both endings now being rendered as -er or -e alternatively for the o-stems).

Vice versa, masculine i-stems with the root ending in either "g" or "k" tended to shift the plural ending to that of the ja-stems while OEN kept the original: "drængiaʀ", "*ælgiaʀ" and "*bænkiaʀ" while OWN "drengir", "elgir" (elks) and "bekkir" (modern Danish "drenge", "elge", "bænke", modern Swedish "drängar", "älgar", "bänkar").

The plural ending of ja-stems were mostly preserved while those of OEN often acquired that of the i-stems: "*bæðiaʀ", "*bækkiaʀ", "*væfiaʀ" while OWN "beðir" (beds), "bekkir", "vefir" (modern Swedish "bäddar", "bäckar", "vävar").

Until the early 12th century, Old East Norse was very much a uniform dialect. It was in Denmark that the first innovations appeared that would differentiate Old Danish from Old Swedish as these innovations spread north unevenly (unlike the earlier changes that spread more evenly over the East Norse area) creating a series of isoglosses going from Zealand to Svealand.

In Old Danish, merged with during the 9th century. From the 11th to 14th centuries, the unstressed vowels -"a", -"o" and -"e" (standard normalization -"a", -"u" and -"i") started to merge into -"ə", represented with the letter "e". This vowel came to be epenthetic, particularly before "-ʀ" endings. At the same time, the voiceless stop consonants "p", "t" and "k" became voiced plosives and even fricative consonants. Resulting from these innovations, Danish has "kage" (cake), "tunger" (tongues) and "gæster" (guests) whereas (Standard) Swedish has retained older forms, "kaka", "tungor" and "gäster" (OEN "kaka", "tungur", "gæstir").

Moreover, the Danish pitch accent shared with Norwegian and Swedish changed into "stød" around this time.

At the end of the 10th and early 11th century initial "h-" before "l", "n" and "r" was still preserved in the middle and northern parts of Sweden, and is sporadically still preserved in some northern dialects as "g-", e.g. "gly" (lukewarm), from "hlýʀ". The Dalecarlian dialects developed independently from Old Swedish and as such can be considered separate languages from Swedish.

This is an extract from "Västgötalagen", the Westrogothic law. It is the oldest text written as a manuscript found in Sweden and from the 13th century. It is contemporaneous with most of the Icelandic literature. The text marks the beginning of Old Swedish as a distinct dialect.
Dræpær maþar svænskan man eller smalenskæn, innan konongsrikis man, eigh væstgøskan, bøte firi atta ørtogher ok þrettan markær ok ænga ætar bot. [...] Dræpar maþær danskan man allæ noræn man, bøte niv markum. Dræpær maþær vtlænskan man, eigh ma frid flyia or landi sinu oc j æth hans. Dræpær maþær vtlænskæn prest, bøte sva mykit firi sum hærlænskan man. Præstær skal i bondalaghum væræ. Varþær suþærman dræpin ællær ænskær maþær, ta skal bøta firi marchum fiurum þem sakinæ søkir, ok tvar marchar konongi.

If someone slays a Swede or a Smålander, a man from the kingdom, but not a West Geat, he will pay eight örtugar (20-pence coins) and thirteen marks, but no weregild. [...] If someone slays a Dane or a Norwegian, he will pay nine marks. If someone slays a foreigner, he shall not be banished and have to flee to his clan. If someone slays a foreign priest, he will pay as much as for a fellow countryman. A priest counts as a freeman. If a Southerner is slain or an Englishman, he shall pay four marks to the plaintiff and two marks to the king.

Due to Gotland's early isolation from the mainland, many features of Old Norse did not spread from or to the island, and Old Gutnish developed as an entirely separate branch from Old East and West Norse. For example, the diphthong "ai" in "aigu", "þair" and "waita" was not retroactively umlauted to "ei" as in e.g. Old Icelandic "eigu", "þeir" and "veita". Breaking was especially active in Old Gutnish, leading to forms such as "bjera" and "bjauþa", mainland "bera" and "bjóða". Dropping of /w/ in initial /wɾ/ is shared only with Old Icelandic.

The Gutasaga is the longest text surviving from Old Gutnish. It was written in the 13th century and dealt with the early history of the Gotlanders. This part relates to the agreement that the Gotlanders had with the Swedish king sometime before the 9th century:
So gingu gutar sielfs wiliandi vndir suia kunung þy at þair mattin frir Oc frelsir sykia suiariki j huerium staþ. vtan tull oc allar utgiftir. So aigu oc suiar sykia gutland firir vtan cornband ellar annur forbuþ. hegnan oc hielp sculdi kunungur gutum at waita. En þair wiþr þorftin. oc kallaþin. sendimen al oc kunungr oc ierl samulaiþ a gutnal þing senda. Oc latta þar taka scatt sinn. þair sendibuþar aighu friþ lysa gutum alla steþi til sykia yfir haf sum upsala kunungi til hoyrir. Oc so þair sum þan wegin aigu hinget sykia.

So, by their own will, the Gotlanders became the subjects of the Swedish king, so that they could travel freely and without risk to any location in the Swedish kingdom without toll and other fees. Likewise, the Swedes had the right to go to Gotland without corn restrictions or other prohibitions. The king was to provide protection and help, when they needed it and asked for it. The king and the jarl shall send emissaries to the Gutnish thing to receive the taxes. These emissaries shall declare free passage for the Gotlanders to all locations in the sea of the king at Uppsala (that is the Baltic Sea was under Swedish control) and likewise for everyone who wanted to travel to Gotland.

Old English and Old Norse were related languages. It is therefore not surprising that many words in Old Norse look familiar to English speakers (e.g., "armr" (arm), "fótr" (foot), "land" (land), "fullr" (full), "hanga" (to hang), "standa" (to stand)). This is because both English and Old Norse stem from a Proto-Germanic mother language. In addition, numerous common, everyday Old Norse words were adopted into the Old English language during the Viking age. A few examples of Old Norse loanwords in modern English are (English/Viking age Old East Norse), in some cases even displacing their Old English cognates:

In a simple sentence like "They are both weak" the extent of the Old Norse loanwords becomes quite clear (Old East Norse with archaic pronunciation: "Þæiʀ eʀu báðiʀ wæikiʀ" while Old English "híe syndon bégen (þá) wáce"). The words "they" and "weak" are both borrowed from Old Norse, and the word "both" might also be a borrowing, though this is disputed (cf. German "beide"). While the number of loanwords adopted from the Norse was not as numerous as that of Norman French or Latin, their depth and everyday nature make them a substantial and very important part of every day English speech as they are part of the very core of the modern English vocabulary.

Words like "bull" and "Thursday" are more difficult when it comes to their origins. "Bull" may be from either Old English "bula" or Old Norse "buli", while "Thursday" may be a borrowing, or it could simply be from the Old English "Þunresdæg", which could have been influenced by the Old Norse cognate. The word "are" is from Old English "earun"/"aron", which stems back to Proto-Germanic as well as the Old Norse cognates.



Cleasby-Vigfússon:








</doc>
<doc id="22667" url="https://en.wikipedia.org/wiki?curid=22667" title="Old English">
Old English

Old English (), or Anglo-Saxon, is the earliest historical form of the English language, spoken in England and southern and eastern Scotland in the early Middle Ages. It was brought to Great Britain by Anglo-Saxon settlers probably in the mid-5th century, and the first Old English literary works date from the mid-7th century. After the Norman conquest of 1066, English was replaced, for a time, as the language of the upper classes by Anglo-Norman, a relative of French. This is regarded as marking the end of the Old English era, as during this period the English language was heavily influenced by Anglo-Norman, developing into a phase known now as Middle English.

Old English developed from a set of Anglo-Frisian or Ingvaeonic dialects originally spoken by Germanic tribes traditionally known as the Angles, Saxons and Jutes. As the Anglo-Saxons became dominant in England, their language replaced the languages of Roman Britain: Common Brittonic, a Celtic language, and Latin, brought to Britain by Roman invasion. Old English had four main dialects, associated with particular Anglo-Saxon kingdoms: Mercian, Northumbrian, Kentish and West Saxon. It was West Saxon that formed the basis for the literary standard of the later Old English period, although the dominant forms of Middle and Modern English would develop mainly from Mercian. The speech of eastern and northern parts of England was subject to strong Old Norse influence due to Scandinavian rule and settlement beginning in the 9th century.

Old English is one of the West Germanic languages, and its closest relatives are Old Frisian and Old Saxon. Like other old Germanic languages, it is very different from Modern English and difficult for Modern English speakers to understand without study. Old English grammar is quite similar to that of modern German: nouns, adjectives, pronouns and verbs have many inflectional endings and forms, and word order is much freer. The oldest Old English inscriptions were written using a runic system, but from about the 9th century this was replaced by a version of the Latin alphabet.

"Englisc", which the term "English" is derived from, means 'pertaining to the Angles'. In Old English, this word was derived from "Angles" (one of the Germanic tribes who conquered parts of Great Britain in the 5th century). During the 9th century, all invading Germanic tribes were referred to as "Englisc". It has been hypothesised that the Angles acquired their name because their land on the coast of Jutland (now mainland Denmark) resembled a fishhook. Proto-Germanic also had the meaning of 'narrow', referring to the shallow waters near the coast. That word ultimately goes back to Proto-Indo-European ", also meaning 'narrow'.

Another theory is that the derivation of 'narrow' is the more likely connection to angling (as in fishing), which itself stems from a Proto-Indo-European (PIE) root meaning "bend, angle". The semantic link is the fishing hook, which is curved or bent at an angle. In any case, the Angles may have been called such because they were a fishing people or were originally descended from such, and therefore England would mean 'land of the fishermen', and English would be 'the fishermen's language'.

Old English was not static, and its usage covered a period of 700 years, from the Anglo-Saxon settlement of Britain in the 5th century to the late 11th century, some time after the Norman invasion. While indicating that the establishment of dates is an arbitrary process, Albert Baugh dates Old English from 450 to 1150, a period of full inflections, a synthetic language. Perhaps around 85 per cent of Old English words are no longer in use, but those that survived are basic elements of Modern English vocabulary.

Old English is a West Germanic language, developing out of Ingvaeonic (also known as North Sea Germanic) dialects from the 5th century. It came to be spoken over most of the territory of the Anglo-Saxon kingdoms which became the Kingdom of England. This included most of present-day England, as well as part of what is now southeastern Scotland, which for several centuries belonged to the Anglo-Saxon kingdom of Northumbria. Other parts of the island – Wales and most of Scotland – continued to use Celtic languages, except in the areas of Scandinavian settlements where Old Norse was spoken. Celtic speech also remained established in certain parts of England: Medieval Cornish was spoken all over Cornwall and in adjacent parts of Devon, while Cumbric survived perhaps to the 12th century in parts of Cumbria, and Welsh may have been spoken on the English side of the Anglo-Welsh border. Norse was also widely spoken in the parts of England which fell under Danish law.

Anglo-Saxon literacy developed after Christianisation in the late 7th century. The oldest surviving text of Old English literature is "Cædmon's Hymn", composed between 658 and 680. There is a limited corpus of runic inscriptions from the 5th to 7th centuries, but the oldest coherent runic texts (notably the Franks Casket) date to the 8th century. The Old English Latin alphabet was introduced around the 9th century.
With the unification of the Anglo-Saxon kingdoms (outside the Danelaw) by Alfred the Great in the later 9th century, the language of government and literature became standardised around the West Saxon dialect (Early West Saxon). Alfred advocated education in English alongside Latin, and had many works translated into the English language; some of them, such as Pope Gregory I's treatise "Pastoral Care", appear to have been translated by Alfred himself. In Old English, typical of the development of literature, poetry arose before prose, but King Alfred the Great (871 to 901) chiefly inspired the growth of prose.

A later literary standard, dating from the later 10th century, arose under the influence of Bishop Æthelwold of Winchester, and was followed by such writers as the prolific Ælfric of Eynsham ("the Grammarian"). This form of the language is known as the "Winchester standard", or more commonly as Late West Saxon. It is considered to represent the "classical" form of Old English. It retained its position of prestige until the time of the Norman Conquest, after which English ceased for a time to be of importance as a literary language.

The history of Old English can be subdivided into:

The Old English period is followed by Middle English (12th to 15th century), Early Modern English (c. 1480 to 1650) and finally Modern English (after 1650).

Old English should not be regarded as a single monolithic entity, just as Modern English is also not monolithic. It emerged over time out of the many dialects and languages of the colonising tribes, and it is only towards the later Anglo-Saxon period that these can be considered to have constituted a single national language. Even then, Old English continued to exhibit much local and regional variation, remnants of which remain in Modern English dialects.

The four main dialectal forms of Old English were Mercian, Northumbrian, Kentish, and West Saxon. Mercian and Northumbrian are together referred to as "Anglian". In terms of geography the Northumbrian region lay north of the Humber River; the Mercian lay north of the Thames and South of the Humber River; West Saxon lay south and southwest of the Thames; and the smallest, Kentish region lay southeast of the Thames, a small corner of England. The Kentish region, settled by the Jutes from Jutland, has the scantiest literary remains.

Each of these four dialects was associated with an independent kingdom on the island. Of these, Northumbria south of the Tyne, and most of Mercia, were overrun by the Vikings during the 9th century. The portion of Mercia that was successfully defended, and all of Kent, were then integrated into Wessex under Alfred the Great.
From that time on, the West Saxon dialect (then in the form now known as Early West Saxon) became standardised as the language of government, and as the basis for the many works of literature and religious materials produced or translated from Latin in that period.

The later literary standard known as Late West Saxon (see History, above), although centred in the same region of the country, appears not to have been directly descended from Alfred's Early West Saxon. For example, the former diphthong tended to become monophthongised to in EWS, but to in LWS.

Due to the centralisation of power and the Viking invasions, there is relatively little written record of the non-Wessex dialects after Alfred's unification. Some Mercian texts continued to be written, however, and the influence of Mercian is apparent in some of the translations produced under Alfred's programme, many of which were produced by Mercian scholars. Other dialects certainly continued to be spoken, as is evidenced by the continued variation between their successors in Middle and Modern English. In fact, what would become the standard forms of Middle English and of Modern English are descended from Mercian rather than West Saxon, while Scots developed from the Northumbrian dialect. It was once claimed that, owing to its position at the heart of the Kingdom of Wessex, the relics of Anglo-Saxon accent, idiom and vocabulary were best preserved in the dialect of Somerset.

For details of the sound differences between the dialects, see Phonological history of Old English (dialects).

The language of the Anglo-Saxon settlers appears not to have been significantly affected by the native British Celtic languages which it largely displaced. The number of Celtic loanwords introduced into the language is very small. However, various suggestions have been made concerning possible influence that Celtic may have had on developments in English syntax in the post-Old English period, such as the regular progressive construction and analytic word order, as well as the eventual development of the periphrastic auxiliary verb "do".

Old English contained a certain number of loanwords from Latin, which was the scholarly and diplomatic "lingua franca" of Western Europe. It is sometimes possible to give approximate dates for the borrowing of individual Latin words based on which patterns of sound change they have undergone. Some Latin words had already been borrowed into the Germanic languages before the ancestral Angles and Saxons left continental Europe for Britain. More entered the language when the Anglo-Saxons were converted to Christianity and Latin-speaking priests became influential. It was also through Irish Christian missionaries that the Latin alphabet was introduced and adapted for the writing of Old English, replacing the earlier runic system. Nonetheless, the largest transfer of Latin-based (mainly Old French) words into English occurred after the Norman Conquest of 1066, and thus in the Middle English rather than the Old English period.

Another source of loanwords was Old Norse, which came into contact with Old English via the Scandinavian rulers and settlers in the Danelaw from the late 9th century, and during the rule of Cnut and other Danish kings in the early 11th century. Many place-names in eastern and northern England are of Scandinavian origin. Norse borrowings are relatively rare in Old English literature, being mostly terms relating to government and administration. The literary standard, however, was based on the West Saxon dialect, away from the main area of Scandinavian influence; the impact of Norse may have been greater in the eastern and northern dialects. Certainly in Middle English texts, which are more often based on eastern dialects, a strong Norse influence becomes apparent. Modern English contains a great many, often everyday, words that were borrowed from Old Norse, and the grammatical simplification that occurred after the Old English period is also often attributed to Norse influence.

The influence of Old Norse certainly helped move English from a synthetic language along the continuum to a more analytic word order, and Old Norse most likely made a greater impact on the English language than any other language. The eagerness of Vikings in the Danelaw to communicate with their southern Anglo-Saxon neighbours produced a friction that led to the erosion of the complicated inflectional word-endings. Simeon Potter notes: "No less far-reaching was the influence of Scandinavian upon the inflexional endings of English in hastening that wearing away and leveling of grammatical forms which gradually spread from north to south. It was, after all, a salutary influence. The gain was greater than the loss. There was a gain in directness, in clarity, and in strength."

The strength of the Viking influence on Old English appears from the fact that the indispensable elements of the language – pronouns, modals, comparatives, pronominal adverbs (like "hence" and "together"), conjunctions and prepositions – show the most marked Danish influence; the best evidence of Scandinavian influence appears in the extensive word borrowings for, as Jespersen indicates, no texts exist in either Scandinavia or in Northern England from this time to give certain evidence of an influence on syntax. The change to Old English from Old Norse was substantive, pervasive, and of a democratic character. Old Norse and Old English resembled each other closely like cousins and with some words in common, they roughly understood each other; in time the inflections melted away and the analytic pattern emerged. It is most "important to recognize that in many words the English and Scandinavian language differed chiefly in their inflectional elements. The body of the word was so nearly the same in the two languages that only the endings would put obstacles in the way of mutual understanding. In the mixed population which existed in the Danelaw these endings must have led to much confusion, tending gradually to become obscured and finally lost." This blending of peoples and languages resulted in "simplifying English grammar".

The inventory of classical Old English (Late West Saxon) surface phones, as usually reconstructed, is as follows.
The sounds enclosed in parentheses in the chart above are not considered to be phonemes:

The above system is largely similar to that of Modern English, except that (and for most speakers) have generally been lost, while the voiced affricate and fricatives (now also including ) have become independent phonemes, as has .
The mid front rounded vowels had merged into unrounded before the Late West Saxon period. During the 11th century such vowels arose again, as monophthongisations of the diphthongs , but quickly merged again with in most dialects.

The exact pronunciation of the West Saxon close diphthongs, spelt , is disputed; it may have been . Other dialects may have had different systems of diphthongs; for example, Anglian dialects retained , which had merged with in West Saxon.

For more on dialectal differences, see Phonological history of Old English (dialects).

Some of the principal sound changes occurring in the pre-history and history of Old English were the following:

For more details of these processes, see the main article, linked above. For sound changes before and after the Old English period, see Phonological history of English.

Nouns decline for five cases: nominative, accusative, genitive, dative, instrumental; three genders: masculine, feminine, neuter; and two numbers: singular, and plural; and are strong or weak. The instrumental is vestigial and only used with the masculine and neuter singular and often replaced by the dative. Only pronouns and strong adjectives retain separate instrumental forms. There is also sparse early Northumbrian evidence of a sixth case: the locative. Adjectives agree with nouns in case, gender, number, and strong, or weak forms. Pronouns and sometimes participles agree in case, gender, and number. First-person and second-person personal pronouns occasionally distinguish dual-number forms. The definite article and its inflections serve as a definite article ("the"), a demonstrative adjective ("that"), and demonstrative pronoun. Other demonstratives are ("this"), and ("yon"). These words inflect for case, gender, number. Adjectives have both strong and weak sets of endings, weak ones being used when a definite or possessive determiner is also present.

Verbs conjugate for three persons: first, second, and third; two numbers: singular, plural; two tenses: present, and past; three moods: indicative, subjunctive, and imperative; and are strong (exhibiting ablaut) or weak (exhibiting a dental suffix). Verbs have two infinitive forms: bare, and bound; and two participles: present, and past. The subjunctive has past and present forms. Finite verbs agree with subjects in person, and number. The future tense, passive voice, and other aspects are formed with compounds.
Adpositions are mostly before but often after their object. If the object of an adposition is marked in the dative case, an adposition may conceivably be located anywhere in the sentence.

Remnants of the Old English case system in Modern English are in the forms of a few pronouns (such as "I/me/mine", "she/her", "who/whom/whose") and in the possessive ending "-'s", which derives from the masculine and neuter genitive ending "-es". The modern English plural ending "-(e)s" derives from the Old English "-as", but the latter applied only to "strong" masculine nouns in the nominative and accusative cases; different plural endings were used in other instances. Old English nouns had grammatical gender, while modern English has only natural gender. Pronoun usage could reflect either natural or grammatical gender when those conflicted, as in the case of , a neuter noun referring to a female person.

In Old English's verbal compound constructions are the beginnings of the compound tenses of Modern English. Old English verbs include strong verbs, which form the past tense by altering the root vowel, and weak verbs, which use a suffix such as . As in Modern English, and peculiar to the Germanic languages, the verbs formed two great classes: weak (regular), and strong (irregular). Like today, Old English had fewer strong verbs, and many of these have over time decayed into weak forms. Then, as now, dental suffixes indicated the past tense of the weak verbs, as in "work" and "worked".

Old English syntax is similar to that of modern English. Some differences are consequences of the greater level of nominal and verbal inflection, allowing freer word order.

Old English was first written in runes, using the futhorc – a rune set derived from the Germanic 24-character elder futhark, extended by five more runes used to represent Anglo-Saxon vowel sounds, and sometimes by several more additional characters. From around the 9th century, the runic system came to be supplanted by a (minuscule) half-uncial script of the Latin alphabet introduced by Irish Christian missionaries. This was replaced by insular script, a cursive and pointed version of the half-uncial script. This was used until the end of the 12th century when continental Carolingian minuscule (also known as "Caroline") replaced the insular.

The Latin alphabet of the time still lacked the letters and , and there was no as distinct from ; moreover native Old English spellings did not use , or . The remaining 20 Latin letters were supplemented by four more: (, modern "ash") and (, now called eth or edh), which were modified Latin letters, and thorn and wynn , which are borrowings from the futhorc. A few letter pairs were used as digraphs, representing a single sound. Also used was the Tironian note (a character similar to the digit 7) for the conjunction "and", and a thorn with a crossbar through the ascender for the pronoun . Macrons over vowels were originally used not to mark long vowels (as in modern editions), but to indicate stress, or as abbreviations for a following "m" or "n".

Modern editions of Old English manuscripts generally introduce some additional conventions. The modern forms of Latin letters are used, including in place of the insular G, for long S, and others which may differ considerably from the insular script, notably , and . Macrons are used to indicate long vowels, where usually no distinction was made between long and short vowels in the originals. (In some older editions an acute accent mark was used for consistency with Old Norse conventions.) Additionally, modern editions often distinguish between velar and palatal and by placing dots above the palatals: , . The letter wynn is usually replaced with , but , eth and thorn are normally retained (except when eth is replaced by thorn).

In contrast with Modern English orthography, that of Old English was reasonably regular, with a mostly predictable correspondence between letters and phonemes. There were not usually any silent letters—in the word "cniht", for example, both the and were pronounced, unlike the and in the modern "knight". The following table lists the Old English letters and digraphs together with the phonemes they represent, using the same notation as in the Phonology section above.

Doubled consonants are geminated; the geminate fricatives /, and cannot be voiced.

Old English literature, though more abundant than literature of the continent before AD 1000, is nonetheless scant. The pagan and Christian streams mingle in Old English, one of the richest and most significant bodies of literature preserved among the early Germanic peoples. In his supplementary article to the 1935 posthumous edition of Bright's "Anglo-Saxon Reader", Dr. James Hulbert writes:

Some of the most important surviving works of Old English literature are "Beowulf", an epic poem; the "Anglo-Saxon Chronicle", a record of early English history; the Franks Casket, an inscribed early whalebone artefact; and Cædmon's Hymn, a Christian religious poem. There are also a number of extant prose works, such as sermons and saints' lives, biblical translations, and translated Latin works of the early Church Fathers, legal documents, such as laws and wills, and practical works on grammar, medicine, and geography. Still, poetry is considered the heart of Old English literature. Nearly all Anglo-Saxon authors are anonymous, with a few exceptions, such as Bede and Cædmon. Cædmon, the earliest English poet we know by name, served as a lay brother in the monastery at Whitby.

The first example is taken from the opening lines of the folk-epic "Beowulf", a poem of some 3,000 lines and the single greatest work of Old English. This passage describes how Hrothgar's legendary ancestor Scyld was found as a baby, washed ashore, and adopted by a noble family. The translation is literal and represents the original poetic word order. As such, it is not typical of Old English prose. The modern cognates of original words have been used whenever practical to give a close approximation of the feel of the original poem.

The words in brackets are implied in the Old English by noun case and the bold words in brackets are explanations of words that have slightly different meanings in a modern context. Notice how "what" is used by the poet where a word like "lo" or "behold" would be expected. This usage is similar to "what-ho!", both an expression of surprise and a call to attention.

English poetry is based on stress and alliteration. In alliteration, the first consonant in a word alliterates with the same consonant at the beginning of another word, as with and . Vowels alliterate with any other vowel, as with and . In the text below, the letters that alliterate are bolded.

A semi-fluent translation in Modern English would be:

Lo! We have heard of majesty of the Spear-Danes, of those nation-kings in the days of yore, and how those noblemen promoted zeal. Scyld Scefing took away mead-benches from bands of enemies, from many tribes; he terrified earls. Since he was first found destitute (he gained consolation for that) he grew under the heavens, prospered in honours, until each of those who lived around him over the sea had to obey him, give him tribute. That was a good king!

This text of the Lord's Prayer is presented in the standardised West Saxon literary dialect, with added macrons for vowel length, markings for probable palatalised consonants, modern punctuation, and the replacement of the letter ƿynn with w.

This is a proclamation from King Cnut the Great to his earl Thorkell the Tall and the English people written in AD 1020. Unlike the previous two examples, this text is prose rather than poetry. For ease of reading, the passage has been divided into sentences while the pilcrows represent the original division.

Like other historical languages, Old English has been used by scholars and enthusiasts of later periods to create texts either imitating Anglo-Saxon literature or deliberately transferring it to a different cultural context. Examples include Alistair Campbell and J. R. R. Tolkien. A number of websites devoted to Modern Paganism and historical reenactment offer reference material and forums promoting the active use of Old English. There is also an . However, one investigation found that many Neo-Old English texts published online bear little resemblance to the historical language and have many basic grammatical mistakes.











</doc>
<doc id="22669" url="https://en.wikipedia.org/wiki?curid=22669" title="Open cluster">
Open cluster

An open cluster is a group of up to a few thousand stars that were formed from the same giant molecular cloud and have roughly the same age. More than 1,100 open clusters have been discovered within the Milky Way Galaxy, and many more are thought to exist. They are loosely bound by mutual gravitational attraction and become disrupted by close encounters with other clusters and clouds of gas as they orbit the galactic center. This can result in a migration to the main body of the galaxy and a loss of cluster members through internal close encounters. Open clusters generally survive for a few hundred million years, with the most massive ones surviving for a few billion years. In contrast, the more massive globular clusters of stars exert a stronger gravitational attraction on their members, and can survive for longer. Open clusters have been found only in spiral and irregular galaxies, in which active star formation is occurring.

Young open clusters may be contained within the molecular cloud from which they formed, illuminating it to create an H II region. Over time, radiation pressure from the cluster will disperse the molecular cloud. Typically, about 10% of the mass of a gas cloud will coalesce into stars before radiation pressure drives the rest of the gas away.

Open clusters are key objects in the study of stellar evolution. Because the cluster members are of similar age and chemical composition, their properties (such as distance, age, metallicity and extinction) are more easily determined than they are for isolated stars. A number of open clusters, such as the Pleiades, Hyades or the Alpha Persei Cluster are visible with the naked eye. Some others, such as the Double Cluster, are barely perceptible without instruments, while many more can be seen using binoculars or telescopes. The Wild Duck Cluster, M11, is an example.

The prominent open cluster the Pleiades has been recognized as a group of stars since antiquity, while the Hyades forms part of Taurus, one of the oldest constellations. Other open clusters were noted by early astronomers as unresolved fuzzy patches of light. The Roman astronomer Ptolemy mentions the Praesepe, the Double Cluster in Perseus, and the Ptolemy Cluster, while the Persian astronomer Al-Sufi wrote of the Omicron Velorum cluster. However, it would require the invention of the telescope to resolve these nebulae into their constituent stars. Indeed, in 1603 Johann Bayer gave three of these clusters designations as if they were single stars.

The first person to use a telescope to observe the night sky and record his observations was the Italian scientist Galileo Galilei in 1609. When he turned the telescope toward some of the nebulous patches recorded by Ptolemy, he found they were not a single star, but groupings of many stars. For Praesepe, he found more than 40 stars. Where previously observers had noted only 6-7 stars in the Pleiades, he found almost 50. In his 1610 treatise "Sidereus Nuncius", Galileo Galilei wrote, "the galaxy is nothing else but a mass of innumerable stars planted together in clusters." Influenced by Galileo's work, the Sicilian astronomer Giovanni Hodierna became possibly the first astronomer to use a telescope to find previously undiscovered open clusters. In 1654, he identified the objects now designated Messier 41, Messier 47, NGC 2362 and NGC 2451.

It was realised as early as 1767 that the stars in a cluster were physically related, when the English naturalist Reverend John Michell calculated that the probability of even just one group of stars like the Pleiades being the result of a chance alignment as seen from Earth was just 1 in 496,000. Between 1774–1781, French astronomer Charles Messier published a catalogue of celestial objects that had a nebulous appearance similar to comets. This catalogue included 26 open clusters. In the 1790s, English astronomer William Herschel began an extensive study of nebulous celestial objects. He discovered that many of these features could be resolved into groupings of individual stars. Herschel conceived the idea that stars were initially scattered across space, but later became clustered together as star systems because of gravitational attraction. He divided the nebulae into eight classes, with classes VI through VIII being used to classify clusters of stars.

The number of clusters known continued to increase under the efforts of astronomers. Hundreds of open clusters were listed in the New General Catalogue, first published in 1888 by the Danish-Irish astronomer J. L. E. Dreyer, and the two supplemental Index Catalogues, published in 1896 and 1905. Telescopic observations revealed two distinct types of clusters, one of which contained thousands of stars in a regular spherical distribution and was found all across the sky but preferentially towards the centre of the Milky Way. The other type consisted of a generally sparser population of stars in a more irregular shape. These were generally found in or near the galactic plane of the Milky Way. Astronomers dubbed the former globular clusters, and the latter open clusters. Because of their location, open clusters are occasionally referred to as "galactic clusters", a term that was introduced in 1925 by the Swiss-American astronomer Robert Julius Trumpler.

Micrometer measurements of the positions of stars in clusters were made as early as 1877 by the German astronomer E. Schönfeld and further pursued by the American astronomer E. E. Barnard prior to his death in 1923. No indication of stellar motion was detected by these efforts. However, in 1918 the Dutch-American astronomer Adriaan van Maanen was able to measure the proper motion of stars in part of the Pleiades cluster by comparing photographic plates taken at different times. As astrometry became more accurate, cluster stars were found to share a common proper motion through space. By comparing the photographic plates of the Pleiades cluster taken in 1918 with images taken in 1943, van Maanen was able to identify those stars that had a proper motion similar to the mean motion of the cluster, and were therefore more likely to be members. Spectroscopic measurements revealed common radial velocities, thus showing that the clusters consist of stars bound together as a group.

The first color-magnitude diagrams of open clusters were published by Ejnar Hertzsprung in 1911, giving the plot for the Pleiades and Hyades star clusters. He continued this work on open clusters for the next twenty years. From spectroscopic data, he was able to determine the upper limit of internal motions for open clusters, and could estimate that the total mass of these objects did not exceed several hundred times the mass of the Sun. He demonstrated a relationship between the star colors and their magnitudes, and in 1929 noticed that the Hyades and Praesepe clusters had different stellar populations than the Pleiades. This would subsequently be interpreted as a difference in ages of the three clusters.

The formation of an open cluster begins with the collapse of part of a giant molecular cloud, a cold dense cloud of gas and dust containing up to many thousands of times the mass of the Sun. These clouds have densities that vary from 10 to 10 molecules of neutral hydrogen per cm, with star formation occurring in regions with densities above 10 molecules per cm. Typically, only 1–10% of the cloud by volume is above the latter density. Prior to collapse, these clouds maintain their mechanical equilibrium through magnetic fields, turbulence, and rotation.

Many factors may disrupt the equilibrium of a giant molecular cloud, triggering a collapse and initiating the burst of star formation that can result in an open cluster. These include shock waves from a nearby supernova, collisions with other clouds, or gravitational interactions. Even without external triggers, regions of the cloud can reach conditions where they become unstable against collapse. The collapsing cloud region will undergo hierarchical fragmentation into ever smaller clumps, including a particularly dense form known as infrared dark clouds, eventually leading to the formation of up to several thousand stars. This star formation begins enshrouded in the collapsing cloud, blocking the protostars from sight but allowing infrared observation. In the Milky Way galaxy, the formation rate of open clusters is estimated to be one every few thousand years.
The hottest and most massive of the newly formed stars (known as OB stars) will emit intense ultraviolet radiation, which steadily ionizes the surrounding gas of the giant molecular cloud, forming an H II region. Stellar winds and radiation pressure from the massive stars begins to drive away the hot ionized gas at a velocity matching the speed of sound in the gas. After a few million years the cluster will experience its first core-collapse supernovae, which will also expel gas from the vicinity. In most cases these processes will strip the cluster of gas within ten million years and no further star formation will take place. Still, about half of the resulting protostellar objects will be left surrounded by circumstellar disks, many of which form accretion disks.

As only 30 to 40 per cent of the gas in the cloud core forms stars, the process of residual gas expulsion is highly damaging to the star formation process. All clusters thus suffer significant infant weight loss, while a large fraction undergo infant mortality. At this point, the formation of an open cluster will depend on whether the newly formed stars are gravitationally bound to each other; otherwise an unbound stellar association will result. Even when a cluster such as the Pleiades does form, it may only hold on to a third of the original stars, with the remainder becoming unbound once the gas is expelled. The young stars so released from their natal cluster become part of the Galactic field population.

Because most if not all stars form clustered, star clusters are to be viewed the fundamental building blocks of galaxies. The violent gas-expulsion events that shape and destroy many star clusters at birth leave their imprint in the morphological and kinematical structures of galaxies. Most open clusters form with at least 100 stars and a mass of 50 or more solar masses. The largest clusters can have 10 solar masses, with the massive cluster Westerlund 1 being estimated at 5 × 10 solar masses; close to that of a globular cluster. While open clusters and globular clusters form two fairly distinct groups, there may not be a great deal of difference in appearance between a very sparse globular cluster and a very rich open cluster. Some astronomers believe the two types of star clusters form via the same basic mechanism, with the difference being that the conditions that allowed the formation of the very rich globular clusters containing hundreds of thousands of stars no longer prevail in the Milky Way.

It is common for two or more separate open clusters to form out of the same molecular cloud. In the Large Magellanic Cloud, both Hodge 301 and R136 are forming from the gases of the Tarantula Nebula, while in our own galaxy, tracing back the motion through space of the Hyades and Praesepe, two prominent nearby open clusters, suggests that they formed in the same cloud about 600 million years ago. Sometimes, two clusters born at the same time will form a binary cluster. The best known example in the Milky Way is the Double Cluster of NGC 869 and NGC 884 (sometimes mistakenly called h and χ Persei; h refers to a neighboring star and χ to "both" clusters), but at least 10 more double clusters are known to exist. Many more are known in the Small and Large Magellanic Clouds—they are easier to detect in external systems than in our own galaxy because projection effects can cause unrelated clusters within the Milky Way to appear close to each other.

Open clusters range from very sparse clusters with only a few members to large agglomerations containing thousands of stars. They usually consist of quite a distinct dense core, surrounded by a more diffuse 'corona' of cluster members. The core is typically about 3–4 light years across, with the corona extending to about 20 light years from the cluster centre. Typical star densities in the centre of a cluster are about 1.5 stars per cubic light year; the stellar density near the Sun is about 0.003 stars per cubic light year.

Open clusters are often classified according to a scheme developed by Robert Trumpler in 1930. The Trumpler scheme gives a cluster a three part designation, with a Roman numeral from I-IV indicating its concentration and detachment from the surrounding star field (from strongly to weakly concentrated), an Arabic numeral from 1 to 3 indicating the range in brightness of members (from small to large range), and "p", "m" or "r" to indication whether the cluster is poor, medium or rich in stars. An 'n' is appended if the cluster lies within nebulosity.

Under the Trumpler scheme, the Pleiades are classified as I3rn (strongly concentrated and richly populated with nebulosity present), while the nearby Hyades are classified as II3m (more dispersed, and with fewer members).

There are over 1,000 known open clusters in our galaxy, but the true total may be up to ten times higher than that. In spiral galaxies, open clusters are largely found in the spiral arms where gas densities are highest and so most star formation occurs, and clusters usually disperse before they have had time to travel beyond their spiral arm. Open clusters are strongly concentrated close to the galactic plane, with a scale height in our galaxy of about 180 light years, compared to a galactic radius of approximately 50,000 light years.

In irregular galaxies, open clusters may be found throughout the galaxy, although their concentration is highest where the gas density is highest. Open clusters are not seen in elliptical galaxies: star formation ceased many millions of years ago in ellipticals, and so the open clusters which were originally present have long since dispersed.

In our galaxy, the distribution of clusters depends on age, with older clusters being preferentially found at greater distances from the galactic centre, generally at substantial distances above or below the galactic plane. Tidal forces are stronger nearer the centre of the galaxy, increasing the rate of disruption of clusters, and also the giant molecular clouds which cause the disruption of clusters are concentrated towards the inner regions of the galaxy, so clusters in the inner regions of the galaxy tend to get dispersed at a younger age than their counterparts in the outer regions.

Because open clusters tend to be dispersed before most of their stars reach the end of their lives, the light from them tends to be dominated by the young, hot blue stars. These stars are the most massive, and have the shortest lives of a few tens of millions of years. The older open clusters tend to contain more yellow stars.

Some open clusters contain hot blue stars which seem to be much younger than the rest of the cluster. These blue stragglers are also observed in globular clusters, and in the very dense cores of globulars they are believed to arise when stars collide, forming a much hotter, more massive star. However, the stellar density in open clusters is much lower than that in globular clusters, and stellar collisions cannot explain the numbers of blue stragglers observed. Instead, it is thought that most of them probably originate when dynamical interactions with other stars cause a binary system to coalesce into one star.

Once they have exhausted their supply of hydrogen through nuclear fusion, medium- to low-mass stars shed their outer layers to form a planetary nebula and evolve into white dwarfs. While most clusters become dispersed before a large proportion of their members have reached the white dwarf stage, the number of white dwarfs in open clusters is still generally much lower than would be expected, given the age of the cluster and the expected initial mass distribution of the stars. One possible explanation for the lack of white dwarfs is that when a red giant expels its outer layers to become a planetary nebula, a slight asymmetry in the loss of material could give the star a 'kick' of a few kilometres per second, enough to eject it from the cluster.

Because of their high density, close encounters between stars in an open cluster are common. For a typical cluster with 1,000 stars with a 0.5 parsec half-mass radius, on average a star will have an encounter with another member every 10 million years. The rate is even higher in denser clusters. These encounters can have a significant impact on the extended circumstellar disks of material that surround many young stars. Tidal perturbations of large disks may result in the formation of massive planets and brown dwarfs, producing companions at distances of 100 AU or more from the host star.

Many open clusters are inherently unstable, with a small enough mass that the escape velocity of the system is lower than the average velocity of the constituent stars. These clusters will rapidly disperse within a few million years. In many cases, the stripping away of the gas from which the cluster formed by the radiation pressure of the hot young stars reduces the cluster mass enough to allow rapid dispersal.

Clusters that have enough mass to be gravitationally bound once the surrounding nebula has evaporated can remain distinct for many tens of millions of years, but over time internal and external processes tend also to disperse them. Internally, close encounters between stars can increase the velocity of a member beyond the escape velocity of the cluster. This results in the gradual 'evaporation' of cluster members.

Externally, about every half-billion years or so an open cluster tends to be disturbed by external factors such as passing close to or through a molecular cloud. The gravitational tidal forces generated by such an encounter tend to disrupt the cluster. Eventually, the cluster becomes a stream of stars, not close enough to be a cluster but all related and moving in similar directions at similar speeds. The timescale over which a cluster disrupts depends on its initial stellar density, with more tightly packed clusters persisting for longer. Estimated cluster half lives, after which half the original cluster members will have been lost, range from 150–800 million years, depending on the original density.

After a cluster has become gravitationally unbound, many of its constituent stars will still be moving through space on similar trajectories, in what is known as a stellar association, moving cluster, or moving group. Several of the brightest stars in the 'Plough' of Ursa Major are former members of an open cluster which now form such an association, in this case, the Ursa Major Moving Group. Eventually their slightly different relative velocities will see them scattered throughout the galaxy. A larger cluster is then known as a stream, if we discover the similar velocities and ages of otherwise unrelated stars.

When a Hertzsprung-Russell diagram is plotted for an open cluster, most stars lie on the main sequence. The most massive stars have begun to evolve away from the main sequence and are becoming red giants; the position of the turn-off from the main sequence can be used to estimate the age of the cluster.

Because the stars in an open cluster are all at roughly the same distance from Earth, and were born at roughly the same time from the same raw material, the differences in apparent brightness among cluster members is due only to their mass. This makes open clusters very useful in the study of stellar evolution, because when comparing one star to another, many of the variable parameters are fixed.

The study of the abundances of lithium and beryllium in open cluster stars can give important clues about the evolution of stars and their interior structures. While hydrogen nuclei cannot fuse to form helium until the temperature reaches about 10 million K, lithium and beryllium are destroyed at temperatures of 2.5 million K and 3.5 million K respectively. This means that their abundances depend strongly on how much mixing occurs in stellar interiors. By studying their abundances in open cluster stars, variables such as age and chemical composition are fixed.

Studies have shown that the abundances of these light elements are much lower than models of stellar evolution predict. While the reason for this underabundance is not yet fully understood, one possibility is that convection in stellar interiors can 'overshoot' into regions where radiation is normally the dominant mode of energy transport.

Determining the distances to astronomical objects is crucial to understanding them, but the vast majority of objects are too far away for their distances to be directly determined. Calibration of the astronomical distance scale relies on a sequence of indirect and sometimes uncertain measurements relating the closest objects, for which distances can be directly measured, to increasingly distant objects. Open clusters are a crucial step in this sequence.

The closest open clusters can have their distance measured directly by one of two methods. First, the parallax (the small change in apparent position over the course of a year caused by the Earth moving from one side of its orbit around the Sun to the other) of stars in close open clusters can be measured, like other individual stars. Clusters such as the Pleiades, Hyades and a few others within about 500 light years are close enough for this method to be viable, and results from the Hipparcos position-measuring satellite yielded accurate distances for several clusters.

The other direct method is the so-called moving cluster method. This relies on the fact that the stars of a cluster share a common motion through space. Measuring the proper motions of cluster members and plotting their apparent motions across the sky will reveal that they converge on a vanishing point. The radial velocity of cluster members can be determined from Doppler shift measurements of their spectra, and once the radial velocity, proper motion and angular distance from the cluster to its vanishing point are known, simple trigonometry will reveal the distance to the cluster. The Hyades are the best known application of this method, which reveals their distance to be 46.3 parsecs.

Once the distances to nearby clusters have been established, further techniques can extend the distance scale to more distant clusters. By matching the main sequence on the Hertzsprung-Russell diagram for a cluster at a known distance with that of a more distant cluster, the distance to the more distant cluster can be estimated. The nearest open cluster is the Hyades: the stellar association consisting of most of the Plough stars is at about half the distance of the Hyades, but is a stellar association rather than an open cluster as the stars are not gravitationally bound to each other. The most distant known open cluster in our galaxy is Berkeley 29, at a distance of about 15,000 parsecs. Open clusters are also easily detected in many of the galaxies of the Local Group.

Accurate knowledge of open cluster distances is vital for calibrating the period-luminosity relationship shown by variable stars such as cepheid stars, which allows them to be used as standard candles. These luminous stars can be detected at great distances, and are then used to extend the distance scale to nearby galaxies in the Local Group. Indeed, the open cluster designated NGC 7790 hosts three classical Cepheids. RR Lyrae variables are too old to be associated with open clusters, and are instead found in globular clusters.

The open cluster NGC 6811 contains two known planetary systems Kepler 66 and Kepler 67.





</doc>
<doc id="22673" url="https://en.wikipedia.org/wiki?curid=22673" title="Orimulsion">
Orimulsion

Orimulsion is a registered trademark name for a bitumen-based fuel that was developed for industrial use by Intevep, the Research and Development Affiliate of Petroleos de Venezuela SA (PDVSA), following earlier collaboration on oil emulsions with BP.

Like coal and oil, bitumen occurs naturally and is obtained from the world's largest deposit in the Orinoco Belt in Venezuela. The deposit is estimated to be more than 1,300 billion barrels (190 billion m) of bitumen, an amount approximately equivalent to the world's estimated proven oil reserves.

Raw bitumen has an extremely high viscosity and specific gravity between 8 and 10 API gravity, at ambient temperatures and is unsuitable for direct use in conventional power stations. Orimulsion is made by mixing the bitumen with about 30% fresh water and a small amount of surfactant. The result behaves similarly to fuel oil. An alcohol-based surfactant recently replaced the original phenol-based version; improving the transport properties of the fuel and eliminating the health concerns associated with the phenol group of surfactants.

As a fuel for electricity generation, Orimulsion has a number of attractive characteristics:

If there is a spill while shipping over water the mixture de-emulsifies and the bitumen drops out of suspension.

It is a non-Newtonian fluid, and if it is allowed to cool below 30 °C, it will 'set'. Pumping becomes impossible, and there is no way of restarting operations or the flow through the pipeline again.

Orimulsion is currently used as a commercial boiler fuel in power plants worldwide ("e.g.", Japan, Italy and China). Use of fuel used to be much wider and demand was increasing. However, many of PDVSA's engineers were fired following the Venezuelan general strike of 2002–03. Orimulsion had been the pride of the PDVSA engineers, so Orimulsion fell out of favor with the key political leaders. As a result, the government is trying to "Wind Down" the Orimulsion program. The one exception is the sales of Orimulsion to China. The Venezuelan government has close ties to China, as it has with Cuba. The result is that China is still supplied with Orimulsion, while the rest of the world has either had their supplies terminated, or are still experiencing the "Wind Down" phase. Orimulsion still has excellent potential for domestic consumption.

Another reason given by current PDVSA management is that with rising crude oil prices, it has been found that mixing or diluting Orinoco bitumen (extra-heavy oil) with a lighter crude oil can make this blend more profitable as a crude oil on the world market than by selling it as Orimulsion. An example of this is the popular Merey blend (Orinoco bitumen and Mesa crude oil). ConocoPhillips along with PDVSA operate the Merey Sweeny (bpd) delayed coker, vacuum tower and related facilities at ConocoPhillips' refinery in Sweeny, Texas, U.S.A. for processing and upgrading heavy sour Merey crude oil. 

Air pollutant control technology that is commonly available can limit emissions from Orimulsion to levels considered "Best Available Control Technology", as defined by the United States Environmental Protection Agency.




</doc>
<doc id="22676" url="https://en.wikipedia.org/wiki?curid=22676" title="Oxfordian theory of Shakespeare authorship">
Oxfordian theory of Shakespeare authorship

The Oxfordian theory of Shakespeare authorship contends that Edward de Vere, 17th Earl of Oxford, wrote the plays and poems traditionally attributed to William Shakespeare. Though most literary scholars reject all alternative authorship candidates, including Oxford, popular interest in the Oxfordian theory continues. Since the 1920s, the Oxfordian theory has been the most popular alternative Shakespeare authorship theory.

The convergence of documentary evidence of the type used by academics for authorial attribution – title pages, testimony by other contemporary poets and historians, and official records – sufficiently establishes Shakespeare's authorship for the overwhelming majority of Shakespeare scholars and literary historians, and no such documentary evidence links Oxford to Shakespeare's works. Oxfordians, however, reject the historical record and claim that circumstantial evidence supports Oxford’s authorship, proposing that the contradictory historical evidence is part of a conspiracy theory that falsified the record to protect the identity of the real author. Scholarly literary specialists consider the Oxfordian method of interpreting the plays and poems as autobiographical, and then using them to construct a hypothetical author's biography, as unreliable and logically unsound.

Oxfordian arguments rely heavily on biographical allusions; adherents find correspondences between incidents and circumstances in Oxford's life and events in Shakespeare's plays, sonnets, and longer poems. The case also relies on perceived parallels of language, idiom, and thought between Shakespeare's works and Oxford's own poetry and letters. Oxfordians claim that marked passages in Oxford's Bible can be linked to Biblical allusions in Shakespeare's plays. That no plays survive under Oxford's name is also important to the Oxfordian theory. Oxfordians interpret certain 16th- and 17th-century literary allusions as indicating that Oxford was one of the more prominent suppressed anonymous and/or pseudonymous writers of the day. Under this scenario, Shakespeare was either a "front man" or "play-broker" who published the plays under his own name or was merely an actor with a similar name, misidentified as the playwright since the first Shakespeare biographies of the early 1700s.

The most compelling evidence against the Oxfordian theory is de Vere's death in 1604, since the generally accepted chronology of Shakespeare's plays places the composition of approximately twelve of the plays after that date. Oxfordians respond that the annual publication of "new" or "corrected" Shakespeare plays stopped in 1604, and that the dedication to "Shakespeare's Sonnets" implies that the author was dead prior to their publication in 1609. Oxfordians believe the reason so many of the "late plays" show evidence of revision and collaboration is because they were completed by other playwrights after Oxford's death.

The theory that the works of Shakespeare were in fact written by someone other than William Shakespeare dates back to the mid-nineteenth century. In 1857, the first book on the topic, "The Philosophy of the Plays of Shakspere Unfolded", by Delia Bacon, was published. Bacon proposed the first "group theory" of Shakespearian authorship, attributing the works to a committee headed by Francis Bacon and including Walter Raleigh. De Vere is mentioned once in the book, in a list of "high-born wits and poets", who were associated with Raleigh. Some commentators have interpreted this to imply that he was part of the group of authors. Throughout the 19th century Bacon was the preferred hidden author. Oxford is not known to have been mentioned again in this context.

By the beginning of the twentieth century other candidates, typically aristocrats, were put forward, most notably Roger Manners, 5th Earl of Rutland, and William Stanley, 6th Earl of Derby. Oxford's candidacy as sole author was first proposed by J. Thomas Looney in his 1920 book "Shakespeare Identified in Edward de Vere, 17th Earl of Oxford". Following earlier anti-Stratfordians, Looney argued that the known facts of Shakespeare's life did not fit the personality he ascribed to the author of the plays. Like other anti-Stratfordians before him, Looney referred to the absence of records concerning Shakespeare's education, his limited experience of the world, his allegedly poor handwriting skills (evidenced in his signatures), and the "dirt and ignorance" of Stratford at the time. Shakespeare had a petty "acquisitive disposition", he said, while the plays made heroes of free-spending figures. They also portrayed middle and lower-class people negatively, while Shakespearian heroes were typically aristocratic. Looney referred to scholars who found in the plays evidence that their author was an expert in law, widely read in ancient Latin literature, and could speak French and Italian. Looney believed that even very early works such as "Love's Labour's Lost" implied that he was already a person of "matured powers", in his forties or fifties, with wide experience of the world. Looney considered that Oxford's personality fitted that he deduced from the plays, and also identified characters in the plays as detailed portraits of Oxford's family and personal contacts. Several characters, including Hamlet and Bertram (in "All's Well that Ends Well"), were, he believed, self-portraits. Adapting arguments earlier used for Rutland and Derby, Looney fitted events in the plays to episodes in Oxford's life, including his travels to France and Italy, the settings for many plays. Oxford's death in 1604 was linked to a drop-off in the publication of Shakespeare plays. Looney declared that the late play "The Tempest" was not written by Oxford, and that others performed or published after Oxford's death were most probably left incomplete and finished by other writers, thus explaining the apparent idiosyncrasies of style found in the late Shakespeare plays. Looney also introduced the argument that the reference to the "ever-living poet" in the 1609 dedication to Shakespeare's sonnets implied that the author was dead at the time of publication.

Sigmund Freud, the novelist Marjorie Bowen, and several 20th-century celebrities found the thesis persuasive, and Oxford soon overtook Bacon as the favoured alternative candidate to Shakespeare, though academic Shakespearians mostly ignored the subject. Looney's theory attracted a number of activist followers who published books supplementing his own and added new arguments, most notably Percy Allen, Bernard M. Ward, Louis P. Bénézet and Charles Wisner Barrell. Mainstream scholar Steven W. May has noted that Oxfordians of this period made genuine contributions to knowledge of Elizabethan history, citing "Ward's quite competent biography of the Earl" and "Charles Wisner Barrell's identification of Edward Vere, Oxford's illegitimate son by Anne Vavasour" as examples. In 1921, Sir George Greenwood, Looney, and others founded The Shakespeare Fellowship, an organization originally dedicated to the discussion and promotion of ecumenical anti-Stratfordian views, but which later became devoted to promoting Oxford as the true Shakespeare.

After a period of decline of the Oxfordian theory beginning with World War II, in 1952 Dorothy and Charlton Greenwood Ogburn published the 1,300-page "This Star of England", which briefly revived Oxfordism. A series of critical academic books and articles, however, held in check any appreciable growth of anti-Stratfordism and Oxfordism, most notably "The Shakespeare Ciphers Examined" (1957), by William and Elizebeth Friedman, "The Poacher from Stratford" (1958), by Frank Wadsworth, "Shakespeare and His Betters" (1958), by Reginald Churchill, "The Shakespeare Claimants" (1962), by H. N. Gibson, and "Shakespeare and his Rivals: A Casebook on the Authorship Controversy" (1962), by George L. McMichael and Edgar M. Glenn. By 1968 the newsletter of The Shakespeare Oxford Society reported that "the missionary or evangelical spirit of most of our members seems to be at a low ebb, dormant, or non-existent". In 1974, membership in the society stood at 80. In 1979, the publication of an analysis of the Ashbourne portrait dealt a further blow to the movement. The painting, long claimed to be one of the portraits of Shakespeare, but considered by Barrell to be an overpaint of a portrait of the Earl of Oxford, turned out to represent neither, but rather depicted Hugh Hamersley.

Charlton Ogburn, Jr., was elected president of The Shakespeare Oxford Society in 1976 and kick-started the modern revival of the Oxfordian movement by seeking publicity through moot court trials, media debates, television and later the Internet, including Wikipedia, methods which became standard for Oxfordian and anti-Stratfordian promoters because of their success in recruiting members of the lay public. He portrayed academic scholars as self-interested members of an "entrenched authority" that aimed to "outlaw and silence dissent in a supposedly free society", and proposed to counter their influence by portraying Oxford as a candidate on equal footing with Shakespeare.

In 1985 Ogburn published his 900-page "The Mysterious William Shakespeare: the Myth and the Reality", with a Foreword by Pulitzer prize-winning historian David McCullough who wrote: “[T]his brilliant, powerful book is a major event for everyone who cares about Shakespeare. The scholarship is surpassing—brave, original, full of surprise... The strange, difficult, contradictory man who emerges as the real Shakespeare, Edward de Vere, 17th Earl of Oxford, is not just plausible but fascinating and wholly believable.” 

By framing the issue as one of fairness in the atmosphere of conspiracy that permeated America after Watergate, he used the media to circumnavigate academia and appeal directly to the public. Ogburn's efforts secured Oxford the place as the most popular alternative candidate.

Although Shakespearian experts disparaged Ogburn's methodology and his conclusions, one reviewer, Richmond Crinkley, the Folger Shakespeare Library's former director of educational programs, acknowledged the appeal of Ogburn's approach, writing that the doubts over Shakespeare, "arising early and growing rapidly", have a "simple, direct plausibility", and the dismissive attitude of established scholars only worked to encourage such doubts. Though Crinkley rejected Ogburn's thesis, calling it "less satisfactory than the unsatisfactory orthodoxy it challenges", he believed that one merit of the book lay in how it forces orthodox scholars to reexamine their concept of Shakespeare as author. Spurred by Ogburn's book, "[i]n the last decade of the twentieth century members of the Oxfordian camp gathered strength and made a fresh assault on the Shakespearean citadel, hoping finally to unseat the man from Stratford and install de Vere in his place."

The Oxfordian theory returned to wide public attention in anticipation of the late October 2011 release of Roland Emmerich's film "Anonymous". Its distributor, Sony Pictures, advertised that the film "presents a compelling portrait of Edward de Vere as the true author of Shakespeare's plays", and commissioned high school and college-level lesson plans to promote the authorship question to history and literature teachers across the United States. According to Sony Pictures, "the objective for our Anonymous program, as stated in the classroom literature, is 'to encourage critical thinking by challenging students to examine the theories about the authorship of Shakespeare's works and to formulate their own opinions.' The study guide does not state that Edward de Vere is the writer of Shakespeare's work, but it does pose the authorship question which has been debated by scholars for decades".

Although most Oxfordians agree on the main arguments for Oxford, the theory has spawned schismatic variants that have not met with wide acceptance by all Oxfordians, although they have gained much attention.

In a letter written by Looney in 1933, he mentions that Allen and Ward were "advancing certain views respecting Oxford and Queen Eliz. which appear to me extravagant & improbable, in no way strengthen Oxford’s Shakespeare claims, and are likely to bring the whole cause into ridicule." Allen and Ward believed that they had discovered that Elizabeth and Oxford were lovers and had conceived a child. Allen developed the theory in his 1934 book "Anne Cecil, Elizabeth & Oxford". He argued that the child was given the name William Hughes, who became an actor under the stage-name "William Shakespeare". He adopted the name because his father, Oxford, was already using it as a pen-name for his plays. Oxford had borrowed the name from a third Shakespeare, the man of that name from Stratford-upon-Avon, who was a law student at the time, but who was never an actor "or" a writer. Allen later changed his mind about Hughes and decided that the concealed child was the Earl of Southampton, the dedicatee of Shakespeare's narrative poems. This secret drama, which has become known as the Prince Tudor theory, was covertly represented in Oxford's plays and poems and remained hidden until Allen and Ward's discoveries. The narrative poems and sonnets had been written by Oxford for his son. "This Star of England" (1952) by Charlton and Dorothy Ogburn included arguments in support of this version of the theory. Their son, Charlton Ogburn, Jr, agreed with Looney that the theory was an impediment to the Oxfordian movement and omitted all discussion about it in his own Oxfordian works.

However, the theory was revived and expanded by Elisabeth Sears in "Shakespeare and the Tudor Rose" (2002), and Hank Whittemore in "The Monument" (2005), an analysis of Shakespeare's Sonnets which interprets the poems as a poetic history of Queen Elizabeth, Oxford, and Southampton. Paul Streitz's "Oxford: Son of Queen Elizabeth I" (2001) advances a variation on the theory: that Oxford himself was the illegitimate son of Queen Elizabeth by her stepfather, Thomas Seymour. Oxford was thus the half-brother of his own son by the queen. Streitz also believes that the queen had children by the Earl of Leicester. These were Robert Cecil, 1st Earl of Salisbury, Robert Devereux, 2nd Earl of Essex, Mary Sidney and Elizabeth Leighton.

As with other candidates for authorship of Shakespeare's works, Oxford's supporters have attributed numerous non-Shakespearian works to him. Looney began the process in his 1921 edition of de Vere's poetry. He suggested that de Vere was also responsible for some of the literary works credited to Arthur Golding, Anthony Munday and John Lyly. Streitz credits Oxford with the Authorized King James Version of the Bible. Two professors of linguistics have claimed that de Vere wrote not only the works of Shakespeare, but most of what is memorable in English literature during his lifetime, with such names as Edmund Spenser, Christopher Marlowe, Philip Sidney, John Lyly, George Peele, George Gascoigne, Raphael Holinshed, Robert Greene, Thomas Phaer, and Arthur Golding being among dozens of further pseudonyms of de Vere. Ramon Jiménez has credited Oxford with such plays as "The True Tragedy of Richard III" and "Edmund Ironside".

Group theories in which Oxford played the principal role as writer, but collaborated with others to create the Shakespeare canon, were adopted by a number of early Oxfordians. Looney himself was willing to concede that Oxford may have been assisted by his son-in-law William Stanley, 6th Earl of Derby, who perhaps wrote "The Tempest". B.M. Ward also suggested that Oxford and Derby worked together. In his later writings Percy Allen argued that Oxford led a group of writers, among whom was William Shakespeare. Group theories with Oxford as the principal author or creative "master mind" were also proposed by Gilbert Standen in "Shakespeare Authorship" (1930), Gilbert Slater in "Seven Shakespeares" (1931) and Montagu William Douglas in "Lord Oxford and the Shakespeare Group" (1952).

Specialists in Elizabethan literary history object to the methodology of Oxfordian arguments. In lieu of any evidence of the type commonly used for authorship attribution, Oxfordians discard the methods used by historians and employ other types of arguments to make their case, the most common being supposed parallels between Oxford's life and Shakespeare's works.

Another is finding cryptic allusions to Oxford's supposed play writing in other literary works of the era that to them suggest that his authorship was obvious to those "in the know". David Kathman writes that their methods are subjective and devoid of any evidential value, because they use a "double standard". Their arguments are "not taken seriously by Shakespeare scholars because they consistently distort and misrepresent the historical record", "neglect to provide necessary context" and calling some of their arguments "outright fabrication". One major evidential objection to the Oxfordian theory is Edward de Vere's 1604 death, after which a number of Shakespeare's plays are generally believed to have been written. In "The Shakespeare Claimants", a 1962 examination of the authorship question, H. N. Gibson concluded that "... on analysis the Oxfordian case appears to me a very weak one".

Mainstream academics have often argued that the Oxford theory is based on snobbery: that anti-Stratfordians reject the idea that the son of a mere tradesman could write the plays and poems of Shakespeare. The Shakespeare Oxford Society has responded that this claim is "a substitute for reasoned responses to Oxfordian evidence and logic" and is merely an "ad hominem" attack, a charge echoed by journalists on both sides of the issue, including Michael Prescott and Joseph Sobran.

Mainstream critics further say that if William Shakespeare were a fraud instead of the true author, the number of people involved in suppressing this information would have made it highly unlikely to succeed. And citing the "testimony of contemporary writers, court records and much else" supporting Shakespeare's authorship, Columbia University professor James S. Shapiro points out the logically fatal tautology of any theory claiming that "there must have been a conspiracy to suppress the truth of de Vere's authorship" based on the idea that "the very absence of surviving evidence proves the case."

While no documentary evidence connects Oxford (or any authorial candidate) to the plays of Shakespeare, Oxfordian writers, including Mark Anderson and Charlton Ogburn, say that connection is made by considerable circumstantial evidence inferred from Oxford's connections to the Elizabethan theatre and poetry scene; the participation of his family in the printing and publication of the First Folio; his relationship with the Earl of Southampton (believed by most Shakespeare scholars to have been Shakespeare's patron); as well as a number of specific incidents and circumstances of Oxford's life that Oxfordians say are depicted in the plays themselves.

Oxford was noted for his literary and theatrical patronage, garnering dedications from a wide range of authors. For much of his adult life, Oxford patronised both adult and boy acting companies, as well as performances by musicians, acrobats and performing animals, and in 1583, he was a leaseholder of the first Blackfriars Theatre in London.

Oxford was related to several noted literary figures. His mother, Margory Golding, was the sister of the Ovid translator Arthur Golding, and his uncle, Henry Howard, Earl of Surrey, was the inventor of the English or Shakespearian sonnet form.

The three dedicatees of Shakespeare's works (the earls of Southampton, Montgomery and Pembroke) were each proposed as husbands for the three daughters of Edward de Vere. "Venus and Adonis" and "The Rape of Lucrece" were dedicated to Southampton (whom many scholars have argued was the Fair Youth of the "Sonnets"), and the "First Folio" of Shakespeare's plays was dedicated to Montgomery (who married Susan de Vere) and Pembroke (who was once engaged to Bridget de Vere).

In the late 1990s, Roger A. Stritmatter conducted a study of the marked passages found in Edward de Vere's Geneva Bible, which is now owned by the Folger Shakespeare Library. The Bible contains 1,028 instances of underlined words or passages and a few hand-written annotations, most of which consist of a single word or fragment. Stritmatter believes about a quarter of the marked passages appear in Shakespeare's works as either a theme, allusion, or quotation. Stritmatter grouped the marked passages into eight themes. Arguing that the themes fitted de Vere's known interests, he proceeded to link specific themes to passages in Shakespeare. Critics have doubted that any of the underlinings or annotations in the Bible can be reliably attributed to de Vere and not the book's other owners prior to its acquisition by the Folger Shakespeare Library in 1925, as well as challenging the looseness of Stritmatter's standards for a Biblical allusion in Shakespeare's works and arguing that there is no statistical significance to the overlap.

Shakespeare's native Avon and Stratford are referred to in two prefatory poems in the 1623 First Folio, one of which refers to Shakespeare as "Swan of Avon" and another to the author's "Stratford monument". Oxfordians say the first of these phrases could refer to one of Edward de Vere's manors, Bilton Hall, near the Forest of Arden, in Rugby, on the River Avon. This view was first expressed by Charles Wisner Barrell, who argued that De Vere "kept the place as a literary hideaway where he could carry on his creative work without the interference of his father-in-law, Burghley, and other distractions of Court and city life." Oxfordians also consider it significant that the nearest town to the parish of Hackney, where de Vere later lived and was buried, was also named Stratford. Mainstream scholar Irvin Matus demonstrated that Oxford sold the Bilton house in 1580, having previously rented it out, making it unlikely that Ben Jonson's 1623 poem would identify Oxford by referring to a property he once owned, but never lived in, and sold 43 years earlier. Nor is there any evidence of a monument to Oxford in Stratford, London, or anywhere else; his widow provided for the creation of one at Hackney in her 1613 will, but there is no evidence that it was ever erected.

Oxfordians also believe that Rev. Dr. John Ward's 1662 diary entry stating that Shakespeare wrote two plays a year "and for that had an allowance so large that he spent at the rate of £1,000 a year" as a critical piece of evidence, since Queen Elizabeth I gave Oxford an annuity of exactly £1,000 beginning in 1586 that was continued until his death. Ogburn wrote that the annuity was granted "under mysterious circumstances", and Anderson suggests it was granted because of Oxford's writing patriotic plays for government propaganda. However, the documentary evidence indicates that the allowance was meant to relieve Oxford's embarrassed financial situation caused by the ruination of his estate.

Almost half of Shakespeare's plays are set in Italy, many of them containing details of Italian laws, customs, and culture which Oxfordians believe could only have been obtained by personal experiences in Italy, and especially in Venice. The author of "The Merchant of Venice", Looney believed, "knew Italy first hand and was touched with the life and spirit of the country". This argument had earlier been used by supporters of the Earl of Rutland and the Earl of Derby as authorship candidates, both of whom had also travelled on the continent of Europe. Oxfordian William Farina refers to Shakespeare's apparent knowledge of the Jewish ghetto, Venetian architecture and laws in "The Merchant of Venice", especially the city's "notorious Alien Statute". Historical documents confirm that Oxford lived in Venice, and travelled for over a year through Italy. He disliked the country, writing in a letter to Lord Burghley dated 24 September 1575, "I am glad I have seen it, and I care not ever to see it any more". Still, he remained in Italy for another six months, leaving Venice in March 1576. According to Anderson, Oxford definitely visited Venice, Padua, Milan, Genoa, Palermo, Florence, Siena and Naples, and probably passed through Messina, Mantua and Verona, all cities used as settings by Shakespeare. In testimony before the Venetian Inquisition, Edward de Vere was said to be fluent in Italian.

However, some Shakespeare scholars say that Shakespeare gets many details of Italian life wrong, including the laws and urban geography of Venice. Kenneth Gross writes that "the play itself knows nothing about the Venetian ghetto; we get no sense of a legally separate region of Venice where Shylock must dwell." Scott McCrea describes the setting as "a nonrealistic Venice" and the laws invoked by Portia as part of the "imaginary world of the play", inconsistent with actual legal practice. Charles Ross points out that Shakespeare's Alien Statute bears little resemblance to any Italian law. For later plays such as "Othello", Shakespeare probably used Lewes Lewknor's 1599 English translation of Gasparo Contarini's "The Commonwealth and Government of Venice" for some details about Venice's laws and customs.

Shakespeare derived much of this material from John Florio, an Italian scholar living in England who was later thanked by Ben Jonson for helping him get Italian details right for his play "Volpone". Kier Elam has traced Shakespeare's Italian idioms in "Shrew" and some of the dialogue to Florio's "Second Fruits", a bilingual introduction to Italian language and culture published in 1591. Jason Lawrence believes that Shakespeare’s Italian dialogue in the play derives "almost entirely" from Florio’s "First Fruits"(1578). He also believes that Shakespeare became more proficient in reading the language as set out in Florio’s manuals, as evidenced by his increasing use of Florio and other Italian sources for writing the plays.

In 1567 Oxford was admitted to Gray's Inn, one of the Inns of Court which Justice Shallow reminisces about in "Henry IV, Part 2". Sobran observes that the Sonnets "abound not only in legal terms – more than 200 – but also in elaborate legal conceits." These terms include: "allege, auditor, defects, exchequer, forfeit, heirs, impeach, lease, moiety, recompense, render, sureties," and "usage". Shakespeare also uses the legal term, "quietus" (final settlement), in Sonnet 134, the last Fair Youth sonnet.

Regarding Oxford's knowledge of court life, which Oxfordians believe is reflected throughout the plays, mainstream scholars say that any special knowledge of the aristocracy appearing in the plays can be more easily explained by Shakespeare's life-time of performances before nobility and royalty, and possibly, as Gibson theorises, "by visits to his patron's house, as Marlowe visited Walsingham."

Some of Oxford's lyric works have survived. Steven W. May, an authority on Oxford's poetry, attributes sixteen poems definitely, and four possibly, to Oxford noting that these are probably "only a good sampling" as "both Webbe (1586) and Puttenham (1589) rank him first among the courtier poets, an eminence he probably would not have been granted, despite his reputation as a patron, by virtue of a mere handful of lyrics".

May describes Oxford as a "competent, fairly experimental poet working in the established modes of mid-century lyric verse" and his poetry as "examples of the standard varieties of mid-Elizabethan amorous lyric". In 2004, May wrote that Oxford's poetry was "one man's contribution to the rhetorical mainstream of an evolving Elizabethan poetic" and challenged readers to distinguish any of it from "the output of his mediocre mid-century contemporaries". C. S. Lewis wrote that de Vere's poetry shows "a faint talent", but is "for the most part undistinguished and verbose."

In the opinion of J. Thomas Looney, as "far as forms of versification are concerned De Vere presents just that rich variety which is so noticeable in Shakespeare; and almost all the forms he employs we find reproduced in the Shakespeare work." Oxfordian Louis P. Bénézet created the "Bénézet test", a collage of lines from Shakespeare and lines he thought were representative of Oxford, challenging non-specialists to tell the difference between the two authors. May notes that Looney compared various motifs, rhetorical devices and phrases with certain Shakespeare works to find similarities he said were "the most crucial in the piecing together of the case", but that Looney used six poems mistakenly attributed to Oxford that were actually written by Greene, Campion, and Greville for some of those "crucial" examples. Bénézet also used two lines from Greene that he thought were Oxford's, while succeeding Oxfordians, including Charles Wisner Barrell, have also misattributed poems to Oxford. "This on-going confusion of Oxford's genuine verse with that of at least three other poets," writes May, "illustrates the wholesale failure of the basic Oxfordian methodology."

According to a computerised textual comparison developed by the Claremont Shakespeare Clinic, the styles of Shakespeare and Oxford were found to be "light years apart", and the odds of Oxford having written Shakespeare were reported as "lower than the odds of getting hit by lightning". Furthermore, while the First Folio shows traces of a dialect identical to Shakespeare's, the Earl of Oxford, raised in Essex, spoke an East Anglian dialect. John Shahan and Richard Whalen condemned the Claremont study, calling it "apples to oranges", and noting that the study did not compare Oxford's songs to Shakespeare's songs, did not compare a clean unconfounded sample of Oxford's poems with Shakespeare's poems, and charged that the students under Elliott and Valenza's supervision incorrectly assumed that Oxford's youthful verse was representative of his mature poetry.

Joseph Sobran's book, "Alias Shakespeare", includes Oxford's known poetry in an appendix with what he considers extensive verbal parallels with the work of Shakespeare, and he argues that Oxford's poetry is comparable in quality to some of Shakespeare's early work, such as "Titus Andronicus". Other Oxfordians say that de Vere's extant work is that of a young man and should be considered juvenilia, while May believes that all the evidence dates his surviving work to his early 20s and later.

Four contemporary critics praise Oxford as a poet and a playwright, three of them within his lifetime:


Mainstream scholarship characterises the extravagant praise for de Vere's poetry more as a convention of flattery than honest appreciation of literary merit. Alan Nelson, de Vere's documentary biographer, writes that "[c]ontemporary observers such as Harvey, Webbe, Puttenham and Meres clearly exaggerated Oxford's talent in deference to his rank."

Before the advent of copyright, anonymous and pseudonymous publication was a common practice in the sixteenth century publishing world, and a passage in the "Arte of English Poesie" (1589), an anonymously published work itself, mentions in passing that literary figures in the court who wrote "commendably well" circulated their poetry only among their friends, "as if it were a discredit for a gentleman to seem learned" (Book 1, Chapter 8). In another passage 23 chapters later, the author (probably George Puttenham) speaks of aristocratic writers who, if their writings were made public, would appear to be excellent. It is in this passage that Oxford appears on a list of poets.

According to Daniel Wright, these combined passages confirm that Oxford was one of the concealed writers in the Elizabethan court. Critics of this view argue that Oxford nor any other writer is not here identified as a concealed writer, but as the first in a list of "known" modern writers whose works have already been "made public", "of which number is first" Oxford, adding to the publicly acknowledged literary tradition dating back to Geoffrey Chaucer. Other critics interpret the passage to mean that the courtly writers and their works are known within courtly circles, but not to the general public. In either case, neither Oxford nor anyone else is identified as a hidden writer or one that used a pseudonym.

Oxfordians argue that at the time of the passage's composition (pre-1589), the writers referenced were not in print, and interpret Puttenham's passage (that the noblemen preferred to 'suppress' their work to avoid the discredit of appearing learned) to mean that they were 'concealed'. They cite Sir Philip Sidney, none of whose poetry was published until after his premature death, as an example. Similarly, by 1589 nothing by Greville was in print, and only one of Walter Raleigh's works had been published.

Critics point out that six of the nine poets listed had appeared in print under their own names long before 1589, including a number of Oxford's poems in printed miscellanies, and the first poem published under Oxford's name was printed in 1572, 17 years before Puttenham's book was published. Several other contemporary authors name Oxford as a poet, and Puttenham himself quotes one of Oxford's verses elsewhere in the book, referring to him by name as the author, so Oxfordians misread Puttenham.

Oxfordians also believe other texts refer to the Edward de Vere as a concealed writer. They argue that satirist John Marston's "Scourge of Villanie" (1598) contains further cryptic allusions to Oxford, named as "Mutius". Marston expert Arnold Davenport believes that Mutius is the bishop-poet Joseph Hall and that Marston is criticising Hall's satires.

There is a description of the figure of Oxford in "The Revenge of Bussy D'Ambois", a 1613 play by George Chapman, who has been suggested as the Rival Poet of Shakespeare's Sonnets. Chapman describes Oxford as "Rare and most absolute" in form and says he was "of spirit passing great / Valiant and learn’d, and liberal as the sun". He adds that he "spoke and writ sweetly" of both learned subjects and matters of state ("public weal").

For mainstream Shakespearian scholars, the most compelling evidence against Oxford (besides the historical evidence for William Shakespeare) is his death in 1604, since the generally accepted chronology of Shakespeare's plays places the composition of approximately twelve of the plays after that date. Critics often cite "The Tempest" and "Macbeth", for example, as having been written after 1604.

The exact dates of the composition of most of Shakespeare's plays are uncertain, although David Bevington says it is a 'virtually unanimous' opinion among teachers and scholars of Shakespeare that the canon of late plays depicts an artistic journey that extends well beyond 1604. Evidence for this includes allusions to historical events and literary sources which postdate 1604, as well as Shakespeare's adaptation of his style to accommodate Jacobean literary tastes and the changing membership of the King's Men and their different venues.

Oxfordians say that the conventional composition dates for the plays were developed by mainstream scholars to fit within Shakespeare's lifetime and that no evidence exists that any plays were written after 1604. Anderson argues that all of the Jacobean plays were written before 1604, selectively citing non-Oxfordian scholars like Alfred Harbage, Karl Elze, and Andrew Cairncross to bolster his case. Anderson notes that from 1593 through 1603, the publication of new plays appeared at the rate of two per year, and whenever an inferior or pirated text was published, it was typically followed by a genuine text described on the title page as "newly augmented" or "corrected". After the publication of the Q1 and Q2 "Hamlet" in 1603, no new plays were published until 1608. Anderson observes that, "After 1604, the 'newly correct[ing]' and 'augment[ing]' stops. Once again, the Shake-speare ["sic"] enterprise appears to have shut down".

Because Shakespeare lived until 1616, Oxfordians question why, if he were the author, did he not eulogise Queen Elizabeth at her death in 1603 or Henry, Prince of Wales, at his in 1612. They believe Oxford's 1604 death provides the explanation. In an age when such actions were expected, Shakespeare also failed to memorialise the coronation of James I in 1604, the marriage of Princess Elizabeth in 1612, and the investiture of Prince Charles as the new Prince of Wales in 1613.

Anderson contends that Shakespeare refers to the latest scientific discoveries and events through the end of the 16th century, but "is mute about science after de Vere’s [Oxford’s] death in 1604". He believes that the absence of any mention of the spectacular supernova of October 1604 or Kepler’s revolutionary 1609 study of planetary orbits are especially noteworthy.

Professor Jonathan Bate writes that Oxfordians cannot "provide any explanation for ... technical changes attendant on the King's Men's move to the Blackfriars theatre four years after their candidate's death ... Unlike the Globe, the Blackfriars was an indoor playhouse" and so required plays with frequent breaks in order to replace the candles it used for lighting. "The plays written after Shakespeare's company began using the Blackfriars in 1608, "Cymbeline" and "The Winter's Tale" for instance, have what most ... of the earlier plays do not have: a carefully planned five-act structure". If new Shakespearian plays were being written especially for presentation at the Blackfriars' theatre after 1608, they could not have been written by Edward de Vere.

Oxfordians argue that Oxford was well acquainted with the Blackfriars Theatre, having been a leaseholder of the venue, and note that the "assumption" that Shakespeare wrote plays for the Blackfriars is not universally accepted, citing Shakespearian scholars such as A. Nicoll who said that "all available evidence is either completely negative or else runs directly counter to such a supposition" and Harley Granville-Barker, who stated "Shakespeare did not write (except for Henry V) five-act plays at any stage of his career. The five-act structure was formalized in the First Folio, and is inauthentic".

Further, attribution studies have shown that certain plays in the canon were written by two or three hands, which Oxfordians believe is explained by these plays being either drafted earlier than conventionally believed, or simply revised/completed by others after Oxford's death. Shapiro calls this a 'nightmare' for Oxfordians, implying a 'jumble sale scenario' for his literary remains long after his death.

Some Oxfordians have identified titles or descriptions of lost works from Oxford's lifetime that suggest a thematic similarity to a particular Shakespearian play and asserted that they were earlier versions. For example, in 1732, the antiquarian Francis Peck published in "Desiderata Curiosa" a list of documents in his possession that he intended to print someday. They included "a pleasant conceit of Vere, earl of Oxford, discontented at the rising of a mean gentleman in the English court, circa 1580." Peck never published his archives, which are now lost. To Anderson, Peck's description suggests that this conceit is "arguably an early draft of "Twelfth Night"."

Oxfordian writers say some literary allusions imply that the playwright and poet died prior to 1609, when "Shake-Speares Sonnets" appeared with the epithet "our ever-living poet" in its dedication. They claim that the phrase "ever-living" rarely, if ever, referred to a living person, but instead was used to refer to the eternal soul of the deceased. Bacon, Derby, Neville, and William Shakespeare all lived well past the 1609 publication of the Sonnets.

However, Don Foster, in his study of Early Modern uses of the phrase "ever-living", argues that the phrase most frequently refers to God or other supernatural beings, suggesting that the dedication calls upon God to bless the living begetter (writer) of the sonnets. He states that the initials "W. H." were a misprint for "W. S." or "W. SH". Bate thinks it a misprint as well, but he thinks it "improbable" that the phrase refers to God and suggests that the "ever-living poet" might be "a great dead English poet who had written on the great theme of poetic immortality", such as Sir Philip Sidney or Edmund Spenser.

Joseph Sobran, in "Alias Shakespeare," argued that in 1607 William Barksted, a minor poet and playwright, implies in his poem "Mirrha the Mother of Adonis" that Shakespeare was already deceased. Shakespeare scholars explain that Sobran has simply misread Barksted’s poem, the last stanza of which is a comparison of Barksted’s poem to Shakespeare’s "Venus and Adonis", and has mistaken the grammar also, which makes it clear that Barksted is referring to Shakespeare’s "song" in the past tense, not Shakespeare himself. This context is obvious when the rest of the stanza is included.

Against the Oxford theory are several references to Shakespeare, later than 1604, which imply that the author was then still alive. Scholars point to a poem written circa 1620 by a student at Oxford, William Basse, that mentioned the author Shakespeare died in 1616, which is the year Shakespeare deceased and not Edward de Vere.

Tom Veal has noted that the early play "The Two Gentlemen of Verona" reveals no familiarity on the playwright's part with Italy other than "a few place names and the scarcely recondite fact that the inhabitants were Roman Catholics." For example, the play's Verona is situated on a tidal river and has a duke, and none of the characters have distinctly Italian names like in the later plays. Therefore, if the play was written by Oxford, it must have been before he visited Italy in 1575. However, the play's principal source, the Spanish "Diana Enamorada", would not be translated into French or English until 1578, meaning that someone basing a play on it that early could only have read it in the original Spanish, and there is no evidence that Oxford spoke this language. Furthermore, Veal argues, the only explanation for the verbal parallels with the English translation of 1582 would be that the translator saw the play performed and echoed it in his translation, which he describes as "not an impossible theory but far from a plausible one."

The composition date of "Hamlet" has been frequently disputed. Several surviving references indicate that a Hamlet-like play was well-known throughout the 1590s, well before the traditional period of composition (1599–1601). Most scholars refer to this lost early play as the Ur-Hamlet; the earliest reference is in 1589. A 1594 performance record of "Hamlet" appears in Philip Henslowe's diary, and Thomas Lodge wrote of it in 1596.

Oxfordian researchers believe that the play is an early version of Shakespeare's own play, and point to the fact that Shakespeare's version survives in three quite different early texts, Q1 (1603), Q2 (1604) and F (1623), suggesting the possibility that it was revised by the author over a period of many years.

Scholars contend that the composition date of "Macbeth" is one of the most overwhelming pieces of evidence against the Oxfordian position; the vast majority of critics believe the play was written in the aftermath of the Gunpowder Plot. This plot was brought to light on 5 November 1605, a year after Oxford died. In particular, scholars identify the porter's lines about "equivocation" and treason as an allusion to the trial of Henry Garnet in 1606. Oxfordians respond that the concept of "equivocation" was the subject of a 1583 tract by Queen Elizabeth's chief councillor (and Oxford's father-in-law) Lord Burghley, as well as of the 1584 "Doctrine of Equivocation" by the Spanish prelate Martín de Azpilcueta, which was disseminated across Europe and into England in the 1590s.

Shakespearian scholar David Haley asserts that if Edward de Vere had written "Coriolanus", he "must have foreseen the Midland Revolt grain riots [of 1607] reported in Coriolanus", possible topical allusions in the play that most Shakespearians accept.

The play that can be dated within a fourteen-month period is The Tempest. This play has long been believed to have been inspired by the 1609 wreck at Bermuda, then feared by mariners as the "Isle of the Devils", of the flagship of the Virginia Company, the Sea Venture, while leading the Third Supply to relieve Jamestown in the Colony of Virginia. The Sea Venture was captained by Christopher Newport, and carried the Admiral of the company's fleet, Sir George Somers (for whom the archipelago would subsequently be named "The Somers Isles"). The survivors spent nine months in Bermuda before most completed the journey to Jamestown on 23 May 1610 aboard two new ships built from scratch. One of the survivors was the newly-appointed Governor, Sir Thomas Gates. Jamestown, then little more than a rudimentary fort, was found in such a poor condition, with the majority of the previous settlers dead or dying, that Gates and Somers decided to abandon the settlement and the continent, returning everyone to England. However, with the company believing all aboard the Sea Venture dead, a new governor, Baron De La Warr, had been sent with the Fourth Supply fleet, which arrived on 10 June 1610 as Jamestown was being abandoned.

De la Warr remained in Jamestown as Governor, while Gates returned to England (and Somers to Bermuda), arriving in September, 1610. The news of the survival of the Sea Venture's passengers and crew caused a great sensation in England. Two accounts were published: Sylvester Jordain's "A Discovery of the Barmvdas, Otherwise Called the Ile of Divels", in October, 1610, and "A True Declaration of the Estate of the Colonie in Virginia" a month later. The "True Reportory of the Wrack, and Redemption of Sir Thomas Gates Knight", an account by William Strachey dated 15 July 1610, returned to England with Gates in the form of a letter which was circulated privately until its eventual publication in 1625. Shakespeare had multiple contacts to the circle of people amongst whom the letter circulated, including to Strachey. "The Tempest" shows clear evidence that he had read and relied on Jordain and especially Strachey. The play shares premise, basic plot, and many details of the Sea Venture's wrecking and the adventures of the survivors, as well as specific details and linguistics. A detailed comparative analysis shows the "Declaration" to have been the primary source from which the play was drawn. This firmly dates the writing of the play to the months between Gates' return to England and 1 November 1611.

Oxfordians have dealt with this problem in several ways. Looney expelled the play from the canon, arguing that its style and the "dreary negativism" it promoted were inconsistent with Shakespeare's "essentially positivist" soul, and so could not have been written by Oxford. Later Oxfordians have generally abandoned this argument; this has made severing the connection of the play with the wreck of the Sea Venture a priority amongst Oxfordians. A variety of attacks have been directed on the links. These include attempting to cast doubt on whether the "Declaration" travelled back to England with Gates, whether Gates travelled back to England early enough, whether the lowly Shakespeare would have had access to the lofty circles in which the "Declaration" was circulated, to understating the points of similarity between the Sea Venture wreck and the accounts of it, on the one hand, and the play on the other. Oxfordians have even claimed that the writers of the first-hand accounts of the real wreck based them on "The Tempest", or, at least, the same antiquated sources that Shakespeare, or rather Oxford, is imagined to have used exclusively, including Richard Eden's "The Decades of the New Worlde Or West India" (1555) and Desiderius Erasmus's "Naufragium"/"The Shipwreck" (1523). Alden Vaughan commented in 2008 that "[t]he argument that Shakespeare could have gotten every thematic thread, every detail of the storm, and every similarity of word and phrase from other sources stretches credulity to the limits."

Oxfordians note that while the conventional dating for "Henry VIII" is 1610–13, the majority of 18th and 19th century scholars, including notables such as Samuel Johnson, Lewis Theobald, George Steevens, Edmond Malone, and James Halliwell-Phillipps, placed the composition of "Henry VIII" prior to 1604, as they believed Elizabeth's execution of Mary, Queen of Scots (the then king James I's mother) made any vigorous defence of the Tudors politically inappropriate in the England of James I. Though it is described as a new play by two witnesses in 1613, Oxfordians argue that this refers to the fact it was new on stage, having its first production in that year.

Although searching Shakespeare's works for encrypted clues supposedly left by the true author is associated mainly with the Baconian theory, such arguments are often made by Oxfordians as well. Early Oxfordians found many references to Oxford's family name "Vere" in the plays and poems, in supposed puns on words such as "ever" (E. Vere). In "The De Vere Code", a book by English actor Jonathan Bond, the author believes that Thomas Thorpe´s 30-word dedication to the original publication of Shakespeare's Sonnets contains six simple encryptions which conclusively establish de Vere as the author of the poems. He also writes that the alleged encryptions settle the question of the identity of "the Fair Youth" as Henry Wriothesley and contain striking references to the sonnets themselves and de Vere's relationship to Sir Philip Sidney and Ben Jonson.

Similarly, a 2009 article in the Oxfordian journal "Brief Chronicles" noted that Francis Meres, in "Palladis Tamia" compares 17 named English poets to 16 named classical poets. Writing that Meres was obsessed with numerology, the authors propose that the numbers should be symmetrical, and that careful readers are meant to infer that Meres knew two of the English poets (viz., Oxford and Shakespeare) to actually be one and the same.

Literary scholars say that the idea that an author's work must reflect his or her life is a Modernist assumption not held by Elizabethan writers, and that biographical interpretations of literature are unreliable in attributing authorship. Further, such lists of similarities between incidents in the plays and the life of an aristocrat are flawed arguments because similar lists have been drawn up for many competing candidates, such as Francis Bacon and William Stanley, 6th Earl of Derby. Harold Love writes that "The very fact that their application has produced so many rival claimants demonstrates their unreliability," and Jonathan Bate writes that the Oxfordian biographical method "is in essence no different from the cryptogram, since Shakespeare's range of characters and plots, both familial and political, is so vast that it would be possible to find in the plays 'self-portraits' of ... anybody one cares to think of."

Despite this, Oxfordians list numerous incidents in Oxford's life that they say parallel those in many of the Shakespeare plays. Most notable among these, they say, are certain similar incidents found in Oxford's biography and "Hamlet", and "Henry IV, Part 1", which includes a well-known robbery scene with uncanny parallels to a real-life incident involving Oxford.

Most Oxfordians consider Hamlet the play most easily seen as portraying Oxford's life story, though mainstream scholars say that incidents from the lives of other contemporary figures such as King James or the Earl of Essex, fit the play just as closely, if not more so.

Hamlet's father was murdered and his mother made an "o'er-hasty marriage" less than two months later. Oxfordians see a parallel with Oxford's life, as Oxford's father died at the age of 46 on 3 August 1562, although not before making a will six days earlier, and his stepmother remarried within 15 months, although exactly when is unknown.

Another frequently-cited parallel involves Hamlet's revelation in Act IV that he was earlier taken captive by pirates. On Oxford's return from Europe in 1576, he encountered a cavalry division outside of Paris that was being led by a German duke, and his ship was hijacked by pirates who robbed him and left him stripped to his shirt, and who might have murdered him had not one of them recognised him. Anderson notes that "[n]either the encounter with Fortinbras' army nor Hamlet's brush with buccaneers appears in any of the play's sources – to the puzzlement of numerous literary critics."

Such speculation often identifies the character of Polonius as a caricature of Lord Burghley, Oxford's guardian from the age of 12.

In the First Quarto the character was not named Polonius, but Corambis. Ogburn writes that "Cor ambis" can be interpreted as "two-hearted" (a view not independently supported by Latinists). He says the name is a swipe "at Burghley's motto, "Cor unum, via una", or 'one heart, one way.'" Scholars suggest that it derives from the Latin phrase "crambe repetita" meaning "reheated cabbage", which was expanded in Elizabethan usage to ""Crambe bis" posita mors est" ("twice served cabbage is deadly"), which implies "a boring old man" who spouts trite rehashed ideas. Similar variants such as "Crambo" and "Corabme" appear in Latin-English dictionaries at the time.

In his "Memoires" (1658), Francis Osborne writes of "the last great "Earle of Oxford", whose "Lady" was brought to his bed under the notion of his "Mistris", and from such a virtuous deceit she (Oxford's youngest daughter) is said to proceed" (p. 79).

Such a bed trick has been a dramatic convention since antiquity and was used more than 40 times by every major playwright in the Early Modern theatre era except for Ben Jonson. Thomas Middleton used it five times and Shakespeare and James Shirley used it four times. Shakespeare's use of it in "All's Well That Ends Well" and "Measure for Measure" followed his sources for the plays (stories by Boccaccio and Cinthio); nevertheless Oxfordians say that de Vere was drawn to these stories because they "paralleled his own", based on Osborne's anecdote.

Oxfordians claim that flattering treatment of Oxford's ancestors in Shakespeare's history plays is evidence of his authorship. Shakespeare omitted the character of the traitorous Robert de Vere, 3rd Earl of Oxford in "The Life and Death of King John", and the character of the 12th Earl of Oxford is given a much more prominent role in "Henry V" than his limited involvement in the actual history of the times would allow. The 12th Earl is given an even more prominent role in the non-Shakespearian play "The Famous Victories of Henry the fifth". Some Oxfordians argue that this was another play written by Oxford, based on the exaggerated role it gave to the 11th Earl of Oxford.

J. Thomas Looney found John de Vere, 13th Earl of Oxford is "hardly mentioned except to be praised" in "Henry VI, Part Three"; the play ahistorically depicts him participating in the Battle of Tewkesbury and being captured. Oxfordians, such as Dorothy and Charlton Ogburn, believe Shakespeare created such a role for the 13th Earl because it was the easiest way Edward de Vere could have "advertised his loyalty to the Tudor Queen" and remind her of "the historic part borne by the Earls of Oxford in defeating the usurpers and restoring the Lancastrians to power". Looney also notes that in "Richard III", when the future Henry VII appears, the same Earl of Oxford is "by his side; and it is Oxford who, as premier nobleman, replies first to the king's address to his followers".

Non-Oxfordian writers do not see any evidence of partiality for the de Vere family in the plays. Richard de Vere, 11th Earl of Oxford, who plays a prominent role in the anonymous "The Famous Victories of Henry V", does not appear in Shakespeare's "Henry V", nor is he even mentioned. In "Richard III", Oxford's reply to the king noted by Looney is a mere two lines, the only lines he speaks in the play. He has a much more prominent role in the non-Shakespearian play "The True Tragedy of Richard III". On these grounds the scholar Benjamin Griffin argues that the non-Shakespearian plays, the "Famous Victories" and "True Tragedy", are the ones connected to Oxford, possibly written for Oxford's Men. Oxfordian Charlton Ogburn Jr. argues that the role of the Earls of Oxford was played down in "Henry V" and "Richard III" to maintain Oxford's nominal anonymity. This is because "It would not do to have a performance of one of his plays at Court greeted with ill-suppressed knowing chuckles."

In 1577 the Company of Cathay was formed to support Martin Frobisher’s hunt for the Northwest Passage, although Frobisher and his investors quickly became distracted by reports of gold at Hall’s Island. With thoughts of an impending Canadian gold-rush and trusting in the financial advice of Michael Lok, the treasurer of the company, de Vere signed a bond for £3,000 in order to invest £1,000 and to assume £2,000 worth – about half – of Lok's personal investment in the enterprise. Oxfordians say this is similar to Antonio in "The Merchant of Venice", who was indebted to Shylock for 3,000 ducats against the successful return of his vessels.

Oxfordians also note that when de Vere travelled through Venice, he borrowed 500 crowns from a Baptista Nigrone. In Padua, he borrowed from a man named Pasquino Spinola. In "The Taming of the Shrew", Kate's father is described as a man "rich in crowns." He, too, is from Padua, and his name is Baptista Minola, which Oxfordians take to be a conflation of Baptista Nigrone and Pasquino Spinola.

When the character of Antipholus of Ephesus in "The Comedy of Errors" tells his servant to go out and buy some rope, the servant (Dromio) replies, "I buy a thousand pounds a year! I buy a rope!" (Act 4, scene 1). The meaning of Dromio’s line has not been satisfactorily explained by critics, but Oxfordians say the line is somehow connected to the fact that de Vere was given a £1,000 annuity by the Queen, later continued by King James.

Oxfordians see Oxford's marriage to Anne Cecil, Lord Burghley's daughter, paralleled in such plays as "Hamlet", "Othello", "Cymbeline", "The Merry Wives of Windsor", "All's Well That Ends Well", "Measure for Measure", "Much Ado About Nothing", and "The Winter's Tale".

Oxford's illicit congress with Anne Vavasour resulted in an intermittent series of street battles between the Knyvet clan, led by Anne's uncle, Sir Thomas Knyvet, and Oxford’s men. As in "Romeo and Juliet", this imbroglio produced three deaths and several other injuries. The feud was finally put to an end only by the intervention of the Queen.

In May 1573, in a letter to Lord Burghley, two of Oxford's former employees accused three of Oxford's friends of attacking them on "the highway from Gravesend to Rochester." In Shakespeare's "Henry IV, Part 1", Falstaff and three roguish friends of Prince Hal also waylay unwary travellers at Gad's Hill, which is on the highway from Gravesend to Rochester. Scott McCrea says that there is little similarity between the two events, since the crime described in the letter is unlikely to have occurred near Gad's Hill and was not a robbery, but rather an attempted shooting. Mainstream writers also say that this episode derives from an earlier anonymous play, "The Famous Victories of Henry V", which was Shakespeare's source. Some Oxfordians argue that "The Famous Victories" was written by Oxford, based on the exaggerated role it gave to the 11th Earl of Oxford.

In 1609, a volume of 154 linked poems was published under the title "SHAKE-SPEARES SONNETS". Oxfordians believe the title ("Shake-Speares Sonnets") suggests a finality indicating that it was a completed body of work with no further sonnets expected, and consider the differences of opinion among Shakespearian scholars as to whether the Sonnets are fictional or autobiographical to be a serious problem facing orthodox scholars. Joseph Sobran questions why Shakespeare (who lived until 1616) failed to publish a corrected and authorised edition if they are fiction, as well as why they fail to match Shakespeare's life story if they are autobiographic. According to Sobran and other researchers, the themes and personal circumstances expounded by the author of the Sonnets are remarkably similar to Oxford's biography.

The focus of the 154 sonnet series appears to narrate the author's relationships with three characters: the Fair Youth, the Dark Lady or Mistress, and the Rival Poet. Beginning with Looney, most Oxfordians (exceptions are Percy Allen and Louis Bénézet) believe that the "Fair Youth" referred to in the early sonnets refers to Henry Wriothesley, 3rd Earl of Southampton, Oxford's peer and prospective son-in-law. The Dark Lady is believed by some Oxfordians to be Anne Vavasour, Oxford's mistress who bore him a son out of wedlock. A case was made by the Oxfordian Peter R. Moore that the Rival Poet was Robert Devereux, Earl of Essex.

Sobran suggests that the so-called procreation sonnets were part of a campaign by Burghley to persuade Southampton to marry his granddaughter, Oxford's daughter Elizabeth de Vere, and says that it was more likely that Oxford would have participated in such a campaign than that Shakespeare would know the parties involved or presume to give advice to the nobility.

Oxfordians also assert that the tone of the poems is that of a nobleman addressing an equal rather than that of a poet addressing his patron. According to them, Sonnet 91 (which compares the Fair Youth's love to such treasures as high birth, wealth, and horses) implies that the author is in a position to make such comparisons, and the 'high birth' he refers to is his own.

Oxford was born in 1550, and was between 40 and 53 years old when he presumably would have written the sonnets. Shakespeare was born in 1564. Even though the average life expectancy of Elizabethans was short, being between 26 and 39 was not considered old. In spite of this, age and growing older are recurring themes in the Sonnets, for example, in Sonnets 138 and 37. In his later years, Oxford described himself as "lame". On several occasions, the author of the sonnets also described himself as lame, such as in Sonnets 37 and 89.

Sobran also believes "scholars have largely ignored one of the chief themes of the Sonnets: the poet's sense of disgrace ... [T]here can be no doubt that the poet is referring to something real that he expects his friends to know about; in fact, he makes clear that a wide public knows about it ... Once again the poet's situation matches Oxford's ... He has been a topic of scandal on several occasions. And his contemporaries saw the course of his life as one of decline from great wealth, honor, and promise to disgrace and ruin. This perception was underlined by enemies who accused him of every imaginable offense and perversion, charges he was apparently unable to rebut." Examples include Sonnets 29 and 112.

As early as 1576, Edward de Vere was writing about this subject in his poem "Loss of Good Name", which Steven W. May described as "a defiant lyric without precedent in English Renaissance verse."

The poems "Venus and Adonis" and "Lucrece", first published in 1593 and 1594 under the name "William Shakespeare", proved highly popular for several decades – with "Venus and Adonis" published six more times before 1616, while "Lucrece" required four additional printings during this same period. By 1598, they were so famous, London poet and sonneteer Richard Barnefield wrote:

<poem>Shakespeare...
Whose "Venus" and whose "Lucrece" (sweet and chaste)
Thy name in fame's immortal Book have plac't
Live ever you, at least in Fame live ever:
Well may the Body die, but Fame dies never.</poem>

Despite such publicity, Sobran observed, "[t]he author of the Sonnets expects and hopes to be forgotten. While he is confident that his poetry will outlast marble and monument, it will immortalize his young friend, not himself. He says that his style is so distinctive and unchanging that 'every word doth almost tell my name,' implying that his name is otherwise concealed – at a time when he is publishing long poems under the name William Shakespeare. This seems to mean that he is not writing these Sonnets under that (hidden) name." Oxfordians have interpreted the phrase "every word" as a pun on the word "every", standing for "e vere" – thus telling his name. Mainstream writers respond that several sonnets literally do tell his name, containing numerous puns on the name Will[iam]; in sonnet 136 the poet directly says "thou lov'st me for my name is Will."

Based on Sonnets 81, 72, and others, Oxfordians assert that if the author expected his "name" to be "forgotten" and "buried", it would not have been the name that permanently adorned the published works themselves.



The UK and US editions of differ significantly in pagination. The citations to the book used in this article list the UK page numbers first, followed by the page numbers of the US edition in parentheses.






</doc>
<doc id="22677" url="https://en.wikipedia.org/wiki?curid=22677" title="Oxymoron">
Oxymoron

An oxymoron (usual plural oxymorons, more rarely oxymora) is a rhetorical device that uses an ostensible self-contradiction to illustrate a rhetorical point or to reveal a paradox.
A more general meaning of "contradiction in terms" (not necessarily for rhetoric effect) is recorded by the "OED" for 1902..

The term is first recorded as latinized Greek ', in Maurus Servius Honoratus (c. AD 400); it is derived from the Greek ' "sharp, keen, pointed" and "dull, stupid, foolish"; as it were, "sharp-dull", "keenly stupid", or "pointedly foolish". The word "oxymoron" is autological, i.e. it is itself an example of an oxymoron. The Greek compound word "", which would correspond to the Latin formation, does not seem to appear in any known Ancient Greek works prior to the formation of the Latin term.

Oxymorons in the narrow sense are a rhetorical device used deliberately by the speaker, and intended to be understood as such by the listener.
In a more extended sense, the term "oxymoron" has also been applied to inadvertent or incidental contradictions, as in the case of "dead metaphors" ("barely clothed" or "terribly good"). Lederer (1990), in the spirit of "recreational linguistics", goes as far as to construct "logological oxymorons" such as reading the word "nook" as composed of "no" and "ok" or the surname "Noyes" as composed of "no" plus "yes", or far-fetched punning such as "divorce court", "U.S. Army Intelligence" or "press release".
There are a number of single-word oxymorons built from "dependent morphemes" (i.e. no longer a productive compound in English, but loaned as a compound from a different language), as with "pre-posterous" (lit. "with the hinder part before", compare "husteron proteron", "upside-down", "head over heels", ass-backwards" etc.) or "sopho-more" (an artificial Greek compound, lit. "wise-foolish").

The most common form of oxymoron involves an adjective–noun combination of two words, but they can also be devised in the meaning of sentences or phrases.
One classic example of the use of oxymorons in English literature can be found in this example from Shakespeare's "Romeo and Juliet", where Romeo strings together thirteen in a row:
Shakespeare heaps up many more oxymorons in "Romeo and Juliet" in particular ("Beautiful tyrant! fiend angelical! Dove-feather'd raven! wolvish-ravening lamb! Despised substance of divinest show!" etc.) and uses them in other plays, e.g. "I must be cruel only to be kind" ("Hamlet"), "fearful bravery" ("Julius Caesar"), "good mischief" ("The Tempest"), and in his sonnets, e.g. "tender churl", "gentle thief".
Other examples from English-language literature include:
"hateful good" (Chaucer, translating "odibile bonum")
"proud humility" (Spenser),
"darkness visible" (Milton), 
"beggarly riches" (John Donne),
"damn with faint praise" (Pope),
"expressive silence" (Thomson, echoing Cicero's ), 
"melancholy merriment" (Byron),
"faith unfaithful", "falsely true" (Tennyson),
"conventionally unconventional", "tortuous spontaneity" (Henry James) 
"delighted sorrow", "loyal treachery", "scalding coolness" (Hemingway).

In literary contexts, the author does not usually signal the use of an oxymoron, but in rhetorical usage, it has become common practice to advertise the use of an oxymoron explicitly to clarify the argument, as in:
In this example, "Epicurean pessimist" would be recognized as an oxymoron in any case, as the core tenet of Epicureanism is equanimity (which would preclude any sort of pessimist outlook). However, the explicit advertisement of the use of oxymorons opened up a sliding scale of less than obvious construction, ending in the "opinion oxymorons" such as "business ethics".

J. R. R. Tolkien interpreted his own surname as derived from the Low German equivalent of "dull-keen" (High German "") which would be a literal equivalent of Greek "oxy-moron".

"Comical oxymoron" is a term for the claim, for comical effect, that a certain phrase or expression is an oxymoron (called "opinion oxymorons" by Lederer (1990)).
The humour derives from implying that an assumption (which might otherwise be expected to be controversial or at least non-evident) is so obvious as to be part of the lexicon. 
An example of such a "comical oxymoron" is "educational television": the humour derives entirely from the claim that it is an oxymoron by the implication that "television" is so trivial as to be inherently incompatible with "education".
In a 2009 article called "Daredevil", Garry Wills accused William F. Buckley of popularising this trend, based on the success of the latter's claim that "an intelligent liberal is an oxymoron."

Examples popularized by comedian George Carlin in 1975 include "military intelligence" (a play on the lexical meanings of the term "intelligence", implying that "military" inherently excludes the presence of "intelligence") and "business ethics" (similarly implying that the mutual exclusion of the two terms is evident or commonly understood rather than the partisan anti-corporate position).

Similarly, the term "civil war" is sometimes jokingly referred to as an "oxymoron" (punning on the lexical meanings of the word "civil").

Other examples include "healthful Mexican food" (1989), "affordable caviar" (1993), and "Microsoft Works" (2000).

Listing of antonyms, such as "good and evil", "male and female", "great and small", etc., does not create oxymorons, as it is not implied that any given object has the two opposing properties simultaneously.
In some languages, it is not necessary to place a conjunction like "and" between the two antonyms; such compounds (not necessarily of antonyms) are known as dvandvas (a term taken from Sanskrit grammar). 
For example, in Chinese, compounds like 男女 (man and woman, male and female, gender), 陰陽 (yin and yang), 善惡 (good and evil, morality) are used to indicate couples, ranges, or the trait that these are extremes of.
The Italian "pianoforte" or "fortepiano" is an example from a Western language; the term is short for "gravicembalo col piano e forte", as it were "harpiscord with a range of different volumes", implying that it is possible to play both soft and loud (as well as intermediate) notes, not that the sound produced is somehow simultaneously "soft and loud".





</doc>
<doc id="22678" url="https://en.wikipedia.org/wiki?curid=22678" title="OSS">
OSS

OSS may refer to:









</doc>
<doc id="22679" url="https://en.wikipedia.org/wiki?curid=22679" title="Office of Strategic Services">
Office of Strategic Services

The Office of Strategic Services (OSS) was a wartime intelligence agency of the United States during World War II, and a predecessor of the modern Central Intelligence Agency (CIA). The OSS was formed as an agency of the Joint Chiefs of Staff (JCS) to coordinate espionage activities behind enemy lines for all branches of the United States Armed Forces. Other OSS functions included the use of propaganda, subversion, and post-war planning. On December 14, 2016, the organization was collectively honored with a Congressional Gold Medal.

Prior to the formation of the OSS, the various departments of the executive branch, including the State, Treasury, Navy, and War Departments conducted American intelligence activities on an "ad hoc" basis, with no overall direction, coordination, or control. The US Army and US Navy had separate code-breaking departments: Signal Intelligence Service and OP-20-G. (A previous code-breaking operation of the State Department, the MI-8, run by Herbert Yardley, had been shut down in 1929 by Secretary of State Henry Stimson, deeming it an inappropriate function for the diplomatic arm, because "gentlemen don't read each other's mail".) The FBI was responsible for domestic security and anti-espionage operations.

President Franklin D. Roosevelt was concerned about American intelligence deficiencies. On the suggestion of William Stephenson, the senior British intelligence officer in the western hemisphere, Roosevelt requested that William J. Donovan draft a plan for an intelligence service based on the British Secret Intelligence Service (MI6) and Special Operations Executive (SOE). After submitting his work, "Memorandum of Establishment of Service of Strategic Information," Colonel Donovan was appointed "coordinator of information" on July 11, 1941, heading the new organization known as the office of the Coordinator of Information (COI). Thereafter the organization was developed with British assistance; Donovan had responsibilities but no actual powers and the existing US agencies were skeptical if not hostile. Until some months after Pearl Harbor, the bulk of OSS intelligence came from the UK. British Security Coordination (BSC) trained the first OSS agents in Canada, until training stations were set up in the US with guidance from BSC instructors, who also provided information on how the SOE was arranged and managed. The British immediately made available their short-wave broadcasting capabilities to Europe, Africa, and the Far East and provided equipment for agents until American production was established.

The Office of Strategic Services was established by a Presidential military order issued by President Roosevelt on June 13, 1942, to collect and analyze strategic information required by the Joint Chiefs of Staff and to conduct special operations not assigned to other agencies. During the war, the OSS supplied policymakers with facts and estimates, but the OSS never had jurisdiction over all foreign intelligence activities. The FBI was left responsible for intelligence work in Latin America, and the Army and Navy continued to develop and rely on their own sources of intelligence.

For the duration of World War II, the Office of Strategic Services was conducting multiple activities and missions, including collecting intelligence by spying, performing acts of sabotage, waging propaganda war, organizing and coordinating anti-Nazi resistance groups in Europe, and providing military training for anti-Japanese guerrilla movements in Asia, among other things. At the height of its influence during World War II, the OSS employed almost 24,000 people.

From 1943–1945, the OSS played a major role in training Kuomintang troops in China and Burma, and recruited Kachin and other indigenous irregular forces for sabotage as well as guides for Allied forces in Burma fighting the Japanese Army. Among other activities, the OSS helped arm, train, and supply resistance movements in areas occupied by the Axis powers during World War II, including Mao Zedong's Red Army in China (known as the Dixie Mission) and the Viet Minh in French Indochina. OSS officer Archimedes Patti played a central role in OSS operations in French Indochina and met frequently with Ho Chi Minh in 1945.

One of the greatest accomplishments of the OSS during World War II was its penetration of Nazi Germany by OSS operatives. The OSS was responsible for training German and Austrian individuals for missions inside Germany. Some of these agents included exiled communists and Socialist party members, labor activists, anti-Nazi prisoners-of-war, and German and Jewish refugees. The OSS also recruited and ran one of the war's most important spies, the German diplomat Fritz Kolbe.
In 1943, the Office of Strategic Services set up operations in Istanbul. Turkey, as a neutral country during the Second World War, was a place where both the Axis and Allied powers had spy networks. The railroads connecting central Asia with Europe, as well as Turkey's close proximity to the Balkan states, placed it at a crossroads of intelligence gathering. The goal of the OSS Istanbul operation called Project Net-1 was to infiltrate and extenuate subversive action in the old Ottoman and Austro-Hungarian Empires.

The head of operations at OSS Istanbul was a banker from Chicago named Lanning "Packy" Macfarland, who maintained a cover story as a banker for the American lend-lease program. Macfarland hired Alfred Schwarz, a Czechoslovakian engineer and businessman who came to be known as "Dogwood" and ended up establishing the Dogwood information chain. Dogwood in turn hired a personal assistant named Walter Arndt and established himself as an employee of the Istanbul Western Electrik Kompani. Through Schwartz and Arndt the OSS was able to infiltrate anti-fascist groups in Austria, Hungary, and Germany. Schwartz was able to convince Romanian, Bulgarian, Hungarian, and Swiss diplomatic couriers to smuggle American intelligence information into these territories and establish contact with elements antagonistic to the Nazis and their collaborators. Couriers and agents memorized information and produced analytical reports; when they were not able to memorize effectively they recorded information on microfilm and hid it in their shoes or hollowed pencils. Through this process information about the Nazi regime made its way to Macfarland and the OSS in Istanbul and eventually to Washington.

While the OSS "Dogwood-chain" produced a lot of information, its reliability was increasingly questioned by British intelligence. By May 1944, through collaboration between the OSS, British intelligence, Cairo, and Washington, the entire Dogwood-chain was found to be unreliable and dangerous. Planting phony information into the OSS was intended to misdirect the resources of the Allies. Schwartz's Dogwood-chain, which was the largest American intelligence gathering tool in occupied territory, was shortly thereafter shut down.

The OSS purchased Soviet code and cipher material (or Finnish information on them) from émigré Finnish army officers in late 1944. Secretary of State Edward Stettinius, Jr., protested that this violated an agreement President Roosevelt made with the Soviet Union not to interfere with Soviet cipher traffic from the United States. General Donovan might have copied the papers before returning them the following January, but there is no record of Arlington Hall receiving them, and CIA and NSA archives have no surviving copies. This codebook was in fact used as part of the Venona decryption effort, which helped uncover large-scale Soviet espionage in North America.

The OSS espionage and sabotage operations produced a steady demand for highly specialized equipment. General Donovan invited experts, organized workshops, and funded labs that later formed the core of the Research & Development Branch. Boston chemist Stanley P. Lovell became its first head, and Donovan humorously called him his "Professor Moriarty". Throughout the war years, the OSS Research & Development successfully adapted Allied weapons and espionage equipment, and produced its own line of novel spy tools and gadgets, including silenced pistols, lightweight sub-machine guns, "Beano" grenades that exploded upon impact, explosives disguised as lumps of coal ("Black Joe") or bags of Chinese flour ("Aunt Jemima"), acetone time delay fuses for limpet mines, compasses hidden in uniform buttons, playing cards that concealed maps, a 16mm Kodak camera in the shape of a matchbox, tasteless poison tablets ("K" and "L" pills), and cigarettes laced with tetrahydrocannabinol acetate (an extract of Indian hemp) to induce uncontrollable chattiness.

The OSS also developed innovative communication equipment such as wiretap gadgets, electronic beacons for locating agents, and the "Joan-Eleanor" portable radio system that made it possible for operatives on the ground to establish secure contact with a plane that was preparing to land or drop cargo. The OSS Research & Development also printed fake German and Japanese-issued identification cards, and various passes, ration cards, and counterfeit money.

On August 28, 1943, Stanley Lovell was asked to make a presentation in front of a not very friendly audience of the Joint Chiefs of Staff, since the U.S. top brass were largely skeptical of all OSS plans beyond collecting military intelligence and were ready to split the OSS between the Army and the Navy. While explaining the purpose and mission of his department and introducing various gadgets and tools, he reportedly casually dropped into a waste basket a Hedy, a panic-inducing explosive device in the shape of a firecracker, which shortly produced a loud shrieking sound followed by a deafening boom. The presentation was interrupted and did not resume since everyone in the room fled. In reality, the Hedy, jokingly named after Hollywood movie star Hedy Lamarr for her ability to distract men, later saved the lives of some trapped OSS operatives.

Not all projects worked. Some ideas were odd, such as producing pathogenic synthetic goat dung in PROJECT Capricious (1942) to spread anthrax by using flies among German troops in Spanish Morocco to prevent Spain from joining the Axis powers. Donovan was not informed about PROJECT Capricious due to its uttermost secrecy; the Germans eventually departed Spain and Operation Capricious was aborted. There were also ideas to introduce estrogen into Hitler's food to deprive him of his trademark mustache and—recognizable to all Germans—baritone voice. A more deadly plot included hiding a capsule with mustard gas in flowers to cause blindness among Nazi generals inside the German High Command Headquarters. Stanley Lovell was later quoted saying, "It was my policy to consider any method whatever that might aid the war, however unorthodox or untried".

In 1939, a young physician named Christian J. Lambertsen developed an oxygen rebreather set (the Lambertsen Amphibious Respiratory Unit) and demonstrated it to the OSS—after already being rejected by the U.S. Navy—in a pool at a hotel in Washington D.C., in 1942. The OSS not only bought into the concept, they hired Lambertsen to lead the program and build up the dive element for the organization. His responsibilities included training and developing methods of combining self-contained diving and swimmer delivery including the Lambertsen Amphibious Respiratory Unit for the OSS "Operational Swimmer Group". Growing involvement of the OSS with coastal infiltration and water-based sabotage eventually led to creation of the OSS Maritime Unit.

After victory in Europe in May 1945, the OSS was better able to concentrate on operations in Japan. Soon Japan surrendered, ending the Pacific Theater of Operations in World War II.

A month later, on September 20, 1945, President Truman signed Executive Order 9621, terminating the OSS. His Order became effective October 1, 1945. In the days following, the functions of the OSS were split between the Department of State and the Department of War. The State Department received the Research and Analysis Branch of OSS (originally created by Edward Mead Earle) which was renamed the Interim Research and Intelligence Service or IRIS, headed by U.S. Army Colonel Alfred McCormack. Later it was renamed the Bureau of Intelligence and Research by the State Department.

The War Department took over the Secret Intelligence (SI) and Counter-Espionage (X-2) Branches, which were then housed in a new unit created for this purpose—the Strategic Services Unit (SSU). The Secretary of War appointed Brigadier General John Magruder (formerly Donovan's Deputy Director for Intelligence in OSS) as the new SSU director. He oversaw the liquidation of the OSS and managed the institutional preservation of its clandestine intelligence capability.

In January 1946, President Truman created the Central Intelligence Group (CIG), which was the direct precursor to the CIA. SSU assets, which now constituted a streamlined "nucleus" of clandestine intelligence, were transferred to the CIG in mid-1946 and reconstituted as the Office of Special Operations (OSO). The National Security Act of 1947 established the first permanent peacetime intelligence agency in the United States, the Central Intelligence Agency, which then took up OSS functions. The direct descendant of the paramilitary component of the OSS is the CIA Special Activities Division.

Today, the joint-branch United States Special Operations Command, founded in 1987, uses the same spearhead design on its insignia, as homage to its indirect lineage.

Prince William Forest Park (then known as Chopawamsic Recreational Demonstration Area) was the site of an OSS training camp that operated from 1942 to 1945. Area "C", consisting of approximately , was used extensively for communications training, whereas Area "A" was used for training some of the OGs. Catoctin Mountain Park, now the location of Camp David, was the site of OSS training Area "B." Congressional Country Club (Area F) in Bethesda, MD was the primary OSS training facility.

The London branch of the OSS, its first overseas facility, was at 70 Grosvenor Street, W1. 
The Facilities of the Catalina Island Marine Institute at Toyon Bay on Santa Catalina Island, Calif., are composed (in part) of a former OSS survival training camp.

The National Park Service commissioned a study of OSS National Park training facilities by Professor John Chambers of Rutgers University.

At Camp X, near Whitby, Ontario, an "assassination and elimination" training program was operated by the British Special Operations Executive such as William E. Fairbairn and Eric A. Sykes. Many members of the US Office of Strategic Services also were trained there. It was dubbed "the school of mayhem and murder" by George Hunter White who trained at the facility in the 1950s.

The main OSS training camps abroad were located initially in Great Britain, French Algeria, and Egypt; later as the Allies advanced, a school was established in southern Italy. In the Far East, OSS training facilities were established in India, Ceylon, and then China. In addition to training local agents, the overseas OSS schools also provided advanced training and field exercises for graduates of the training camps in the United States and for Americans who enlisted in the OSS in the war zones. The most famous of the latter was Virginia Hall in France.

The OSS's Mediterranean training center in Cairo, Egypt, known to many as the "Spy School", was once a lavish palace belonging to King Farouk's brother-in-law. It was modeled after SOE's training facility STS 102 in Haifa, then Palestine. Americans whose heritage stemmed from Italy, Yugoslavia, and Greece were trained at the "Spy School" and also sent for parachute, weapons and commando training, and Morse code and encryption lessons at STS 102. After completion of their spy training, these agents were sent back on missions to the Balkans and Italy where their accents would not pose a problem for their assimilation.

The names of all OSS personnel and documents of their OSS service, previously a closely guarded secret, were released by the US National Archives on August 14, 2008. Among the 24,000 names were those of Julia Child, Ralph Bunche, Arthur Goldberg, Saul K. Padover, Arthur Schlesinger, Jr., Bruce Sundlun, Rene Joyeuse MD and John Ford. The 750,000 pages in the 35,000 personnel files include applications of people who were not recruited or hired, as well as the service records of those who were.

Major League Baseball player Moe Berg was recruited by the OSS in 1943 because of his language skills, was assigned to the Secret Intelligence branch, and took part in missions in the Caribbean, South America, France, England, Norway, Italy, and the Balkans. Later, Berg was briefed in nuclear physics, and sent to Zürich, Switzerland, posing as a Swiss physics student, with the mission of attending a lecture at the Technische Hochschule by Germany's top nuclear scientist, Werner Heisenberg. His orders were to kill the scientist if he determined that the Germans were far along in their efforts to build an atomic weapon; he found that the scientist was not a threat. Berg was awarded the Presidential Medal of Freedom, but declined to accept it as he was forbidden from saying what he had done to receive the award. He is the only former Major League Baseball player whose baseball card is displayed at CIA headquarters.

"Jumping Joe" Savoldi (code name Sampson) was recruited by the OSS in 1942 because of his hand-to-hand combat and language skills as well as his deep knowledge of the Italian geography and interior of Benito Mussolini's compound. He was assigned to the Special Operations branch and took part in missions in North Africa, Italy, and France during 1943–1945. A member of the McGregor Project (2677th Regiment APO 512), Savoldi, a former all-American running back at Notre Dame under coach Knute Rockne, and World Champion professional wrestler/originator of the flying "dropkick", took part in multiple missions behind enemy lines. His service in Special Operations included at least three successful, high-visibility operations, all within the McGregor Project and all under the assumed identity of Giuseppe DeLeo. The real Giuseppe DeLeo was a captain in the Italian Army who had been captured in North Africa during Operation Torch. Savoldi's first operation under the McGregor Mission, or McGregor Project (July 1943 – December 1943), was approved by President Roosevelt and overseen by Major General William J. "Wild Bill" Donovan, Secretary of the Navy Frank Knox, Captain Edward A. Hayes, Captain E.M. Zacharias, Col. William Eddy and led by John Shaheen. Savoldi worked with Lt. Mike Burke, Lt. John Ringling North, and Peter Tompkins, and his orders were to protect Marcello Girosi during time spent in Morocco, Algiers, Palermo, Messina, Stromboli, Calabria, Terracina, Maiori, Capri, Ischia, Salerno and Naples. The McGregor Mission secretly arranged the surrender of the entire Italian Naval Fleet (just prior to D-day Salerno) through direct contact with Rear Admiral Massimo Girosi, Commander of the Royal Italian Navy and brother of Marcello Girosi. On a larger scale, the mission also worked through the Girosi brothers in Italy (all royalists and loyal to King Umberto) to orchestrate the over-throw of Benito Mussolini and to set ground work for the subsequent armistice agreement that led to Pietro Badaglio's position as Prime Minister of Italy. During this period it is thought that Savoldi was also tasked with the protection of OSS leader General Donovan, during his meetings with General Patton in and around Palermo Sicily during July 10, 1943. Savoldi's second operation as part of the ongoing McGregor Mission was to extract secret documents along with torpedo scientist, professor Carlo Calosi and his wife out of Nazi-occupied Rome. Calosi was the inventor of the highly effective SIC trigger "Silvrifici Italiano Calosi" torpedo and the OSS knew about the Germans' intention to locate Calosi and kill him if he refused to help German scientists with weapons development. This mission, carried out by Joe Savoldi, Donald Downes, and Andre Pacatte, was successful as they located Calosi, extracted him from Rome, and transported him and his wife to the Amalfi Coast, the hills of Algiers, and eventually the U.S. Naval Torpedo Station at Newport, Rhode Island. Once in the USA, Calosi worked to reconstruct the SIC torpedoes and developed effective countermeasures later used against the Germans. During his third major mission, Savoldi worked undercover (as Giuseppe DeLeo) in Naples, Italy where he was credited with infiltrating the local mafia and helping to break up one of the largest black market operation in all of Italy. While undercover, Savoldi again assumed the identity of Giuseppe De Leo, but he worked in civilian clothing posing as a rogue operator. Savoldi spoke Italian in several dialects as well as French, Spanish, and some German, and his dangerous work behind enemy lines was highly regarded according to several, now declassified, documents. The 1946 book titled "Cloak and Dagger", written by Alastair MacBain and Corey Ford, includes an entire chapter titled, "The Saga of Jumping Joe", recounting Savoldi's participation in the McGregor Mission. 
One of the forefathers of today's commandos was Navy Lieutenant Jack Taylor. He was sequestered by the OSS early in the war and had a long career behind enemy lines.

Taro and Mitsu Yashima, both Japanese political dissidents who were imprisoned in Japan for protesting its regime, worked for the OSS in psychological warfare against the Japanese Empire.

Nisei Linguists

In late 1943, a representative from OSS visited the 442nd Infantry Regiment looking to recruit volunteers willing to undertake "extremely hazardous assignment." All selected were Nisei. The recruits were assigned to OSS Detachments 101 and 202, in the China-Burma-India Theater. "Once deployed, they were to interrogate prisoners, translate documents, monitor radio communications, and conduct covert operations... Detachment 101 and 102’s clandestine operations were extremely successful." 



Films

Television
Literature

Comics

Tabletop Roleplaying Games

Video games
In "" (2008), Dr. Peter McCain is an OSS spy.






</doc>
<doc id="22680" url="https://en.wikipedia.org/wiki?curid=22680" title="Oda Nobunaga">
Oda Nobunaga

The goal of national unification and a return to the comparative political stability of the earlier Muromachi period was widely shared by the multitude of autonomous "daimyōs" during the Sengoku period. Oda Nobunaga was the first for whom this goal seemed attainable. Nobunaga had gained control over most of Honshu (see map below) before his death during the 1582 Honnō-ji incident, a coup attempt executed by Nobunaga's vassal, Akechi Mitsuhide. It is not certain whether Nobunaga was killed in the attack or committed seppuku. The motivation behind Mitsuhide's betrayal was never revealed to anyone who survived the incident, and has been a subject of debate and conjecture ever since the incident.

Following the incident, Akechi Mitsuhide declared himself master over Nobunaga's domains, but was quickly defeated by Toyotomi Hideyoshi, who regained control of and greatly expanded the Oda holdings. Oda Nobunaga's successful subjugation of much of Honshu enabled the later successes of his allies Toyotomi Hideyoshi and Tokugawa Ieyasu toward the goal of national unification by subjugating local "daimyōs" under a hereditary shogunate, which was ultimately accomplished in 1603 when Tokugawa Ieyasu was granted the title of "shōgun" by Emperor Go-Yōzei following the successful Sekigahara Campaign of 1600. The nature of the succession of power through the three "daimyōs" is reflected in a well-known Japanese idiom: "Nobunaga pounds the national rice cake, Hideyoshi kneads it, and in the end Ieyasu sits down and eats it."

Oda Nobunaga was born on June 23, 1534, in the Owari domain, and was given the childhood name of . He was the second son of Oda Nobuhide, a deputy "shugo" (military governor) with land holdings in Owari Province. He is said to have been born in Nagoya Castle, although this is subject to debate. Through his childhood and early teenage years, he was well known for his bizarre behavior and received the name of . He was known to run around with other youths from the area, without any regard to his own rank in society. With the introduction of firearms into Japan, however, he became known for his fondness of tanegashima firearms.

In 1551, Oda Nobuhide died unexpectedly. Nobunaga was said to have acted outrageously during his funeral, throwing ceremonial incense at the altar. Hirate Masahide, a valuable mentor and retainer to Nobunaga, performed "seppuku" to startle Nobunaga into his obligations.

Although Nobunaga was Nobuhide's legitimate heir, some of the Oda clan were divided against him. Collecting about a thousand men, Nobunaga suppressed those members of his family who were hostile to his rule, including his younger brother, Oda Nobuyuki. Then in 1556, he destroyed a rival branch located in Kiyosu Castle. 

Although Nobuyuki and his supporters were still at large, Nobunaga took an army to Mino Province to aid Saitō Dōsan after Dōsan's son, Saitō Yoshitatsu, turned against him. The campaign failed, as Dōsan was killed in the Battle of Nagara-gawa, and Yoshitatsu became the new master of Mino in 1556.

Nobunaga then moved against Nobuyuki and his retainers, and had him killed. By 1559, Nobunaga had control of Owari Province through Gekokujo.

In 1557, Nobunaga's brother, Nobuyuki, was defeated in the Siege of Suemori by Ikeda Nobuteru.

In 1558, he protected Suzuki Shigeteru in the Siege of Terabe.

By 1559, Nobunaga had eliminated all opposition within the clan and Owari Province.

In 1560, Imagawa Yoshimoto gathered an army of 25,000 men and started his march toward Kyoto, with the pretext of aiding the frail Ashikaga shogunate. The Matsudaira clan of Mikawa Province also joined Yoshimoto's forces. Against this, the Oda clan could rally an army of only 2,000 to 3,000. Some of Nobunaga's advisers suggested "to stand a siege at Kiyosu". Nobunaga refused, stating that "only a strong offensive policy could make up for the superior numbers of the enemy", and calmly ordered a counterattack.

Nobunaga's scouts reported that Yoshimoto was resting at the narrow gorge of Dengaku-hazama, ideal for a surprise attack, and that the Imagawa army were celebrating their victories while Yoshimoto viewed the heads. Nobunaga moved towards Imagawa's camp, and set up a position some distance away. An array of flags and dummy troops made of straw and spare helmets gave the impression of a large host, while the real Oda army hurried round in a rapid march to get behind Yoshimoto's camp. The heat gave way to a terrific thunderstorm. As the Imagawa samurai sheltered from the rain Nobunaga deployed his troops, and when the storm ceased they charged down upon the enemy in the gorge, so suddenly that Yoshimoto thought a brawl had broken out among his men, only realizing it was an attack when two samurai charged up. One aimed a spear at him, which Yoshimoto deflected with his sword, but the second swung his blade and cut off Imagawa's head.

Rapidly weakening in the wake of this battle, the Imagawa clan no longer exerted control over the Matsudaira clan. In 1561, an alliance was forged between Oda Nobunaga and Matsudaira Motoyasu (who would become Tokugawa Ieyasu), despite the decades-old hostility between the two clans. Nobunaga also formed an alliance with Takeda Shingen through the marriage of his daughter to Shingen's son. A similar relationship was forged when Nobunaga's sister Oichi married Azai Nagamasa of Ōmi Province.

Tradition dates this battle as the first time that Nobunaga noticed the talents of the sandal-bearer who would eventually become Toyotomi Hideyoshi.

In Mino, Saitō Yoshitatsu died suddenly of illness in 1561, and was succeeded by his son, Saitō Tatsuoki. Tatsuoki, however, was young and much less effective as a ruler and military strategist compared to his father and grandfather.

Taking advantage of this situation, Nobunaga moved his base to Komaki Castle and started his campaign in Mino at the 1561 Battle of Moribe. By convincing Saitō retainers to abandon their incompetent and foolish master, Nobunaga weakened the Saitō clan significantly, eventually mounting a final attack in 1567 when he captured Inabayama Castle.

After taking possession of the castle, Nobunaga changed the name of both the castle and the surrounding town to Gifu. Remains of Nobunaga's residence in Gifu can be found today in Gifu Park. Naming it after the legendary Mount Qi (岐山 "Qi" in Standard Chinese) in China, on which the Zhou dynasty started, Nobunaga revealed his ambition to conquer the whole of Japan. He also started using a new personal seal that read "Tenka Fubu" (天下布武), which means "All the world by force of arms" or "Rule the Empire by Force".

In 1568, Ashikaga Yoshiaki went to Gifu to ask Nobunaga to start a campaign toward Kyoto. Yoshiaki was the brother of the murdered thirteenth "shōgun" of the Ashikaga shogunate, Yoshiteru, and wanted revenge against the killers who had already set up a puppet "shōgun", Ashikaga Yoshihide. Nobunaga agreed to install Yoshiaki as the new "shōgun" and, grasping the opportunity to enter Kyoto, started his campaign. An obstacle in southern Ōmi Province was the Rokkaku clan. Led by Rokkaku Yoshikata, the clan refused to recognize Yoshiaki as "shōgun" and was ready to go to war. In response, Nobunaga launched a rapid attack, driving the Rokkaku clan out of their castles.

On 9 Nov. 1568, Nobunaga entered Kyoto. Yoshiaki was made the 15th "shōgun" of the Ashikaga shogunate. However, Nobunaga refused any appointment from Yoshiaki, and their relationship grew difficult, though Nobunaga showed the Emperor great respect.

The Asakura clan was particularly disdainful of the Oda clan's increasing power. Furthermore, Asakura Yoshikage had also protected Ashikaga Yoshiaki, but had not been willing to march toward Kyoto.

When Nobunaga launched a campaign into the Asakura clan's domain, Azai Nagamasa, to whom Nobunaga's sister Oichi was married, broke the alliance with Oda to honor the Azai-Asakura alliance which had lasted for generations. With the help of Ikko rebels, the anti-Nobunaga alliance sprang into full force, taking a heavy toll on the Oda clan. At the Battle of Anegawa, Tokugawa Ieyasu joined forces with Nobunaga and defeated the combined forces of the Asakura and Azai clans.
The Enryaku-ji monastery on Mt. Hiei, with its "sōhei" (warrior monks) of the Tendai school who aided the anti-Nobunaga group by helping Azai-Asakura alliance, was an issue for Nobunaga since the monastery was so close to his base of power. Nobunaga attacked Enryaku-ji and razed it in October 1571, killing "monks, laymen, women and children" in the process. The whole mountainside was a great slaughterhouse, and the sight was one of unbearable horror."

During the siege of Nagashima, Nobunaga inflicted tremendous losses to the Ikkō-ikki resistance who opposed samurai rule. The siege finally ended when Nobunaga surrounded the enemy complex and set fire to it, killing tens of thousands.

He later succeeded in taking their main stronghold at Ishiyama Hongan-ji after an 11-year siege that ended with its surrender.

One of the strongest rulers in the anti-Nobunaga alliance was Takeda Shingen, in spite of his generally peaceful relationship and a nominal alliance with the Oda clan. In 1572, at the urgings of the "shōgun", Shingen decided to make a drive for the capital starting with invading Tokugawa territory. Tied down on the western front, Nobunaga sent lackluster aid to Ieyasu, who suffered defeat at the Battle of Mikatagahara in 1573. However, after the battle, Tokugawa's forces launched night raids and convinced Takeda of an imminent counter-attack, thus saving the vulnerable Tokugawa with the bluff. This would play a pivotal role in Tokugawa's philosophy of strategic patience in his campaigns with Oda Nobunaga. Shortly thereafter, the Takeda forces were neutralized after Shingen died from throat cancer in April 1573.
This was a relief for Nobunaga because he could now focus on Yoshiaki, who had openly declared hostility more than once, despite the imperial court's intervention. Nobunaga was able to defeat Yoshiaki's forces and send him into exile, bringing the Ashikaga shogunate to an end in the same year. Also in 1573, Nobunaga successfully destroyed Asakura and Asai, driving them both to suicide.

At the decisive Battle of Nagashino, the combined forces of Nobunaga and Tokugawa Ieyasu devastated the Takeda clan with the strategic use of arquebuses. Nobunaga compensated for the arquebus's slow reloading time by arranging the arquebusiers in three lines, firing in rotation. From there, Nobunaga continued his expansion, sending Akechi Mitsuhide to pacify Tanba Province before advancing upon the Mori

In 1574 Nobunaga became "Gondainagon" and "Ukon'etaishō". By 1576 he was given the title of Minister of the Right ("Udaijin"). The Oda clan's siege of Ishiyama Hongan-ji in Osaka made some progress, but the Mori clan of the Chūgoku region broke the naval blockade and started sending supplies into the strongly fortified complex by sea. As a result, in 1577, Hashiba Hideyoshi was ordered to confront the warrior monks at Negoroji.

However, Uesugi Kenshin, rival of Takeda Shingen and Oda, clashed with Oda during the Battle of Tedorigawa. The result was a decisive Uesugi victory. However, Kenshin's sudden death in 1578, ended his movement south.

Nobunaga forced the Ishiyama Hongan-ji to surrender in 1580, employed the only samurai of African origin Yasuke as one of his retainers in 1581 and destroyed the Takeda clan in 1582. Nobunaga's administration was at its height of power and he was about to launch invasions into Echigo Province and Shikoku.

In the 1582 Battle of Tenmokuzan, Takeda Katsuyori committed suicide after his defeat at the hands of Oda Nobunaga and Tokugawa Ieyasu.

In 1582, Nobunaga's former sandal bearer Hashiba Hideyoshi invaded Bitchū Province, laying siege to Takamatsu Castle. The castle was vital to the Mori clan, and losing it would have left the Mori home domain vulnerable. Led by Mōri Terumoto, reinforcements arrived, prompting Hideyoshi to ask for reinforcements from Nobunaga. Nobunaga promptly ordered his leading generals to prepare their armies, the overall expedition to be led by Nobunaga.

Nobunaga left Azuchi Castle for Honnō-ji in Kyoto, where he was to hold a tea ceremony. Hence, he only had 30 pages with him, while his son Nobutada had brought 2000 of his cavalrymen.

Mitsuhide chose that time to attack. On June 21, 1582, Mitsuhide took a unit of his men and surrounded the Honnō-ji while sending another unit of Akechi troops to assault Myōkaku-ji, initiating a full coup d'état. At Honnō-ji, Nobunaga's small entourage was soon overwhelmed and, as the Akechi troops closed in on the burning temple where Nobunaga had been residing, he decided to commit seppuku in one of the inner rooms. His son Nobutada was then killed. 

The cause of Mitsuhide's "betrayal" is controversial. It has been proposed that Mitsuhide may have heard a rumor that Nobunaga would transfer Mitsuhide's fief to the page, Mōri Ranmaru, with whom Nobunaga is alleged to have been in a ritualized homosexual relationship, a form of patronage, known as "shudō".

Oda Nobunaga, Toyotomi Hideyoshi and Tokugawa Ieyasu are three men credited with the unification of Japan. All three were born within 8 years of each other (1534 to 1542), started their careers as samurai and finished them as statesmen. Nobunaga inherited his father's domain at the age of 17, and quickly gained control of Owari province through "gekokujo". Hideyoshi started his career in Nobunaga's army as an ashigaru, but quickly rose up through the ranks as a samurai. Ieyasu initially fought against Nobunaga, but later joined his army. 

Militarily, Nobunaga changed the way war was fought in Japan. His matchlock armed foot soldiers displaced mounted soldiers armed with bow and sword. He built iron plated warships and imported saltpeter and lead for manufacturing gunpowder and bullets respectively, while also manufacturing artillery. His ashigaru foot soldiers were trained and disciplined for mass movements, which replaced hand-to-hand fighting tactics. They wore distinctive uniforms which fostered esprit de corps. He was ruthless and cruel in battle, pursuing fugitives without compassion. Through wanton slaughter, he became the ruler of 20 provinces.

After consolidating military power in provinces he came to dominate, starting with Owari and Mino, Nobunaga implemented a plan for economic development. This included the declaration of free markets ("rakuichi"), the breaking of trade monopolies, and providing for open guilds ("rakuza"). Nobunaga instituted policies as a way to stimulate business and the overall economy through the use of a free market system. These policies abolished and prohibited monopolies and opened once closed and privileged unions, associations and guilds, which he saw as impediments to commerce. Even though these policies provided a major boost to the economy, it was still heavily dependent on "daimyōs" support. Copies of his original proclamations can be found in Entoku-ji in the city of Gifu. 

Nobunaga initiated policies for civil administration, which included currency regulations, construction of roads and bridges. This included setting standards for the road widths and planting trees along roadsides. This was to ease transport of soldiers and war material in addition to commerce. In general, Nobunaga thought in terms of "unifying factors," in the words of George Sansom.

Nobunaga initiated a period in Japanese art history known as Fushimi, or the Azuchi-Momoyama period, in reference to the area south of Kyoto. He built extensive gardens and castles which were themselves great works of art. Azuchi Castle included a 7 storey Tenshukaku, which included a treasury filled with gold and precious objects. Works of art included paintings on movable screens ("byobu"), sliding doors ("fusuma"), and walls by Kanō Eitoku. During this time, Nobunaga's tea master Sen no Rikyū established key elements of the Japanese tea ceremony. 

Additionally, Nobunaga was very interested in European culture which was still very new to Japan. He collected pieces of Western art as well as arms and armor, and he is considered to be among the first Japanese people in recorded history to wear European clothes. He also became the patron of the Jesuit missionaries in Japan and supported the establishment of the first Christian church in Kyoto in 1576, although he never converted to Christianity.

Depending upon the source, Oda Nobunaga and the entire Oda clan are descendents of either the Fujiwara clan or the Taira clan (specifically, Taira no Shigemori's branch). His lineage can be directly traced to his great-great-grandfather, Oda Hisanaga, who was followed by Oda Toshisada, Oda Nobusada, Oda Nobuhide, and Nobunaga himself.

Nobunaga was the eldest legitimate son of Nobuhide, a minor warlord from Owari Province, and Tsuchida Gozen, who was also the mother to three of his brothers (Nobuyuki, Nobukane, and Hidetaka) and two of his sisters (Oinu and Oichi).

Nobunaga married Nōhime, the daughter of Saitō Dōsan, as a matter of political strategy; however, she was unable to give birth to children and was considered to be barren. It was his concubines Kitsuno and Lady Saka who bore his children. Kitsuno gave birth to Nobunaga's eldest son, Nobutada. Nobutada's son Hidenobu became ruler of the Oda clan after the deaths of Nobunaga and Nobutada. His son Oda Nobuhide was a Christian, and took the baptismal name Peter; he was adopted by Toyotomi Hideyoshi and commissioned chamberlain.

One of Nobunaga's younger sisters, Oichi, gave birth to three daughters. These three nieces of Nobunaga became involved with important historical figures. Chacha (also known as Lady Yodo), the eldest, became the mistress of Toyotomi Hideyoshi. O-Hatsu married Kyōgoku Takatsugu. The youngest, O-go, married the son of Tokugawa Ieyasu, Tokugawa Hidetada (the second "shōgun" of the Tokugawa shogunate). O-go's daughter Senhime married her cousin Toyotomi Hideyori, Lady Yodo's son.

Nobunaga's nephew was Tsuda Nobuzumi, the son of Nobuyuki. Nobusumi married Akechi Mitsuhide's daughter, and was killed after the Honnō-ji coup by Nobunaga's third son, Nobutaka, who suspected him of being involved in the plot.

Nobunaga's granddaughter Oyu no Kata, by his son Oda Nobuyoshi, married Tokugawa Tadanaga.

Nobunari Oda, a retired figure skater, claims to be a 17th generation direct descendant of Nobunaga. The ex-monk celebrity Mudō Oda also claims descent from the Sengoku period warlord, but his claims have not been verified.


Nobunaga appears frequently within fiction and continues to be portrayed in many different anime, manga, video games, and cinematic films. Many depictions show him as villainous or even demonic in nature, though some portray him in a more positive light. The latter type of works include Akira Kurosawa's film "Kagemusha", which portrays Nobunaga as energetic, athletic and respectful towards his enemies. The film "Goemon" portrays him as a saintly mentor of Ishikawa Goemon. Nobunaga is a central character in Eiji Yoshikawa's historical novel "Taiko Ki", where he is a firm but benevolent lord. Nobunaga is also portrayed in a heroic light in some video games such as "Kessen III", "Ninja Gaiden II", and the "Warriors Orochi" series.. While in the anime series "Nobunaga no Shinobi" Nobunaga is portrayed as a kind person as well as having a major sweet tooth.

By contrast, the novel and anime series "Yōtōden" portrays Nobunaga as a literal demon in addition to a power-mad warlord. In the novel "The Samurai's Tale" by Erik Christian Haugaard, he is portrayed as an antagonist "known for his merciless cruelty". He is portrayed as evil or megalomaniacal in some anime and manga series including "Samurai Deeper Kyo" and "Flame of Recca". Nobunaga is portrayed as evil, villainous, bloodthirsty, and/or demonic in many video games such as "Ninja Master's", "Sengoku", "Maplestory", "" and "Atlantica Online", and the video game series "Onimusha", "Samurai Warriors", "Sengoku Basara" (and its anime adaptation), and "Soulcalibur".

Nobunaga has been portrayed numerous times in a more neutral or historic framework, especially in the Taiga dramas shown on television in Japan. Oda Nobunaga appears in the manga series "Tail of the Moon", "Kacchū no Senshi Gamu", and Tsuji Kunio's historical fiction "The Signore: Shogun of the Warring States". Historical representations in video games (mostly Western-made strategy titles) include "", "", "Throne of Darkness", the eponymous "Nobunaga's Ambition" series, as well as "Civilization V" and "". Kamenashi Kazuya of the Japanese pop group KAT-TUN wrote and performed a song titled "1582" which is written from the perspective of Mori Ranmaru during the coup at Honnō temple.

Nobunaga has also been portrayed fictively, such as when the figure of Nobunaga influences a story or inspires a characterization. In James Clavell's novel "Shōgun", the character Goroda is a pastiche of Nobunaga. In the film "Sengoku Jieitai 1549", Nobunaga is killed by time-travellers. Nobunaga also appears as a major character in the eroge "Sengoku Rance" and is a playable character in "Pokémon Conquest", with his partner Pokémon being Hydreigon, Rayquaza and Zekrom. In the anime "", in "Sengoku Collection", and the light novel and anime series "The Ambition of Oda Nobuna", he is depicted as a female character. He is the main character of the stage action and anime adaptation of "Nobunaga the Fool". In Kouta Hirano's "Drifters", Nobunaga is sent to another world to fight against other historical figures and displays equal parts tactical brilliance and gleeful brutality. In the 2014 anime "Nobunaga Concerto", and its 2015 film adaptation, he is the subject of a complex plot involving time travel and alternate history.




</doc>
<doc id="22684" url="https://en.wikipedia.org/wiki?curid=22684" title="Otto Wilhelm Hermann von Abich">
Otto Wilhelm Hermann von Abich

Otto Wilhelm Hermann von Abich (December 11, 1806July 1, 1886) was a German mineralogist and geologist. Full member of St Petersburg Academy of Sciences (hon. member since 1866).

He was born at Berlin and educated at the local university. His earliest scientific work is related to spinels and other minerals. Later he made special studies of fumaroles, of the mineral deposits around volcanic vents, and of the structure of volcanoes. In 1842 he was appointed professor of mineralogy in the university of Dorpat (Tartu), and henceforth gave attention to the geology and mineralogy of the Russian Empire. Residing for some time at Tiflis, he investigated the geology of the Armenian Highland (this term was introduced by Abich) and Caucasus. In 1844 and 1845 he ascended Ararat volcano several times, studied the geological event of 1840 that was centered on Ararat (Akori village). In 1877 he retired to Vienna, where he died. The mineral Abichite was named after him.

The following are listed in Chisholm (1911), p. 62:


</doc>
<doc id="22685" url="https://en.wikipedia.org/wiki?curid=22685" title="Organization of the Communist Party of the Soviet Union">
Organization of the Communist Party of the Soviet Union

The organization of the Communist Party of the Soviet Union was nominally based on the principles of democratic centralism.

The governing body of the Communist Party of the Soviet Union (CPSU) was the Party Congress which initially met annually but whose meetings became less frequent, particularly under Joseph Stalin. Party Congresses would elect a Central Committee which, in turn, would elect a Politburo. Under Stalin, the most powerful position in the party became the General Secretary who was elected by the Politburo. In 1952 the title of "General Secretary" became "First Secretary" and the "Politburo" became the "Presidium" before reverting to their former names under Leonid Brezhnev in 1966.

In theory, supreme power in the party was invested in the Party Congress. However, in practice the power structure became reversed and, particularly after the death of Lenin, supreme power became the domain of the General Secretary.

In the late Soviet Union the CPSU incorporated the communist parties of the 15 constituent republics (the communist branch of the Russian SFSR was established in 1990). Before 1990 the communist party organization in Russian oblasts, autonomous republics and some other major administrative units were subordinated directly to the CPSU Central Committee.

At lower levels, the organizational hierarchy was managed by Party Committees, or partkoms (партком). A partkom was headed by the elected "partkom bureau secretary" ("partkom secretary", секретарь парткома). At enterprises, institutions, kolkhozes, etc., they were called as such, i.e., "partkoms". At higher levels the Committees were abbreviated accordingly: obkoms (обком) at oblast (zone) levels (known earlier as gubkoms (губком) for guberniyas), raikoms (райком) at raion (district) levels (known earlier as ukoms (уком) for uyezds), gorkom (горком) at city levels, etc.

The same terminology ("raikom", etc.) was used in the organizational structure of Komsomol.

The bottom level of the Party was the primary party organization (первичная партийная организация) or party cell (партийная ячейка). It was created within any organizational entity of any kind where there were at least three communists. The management of a cell was called party bureau/partbureau (партийное бюро, партбюро). A partbureau was headed by the elected bureau secretary (секретарь партбюро).

At smaller party cells, secretaries were regular employees of the corresponding plant/hospital/school/etc. Sufficiently large party organizations were usually headed by an exempt secretary, who drew his salary from the Party money.





</doc>
<doc id="22686" url="https://en.wikipedia.org/wiki?curid=22686" title="Oromo people">
Oromo people

The Oromo people (; , "’Oromo") are an ethnic group inhabiting Ethiopia and parts of Kenya and Somalia. They are the largest ethnic group in Ethiopia and the wider Horn of Africa. According to the 2007 census, they represent approximately 34.5% of Ethiopia's population, while others estimate they make up about 40% of the population. With the total Ethiopian population thought to be over 102 million, the number of Oromo people exceeds 35 million in Ethiopia alone. In Ethiopia, they predominantly live in the Oromia Region.

Oromos speak the Oromo language as a mother tongue (also called "Afaan Oromoo" and "Oromiffa"), which is part of the Cushitic branch of the Afro-Asiatic language family. The word "Oromo" appeared in European literature for the first time in 1893 and then slowly became common in the second half of the 20th century.

The Oromo people followed their traditional religion and used the "gadaa" system of governance. A leader elected by the "gadaa" system remains in power only for 8 years, with an election taking place at the end of that 8 years. From the 18th century to the 19th century, Oromos were the dominant influence in northern Ethiopia during the Zemene Mesafint period. Various current estimates state that probably around 50-60% of Oromos follow Islam, 30-35% follow Christianity, and fewer than 3% have retained their traditional beliefs. They have been one of the parties to historic migrations, and wars particularly with northern Christians and with southern and eastern Muslims, in the Horn of Africa.

The origins and prehistory of the Oromo people is unclear, in part because the Oromo people did not have a written history and instead passed on stories orally prior to the 16th century. Older and subsequent colonial era documents mention the Oromo people as "Galla", but these documents were generally written by members of ethnic groups who were hostile towards them. Anthropologists and historians such as Herbert S. Lewis consider these sources to be fraught with biases, distortions and misunderstandings.

Historical linguistics and comparative ethnology studies suggest that the Oromo people probably originated around the lakes Shamo ("Chamo") and Stephanie ("Chew Bahir"). They are a Cushitic people who have inhabited the East and Northeast Africa since at least the early 1st millennium. The aftermath of the sixteenth century Abyssinian–Adal war led to Oromos being able to occupy lands of the Ethiopian Empire and Adal Sultanate. The Harla were assimilated by the Oromo in Ethiopia.

The first verifiable record mentioning the Oromo people by a European cartographer is in the map made by the Italian Fra Mauro in 1460, which uses the term "Galla". The map was likely drawn after consultations with Tigriyan monks (which produced notable monks such as Abba Bahrey) who visited Italy in 1441 . "Galla" was a term for a river and a forest, as well as for the pastoral people established in the highlands of southern Ethiopia. This historical information, according to Mohammed Hassen, is consistent with the written and oral traditions of the Somalis. . The historical evidence therefore suggests that the Oromo people were already established in the southern highlands in or before the 15th century, and that at least some Oromo people were interacting with other Ethiopian ethnic groups.
After Fra Mauro's mention, there is a profusion of literature about the peoples of this region including the Oromo, particularly mentioning their wars and resistance to religious conversion, primarily by European sea explorers, Christian missionaries as well as regional writers. Fra Mauro's term Galla is the most used term, however, until the early 20th century. The earliest primary account of Oromo ethnography is the 16th-century ""History of Galla"" by Christian monk Bahrey who comes from the Sidama country of Gammo, written in the Ge'ez language. He begins his treatise on the Oromo by introducing them in racist terms. According to an 1861 book by D'Abbadie, a French explorer who traveled up to Kaffa in 1843, he was told that the word Galla was derived from a "war cry" and used by the Gallas themselves. A journal published by International African Institute suggests it is an Oromo word (adopted by neighbours) for there is a word galla "wandering" in their language. The first known use of the word "Oromo" to refer to this ethnic group is traceable to 1893. The historic term for them has been "Galla". This term, stated Juxon Barton in 1924, was in use for these people by Abyssinians and Arabs. The word "Galla" has been variously interpreted, such as it means "to go home", or it refers to a river named Galla in early Abyssinian tradition.
Scholarship that followed Barton, states that the label "Galla" for them, in historic documents, is a stereotype and has been translated by other ethnic groups as "pagan, savage, inferior, enemy", and "heathen, that is non-Muslim". In Afar language, states Morin, "Galli" (pl. "Galla") means "crowd", "foreigners" and carries derogatory connotation "ordinary, commoner" as opposed to "moddai" or "high descent". Other societies such as the Anuak people refer to all the migrant highlanders consisting of largely Amharas as Galla people while the Tigreans, in the past, refer to Amharas as "half Galla". The term "Galla" was also used by Europeans before the 1974 revolution without any derogatory connotations. The Oromo never called themselves "Galla", and resist its use. They traditionally identified themselves by one of their clans ("gosas"), and in contemporary times have used the common umbrella term of "Oromo" which connotes "free born people".

While Oromo people have lived in this region for a long time, the ethnic mixture of peoples who have lived here is unclear. According to Alessandro Triulzi, the interactions and encounters between Oromo people and Nilo-Saharan groups likely began early. Different groups have attempted to reconstruct a speculative origin theories, wherein either Oromo are presumed "heathen and expansionists who displaced another ethnic group", or the Oromo are presumed to be original people who were "displaced by others". However, persuasive evidence to support various speculations has been missing. The original Oromos increased their numbers through Oromization ("Meedhicca", "Mogasa" and "Gudifacha") of conquered people ("Gabbaro") from other ethnic groups, and in turn others conquered people from them and converted them to their side. The native ancient names of the territories were replaced by the name of the Oromo clans who conquered it while the people were made Gabbaros (serfs). This, in part, was a Oromo response to preserve their identity, as they as the third major group faced forced mass conversion by the conquering armies of Christian Abyssinians or Islamic Sultanates, often at war with each other. The word Oromo is derived from "Ilm Orma" meaning "children of Oromo", or "sons of Men", or "person, stranger".

Historically, Afaan Oromo-speaking people used their own "Gadaa" system of governance. Oromos also had a number of independent kingdoms, which they shared with the Sidama people. Among these were the Gibe region kingdoms of Gera, Gomma, Garo, Gumma, Jimma, Leeqa-Nekemte and Limmu-Ennarea.

The earliest known documented and detailed history of the Oromo people was by the Ethiopian monk Abba Bahrey who wrote "Zenahu le Galla" in 1593, though the synonymous term "Gallas" was mentioned in maps or elsewhere much earlier. After the 16th century, they are mentioned more often, such as in the records left by Abba Pawlos, Joao Bermudes, Jerorimo Lobo, Galawdewos, Sarsa Dengel and others. These records suggest that the Oromo were pastoral people in their history, who stayed together. Their animal herds began to expand rapidly and they needed more grazing lands. They began migrating, not together, but after separating. They lacked kings, and had elected leaders called "luba" based on a "gada" system of government instead. By the late 16th century, two major Oromo confederations emerged: "Afre" and "Sadaqa", which respectively refer to four and three in their language, with Afre emerging from four older clans, and Sadaqa out of three. These Oromo confederations were originally based on southern parts of Ethiopia, but started moving north in the 16th century in what is termed as the "Great Oromo Migration".

According to Richard Pankhurst, an Ethiopia historian, this migration is linked to the first incursions into inland Horn of Africa by Imam Ahmad ibn Ibrahim. According to historian Marianne Bechhaus-Gerst, the migration was one of the consequences of fierce wars of attrition between Christian and Muslim armies in the Horn of Africa region in the 15th and 16th century which killed a lot of people and depopulated the regions near the Galla lands, but also probably a result of droughts in their traditional homelands. Further, they acquired horses and their "gada" system helped coordinate well equipped Oromo warriors who enabled fellow Oromos to advance and settle into newer regions starting in the 1520s. This expansion continued through the 17th century.

Both peaceful integration and violent competition between Oromos and other neighboring ethnicities such as the Amhara, Sidama, Afar and the Somali affected politics within the Oromo community. Between 1500 and 1800, there were waves of wars and struggle between highland Christians, coastal Muslim and polytheist population in the Horn of Africa. This caused major redistribution of populations. The northern, eastern and western movement of the Oromos from the south around 1535 mirrored the large-scale expansion by Somalis inland. The 1500–1800 period also saw relocation of the Amhara people, and helped influence contemporary ethnic politics in Ethiopia.

According to oral and literary evidence, Borana Oromo clan and Garre Somali clan mutually victimized each other in seventeenth and eighteenth centuries, particularly near their eastern borders. There were also periods of relative peace. According to Günther Schlee, the Garre Somali clan replaced the Borana Oromo clan as the dominant ethnic group in this region. The Borana violence against their neighbors, states Schlee, was unusual and unlike their behavior inside their community where violence was considered deviant.

Oromo, along with Sidama, were the main targets for slaves before the 16th-century. Warfare, kidnapping and judicial sentencing were some reasons for the enslavement of the Oromo people. In certain cases "pastoralist Oromo enslaved settled Oromo", and as some Oromo converted to Islam or Christianity, the relationships within the Oromo societies fissured. According to Rudolph Ware, a professor of history specializing in Africa, slavery had become so central to the political economy in the Horn of Africa that the Oromo had little choice. The slavery historian Paul E. Lovejoy concurs, and states, "slavery was fundamental to the social, political and economic order of the northern savanna, Ethiopia and the East African coast for several centuries before 1600".

The slaves were classified into two groups according to Amhara color conventions: "red" slaves, who were light-brown in complexion and mostly from the Cushitic-speaking populations of southern and southwestern Ethiopia, and "black" slaves called "Bareya" or "Shanqalla" ("Negro"), who were from the Sudanese-Ethiopian border – the western fringes of the Christian Ethiopia at the time. The "red" slaves were more expensive and exported, while the "black" slaves were kept domestically such as by Christian Abyssinians. The bulk of red "habasha" slaves were Oromos along with the Sidama. The Cushitic-speaking Oromos formed the expensive and the bulk of 'Ethiopian' slaves exported to Arabian peninsula and Persian Gulf regions, to Ottoman Empire markets, to Egypt and elsewhere. Young female Oromo slaves served as concubines and household workers, while males were in demand for private armies and servile labor.

Oromos too enslaved other ethnic groups. According to a report by Bermudes, in the 16th century, Oromos during their wars were fierce and cruel, mutilating and enslaving the people in the regions they conquered. Emperor Galawdewos battled with Oromos without much success and sought Portuguese help. In the era of Imam Ahmad, according to Bahrey's records, Oromo "Luba" 'tribes' made war in Dawaro against Adal Mabraq, devastating the region and occupying it. They also took over Fatagar and Faj, forcing its previous inhabitants into slavery.

The pagan Galla and animist Sidama or Agew slaves made up the slave caravans coming out of Ethiopia, as slavers avoided Christian or Muslim slaves. The central Amhara provinces were a part of major Afar slave caravan trade routes from the southern and southwestern Galla, Sidama and Gurage regions to the northern and eastern Ethiopia. Thousands of slaves were exported every year by Jabarti, Jalaba, Afar, Somali and Arab merchants as the income from this trade was lucrative. According to Ira M. Lapidus, a professor of Middle Eastern and Islamic History, the Ethiopian slave trade benefited the Muslims, and increased the Islamization of the Oromo people.

In the first decades of the 19th century, three Oromo monarchies, Enarya, Goma and Guma, rose to prominence. The collective area was known as Galla-land and comprised most of central and southern Ethiopia, including lands now held by other ethnic regions. In the general view of Oromo people's role in Ethiopia, one of the Oromo leaders named Ras Gobana Dacche led the development of modern Ethiopia and the political and military incorporation of more territories into Ethiopian borders. Gobana, under the authority of Amhara ruler Emperor Menelik II, incorporated several and brought large sections of the Horn of Africa into a centralized Ethiopian state.

The demand for Ethiopian slaves in Arabia, the Persian Gulf region, Egypt, the Ottoman Empire and elsewhere far outpaced the supply in the first half of the 19th century, leading slavers such as the Jabarti caravans to penetrate deeper into Ethiopia. According to Richard Pankhurst, a significant part of the extensive slave trading in early 19th-century, from many parts of Ethiopia to coastal regions of Red Sea, Gulf of Aden, Sudan and Egypt, came from the Oromo regions. In the second half of nineteenth century, Ethiopian slaves, called "Habash", continued to be in high demand in Arabian and Middle Eastern markets as servants, treasurers and bodyguards for merchants and rulers, or as concubines and wives for the well-to-do. These slaves also came mainly from Oromo and Sidama country. Slaves were brought from slave markets in Jimma, Basso and elsewhere, and their caravans sent thousands to the ports of Matamma, Massawa and Tajura on the Red Sea.

By the early nineteenth century Asandabo, Saqa, Hermata and Billo were the primary slave markets of Guduru, Limmu-Enaria, Jimma and Leqa-Naqamite respectively where long distance slave caravans and local traders met. Merchant Villages adjacent to these major markets of the south were invariably full of slaves in which the upper classes exchanged them for the imported goods they coveted. Slave trading for those states was the primary source of revenue. Many were raided from neighboring territories, in other cases Oromo took other Oromo as captives for slave trading, sometimes Oromo people were sentenced into slavery for minor crimes such as tax arrears, and sold by Oromo and Sidama rulers. Slaves from these major markets of the south were walked northward to large distribution markets like Basso in Gojjam, Aliyu Amba and Abdul Resul in Shewa. There, the kingdoms taxed the transaction before the slaves were exported.

Coastal and border towns such as Massawa, Tajura and Metemma, as well as merchant centers such as in Gondar and Debarq had custom officials called "Nagadras" who collected taxes on each slave. A study of slave shipping records shows that over 3,300 slaves were exported every year from northern routes from Ethiopia, while over 4,200 slaves exported annually from southern routes. On average, available records suggest that 15,000 new slaves every year were produced in southern Ethiopia, the region inhabited by Oromo and Sidama people. Of these, Jimma alone exported up to 4,000 annually and slave trading was the largest export item in terms of value for the state, while Gojjam produced 7,000.

According to Francesca Declich, there was a flourishing market in Somalia for Oromo slaves in 19th and early 20th centuries, both in agricultural areas surrounding the Shebelle River, as well as in urban and pastoral settings. These Oromo slaves were either kidnapped from their home regions, captured in raids or were captives who had fought back. The records from the Somali markets, such as in Mogadishu, indicate that the highest price for slaves were paid for "Galla woman" in the 15–20 year age group meant to work as a concubine, while a Galla teenage for domestic work and girl child in the age group of 8–10 attracted much lower prices.

According to colonial British consulate records, slaves were exported from Oromo lands in particular, but also other parts of Ethiopia. In 1866 alone thousands of Christians were exported from Taka and Metema, which were key outlets for Ethiopian slave exports. Since religious law did not permit Christians to participate in the trade, Muslims dominated the slave trade, often going farther and farther afield to find supplies. In the centralized Oromo states of Gibe valleys and Didesa, the agricultural and industrial sectors mainly relied on slave labour. Slave labour became so large that in states such as Gibe, Jimma, Gera and Janjero, slaves constituted a third to two-thirds of the total population.

The inter-clan relationships within the Oromo people, as well as their relationship with the Amhara people who are the second largest ethnic group, have been historically complicated. There was inter-clan fighting within Oromo. Over 450 years, through the 19th century, states Donald N. Levine, the warfare between Amhara and Oromo had been "more or less continuous". In the southern and western regions, the Oromo-Amhara wars have been as terribly destructive as those between Amhara and Muslim Sultanates in the east. In certain regions, some Oromo groups formed an alliance and cooperated with Amhara-Tirgrean authorities.

The inter-relationship between Oromo and Amhara peoples has been a subject of dispute, some suggesting evidence of integration while others suggesting on-going abuse that continued through the 20th century. From one perspective, ethnically mixed Ethiopians with Oromo background made up a small percentage of Ethiopian generals and leaders. The Wollo Oromo (particularly the Raya Oromo and Yejju Oromo) were early Oromo holders of power among the increasingly mixed Ethiopian state. The later north-to-south movement of central power in Ethiopia led to Oromos in Shewa holding power in Ethiopia together with the Shewan Amhara.

The accounts of integration of Oromo people into a united Ethiopian nation vary widely.
By one account it was violent and forced. A school of scholars state that during the conquest of the southern territories that created the modern Ethiopia, the "neftenya-gabbar" system brutally subordinated the Oromos, Menelik's Army carried out mass atrocities during the conquest against civilians and combatants including torture, mass killings and large-scale slavery. Large-scale atrocities were also committed against the Dizi people and the people of the Kaficho kingdom. Some estimates for the number of Southern people (Oromos, Dizi people, Kaffa, Shanqella) killed as a result of the conquest from war, famine and atrocities go into the millions.

Other accounts disagree. The Russian writer Aleksandr Bulatovich, for example, travelled to Ethiopia around the start of the 20th century, and he stated that in territories incorporated peacefully like Jima, Leka and Wolega, the former order had been preserved without interference in Oromo self-government. He said that in areas incorporated after war, the assigned rulers treated the people lawfully and justly, and did not violate their religious beliefs . He explained the deaths as a consequence of many factors such as famine and diseases that ravaged this period of Ethiopian history.

In some accounts, the relationship between the two largest ethnic groups of Ethiopia, Oromo and Amhara, has been described as "Abyssinian feudal colonialism". According to Mekuria Bulcha, for example, Oromo were colonized by Amhara just like European colonialists in the pre-20th century period. The Abyssinian general Ras Darge, according to these accounts, ordered "Arsi mutilation". Oromo populations who resisted Amhara occupation were subject to amputations and disfigurement. Villages were decimated. By 1901, parts of the Oromo territory were reduced to a third or half of their original population. According to Akbar Ahmad, Amharic sayings such as ""Saw naw Galla? (is it human or Galla?)"" highlighted Amhara's contempt towards the Oromo. In the 1960s, political disputes emerged with reports of discrimination in educational opportunities for Oromo by Amhara leaders.

Others disagree with the colonial thesis. According to Paulos Milkias and Getachew Metaferia, Amhara and Oromo have shared the same geographical and historical space; they are intertwined culturally, economically and politically; millions of people trace their origin from both groups; elite Oromos were the regional kingmakers in, e.g., Gondar and Shewa; and intermarriage between Amhara and Oromo ruling elites was and is extensive. Thousands of Amharas from nobles to peasants and from educated to illiterate served loyally under Oromo ministers and generals, which, Milkias and Metaferia state, "would be like having thousands of Englishmen, nobles and commoners loyally serving African ministers and generals in England". All this is inconsistent with the "Amhara colonized Oromo" thesis, as it neither happened nor can happen under a colonial system.

According to some scholars, Oromo became part of the Ethiopian nobility without losing their identity. Others state that the marginalization of the Oromos during Amhara rule led many to change their names to blend in with the Amhara population.

According to Atsuko Karin Matsuoka and John Sorenson, there was large-scale cruel exploitation of Oromo peasants in the 19th and 20th centuries, but the situation was more complex because some Oromos crossed ethnic lines and collaborated with the Abyssinian forces. This period also marked individual Oromos reaching high ranks in the Ethiopian military, and several royal family members of this era were partially of Oromo descent. For example, Iyasu V was the designated but uncrowned Emperor of Ethiopia (1913–1916), while Haile Selassie I was the crowned and generally acknowledged Emperor of Ethiopia from 1930 to 1974. Other Oromo people reached high positions in the emerging Ethiopia.

The Oromo traditionalists state that those who rose to powerful positions did so by rejecting their "Oromo-ness": rejecting their culture and adopting the other culture. Some in Ethiopia see Oromo as integral architects and a part of one nation, while others see a long history of denigration, treatment as "primitive barbarians" by Amhara and other ethnic groups, and a need to "resurrect Oromo culture and history".

The Oromo people are the largest ethnic group in Ethiopia, estimated to be over 35 million people in 2016. Their population is dispersed over a large region. They speak 74 ethnically diverse language groups. About 95% are settled agriculturalists and nomadic pastoralists, practising archaic farming methods and living at subsistence level. A few live in the urban centres.

Oromos today are concentrated in the Oromia region in central Ethiopia, which is the largest region in the country in terms of both population and size. They are present in large numbers in other central, western and southern provinces of Ethiopia. Group members also have a notable presence in northern Kenya in the Marsabit County, and in the Welo and Tigre regions of Eritrea.

The Oromo are divided into two major branches that break down into an assortment of clan families. From west to east. The Borana Oromo, also called the Boran, are a pastoralist group living in southern Ethiopia (Oromia) and northern Kenya. The Boran inhabit the former provinces of Shewa, Welega, Illubabor, Kafa, Jimma, Sidamo, northern and northeastern Kenya, and a small refugee population in some parts of Somalia.

Barentu/Barentoo or (older) Baraytuma is the other moiety of the Oromo people. The Barentu Oromo inhabit the eastern parts of the Oromia Region in the Zones of Mirab Hararghe or West Hararghe, Arsi Zone, Bale Zone, Debub Mirab Shewa Zone or South West Shewa, Dire Dawa region, the Jijiga Zone of the Somali Region, Administrative Zone 3 of the Afar Region, Oromia Zone of the Amhara Region, and are also found in the Raya Azebo woreda in the Tigray Region.

The Oromo speak the Oromo language as a mother tongue (also known as "Afaan Oromoo" and "Oromiffa"). It belongs to the Cushitic branch of the Afroasiatic family.

According to "Ethnologue", there are around 17,465,900 Oromo speakers worldwide.

The Oromo language is divided into four main linguistic varieties: Borana-Arsi-Guji Oromo, Eastern Oromo, Orma and West Central Oromo.. Onesimos Nesib was a founder of the Oromo modern literature.

Modern writing systems used to transcribe Oromo include the Latin script. The Ethiopic script had previously been used by Oromo communities in west-central Ethiopia up until the 1990s. Additionally, the Sapalo script was historically used to write Oromo. It was invented by the Oromo scholar Sheikh Bakri Sapalo (also known by his birth name, Abubaker Usman Odaa) during the 1950s.

Christianity was adopted in Ethiopia early in 340 CE by the Kingdom of Axum. Ethiopia was an early Christian kingdom that remained in power through the modern era. Islam arrived from the coastal region during the medieval era, across the Gulf of Aden, and led to the creation of warring Islamic sultanates such as Hadiya, Bali, Fatagar, Dawaro and Adal. These kingdoms and sultanates ruled or influenced the history of Oromo people. The influential 30-year war from 1529 to 1559 between the three parties – the Oromo, the Christians and the Muslims – dissipated the political strengths of all three. The religious beliefs of the Oromo people evolved in this socio-political environment. In the 19th century and first half of the 20th century, neither the Muslim-controlled areas, nor the areas where the Ethiopian Orthodox Church was dominant, would allow Protestant or Catholic missionaries to proselytize among them, and these missions focused their efforts in the southern provinces of Greater Ethiopia where Oromo people following the traditional religions lived.

In the 2007 Ethiopian census for Oromia region, which included both Oromo and non-Oromo residents, there was a total of 13,107,963 followers of Christianity (8,204,908 Orthodox, 4,780,917 Protestant, 122,138 Catholic), 12,835,410 followers of Islam, 887,773 followers of traditional religions, and 162,787 followers of other religions.

According to a 2009 publication of Association of Muslim Social Scientists and International Institute of Islamic Thought, "probably just over 60% of the Oromos follow Islam, over 30% follow Christianity and less than 3% follow traditional religion".

According to a 2016 estimate by James Minahan, about half of the Oromo people are Sunni Muslim, a third are Ethiopian Orthodox, and the rest are mostly Protestants or follow their traditional religious beliefs. The traditional religion is more common in southern Oromo populations, Christianity more common in and near the urban centers, while Muslims are more common near the Somalian border and in the north.

Oromo people were traditionally a culturally homogeneous society with genealogical ties. They governed themselves in accordance with "Gadaa" (literally "era"), an outstanding democratic socio-political system long before the 16th century, when major three party wars commenced between them and the Christian kingdom to their north and Islamic sultanates to their east and south. The "Gadaa" system elected males from five Oromo "miseensa" (groups), for a period of eight years, for various judicial, political, ritual and religious roles. Retirement was compulsory after the eight year term, and each major clan followed the same "Gadaa" system. Women and people belonging to the lower Oromo castes were excluded. A male born in the upper Oromo society went through five stages of eight years, where his life established his role and status for consideration to a "Gadaa" office.

Under "Gadaa", every eight years, the Oromo would choose by consensus an "Abbaa Bokkuu" responsible for justice, peace, judicial and ritual processes, an "Abbaa Duulaa" responsible as the war leader, an "Abbaa Sa'aa" responsible as the leader for cows, and other positions.

Like other ethnic groups in the Horn of Africa and East Africa, Oromo people regionally developed social stratification consisting of four hierarchical strata. The highest strata were the nobles called the "Borana", below them were the "Gabbaro" (some 17th to 19th century Ethiopian texts refer them as the "dhalatta"). Below these two upper castes were the despised castes of artisans, and at the lowest level were the slaves.

In the Islamic Kingdom of Jimma, the Oromo society's caste strata predominantly consisted of endogamous, inherited artisanal occupations. Each caste group has specialized in a particular occupation such as iron working, carpentry, weapon making, pottery, weaving, leather working and hunting.

Each caste in the Oromo society had a designated name. For example, "Tumtu" were smiths, "Fuga" were potters, "Faqi" were tanners and leatherworkers, "Semmano" were weavers, "Gagurtu" were bee keepers and honey makers, and "Watta" were hunters and foragers. While slaves were a stratum within the society, many Oromos, regardless of caste, were sold into slavery elsewhere. By the 19th century, Oromo slaves were sought after and a major part of slaves sold in Gondar and Gallabat slave markets at Ethiopia-Sudan border, as well as the Massawa and Tajura markets on the Red Sea.

The Oromo people developed a luni-solar calendar, which likely dates from a pre-16th century period and before the great migration because different geographically and religiously distinct Oromo communities use the same calendar. This calendar is sophisticated and similar to ones found among the Chinese, the Hindus and the Mayans. It was tied to the traditional religion of the Oromos, and used to schedule the "Gadda" system of elections and power transfer.

The Borana Oromo calendar system was once thought to be based upon an earlier Cushitic calendar developed around 300 BC found at Namoratunga. Reconsideration of the Namoratunga site led astronomer and archaeologist Clive Ruggles to conclude that there is no relationship. The new year of the Oromo people, according to this calendar, falls in the month of October. The calendar has no weeks but a name for each day of the month. It is a lunar-stellar calendar system.

Some modern authors such as Gemetchu Megerssa have proposed the concept of "Oromumma", or "Oromoness" as a cultural common between Oromo people. The word is derived by combining "Oromo" with the Arabic term "Ummah" (community). However, according to Terje Østebø and other scholars this term is a neologism from the late 1990s and has been questioned to its link to Oromo ethno-nationalism and Salafi Islamic discourse, in their disagreement with Christian Amhara and other ethnic groups.

The Oromo people, depending on their geographical location and historical events, have variously converted to Islam, to Christianity, or remained with their traditional religion. According to Gemetchu Megerssa, the subjective reality is that "neither traditional Oromo rituals nor traditional Oromo beliefs function any longer as a cohesive and integral symbol system" for the Oromo people, not just regionally but even locally. The cultural and ideological divergence within the Oromo people, in part from their religious differences, is apparent from the constant impetus for negotiations between broader Oromo spokespersons and those Oromo who are Ahl al-Sunna followers, states Terje Østebø. The internally evolving cultural differences within the Oromos have led some scholars such as Mario Aguilar and Abdullahi Shongolo to conclude that "that a common identity acknowledged by all Oromo in general does not exist".

In 1973, Oromo discontent with their position led to the formation of the Oromo Liberation Front (OLF), which began political agitation in the Oromo areas. That year there was also a catastrophic famine in which over one quarter of a million people died from starvation before the government recognised the disaster and permitted relief measures. The majority who died were Oromos and Amharas from Wollo, Afars and Tigrayans. There were strikes and demonstrations in Addis Ababa in 1974; and in February of that year, Haile Selassie’s government was replaced by the Derg, a military junta led by Mengistu Haile Mariam (a mixed Ethiopian with ethnic Konso heritage); but the Council was still Amhara-dominated, with only 25 non-Amhara members out of 125. In 1975 the government declared all rural land State-owned, and announced the end of the tenancy system. However, much of the benefit of this reform was counteracted by compulsive collectivization, State farms and forced resettlement programmes.

In 1991, the Derg was replaced by the EPRDF. Initially, Oromo intellectuals and the OLF joined the transitional government alongside EPRDF. However, the TPLF branch of EPRDF created an Oromo party (OPDO) to marginalized the OLF and eventually expel it from the country. Despite increased harassment on Oromos, the OPDO presided over the advancement of Oromo language and culture over the last two decades. The TPLF is widely known to use this progress in Oromo cultural and linguistic empowerment as an achievement and a mandate for EPRDF rule the nation. However, most Oromos still do not believe they have political rights and many of them support the OLF and other opposition parties including the Oromo Federalist Congress (OFC).

In December 2009, a 96-page report titled "Human Rights in Ethiopia: Through the Eyes of the Oromo Diaspora", compiled by the Advocates for Human Rights, documented human rights violations against the Oromo in Ethiopia under three successive regimes: the Ethiopian Empire under Haile Selassie, the Marxist Derg and the current Ethiopian government of the Ethiopian People’s Revolutionary Democratic Front (EPRDF), dominated by members of the Tigrayan People’s Liberation Front (TPLF) and which was accused to have arrested approximately 20,000 suspected OLF members, to have driven most OLF leadership into exile, and to have effectively neutralized the OLF as a political force in Ethiopia.

According to the Office of the United Nations High Commissioner for Human Rights, the Oromia Support Group (OSG) recorded 594 extra-judicial killings of Oromos by Ethiopian government security forces and 43 disappearances in custody between 2005 and August 2008.

Starting in November 2015, during a wave of mass protests, mainly by Oromos, over the expansion of the municipal boundary of the capital, Addis Ababa, into Oromia, over 500 people have been killed and many more have been injured, according to human-rights advocates and independent monitors. The protests have since spread to other ethnic groups and encompass wider social grievances. Ethiopia declared a state of emergency in response to Oromo and Amhara protests in October 2016.

Most Oromos do not have political unity today due to their historical roles in the Ethiopian state and the region, the spread-out movement of different Oromo clans, and the differing religions inside the Oromo nation. Accordingly, Oromos played major roles in all three main political movements in Ethiopia (centralist, federalist and secessionist) during the 19th and 20th century. In addition to holding high powers during the centralist government and the monarchy, the Raya Oromos in the Tigray regional state played a major role in the "Weyane" revolt, challenging Emperor Haile Selassie I's rule in the 1940s. Simultaneously, both federalist and secessionist political forces developed inside the Oromo community.

At present a number of ethnic-based political organizations have been formed to promote the interests of the Oromo. The first was the Mecha and Tulama Self-Help Association founded in January 1963, but disbanded by the government after several increasingly tense confrontations in November, 1966. Later groups include the Oromo Liberation Front (OLF), Oromo Federalist Democratic Movement (OFDM), the United Liberation Forces of Oromia (ULFO), the Islamic Front for the Liberation of Oromia (IFLO), the Oromia Liberation Council (OLC), the Oromo National Congress (ONC, recently changed to OPC) and others. Another group, the Oromo People's Democratic Organization (OPDO), is one of the four parties that form the ruling Ethiopian People's Revolutionary Democratic Front (EPRDF) coalition. However, these Oromo groups do not act in unity: the ONC, for example, was part of the United Ethiopian Democratic Forces coalition that challenged the EPRDF in the Ethiopian general elections of 2005.

A number of these groups seek to create an independent Oromo nation, some using armed force. Meanwhile, the ruling OPDO and several opposition political parties in the Ethiopian parliament believe in the unity of the country which has 80 different ethnicities. But most Oromo opposition parties in Ethiopia condemn the economic and political inequalities in the country. Progress has been very slow, with the Oromia International Bank just recently established in 2008, though Oromo-owned Awash International Bank started early in the 1990s. The first private Afaan Oromoo newspaper in Ethiopia, Jimma Times, also known as Yeroo, was recently established, but it has faced a lot of harassment and persecution from the Ethiopian government since its beginning. Abuse of Oromo media is widespread in Ethiopia and reflective of the general oppression Oromos face in the country. University departments in Ethiopia did not establish a curriculum in Afaan Oromo until the late 1990s because they lacked the technical expertise and resources.

Various human rights organizations have publicized the government persecution of Oromos in Ethiopia for decades. In 2008, OFDM opposition party condemned the government's indirect role in the death of hundreds of Oromos in western Ethiopia. According to Amnesty International, "between 2011 and 2014, at least 5000 Oromos have been arrested based on their actual or suspected peaceful opposition to the government. These include thousands of peaceful protestors and hundreds of opposition political party members. The government anticipates a high level of opposition in Oromia, and signs of dissent are sought out and regularly, sometimes pre-emptively, suppressed. In numerous cases, actual or suspected dissenters have been detained without charge or trial, killed by security services during protests, arrests and in detention."

According to Amnesty international, there is a sweeping repression in the Oromo region of Ethiopia. On December 12, 2015, the German broadcaster Deutsche Welle reported violent protests in the Oromo region of Ethiopia in which more than 20 students were killed. According to the report, the students were protesting against the government's re-zoning plan named 'Addis Ababa Master5 Plan'.

On October 2, 2016, nearly 700 festival goers were massacred at the most sacred and largest event among the Oromo, the Irreecha cultural thanksgiving festival. In just one day, hundreds were killed and many more injured in what will go down in history as one of the darkest days for the Oromo people. Every year, millions of Oromos, the largest ethnic group in Ethiopia, gather in Bishoftu for this annual celebration. However this year, the festive mood quickly turned chaotic after Ethiopian security forces responded to peaceful protests by firing tear gas and live bullets at over two million people surrounded by a lake and cliffs.






</doc>
<doc id="22687" url="https://en.wikipedia.org/wiki?curid=22687" title="Oral history">
Oral history

Oral history is the collection and study of historical information about individuals, families, important events, or everyday life using audiotapes, videotapes, or transcriptions of planned interviews. These interviews are conducted with people who participated in or observed past events and whose memories and perceptions of these are to be preserved as an aural record for future generations. Oral history strives to obtain information from different perspectives and most of these cannot be found in written sources. "Oral history" also refers to information gathered in this manner and to a written work (published or unpublished) based on such data, often preserved in archives and large libraries. Knowledge presented by Oral History (OH) is unique in that it shares the tacit perspective, thoughts, opinions and understanding of the interviewee in its primary form.

The term is sometimes used in a more general sense to refer to any information about past events that people who experienced them tell anybody else, but professional historians usually consider this to be oral tradition. However, as the Columbia Encyclopedia explains: 
Primitive societies have long relied on oral tradition to preserve a record of the past in the absence of written histories. In Western society, the use of oral material goes back to the early Greek historians Herodotus and Thucydides, both of whom made extensive use of oral reports from witnesses. The modern concept of oral history was developed in the 1940s by Allan Nevins and his associates at Columbia University. 

Oral history has become an international movement in historical research. Oral historians in different countries have approached the collection, analysis, and dissemination of oral history in different modes. However, it should also be noted that there are many ways of creating oral histories and carrying out the study of oral history even within individual national contexts.

According to the "Columbia Encyclopedia":, the accessibility of tape recorders in the 1960s and 1970s led to oral documentation of the era's movements and protests. Following this, oral history has increasingly become a respected record type. Some oral historians now also account for the subjective memories of interviewees due to the research of Italian historian Alessandro Portelli and his associates.

Oral histories are also used in many communities to document the experiences of survivors of tragedies. Following the Holocaust, there has emerged a rich tradition of oral history, particularly of Jewish survivors. The United States Holocaust Memorial Museum has an extensive archive of over 70,000 oral history interviews. There are also several organizations dedicated specifically to collecting and preserving oral histories of survivors. Oral history as a discipline has fairly low barriers to entry, so it is an act in which laypeople can readily participate–in his book Doing Oral History, Donald Ritchie wrote that "oral history has room for both the academic and the layperson. With reasonable training... anyone can conduct a useable oral history." This is especially meaningful in cases like the Holocaust, where survivors may be less comfortable telling their story to a journalist than they would be to a historian or family member.

In the United States, there are several organizations dedicated to doing oral history which are not affiliated with universities or specific locations. StoryCorps is one of the most well-known of these: following the model of the Federal Writers’ Project created as part of the Works Progress Administration, StoryCorps’ mission is to record the stories of Americans from all walks of life. On contrast to the scholarly tradition of oral history, StoryCorps subjects are interviewed by people they know. There are a number of StoryCorps initiatives that have targeted specific populations or problems, following in the tradition of using oral history as a method to amplify voices that might otherwise be marginalized.

Since the early 1970s, oral history in Britain has grown from being a method in folklore studies (see for example the work of the School of Scottish Studies in the 1950s) to becoming a key component in community histories. Oral history continues to be an important means by which non-academics can actively participate in the compilation and study of history. However, practitioners across a wide range of academic disciplines have also developed the method into a way of recording, understanding, and archiving narrated memories. Influences have included women's history and labour history.

In Britain, the Oral History Society has played a key role in facilitating and developing the use of oral history.

A more complete account of the history of oral history in Britain and Northern Ireland can be found at "Making Oral History" on the Institute of Historical Research's website.

The Bureau of Military History conducted over 1700 interviews with veterans of the First World War and related episodes in Ireland. The documentation was released for research in 2003.

During 1998 and 1999, 40 BBC local radio stations recorded personal oral histories from a broad cross-section of the population for "The Century Speaks" series. The result was 640 half-hour radio documentaries, broadcast in the final weeks of the millennium, and one of the largest single oral history collections in Europe, the Millennium Memory Bank (MMB). The interview based recordings are held by the British Library Sound Archive in the oral history collection.

In one of the largest memory project anywhere, The BBC in 2003-6 invited its audiences to send in recollections of the homefront in the Second World War. It put 47,000 of the recollections online, along with 15,000 photographs.

Alessandro Portelli is an Italian oral historian. He is known for his work which compared workers' experiences in Harlan County, Kentucky and Terni, Italy. Other oral historians have drawn on Portelli's analysis of memory, identity, and the construction of history.

, since the government-run historiography in modern Belarus almost fully excludes repression during the epoch when Belarus was part of the Soviet Union, only private initiatives cover these aspects. Citizens' groups in Belarus use the methods of oral history and record narrative interviews on video: the Virtual Museum of Soviet Repression in Belarus presents a full Virtual museum with intense use of oral history. The Belarusian Oral History Archive project also provides material based on oral history recordings.

Czech oral history began to develop beginning in the 1980s with a focus on social movements and political activism. The practice of oral history and any attempts to document stories prior to this is fairly unknown. [citation needed] The practice of oral history began to take shape in the 1990s. In 2000, The Oral History Center (COH) at the Institute of Contemporary History, Academy of Sciences, Czech Republic (AV ČR) was established with the aim of "systematically support the development of oral history methodology and its application in historical research." 

In 2001, Post Bellum, a nonprofit organization, was established to "documents the memories of witnesses of the important historical phenomenons of the 20th century" within the Czech Republic and surrounding European countries. Post Bellum works in partnership with Czech Radio and Institute for the Study of Totalitarian Regimes. Their oral history project "Memory of Nation" was created in 2008 and interviews are archived online for user access. As of January 2015, the project has more than 2100 published witness accounts in several languages, with more than 24,000 pictures.

Other projects, including articles and books have been funded by the Czech Science Foundation (AV ČR) including:
These publications aim to demonstrate that oral history contributes to the understanding of human lives and history itself, such as the motives behind the dissidents' activities, the formation of opposition groups, communication between dissidents and state representatives and the emergence of ex-communist elites and their decision-making processes.

Oral history centers in the Czech Republic emphasize educational activities (seminars, lectures, conferences), archiving and maintaining interview collections, and providing consultations to those interested in the method.

Because of repression during the Franco dictatorship (1939–75), the development of oral history in Spain was quite limited until the 1970s. It became well-developed in the early 1980s, and often had a focus on the Civil War years (1936–39), especially regarding the losers whose stories had been suppressed. The field was based at the University of Barcelona. Professor Mercedes Vilanova was a leading exponent, and combined it with her interest in quantification and social history. The Barcelona group sought to integrate oral sources with traditional written sources to create mainstream, not ghettoized, historical interpretations. They sought to give a public voice to neglected groups, such as women, illiterates, political leftists, and ethnic minorities.

Oral history began with a focus on national leaders in the United States, but has expanded to include groups representing the entire population. In Britain, the influence of 'history from below' and interviewing people who had been 'hidden from history' was more influential. However, in both countries elite oral history has emerged as an important strand. Scientists, for example, have been covered in numerous oral history projects. Doel (2003) discusses the use of oral interviews by scholars as primary sources, He lists major oral history projects in the history of science begun after 1950. Oral histories, he concludes, can augment the biographies of scientists and help spotlight how their social origins influenced their research. Doel acknowledges the common concerns historians have regarding the validity of oral history accounts. He identifies studies that used oral histories successfully to provide critical and unique insight into otherwise obscure subjects, such as the role scientists played in shaping US policy after World War II. Interviews furthermore can provide road maps for researching archives, and can even serve as a fail-safe resource when written documents have been lost or destroyed. Roger D. Launius (2003) shows the huge size and complexity of the National Aeronautics and Space Administration (NASA) oral history program since 1959. NASA systematically documented its operations through oral histories. They can help to explore broader issues regarding the evolution of a major federal agency. The collection consists primarily of oral histories conducted by scholars working on books about the agency. Since 1996, however, the collection has also included oral histories of senior NASA administrators and officials, astronauts, and project managers, part of a broader project to document the lives of key agency individuals. Launius emphasizes efforts to include such less-well-known groups within the agency as the Astrobiology Program, and to collect the oral histories of women in NASA.

Contemporary oral history involves recording or transcribing eyewitness accounts of historical events. Some anthropologists started collecting recordings (at first especially of Native American folklore) on phonograph cylinders in the late 19th century. In the 1930s, the Federal Writers' Project—part of the Works Progress Administration (WPA)—sent out interviewers to collect accounts from various groups, including surviving witnesses of the Civil War, slavery, and other major historical events. The Library of Congress also began recording traditional American music and folklore onto acetate discs. With the development of audio tape recordings after World War II, the task of oral historians became easier.

In 1946, David P. Boder, a professor of psychology at the Illinois Institute of Technology in Chicago, traveled to Europe to record long interviews with "displaced persons"—most of them Holocaust survivors. Using the first device capable of capturing hours of audio—the wire recorder—Boder came back with the first recorded Holocaust testimonials and in all likelihood the first recorded oral histories of significant length.

Many state and local historical societies have oral history programs. Sinclair Kopp (2002) report on the Oregon Historical Society's program. It began in 1976 with the hiring of Charles Digregorio, who had studied at Columbia with Nevins. Thousands of sound recordings, reel-to-reel tapes, transcriptions, and radio broadcasts have made it one of the largest collections of oral history on the Pacific Coast. In addition to political figures and prominent businessmen, the Oregon Historical Society has done interviews with minorities, women, farmers, and other ordinary citizens, who have contributed extraordinary stories reflecting the state's cultural and social heritage. Hill (2004) encourages oral history projects in high school courses. She demonstrates a lesson plan that encourages the study of local community history through interviews. By studying grassroots activism and the lived experiences of its participants, her high school students came to appreciate how African Americans worked to end Jim Crow laws in the 1950s.

Naison (2005) describes the Bronx African American History Project (BAAHP), an oral community history project developed by the Bronx County Historical Society. Its goal was to document the histories of black working- and middle-class residents of the South Bronx neighborhood of Morrisania in New York City since the 1940s.

The Middle East often requires oral history methods of research, mainly because of the relative lack in written and archival history and its emphasis on oral records and traditions. Furthermore, because of its population transfers, refugees and émigrés become suitable objects for oral history research.

Katharina Lange studied the tribal histories of Syria. The oral histories in this area could not be transposed into tangible, written form due to their positionalities, which Lange describes as “taking sides.” The positionality of oral history could lead to conflict and tension. The tribal histories are typically narrated by men. While histories are also told by women, they are not accepted locally as “real history.” Oral histories often detail the lives and feats of ancestors.

Genealogy is a prominent subject in the area. According to Lange, the oral historians often tell their own personalized genealogies to demonstrate their credibility, both in their social standing and their expertise in the field.

From 2003 to 2004, Professors Marianne Kamp and Russell Zanca researched agricultural collectivization in Uzbekistan; one part of their project involved using oral history methodology. The goal of the project was to learn more about life in the 1920s and 1930s to study the impact of the Soviet Union’s conquest. 20 interviews each were conducted in the Fergana valley, Tashkent, Bukhara, Khorezm, and Kashkadarya regions. These interviews uncovered untold stories of famine and death. The oral histories filled in a gap of information missing from the Central State Archive of Uzbekistan.

The rise of oral history is a new trend in historical studies in China that began in the late twentieth century. Some oral historians, stress the collection of eyewitness accounts of the words and deeds of important historical figures and what really happened during those important historical events, which is similar to common practice in the west, while the others focus more on important people and event, asking important figures to describe the decision making and details of important historical events. In December 2004, the Chinese Association of Oral History Studies was established. The establishment of this institution is thought to signal that the field of oral history studies in China has finally moved into a new phase of organized development.

Te-Kong Tong (1920-2009) was a prominent Chinese American historian and recognized as the "pioneer of oral history in contemporary China (中国近代口述史学的开创者)". His view on oral history of historical characters is that historians should identify the changes of the people, and posit their evolution in parallel with that of history.His work include: 

While oral tradition is an integral part of ancient Southeast Asian history, oral history is a relatively recent development. Since the 1960s, oral history has been accorded increasing attention both on institutional as well as individual levels, representing “history from above” and “history from below”.

In Oral History and Public Memories, Blackburn writes about oral history as a tool that was used “by political elites and state-run institutions to contribute to the goal of national building” in postcolonial Southeast Asian countries. Blackburn draws most of his examples of oral history as a vehicle for “history from above” from Malaysia and Singapore.

In terms of “history from below”, various oral history initiatives are being undertaken in Cambodia in an effort to record lived experiences from the rule of the Khmer Rouge regime while survivors are still living. These initiative take advantage of crowdsourced history to uncover the silences imposed on the oppressed.

Two prominent and ongoing oral history projects out of South Asia stem from time periods of ethnic violence that were decades apart: 1947 and 1984.

The 1947 Partition Archive was founded in 2010 by Guneeta Singe Bhalla, a physicist in Berkeley, California, who began conducting and recording interviews "to collect and preserve the stories of those who lived through this tumultuous time, to make sure this great human tragedy isn't forgotten." 

The Sikh Diaspora Project was founded in 2014 by Brajesh Samarth, senior lecturer in Hindi-Urdu at Emory University in Atlanta, when he was a lecturer at Stanford University in California. The project focuses on interviews with members of the Sikh diaspora in the U.S. and Canada, including the many who migrated after the 1984 massacre of Sikhs in India.

In 1948, Allan Nevins, a Columbia University historian, established the Columbia Oral History Research Office, now known as the Columbia Center for Oral History, with a mission of recording, transcribing, and preserving oral history interviews. The Regional Oral History Office was founded in 1954 as a division of the University of California, Berkeley's Bancroft Library. In 1967, American oral historians founded the Oral History Association, and British oral historians founded the Oral History Society in 1969. In 1981, Mansel G. Blackford, a business historian at Ohio State University, argued that oral history was a useful tool to write the history of corporate mergers. More recently, Harvard Business School launched the Creating Emerging Markets project, which "explores the evolution of business leadership in Africa, Asia, and Latin America throughout recent decades" through oral history. "At its core are interviews, many on video, by the School’s faculty with leaders or former leaders of firms and NGOs who have had a major impact on their societies and enterprises across three continents." There are now numerous national organizations and an International Oral History Association, which hold workshops and conferences and publish newsletters and journals devoted to oral history theory and practices. Specialized collections of oral history sometimes have archives of widespread global interest; an example is the Lewis Walpole Library in Farmington, Connecticut, a department of the University Library of Yale.

Historians, folklorists, anthropologists, human geographers, sociologists, journalists, linguists, and many others employ some form of interviewing in their research. Although multi-disciplinary, oral historians have promoted common ethics and standards of practice, most importantly the attaining of the "informed consent" of those being interviewed. Usually this is achieved through a deed of gift, which also establishes copyright ownership that is critical for publication and archival preservation.

Oral historians generally prefer to ask open-ended questions and avoid leading questions that encourage people to say what they think the interviewer wants them to say. Some interviews are "life reviews," conducted with people at the end of their careers. Other interviews focus on a specific period or a specific event in people's lives, such as in the case of war veterans or survivors of a hurricane.

Feldstein (2004) considers oral history to be akin to journalism, Both are committed to uncovering truths and compiling narratives about people, places, and events. Felstein says each could benefit from adopting techniques from the other. Journalism could benefit by emulating the exhaustive and nuanced research methodologies used by oral historians. The practice of oral historians could be enhanced by utilizing the more sophisticated interviewing techniques employed by journalists, in particular, the use of adversarial encounters as a tactic for obtaining information from a respondent.

The first oral history archives focused on interviews with prominent politicians, diplomats, military officers, and business leaders. By the 1960s and '70s, influenced by the rise of new social history, interviewing began to be employed more often when historians investigated history from below. Whatever the field or focus of a project, oral historians attempt to record the memories of many different people when researching a given event. Interviewing a single person provides a single perspective. Individuals may misremember events or distort their account for personal reasons. By interviewing widely, oral historians seek points of agreement among many different sources, and also record the complexity of the issues. The nature of memory—both individual and community—is as much a part of the practice of oral history as are the stories collected.

Archaeologists sometimes conduct oral history interviews to learn more about unknown artifacts. Oral interviews can provide narratives, social meaning, and contexts for objects. When describing the use of oral histories in archaeological work, Paul Mullins emphasizes the importance of using these interviews to replace “it-narratives.” It-narratives are the voices from objects themselves rather than people; according to Mullins, these lead to narratives that are often “sober, pessimistic, or even dystopian.”

Oral history interviews were used to provide context and social meaning in the Overstone excavation project in Northumberland. Overstone consists of a row of four cottages. The excavation team, consisting of Jane Webster, Louise Tolson, Richard Carlton, and volunteers, found the discovered artifacts difficult to identify. The team first took the artifacts to an archaeology group, but the only person with knowledge about a found fragment recognized the fragment from a type of pot her mother had. This inspired the team to conduct group interviews volunteers who grew up in households using such objects. The team took their reference collection of artifacts to the interviews in order to trigger the memories of volunteers, revealing a “shared cultural identity.”

In 1997 the Supreme Court of Canada, in the "Delgamuukw v. British Columbia" trial, ruled that oral histories were just as important as written testimony. Of oral histories, it said "that they are tangential to the ultimate purpose of the fact-finding process at trial – the determination of the historical truth."

Writers who use oral history have often discussed its relationship to historical truth. Gilda O'Neill writes in "Lost Voices", an oral history of East End hop-pickers: "I began to worry. Were the women's, and my, memories true or were they just stories? I realised that I had no 'innocent' sources of evidence - facts. I had, instead, the stories and their tellers' reasons for remembering in their own particular ways.' Duncan Barrett, one of the co-authors of "The Sugar Girls" describes some of the perils of relying on oral history accounts: "On two occasions, it became clear that a subject was trying to mislead us about what happened – telling a self-deprecating story in one interview, and then presenting a different, and more flattering, version of events when we tried to follow it up. ... often our interviewees were keen to persuade us of a certain interpretation of the past, supporting broad, sweeping comments about historical change with specific stories from their lives." Alessandro Portelli argues that oral history is valuable nevertheless: "it tells us less about events as such than about their meaning [...] the unique and precious element which oral sources force upon the historian ... is the speaker's subjectivity."

Regarding the accuracy of oral history, Jean-Loup Gassend concludes in the book "Autopsy of a Battle" "I found that each witness account can be broken down into two parts: 1) descriptions of events that the witness participated in directly, and 2) descriptions of events that the witness did not actually participate in, but that he heard about from other sources. The distinction between these two parts of a witness account is of the highest importance. I noted that concerning events that the witnesses participated in, the information provided was surprisingly reliable, as was confirmed by comparison with other sources. The imprecision or mistakes usually concerned numbers, ranks, and dates, the first two tending to become inflated with time. Concerning events that the witness had not participated in personally, the information was only as reliable as whatever the source of information had been (various rumors); that is to say, it was often very unreliable and I usually discarded such information."

Another noteworthy case is the Mau Mau uprising in Kenya against the British colonizers. Central to the case was Historian Caroline Elkins' study on UK's brutal suppression of the uprising. Elkin's work on this matter is largely based on oral testimonies of survivors and witnesses, which causes controversy in academia: "Some praised Elkins for breaking the 'code of silence' that had squelched discussion of British imperial violence. Others branded her a self-aggrandising crusader whose overstated findings had relied on sloppy methods and dubious oral testimonies." The British court eventually ruled in the Kenyan claimants' favor, which also serves as a response to Elkin's critics as Justice McCombe's 2011 decision stressed the "substantial documentation supporting accusations of systematic abuses". After the ruling, newly discovered files containing relevant records of former colonies from the Hanslope disclosure corroborated Elkin's finding.

In Guatemalan literature, "I, Rigoberta Menchú" (1983), brings oral history into the written form through the "testimonio" genre. "I, Rigoberta Menchú" is compiled by Venezuelan anthropologist Burgos-Debray, based on a series of interviews she conducted with Menchú. The Menchú-controversy arose when historian David Stoll took issue with Menchú’s claim that “this is a story of all poor Guatemalans”. In "Rigoberta Menchú and the Story of All Poor Guatemalans" (1999), Stoll argues that the details in Menchú’s "testimonio" are inconsistent with his own fieldwork and interviews he conducted with other Mayas. According to Guatemalan novelist and critic Arturo Arias, this controversy highlights a tension in oral history. On one hand, it presents an opportunity to convert the subaltern subject into a “speaking subject”. On the other hand, it challenges the historical profession in certifying the “factuality of her mediated discourse” as “subaltern subjects are forced to [translate across epistemological and linguistic frameworks and] use the discourse of the colonizer to express their subjectivity”.
















</doc>
<doc id="22689" url="https://en.wikipedia.org/wiki?curid=22689" title="Oncogene">
Oncogene

An oncogene is a gene that has the potential to cause cancer. In tumor cells, they are often mutated and/or expressed at high levels.

Most normal cells will undergo a programmed form of rapid cell death (apoptosis) when critical functions are altered and malfunctioning. Activated oncogenes can cause those cells designated for apoptosis to survive and proliferate instead. Most oncogenes began as proto-oncogenes, normal genes involved in cell growth and proliferation or inhibition of apoptosis. If normal genes promoting cellular growth, through mutation, are up-regulated, (gain of function mutation) they will predispose the cell to cancer and are thus termed oncogenes. Usually multiple oncogenes, along with mutated apoptotic and/or tumor suppressor genes will all act in concert to cause cancer. Since the 1970s, dozens of oncogenes have been identified in human cancer. Many cancer drugs target the proteins encoded by oncogenes.

The theory of oncogenes was foreshadowed by the German biologist Theodor Boveri in his 1914 book "Zur Frage der Entstehung Maligner Tumoren" ('The Origin of Malignant Tumours'), Gustav Fisher, Jena, 1914. Oncogenes (Teilungsfoerdernde Chromosomen) that become amplified (im permanenten Übergewicht) during tumour development.

Later on the term "oncogene" was rediscovered in 1969 by National Cancer Institute scientists George Todaro and Robert Heubner.

The first confirmed oncogene was discovered in 1970 and was termed src (pronounced "sarc" as in "sarcoma"). Src was in fact first discovered as an oncogene in a chicken retrovirus. Experiments performed by Dr. G. Steve Martin of the University of California, Berkeley demonstrated that the Src was indeed the oncogene of the virus. The first nucleotide sequence of v-src was sequenced in 1980 by A.P. Czernilofsky et al.

In 1976 Drs. Dominique Stehelin, J. Michael Bishop and Harold E. Varmus of the University of California, San Francisco demonstrated that oncogenes were activated proto-oncogenes, found in many organisms including humans. Bishop and Varmus were awarded the Nobel Prize in Physiology or Medicine in 1989 for their discovery of the cellular origin of retroviral oncogenes.

The resultant protein encoded by an oncogene is termed oncoprotein. Oncogenes play an important role in the regulation or synthesis of proteins linked to tumorigenic cell growth. Some oncoproteins are accepted and used as tumor markers. The Spanish biochemist Mariano Barbacid isolated the first oncogene. His discovery was published in the prestigious journal Nature in 1982 in an article titled "A point mutation is responsible for the acquisition of transforming properties by the T24 human bladder-carcinoma oncogene". He spent the following months extending his research, eventually discovering that such oncogene was the mutation of an allele of the Ras subfamily, as well as its activation mechanism.

A proto-oncogene is a normal gene that could become an oncogene due to mutations or increased expression. Proto-oncogenes code for proteins that help to regulate the cell growth and differentiation. Proto-oncogenes are often involved in signal transduction and execution of mitogenic signals, usually through their protein products. Upon acquiring an activating mutation, a proto-oncogene becomes a tumor-inducing agent, an oncogene. Examples of proto-oncogenes include RAS, WNT, MYC, ERK, and TRK. The MYC gene is implicated in Burkitt's lymphoma, which starts when a chromosomal translocation moves an enhancer sequence within the vicinity of the MYC gene. The MYC gene codes for widely used transcription factors. When the enhancer sequence is wrongly placed, these transcription factors are produced at much higher rates. Another example of an oncogene is the Bcr-Abl gene found on the Philadelphia chromosome, a piece of genetic material seen in Chronic Myelogenous Leukemia caused by the translocation of pieces from chromosomes 9 and 22. Bcr-Abl codes for a tyrosine kinase, which is constitutively active, leading to uncontrolled cell proliferation. (More information about the Philadelphia Chromosome below)

The proto-oncogene can become an oncogene by a relatively small modification of its original function. There are three basic methods of activation:

The expression of oncogenes can be regulated by microRNAs (miRNAs), small RNAs 21-25 nucleotides in length that control gene expression by downregulating them. Mutations in such microRNAs (known as oncomirs) can lead to activation of oncogenes. Antisense messenger RNAs could theoretically be used to block the effects of oncogenes.

There are several systems for classifying oncogenes, but there is not yet a widely accepted standard. They are sometimes grouped both spatially (moving from outside the cell inwards) and chronologically (parallelling the "normal" process of signal transduction). There are several categories that are commonly used:

Additional oncogenetic regulator properties include:




</doc>
<doc id="22691" url="https://en.wikipedia.org/wiki?curid=22691" title="Orthogonal frequency-division multiplexing">
Orthogonal frequency-division multiplexing

In telecommunications, orthogonal frequency-division multiplexing (OFDM) is a method of encoding digital data on multiple carrier frequencies. OFDM has developed into a popular scheme for wideband digital communication, used in applications such as digital television and audio broadcasting, DSL internet access, wireless networks, power line networks, and 4G mobile communications.

In coded orthogonal frequency-division multiplexing (COFDM), forward error correction (convolutional coding) and time/frequency interleaving are applied to the signal being transmitted. This is done to overcome errors in mobile communication channels affected by multipath propagation and Doppler effects. COFDM was introduced by Alard in 1986 for Digital Audio Broadcasting for Eureka Project 147. In practice, OFDM has become used in combination with such coding and interleaving, so that the terms COFDM and OFDM co-apply to common applications.

OFDM is a frequency-division multiplexing (FDM) scheme used as a digital multi-carrier modulation method. OFDM was introduced by Chang of Bell Labs in 1966. Numerous closely spaced orthogonal sub-carrier signals with overlapping spectra are emitted to carry data. Demodulation is based on Fast Fourier Transform algorithms. OFDM was improved by Weinstein and Ebert in 1971 with the introduction of a guard interval, providing better orthogonality in transmission channels affected by multipath propagation. Each sub-carrier (signal) is modulated with a conventional modulation scheme (such as quadrature amplitude modulation or phase-shift keying) at a low symbol rate. This maintains total data rates similar to conventional single-carrier modulation schemes in the same bandwidth.

The main advantage of OFDM over single-carrier schemes is its ability to cope with severe channel conditions (for example, attenuation of high frequencies in a long copper wire, narrowband interference and frequency-selective fading due to multipath) without complex equalization filters. Channel equalization is simplified because OFDM may be viewed as using many slowly modulated narrowband signals rather than one rapidly modulated wideband signal. The low symbol rate makes the use of a guard interval between symbols affordable, making it possible to eliminate intersymbol interference (ISI) and utilize echoes and time-spreading (in analog television visible as ghosting and blurring, respectively) to achieve a diversity gain, i.e. a signal-to-noise ratio improvement. This mechanism also facilitates the design of single frequency networks (SFNs) where several adjacent transmitters send the same signal simultaneously at the same frequency, as the signals from multiple distant transmitters may be re-combined constructively, sparing interference of a traditional single-carrier system.

The following list is a summary of existing OFDM-based standards and products. For further details, see the Usage section at the end of the article.



The OFDM-based multiple access technology OFDMA is also used in several 4G and pre-4G cellular networks, mobile broadband standards and the next generation WLAN:

The advantages and disadvantages listed below are further discussed in the Characteristics and principles of operation section below.



Conceptually, OFDM is a specialized frequency-division multiplexing (FDM) method, with the additional constraint that all subcarrier signals within a communication channel are orthogonal to one another.

In OFDM, the sub-carrier frequencies are chosen so that the sub-carriers are orthogonal to each other, meaning that cross-talk between the sub-channels is eliminated and inter-carrier guard bands are not required. This greatly simplifies the design of both the transmitter and the receiver; unlike conventional FDM, a separate filter for each sub-channel is not required.

The orthogonality requires that the sub-carrier spacing is formula_1 Hertz, where "T" seconds is the useful symbol duration (the receiver-side window size), and "k" is a positive integer, typically equal to 1. Therefore, with "N" sub-carriers, the total passband bandwidth will be "B" ≈ "N"·Δ"f" (Hz).

The orthogonality also allows high spectral efficiency, with a total symbol rate near the Nyquist rate for the equivalent baseband signal (i.e. near half the Nyquist rate for the double-side band physical passband signal). Almost the whole available frequency band can be utilized. OFDM generally has a nearly 'white' spectrum, giving it benign electromagnetic interference properties with respect to other co-channel users.

OFDM requires very accurate frequency synchronization between the receiver and the transmitter; with frequency deviation the sub-carriers will no longer be orthogonal, causing "inter-carrier interference" (ICI) (i.e., cross-talk between the sub-carriers). Frequency offsets are typically caused by mismatched transmitter and receiver oscillators, or by Doppler shift due to movement. While Doppler shift alone may be compensated for by the receiver, the situation is worsened when combined with multipath, as reflections will appear at various frequency offsets, which is much harder to correct. This effect typically worsens as speed increases, and is an important factor limiting the use of OFDM in high-speed vehicles. In order to mitigate ICI in such scenarios, one can shape each sub-carrier in order to minimize the interference resulting in a non-orthogonal subcarriers overlapping. For example, a low-complexity scheme referred to as WCP-OFDM ("Weighted Cyclic Prefix Orthogonal Frequency-Division Multiplexing") consists of using short filters at the transmitter output in order to perform a potentially non-rectangular pulse shaping and a near perfect reconstruction using a single-tap per subcarrier equalization. Other ICI suppression techniques usually increase drastically the receiver complexity.

The orthogonality allows for efficient modulator and demodulator implementation using the FFT algorithm on the receiver side, and inverse FFT on the sender side. Although the principles and some of the benefits have been known since the 1960s, OFDM is popular for wideband communications today by way of low-cost digital signal processing components that can efficiently calculate the FFT.

The time to compute the inverse-FFT or FFT transform has to take less than the time for each symbol, which for example for DVB-T means the computation has to be done in or less.

For an -point FFT this may be approximated to:


The computational demand approximately scales linearly with FFT size so a double size FFT needs double the amount of time and vice versa.
As a comparison an Intel Pentium III CPU at 1.266 GHz is able to calculate a FFT in using FFTW. Intel Pentium M at 1.6 GHz does it in Intel Core Duo at 3.0 GHz does it in .

One key principle of OFDM is that since low symbol rate modulation schemes (i.e., where the symbols are relatively long compared to the channel time characteristics) suffer less from intersymbol interference caused by multipath propagation, it is advantageous to transmit a number of low-rate streams in parallel instead of a single high-rate stream. Since the duration of each symbol is long, it is feasible to insert a guard interval between the OFDM symbols, thus eliminating the intersymbol interference.

The guard interval also eliminates the need for a pulse-shaping filter, and it reduces the sensitivity to time synchronization problems.

The cyclic prefix, which is transmitted during the guard interval, consists of the end of the OFDM symbol copied into the guard interval, and the guard interval is transmitted followed by the OFDM symbol. The reason that the guard interval consists of a copy of the end of the OFDM symbol is so that the receiver will integrate over an integer number of sinusoid cycles for each of the multipaths when it performs OFDM demodulation with the FFT. In some standards such as Ultrawideband, in the interest of transmitted power, cyclic prefix is skipped and nothing is sent during the guard interval. The receiver will then have to mimic the cyclic prefix functionality by copying the end part of the OFDM symbol and adding it to the beginning portion.

The effects of frequency-selective channel conditions, for example fading caused by multipath propagation, can be considered as constant (flat) over an OFDM sub-channel if the sub-channel is sufficiently narrow-banded (i.e., if the number of sub-channels is sufficiently large). This makes frequency domain equalization possible at the receiver, which is far simpler than the time-domain equalization used in conventional single-carrier modulation. In OFDM, the equalizer only has to multiply each detected sub-carrier (each Fourier coefficient) in each OFDM symbol by a constant complex number, or a rarely changed value. On a fundamental level, simpler digital equalizers are better because they require less operations, which translate to less round-off errors in the equalizer. Those round-off errors can be viewed as numerical noise and are inevitable.

If differential modulation such as DPSK or DQPSK is applied to each sub-carrier, equalization can be completely omitted, since these non-coherent schemes are insensitive to slowly changing amplitude and phase distortion.

In a sense, improvements in FIR equalization using FFTs or partial FFTs leads mathematically closer to OFDM, but the OFDM technique is easier to understand and implement, and the sub-channels can be independently adapted in other ways than varying equalization coefficients, such as switching between different QAM constellation patterns and error-correction schemes to match individual sub-channel noise and interference characteristics.

Some of the sub-carriers in some of the OFDM symbols may carry pilot signals for measurement of the channel conditions (i.e., the equalizer gain and phase shift for each sub-carrier). Pilot signals and training symbols (preambles) may also be used for time synchronization (to avoid intersymbol interference, ISI) and frequency synchronization (to avoid inter-carrier interference, ICI, caused by Doppler shift).

OFDM was initially used for wired and stationary wireless communications. However, with an increasing number of applications operating in highly mobile environments, the effect of dispersive fading caused by a combination of multi-path propagation and doppler shift is more significant. Over the last decade, research has been done on how to equalize OFDM transmission over doubly selective channels.

OFDM is invariably used in conjunction with channel coding (forward error correction), and almost always uses frequency and/or time interleaving.

Frequency (subcarrier) interleaving increases resistance to frequency-selective channel conditions such as fading. For example, when a part of the channel bandwidth fades, frequency interleaving ensures that the bit errors that would result from those subcarriers in the faded part of the bandwidth are spread out in the bit-stream rather than being concentrated. Similarly, time interleaving ensures that bits that are originally close together in the bit-stream are transmitted far apart in time, thus mitigating against severe fading as would happen when travelling at high speed.

However, time interleaving is of little benefit in slowly fading channels, such as for stationary reception, and frequency interleaving offers little to no benefit for narrowband channels that suffer from flat-fading (where the whole channel bandwidth fades at the same time).

The reason why interleaving is used on OFDM is to attempt to spread the errors out in the bit-stream that is presented to the error correction decoder, because when such decoders are presented with a high concentration of errors the decoder is unable to correct all the bit errors, and a burst of uncorrected errors occurs. A similar design of audio data encoding makes compact disc (CD) playback robust.

A classical type of error correction coding used with OFDM-based systems is convolutional coding, often concatenated with Reed-Solomon coding. Usually, additional interleaving (on top of the time and frequency interleaving mentioned above) in between the two layers of coding is implemented. The choice for Reed-Solomon coding as the outer error correction code is based on the observation that the Viterbi decoder used for inner convolutional decoding produces short error bursts when there is a high concentration of errors, and Reed-Solomon codes are inherently well suited to correcting bursts of errors.

Newer systems, however, usually now adopt near-optimal types of error correction codes that use the turbo decoding principle, where the decoder iterates towards the desired solution. Examples of such error correction coding types include turbo codes and LDPC codes, which perform close to the Shannon limit for the Additive White Gaussian Noise (AWGN) channel. Some systems that have implemented these codes have concatenated them with either Reed-Solomon (for example on the MediaFLO system) or BCH codes (on the DVB-S2 system) to improve upon an error floor inherent to these codes at high signal-to-noise ratios.

The resilience to severe channel conditions can be further enhanced if information about the channel is sent over a return-channel. Based on this feedback information, adaptive modulation, channel coding and power allocation may be applied across all sub-carriers, or individually to each sub-carrier. In the latter case, if a particular range of frequencies suffers from interference or attenuation, the carriers within that range can be disabled or made to run slower by applying more robust modulation or error coding to those sub-carriers.

The term "discrete multitone modulation" ("DMT") denotes OFDM-based communication systems that adapt the transmission to the channel conditions individually for each sub-carrier, by means of so-called "bit-loading". Examples are ADSL and VDSL.

The upstream and downstream speeds can be varied by allocating either more or fewer carriers for each purpose. Some forms of rate-adaptive DSL use this feature in real time, so that the bitrate is adapted to the co-channel interference and bandwidth is allocated to whichever subscriber needs it most.

OFDM in its primary form is considered as a digital modulation technique, and not a multi-user channel access method, since it is utilized for transferring one bit stream over one communication channel using one sequence of OFDM symbols. However, OFDM can be combined with multiple access using time, frequency or coding separation of the users.

In orthogonal frequency-division multiple access (OFDMA), frequency-division multiple access is achieved by assigning different OFDM sub-channels to different users. OFDMA supports differentiated quality of service by assigning different number of sub-carriers to different users in a similar fashion as in CDMA, and thus complex packet scheduling or Media Access Control schemes can be avoided. OFDMA is used in:
OFDMA is also a candidate access method for the IEEE 802.22 "Wireless Regional Area Networks" (WRAN). The project aims at designing the first cognitive radio-based standard operating in the VHF-low UHF spectrum (TV spectrum).

In multi-carrier code division multiple access (MC-CDMA), also known as OFDM-CDMA, OFDM is combined with CDMA spread spectrum communication for coding separation of the users. Co-channel interference can be mitigated, meaning that manual fixed channel allocation (FCA) frequency planning is simplified, or complex dynamic channel allocation (DCA) schemes are avoided.

In OFDM-based wide-area broadcasting, receivers can benefit from receiving signals from several spatially dispersed transmitters simultaneously, since transmitters will only destructively interfere with each other on a limited number of sub-carriers, whereas in general they will actually reinforce coverage over a wide area. This is very beneficial in many countries, as it permits the operation of national single-frequency networks (SFN), where many transmitters send the same signal simultaneously over the same channel frequency. SFNs utilise the available spectrum more effectively than conventional multi-frequency broadcast networks (MFN), where program content is replicated on different carrier frequencies. SFNs also result in a diversity gain in receivers situated midway between the transmitters. The coverage area is increased and the outage probability decreased in comparison to an MFN, due to increased received signal strength averaged over all sub-carriers.

Although the guard interval only contains redundant data, which means that it reduces the capacity, some OFDM-based systems, such as some of the broadcasting systems, deliberately use a long guard interval in order to allow the transmitters to be spaced farther apart in an SFN, and longer guard intervals allow larger SFN cell-sizes. A rule of thumb for the maximum distance between transmitters in an SFN is equal to the distance a signal travels during the guard interval — for instance, a guard interval of 200 microseconds would allow transmitters to be spaced 60 km apart.

A "single frequency network" is a form of transmitter macrodiversity. The concept can be further utilized in "dynamic single-frequency networks" (DSFN), where the SFN grouping is changed from timeslot to timeslot.

OFDM may be combined with other forms of space diversity, for example antenna arrays and MIMO channels. This is done in the IEEE 802.11 Wireless LAN standards.

An OFDM signal exhibits a high peak-to-average power ratio (PAPR) because the independent phases of the sub-carriers mean that they will often combine constructively. Handling this high PAPR requires:

Any non-linearity in the signal chain will cause intermodulation distortion that

The linearity requirement is demanding, especially for transmitter RF output circuitry where amplifiers are often designed to be non-linear in order to minimise power consumption. In practical OFDM systems a small amount of peak clipping is allowed to limit the PAPR in a judicious trade-off against the above consequences. However, the transmitter output filter which is required to reduce out-of-band spurs to legal levels has the effect of restoring peak levels that were clipped, so clipping is not an effective way to reduce PAPR.

Although the spectral efficiency of OFDM is attractive for both terrestrial and space communications, the high PAPR requirements have so far limited OFDM applications to terrestrial systems.

The crest factor CF (in dB) for an OFDM system with n uncorrelated sub-carriers is

where CFc is the crest factor (in dB) for each sub-carrier.
(CFc is 3.01 dB for the sine waves used for BPSK and QPSK modulation).

For example, the DVB-T signal in 2K mode is composed of 1705 sub-carriers that are each QPSK-modulated, giving a crest factor of 35.32 dB.

Many crest factor reduction techniques have been developed.

The dynamic range required for an FM receiver is while DAB only require about As a comparison, each extra bit per sample increases the dynamic range with 

The performance of any communication system can be measured in terms of its power efficiency and bandwidth efficiency.
The power efficiency describes the ability of communication system to preserve bit error rate (BER) of the transmitted signal at low power levels.
Bandwidth efficiency reflects how efficiently the allocated bandwidth is utilized and is defined as the throughput data rate per Hertz in a given bandwidth.
If the large number of subcarriers are used, the bandwidth efficiency of multicarrier system such as OFDM with using optical fiber channel is defined as
Factor 2 is because of two polarization states in the fiber.

where formula_8 is the symbol rate in giga symbol per second (Gsps), and formula_9 is the bandwidth of OFDM signal.

There is saving of bandwidth by using Multicarrier modulation with orthogonal frequency division multiplexing . So the bandwidth for multicarrier system is less in comparison with single carrier system and hence bandwidth efficiency of multicarrier system is larger than single carrier system.
There is only 1 dBm increase in receiver power, but we get 76.7% improvement in bandwidth efficiency with using multicarrier transmission technique.

This section describes a simple idealized OFDM system model suitable for a time-invariant AWGN channel.

An OFDM carrier signal is the sum of a number of orthogonal sub-carriers, with baseband data on each sub-carrier being independently modulated commonly using some type of quadrature amplitude modulation (QAM) or phase-shift keying (PSK). This composite baseband signal is typically used to modulate a main RF carrier.

formula_10 is a serial stream of binary digits. By inverse multiplexing, these are first demultiplexed into formula_11 parallel streams, and each one mapped to a (possibly complex) symbol stream using some modulation constellation (QAM, PSK, etc.). Note that the constellations may be different, so some streams may carry a higher bit-rate than others.

An inverse FFT is computed on each set of symbols, giving a set of complex time-domain samples. These samples are then quadrature-mixed to passband in the standard way. The real and imaginary components are first converted to the analogue domain using digital-to-analogue converters (DACs); the analogue signals are then used to modulate cosine and sine waves at the carrier frequency, formula_12, respectively. These signals are then summed to give the transmission signal, formula_13.

The receiver picks up the signal formula_14, which is then quadrature-mixed down to baseband using cosine and sine waves at the carrier frequency. This also creates signals centered on formula_15, so low-pass filters are used to reject these. The baseband signals are then sampled and digitised using analog-to-digital converters (ADCs), and a forward FFT is used to convert back to the frequency domain.

This returns formula_11 parallel streams, each of which is converted to a binary stream using an appropriate symbol detector. These streams are then re-combined into a serial stream, formula_17, which is an estimate of the original binary stream at the transmitter.

If formula_11 sub-carriers are used, and each sub-carrier is modulated using formula_19 alternative symbols, the OFDM symbol alphabet consists of formula_20 combined symbols.

The low-pass equivalent OFDM signal is expressed as:
where formula_22 are the data symbols, formula_11 is the number of sub-carriers, and formula_24 is the OFDM symbol time. The sub-carrier spacing of formula_25 makes them orthogonal over each symbol period; this property is expressed as:
where formula_27 denotes the complex conjugate operator and formula_28 is the Kronecker delta.

To avoid intersymbol interference in multipath fading channels, a guard interval of length formula_29 is inserted prior to the OFDM block. During this interval, a "cyclic prefix" is transmitted such that the signal in the interval formula_30 equals the signal in the interval formula_31. The OFDM signal with cyclic prefix is thus:

The low-pass signal above can be either real or complex-valued. Real-valued low-pass equivalent signals are typically transmitted at baseband—wireline applications such as DSL use this approach. For wireless applications, the low-pass signal is typically complex-valued; in which case, the transmitted signal is up-converted to a carrier frequency formula_12. In general, the transmitted signal can be represented as:

OFDM is used in:

Key features of some common OFDM-based systems are presented in the following table.
OFDM is used in ADSL connections that follow the ANSI T1.413 and G.dmt (ITU G.992.1) standards, where it is called "discrete multitone modulation" (DMT). DSL achieves high-speed data connections on existing copper wires. OFDM is also used in the successor standards ADSL2, ADSL2+, VDSL, VDSL2, and G.fast. ADSL2 uses variable sub-carrier modulation, ranging from BPSK to 32768QAM (in ADSL terminology this is referred to as bit-loading, or bit per tone, 1 to 15 bits per sub-carrier).

Long copper wires suffer from attenuation at high frequencies. The fact that OFDM can cope with this frequency selective attenuation and with narrow-band interference are the main reasons it is frequently used in applications such as ADSL modems.

OFDM is used by many powerline devices to extend digital connections through power wiring. Adaptive modulation is particularly important with such a noisy channel as electrical wiring. Some medium speed smart metering modems, "Prime" and "G3" use OFDM at modest frequencies (30–100 kHz) with modest numbers of channels (several hundred) in order to overcome the intersymbol interference in the power line environment.
The IEEE 1901 standards include two incompatible physical layers that both use OFDM. The ITU-T G.hn standard, which provides high-speed local area networking over existing home wiring (power lines, phone lines and coaxial cables) is based on a PHY layer that specifies OFDM with adaptive modulation and a Low-Density Parity-Check (LDPC) FEC code.

OFDM is extensively used in wireless LAN and MAN applications, including IEEE 802.11a/g/n and WiMAX.

IEEE 802.11a/g/n, operating in the 2.4 and 5 GHz bands, specifies per-stream airside data rates ranging from 6 to 54 Mbit/s. If both devices can utilize "HT mode" (added with 802.11n), the top 20 MHz per-stream rate is increased to 72.2 Mbit/s, with the option of data rates between 13.5 and 150 Mbit/s using a 40 MHz channel. Four different modulation schemes are used: BPSK, QPSK, 16-QAM, and 64-QAM, along with a set of error correcting rates (1/2–5/6). The multitude of choices allows the system to adapt the optimum data rate for the current signal conditions.

OFDM is also now being used in the WiMedia/Ecma-368 standard for high-speed wireless personal area networks in the 3.1–10.6 GHz ultrawideband spectrum (see MultiBand-OFDM).

Much of Europe and Asia has adopted OFDM for terrestrial broadcasting of digital television (DVB-T, DVB-H and T-DMB) and radio (EUREKA 147 DAB, Digital Radio Mondiale, HD Radio and T-DMB).

By Directive of the European Commission, all television services transmitted to viewers in the European Community must use a transmission system that has been standardized by a recognized European standardization body, and such a standard has been developed and codified by the DVB Project, "Digital Video Broadcasting (DVB); Framing structure, channel coding and modulation for digital terrestrial television". Customarily referred to as DVB-T, the standard calls for the exclusive use of COFDM for modulation. DVB-T is now widely used in Europe and elsewhere for terrestrial digital TV.

The ground segments of the Digital Audio Radio Service (SDARS) systems used by XM Satellite Radio and Sirius Satellite Radio are transmitted using Coded OFDM (COFDM). The word "coded" comes from the use of forward error correction (FEC).

The question of the relative technical merits of COFDM versus 8VSB for terrestrial digital television has been a subject of some controversy, especially between European and North American technologists and regulators. The United States has rejected several proposals to adopt the COFDM-based DVB-T system for its digital television services, and has instead opted for 8VSB (vestigial sideband modulation) operation.

One of the major benefits provided by COFDM is in rendering radio broadcasts relatively immune to multipath distortion and signal fading due to atmospheric conditions or passing aircraft. Proponents of COFDM argue it resists multipath far better than 8VSB. Early 8VSB DTV (digital television) receivers often had difficulty receiving a signal. Also, COFDM allows single-frequency networks, which is not possible with 8VSB.

However, newer 8VSB receivers are far better at dealing with multipath, hence the difference in performance may diminish with advances in equalizer design.

COFDM is also used for other radio standards, for Digital Audio Broadcasting (DAB), the standard for digital audio broadcasting at VHF frequencies, for Digital Radio Mondiale (DRM), the standard for digital broadcasting at shortwave and medium wave frequencies (below 30 MHz) and for DRM+ a more recently introduced standard for digital audio broadcasting at VHF frequencies. (30 to 174 MHz)

The USA again uses an alternate standard, a proprietary system developed by iBiquity dubbed "HD Radio". However, it uses COFDM as the underlying broadcast technology to add digital audio to AM (medium wave) and FM broadcasts.

Both Digital Radio Mondiale and HD Radio are classified as in-band on-channel systems, unlike Eureka 147 (DAB: Digital Audio Broadcasting) which uses separate VHF or UHF frequency bands instead.

The "band-segmented transmission orthogonal frequency division multiplexing" ("BST-OFDM") system proposed for Japan (in the ISDB-T, ISDB-TSB, and ISDB-C broadcasting systems) improves upon COFDM by exploiting the fact that some OFDM carriers may be modulated differently from others within the same multiplex. Some forms of COFDM already offer this kind of hierarchical modulation, though BST-OFDM is intended to make it more flexible. The 6 MHz television channel may therefore be "segmented", with different segments being modulated differently and used for different services.

It is possible, for example, to send an audio service on a segment that includes a segment composed of a number of carriers, a data service on another segment and a television service on yet another segment—all within the same 6 MHz television channel. Furthermore, these may be modulated with different parameters so that, for example, the audio and data services could be optimized for mobile reception, while the television service is optimized for stationary reception in a high-multipath environment.

Ultra-wideband (UWB) wireless personal area network technology may also utilise OFDM, such as in Multiband OFDM (MB-OFDM). This UWB specification is advocated by the WiMedia Alliance (formerly by both the Multiband OFDM Alliance [MBOA] and the WiMedia Alliance, but the two have now merged), and is one of the competing UWB radio interfaces.

"Fast low-latency access with seamless handoff orthogonal frequency division multiplexing" (Flash-OFDM), also referred to as F-OFDM, was based on OFDM and also specified higher protocol layers. It was developed by Flarion, and purchased by Qualcomm in January 2006. Flash-OFDM was marketed as a packet-switched cellular bearer, to compete with GSM and 3G networks. As an example, 450 MHz frequency bands previously used by NMT-450 and C-Net C450 (both 1G analogue networks, now mostly decommissioned) in Europe are being licensed to Flash-OFDM operators.

In Finland, the license holder Digita began deployment of a nationwide "@450" wireless network in parts of the country since April 2007. It was purchased by Datame in 2011. In February 2012 Datame announced they would upgrade the 450 MHz network to competing CDMA2000 technology.

Slovak Telekom in Slovakia offers Flash-OFDM connections with a maximum downstream speed of 5.3 Mbit/s, and a maximum upstream speed of 1.8 Mbit/s, with a coverage of over 70 percent of Slovak population. The Flash-OFDM network was switched off in the majority of Slovakia on 30 September 2015.

T-Mobile Germany used Flash-OFDM to backhaul Wi-Fi HotSpots on the Deutsche Bahn's ICE high speed trains between 2005 and 2015, until switching over to UMTS and LTE.

American wireless carrier Nextel Communications field tested wireless broadband network technologies including Flash-OFDM in 2005. Sprint purchased the carrier in 2006 and decided to deploy the mobile version of WiMAX, which is based on Scalable Orthogonal Frequency Division Multiple Access (SOFDMA) technology.

Citizens Telephone Cooperative launched a mobile broadband service based on Flash-OFDM technology to subscribers in parts of Virginia in March 2006. The maximum speed available was 1.5 Mbit/s. The service was discontinued on April 30, 2009.

Digiweb Ltd. launched a mobile broadband network using Flash-OFDM technology at 872 MHz in July 2007 in Ireland and Digiweb also owns a national 872 MHz license in Norway. Voice handsets are not yet available as of November 2007. The deployment is live in a small area north of Dublin only.

Butler Networks operates a Flash-OFDM network in Denmark at 872 MHz.

In Netherlands, KPN-telecom will start a pilot around July 2007.

OFDM has become an interesting technique for power line communications (PLC). In this area of research, a wavelet transform is introduced to replace the DFT as the method of creating orthogonal frequencies. This is due to the advantages wavelets offer, which are particularly useful on noisy power lines.

Instead of using an IDFT to create the sender signal, the wavelet OFDM uses a synthesis bank consisting of a formula_35-band transmultiplexer followed by the transform function

On the receiver side, an analysis bank is used to demodulate the signal again. This bank contains an inverse transform

followed by another formula_35-band transmultiplexer.\ The relationship between both transform functions is

An example of W-OFDM uses the Perfect Reconstruction Cosine Modulated Filter Bank (PR-CMFB) and Extended Lapped Transform (ELT) is used for the wavelet TF. Thus, formula_41 and formula_42 are given as

These two functions are their respective inverses, and can be used to modulate and demodulate a given input sequence. Just as in the case of DFT, the wavelet transform creates orthogonal waves with formula_46, formula_47, ..., formula_48. The orthogonality ensures that they do not interfere with each other and can be sent simultaneously. At the receiver, formula_49, formula_50, ..., formula_51 are used to reconstruct the data sequence once more.

W-OFDM is an evolution of the standard OFDM, with certain advantages.

Mainly, the sidelobe levels of W-OFDM are lower. This results in less ICI, as well as greater robustness to narrowband interference. These two properties are especially useful in PLC, where most of the lines aren't shielded against EM-noise, which creates noisy channels and noise spikes.

A comparison between the two modulation techniques also reveals that the complexity of both algorithms remains approximately the same.






</doc>
<doc id="22693" url="https://en.wikipedia.org/wiki?curid=22693" title="Operator overloading">
Operator overloading

In programming, operator overloading, sometimes termed "operator ad hoc polymorphism", is a specific case of polymorphism, where different operators have different implementations depending on their arguments. Operator overloading is generally defined by a programming language, a programmer, or both.

Operator overloading is syntactic sugar, and is used because it allows programming using notation nearer to the target domain and allows user-defined types a similar level of syntactic support as types built into a language. It is common, for example, in scientific computing, where it allows computing representations of mathematical objects to be manipulated with the same syntax as on paper.

Operator overloading does not change the expressive power of a language (with functions), as it can be emulated using function calls. For example, consider variables codice_1 of some user-defined type, such as matrices:

codice_2

In a language that supports operator overloading, and with the usual assumption that the '*' operator has higher precedence than '+' operator, this is a concise way of writing:

codice_3

However, the former syntax reflects common mathematical usage.

In this case, the addition operator is overloaded to allow addition on a user-defined type "Time" (in C++):

Addition is a binary operation, which means it has two operands. In C++, the arguments being passed are the operands, and the codice_4 object is the returned value.

The operation could also be defined as a class method, replacing codice_5 by the hidden codice_6 argument; however this forces the left operand to be of type codice_7:

Note that a unary operator defined as a class method would receive no apparent argument (it only works from codice_6):

Less than(<) operator is often overloaded to sort a structure or class.

Since "multi" was used, the function gets added to the list of multidispatch candidates, and "+" is only overloaded for the case where the type constraints in the function signature are met.
While the capacity for overloading includes +, *, >=, the postfix and term i, and so on, it also allows for overloading various brace operators: "[x, y]", "x[ y ]", "x{ y }", and "x( y )".

Kotlin has supported operator overloading since its creation.



</doc>
<doc id="22700" url="https://en.wikipedia.org/wiki?curid=22700" title="Omphalos hypothesis">
Omphalos hypothesis

The omphalos hypothesis is one attempt to reconcile the scientific evidence that the universe is billions of years old with the Genesis creation narrative, which implies that the Earth is only a few thousand years old. It is based on the religious belief that the universe was created by a divine being, within the past ten thousand years (in keeping with flood geology), and that the presence of objective, verifiable evidence that the universe is older than approximately ten millenia is entirely due to the creator introducing false evidence that makes the universe appear much, much older.

The idea was named after the title of an 1857 book, "Omphalos" by Philip Henry Gosse, in which Gosse argued that in order for the world to be "functional", God must have created the Earth with mountains and canyons, trees with growth rings, Adam and Eve with hair, fingernails, and navels (ὀμφαλός "omphalos" is Greek for "navel"), and that therefore "no" empirical evidence about the age of the Earth or universe can be taken as reliable. 
Various supporters of Young Earth creationism have given different explanations for their belief that the universe is filled with false evidence of the universe's age, including a belief that some things needed to be created at a certain age for the ecosystems to function, or their belief that the creator was deliberately planting deceptive evidence. 

The idea was widely rejected in the 19th century, when Gosse published his book. It saw some revival in the 20th century by some Young Earth creationists, who extended the argument to include visible light that appears to originate in far-off stars and galaxies.

Stories of the beginning of human life based on the creation story in Genesis have been published for centuries. The 4th-century theologian, St. Ephrem the Syrian described a world in which divine creation instantly produced fully grown organisms:

By the 19th century, scientific evidence of the Earth's age had been collected, and it disagreed with a literal reading of the biblical accounts. This evidence was rejected by some writers at the time, such as François-René de Chateaubriand. Chateaubriand wrote in his 1802 book, "Génie du christianisme" (Part I Book IV Chapter V) that "God might have created, and doubtless did create, the world with all the marks of antiquity and completeness which it now exhibits." In modern times, Rabbi Dovid Gottlieb supported a similar position, saying that the objective scientific evidence for an old universe is strong, but wrong, and that the traditional Jewish calendar is correct.

In the middle of the 19th century, the disagreement between scientific evidence about the age of the Earth and the Western religious traditions was a significant debate among intellectuals. Gosse published "Omphalos" in 1857 to explain his answer to this question. He concluded that the religious tradition was correct. Gosse began with the earlier idea that the Earth contained mature organisms at the instant they were created, and that these organisms had false signs of their development, such as hair on mammals, which grows over time. He extended this idea of creating a single mature organism to creating mature systems, and concluded that fossils were an artifact of the creation process and merely part of what was necessary to make creation work. Therefore, he reasoned, fossils and other signs of the Earth's age could not be used to prove the age. His book sold poorly and was widely rejected.

Other contemporary proposals for reconciling the stories of creation in Genesis with the scientific evidence included the "interval theory" or gap theory of creation, in which a large interval of time passed in between the initial creation of the universe and the beginning of the six days of creation. This idea was put forward by Archbishop John Bird Sumner of Canterbury in "Treatise on the Records of Creation". Another popular idea, promoted by the English theologian John Pye Smith, was that the Garden of Eden described the events of only one small location. A third proposal, by French naturalist Georges-Louis Leclerc, Comte de Buffon, held that the six "days" of the creation story were arbitrary and large ages rather than 24-hour periods.

Theologians rejected Gosse's proposal on the grounds that it seemed to make the divine creator tell lies – either lying in the scriptures, or lying in nature. Scientists rejected it on the grounds that it disagreed with uniformitarianism, an explanation of geology that was widely supported at the time, and the impossibility of testing or falsifying the idea.

Some modern creationists still argue against scientific evidence in the same way. For instance, John D. Morris, president of the Institute for Creation Research wrote in 1990 about the "appearance of age":
He does not extend this idea to the geological record, preferring to believe that it was all created in the Flood, but others such as Gerald E. Aardsma go further, with his idea of "virtual history". This appears to suggest that events after the creation have changed the "virtual history" we now see, including the fossils:
The past president of the Missouri Association for Creation has said:

Though Gosse's original omphalos hypothesis specifies a popular creation story, others have proposed that the idea does not preclude creation as recently as five minutes ago, including memories of times before this created "in situ". This idea is sometimes called Last Thursdayism by its opponents, as in "the world might as well have been created last Thursday."

The concept is both unverifiable and unfalsifiable through any conceivable scientific study—in other words, it is impossible even "in principle" to subject it to any form of test, by reference to any empirical data, because the empirical data themselves are considered to have been arbitrarily created to look the way they do at every observable level of detail.

From a religious viewpoint, it can be interpreted as God having "created a fake", such as illusions of light in space of stellar explosions (supernovae) that never really happened, or volcanic mountains that were never really volcanoes in the first place and that never actually experienced erosion.

This conception has therefore drawn harsh rebuke from some theologians. Reverend Canon Brian Hebblethwaite, for example, preached against Bertrand Russell's Five-minute hypothesis:

The basis for Hebblethwaite's objection, however, is the presumption of a God that would not deceive people about their very humanity—an unprovable presumption that the omphalos hypothesis rejects at the outset. Hebblethwaite also suggests that God necessarily had to create certain elements of the Universe in combination with the creation of man:

In a rebuttal of the claim that God might have implanted a false history of the age of the Universe in order to test our faith in the truth of the Torah, Rabbi Natan Slifkin, an author whose works have been banned by several Haredi rabbis for going against the tenets of the Talmud, writes:

The five-minute hypothesis is a skeptical hypothesis put forth by the philosopher Bertrand Russell that proposes that the universe sprang into existence five minutes ago from nothing, with human memory and all other signs of history included. It is a commonly used example of how one may maintain extreme philosophical skepticism with regard to memory.

Jorge Luis Borges, in his 1940 work, "Tlön, Uqbar, Orbis Tertius", describes a fictional world in which some essentially follow as a religious belief a philosophy much like Russell's discussion on the logical extreme of Gosse's theory:

Borges had earlier written a short essay, "The Creation and P. H. Gosse" that explored the rejection of Gosse's "Omphalos". Borges argued that its unpopularity stemmed from Gosse's explicit (if inadvertent) outlining of what Borges characterized as absurdities in the Genesis story.




</doc>
<doc id="22702" url="https://en.wikipedia.org/wiki?curid=22702" title="Origen">
Origen

Origen of Alexandria ( 184 – 253), also known as Origen Adamantius, was an early Christian scholar, ascetic, and theologian who was born and spent the first half of his career in Alexandria. He was a prolific writer who wrote roughly 2,000 treatises in multiple branches of theology, including textual criticism, biblical exegesis and biblical hermeneutics, homiletics, and spirituality. He was one of the most influential figures in early Christian theology, apologetics, and asceticism. He has been described as "the greatest genius the early church ever produced".

Origen sought martyrdom with his father at a young age, but was prevented from turning himself in to the authorities by his mother. When he was eighteen years old, Origen became a catechist at the Catechetical School of Alexandria. He came into conflict with Demetrius, the bishop of Alexandria, in 231 after he was ordained as a presbyter by his friend, the bishop of Caesarea, while on a journey to Athens through Palestine. Demetrius condemned Origen for insubordination and accused him of having castrated himself and of having taught that even Satan would eventually attain salvation, an accusation which Origen himself vehemently denied. Origen founded the Christian School of Caesarea, where he taught logic, cosmology, natural history, and theology, and became regarded by the churches of Palestine and Arabia as the ultimate authority on all matters of theology. He was tortured for his faith during the Decian persecution in 250 and died three to four years later from his injuries.

Origen was able to produce a massive quantity of writings due to the patronage of his close friend Ambrose, who provided him with a team of secretaries to copy his works, and may be the most prolific of all ancient writers. His treatise "On the First Principles" systematically laid out the principles of Christian theology and became the foundation for later theological writings. He also authored "Contra Celsum", the most influential work of early Christian apologetics, in which he defended Christianity against the pagan philosopher Celsus, one of its foremost early critics. Origen produced the "Hexapla", the first critical edition of the Hebrew Bible, which contained the original Hebrew text as well as five different Greek translations of it, all written in columns, side-by-side. He wrote hundreds of homilies covering almost the entire Bible, interpreting many passages as allegorical. Origen taught that, before the creation of the material universe, God had created the souls of all the intelligent beings. These souls, at first fully devoted to God, fell away from him and were given physical bodies. Origen was the first to propose the ransom theory of atonement in its fully developed form and, though he was probably a Subordinationist, he also significantly contributed to the development of the concept of the Trinity. Origen hoped that all people might eventually attain salvation, but was always careful to maintain that this was only speculation. He firmly believed in free will and advocated Christian pacifism.

Origen is a Church Father and is widely regarded as one of the most important Christian theologians of all time. His teachings were especially influential in the east, with Athanasius of Alexandria and the three Cappadocian Fathers being among his most devoted followers. Despite this, Origen was never canonized as a saint because some groups believed that some of his teachings contradicted those attributed to the apostles, notably the Apostles Paul and John. Origen was posthumously condemned as a heretic by a Council of Alexandria in the year 400. In 543, the emperor Justinian I again condemned him as a heretic and ordered all his writings to be burned. The Second Council of Constantinople in 553 may have anathemized Origen, or it may have only condemned certain heretical teachings which claimed to be derived from Origen. His teachings on the pre-existence of souls were rejected by the Church.

Origen's Greek name "Ōrigénēs" () probably means "child of Horus" (from , "Horus", and , "born"). His nickname or cognomen "Adamantios" () derives from Greek "adámas" (), which means "adamant", "unalterable", "unbreakable", "unconquerable", "diamond".

Origen was born in either 185 or 186 AD in Alexandria. Origen's father was Leonides of Alexandria, a respected professor of literature and also devout Christian who practiced his religion openly. Origen's mother, whose name is unknown, may have been a member of the lower class who did not have the right of citizenship. It is likely that, on account of his mother's status, Origen himself was not a Roman citizen. Origen's father taught him about literature and philosophy, and also about the Bible and Christian doctrine. The later Christian historian Eusebius, who is the main source of information about Origen, states that Origen was so learned about the holy scriptures at an early age that his father was unable to answer his questions.

In 202, when Origen was "not yet seventeen", the Roman emperor Septimius Severus ordered Roman citizens who openly practiced Christianity to be executed. Origen's father Leonides was arrested and thrown in prison. Eusebius reports that Origen wanted to turn himself in to the authorities so they would execute him as well, but his mother hid all his clothes and he was unable to go to the authorities since he refused to leave the house naked. Even if Origen had turned himself in, it is unlikely that he would have been punished, since the emperor was only intent on executing Roman citizens. Leonides was beheaded and the state confiscated the family's entire property, leaving them broken and impoverished. Origen was the eldest of eleven children and, as his father's heir, it became his responsibility to provide for the whole family.

When he was eighteen years old, Origen was appointed as a catechist at the Catechetical School of Alexandria. Many scholars have assumed that Origen became the head of the school, but this is highly improbable and it is more likely that he was simply given a paid teaching position, perhaps as a "relief effort" for his destitute family. While employed at the school, he adopted the ascetic lifestyle of the Greek Sophists. He spent the whole day teaching and would stay up late at night writing treatises and commentaries. He was a teetotaler and a vegetarian and he often fasted for long periods of time. Although Eusebius goes to great lengths to portray Origen as one of the Christian monastics of his own era, this portrayal is now generally recognized as anachronistic.

According to Eusebius, as a young man, Origen was taken in by a wealthy Gnostic woman, who was also the patron of a very influential Gnostic theologian from Antioch, who frequently lectured in her home. Eusebius goes to great lengths to insist that, although Origen studied while in her home, he never once "prayed in common" with her or the Gnostic theologian. Later, Origen succeeded in converting a wealthy man named Ambrose from Valentinian Gnosticism to orthodox Christianity. Ambrose was so impressed by the young scholar that he gave Origen a house, a secretary, seven stenographers, a crew of copyists and calligraphers, and paid for all of his writings to be published.

Sometime when he was in his early twenties, Origen sold the small library which he had inherited from his father for a sum which netted him a daily income of four obols. He used this money to continue his study of philosophy. Origen studied at numerous schools throughout Alexandria, including the Platonic Academy of Alexandria, where he was a student of Ammonius Saccas. Eusebius claims that Origen studied under Clement of Alexandria, but this is almost certainly a retrospective assumption based on the similarity of their teachings. Origen himself rarely mentions Clement in his own writings and, when he does, it is usually to correct him.

In his early twenties, Origen became less interested in being a grammarian and more interested in being a rhetor-philosopher. He gave his job as a catechist to his younger colleague Heraclas. Meanwhile, Origen began to style himself as a "master of philosophy". Origen's new position as a self-styled Christian philosopher brought him into conflict with Demetrius, the bishop of Alexandria. Demetrius was a charismatic leader who ruled the Christian congregation of Alexandria with an iron fist and he was the one who was most directly responsible for the elevation of the bishop of Alexandria; prior to Demetrius, the bishop of Alexandria had merely been a priest who was elected to represent his fellows, but after Demetrius, the bishop was seen as clearly a rank higher than his fellow priests. By styling himself as an independent philosopher, Origen was reviving a role that had been prominent in earlier Christianity, but which challenged the authority of the now-powerful bishop.

Meanwhile, Origen began composing his massive theological treatise "On the First Principles", a landmark book which systematically laid out the foundations of Christian theology for centuries to come. Origen also began travelling abroad to visit schools across the Mediterranean. In 212 AD, he travelled to Rome, which was a major center of philosophy at the time. In Rome, Origen attended lectures by Hippolytus of Rome and was influenced by his "logos" theology. In 213 or 214, the governor of Arabia sent a message to the prefect of Egypt requesting him to send Origen to meet with him so that he could interview him and learn more about Christianity from its leading intellectual. Origen was escorted by official bodyguards and spent a short time in Arabia with the governor before returning to Alexandria.

In the autumn of 215, the Roman emperor Caracalla visited Alexandria. During the visit, the students at the schools there protested and made fun of him for having murdered his brother Geta. Caracalla was incensed and ordered his troops to ravage the city, execute the governor, and kill all the protesters. He also commanded them to expel all the teachers and intellectuals from the city. Origen fled Alexandria and travelled to the city of Caesarea Maritima in Roman province of Palestine, where the bishops Theoctistus of Caesarea and Alexander of Jerusalem became his devoted admirers and asked him to deliver discourses on the scriptures in their respective churches. This effectively amounted to letting Origen deliver homilies, even though he was not formally ordained. While this was not at all a completely unexpected phenomenon, especially given Origen's international fame as a teacher and philosopher, it infuriated Demetrius, who saw it as a direct undermining of his authority. Demetrius sent deacons from Alexandria to demand that the Palestinian hierarchs immediately return "his" catechist to Alexandria. He also issued a decree chastising the Palestinians for allowing a person who was not ordained to preach. The Palestinian bishops, in turn, issued their own condemnation, accusing Demetrius of being jealous of Origen's fame and prestige.

Origen obeyed Demetrius's order and returned to Alexandria, bringing with him an antique scroll he had purchased at Jericho containing the full text of the Hebrew Bible. The manuscript, which had purportedly been found "in a jar", became the source text for one of the two Hebrew columns in Origen's "Hexapla", the first critical edition of the Old Testament, containing two versions of the Hebrew text and four Greek translations of it, all written in columns next to each other. Origen studied the Old Testament in great depth; Eusebius even claims that Origen learned Hebrew. Most modern scholars agree that this is implausible, but they disagree on how much Origen actually knew about the language. H. Lietzmann concludes that Origen probably only knew the Hebrew alphabet and not much else; whereas, R. P. C. Hanson and G. Bardy argue that Origen had a superficial understanding of the language, but not enough to have composed the entire "Hexapla". A note in Origen's "On the First Principles" mentions an unknown "Hebrew master", but this was probably a consultant, not a teacher.

Origen also studied the entire New Testament, but especially the epistles of the apostle Paul and the Gospel of John, the writings which Origen regarded as the most important and authoritative. At Ambrose's request, Origen composed the first five books of his exhaustive "Commentary on the Gospel of John", He also wrote the first eight books of his "Commentary on Genesis", his "Commentary on Psalms 1-25", and his "Commentary on Lamentations". In addition to these commentaries, Origen also wrote two books on the resurrection of Jesus and ten books of "Stromata". It is likely that these works contained much theological speculation, which brought Origen into even greater conflict with Demetrius.

Origen repeatedly asked Demetrius to ordain him as a priest, but Demetrius continually refused. In around 231, Demetrius sent Origen on a mission to Athens. Along the way, Origen stopped in Caesarea, where he was warmly greeted by the bishops Theoctistus and Alexander of Jerusalem, who had become his close friends during his previous stay. While he was visiting Caesarea, Origen asked Theoctistus to ordain him as a priest. Theoctistus gladly complied. Upon learning of Origen's ordination, Demetrius was outraged and issued a condemnation declaring that Origen's ordination by a foreign bishop was an act of insubordination.

Eusebius reports that, as a result of Demetrius's condemnations, Origen decided not to return to Alexandria and to instead take up permanent residence in Caesarea. John Anthony McGuckin, however, argues that Origen had probably already been planning to stay in Caesarea. The Palestinian bishops declared Origen the chief theologian of Caesarea. Firmilian, the bishop of Caesarea Mazaca in Cappadocia, was such a devoted disciple of Origen that he begged him to come to Cappadocia and teach there.

Demetrius raised a storm of protests against the bishops of Palestine and the church synod in Rome itself. According to Eusebius, Demetrius published the salacious allegation that Origen had secretly castrated himself, a capital offense under Roman law at the time and one which would have made Origen's ordination invalid, since eunuchs were forbidden from becoming priests. Demetrius also alleged that Origen had taught a form of "apokatastasis", which held that all beings, including even Satan himself, would eventually attain salvation. This allegation probably arose from a misunderstanding of Origen's argument during a debate with the Valentinian heretic Candidus. Candidus had argued in favor of predestination by declaring that the Devil was beyond salvation. Origen had responded by arguing that, if the Devil is destined for eternal damnation, it was on account of his actions, which were the result of his own free will. Therefore, Origen had declared that Satan was only morally reprobate, not absolutely reprobate.

Demetrius died in 232, within less than a year after Origen's departure from Alexandria. The accusations against Origen faded with the death of Demetrius, but they did not disappear entirely and they continued to haunt him for the rest of his career. Origen defended himself in his "Letter to Friends in Alexandria", in which he vehemently denied that he had ever taught that the Devil would attain salvation and insisted that the very notion of the Devil attaining salvation was simply ludicrous.

Origen never mentions anything about having castrated himself in his writings and, in his exegesis of , he strongly condemns any literal interpretation of the passage, asserting that only an idiot would interpret the passage in such a way. Eusebius, however, accepts the story of Origen's self-castration as genuine, seeing it as a misguided literal interpretation of this very passage. The report of self-castration was accepted throughout the Middle Ages and was cited by Peter Abelard in his letters to Heloise. Edward Gibbon, in his "History of the Decline and Fall of the Roman Empire", also accepts this story as true. Since the beginning of the twentieth century, some scholars have questioned the historicity of Origen's self-castration, instead seeing it as a wholesale invention by Demetrius. Henry Chadwick argues that, while the story may be true, it seems unlikely, given that Origen's exposition of Matthew 19:12 "strongly deplored any literal interpretation of the words". However, many noted historians, such as Peter Brown and William Placher, continue to find no reason to conclude that the story is false. Placher theorizes that, if it is true, it may have followed an episode in which Origen received some raised eyebrows while privately tutoring a woman.

During his early years in Caesarea, Origen's primary task was the establishment of a Christian School; Caesarea had long been seen as a center of learning for Jews and Hellenistic philosophers, but, until Origen's arrival, it had lacked a Christian center of higher education. According to Eusebius, the school Origen founded was primarily targeted towards young pagans who had expressed interest in Christianity, but were not yet ready to ask for baptism. The school therefore sought to explain Christian teachings through Middle Platonism. Origen started his curriculum by teaching his students classical Socratic reasoning. After they had mastered this, he taught them cosmology and natural history. Finally, once they had mastered all of these subjects, he taught them theology, which was the highest of all philosophies, the accumulation of everything they had learning previously.

With the establishment of the Caesarean school, Origen's reputation as a scholar and theologian reached its zenith and he became known throughout the Mediterranean world as a brilliant intellectual. The hierarchs of the Palestinian and Arabian church synods regarded Origen as the ultimate expert on all matters dealing with theology. While teaching in Caesarea, Origen resumed work on his "Commentary on John", composing at least books six through ten. In the first of these books, Origen compares himself to "an Israelite who has escaped the perverse persecution of the Egyptians." Origen also wrote the treatise "On Prayer" at the request of his friend Ambrose and his "sister" Tatiana, in which he analyzes the different types of prayers described in the Bible and offers a detailed exegesis on the Lord's Prayer.

Not only Christians, but also pagans took a fascination with Origen. The Neoplatonist philosopher Porphyry heard of Origen's fame and travelled to Caesarea to listen to his lectures. Porphyry recounts that Origen had extensively studied the teachings of Pythagoras, Plato, and Aristotle, but also those of important Middle Platonists, Neopythagoreans, and Stoics, including Numenius of Apamea, Chronius, Apollophanes, Longinus, Moderatus of Gades, Nicomachus, Chaeremon, and Cornutus. Nonetheless, Porphyry accused Origen of having betrayed true philosophy by subjugating its insights to the exegesis of the Christian scriptures. Eusebius reports that Origen was summoned from Caesarea to Antioch at the behest of Julia Avita Mamaea, the mother of the Roman emperor Severus Alexander, "to discuss Christian philosophy and doctrine with her."

In 235, approximately three years after Origen began teaching in Caesarea, Alexander Severus, who had been tolerant towards Christians, was murdered and the new emperor Maximinus Thrax instigated a purge of all those who had supported his predecessor. His pogroms targeted Christian leaders and, in Rome, Pope Pontianus and Hippolytus of Rome were both sent into exile. Origen knew that he was in danger and went into hiding in the home of a faithful Christian woman named Juliana the Virgin, who had been a student of the Ebionite leader Symmachus. Origen's close friend and longtime patron Ambrose was arrested in Nicomedia and Protoctetes, the leading priest in Caesarea, was also arrested. In their honor, Origen composed his treatise "Exhortation to Martyrdom", which is now regarded as one of the greatest classics of Christian resistance literature. After coming out of hiding following Maximinus's death, Origen founded a school where Gregory Thaumaturgus, later bishop of Pontus, was one of the pupils. He preached regularly on Wednesdays and Fridays, and later daily.

Sometime between 238 and 244, Origen visited Athens, where he completed his "Commentary on the Book of Ezekiel" and began writing his "Commentary on the Song of Songs". After visiting Athens, he visited Ambrose in Nicomedia. According to Porphyry, Origen also travelled to Rome or Antioch, where he met Plotinus, the founder of Neoplatonism. The Christians of the eastern Mediterranean continued to revere Origen as the most orthodox of all theologians and, when the Palestinian hierarchs learned that Beryllus, the bishop of Bostra and one of the most energetic Christian leaders of the time, had been preaching adoptionism (i.e., belief that Jesus was born human and only became divine after his baptism), they sent Origen to convert him to orthodoxy. Origen engaged Beryllus in a public disputation, which went so successfully that Beryllus promised to only teach Origen's theology from then on. On another occasion, a Christian leader in Arabia named Heracleides began teaching that the soul was mortal and that it perished with the body. Origen refuted these teachings, arguing that the soul is immortal and can never die.

In 249, the Plague of Cyprian broke out. In 250, Emperor Decius, believing that the plague was caused by Christians' failure to recognise him as Divine, issued a decree for Christians to be persecuted. This time Origen did not escape. Eusebius recounts how Origen suffered "bodily tortures and torments under the iron collar and in the dungeon; and how for many days with his feet stretched four spaces in the stocks". The governor of Caesarea gave very specific orders that Origen was not to be killed until he had publicly renounced in faith in Christ. Origen endured two years of imprisonment and torture, but obstinately refused to renounce his faith. In 252, the emperor Decius was assassinated and Origen was released from prison. Nonetheless, Origen's health was broken by the physical tortures enacted on him and he died less than a year later at the age of sixty-nine. A later legend, recounted by Jerome and numerous itineraries, places his death and burial at Tyre, but little value can be attached to this.

Origen was an extremely prolific writer. According to Epiphanius, he wrote a grand total of roughly 6,000 works over the course of his lifetime. Most scholars agree that this estimate is probably somewhat exaggerated. According to Saint Jerome, Eusebius listed the titles of just under 2,000 treatises written by Origen in his lost "Life of Pamphilus". Jerome compiled an abbreviated list of Origen's major treatises, itemizing 800 different titles.

By far the most important work of Origen on textual criticism was the "Hexapla" ("Sixfold"), a massive comparative study of various translations of the Old Testament in six columns: Hebrew, Hebrew in Greek characters, the Septuagint, and the Greek translations of Theodotion (a Jewish scholar from 180 AD), Aquila of Sinope (another Jewish scholar from 117-138), and Symmachus (an Ebionite scholar from 193-211). Origen was the first Christian scholar to introduce critical markers to a Biblical text. He marked the Septuagint column of the "Hexapla" using signs adapted from those used by the textual critics of the Great Library of Alexandria: a passage found in the Septuagint that was not found in the Hebrew text would be marked with an "asterisk" (*) and a passage that was found in other Greek translations, but not in the Septuagint, would be marked with an "obelus" (÷).

The "Hexapla" was the cornerstone of the Great Library of Caesarea, which Origen founded. It was still the centerpiece of the library's collection by the time of Jerome, who records having used it in his letters on multiple occasions. When the emperor Constantine the Great ordered fifty complete copies of the Bible to be transcribed and disseminated across the empire, Eusebius used the "Hexapla" as the master copy for the Old Testament. Although the original "Hexapla" has been lost, the text of it has survived in numerous fragments and a more-or-less complete Syraic copy of it made by the seventh-century bishop Paul of Tella has also survived. For some sections of the "Hexapla", Origen also added additional columns containing other Greek translations; for the Book of Psalms, he included no less than eight Greek translations, making this section known as "Enneapla" ("Ninefold"). Origen also produced the "Tetrapla" ("Fourfold"), a smaller, abridged version of the "Hexapla" containing only the four Greek translations and not the original Hebrew text.

According to Jerome's "Epistle" 33, Origen wrote extensive "scholia" on the books of Exodus, Leviticus, Isaiah, Psalms 1-15, Ecclesiastes, and the Gospel of John. None of these "scholia" have survived intact, but parts of them were incorporated into the "Catenaea", a collection of excerpts from major works of Biblical commentary written by the Church Fathers. Other fragments of the "scholia" are preserved in Origen's "Philocalia" and in Pamphilus of Caesarea's apology for Origen. The "Stromateis" were of a similar character, and the margin of "Codex Athous Laura", 184, contains citations from this work on Rom. 9:23; I Cor. 6:14, 7:31, 34, 9:20-21, 10:9, besides a few other fragments. Origen composed homilies covering almost the entire Bible. There are 205, and possibly 279, homilies of Origen that are extant either in Greek or in Latin translations.

The homilies preserved are on Genesis (16), Exodus (13), Leviticus (16), Numbers (28), Joshua (26), Judges (9), I Sam. (2), Psalms 36-38 (9), Canticles (2), Isaiah (9), Jeremiah (7 Greek, 2 Latin, 12 Greek and Latin), Ezekiel (14), and Luke (39). The homilies were preached in the church at Caesarea, with the exception of the two on 1 Samuel which were delivered in Jerusalem. Nautin has argued that they were all preached in a three-year liturgical cycle some time between 238 and 244, preceding the "Commentary on the Song of Songs", where Origen refers to homilies on Judges, Exodus, Numbers, and a work on Leviticus. On June 11, 2012, the Bavarian State Library announced that the Italian philologist Marina Molin Pradel had discovered twenty-nine previously unknown homilies by Origen in a twelfth-century Byzantine manuscript from their collection. Prof. Lorenzo Perrone of the Bologna University and other experts confirmed the authenticity of the homilies. The texts of these manuscripts can be found online.

Origen is the main source of information on the use of the texts that were later officially canonized as the New Testament. The information used to create the late-fourth-century Easter Letter, which declared accepted Christian writings, was probably based on the lists given in Eusebius's "Ecclesiastical History" HE 3:25 and 6:25, which were both primarily based on information provided by Origen. Origen accepted the authenticity of the epistles of 1 John, 1 Peter, and Jude without question and accepted the Epistle of James as authentic with only slight hesitation. He also refers to 2 John, 3 John, and 2 Peter, but notes that all three were suspected to be forgeries. Origen may have also considered other writings to be "inspired" that were rejected by later authors, including the Epistle of Barnabas, Shepherd of Hermas, and 1 Clement. "Origen is not the originator of the idea of biblical canon, but he certainly gives the philosophical and literary-interpretative underpinnings for the whole notion."

Origen's commentaries written on specific books of scripture are much more focused on systematic exegesis than his homilies. In these writings, Origen applies the precise critical methodology that had been developed by the scholars of the Mouseion in Alexandria to the Christian scriptures. The commentaries also display Origen's impressive, encyclopedic knowledge of various subjects and his ability to cross-reference specific words, listing every place in which a word appears in the scriptures along with all the word's known meanings, a feat made all the more impressive by the fact that he did this in a time when Bible concordances had not yet been invented. Origen's massive "Commentary on the Gospel of John", which spanned more than thirty-two volumes once it was completed, was written with the specific intention to not only expound the correct interpretation of the scriptures, but also to refute the interpretations of the Valentinian heretic Heracleon, who had used the Gospel of John to support his argument that there were really two gods, not one. Of the original thirty-two books in the "Commentary on John", only nine have been preserved: Books I, II, X, XIII, XX, XXVIII, XXXII, and a fragment of XIX.

Of the original twenty-five books in Origen's "Commentary on the Gospel of Matthew", only eight have survived in the original Greek (Books 10-17), covering Matthew 13.36-22.33. An anonymous Latin translation beginning at the point corresponding to Book 12, Chapter 9 or the Greek text and covering Matthew 16.13-27.66 has also survived. The translation contains parts that are not found in the original Greek and is missing parts that are found in it. Origen's "Commentary on the Gospel of Matthew" was universally regarded as a classic, even after his condemnation, and it ultimately became the work which established the Gospel of Matthew as the primary gospel. Origen's "Commentary on the Epistle to the Romans" was originally fifteen books long, but only tiny fragments of it have survived in the original Greek. An abbreviated Latin translation in ten books produced by the monk Tyrannius Rufinus at the end of the fourth century. The historian Socrates Scholasticus records that Origen had included an extensive discussion of the application of the title "theotokos" to the Virgin Mary in his commentary, but this discussion is not found in Rufinus's translation, probably because Rufinus did not approve of Origen's position on the matter, whatever that might have been.

Origen also composed a "Commentary on the Song of Songs", in which he took explicit care to explain why the Song of Songs was relevant to a Christian audience. The "Commentary on the Song of Songs" was Origen's most celebrated commentary and Jerome famously writes in his preface to his translation of two of Origen's homilies over the Song of Songs that "In his other works, Origen habitually excels others. In this commentary, he excelled himself." Origen expanded on the exegesis of the Jewish Rabbi Akiva, interpreting the Song of Songs as a mystical allegory in which the bridegroom represents the Logos and the bride represents the soul of the believer. This was the first Christian commentary to expound such an interpretation and it became extremely influential on later interpretations of the Song of Songs. Despite this, the commentary now only survives in part through a Latin translation of it made by Tyrannius Rufinus in the year 410. Fragments of some other commentaries survive. Citations in Origen's "Philokalia" include fragments of the third book of the commentary on Genesis. There is also Ps. i, iv.1, the small commentary on Canticles, and the second book of the large commentary on the same, the twentieth book of the commentary on Ezekiel, and the commentary on Hosea. Of the non-extant commentaries, there is limited evidence of their arrangement.

Origen's "On the First Principles" was the first ever systematic exposition of Christian theology. He composed it as a young man between the years 220 and 230 while he was still living in Alexandria. Fragments from Books 3.1 and 4.1-3 of Origen's Greek original are preserved in Origen's "Philokalia". A few smaller quotations of the original Greek are preserved in Justinian's "Letter to Mennas". The vast majority of the text has only survived in a heavily abridged Latin translation produced by Tyrannius Rufinus in 397. Rufinus was convinced that Origen's original treatise had been interpolated by heretics and that these interpolations were the source of the heterodox teachings found in it. He therefore heavily modified Origen's text, omitting and altering any parts which disagreed with contemporary Christian orthodoxy. Jerome was so appalled by Rufinus's lack of fidelity to the original Greek that he resolved to produce his own Latin translation of "On the First Principles" in which he would translate every word exactly as it was written and lay bare Origen's heresies to the whole world. Jerome's translation, however, has been lost in its entirety. "On the First Principles" begins with an essay explaining the nature of theology. Book One describes the heavenly world, and includes descriptions of the oneness of God, the relationship between the three persons of the Trinity, the nature of the divine spirit, reason, and angels. Book Two describes the world of man, including the incarnation of the Logos, the soul, free will, and eschatology. Book Three deals with cosmology, sin, and redemption. Book Four deals with teleology and the interpretation of the scriptures.

Between 232-235, while in Caesarea in Palestine, Origen wrote "On Prayer", of which the full text has been preserved in the original Greek. After an introduction on the object, necessity, and advantage of prayer, ends with an exegesis of the Lord's Prayer, concluding with remarks on the position, place, and attitude to be assumed during prayer, as well as on the classes of prayer. "On Martyrdom", or the "Exhortation to Martyrdom", also preserved entire in Greek, was written some time after the beginning of the persecution of Maximinus in the first half of 235. In it, Origen warns against any trifling with idolatry and emphasises the duty of suffering martyrdom manfully; while in the second part he explains the meaning of martyrdom.

"Against Celsus" (Greek: Κατὰ Κέλσου; Latin: "Contra Celsum"), preserved entirely in Greek, was Origen's last treatise, written about 248. It is an apologetic work defending orthodox Christianity against the attacks of the pagan philosopher Celsus, who was seen in the ancient world as early Christianity's foremost opponent. In 178, Celsus had written a polemic entitled "On the True Word", in which he had made numerous arguments against Christianity. The church had responded by ignoring Celsus's attacks, but Origen's patron Ambrose brought the matter to his attention. Origen initially wanted to ignore Celsus and let his attacks fade, but one of Celsus's major claims, which held that no self-respecting philosopher of the Platonic tradition would ever be so stupid as to become a Christian, provoked him to write a rebuttal.

In the book, Origen systematically refutes each of Celsus's arguments point-by-point and argues for a rational basis of Christian faith. Origen draws heavily on the teachings of Plato and argues that Christianity and Greek philosophy are not incompatible, and that philosophy contains much that is true and admirable, but that the Bible contains far greater wisdom than anything Greek philosophers could ever grasp. Origen responds to Celsus's accusation that Jesus had performed his miracles using magic rather than divine powers by asserting that, unlike magicians, Jesus had not performed his miracles for show, but rather to reform his audiences. "Contra Celsum" became the most impactful of all early Christian apologetics works; before it was written, Christianity was seen by many as merely a folk religion for the illiterate and uneducated, but Origen raised it to a level of academic respectability. Eusebius admired "Against Celsus" so much that, in his "Against Hierocles" 1, he declared that "Against Celsus" provided an adequate rebuttal to all criticisms the church would ever face.

The papyri discovered at Tura in 1941 contained the Greek texts of two previously unknown works of Origen. Neither work can be dated precisely, though both were probably written after the persecution of Maximinus in 235. One is "On the Pascha". The other is "Dialogue with Heracleides", a record written by one of Origen's stenographers of a debate between Origen and the Arabian bishop Heracleides, a Quasi-Monarchianist who taught that the Father and the Son were the same. In the dialogue, Origen uses Socratic questioning to persuade Heracleides to believe in the "Logos theology", which was essentially the prototypical form of later Trinitarianism.

Lost works include two books on the resurrection, written before "On First Principles", and also two dialogues on the same theme dedicated to Ambrose. Eusebius had a collection of more than one hundred letters of Origen, and the list of Jerome speaks of several books of his epistles. Except for a few fragments, only three letters have been preserved. The first, partly preserved in the Latin translation of Rufinus, is addressed to friends in Alexandria. The second is a short letter to Gregory Thaumaturgus, preserved in the "Philocalia". The third is an epistle to Sextus Julius Africanus, extant in Greek, replying to a letter from Africanus (also extant), and defending the authenticity of the Greek additions to the book of Daniel. Forgeries of the writings of Origen made in his lifetime are discussed by Rufinus in "De adulteratione librorum Origenis". The "Dialogus de recta in Deum fide", the "Philosophumena" attributed to Hippolytus of Rome, and the "Commentary on Job" by Julian the Arian have also been ascribed to him.

One of Origen's main teachings was the doctrine of the preexistence of souls, which held that before God created the material world he created a vast number of incorporeal "spiritual intelligences" (ψυχαί). All of these souls were at first devoted to the contemplation and love of their Creator, but, as the fervor of the divine fire cooled, almost all of these intelligences eventually grew bored of contemplating God, and their love for him "cooled off" (ψύχεσθαι). When God created the world, the souls which had previously existed without bodies became incarnate. Those whose love for God diminished the most became demons. Those whose love diminished moderately became human souls, eventually to be incarnated in fleshly bodies. Those whose love diminished the least became angels. One soul, however, who remained perfectly devoted to God became, through love, one with the Word (Logos) of God. The Logos eventually took flesh and was born of the Virgin Mary, becoming the God-man Jesus Christ.

Origen may or may not have believed in the Platonic teaching of "metempsychosis" ("the transmigration of souls"; i.e. reincarnation). He explicitly rejects "the false doctrine of the transmigration of souls into bodies", but this may refer only to a specific kind of transmigration. Geddes MacGregor has argued that Origen must have believed in "metempsychosis" because it makes sense within his eschatology and is never explicitly denied in the Bible. Roger E. Olson, however, dismisses the view that Origen believed in reincarnation as a New Age misunderstanding of Origen's teachings. It is certain that Origen rejected the Stoic notion of a cyclical universe, which is directly contrary to his eschatology.

Origen was an ardent believer in free will and he adamantly rejected the Valentinian idea of election. Instead, Origen believed that even disembodied souls have the power to make their own decisions. Furthermore, in his interpretation of the story of Jacob and Esau, Origen argued that the condition into which a person is born is actually dependent upon what their souls did in this pre-existent state. According to Origen, the superficial unfairness of a person's condition at birth—with some humans being poor, others rich, some being sick, and others healthy—is actually a by-product of what the person's soul had done in the pre-existent state. Origen defends free will in his interpretations of instances of divine foreknowledge in the scriptures, arguing that Jesus's knowledge of Judas's future betrayal in the gospels and God's knowledge of Israel's future disobedience in the Deuteronomistic History only show that God knew these events would happen in advance. Origen therefore concludes that the individuals involved in these incidents still made their decisions out of their own free will.

Origen bases every part of his theology on the Christian scriptures and never appeals to Platonic teachings without having first supported his argument with a firm scriptural basis. He saw the scriptures as divinely inspired and was also very cautious to never contradict his own interpretation of what was written in them. Nonetheless, Origen did have a penchant for speculating beyond what was explicitly stated in the Bible, and this habit frequently placed him in the hazy realm between strict orthodoxy and heresy.

According to Origen, there are two kinds of Biblical literature, which are found in both the Old and New Testaments: "historia" ("history, or narrative") and "nomothesia" ("legislation or ethical prescription"). Origen expressly states that the Old and New Testaments should be read together and according to the same rules. Origen further taught that there were three different ways in which passages of scripture could be interpreted. The "flesh" was the literal, historical interpretation of the passage; the "soul" was the moral message behind the passage; and the "spirit" was the eternal, incorporeal reality that the passage conveyed. In Origen's exegesis, the Book of Proverbs, Ecclesiastes, and the Song of Songs represent perfect examples of the bodily, soulful, and spiritual components of scripture respectively.

Origen saw the "spiritual" interpretation as the deepest and most important meaning of the text and taught that some passages held no literal meaning at all and that their meanings were "purely" allegorical. Nonetheless, he stressed that "the passages which are historically true are far more numerous than those which are composed with purely spiritual meanings." Origen noticed that the accounts of Jesus's life in the four canonical gospels contain irreconcilable contradictions, but he argued that these contradictions did not undermine the spiritual meanings of the passages in question. Origen's idea of a twofold creation was based on an allegorical interpretation of the creation story found in the first two chapters of the Book of Genesis. The first creation, described in , was the creation of the primeval spirits, who are made "in the image of God" and are therefore incorporeal like Him; the second creation described in is when the human souls are given ethereal, spiritual bodies and the description in of God clothing Adam and Eve in "tunics of skin" refers to the transformation of these spiritual bodies into corporeal ones. Thus, each phase represents a degradation from the original state of incorporeal holiness.

Origen's conception of God the Father is apophatic—a perfect unity, invisible and incorporeal, transcending all things material, and therefore inconceivable and incomprehensible. He is likewise unchangeable and transcends space and time. But his power is limited by his goodness, justice, and wisdom; and, though entirely free from necessity, his goodness and omnipotence constrained him to reveal himself. This revelation, the external self-emanation of God, is expressed by Origen in various ways, the Logos being only one of many. The revelation was the first creation of God (cf. Prov. viii. 22), in order to afford creative mediation between God and the world, such mediation being necessary, because God, as changeless unity, could not be the source of a multitudinous creation.

The Logos is the rational creative principle that permeates the universe. The Logos acts on all human beings through their capacity for logic and rational thought, guiding them to the truth of God's revelation. As they progress in their rational thinking, all humans become more and more like Christ. Nonetheless, they retain their individuality and do not become subsumed into Christ. Creation came into existence only through the Logos, and God's nearest approach to the world is the command to create. While the Logos is substantially a unity, he comprehends a multiplicity of concepts, so that Origen terms him, in Platonic fashion, "essence of essences" and "idea of ideas".

Origen significantly contributed to the development of the idea of the Trinity. He declared the Holy Spirit to be a part of the Godhead and interpreted the Parable of the Lost Coin to mean that the Holy Spirit dwells within each and every person and that the inspiration of the Holy Spirit was necessary for any kind of speech dealing with God. Origen taught that the activity of all three parts of the Trinity were necessary for a person to attain salvation. In one fragment preserved by Rufinus in his Latin translation of Pamphilus's "Defense of Origen", Origen seems to apply the phrase "homooúsios" (ὁμοούσιος; "of the same substance") to the relationship between the Father and the Son, but in other passages, Origen rejected the belief that the Son and the Father were one "hypostasis" as heretical. According to Rowan Williams, because the words "ousia" and "hypostasis" were used synonymously in Origen's time, Origen almost certainly would have rejected "homoousios" as heretical. Williams states that it is impossible to verify whether the quote that uses the word "homoousios" really comes from Pamphilus at all, let alone Origen.

Nonetheless, Origen was a Subordinationist, meaning he believed that the Father was superior to the Son and the Son was superior to the Holy Spirit, a model based on Platonic proportions. Saint Jerome records that Origen had written that God the Father is invisible to all beings, including even the Son and the Holy Spirit, and that the Son is invisible to the Holy Spirit as well. At one point Origen suggests that the Son was created by the Father and that the Holy Spirit was created by the Son, but, at another point, he writes that "Up to the present I have been able to find no passage in the Scriptures that the Holy Spirit is a created being." At the time when Origen was alive, orthodox views on the Trinity had not yet been formulated and Subordinationism was not yet considered heretical. In fact, virtually all orthodox theologians prior to the Arian controversy in the latter half of the fourth century were Subordinationists to some extent. Origen's Subordinationism may have developed out of his efforts to defend the unity of God against the Gnostics.

Origen writes that Jesus was "the firstborn of all creation [who] assumed a body and a human soul." He firmly believed that Jesus had a human soul and abhorred Docetism (the heretical teaching which held that Jesus had come to earth in spirit form rather than a physical human body). Origen envisioned Jesus's human nature as the one soul that stayed closest to God and remained perfectly faithful to Him, even when all other souls fell away. At Jesus's incarnation, his soul became fused with the Logos and they "intermingled" to become one. Thus, according to Origen, Christ was both human and divine, but, like all human souls, Christ's human nature was existent from the beginning.

Origen was the first to propose the ransom theory of atonement in its fully developed form, although Irenaeus had previously proposed a prototypical form of it. According to this theory, Christ's death on the cross was a ransom to Satan in exchange for humanity's liberation. This theory holds that Satan was tricked by God because Christ was not only free of sin, but also the incarnate Deity, whom Satan lacked the ability to enslave. The theory was later expanded by theologians such as Gregory of Nyssa and Rufinus of Aquileia. In the eleventh century, Saint Anselm criticized the ransom theory, along with the associated Christus Victor theory, resulting in the theory's decline in western Europe. The theory has nonetheless retained some of its popularity in the Eastern Orthodox Church.

Origen believed that, eventually, the whole world would be converted to Christianity, "since the world is continually gaining possession of more souls." He believed that the Kingdom of Heaven was not yet come, but that it was the duty of every Christian to make the eschatological reality of the kingdom present in their lives. Origen was a Universalist, who suggested that all people might eventually attain salvation, but only after being purged of their sins through "divine fire". This, of course, in line of Origen's allegorical interpretation, was not "literal" fire, but rather the inner anguish of knowing one's own sins. Origen was also careful to always maintain that universal salvation was merely a possibility and not a definitive doctrine.

Jerome quotes Origen as having allegedly written that "after aeons and the one restoration of all things, the state of Gabriel will be the same as that of the Devil, Paul's as that of Caiaphas, that of virgins as that of prostitutes." Jerome, however, was not above deliberately altering quotations to make Origen seem more like a heretic and Origen himself expressly stated in his "Letter to Friends in Alexandria" that Satan and his demons would be not included in the final salvation.

Origen was an ardent pacifist and, in his "Against Celsus", he argued that Christianity's inherent pacifism was the most noticeable feature of the religion. While Origen did admit that some Christians served in the Roman army, he pointed out that most did not and insisted that engaging in earthly wars was against the way of Christ. Origen accepted that it was sometimes necessary for a non-Christian state to wage wars, but insisted that it was impossible for a Christian to fight in such a war without compromising his or her faith. Origen explained the violence found in certain passages of the Old Testament as allegories and pointed out Old Testament passages supporting nonviolence, such as and .

Origen is often seen as the first major Christian theologian. Though his orthodoxy had been questioned in Alexandria while he was alive, Origen's torture during the Decian persecution led Pope Dionysius of Alexandria to rehabilitate Origen's memory there, hailing him as a martyr for the faith. Later, after Origen's death, Dionysius became one of the foremost proponents of Origen's theology. Every Christian theologian who came after him was influenced by his theology, whether directly or indirectly. Origen's contributions to theology were so vast and complex, however, that his followers frequently emphasized drastically different parts of his teachings to the expense of other parts. Dionysius emphasized Origen's Subordinationist views, which led him to deny the unity of the Trinity, causing controversy throughout North Africa. At the same time, Origen's other disciple Theognostus of Alexandria taught that the Father and the Son were "of one substance".

For centuries after his death, Origen was regarded as the bastion of orthodoxy and his philosophy practically defined Eastern Christianity. Origen himself was revered as one of the greatest of all Christian teachers; he was especially beloved by monks, who saw themselves as continuing in Origen's ascetic legacy. As time progressed, however, Origen became criticized under the standard of orthodoxy in later eras, rather than the standards of his own lifetime. In the early fourth century, the Christian writer Methodius of Olympus criticized some of Origen's more speculative arguments, but otherwise agreed with Origen on all other points of theology. Peter of Antioch and Eustathius of Antioch also criticized Origen as heretical. Both orthodox theologians and heretics claimed to be following in the tradition Origen had established. Athanasius of Alexandria, the most prominent supporter of the Holy Trinity at the First Council of Nicaea, was a devoted Origenist and so were Basil of Caesarea, Gregory of Nyssa, and Gregory of Nazianzus (the so-called "Cappadocian Fathers").

At the same time, however, many commonalities exist between Origen's theology and Arianism. Although the relationship between the two is still disputed, in antiquity, many orthodox Christians believed that Origen was the true and ultimate source of the Arian heresy. Origen's name was also invoked by heretics of all varieties, many of them supporting views completely different from anything Origen himself had actually taught. In the late fourth century, the heretic-hunter Epiphanius of Salamis attacked Origen, portraying him as an originally orthodox Christian who had been corrupted and turned into a heretic by the evils of "Greek education". Epiphanius further deplores all academic Christians as dangerous heretics and declares that God alone is the source of true wisdom. Around the same time, John Cassian, a Semipelagian monk, introduced Origen's teachings to the West.

In the year 400, Pope Theophilus of Alexandria assembled a council in Alexandria, which condemned Origenism as heretical and labelled Origen himself as the "hydra of all heresies". By the sixth century, Origen's name had become synonymous with heresy. In 543 AD, the Emperor Justinian I denounced Origen as a heretic and ordered all of his writings to be burned. Later that year, Patriarch Mennas of Constantinople condemned Origen and a form of apocatastasis at the Synod of Constantinople (543). In the west, the "Decretum Gelasianum", which was written sometime between 519 and 553, listed Origen as an author whose writings were to be categorically banned.

The Second Council of Constantinople (the Fifth Ecumenical Council) in 553 addressed what was called "The Three Chapters" and opposed a form of Origenism which truly had nothing to do with anything Origen himself had taught. The council is traditionally said to have condemned Origen himself as a heretic along with "The Three Chapters", but it is disputed whether or not this was actually the case, since "It is [only] certain that the council opened on 5 May, 553, in spite of the protestations of Pope Vigilius, who though at Constantinople refused to attend it, and that in the eight conciliary sessions (from 5 May to 2 June), the Acts of which we possess, only the question of the Three Chapters is treated." Many heteroclite views became associated with Origen, and the 15 anathemas attributed to the council condemn a form of apocatastasis along with the pre-existence of the soul, animism (in this context, a heterodox Christology), and a denial of real and lasting resurrection of the body. Some authorities believe these anathemas belong to an earlier local synod. In fact, Popes Vigilius (537–555), Pelagius I (556–61), Pelagius II (579–90), and Gregory the Great (590–604) were only aware that the Fifth Council specifically dealt with "The Three Chapters" and make no mention of Origenism or Universalism, nor spoke as if they knew of its condemnation—even though Gregory the Great was opposed to Universalism.

As a direct result of the numerous condemnations of his work, only a tiny fraction of Origen's voluminous writings have survived. Nonetheless, these writings still amount to a massive number of Greek and Latin texts, very few of which have yet been translated into English. Many more writings have survived in fragments through quotations from later Church Fathers. It is likely that the writings containing Origen's most unusual and speculative ideas have been lost to time, making it nearly impossible to determine whether Origen actually held the heretical views which the anathemas against him ascribed to him. Nonetheless, in spite of the decrees against Origen, the church remained enamored of him and he remained a central figure of Christian theology throughout the first millennium. He continued to be revered as the founder of Biblical exegesis and anyone in the first millennium who took the interpretation of the scriptures seriously would have had knowledge of Origen's teachings.

Jerome's Latin translations of Origen's homilies were widely read in western Europe throughout the Middle Ages and Origen's teachings greatly influenced those of the Byzantine monk Maximus the Confessor ( 550–662) and the Irish theologian John Scotus Eriugena ( 810–877). Since the Renaissance, the debate over Origen's orthodoxy has continued to rage. Western Christians have generally tended to appraise Origen more favorably than eastern ones. In the seventeenth century, the English Cambridge Platonist Henry More (1614 – 1687) was a devoted Origenist and, although he did reject the notion of universal salvation, he accepted most of Origen's other teachings. Pope Benedict XVI expressed admiration for Origen, describing him in a sermon as part of a series on the Church Fathers as "a figure crucial to the whole development of Christian thought", "a true 'maestro'", and "not only a brilliant theologian but also an exemplary witness of the doctrine he passed on". He concludes the sermon by inviting his audience to "welcome into your hearts the teaching of this great master of the faith". Modern Protestant evangelicals admire Origen for his passionate devotion to the scriptures, but are frequently baffled or even appalled by his allegorical interpretation of them, which many believe ignores the literal, historical truth behind them.






</doc>
<doc id="22703" url="https://en.wikipedia.org/wiki?curid=22703" title="Oliver Hazard Perry-class frigate">
Oliver Hazard Perry-class frigate

The "Oliver Hazard Perry" class is a class of guided missile frigates named after the U.S. Commodore Oliver Hazard Perry, the hero of the naval Battle of Lake Erie. Also known as the "Perry" or FFG-7 class, the warships were designed in the United States in the mid-1970s as general-purpose escort vessels inexpensive enough to be bought in large quantities to replace World War II-era destroyers and complement 1960s-era s. In Admiral Elmo Zumwalt's "high low fleet plan", the FFG-7s were the low capability ships with the s serving as the high capability ships. Intended to protect amphibious landing forces, supply and replenishment groups, and merchant convoys from aircraft and submarines, they were also later part of battleship-centred surface action groups and aircraft carrier battle groups/strike groups. Fifty-five ships were built in the United States: 51 for the United States Navy and four for the Royal Australian Navy (RAN). In addition, eight were built in Taiwan, six in Spain, and two in Australia for their navies. Former U.S. Navy warships of this class have been sold or donated to the navies of Bahrain, Egypt, Poland, Pakistan, Taiwan and Turkey.

The first of the 51 U.S. Navy built "Oliver Hazard Perry" frigates entered into service in 1977, and the last remaining in active service, the , was decommissioned on September 29, 2015. The retired vessels were either mothballed or transferred to other navies for continued service. Some of the U.S. Navy's frigates, such as USS "Duncan" (14.6 years in service) had fairly short careers, while a few lasted as long as 30+ years in active U.S. service, with some lasting even longer after being sold or donated to other navies.

The ships were designed by the Bath Iron Works shipyard in Maine in partnership with the New York-based naval architects Gibbs & Cox. The design process was notable as the initial design was accomplished with the help of computers in 18 hours by Raye Montague, a United States Naval Engineer; making it the first ship designed by computer.

The "Oliver Hazard Perry"-class ships were produced in long "short-hull" (Flight I) and long "long-hull" (Flight III) variants. The long-hull ships (FFG 8, 28, 29, 32, 33, and 36-61) carry the larger SH-60 Seahawk LAMPS III helicopters, while the short-hulled warships carry the smaller and less-capable SH-2 Seasprite LAMPS I. Aside from the lengths of their hulls, the principal difference between the versions is the location of the aft capstan: on long-hull ships, it sits a step below the level of the flight deck in order to provide clearance for the tail rotor of the longer Seahawk helicopters. The long-hull ships also carry the RAST (Recovery Assist Securing and Traversing) system (also known as a Beartrap (hauldown device)) for the Seahawk, a hook, cable, and winch system that can reel in a Seahawk from a hovering flight, expanding the ship's pitch-and-roll range in which flight operations are permitted. The FFG 8, 29, 32, and 33 were built as "short-hull" warships but were later modified into "long-hull" warships. "Oliver Hazard Perry"-class frigates were the second class of surface ship (after the s) in the US Navy to be built with gas turbine propulsion. The gas turbine propulsion plant was more automated than other Navy propulsion plants at the time and could be centrally monitored and controlled from a remote engineering control center away from the engines. The gas turbine propulsion plants also allowed the ship's speed to be controlled directly from the bridge via a throttle control, a first for the US Navy.

American shipyards constructed "Oliver Hazard Perry"-class ships for the U.S. Navy and the Royal Australian Navy (RAN). Early American-built Australian ships were originally built as the "short-hull" version, but they were modified during the 1980s to the "long-hull" design. Shipyards in Australia, Spain, and Taiwan have produced several warships of the "long-hull" design for their navies.

Although the per-ship costs rose greatly over the period of production, all 51 ships planned for the U.S. Navy were built.

During the design phase of the "Oliver Hazard Perry" class, head of the Royal Corps of Naval Constructors, R.J. Daniels, was invited by an old friend, US Chief of the Bureau of Ships, Adm Robert C Gooding, to advise upon the use of variable-pitch propellers in the class. During the course of this conversation, Daniels warned Gooding against the use of aluminium in the superstructure of the FFG-7 class as he believed it would lead to structural weaknesses. A number of ships subsequently developed structural cracks, including a fissure in USS "Duncan", before the problems were remedied.

The "Oliver Hazard Perry"-class frigates were designed primarily as anti-aircraft and anti-submarine warfare guided-missile warships intended to provide open-ocean escort of amphibious warfare ships and merchant ship convoys in moderate threat environments in a potential war with the Soviet Union and the Warsaw Pact countries. They could also provide air defense against 1970s- and 1980s-era aircraft and anti-ship missiles. These warships are equipped to escort and protect aircraft carrier battle groups, amphibious landing groups, underway replenishment groups, and merchant ship convoys. They can conduct independent operations to perform such tasks as surveillance of illegal drug smugglers, maritime interception operations, and exercises with other nations.

The addition of the Naval Tactical Data System, LAMPS helicopters, and the Tactical Towed Array System (TACTAS) gave these warships a combat capability far beyond the original expectations. They are well suited to operations in littoral regions, and for most war-at-sea scenarios.

"Oliver Hazard Perry"-class frigates made worldwide news during the 1980s. Despite being small, these frigates were shown to be extremely durable. During the Iran–Iraq War, on 17 May 1987, was attacked by an Iraqi warplane. Struck by two Exocet anti-ship missiles, thirty-seven U.S. Navy sailors died in the deadly prelude to the American Operation Earnest Will, the reflagging and escorting of oil tankers through the Persian Gulf and the Straits of Hormuz.

Less than a year later, on 14 April 1988, was nearly sunk by an Iranian mine. No lives were lost, but 10 sailors were evacuated from the warship for medical treatment. The crew of "Samuel B. Roberts" battled fire and flooding for two days, ultimately managing to save the ship. The U.S. Navy retaliated four days later with Operation Praying Mantis, a one-day attack on Iranian oil platforms being used as bases for raids on merchant shipping. Those had included bases for the minelaying operations that damaged "Samuel B. Roberts". Both frigates were repaired in American shipyards and returned to full service. "Stark" was decommissioned in 1999, and scrapped in 2006. "Roberts" was decommissioned at Mayport on 22 May 2015,

On April 18, 1988, was accompanying the cruiser and frigate when they came under attack from the Iranian gunboat which fired a U.S. made Harpoon missile at the ships. With "Simpson" having the only clear shot, the frigate fired an SM-1 standard missile which struck "Joshan". "Simpson" fired three more SM-1s, and with later naval fire from "Wainwright", sank the Iranian vessel.

On July 14, 2016, took over 12 hours to sink after being used in a live-fire, SINKEX during naval exercise RIMPAC 2016. During the exercise, the ship was directly or indirectly hit with the following ordnance: a Harpoon missile from a South Korean submarine, another Harpoon missile from the Australian frigate , a Hellfire missile from an Australian SH-60S helicopter, another Harpoon missile and a Maverick missile from US maritime patrol aircraft, another Harpoon missile from the cruiser , additional Hellfire missiles from an American SH-60S Navy helicopter, a 2,000-pound Mark 84 bomb from a US Navy F/A-18 Hornet, a GBU-12 Paveway laser-guided 500-pound bomb from a US Air Force B-52 bomber, and a Mark 48 torpedo from an unnamed US Navy submarine.

The U.S. Navy and Royal Australian Navy modified their remaining "Perry"s to reduce their operating costs, replacing Detroit Diesel Company 16V149TI electrical generators with Caterpillar, Inc.- 3512B diesel engines.

From 2004 to 2005, the U.S. Navy removed the frigates' Mk 13 single-arm missile launchers and magazines because the primary missile, the Standard SM-1MR, had become outmoded.

The "zone-defense" anti-aircraft warfare (AAW) capability has vanished, and all that remains is a "point-defense" type of anti-air warfare armament. It would supposedly have been too costly to refit the Standard SM-1MR missiles, which had little ability to bring down sea-skimming missiles. Another reason is to allow more SM-1MRs to go to American allies that operate "Perry"s, such as Poland, Spain, Australia, Turkey, and Taiwan.

The removal of the launchers also stripped the frigates of their Harpoon anti-ship missiles. However, their Seahawk helicopters could still carry the much shorter-range Penguin and Hellfire anti-ship missiles. The last nine ships of the class had new remotely operated 25 mm Mk 38 Mod 2 Naval Gun Systems installed on platforms over the old MK 13 launcher magazine.

Up to 2002, the U.S. Navy updated the remaining active "Oliver Hazard Perry"-class warships' Phalanx CIWS to the "Block 1B" capability, which allowed the Mk 15 20 mm Phalanx gun to shoot at fast-moving surface craft and helicopters. They were also to have been fitted with the Mk 53 DLS "Nulka" missile decoy system, in place of the SRBOC (Super Rapid Blooming Offboard Chaff) and flares, which would have better protected the ship against anti-ship missiles. It had been planned to outfit the remaining ships with a 32-cell RIM-116 Rolling Airframe Missile launcher at the location of the former Mk-13, but this did not occur.

On May 11, 2009, the first International Frigate Working Group met in Mayport Naval Station to discuss maintenance, obsolescence and logistics issues regarding "Oliver Hazard Perry"-class ships of the U.S. and foreign navies.

On June 16, 2009, Vice Admiral Barry McCullough turned down the suggestion of then-U.S. Senator Mel Martinez (R-FL) to keep the "Perry"s in service, citing their worn-out and maxed-out condition. However, U.S. Representative Ander Crenshaw (R-FL) and former U.S. Representative Gene Taylor (D-MS) took up the cause to retain the vessels.

The "Oliver Hazard Perry"-class frigates were to have been eventually replaced by Littoral Combat Ships by 2019. However, the worn out frigates were being retired faster than the LCSs are being built, which may lead to a gap in United States Southern Command mission coverage. According to Navy deactivation plans, all "Oliver Hazard Perry"-class frigates would be retired by October 2015. "Simpson" was the last to be retired (on 29 September 2015), leaving the Navy devoid of frigates for the first time since 1943. The ships will either be made available for sale to foreign navies or dismantled. "Perry"-class frigate retirement was accelerated by budget pressures, which will lead to the remaining 11 ships being replaced by only eight LCS hulls. With the timeline LCS mission packages will come online unknown, there is uncertainty if they will be able to perform the frigates' counter-narcotics and anti-submarine roles when they are gone. The Navy is looking into Military Sealift Command to see if the Joint High Speed Vessel, Mobile Landing Platform, and other auxiliary ships could handle low-end missions that the frigates performed.

The U.S. Coast Guard harvested weapons systems components from decommissioned Navy "Perry"-class frigates to save money. Harvesting components from four decommissioned frigates resulted in more than $24 million in cost savings, which increases with parts from more decommissioned frigates. Equipment including Mk 75, 76 mm/62 caliber gun mounts, gun control panels, barrels, launchers, junction boxes, and other components were returned to service aboard s to extend their service lives into the 2030s.

In June 2017, Chief of Naval Operations Admiral John Richardson revealed the Navy was "taking a hard look" at reactivating 7-8 out of 12 mothballed "Perry"-class frigates to increase fleet numbers. While the move is under consideration, there would be difficulties in returning them to service given the age of the ships and their equipment, likely requiring a significant modernization effort. Although bringing the frigates out of retirement would provide a short-term solution to fleet size, their limited combat capability would restrict them to acting as a theater security cooperation, maritime security asset. Their likely role would be serving as basic surface platforms that stay close to U.S. shores, performing missions such as assisting drug interdiction efforts or patrolling the Arctic so an extensive upgrade to the ships' combat systems wouldn't need to be undertaken. An October 2017 memo recommended against reactivating the frigates, claiming it would cost too much money that would take funding from other Navy priorities to get little effectiveness.

Australia spent A$1.46bn to upgrade the Royal Australian Navy's (RAN) guided-missile frigates, including equipping them to fire the SM-2 version of the Standard missile, adding an eight-cell Mark 41 Vertical Launching System for Evolved Sea Sparrow missiles, and installing better air-search radars and long-range sonar.

The first of the upgraded frigates, , returned to the RAN fleet in 2005. Four frigates eventually upgraded the Garden Island shipyard in Sydney, Australia, with the modernizations lasting between 18 months and two years. The cost of the upgrades was partly offset, in the short run, by the decommissioning and disposal of the two older frigates. was decommissioned on 12 November 2005 at naval base in Western Australia, and was decommissioned at that same naval base on 20 January 2008. was decommissioned at the Garden Island naval base in 2016.

The "Adelaide" class frigates will be replaced by three "Hobart"-class air warfare destroyers equipped with the AEGIS combat system.

The Turkish Navy had commenced the modernization of its s with the GENESIS (Gemi Entegre Savaş İdare Sistemi) combat management system in 2007. The first GENESIS upgraded ship was delivered in 2007, and the last delivery is scheduled for 2011. The "short-hull" "Oliver Hazard Perry"-class frigates that are currently part of the Turkish Navy were modified with the ASIST landing platform system at the Gölcük Naval Shipyard, so that they can accommodate the S-70B Seahawk helicopters. Turkey is planning to add one eight-cell Mk 41 Vertical Launching System (VLS) for the Evolved Sea Sparrow missile, to be installed forward of the present Mk 13 missile launchers, similar to the case in the modernization program of the Australian "Adelaide"-class frigates. TCG "Gediz" was the first ship in the class to receive the Mk 41 VLS installation.
There are also plans for new components to be installed that are being developed for the Milgem-class warships ("Ada"-class corvettes and F-100-class frigates) of the Turkish Navy. These include modern Three-dimensional and X-band radars developed by Aselsan and Turkish-made hull-mounted sonars. One of the G-class frigates will also be used as a test-bed for Turkey's 6,000+ ton AAW frigates that are currently being designed by the Turkish Naval Institute.




On April 7, 2014, the United States House of Representatives voted to pass the Taiwan Relations Act Affirmation and Naval Vessel Transfer Act of 2014 (H.R. 3470; 113th Congress), a bill that would allow eight more "Perry" frigates to be transferred to foreign countries. The bill would authorize the President to transfer and to Mexico, and and to Thailand. The bill would also authorize the President to sell four units (, , , and ) to the Taipei Economic and Cultural Representative Office of the United States (which is the Taiwan agency designated pursuant to the Taiwan Relations Act) for about $10 million each.

On June 13, 2017, the Chief of Naval Operations, Admiral John M. Richardson announced that U.S. Navy officials are currently looking into the possibility of recommissioning several "Oliver Hazard Perry" class frigates from its inactive fleet to help build up and support President Donald Trump's proposed 355 ship navy plan. On December 11, 2017, the Navy decided against reactivating the class citing that reactivating the ships would prove too costly.





</doc>
<doc id="22705" url="https://en.wikipedia.org/wiki?curid=22705" title="Ottawa Senators">
Ottawa Senators

The Ottawa Senators () are a professional ice hockey team based in Ottawa, Ontario. They are members of the Atlantic Division of the Eastern Conference of the National Hockey League (NHL). The Senators play their home games at the 17,373-seat Canadian Tire Centre, which opened in 1996 as the Palladium.

Founded and established by Ottawa real estate developer Bruce Firestone, the team is the second NHL franchise to use the Ottawa Senators name. The original Ottawa Senators, founded in 1883, had a famed history, winning 11 Stanley Cups, playing in the NHL from 1917 until 1934. On December 6, 1990, after a two-year public campaign by Firestone, the NHL awarded a new franchise, which began play in the 1992–93 season. The current team owner is Eugene Melnyk, and in 2017, the franchise was valued by "Forbes" magazine at $420 million.

The Senators have won four division titles and, in 2003, the Presidents' Trophy; and have once appeared in the Stanley Cup Finals (2007).

Ottawa had been home to the original Senators, a founding NHL franchise and 11-time Stanley Cup champions. After the NHL expanded to the United States in the late 1920s, the original Senators' eventual financial losses forced the franchise to move to St. Louis in 1934 operating as the Eagles while a Senators senior amateur team took over the Senators' place in Ottawa. The NHL team was unsuccessful in St. Louis, and planned to return to Ottawa, but the NHL decided instead to suspend the franchise and transfer the players to other NHL teams.

Fifty-four years later, after the NHL announced plans to expand, Ottawa real estate developer Bruce Firestone decided along with colleagues Cyril Leeder and Randy Sexton that Ottawa was now able to support an NHL franchise, and the group proceeded to put a bid together. His firm, Terrace Investments, did not have the liquid assets to finance the expansion fee and the team, but the group conceived a strategy to leverage a land development. In 1989, after finding a suitable site on farmland just west of Ottawa in Kanata on which to construct a new arena, Terrace announced its intention to win a franchise and launched a successful "Bring Back the Senators" campaign to both woo the public and persuade the NHL that the city could support an NHL franchise. Public support was high and the group would secure over 11,000 season ticket pledges. On December 12, 1990, the NHL approved a new franchise for Firestone's group, to start play in the 1992–93 season.

The new team hired former NHL player Mel Bridgman, who had no previous NHL management experience, as its first general manager in 1992. The team was initially interested in hiring former Jack Adams Award winner Brian Sutter as its first head coach, but Sutter came with a high price tag and was reluctant to be a part of an expansion team. When Sutter was eventually signed to coach the Boston Bruins, Ottawa signed Rick Bowness, the man Sutter replaced in Boston. The new Senators played their first game on October 8, 1992, in the Ottawa Civic Centre against the Montreal Canadiens with lots of pre-game spectacle. The Senators defeated the Canadiens 5–3 in one of the few highlights that season. Following the initial excitement of the opening night victory, the club floundered badly and eventually tied the San Jose Sharks for the worst record in the league, winning only 10 games with 70 losses and four ties for 24 points, three points better than the NHL record for futility. The Senators had aimed low and considered the 1992–93 season a small success, as Firestone had set a goal for the season of not setting a new NHL record for fewest points in a season. The long term plan was to finish low in the standings for its first few years in order to secure high draft picks and eventually contend for the Stanley Cup.

Bridgman was fired after one season and Team President Randy Sexton took over the general manager duties. Firestone himself soon left the team and Rod Bryden emerged as the new owner. The strategy of aiming low and securing a high draft position did not change. The Senators finished last overall for the next three seasons. Although 1993 first overall draft choice Alexandre Daigle wound up being one of the greatest draft busts in NHL history, they chose Radek Bonk in 1994, Bryan Berard (traded for Wade Redden) in 1995, Chris Phillips in 1996 and Marian Hossa in 1997, all of whom would become solid NHL players and formed a strong core of players in years to come. Alexei Yashin, the team's first-ever draft selection from 1992, emerged as one of the NHL's brightest young stars. The team traded many of their better veteran players of the era, including 1992–93 leading scorer Norm Maciver and fan favourites Mike Peluso and Bob Kudelski in an effort to stockpile prospects and draft picks.

As the 1995–96 season began, star centre Alexei Yashin refused to honour his contract and did not play. In December, after three straight last-place finishes and a team which was ridiculed throughout the league, fans began to grow restless waiting for the team's long term plan to yield results, and arena attendance began to decline. Rick Bowness was fired in late 1995 and was replaced by the Prince Edward Island Senators' head coach Dave Allison. Allison would fare no better than his predecessor, and the team would stumble to a 2–22–3 record under him. Sexton himself was fired and replaced by Pierre Gauthier, the former assistant GM of Anaheim. Before the end of January 1996, Gauthier had resolved the team's most pressing issues by settling star player Alexei Yashin's contract dispute, and hiring the highly regarded Jacques Martin as head coach. While Ottawa finished last overall once again, the 1995–96 season ended with renewed optimism, due in part to the upgraded management and coaching, and also to the emergence of an unheralded rookie from Sweden named Daniel Alfredsson, who would win the Calder Memorial Trophy as NHL Rookie of the Year in 1996.

Martin would impose a "strong defence first" philosophy that led to the team qualifying for the playoffs every season that he coached, but he was criticized for the team's lack of success in the playoffs, notably losing four straight series against the provincial rival Toronto Maple Leafs. Martin outlasted several general managers and a change in ownership.

In 1996–97, his first season, the club qualified for the playoffs in the last game of the season, and nearly defeated the Buffalo Sabres in the first round. In 1997–98, the club finished with their first winning record and upset the heavily favoured New Jersey Devils to win their first playoff series. In 1998–99, the Senators jumped from fourteenth overall in the previous season to third, with 103 points—the first 100-point season in club history, only to be swept in the first round. In 1999–2000 despite the holdout of team captain Alexei Yashin, Martin guided the team to the playoffs, only to lose to the Maple Leafs in the first Battle of Ontario series. Yashin returned for 2000–01 and the team improved to win their division and place second in the Eastern Conference. Yashin played poorly in another first round playoff loss and on the day of the 2001 NHL Entry Draft, he was traded to the New York Islanders in exchange for Zdeno Chara, Bill Muckalt and the second overall selection in the draft, which Ottawa used to select centre Jason Spezza.

The 2001–02 Senators regular season points total dropped, but in the playoffs, they upset the Philadelphia Flyers for the franchise's second playoff series win. Yet the Sens would lose in game seven of the second round of the playoffs. Despite speculation that Martin would be fired, it was GM Marshall Johnston who left, retiring from the team, replaced by John Muckler, the Senators' first with previous GM experience.

In 2002–03 off-ice problems dominated the headlines, as the Senators filed for bankruptcy in mid-season, but continued play after getting emergency financing. Despite the off-ice problems, Ottawa had an outstanding season, placing first overall in the NHL to win the Presidents' Trophy. In the playoffs, they came within one game of making it into the finals. Prior to the 2003–04 season, pharmaceutical billionaire Eugene Melnyk would purchase the club to bring financial stability. Martin would guide the team to another good regular season but again would lose in the first round of the playoffs, leading to Martin's dismissal as management felt that a new coach was required for playoff success.

After the playoff loss, owner Melnyk promised that changes were coming and they came quickly. In June 2004, Anaheim Ducks GM Bryan Murray of nearby Shawville, became head coach. That summer, the team also made substantial personnel changes, trading long-time players Patrick Lalime and Radek Bonk, and signing free agent goaltender Dominik Hasek. The team would not be able to show its new line-up for a year, as the 2004–05 NHL lock-out intervened and most players played in Europe or in the minors. In a final change, just before the 2005–06 season, the team traded long-time player Marian Hossa for Dany Heatley.

The media predicted the Senators to be Stanley Cup contenders in 2005–06, as they had a strong core of players returning, played in an up-tempo style fitting the new rule changes and Hasek was expected to provide top-notch goaltending. The team rushed out of the gate, winning 19 of the first 22 games, in the end winning 52 games and 113 points, placing first in the conference, and second overall. The newly formed 'CASH' line of Alfredsson, Spezza and newly acquired Dany Heatley established itself as one of the league's top offensive lines. Hasek played well until he was injured during the 2006 Winter Olympics, forcing the team to enter the playoffs with rookie netminder Ray Emery as their starter. Without Hasek, the club bowed out in a second round loss to the Buffalo Sabres.

In 2006–07, the Senators reached the Stanley Cup Finals after qualifying for the playoffs in nine consecutive seasons. The Senators had a high turn-over of personnel and the disappointment of 2006 to overcome and started the season poorly. Trade rumours swirled around Daniel Alfredsson for most of the last months of 2006. The team lifted itself out of last place in the division to nearly catch the Buffalo Sabres by season's end, placing fourth in the Eastern Conference. The team finished with 105 points, their fourth straight 100-point season and sixth in the last eight. In the playoffs, Ottawa continued its good play. Led by the 'CASH' line, goaltender Ray Emery, and the strong defence of Chris Phillips and Anton Volchenkov, the club defeated the Pittsburgh Penguins, the second-ranked New Jersey Devils and the top-ranked Buffalo Sabres to advance to the Stanley Cup Finals.

The 2006–07 Senators thus became the first Ottawa team to be in the Stanley Cup final since 1927 and the city was swept up in the excitement. Businesses along all of the main streets posted large hand-drawn "Go Sens Go" signs, residents put up large displays in front of their homes or decorated their cars. A large Ottawa Senators flag was draped on the City Hall, along with a large video screen showing the games. A six-storey likeness of Daniel Alfredsson was hung on the Corel building. Rallies were held outside of City Hall, car rallies of decorated cars paraded through town and a section of downtown, dubbed the "Sens Mile", was closed off to traffic during and after games for fans to congregate.

In the Final, the Senators now faced the Anaheim Ducks, considered a favourite since the start of the season, a team the Senators had last played in 2006, and a team known for its strong defence. The Ducks won the first two games in Anaheim 3–2 and 1–0. Returning home, the Senators won game three 5–3, but lost game four 3–2. The Ducks won game five 6–2 in Anaheim to clinch the series. The Ducks had played outstanding defence, shutting down the 'CASH' line, forcing Murray to split up the line. The Ducks scored timely goals and Ducks' goaltender Jean-Sebastien Giguere out-played Emery.

In the off-season after the Stanley Cup Final, Bryan Murray's contract was expiring, while GM John Muckler had one season remaining, at which he was expected to retire. Murray, who had previously been at GM for other NHL clubs, was expected to take over the GM position, although no public timetable was given. Owner Melnyk decided to offer Muckler another position in the organization and give the GM position to Murray. Muckler declined the offer and was relieved from his position. Melnyk publicly justified the move, saying that he expected to lose Murray if his contract ran out. Murray then elevated John Paddock, the assistant coach, to head coach of the Senators. Under Paddock, the team came out to a record start to the 2007–08 season. However, team play declined to a .500 level and the team looked to be falling out of the playoffs. Paddock was fired by Murray, who took over coaching on an interim basis. The club managed to qualify for the playoffs by a tie-breaker, but was swept in the first round of the playoffs to the Pittsburgh Penguins. In June, the club bought-out goaltender Ray Emery, who had become notorious for off-ice events in Ottawa and lateness to several team practices.

For 2008–09, Murray hired Craig Hartsburg to coach the Senators. Under Hartsburg's style, the Senators struggled and played under .500. Uneven goaltending with Martin Gerber and Alex Auld meant the team played cautiously to protect the goaltender. Murray's patience ran out in February 2009 with the team well out of playoff contention and Hartsburg was fired, although he had two years left on his contract, and the team also had Paddock under contract. Cory Clouston was elevated from the Binghamton coaching position. The team played above .500 under Clouston and rookie goaltender Brian Elliott, who had been promoted from Binghamton. Gerber was waived from the team at the trading deadline and the team traded for goaltender Pascal Leclaire, although he would not play due to injury. The team failed to make the playoffs for the first time in 12 seasons. Auld would be traded in the off-season to make room. Clouston's coaching had caused a rift with top player Dany Heatley (although unspecified "personal issues" were also noted by Heatley) and after Clouston was given a contract to continue coaching, Heatley made a trade demand and was traded just before the start of the 2009–10 season.

In 2009–10, the Senators were a .500 team, until going on a team-record 11-game winning streak in January. The streak propelled the team to the top of the Northeast Division standings and a top-three placing for the playoffs. The team was unable to hold off the Sabres for the division lead, but qualified for the playoffs in the fifth position. For the third season in four, the Senators played off against the Pittsburgh Penguins in the first round. A highlight for the Senators was winning a triple-overtime fifth game in Pittsburgh, but the team was unable to win a playoff game on home ice, losing the series in six games.

The Senators had a much poorer than expected 2010–2011 campaign, resulting in constant rumours of a shakeup right through until December. The rumours were heightened in January after the team went on a lengthy losing streak. January was a dismal month for the Senators, winning only one game all month. Media speculated on the imminent firing of Clouston, Murray or both. Owner Melynk cleared the air in an article in the edition of January 22, 2011 of the "Ottawa Sun." Melnyk stated that he would not fire either Clouston or Murray, but that he had given up on this season and was in the process of developing a plan for the future. On Monday, January 24, the "Globe and Mail" reported that the plan included hiring a new general manager before the June entry draft and that Murray would be retained as an advisor to the team. A decision on whether to retain Clouston would be made by the new general manager. The article by Roy MacGregor, a long-time reporter of the Ottawa Senators, stated that former assistant coach Pierre McGuire had already been interviewed. Murray, in a press conference that day, stated that he wished to stay on as the team's general manager. He also stated that Melnyk was allowing him to continue as general manager without restraint. Murray said that the players were now to be judged by their play until the February 28 trade deadline. Murray would attempt to move "a couple, at least" of the players for draft picks or prospects at that time if the Senators remained out of playoff contention. At the time of Murray's comments the team was eight games under .500 and 14 points out of a playoff position after 49 games.

Murray started with the trading of Mike Fisher to the Nashville Predators in exchange for a first round pick in the 2011 draft. Fisher already had a home in Nashville with new wife Carrie Underwood. The trading of Fisher, a fan favourite in Ottawa, lead to a small anti-Underwood backlash in the city with the banning of her songs from the play lists of some local radio stations. Murray next traded Chris Kelly, another veteran, to the Boston Bruins for a second round pick in the 2011 draft. A few days later, pending unrestricted free agent Jarkko Ruutu was sent to the Anaheim Ducks in exchange for a sixth round pick in 2011. A swap of goaltenders was made with the Colorado Avalanche which brought Craig Anderson to Ottawa in exchange for Brian Elliott. Both goalies were having sub-par seasons prior to the trade. Under-achieving forward Alex Kovalev was traded to the Pittsburgh Penguins for a seventh round draft pick. On trade deadline day, Ottawa picked up goaltender Curtis McElhinney on waivers and traded Chris Campoli with a seventh round pick to the Chicago Blackhawks for a second round pick and Ryan Potulny. Goaltender Anderson played very well down the stretch for Ottawa, and the team quickly signed the soon-to-be unrestricted free agent to a four-year contract. After media speculation on the future of Murray within the organization, Murray was re-signed as general manager on April 8 to a three-year extension. On April 9, Head Coach Cory Clouston and assistants Greg Carvel and Brad Lauer were dismissed from their positions. Murray said that the decision was made based on the fact that the team entered the season believing it was a contender, but finished with a 32–40–10 record. Former Detroit Red Wings' assistant coach Paul MacLean was hired as Clouston's replacement on June 14, 2011.

As the 2011–12 season began, many hockey writers and commentators were convinced that the Senators would finish at or near the bottom of the NHL standings. In the midst of rebuilding, the Ottawa line-up contained many rookies and inexperienced players. The team struggled out of the gate, losing five of their first six games before a reversal of fortunes saw them win six games in a row. In December 2011, the team acquired forward Kyle Turris from the Phoenix Coyotes in exchange for David Rundblad and a draft pick. The team improved its play afterwards and moved into a playoff position before the All-Star Game. For the first time in Senators' history, the All-Star Game was held in Ottawa, and it was considered a great success. Five Senators were voted in or named to the event, including Daniel Alfredsson, who was named captain of one team. The team continued its playoff push after the break. After starting goalie Craig Anderson injured his hand in a kitchen accident at home, the Senators called up Robin Lehner from Binghamton and acquired highly regarded goaltender Ben Bishop from the St. Louis Blues. While Anderson recovered, the team continued its solid play. On April 1, 2012, the Senators defeated the New York Islanders 5–1, officially ensuring a playoff position. The team finished as the eighth seed in the Eastern Conference, drawing a first round playoff matchup against the Conference champion New York Rangers. Ultimately, Ottawa lost the series in seven games.

The next season, Ottawa would be challenged to repeat the success they had in 2011–12, due to long-term injuries to key players such as Erik Karlsson, Jason Spezza, Milan Michalek and Craig Anderson. Despite these injuries, the Senators would finish seventh in the Eastern Conference and head coach Paul MacLean would go on to win the Jack Adams Award as the NHL's coach of the year. Ottawa would play the second-seeded Montreal Canadiens in the first round of the playoffs, eventually winning in five games, blowing out Montreal 6–1 in games three and five. The Senators would advance to play the top-seeded Pittsburgh Penguins in the second round, this time losing in five games. During the off-season, the Senators traded veteran defenceman Sergei Gonchar to the Dallas Stars for a sixth round pick in the 2013 draft. July 5, 2013, would be a day of mixed emotions for the city and fans, as long-time captain Daniel Alfredsson signed a one-year contract with the Detroit Red Wings, leaving Ottawa after 17 seasons with the Senators and 14 as captain. The signing shocked numerous fans across the city and many within the Senators organization. The day finished optimistically however, as Murray acquired star forward Bobby Ryan from the Anaheim Ducks in exchange for forwards Jakob Silfverberg, Stefan Noesen and a first round pick in the 2014 draft. The hope was that Ryan would be the guy to play on the top line with Jason Spezza after Alfredsson's departure. Murray would also sign free agent forward Clarke MacArthur to a two-year contract that same day and bring back former defenceman Joe Corvo to a one-year contract three days later on July 8, 2013.

For the 2013–14 NHL season, the league realigned and Ottawa was assigned to the new Atlantic Division along with the rest of the old Northeast Division, with the additions of the Columbus Blue Jackets and Detroit Red Wings, formerly of the Western Conference. The re-alignment brought increased competition to qualify for the playoffs, as there were now 16 teams in the Eastern Conference fighting for eight playoff spots. The season began with a changing of leadership, as on September 14, 2013, the Ottawa Senators named Jason Spezza their eighth captain in franchise history. While new addition Clarke MacArthur had a career year, Ryan and Spezza struggled to find chemistry, and Ryan was moved to a line with MacArthur and Kyle Turris, where he fared much better. Bobby Ryan also ran into injury problems during the season, and while there were times where Joe Corvo played solidly, he eventually lost his place in the line-up. The club struggled on defence, as shots and goals against numbers increased from the previous season. The club was a sub .500 team much of the season, or only a few games above and never was in a playoff position all season. At the trade deadline, Murray traded for flashy right winger Ales Hemsky from the Edmonton Oilers, quickly finding success on a line with Spezza and Michalek. The club, however, was eliminated from playoff contention in the last week of the season. At the end of the season, the club failed to come to terms on a new contract with Hemsky and captain Jason Spezza requested a trade out of Ottawa. At the 2014 NHL Entry Draft, a potential trade to the Nashville Predators was negotiated by Murray but rejected by Spezza, as the Predators were one of the teams on his limited no-trade list. A deal with the Dallas Stars was eventually reached, and Spezza was sent, along with Ludwig Karlsson, in exchange for Alex Chiasson, Nick Paul, Alex Guptill and a 2015 second-round pick. During the off-season, the club signed free agent forward David Legwand to a two-year, $6 million contract.

At the beginning of the 2014–15 season, defenceman Erik Karlsson was named the franchise's ninth captain, with the club also re-signing Bobby Ryan to a seven-year extension. After firing head coach Paul MacLean after 27 games with an 11–11–5 record and replacing him with Dave Cameron, the Senators would win 32 of their last 55 games. Goaltender Andrew Hammond would compile a record of 20–1–2, a goals against average of 1.79, and a save percentage of .941 to get the team back into playoff position. The Senators later became the first team in modern NHL history to overcome a 14-point deficit at any juncture of the season to qualify for the playoffs. However, the Senators lost to the Canadiens in six games in the first round of the playoffs.

During the 2014–15 season, it was announced that Murray had cancer. Taking regular treatment, Murray chose to stay on as GM through the 2015–16 season. Despite posting the best record of any Canadian team in the league, the Senators failed to make the playoffs in what was considered a disappointing season (all seven Canadian teams missed the playoffs). Murray made one 'blockbuster' 11-player trade that brought Toronto Maple Leafs' captain Dion Phaneuf to the Senators before the trade deadline. The Senators outside of a playoff position at the time of the deal, but could not put together another run and finished with 85 points for fifth in the division.

On April 10, 2016, the day after the final game of the 2015–16 season, Murray announced his resignation as manager and that he would continue in an advisory role with the club. Assistant general manager Pierre Dorion was promoted to the general manager position. On April 12, 2016, the Senators fired head coach Dave Cameron. On May 8, 2016, the Senators hired former Tampa Bay Lightning head coach Guy Boucher as their new head coach. On the following day, Marc Crawford was announced as associate coach. On June 13, 2016, the Senators hired Daniel Alfredsson as the senior advisor of hockey operations. In June 2016, the Senators hired Rob Cookson as an assistant coach, who had worked with both Boucher and Crawford in Switzerland, and Pierre Groulx as a goaltending coach.

The Senators finished second in the Atlantic Division during the 2016–17 season and faced the Boston Bruins in the first round of the playoffs, winning that series in six games. In the second round, they defeated the New York Rangers in six games. During the second game of that series, Jean-Gabriel Pageau scored four goals, including the game-winning goal in double overtime. The Senators would come within one game of the Stanley Cup Final, but lost in double overtime of the seventh game of their Eastern Conference Final series against the Pittsburgh Penguins, who went on to win their second consecutive Stanley Cup.

The new Senators' first home arena was the Ottawa Civic Centre, located on Bank Street, where they played from the 1992–93 season to January of the 1995–96 season; their last game was New Years Eve, 1995, versus the Tampa Bay Lightning. They played their first home game on October 8, 1992 against the Montreal Canadiens with lots of pre-game spectacle. The Senators would defeat the Canadiens 5–3 in one of few highlights that season. Following the initial excitement of the opening night victory, the club floundered badly and would eventually tie with the San Jose Sharks for the worst record in the league, finishing with only 10 wins, 70 losses and 4 ties for 24 points, three points better than the NHL record for futility.

As part of its bid to land an NHL franchise for Ottawa, Terrace Corporation unveiled the original proposal for the arena development at a press conference in September 1989. The proposal included a hotel and 20,500 seat arena, named The Palladium on , surrounded by a mini-city, named "West Terrace." The site itself, of farmland, on the western border of Kanata, had been acquired in May 1989 by Terrace. Rezoning approval was granted by the Board on August 28, 1991, with conditions. The conditions imposed by the board included a scaling down of the arena to 18,500 seats, a moratorium on development outside the initial arena site, and that the cost of the highway interchange with highway 417 be paid by Terrace. A two-year period was used seeking financing for the site and interchange by Terrace Corporation. The corporation received a $6 million grant from the federal government, but needed to borrow to pay for the rest of the costs of construction. A ground-breaking ceremony was held in June 1992 but actual construction did not start until July 7, 1994. Actual construction took 18 months, finishing in January 1996.

The newly built Palladium opened on January 15, 1996 with a concert by Canadian rocker Bryan Adams. The Senators played their first game in their new arena two days later, falling 3–0 to the Montreal Canadiens. On February 17, 1996, the name 'Palladium' was changed to the 'Corel Centre' when Corel Corporation, an Ottawa software company, signed a 10-year deal for the naming rights.

When mortgage holder Covanta Energy (the former Ogden Entertainment) went into receivership in 2001, Terrace was expected to pay off the entire debt. The ownership was not able to refinance the arena, eventually leading Terrace itself to declare bankruptcy in 2003. However, on August 26, 2003, billionaire businessman Eugene Melnyk finalized the purchase of the Senators and the arena. The arena and club became solely owned by Melnyk through a new company, Capital Sports Properties.

In 2004, the ownership applied to expand its seating. In December 2004, the City of Ottawa amended its by-laws and in 2005, the venue was allowed to increase its seating capacity to 19,153 and total attendance capacity to 20,500 when including standing room.

On January 19, 2006, the arena became known as 'Scotiabank Place' after reaching a new 15-year naming agreement with Canadian bank Scotiabank on January 11, 2006. Scotiabank had been an advertising partner with the club for several years and took over the naming after Corel declined to renew its naming agreement with the Senators, but continued as an advertising sponsor. On June 18, 2013, the Ottawa Senators announced a new marketing agreement with Canadian Tire, and as a result, the arena was renamed the Canadian Tire Centre on July 1, 2013.

In 2015 the National Capital Commission (NCC) put out a request for proposals to redevelop the LeBreton Flats area in Ottawa. In 2016 the NCC settled on the proposal presented by Senators owner Eugene Melnyk and his RendezVous LeBreton Group. The proposal includes housing units, park space, a recreation facility, a library and a new arena for the Ottawa Senators. Negotiations are proceeding, with a possible 2021 opening for the new arena.

The team colours are red, black and white, with added trim of gold. The team's away jersey is mostly white with red and black trim, while the home jersey is red, with white and black trim. The club logo is officially the head of a Roman general, a member of the Senate of the Roman Republic, projecting from a gold circle. The original, unveiled on May 23, 1991, described the general as a "centurion figure, strong and prominent" according to its designer, Tony Milchard.
The current jersey design was unveiled on August 22, 2007, in conjunction with the league-wide adoption of the "Rbk EDGE" jerseys by Reebok for the 2007–08 season. The jersey incorporates the original Senators' 'O' logo as a shoulder patch. At the same time, the team updated its logos, and switched their usage. The primary logo, which according to team owner Eugene Melnyk, "represents strength and determination" is an update of the old secondary logo. The old primary logo has become the team's secondary logo and only appears on Senators' merchandise.

In 2011, the Senators introduced their current third jersey design. Mostly black, the jersey incorporated horizontal striping intended to be reminiscent of the original Senators' 'barber-pole' designs. Shield-type patches were added to the shoulders. The design of the shield-type patches was intended to be similar to the shield patches that the original Senators added to their jerseys after each Stanley Cup championship win. The patches spell the team name, one in English, and one in French. The design was a collaborative effort between the Senators and a fan in Gatineau, Quebec who had been circulating a version of it on the internet since 2009.

On April 18, 2008, the club announced its final attendance figures for 2007–08. The club had 40 sell-outs out of 41 home dates, a total attendance of 812,665 during the regular season, placing the club third in attendance in the NHL. The number of sell-outs and the total attendance were both club records. The previous attendance records were set during the 2005–06 with a season total of 798,453 and 33 sell-outs. In 2006–07 regular season attendance was 794,271, with 31 sell-outs out of 41 home dates or an average attendance of 19,372. In the 2007 playoffs, the Senators played 9 games with 9 sell-outs and an attendance of 181,272 for an average of 20,141, the highest in team history. The club has been regularly represented in the top half in attendance in the NHL.

On November 29, 2011, a "Forbes" magazine report valued the Ottawa Senators Hockey Club at $201 million, (17th highest in NHL). The valuation was based on $27 million for the sport, $70 million for the arena, $80 million for the market and $25 million for the brand. For 2010–11, the club had an operating income of $2.8 million on revenues of $100 million. The gate receipts for the 2010–11 season were $46 million and player expenses were $57 million. The operating income followed two years where the team posted a loss. Forbes estimates that the organization has a debt/value ratio of 65%, including arena debt. Eugene Melnyk bought the team for $92 million in 2003. A November 2014 report by Forbes valued the Senators at $400 million, 16th highest in the NHL.

At many home games the fans are entertained both outside and inside Canadian Tire Centre with a myriad of talent – live music, rock bands, giveaways and promotions. The live music includes the traditional Scottish music of the 'Sons of Scotland Pipe Band' of Ottawa along with highland dancers. Before and during games, entertainment is provided by Spartacat, the official mascot of the Senators, an anthropomorphic lion. He made his debut on the Senators' opening night: October 8, 1992. The national anthems are usually sung by former Ontario Provincial Police Constable Lyndon Slewidge. At home games, "O Canada" is traditionally sung in both English and French with the first half of first stanza and chorus sung in English and the second half of first stanza sung in French. The Senators have their own theme song "Ottawa Senators Theme Song" which is played as the team comes on the ice and is also used in Sens TV web videos. It was composed locally in Ottawa.
The fans of the Senators are known as the "Sens Army". Like most hockey fanatics, they are known to dress up for games; some in Roman legionary clothing. For the 2006–2007 playoff run, more fans than ever before would wear red, and fan activities included 'Red Rallies' of decorated cars, fan rallies at Ottawa City Hall Plaza and the 'Sens Mile' along Elgin Street where fans would congregate.

Much like the Red Mile in Calgary during the Flames' 2004 cup run and the Copper Kilometer in Edmonton during the Oilers' 2006 cup run, Ottawa Senators fans took to the streets to celebrate their team's success during the 2006–07 playoffs. The idea to have a 'Sens Mile' on the downtown Elgin Street, a street with numerous restaurants and pubs, began as a grassroots campaign on Facebook by Ottawa residents before Game 4 of the Ottawa-Buffalo Eastern Conference Final series. After the Game 5 win, Ottawa residents closed the street to traffic for a spontaneous celebration. The City of Ottawa then closed Elgin Street for each game of the Final.

Ottawa Senators games are broadcast locally in both the English and French languages. As of the 2014–15 season, regional television rights to the Senators' regular season games not broadcast nationally by Sportsnet, TVA Sports, or "Hockey Night in Canada" are owned by Bell Media under a 12-year contract, with games airing in English on TSN5, and in French on RDS. Regional broadcasts are available within the team's designated region (shared with the Montreal Canadiens), which includes the Ottawa River valley, Eastern Ontario (portions are shared with the Toronto Maple Leafs, along with Quebec, the Maritime provinces and Newfoundland and Labrador.

On radio, all home and away games are broadcast on a five-station network stretching across Eastern Ontario, and including one American station, WQTK in Ogdensburg, New York. The flagship radio station is CFGO "TSN Radio 1200" in Ottawa. Radio broadcasts on CFGO began in 1997–98; the contract has since been extended through the 2025–2026 as part of Bell Media's rights deal with the team. The Senators are broadcast on radio in French through Intersport Production and CJFO Unique FM in Ottawa. Nicolas St. Pierre provides play-by-play, with Alain Sanscartier as colour commentator.

Sportsnet East held English regional rights to the Sens prior to the 2014–15 season. In April 2014, Dean Brown, who had called play-by-play for Senators games the team's inception, stated that it was "extremely unlikely" that he would move to TSN and continue his role. He noted that the network already had four commentators among its personalities – including Gord Miller, Chris Cuthbert, Rod Black, and Paul Romanuk (who was, however, picked up by Rogers for its national NHL coverage in June 2014), who were likely candidates to serve as the new voices of the Senators. Brown ultimately moved to the Senators' radio broadcasts alongside Gord Wilson.

During the 2006–07 and 2007–08 seasons, several games were only available in video on pay-per-view or at local movie theatres in the Ottawa area. The "Sens TV" service was suspended indefinitely as of September 24, 2008. In 2010, Sportsnet launched a secondary channel for selected Senators games as part of its Sportsnet One service. Selected broadcasts of Senators games in the French language were broadcast by RDS and TVA Sports. On the RDS network, Félix Séguin and former Senators goaltender Patrick Lalime were the announcers from the 2011–12 season to the 2013–14 season, and Michel Y. Lacroix and Norman Flynn starting in the 2014–15 season. The TVA Sports broadcast team consisted of Michel Langevin, Yvon Pedneault and Enrico Ciccone.


Source: "Ottawa Senators 2009–10 Media Guide", p. 206.




"This is a partial list of the last five seasons completed by the Senators. For the full season-by-season history, see List of Ottawa Senators seasons"

"Note: GP = Games Played, W = Wins, L = Losses, T = Ties, OTL = Overtime Losses, Pts = Points, GF = Goals for, GA = Goals against, PIM = Penalties in minutes"
These are the top-ten regular season point-scorers in franchise history after the 2017–18 season:
"Note: Pos = Position; GP = Games Played; G = Goals; A = Assists; Pts = Points; P/G = Points per game average;

Source: Ottawa Senators Media Guide
Presidents' Trophy

Prince of Wales Trophy

Calder Memorial Trophy

NHL Plus-Minus Award

Jack Adams Award
James Norris Memorial Trophy

King Clancy Memorial Trophy

Mark Messier Leadership Award

Bill Masterton Memorial Trophy

NHL All-Rookie Team

NHL First All-Star Team

NHL Second All-Star Team

Source: Ottawa Senators.





</doc>
<doc id="22706" url="https://en.wikipedia.org/wiki?curid=22706" title="Orchestra">
Orchestra

An orchestra (; ) is a large instrumental ensemble typical of classical music, which mixes instruments from different families, including bowed string instruments such as violin, viola, cello and double bass, as well as brass, woodwinds, and percussion instruments, each grouped in sections. Other instruments such as the piano and celesta may sometimes appear in a fifth keyboard section or may stand alone, as may the concert harp and, for performances of some modern compositions, electronic instruments.

A full-size orchestra may sometimes be called a "symphony orchestra" or "philharmonic orchestra". The actual number of musicians employed in a given performance may vary from seventy to over one hundred musicians, depending on the work being played and the size of the venue. The term "chamber orchestra" (and sometimes "concert orchestra") usually refers to smaller-sized ensembles of about fifty musicians or fewer. Orchestras that specialize in the Baroque music of, for example, Johann Sebastian Bach and George Frideric Handel, or Classical repertoire, such as that of Haydn and Mozart, tend to be smaller than orchestras performing a Romantic music repertoire, such as the symphonies of Johannes Brahms. The typical orchestra grew in size throughout the 18th and 19th centuries, reaching a peak with the large orchestras (of as many as 120 players) called for in the works of Richard Wagner, and later, Gustav Mahler. 

Orchestras are usually led by a conductor who directs the performance with movements of the hands and arms, often made easier for the musicians to see by use of a conductor's baton. The conductor unifies the orchestra, sets the tempo and shapes the sound of the ensemble. The conductor also prepares the orchestra by leading rehearsals before the public concert, in which the conductor provides instructions to the musicians on their interpretation of the music being performed.

The leader of the first violin section, commonly called the concertmaster, also plays an important role in leading the musicians. In the Baroque music era (1600–1750), orchestras were often led by the concertmaster or by a chord-playing musician performing the basso continuo parts on a harpsichord or pipe organ, a tradition that some 20th century and 21st century early music ensembles continue. Orchestras play a wide range of repertoire, including symphonies, opera and ballet overtures, concertos for solo instruments, and as pit ensembles for operas, ballets and some types of musical theater (e.g., Gilbert and Sullivan operettas).

Amateur orchestras include those made up of students from an elementary school or a high school, youth orchestras, and community orchestras; the latter two typically being made up of amateur musicians from a particular city or region.

The term "orchestra" derives from the Greek ὀρχήστρα ("orchestra"), the name for the area in front of a stage in ancient Greek theatre reserved for the Greek chorus.

The typical symphony orchestra consists of four groups of related musical instruments called the woodwinds, brass, percussion, and strings (violin, viola, cello and double bass). Other instruments such as the piano and celesta may sometimes be grouped into a fifth section such as a keyboard section or may stand alone, as may the concert harp and electric and electronic instruments. The orchestra, depending on the size, contains almost all of the standard instruments in each group.

In the history of the orchestra, its instrumentation has been expanded over time, often agreed to have been standardized by the classical period and Ludwig van Beethoven's influence on the classical model. In the 20th century, new repertory demands expanded the instrumentation of the orchestra, resulting in a flexible use of the classical-model instruments and newly developed electric and electronic instruments in various combinations.

The terms "symphony orchestra" and "philharmonic orchestra" may be used to distinguish different ensembles from the same locality, such as the London Symphony Orchestra and the London Philharmonic Orchestra. A symphony orchestra will usually have over eighty musicians on its roster, in some cases over a hundred, but the actual number of musicians employed in a particular performance may vary according to the work being played and the size of the venue.

"Chamber orchestra" usually refers to smaller-sized ensembles; a major chamber orchestra might employ as many as fifty musicians; some are much smaller than that. The term "concert orchestra" may also be used, as in the BBC Concert Orchestra and the RTÉ Concert Orchestra.

The so-called "standard complement" of doubled winds and brass in the orchestra from the first half of the 19th century is generally attributed to the forces called for by Beethoven. The composer's instrumentation almost always included paired flutes, oboes, clarinets, bassoons, horns and trumpets. The exceptions to this are his Symphony No. 4, Violin Concerto, and Piano Concerto No. 4, which each specify a single flute. Beethoven carefully calculated the expansion of this particular timbral "palette" in Symphonies 3, 5, 6, and 9 for an innovative effect. The third horn in the "Eroica" Symphony arrives to provide not only some harmonic flexibility, but also the effect of "choral" brass in the Trio movement. Piccolo, contrabassoon, and trombones add to the triumphal finale of his Symphony No. 5. A piccolo and a pair of trombones help deliver the effect of storm and sunshine in the Sixth, also known as the "Pastoral Symphony". The Ninth asks for a second pair of horns, for reasons similar to the "Eroica" (four horns has since become standard); Beethoven's use of piccolo, contrabassoon, trombones, and untuned percussion—plus chorus and vocal soloists—in his finale, are his earliest suggestion that the timbral boundaries of symphony might be expanded. For several decades after his death, symphonic instrumentation was faithful to Beethoven's well-established model, with few exceptions.

Apart from the core orchestral complement, various other instruments are called for occasionally. These include the flugelhorn and cornet. Saxophones and classical guitars, for example, appear in some 19th- through 21st-century scores. While appearing only as featured solo instruments in some works, for example Maurice Ravel's orchestration of Modest Mussorgsky's "Pictures at an Exhibition" and Sergei Rachmaninoff's "Symphonic Dances", the saxophone is included in other works, such as Ravel's "Boléro", Sergei Prokofiev's Romeo and Juliet Suites 1 and 2, Vaughan Williams' Symphonies No.6 and 9 and William Walton's "Belshazzar's Feast", and many other works as a member of the orchestral ensemble. The euphonium is featured in a few late Romantic and 20th-century works, usually playing parts marked "tenor tuba", including Gustav Holst's "The Planets", and Richard Strauss's "Ein Heldenleben". The Wagner tuba, a modified member of the horn family, appears in Richard Wagner's cycle "Der Ring des Nibelungen" and several other works by Strauss, Béla Bartók, and others; it has a prominent role in Anton Bruckner's Symphony No. 7 in E Major. Cornets appear in Pyotr Ilyich Tchaikovsky's ballet "Swan Lake", Claude Debussy's "La Mer", and several orchestral works by Hector Berlioz. Unless these instruments are played by members "doubling" on another instrument (for example, a trombone player changing to euphonium or a bassoon player switching to contrabassoon for a certain passage), orchestras typically hire freelance musicians to augment their regular ensemble.

The 20th-century orchestra was far more flexible than its predecessors. In Beethoven's and Felix Mendelssohn's time, the orchestra was composed of a fairly standard core of instruments, which was very rarely modified by composers. As time progressed, and as the Romantic period saw changes in accepted modification with composers such as Berlioz and Mahler; some composers used multiple harps and sound effect such as the wind machine. During the 20th century, the modern orchestra was generally standardized with the modern instrumentation listed below. Nevertheless, by the mid- to late 20th century, with the development of contemporary classical music, instrumentation could practically be hand-picked by the composer (e.g., to add electric instruments such as electric guitar, electronic instruments such as synthesizers, non-Western instruments, or other instruments not traditionally used in orchestra).

With this history in mind, the orchestra can be analyzed in five periods: the Baroque era, the Classical music period, early/mid-Romantic music era, late-Romantic/early 20th century music and 21st century era. The first is a Baroque orchestra (i.e., J.S. Bach, Handel, Vivaldi), which generally had a smaller number of performers, and in which one or more chord-playing instruments, the basso continuo group (e.g., harpsichord or pipe organ and assorted bass instruments to perform the bassline), played an important role; the second is a typical classical period orchestra (e.g., early Beethoven along with Mozart and Haydn), which used a smaller group of performers than a Romantic music orchestra and a fairly standardized instrumentation; the third is typical of an early/mid-Romantic era (e.g., Schubert, Berlioz, Schumann); the fourth is a late-Romantic/early 20th century orchestra (e.g., Wagner, Brahms, Mahler, Stravinsky), to the common complement of a 2010-era modern orchestra (e.g., Adams, Barber, Aaron Copland, Glass, Penderecki).



















Among the instrument groups and within each group of instruments, there is a generally accepted hierarchy. Every instrumental group (or section) has a principal who is generally responsible for leading the group and playing orchestral solos. The violins are divided into two groups, first violin and second violin, with the second violins playing in lower registers than the first violins, playing an accompaniment part, or harmonizing the melody played by the first violins. The principal first violin is called the concertmaster (or "leader" in the UK) and is not only considered the leader of the string section, but the second-in-command of the entire orchestra, behind only the conductor. The concertmaster leads the pre-concert tuning and handles musical aspects of orchestra management, such as determining the bowings for the violins or for all of the string section. The concertmaster usually sits to the conductor's left, closest to the audience. There is also a principal second violin, a principal viola, a principal cello and a principal bass.

The principal trombone is considered the leader of the low brass section, while the principal trumpet is generally considered the leader of the entire brass section. While the oboe often provides the tuning note for the orchestra (due to 300-year-old convention), no principal is the leader of the woodwind section though in woodwind ensembles, often the flute is leader. Instead, each principal confers with the others as equals in the case of musical differences of opinion. Most sections also have an assistant principal (or co-principal or associate principal), or in the case of the first violins, an assistant concertmaster, who often plays a tutti part in addition to replacing the principal in his or her absence.

A section string player plays in unison with the rest of the section, except in the case of divided ("divisi") parts, where upper and lower parts in the music are often assigned to "outside" (nearer the audience) and "inside" seated players. Where a solo part is called for in a string section, the section leader invariably plays that part. The section leader (or principal) of a string section is also responsible for determining the bowings, often based on the bowings set out by the concertmaster. In some cases, the principal of a string section may use a slightly different bowing than the concertmaster, to accommodate the requirements of playing their instrument (e.g., the double-bass section). Principals of a string section will also lead entrances for their section, typically by lifting the bow before the entrance, to ensure the section plays together. Tutti wind and brass players generally play a unique but non-solo part. Section percussionists play parts assigned to them by the principal percussionist.

In modern times, the musicians are usually directed by a conductor, although early orchestras did not have one, giving this role instead to the concertmaster or the harpsichordist playing the continuo. Some modern orchestras also do without conductors, particularly smaller orchestras and those specializing in historically accurate (so-called "period") performances of baroque and earlier music.

The most frequently performed repertoire for a symphony orchestra is Western classical music or opera. However, orchestras are used sometimes in popular music (e.g., to accompany a rock or pop band in a concert), extensively in film music, and increasingly often in video game music. Orchestras are also used in the symphonic metal genre. The term "orchestra" can also be applied to a jazz ensemble, for example in the performance of big-band music.

In the 2000s, all tenured members of a professional orchestra normally audition for positions in the ensemble. Performers typically play one or more solo pieces of the auditionee's choice, such as a movement of a concerto, a solo Bach movement, and a variety of excerpts from the orchestral literature that are advertised in the audition poster (so the auditionees can prepare). The excerpts are typically the most technically challenging parts and solos from the orchestral literature. Orchestral auditions are typically held in front of a panel that includes the conductor, the concertmaster, the principal player of the section for which the auditionee is applying and possibly other principal players and regular orchestra members.

The most promising candidates from the first round of auditions are invited to return for a second or third round of auditions, which allows the conductor and the panel to compare the best candidates. Performers may be asked to sight read orchestral music. The final stage of the audition process in some orchestras is a "test week", in which the performer plays with the orchestra for a week or two, which allows the conductor and principal players to see if the individual can function well in an actual rehearsal and performance setting.

There are a range of different employment arrangements. The most sought-after positions are permanent, tenured positions in the orchestra. Orchestras also hire musicians on contracts, ranging in length from a single concert to a full season or more. Contract performers may be hired for individual concerts when the orchestra is doing an exceptionally large late-Romantic era orchestral work, or to substitute for a permanent member who is sick. A professional musician who is hired to perform for a single concert is sometimes called a "sub". Some contract musicians may be hired to replace permanent members for the period that the permanent member is on parental leave or disability leave.

Historically, major professional orchestras have been mostly or entirely composed of male musicians. The first female members hired in professional orchestras have been harpists. The Vienna Philharmonic, for example, did not accept women to permanent membership until 1997, far later than comparable orchestras (the other orchestras ranked among the world’s top five by "Gramophone" in 2008). The last major orchestra to appoint a woman to a permanent position was the Berlin Philharmonic. In February 1996, the Vienna Philharmonic's principal flute, Dieter Flury, told "Westdeutscher Rundfunk" that accepting women would be "gambling with the emotional unity () that this organism currently has". In April 1996, the orchestra’s press secretary wrote that "compensating for the expected leaves of absence" of maternity leave would be a problem.

In 1997, the Vienna Philharmonic was "facing protests during a [US] tour" by the National Organization for Women and the International Alliance for Women in Music. Finally, "after being held up to increasing ridicule even in socially conservative Austria, members of the orchestra gathered [on 28 February 1997] in an extraordinary meeting on the eve of their departure and agreed to admit a woman, Anna Lelkes, as harpist." As of 2013, the orchestra has six female members; one of them, violinist Albena Danailova, became one of the orchestra’s concertmasters in 2008, the first woman to hold that position. In 2012, women still made up just 6% of the orchestra's membership. VPO president Clemens Hellsberg said the VPO now uses completely screened blind auditions.

In 2013, an article in "Mother Jones" stated that while "[m]any prestigious orchestras have significant female membership—women outnumber men in the New York Philharmonic's violin section—and several renowned ensembles, including the National Symphony Orchestra, the Detroit Symphony, and the Minnesota Symphony, are led by women violinists", the double bass, brass, and percussion sections of major orchestras "...are still predominantly male." A 2014 BBC article stated that the "...introduction of ‘blind’ auditions, where a prospective instrumentalist performs behind a screen so that the judging panel can exercise no gender or racial prejudice, has seen the gender balance of traditionally male-dominated symphony orchestras gradually shift."

There are also a variety of amateur orchestras:

Orchestras play a wide range of repertoire ranging from 17th-century dance suites, 18th century divertimentos to 20th century film scores and 21st-century symphonies. Orchestras have become synonymous with the symphony, an extended musical composition in Western classical music that typically contains multiple movements which provide contrasting keys and tempos. Symphonies are notated in a musical score, which contains all the instrument parts. The conductor uses the score to study the symphony before rehearsals and decide on their interpretation (e.g., tempos, articulation, phrasing, etc.), and to follow the music during rehearsals and concerts, while leading the ensemble. Orchestral musicians play from parts containing just the notated music for their instrument. A small number of symphonies also contain vocal parts (e.g., Beethoven's Ninth Symphony).

Orchestras also perform overtures, a term originally applied to the instrumental introduction to an opera. During the early Romantic era, composers such as Beethoven and Mendelssohn began to use the term to refer to independent, self-existing instrumental, programmatic works that presaged genres such as the symphonic poem, a form devised by Franz Liszt in several works that began as dramatic overtures. These were "at first undoubtedly intended to be played at the head of a programme". In the 1850s the concert overture began to be supplanted by the symphonic poem.

Orchestras also play with instrumental soloists in concertos. During concertos, the orchestra plays an accompaniment role to the soloist (e.g., a solo violinist or pianist) and, at times, introduces musical themes or interludes while the soloist is not playing. Orchestras also play during operas, ballets, some musical theatre works and some choral works (both sacred works such as Masses and secular works). In operas and ballets, the orchestra accompanies the singers and dancers, respectively, and plays overtures and interludes where the melodies played by the orchestra take centre stage.

In the Baroque era, orchestras performed in a range of venues, including at the fine houses of aristocrats, in opera halls and in churches. Some wealthy aristocrats had an orchestra in residence at their estate, to entertain them and their guests with performances. During the Classical era, as composers increasing sought out financial support from the general public, orchestra concerts were increasingly held in public concert halls, where music lovers could buy tickets to hear the orchestra. Of course, aristocratic patronage of orchestras continued during the Classical era, but this went on alongside public concerts. In the 20th and 21st century, orchestras found a new patron: governments. Many orchestras in North America and Europe receive part of their funding from national, regional level governments (e.g., state governments in the U.S.) or city governments. These government subsidies make up part of orchestra revenue, along with ticket sales, charitable donations (if the orchestra is a registered as a charity) and other fundraising activities. With the invention of successive technologies, including sound recording, radio broadcasting, television broadcasting and Internet-based streaming and downloading of concert videos, orchestras have been able to find new revenue sources.

One of the "great unmentionable [topics] of orchestral playing" is "faking", the process by which an orchestral musician gives the "...impression of playing every note as written", typically for a very challenging passage that is very high or very fast, while not actually playing the notes that are in the printed music part. An article in "The Strad" states that all orchestral musicians, even those in the top orchestras, occasionally "fake" certain passages. One reason that musicians "fake" is because there are not enough rehearsals. Another factor is the extreme challenges in 20th century and 21st century contemporary pieces; professionals interviewed by the magazine said "faking" was "...necessary in anything from ten to almost ninety per cent of some modern works. Professional players who were interviewed were of a consensus that faking may be acceptable when a part is not written well for the instrument, but faking "just because you haven’t practised" the music is not acceptable.

The invention of the piston and rotary valve by Heinrich Stölzel and Friedrich Blühmel, both Silesians, in 1815, was the first in a series of innovations which impacted the orchestra, including the development of modern keywork for the flute by Theobald Boehm and the innovations of Adolphe Sax in the woodwinds, notably the invention of the saxophone. These advances would lead Hector Berlioz to write a landmark book on instrumentation, which was the first systematic treatise on the use of instrumental sound as an expressive element of music.

The next major expansion of symphonic practice came from Richard Wagner's Bayreuth orchestra, founded to accompany his musical dramas. Wagner's works for the stage were scored with unprecedented scope and complexity: indeed, his score to "Das Rheingold" calls for six harps. Thus, Wagner envisioned an ever-more-demanding role for the conductor of the theatre orchestra, as he elaborated in his influential work "On Conducting". This brought about a revolution in orchestral composition, and set the style for orchestral performance for the next eighty years. Wagner's theories re-examined the importance of tempo, dynamics, bowing of string instruments and the role of principals in the orchestra. Conductors who studied his methods would go on to be influential themselves.

As the early 20th century dawned, symphony orchestras were larger, better funded, and better trained than ever before; consequently, composers could compose larger and more ambitious works. The influence of Gustav Mahler was particularly innovational; in his later symphonies, such as the mammoth Symphony No. 8, Mahler pushes the furthest boundaries of orchestral size, employing huge forces. By the late Romantic era, orchestras could support the most enormous forms of symphonic expression, with huge string sections, massive brass sections and an expanded range of percussion instruments. With the recording era beginning, the standards of performance were pushed to a new level, because a recorded symphony could be listened to closely and even minor errors in intonation or ensemble, which might not be noticeable in a live performance, could be heard by critics. As recording technologies improved over the 20th and 21st centuries, eventually small errors in a recording could be "fixed" by audio editing or overdubbing. Some older conductors and composers could remember a time when simply "getting through" the music as best as possible was the standard. Combined with the wider audience made possible by recording, this led to a renewed focus on particular star conductors and on a high standard of orchestral execution.

With the advent of the early music movement, smaller orchestras where players worked on execution of works in styles derived from the study of older treatises on playing became common. These include the Orchestra of the Age of Enlightenment, the London Classical Players under the direction of Sir Roger Norrington and the Academy of Ancient Music under Christopher Hogwood, among others.

In the United States, the late 20th century saw a crisis of funding and support for orchestras. The size and cost of a symphony orchestra, compared to the size of the base of supporters, became an issue that struck at the core of the institution. Few orchestras could fill auditoriums, and the time-honored season-subscription system became increasingly anachronistic, as more and more listeners would buy tickets on an ad hoc basis for individual events. Orchestral endowments and—more centrally to the daily operation of American orchestras—orchestral donors have seen investment portfolios shrink or produce lower yields, reducing the ability of donors to contribute; further, there has been a trend toward donors finding other social causes more compelling. Also, while government funding is less central to American than European orchestras, cuts in such funding are still significant for American ensembles. Finally, the drastic falling-off of revenues from recording, tied to no small extent to changes in the recording industry itself, began a period of change that has yet to reach its conclusion.

U.S. orchestras that have gone into Chapter 11 bankruptcy include the Philadelphia Orchestra (in April 2011), and the Louisville Orchestra, in December 2010; orchestras that have gone into Chapter 7 bankruptcy and have ceased operations include the Northwest Chamber Orchestra in 2006, the Honolulu Orchestra in March 2011, the New Mexico Symphony Orchestra in April 2011, and the Syracuse Symphony in June 2011. The Festival of Orchestras in Orlando, Florida ceased operations at the end of March, 2011.

One source of financial difficulties that received notice and criticism was high salaries for music directors of US orchestras, which led several high-profile conductors to take pay cuts in recent years. Music administrators such as Michael Tilson Thomas and Esa-Pekka Salonen argued that new music, new means of presenting it, and a renewed relationship with the community could revitalize the symphony orchestra. The American critic Greg Sandow has argued in detail that orchestras must revise their approach to music, performance, the concert experience, marketing, public relations, community involvement, and presentation to bring them in line with the expectations of 21st-century audiences immersed in popular culture.

It is not uncommon for contemporary composers to use unconventional instruments, including various synthesizers, to achieve desired effects. Many, however, find more conventional orchestral configuration to provide better possibilities for color and depth. Composers like John Adams often employ Romantic-size orchestras, as in Adams' opera "Nixon in China"; Philip Glass and others may be more free, yet still identify size-boundaries. Glass in particular has recently turned to conventional orchestras in works like the "Concerto for Cello and Orchestra" and the Violin Concerto No. 2.

Along with a decrease in funding, some U.S. orchestras have reduced their overall personnel, as well as the number of players appearing in performances. The reduced numbers in performance are usually confined to the string section, since the numbers here have traditionally been flexible (as multiple players typically play from the same part).

Conducting is the art of directing a musical performance, such as an orchestral or choral concert. The primary duties of the conductor are to set the tempo, ensure correct entries by various members of the ensemble, and to "shape" the phrasing where appropriate. To convey their ideas and interpretation, a conductor communicates with their musicians primarily through hand gestures, typically though not invariably with the aid of a baton, and may use other gestures or signals, such as eye contact with relevant performers. A conductor's directions will almost invariably be supplemented or reinforced by verbal instructions or suggestions to their musicians in rehearsal prior to a performance.

The conductor typically stands on a raised podium with a large music stand for the full score, which contains the musical notation for all the instruments and voices. Since the mid-18th century, most conductors have not played an instrument when conducting, although in earlier periods of classical music history, leading an ensemble while playing an instrument was common. In Baroque music from the 1600s to the 1750s, the group would typically be led by the harpsichordist or first violinist (see concertmaster), an approach that in modern times has been revived by several music directors for music from this period. Conducting while playing a piano or synthesizer may also be done with musical theatre pit orchestras. Communication is typically non-verbal during a performance (this is strictly the case in art music, but in jazz big bands or large pop ensembles, there may be occasional spoken instructions, such as a "count in"). However, in rehearsals, frequent interruptions allow the conductor to give verbal directions as to how the music should be played or sung.

Conductors act as guides to the orchestras or choirs they conduct. They choose the works to be performed and study their scores, to which they may make certain adjustments (e.g., regarding tempo, articulation, phrasing, repetitions of sections, and so on), work out their interpretation, and relay their vision to the performers. They may also attend to organizational matters, such as scheduling rehearsals, planning a concert season, hearing auditions and selecting members, and promoting their ensemble in the media. Orchestras, choirs, concert bands and other sizable musical ensembles such as big bands are usually led by conductors.

In the Baroque music era (1600–1750), most orchestras were led by one of the musicians, typically the principal first violin, called the concertmaster. The concertmaster would lead the tempo of pieces by lifting his or her bow in a rhythmic manner. Leadership might also be provided by one of the chord-playing instrumentalists playing the basso continuo part which was the core of most Baroque instrumental ensemble pieces. Typically, this would be a harpsichord player, a pipe organist or a luteist or theorbo player. A keyboard player could lead the ensemble with his or her head, or by taking one of the hands off the keyboard to lead a more difficult tempo change. A lutenist or theorbo player could lead by lifting the instrument neck up and down to indicate the tempo of a piece, or to lead a ritard during a cadence or ending. In some works which combined choirs and instrumental ensembles, two leaders were sometimes used: a concertmaster to lead the instrumentalists and a chord-playing performer to lead the singers. During the Classical music period (ca. 1720–1800), the practice of using chordal instruments to play basso continuo was gradually phased out, and it disappeared completely by 1800. Instead, ensembles began to use conductors to lead the orchestra's tempos and playing style, while the concertmaster played an additional leadership role for the musicians, especially the string players, who imitate the bowstroke and playing style of the concertmaster, to the degree that is feasible for the different stringed instruments.

In 1922, the idea of a conductor-less orchestra was revived in post-revolutionary Soviet Union. The symphony orchestra Persimfans was formed without a conductor, because the founders believed that the ensemble should be modeled on the ideal Marxist state, in which all people are equal. As such, its members felt that there was no need to be led by the dictatorial baton of a conductor; instead they were led by a committee, which determined tempos and playing styles. Although it was a partial success within the Soviet Union, the principal difficulty with the concept was in changing tempo during performances, because even if the committee had issued a decree about where a tempo change should take place, there was no leader in the ensemble to guide this tempo change. The orchestra survived for ten years before Stalin's cultural politics disbanded it by taking away its funding.

In Western nations, some ensembles, such as the Orpheus Chamber Orchestra, based in New York City, have had more success with conductorless orchestras, although decisions are likely to be deferred to some sense of leadership within the ensemble (for example, the principal wind and string players, notably the concertmaster). Others have returned to the tradition of a principal player, usually a violinist, being the artistic director and running rehearsal and leading concerts. Examples include the Australian Chamber Orchestra, Amsterdam Sinfonietta & Candida Thompson and the New Century Chamber Orchestra. As well, as part of the early music movement, some 20th and 21st century orchestras have revived the Baroque practice of having no conductor on the podium for Baroque pieces, using the concertmaster or a chord-playing basso continuo performer (e.g., harpsichord or organ) to lead the group.

Some orchestral works specify that an offstage trumpet should be used or that other instruments from the orchestra should be positioned off-stage or behind the stage, to create a haunted, mystical effect. To ensure that the offstage instrumentalist(s) play in time, sometimes a sub-conductor will be stationed offstage with a clear view of the principal conductor. Examples include the ending of "Neptune" from Gustav Holst's "The Planets". The principal conductor leads the large orchestra, and the sub-conductor relays the principal conductor's tempo and gestures to the offstage musician (or musicians). One of the challenges with using two conductors is that the second conductor may get out of synchronization with the main conductor, or may mis-convey (or misunderstand) the principal conductor's gestures, which can lead to the offstage instruments being out of time. In the late 20th century and early 21st century, some orchestras use a video camera pointed at the principal conductor and a closed-circuit TV set in front of the offstage performer(s), instead of using two conductors.

The techniques of polystylism and polytempo music have led a few 20th and 21st century composers to write music where multiple orchestras or ensembles perform simultaneously. These trends have brought about the phenomenon of polyconductor music, wherein separate sub-conductors conduct each group of musicians. Usually, one principal conductor conducts the sub-conductors, thereby shaping the overall performance. In Percy Grainger's "The Warriors" which includes three conductors: the primary conductor of the orchestra, a secondary conductor directing an off-stage brass ensemble, and a tertiary conductor directing percussion and harp. One example in the late century orchestral music is Karlheinz Stockhausen's "Gruppen", for three orchestras, which are placed around the audience. This way, the "sound masses" could be spacialized, as in an electroacoustic work. "Gruppen" was premiered in Cologne, in 1958, conducted by Stockhausen, Bruno Maderna and Pierre Boulez. It has been performed by Simon Rattle, John Carewe and Daniel Harding.





</doc>
<doc id="22707" url="https://en.wikipedia.org/wiki?curid=22707" title="Oolong">
Oolong

Oolong () () is a traditional semi-fermented Chinese tea ("Camellia sinensis)" produced through a process including withering the plant under strong sun and oxidation before curling and twisting. Most oolong teas, especially those of fine quality, involve unique tea plant cultivars that are exclusively used for particular varieties. The degree of fermentation, which varies according to the chosen oxidation duration, can range from 8–85%, depending on the variety and production style. Oolong is especially popular in south China and among Chinese expatriates in Southeast Asia, as is the Fujian preparation process known as the Gongfu tea ceremony.

Different styles of oolong tea can vary widely in flavor. They can be sweet and fruity with honey aromas, or woody and thick with roasted aromas, or green and fresh with complex aromas, all depending on the horticulture and style of production. Several types of oolong tea, including those produced in the Wuyi Mountains of northern Fujian, such as Da Hong Pao, are among the most famous Chinese teas. Different varieties of oolong are processed differently, but the leaves are usually formed into one of two distinct styles. Some are rolled into long curly leaves, while others are 'wrap-curled' into small beads, each with a tail. The former style is the more traditional.

The name "oolong tea" came into the English language from the Chinese name (), meaning "black dragon tea". In Chinese, oolong teas are also known as "qingcha" () or "dark green teas".

The manufacture of oolong tea involves repeating stages to achieve the desired amount of bruising and browning of leaves. Withering, rolling, shaping, and firing are similar to black tea, but much more attention to timing and temperature is necessary.

There are three widely accepted explanations of the origin of the Chinese name. According to the "tribute tea" theory, oolong tea came directly from Dragon-Phoenix Tea Cake tribute tea. The term oolong tea replaced the old term when loose tea came into fashion. Since it was dark, long, and curly, it was called Black Dragon tea.

According to the "Wuyi" theory, oolong tea first existed in the Wuyi Mountains region. This is evidenced by Qing dynasty poems such as Wuyi Tea Song (Wuyi Chage) and Tea Tale (Chashuo). It was said that oolong tea was named after the part of the Wuyi Mountain where it was originally produced.

According to the "Anxi" theory, oolong tea had its origin in the Anxi oolong tea plant, which was discovered by a man named Sulong, Wulong, or Wuliang.

Another tale tells of a man named Wu Liang (later corrupted to Wu Long, or Oolong) who discovered oolong tea by accident when he was distracted by a deer after a hard day's tea-picking, and by the time he remembered to return to the tea it had already started to oxidize.

Tea production in Fujian is concentrated in two regions: the Wuyi Mountains and Anxi County. Both are major historical centers of oolong tea production in China.

The most famous and expensive oolong teas are made here, and the production is still usually accredited as being organic. Some of the better known cliff teas are:



The term "dancong" originally meant phoenix teas all picked from one tree. In recent times though it has become a generic term for all Phoenix Mountain oolongs. True dancongs are still produced, but are not common outside China.

Tea cultivation in Taiwan began in the 18th century. Since then, many of the teas which are grown in Fujian province have also been grown in Taiwan. Since the 1970s, the tea industry in Taiwan has expanded at a rapid rate, in line with the rest of the economy. Due to high domestic demand and a strong tea culture, most Taiwanese tea is bought and consumed in Taiwan.

As the weather in Taiwan is highly variable, tea quality may differ from season to season. Although the island is not particularly large, it is geographically varied, with high, steep mountains rising abruptly from low-lying coastal plains. The different weather patterns, temperatures, altitudes, and soil ultimately result in differences in appearance, aroma, and flavour of the tea grown in Taiwan. In some mountainous areas, teas have been cultivated at ever higher elevations to produce a unique sweet taste that fetches a premium price.



Generally, 3 grams of tea per 200 ml of water, or about two teaspoons of oolong tea per cup, should be used. Oolong teas should be prepared with 200 to 205 °F (93 to 96 °C) water (not boiling) and steeped 2-3 minutes. High quality oolong can be steeped several times from the same leaves and improves with rebrewing: it is common to steep the same leaves three to five times, the third or fourth steeping usually being considered the best.

A widely used ceremonial method of steeping oolongs in Taiwan and China is called gongfucha. This method uses a small steeping vessel, such as a gaiwan or Yixing clay teapot, with more tea than usual for the amount of water used. Multiple short steeps of 20 seconds to 1 minute are performed; the tea is often served in one- to two-ounce tasting cups.

Oolong generally contains caffeine, although the caffeine content in tea will vary based on terroir, when the leaf is plucked, and the production processes.



</doc>
<doc id="22709" url="https://en.wikipedia.org/wiki?curid=22709" title="Okapi">
Okapi

The okapi (; "Okapia johnstoni"), also known as the forest giraffe, congolese giraffe or zebra giraffe, is an artiodactyl mammal native to the northeast of the Democratic Republic of the Congo in Central Africa. Although the okapi bears striped markings reminiscent of zebras, it is most closely related to the giraffe. The okapi and the giraffe are the only living members of the family Giraffidae. The okapi stands about tall at the shoulder and has an average body length around . Its weight ranges from . It has a long neck, and large, flexible ears. Its coat is a chocolate to reddish brown, much in contrast with the white horizontal stripes and rings on the legs and white ankles. Male okapis have short, hair-covered, horn-like protuberances on their heads called ossicones, less than in length. Females possess hair whorls, and ossicones are absent.

Okapis are primarily diurnal, but may be active for a few hours in darkness. They are essentially solitary, coming together only to breed. Okapis are herbivores, feeding on tree leaves and buds, grasses, ferns, fruits, and fungi. Rut in males and estrus in females does not depend on the season. In captivity, estrous cycles recur every 15 days. The gestational period is around 440 to 450 days long, following which usually a single calf is born. The juveniles are kept in hiding, and nursing takes place infrequently. Juveniles start taking solid food from three months, and weaning takes place at six months.

Okapis inhabit canopy forests at altitudes of . They are endemic to the tropical forests of the Democratic Republic of the Congo, where they occur across the central, northern, and eastern regions. The International Union for the Conservation of Nature and Natural Resources classifies the okapi as endangered. Major threats include habitat loss due to logging and human settlement. Extensive hunting for bushmeat and skin and illegal mining have also led to a decline in populations. The Okapi Conservation Project was established in 1987 to protect okapi populations.

Although the okapi was unknown to the Western world until the 20th century, it may have been depicted since the early fifth century BCE on the façade of the Apadana at Persepolis, a gift from the Ethiopian procession to the Achaemenid kingdom.

For years, Europeans in Africa had heard of an animal that they came to call the African unicorn. In his travelogue of exploring the Congo, Henry Morton Stanley mentioned a kind of donkey that the natives called the "atti", which scholars later identified as the okapi. Explorers may have seen the fleeting view of the striped backside as the animal fled through the bushes, leading to speculation that the okapi was some sort of rainforest zebra.

When the British governor of Uganda, Sir Harry Johnston, discovered some pygmy inhabitants of the Congo being abducted by a showman for exhibition, he rescued them and promised to return them to their homes. The grateful pygmies fed Johnston's curiosity about the animal mentioned in Stanley's book. Johnston was puzzled by the okapi tracks the natives showed him; while he had expected to be on the trail of some sort of forest-dwelling horse, the tracks were of a cloven-hoofed beast.

Though Johnston did not see an okapi himself, he did manage to obtain pieces of striped skin and eventually a skull. From this skull, the okapi was correctly classified as a relative of the giraffe; in 1901, the species was formally recognized as "Okapia johnstoni".

"Okapia johnstoni" was first described as "Equus johnstoni" by English zoologist Philip Lutley Sclater in 1901. The generic name "Okapia" derives from the Lese Karo name "o'api", while the specific name ("johnstoni") is in recognition of the British Governor of Uganda, Sir Harry Johnston, who first acquired an okapi specimen for science from the Ituri Forest while repatriating a group of Pygmies to the Belgian Congo. The animal was brought to prominent European attention by speculation on its existence found in press reports covering Henry Morton Stanley's journeys in 1887. Remains of a carcass were later sent to London by the English adventurer and colonial administrator Harry Johnston and became a media event in 1901.

In 1901, Sclater presented a painting of the okapi before the Zoological Society of London that depicted its physical features with some clarity. Much confusion arose regarding the taxonomical status of this newly discovered animal. Sir Harry Johnston himself called it a "Helladotherium", or a relative of other extinct giraffids. Based on the description of the okapi by Pygmies, who referred to it as a "horse", Sclater named the species "Equus johnstoni". Subsequently, Lankester declared that the okapi represented an unknown genus of the Giraffidae, which he placed in its own genus, "Okapia", and assigned the name "Okapia johnstoni" to the species.

In 1902, Swiss zoologist Charles Immanuel Forsyth Major suggested the inclusion of "O. johnstoni" in the extinct giraffid subfamily Palaeotraginae. However, the species was placed in its own subfamily Okapiinae, by Swedish palaeontologist Birger Bohlin in 1926, mainly due to the lack of a cingulum, a major feature of the palaeotragids. In 1986, "Okapia" was finally established as a sister genus of "Giraffa" on the basis of cladistic analysis. The two genera together with "Palaeotragus" constitute the tribe Giraffini.

The earliest members of the Giraffidae first appeared in the early Miocene in Africa, having diverged from the superficially deer-like climacoceratids. Giraffids spread into Europe and Asia by the middle Miocene in a first radiation. Another radiation began in the Pliocene, but was terminated by a decline in diversity in the Pleistocene. Several important primitive giraffids existed more or less contemporaneously in the Miocene (23-10 million years ago), including "Canthumeryx", "Giraffokeryx", "Palaeotragus", and "Samotherium". According to palaeontologist and author Kathleen Hunt, "Samotherium" split into "Okapia" (18 million years ago) and "Giraffa" (12 million years ago). However, J. D. Skinner argued that "Canthumeryx" gave rise to the okapi and giraffe through the latter three genera and that the okapi is the extant form of "Palaeotragus". The okapi is sometimes referred to as an example of a living fossil, as it has existed as a species over a long geological time period, and morphologically resembles more primitive forms (e.g. "Samotherium").

A 2016 study found that the common ancestor of giraffe and okapi lived about 11.5 million years ago.

The okapi is a medium-sized giraffid, standing tall at the shoulder. Its average body length is about and its weight ranges from . It has a long neck, and large and flexible ears. The coat is a chocolate to reddish brown, much in contrast with the white horizontal stripes and rings on the legs and white ankles. The striking stripes make it resemble a zebra. These features serve as an effective camouflage amidst dense vegetation. The face, throat, and chest are greyish white. Interdigital glands are present on all four feet, and are slightly larger on the front feet. Male okapis have short, hair-covered horns called ossicones, less than in length. The okapi exhibits sexual dimorphism, with females taller on average, slightly redder, and lacking prominent ossicones, instead possessing hair whorls.

The okapi shows several adaptations to its tropical habitat. The large number of rod cells in the retina facilitate night vision, and an efficient olfactory system is present. The large auditory bullae allow a strong sense of hearing. The dental formula of the okapi is . Teeth are low-crowned and finely cusped, and efficiently cut tender foliage. The large caecum and colon help in microbial digestion, and a quick rate of food passage allows for lower cell wall digestion than in other ruminants.
The okapi can be easily distinguished from its nearest extant relative, the giraffe. It is much smaller and shares more external similarities with the deer and bovids than with the giraffe. While both sexes possess horns in the giraffe, only males bear horns in the okapi. The okapi has large palatine sinuses, unique among the giraffids. Morphological similarities shared between the giraffe and the okapi include a similar gait – both use a pacing gait, stepping simultaneously with the front and the hind leg on the same side of the body, unlike other ungulates that walk by moving alternate legs on either side of the body – and a long, black tongue (longer in the okapi) useful in plucking buds and leaves, as well as for grooming.

Okapis are primarily diurnal, but may be active for a few hours in darkness. They are essentially solitary, coming together only to breed. They have overlapping home ranges and typically occur at densities around 0.6 animals per square kilometre. Male home ranges average , while female home ranges average . Males migrate continuously, while females are sedentary. Males often mark territories and bushes with their urine, while females use common defecation sites. Grooming is a common practice, focused at the earlobes and the neck. Okapis often rub their necks against trees, leaving a brown exudate.

The male is protective of his territory, but allows females to pass through the domain to forage. Males visit female home ranges at breeding time. Although generally tranquil, the okapi can kick and butt with its head to show aggression. As the vocal cords are poorly developed, vocal communication is mainly restricted to three sounds — "chuff" (contact calls used by both sexes), "moan" (by females during courtship) and "bleat" (by infants under stress). Individuals may engage in Flehmen response, a visual expression in which the animal curls back its upper lips, displays the teeth, and inhales through the mouth for a few seconds. The leopard is the main predator of the okapi.

Okapis are herbivores, feeding on tree leaves and buds, grasses, ferns, fruits, and fungi. They are unique in the Ituri Forest as they are the only known mammal that feeds solely on understory vegetation, where they use their 18 inch tongues to selectively browse for suitable plants. The tongue is also used to groom their ears and eyes. They prefer to feed in treefall gaps. The okapi has been known to feed on over 100 species of plants, some of which are known to be poisonous to humans and other animals. Fecal analysis shows that none of those 100 species dominates the diet of the okapi. Staple foods comprise shrubs and lianas. The main constituents of the diet are woody, dicotyledonous species; monocotyledonous plants are not eaten regularly. In the Ituri forest, the okapi feeds mainly upon the plant families Acanthaceae, Ebenaceae, Euphorbiaceae, Flacourtiaceae, Loganiaceae, Rubiaceae, and Violaceae.

Female okapis become sexually mature at about one-and-a-half years old, while males reach maturity after two years. Rut in males and estrous in females does not depend on the season. In captivity, estrous cycles recur every 15 days. The male and the female begin courtship by circling, smelling and licking each other. The male shows his dominance by extending his neck, tossing his head, and protruding one leg forward. This is followed by mounting and copulation.

The gestational period is around 440 to 450 days long, following which usually a single calf is born, weighing . The udder of the pregnant female starts swelling two months before parturition, and vulval discharges may occur. Parturition takes 3–4 hours, and the female stands throughout this period, though she may rest during brief intervals. The mother consumes the afterbirth and extensively grooms the infant. Her milk is very rich in proteins and low in fat.

As in other ruminants, the infant can stand within 30 minutes of birth. Although generally similar to adults, newborn calves have false eyelashes, a long dorsal mane, and long white hairs in the stripes. These features gradually disappear and give way to the general appearance within a year. The juveniles are kept in hiding, and nursing takes place infrequently. Calves are known not to defecate for the first month or two of life, which is hypothesized to help avoid predator detection in their most vulnerable phase of life. The growth rate of calves is appreciably high in the first few months of birth, after which it gradually declines. Juveniles start taking solid food from three months, and weaning takes place at six months. Horn development in males takes one year after birth. The okapi's typical lifespan is 20 to 30 years.

Okapis inhabit canopy forests at altitudes of . They are endemic to the tropical forests of the Democratic Republic of the Congo. They do not occur in gallery forests, habitats disturbed by human settlement, and swamp forests, but may occasionally use seasonally inundated areas. In the wet season, they visit rocky inselbergs that offer forage uncommon elsewhere. The population density of the okapi averaged 0.53 animals per square kilometre in mixed "Cynometera" forests.

The okapi occurs across central, northern, and eastern Democratic Republic of the Congo, and north and east of the Congo River. The species ranges from the Maiko forest northward to the Ituri forest, then through the river basins of the Rubi, Lake Tele, and Ebola to the west and the Ubangi River further north. Smaller populations exist west and south of the Congo. They are also common in the Wamba and Epulu areas. The okapi is extinct in Uganda.

The IUCN classifies the okapi as endangered. It is fully protected under Congolese law. The Okapi Wildlife Reserve and Maiko National Park support significant populations of the okapi, though a steady decline in numbers has occurred due to several threats. Other areas of occurrence are the Rubi Tele Hunting Reserve and the Abumombanzi Reserve. Major threats include habitat loss due to logging and human settlement. Extensive hunting for bushmeat and skin and illegal mining have also led to population declines. A threat that has emerged quite recently is the presence of illegal armed groups around protected areas, inhibiting conservation and monitoring actions. A small population occurs north of the Virunga National Park, but is bereft of protection due to the presence of armed groups in the vicinity. In June 2012, a gang of poachers attacked the headquarters of the Okapi Wildlife Reserve, killing six guards and other staff as well as all 14 okapis at their breeding center.

The Okapi Conservation Project, established in 1987, works towards the conservation of the okapi, as well as the growth of the indigenous Mbuti people. In November 2011, the White Oak Conservation center and Jacksonville Zoo and Gardens hosted an international meeting of the Okapi Species Survival Plan and the Okapi European Endangered Species Programme at Jacksonville, which was attended by representatives from zoos from the USA, Europe, and Japan. The aim was to discuss the management of captive okapis and arrange support for okapi conservation. Many zoos in North America and Europe currently have okapis in captivity.

Around 100 okapis are in accredited Association of Zoos and Aquariums (AZA) zoos. The okapi population is managed in America by the AZA's Species Survival Plan, a breeding program that works to ensure genetic diversity in the captive population of endangered animals, while the EEP (European studbook) and ISB (Global studbook) are managed by Antwerp Zoo, which was the first zoo to have an Okapi on display in 1919, as well as one of the most successful in breeding them.

The San Diego Zoo has exhibited okapis since 1956, and had their first birth of an okapi in 1962. Since then, over 60 births have occurred between the zoo and the San Diego Zoo Safari Park, with the most recent being Mosi, a male calf born in early August 2017 at the San Diego Zoo.

The Brookfield Zoo in Chicago has also greatly contributed to the captive population of okapis in accredited zoos. The zoo has had 28 okapi births since 1959.

Other North American Zoos that exhibit and breed okapis include Bronx Zoo (New York); Denver Zoo, Cheyenne Mountain Zoo (Colorado); Houston Zoo, Dallas Zoo, San Antonio Zoo (Texas); Disney's Animal Kingdom, Miami Zoo, Tampa's Lowry Park Zoo, Jacksonville Zoo (Florida); Los Angeles Zoo (California); Saint Louis Zoo (Missouri); Cincinnati Zoo, Columbus Zoo (Ohio); Memphis Zoo (Tennessee); Maryland Zoo (Maryland); and Omaha's Henry Doorly Zoo (Nebraska).

In Europe, zoos that exhibit and breed okapis include Chester Zoo, London Zoo, Bristol Zoo, Marwell Zoo, The Wild Place (United Kingdom); Dublin Zoo (Ireland); Berlin Zoo, Frankfurt Zoo, Wilhelma Zoo, Wuppertal Zoo, Cologne Zoo, Leipzig Zoo (Germany) and Antwerp Zoo (Belgium); Zurich Zoo, Zoo basel (Switzerland); Copenhagen Zoo (Denmark); Rotterdam Zoo, Safaripark Beekse Bergen (Netherlands) and Dvůr Králové Zoo (Czech Republic), Wrocław Zoo (Poland); Bioparc Zoo de Doué, ZooParc de Beauval (France); Lisbon Zoo (Portugal).

In Asia, only two zoos in Japan exhibit okapis; Ueno Zoo in Tokyo and Zoorasia in Yokohama.





</doc>
<doc id="22710" url="https://en.wikipedia.org/wiki?curid=22710" title="Ovary">
Ovary

The ovary is an organ found in the female reproductive system that produces an ovum. When released, this travels down the fallopian tube into the uterus, where it may become fertilised by a sperm. There is an ovary () found on the left and right sides of the body. The ovaries also secrete hormones that play a role in the menstrual cycle and fertility. The ovary progresses through many stages beginning in the prenatal period through menopause. It is also an endocrine gland because of the various hormones that it secretes.

The ovaries are considered the female gonads. Each ovary is whitish in color and located alongside the lateral wall of the uterus in a region called the ovarian fossa. The ovarian fossa is the region that is bounded by the external iliac artery and in front of the ureter and the internal iliac artery. This area is about 4 cm x 3 cm x 2 cm in size. The ovaries are surrounded by a capsule, and have an outer cortex and an inner medulla.

Usually, ovulation occurs in one of the two ovaries releasing an egg each menstrual cycle; however, if there was a case where one ovary was absent or dysfunctional then the other ovary would continue providing eggs to be released without any changes in cycle length or frequency.

The side of the ovary closest to the fallopian tube is connected to it by infundibulopelvic ligament, and the other side points downwards attached to the uterus via the ovarian ligament.

Other structures and tissues of the ovaries include: 

The ovaries lie within the pelvic cavity, on either side of the uterus, to which they are attached via a fibrous cord called the ovarian ligament. The ovaries are uncovered in the peritoneal cavity but are tethered to the body wall via the suspensory ligament of the ovary which is a posterior extension of the broad ligament of the uterus. The part of the broad ligament of the uterus that covers the ovary is known as the mesovarium.

The surface of the ovaries is covered with membrane consisting of a lining of simple cuboidal-to-columnar shaped mesothelium.

The outermost layer is called the germinal epithelium.

The outer layer is the ovarian cortex, consisting of ovarian follicles and stroma in between them. Included in the follicles are the cumulus oophorus, membrana granulosa (and the granulosa cells inside it), corona radiata, zona pellucida, and primary oocyte. Theca of follicle, antrum and liquor folliculi are also contained in the follicle. Also in the cortex is the corpus luteum derived from the follicles. The innermost layer is the ovarian medulla. It can be hard to distinguish between the cortex and medulla, but follicles are usually not found in the medulla.

Follicular cells flat epithelial cells that originate from surface epithelium covering the ovary, are surrounded by Granulosa cells - that have changed from flat to cuboidal and proliferated to produce a stratified epithelium

Other

The ovary also contains blood vessels and lymphatics.

At puberty, the ovary begins to secrete increasing levels of hormones. Secondary sex characteristics begin to develop in response to the hormones. The ability to produce eggs and reproduce develops. The ovary changes structure and function beginning at puberty.

The ovaries are the site of production and periodical release of egg cells, the female gametes. In the ovaries, the developing egg cells (or oocytes) mature in the fluid-filled follicles. Typically, only one oocyte develops at a time, but others can also mature simultaneously. Follicles are composed of different types and number of cells according to the stage of their maturation, and their size is indicative of the stage of oocyte development.

When the oocyte finishes its maturation in the ovary, a surge of luteinizing hormone secreted by the pituitary gland stimulates the release of the oocyte through the rupture of the follicle, a process called ovulation. The follicle remains functional and reorganizes into a corpus luteum, which secretes progesterone in order to prepare the uterus for an eventual implantation of the embryo.

At maturity, ovaries secrete estrogen, testosterone, inhibin, and progesterone. In women, fifty percent of testosterone is produced by the ovaries and adrenal glands and released directly into the blood stream. Estrogen is responsible for the appearance of secondary sex characteristics for females at puberty and for the maturation and maintenance of the reproductive organs in their mature functional state. Progesterone prepares the uterus for pregnancy, and the mammary glands for lactation. Progesterone functions with estrogen by promoting menstrual cycle changes in the endometrium.

As women age, they experience a decline in reproductive performance leading to menopause. This decline is tied to a decline in the number of ovarian follicles. Although about 1 million oocytes are present at birth in the human ovary, only about 500 (about 0.05%) of these ovulate, and the rest are wasted. The decline in ovarian reserve appears to occur at a constantly increasing rate with age, and leads to nearly complete exhaustion of the reserve by about age 52. As ovarian reserve and fertility decline with age, there is also a parallel increase in pregnancy failure and meiotic errors resulting in chromosomally abnormal conceptions.

Women with an inherited mutation in the DNA repair gene BRCA1 undergo menopause prematurely, suggesting that naturally occurring DNA damages in oocytes are repaired less efficiently in these women, and this inefficiency leads to early reproductive failure. The BRCA1 protein plays a key role in a type of DNA repair termed homologous recombinational repair that is the only known cellular process that can accurately repair DNA double-strand breaks. Titus et al. showed that DNA double-strand breaks accumulate with age in humans and mice in primordial follicles. Primordial follicles contain oocytes that are at an intermediate (prophase I) stage of meiosis. Meiosis is the general process in eukaryotic organisms by which germ cells are formed, and it is likely an adaptation for removing DNA damages, especially double-strand breaks, from germ line DNA. (see Meiosis and Origin and function of meiosis). Homologous recombinational repair is especially promoted during meiosis. Titus et al. also found that expression of 4 key genes necessary for homologous recombinational repair of DNA double-strand breaks (BRCA1, MRE11, RAD51 and ATM) decline with age in the oocytes of humans and mice. They hypothesized that DNA double-strand break repair is vital for the maintenance of oocyte reserve and that a decline in efficiency of repair with age plays a key role in ovarian aging.

Ovarian diseases can be classified as endocrine disorders or as a disorders of the reproductive system.

If the egg fails to release from the follicle in the ovary an ovarian cyst may form. Small ovarian cysts are common in healthy women. Some women have more follicles than usual (polycystic ovary syndrome), which inhibits the follicles to grow normally and this will cause cycle irregularities.

Cryopreservation of ovarian tissue, often called "ovarian tissue cryopreservation", is of interest to women who want to preserve their reproductive function beyond the natural limit, or whose reproductive potential is threatened by cancer therapy, for example in hematologic malignancies or breast cancer. The procedure is to take a part of the ovary and carry out slow freezing before storing it in liquid nitrogen whilst therapy is undertaken. Tissue can then be thawed and implanted near the fallopian, either orthotopic (on the natural location) or heterotopic (on the abdominal wall), where it starts to produce new eggs, allowing normal conception to take place. A study of 60 procedures concluded that ovarian tissue harvesting appears to be safe. The ovarian tissue may also be transplanted into mice that are immunocompromised (SCID mice) to avoid graft rejection, and tissue can be harvested later when mature follicles have developed.

Birds have only one functional ovary (the left), while the other remains vestigial. Ovaries in females are analogous to testes in males, in that they are both gonads and endocrine glands. Ovaries of some kind are found in the female reproductive system of many animals that employ sexual reproduction, including invertebrates. However, they develop in a very different way in most invertebrates than they do in vertebrates, and are not truly homologous.

Many of the features found in human ovaries are common to all vertebrates, including the presence of follicular cells, tunica albuginea, and so on. However, many species produce a far greater number of eggs during their lifetime than do humans, so that, in fish and amphibians, there may be hundreds, or even millions of fertile eggs present in the ovary at any given time. In these species, fresh eggs may be developing from the germinal epithelium throughout life. Corpora lutea are found only in mammals, and in some elasmobranch fish; in other species, the remnants of the follicle are quickly resorbed by the ovary. In birds, reptiles, and monotremes, the egg is relatively large, filling the follicle, and distorting the shape of the ovary at maturity.

Amphibians and reptiles have no ovarian medulla; the central part of the ovary is a hollow, lymph-filled space.

The ovary of teleosts is also often hollow, but in this case, the eggs are shed into the cavity, which opens into the oviduct. Certain nematodes of the genus "Philometra" are parasitic in the ovary of marine fishes and can be spectacular, with females as long as 40 cm, coiled in the ovary of a fish half this length. Although most normal female vertebrates have two ovaries, this is not the case in all species. In most birds and in platypuses, the right ovary never matures, so that only the left is functional. (Exceptions include the kiwi and some, but not all raptors, in which both ovaries persist.) In some elasmobranchs, only the right ovary develops fully. In the primitive jawless fish, and some teleosts, there is only one ovary, formed by the fusion of the paired organs in the embryo.




</doc>
<doc id="22713" url="https://en.wikipedia.org/wiki?curid=22713" title="Opium">
Opium

Opium (poppy tears, with the scientific name: "Lachryma papaveris") is the dried latex obtained from the opium poppy (scientific name: "Papaver somniferum"). Approximately 12 percent of the opium latex is made up of the analgesic alkaloid morphine, which is processed chemically to produce heroin and other synthetic opioids for medicinal use and for illegal drug trade. The latex also contains the closely related opiates codeine and thebaine, and non-analgesic alkaloids such as papaverine and noscapine. The traditional, labor-intensive method of obtaining the latex is to scratch ("score") the immature seed pods (fruits) by hand; the latex leaks out and dries to a sticky yellowish residue that is later scraped off and dehydrated. The word "meconium" (derived from the Greek for "opium-like", but now used to refer to infant stools) historically referred to related, weaker preparations made from other parts of the opium poppy or different species of poppies.

The production methods have not changed since ancient times. Through selective breeding of the "Papaver somniferum" plant, the content of the phenanthrene alkaloids morphine, codeine, and to a lesser extent thebaine has been greatly increased. In modern times, much of the thebaine, which often serves as the raw material for the synthesis for hydrocodone, hydromorphone, and other semisynthetic opiates, originates from extracting "Papaver orientale" or "Papaver bracteatum".

For the illegal drug trade, the morphine is extracted from the opium latex, reducing the bulk weight by 88%. It is then converted to heroin which is two to four times as potent, and increases the value by a similar factor. The reduced weight and bulk make it easier to smuggle.

The Mediterranean region contains the earliest archeological evidence of human use; the oldest known seeds date back to more than 5000 BCE in the Neolithic age with purposes such as food, anaesthetics, and ritual. Evidence from ancient Greece indicates that opium was consumed in several ways, including inhalation of vapors, suppositories, medical poultices, and as a combination with hemlock for suicide. The Sumerian, Assyrian, Egyptian, Indian, Minoan, Greek, Roman, Persian and Arab Empires all made widespread use of opium, which was the most potent form of pain relief then available, allowing ancient surgeons to perform prolonged surgical procedures. Opium is mentioned in the most important medical texts of the ancient world, including the Ebers Papyrus and the writings of Dioscorides, Galen, and Avicenna. Widespread medical use of unprocessed opium continued through the American Civil War before giving way to morphine and its successors, which could be injected at a precisely controlled dosage.

Opium has been actively collected since prehistoric times, since approximately 3400 BCE.

A common name for males in Afghanistan is "Redey", which in Pashto means "poppy". This term may be derived from the Sanskrit words "rddhi" and "hrdya", which mean "magical", "a type of medicinal plant", and "heart-pleasing", respectively. The upper Asian belt of Afghanistan, Pakistan, northern India, and Burma still account for the world's largest supply of opium.

At least 17 finds of "Papaver somniferum" from Neolithic settlements have been reported throughout Switzerland, Germany, and Spain, including the placement of large numbers of poppy seed capsules at a burial site (the "Cueva de los Murciélagos", or "Bat Cave", in Spain), which have been carbon-14 dated to 4200 BCE. Numerous finds of "P. somniferum" or "P. setigerum" from Bronze Age and Iron Age settlements have also been reported.
The first known cultivation of opium poppies was in Mesopotamia, approximately 3400 BCE, by Sumerians, who called the plant "hul gil", the "joy plant". Tablets found at Nippur, a Sumerian spiritual center south of Baghdad, described the collection of poppy juice in the morning and its use in production of opium. Cultivation continued in the Middle East by the Assyrians, who also collected poppy juice in the morning after scoring the pods with an iron scoop; they called the juice "aratpa-pal", possibly the root of "Papaver". Opium production continued under the Babylonians and Egyptians.

Opium was used with poison hemlock to put people quickly and painlessly to death, but it was also used in medicine. "Spongia somnifera", sponges soaked in opium, were used during surgery. The Egyptians cultivated "opium thebaicum" in famous poppy fields around 1300 BCE. Opium was traded from Egypt by the Phoenicians and Minoans to destinations around the Mediterranean Sea, including Greece, Carthage, and Europe. By 1100 BCE, opium was cultivated on Cyprus, where surgical-quality knives were used to score the poppy pods, and opium was cultivated, traded, and smoked. Opium was also mentioned after the Persian conquest of Assyria and Babylonian lands in the 6th century BCE.

From the earliest finds, opium has appeared to have ritual significance, and anthropologists have speculated ancient priests may have used the drug as a proof of healing power. In Egypt, the use of opium was generally restricted to priests, magicians, and warriors, its invention is credited to Thoth, and it was said to have been given by Isis to Ra as treatment for a headache. A figure of the Minoan "goddess of the narcotics", wearing a crown of three opium poppies, BCE, was recovered from the Sanctuary of Gazi, Crete, together with a simple smoking apparatus. The Greek gods Hypnos (Sleep), Nyx (Night), and Thanatos (Death) were depicted wreathed in poppies or holding them. Poppies also frequently adorned statues of Apollo, Asklepios, Pluto, Demeter, Aphrodite, Kybele and Isis, symbolizing nocturnal oblivion.

As the power of the Roman Empire declined, the lands to the south and east of the Mediterranean Sea became incorporated into the Islamic Empires. Some Muslims believe "hadiths", such as in "Sahih Bukhari", prohibits every intoxicating substance, though the use of intoxicants in medicine has been widely permitted by scholars. Dioscorides' five-volume "De Materia Medica", the precursor of pharmacopoeias, remained in use (with some improvements in Arabic versions) from the 1st to 16th centuries, and described opium and the wide range of its uses prevalent in the ancient world.

Between 400 and 1200 CE, Arab traders introduced opium to China, and to India by 700. The Persian physician Muhammad ibn Zakariya al-Razi ("Rhazes", 845–930 CE) maintained a laboratory and school in Baghdad, and was a student and critic of Galen; he made use of opium in anesthesia and recommended its use for the treatment of melancholy in "Fi ma-la-yahdara al-tabib", "In the Absence of a Physician", a home medical manual directed toward ordinary citizens for self-treatment if a doctor was not available.

The renowned Andalusian ophthalmologic surgeon Abu al-Qasim al-Zahrawi ("Abulcasis", 936–1013 CE) relied on opium and mandrake as surgical anaesthetics and wrote a treatise, "al-Tasrif", that influenced medical thought well into the 16th century.

The Persian physician Abū ‘Alī al-Husayn ibn Sina ("Avicenna") described opium as the most powerful of the stupefacients, in comparison to mandrake and other highly effective herbs, in "The Canon of Medicine". The text lists medicinal effects of opium, such as analgesia, hypnosis, antitussive effects, gastrointestinal effects, cognitive effects, respiratory depression, neuromuscular disturbances, and sexual dysfunction. It also refers to opium's potential as a poison. Avicenna describes several methods of delivery and recommendations for doses of the drug. This classic text was translated into Latin in 1175 and later into many other languages and remained authoritative until the 19th century. Şerafeddin Sabuncuoğlu used opium in the 14th-century Ottoman Empire to treat migraine headaches, sciatica, and other painful ailments.

Manuscripts of Pseudo-Apuleius's 5th-century work from the 10th and 11th centuries refer to the use of wild poppy "Papaver agreste" or "Papaver rhoeas" (identified as "P. silvaticum") instead of "P. somniferum" for inducing sleep and relieving pain.

The use of Paracelsus' laudanum was introduced to Western medicine in 1527, when Philippus Aureolus Theophrastus Bombastus von Hohenheim, better known by the name Paracelsus, returned from his wanderings in Arabia with a famous sword, within the pommel of which he kept "Stones of Immortality" compounded from opium thebaicum, citrus juice, and "quintessence of gold". The name "Paracelsus" was a pseudonym signifying him the equal or better of Aulus Cornelius Celsus, whose text, which described the use of opium or a similar preparation, had recently been translated and reintroduced to medieval Europe. "The Canon of Medicine", the standard medical textbook Paracelsus burned in a public bonfire three weeks after being appointed professor at the University of Basel, also described the use of opium, though many Latin translations were of poor quality. "Laudanum" ("worthy of praise") was originally the 16th-century term for a medicine associated with a particular physician that was widely well-regarded, but became standardized as "tincture of opium", a solution of opium in ethanol, which Paracelsus has been credited with developing. During his lifetime, Paracelsus was viewed as an adventurer who challenged the theories and mercenary motives of contemporary medicine with dangerous chemical therapies, but his therapies marked a turning point in Western medicine. In the 1660s, laudanum was recommended for pain, sleeplessness, and diarrhea by Thomas Sydenham, the renowned "father of English medicine" or "English Hippocrates", to whom is attributed the quote, "Among the remedies which it has pleased Almighty God to give to man to relieve his sufferings, none is so universal and so efficacious as opium."
Use of opium as a cure-all was reflected in the formulation of mithridatium described in the 1728 "Chambers Cyclopedia", which included true opium in the mixture. Subsequently, laudanum became the basis of many popular patent medicines of the 19th century.

Compared to other chemicals available to 18th century regular physicians, opium was a benign alternative to the arsenics, mercuries, or emetics, and it was remarkably successful in alleviating a wide range of ailments. Due to the constipation often produced by the consumption of opium, it was one of the most effective treatments for cholera, dysentery, and diarrhea. As a cough suppressant, opium was used to treat bronchitis, tuberculosis, and other respiratory illnesses. Opium was additionally prescribed for rheumatism and insomnia. Medical textbooks even recommended its use by people in good health, to "optimize the internal equilibrium of the human body".

During the 18th century, opium was found to be a good remedy for nervous disorders. Due to its sedative and tranquilizing properties, it was used to quiet the minds of those with psychosis, help with people who were considered insane, and also to help treat patients with insomnia. However, despite its medicinal values in these cases, it was noted that in cases of psychosis, it could cause anger or depression, and due to the drug's euphoric effects, it could cause depressed patients to become more depressed after the effects wore off because they would get used to being high.

The standard medical use of opium persisted well into the 19th century. US president William Henry Harrison was treated with opium in 1841, and in the American Civil War, the Union Army used of opium tincture and powder and about 500,000 opium pills. During this time of popularity, users called opium "God's Own Medicine".

One reason for the increase in opiate consumption in the United States during the 19th century was the prescribing and dispensing of legal opiates by physicians and pharmacists to women with "female complaints" (mostly to relieve menstrual pain and hysteria). Because opiates were viewed as more humane than punishment or restraint, they were often used to treat the mentally ill. Between 150,000 and 200,000 opiate addicts lived in the United States in the late 19th century and between two-thirds and three-quarters of these addicts were women.

Opium addiction in the later 19th century received a hereditary definition. Dr. George Beard in 1869 proposed his theory of neurasthenia, a hereditary nervous system deficiency that could predispose an individual to addiction. Neurasthenia was increasingly tied in medical rhetoric to the "nervous exhaustion" suffered by many a white-collar worker in the increasingly hectic and industrialized U.S. life—the most likely potential clients of physicians.

Soldiers returning home from the Crusades in the eleventh century brought opium with them. Opium is said to have been used for recreational purposes from the 14th century onwards in Muslim societies. Ottoman and European testimonies confirm that from the 16th to the 19th centuries Anatolian opium was eaten in Constantinople as much as it was exported to Europe. In 1573, for instance, a Venetian visitor to the Ottoman Empire observed many of the Turkish natives of Constantinople regularly drank a "certain black water made with opium" that makes them feel good, but to which they become so addicted, if they try to go without, they will "quickly die". From drinking it, dervishes claimed the drugs bestowed them with visionary glimpses of future happiness. Indeed, Turkey supplied the West with opium long before China and India.

Thomas de Quincey's "Confessions of an English Opium-Eater" (1822), one of the first and most famous literary accounts of opium addiction written from the point of view of an addict, details the pleasures and dangers of the drug. In the book, it is not Ottoman, nor Chinese, addicts about whom he writes, but English opium users: "I question whether any Turk, of all that ever entered the paradise of opium-eaters, can have had half the pleasure I had." De Quincey writes about the great English Romantic poet, Samuel Taylor Coleridge (1772–1834), whose "Kubla Khan" is also widely considered to be a poem of the opium experience. Coleridge began using opium in 1791 after developing jaundice and rheumatic fever, and became a full addict after a severe attack of the disease in 1801, requiring 80–100 drops of laudanum daily.

Extensive textual and pictorial sources also show that poppy cultivation and opium consumption were widespread in Safavid Iran and Mughal India.

In China, recreational use began in the 15th century, but was limited by its rarity and expense. Opium trade became more regular by the 17th century, when it was mixed with tobacco for smoking, and addiction was first recognized. Prior to the arrival of the tobacco pipe, opium was only taken orally; when smoked, the drug has a far more potent effect, and its addictive effect is greatly magnified. Opium prohibition in China began in 1729, yet was followed by nearly two centuries of increasing opium use. China had a positive balance sheet in trading with the British, which led to a decrease of the British silver stocks. Therefore, the British tried to encourage Chinese opium use to enhance their balance, and they delivered it from Indian provinces under British control. In India, its cultivation, as well as the manufacture and traffic to China, were subject to the British East India Company (BEIC), as a strict monopoly of the British government. Indian farmers were forced by the British East India company to grow poppy against their wishes, often using a combination of strong arm tactics and debt. There was an extensive and complicated system of BEIC agencies involved in the supervision and management of opium production and distribution in India. A massive destruction of opium by an emissary of the Chinese Daoguang Emperor in an attempt to stop opium imports led to the First Opium War (18391842), in which Britain defeated China. After 1860, opium use continued to increase with widespread domestic production in China. By 1905, an estimated 25 percent of the male population were regular consumers of the drug. Recreational use of opium elsewhere in the world remained rare into late in the 19th century, as indicated by ambivalent reports of opium usage.

Global regulation of opium began with the stigmatization of Chinese immigrants and opium dens in San Francisco, California. This led rapidly to town ordinances in the 1870s and the formation of the International Opium Commission in 1909. During this period, the portrayal of opium in literature became squalid and violent. British opium trade was largely supplanted by domestic Chinese production. Purified morphine and heroin became widely available for injection and patent medicines containing opiates reached a peak in recreational use. Opium was prohibited in many countries during the early 20th century, leading to the modern pattern of opium production as a precursor for illegal recreational drugs or tightly regulated legal prescription drugs. Illicit opium production, now dominated by Afghanistan, was decimated in 2000, when production was banned by the Taliban, but has increased steadily since the fall of the Taliban and western occupation in 2001 and over the course of the war in Afghanistan. Worldwide production in 2006 was 6610 metric tons—about one-fifth the level of production in 1906.

The earliest clear description of the use of opium as a recreational drug in China came from Xu Boling, who wrote in 1483 that opium was "mainly used to aid masculinity, strengthen sperm and regain vigor", and that it "enhances the art of alchemists, sex and court ladies". He also described an expedition sent by the Ming dynasty Chenghua Emperor in 1483 to procure opium for a price "equal to that of gold" in Hainan, Fujian, Zhejiang, Sichuan and Shaanxi, where it is close to the western lands of Xiyu. A century later, Li Shizhen listed standard medical uses of opium in his renowned "Compendium of Materia Medica" (1578), but also wrote that "lay people use it for the art of sex," in particular the ability to "arrest seminal emission". This association of opium with sex continued in China until the end of the 19th century.

Opium smoking began as a privilege of the elite and remained a great luxury into the early 19th century. However, by 1861, Wang Tao wrote that opium was used even by rich peasants, and even a small village without a rice store would have a shop where opium was sold.

Smoking of opium came on the heels of tobacco smoking and may have been encouraged by a brief ban on the smoking of tobacco by the Ming emperor. The prohibition ended in 1644 with the coming of the Qing dynasty, which encouraged smokers to mix in increasing amounts of opium. In 1705, Wang Shizhen wrote, "nowadays, from nobility and gentlemen down to slaves and women, all are addicted to tobacco." Tobacco in that time was frequently mixed with other herbs (this continues with clove cigarettes to the modern day), and opium was one component in the mixture. Tobacco mixed with opium was called "madak" (or "madat") and became popular throughout China and its seafaring trade partners (such as Taiwan, Java, and the Philippines) in the 17th century. In 1712, Engelbert Kaempfer described addiction to "madak": "No commodity throughout the Indies is retailed with greater profit by the Batavians than opium, which [its] users cannot do without, nor can they come by it except it be brought by the ships of the Batavians from Bengal and Coromandel."

Fueled in part by the 1729 ban on "madak", which at first effectively exempted pure opium as a potentially medicinal product, the smoking of pure opium became more popular in the 18th century. In 1736, the smoking of pure opium was described by Huang Shujing, involving a pipe made from bamboo rimmed with silver, stuffed with palm slices and hair, fed by a clay bowl in which a globule of molten opium was held over the flame of an oil lamp. This elaborate procedure, requiring the maintenance of pots of opium at just the right temperature for a globule to be scooped up with a needle-like skewer for smoking, formed the basis of a craft of "paste-scooping" by which servant girls could become prostitutes as the opportunity arose.

The Chinese Diaspora (1800s to 1949) first began during the 19th-century due to famine and political upheaval, as well as rumors of wealth to be had outside of Southeast Asia. Chinese emigrants to cities such as San Francisco, London, and New York brought with them the Chinese manner of opium smoking, and the social traditions of the opium den. The Indian Diaspora distributed opium-eaters in the same way, and both social groups survived as "lascars" (seamen) and "coolies" (manual laborers). French sailors provided another major group of opium smokers, having gotten the habit while in French Indochina, where the drug was promoted and monopolized by the colonial government as a source of revenue. Among white Europeans, opium was more frequently consumed as laudanum or in patent medicines. Britain's All-India Opium Act of 1878 formalized ethnic restrictions on the use of opium, limiting recreational opium sales to registered Indian opium-eaters and Chinese opium-smokers only and prohibiting its sale to workers from Burma. Likewise, American law sought to contain addiction to immigrants by prohibiting Chinese from smoking opium in the presence of a white man.

Because of the low social status of immigrant workers, contemporary writers and media had little trouble portraying opium dens as seats of vice, white slavery, gambling, knife- and revolver-fights, and a source for drugs causing deadly overdoses, with the potential to addict and corrupt the white population. By 1919, anti-Chinese riots attacked Limehouse, the Chinatown of London. Chinese men were deported for playing keno and sentenced to hard labor for opium possession. Due to this, both the immigrant population and the social use of opium fell into decline. Yet despite lurid literary accounts to the contrary, 19th-century London was not a hotbed of opium smoking. The total lack of photographic evidence of opium smoking in Britain, as opposed to the relative abundance of historical photos depicting opium smoking in North America and France, indicates the infamous Limehouse opium-smoking scene was little more than fantasy on the part of British writers of the day, who were intent on scandalizing their readers while drumming up the threat of the "yellow peril".

Opium prohibition began in 1729, when the Qing Yongzheng Emperor, disturbed by "madak" smoking at court and carrying out the government's role of upholding Confucian virtues, officially prohibited the sale of opium, except for a small amount for medicinal purposes. The ban punished sellers and opium den keepers, but not users of the drug. Opium was banned completely in 1799, and this prohibition continued until 1860.

During the Qing dynasty, China opened itself to foreign trade under the Canton System through the port of Guangzhou (Canton), with traders from the East India Company visiting the port by the 1690s. Due to the growing British demand for Chinese tea and the Chinese Emperor's lack of interest in British commodities other than silver, British traders resorted to trade in opium as a high-value commodity for which China was not self-sufficient. The English traders had been purchasing small amounts of opium from India for trade since Ralph Fitch first visited in the mid-16th century. Trade in opium was standardized, with production of balls of raw opium, , 30% water content, wrapped in poppy leaves and petals, and shipped in chests of (one picul).
Chests of opium were sold in auctions in Calcutta with the understanding that the independent purchasers would then smuggle it into China.

After the 1757 Battle of Plassey and 1764 Battle of Buxar, the British East India Company gained the power to act as "diwan" of Bengal, Bihar, and Odisha "(See company rule in India)". This allowed the company to exercise a monopoly over opium production and export in India, to encourage ryots to cultivate the cash crops of indigo and opium with cash advances, and to prohibit the "hoarding" of rice. This strategy led to the increase of the land tax to 50 percent of the value of crops and to the doubling of East India Company profits by 1777. It is also claimed to have contributed to the starvation of 10 million people in the Bengal famine of 1770. Beginning in 1773, the British government began enacting oversight of the company's operations, and in response to the Indian Rebellion of 1857, this policy culminated in the establishment of direct rule over the presidencies and provinces of British India. Bengal opium was highly prized, commanding twice the price of the domestic Chinese product, which was regarded as inferior in quality.

Some competition came from the newly independent United States, which began to compete in Guangzhou, selling Turkish opium in the 1820s. Portuguese traders also brought opium from the independent Malwa states of western India, although by 1820, the British were able to restrict this trade by charging "pass duty" on the opium when it was forced to pass through Bombay to reach an "entrepot".
Despite drastic penalties and continued prohibition of opium until 1860, opium importation rose steadily from 200 chests per year under the Yongzheng Emperor to 1,000 under the Qianlong Emperor, 4,000 under the Jiaqing Emperor, and 30,000 under the Daoguang Emperor. The illegal sale of opium became one of the world's most valuable single commodity trades and has been called "the most long continued and systematic international crime of modern times". Opium smuggling provided 15 to 20 percent of the British Empire's revenue and simultaneously caused scarcity of silver in China.

In response to the ever-growing number of Chinese people becoming addicted to opium, the Qing Daoguang Emperor took strong action to halt the import of opium, including the seizure of cargo. In 1838, the Chinese Commissioner Lin Zexu destroyed 20,000 chests of opium in Guangzhou. Given that a chest of opium was worth nearly in 1800, this was a substantial economic loss. The British queen Victoria, not willing to replace the cheap opium with costly silver, began the First Opium War in 1840, the British winning Hong Kong and trade concessions in the first of a series of Unequal Treaties.

The opium trade incurred intense enmity from the later British Prime Minister William Ewart Gladstone. As a member of Parliament, Gladstone called it "most infamous and atrocious" referring to the opium trade between China and British India in particular. Gladstone was fiercely against both of the Opium Wars Britain waged in China in the First Opium War initiated in 1840 and the Second Opium War initiated in 1857, denounced British violence against Chinese, and was ardently opposed to the British trade in opium to China. Gladstone lambasted it as "Palmerston's Opium War" and said that he felt "in dread of the judgments of God upon England for our national iniquity towards China" in May 1840. A famous speech was made by Gladstone in Parliament against the First Opium War. Gladstone criticized it as "a war more unjust in its origin, a war more calculated in its progress to cover this country with permanent disgrace". His hostility to opium stemmed from the effects of opium brought upon his sister Helen. Due to the First Opium war brought on by Palmerston, there was initial reluctance to join the government of Peel on part of Gladstone before 1841.

Following China's defeat in the Second Opium War in 1858, China was forced to legalize opium and began massive domestic production. Importation of opium peaked in 1879 at 6,700 tons, and by 1906, China was producing 85 percent of the world's opium, some 35,000 tons, and 27 percent of its adult male population regularly used opium13.5million people consuming 39,000 tons of opium yearly. From 1880 to the beginning of the Communist era, the British attempted to discourage the use of opium in China, but this effectively promoted the use of morphine, heroin, and cocaine, further exacerbating the problem of addiction.

Scientific evidence of the pernicious nature of opium use was largely undocumented in the 1890s, when Protestant missionaries in China decided to strengthen their opposition to the trade by compiling data which would demonstrate the harm the drug did. Faced with the problem that many Chinese associated Christianity with opium, partly due to the arrival of early Protestant missionaries on opium clippers, at the 1890 Shanghai Missionary Conference, they agreed to establish the Permanent Committee for the Promotion of Anti-Opium Societies in an attempt to overcome this problem and to arouse public opinion against the opium trade. The members of the committee were John Glasgow Kerr, MD, American Presbyterian Mission in Canton; B.C. Atterbury, MD, American Presbyterian Mission in Peking; Archdeacon Arthur E. Moule, Church Missionary Society in Shanghai; Henry Whitney, MD, American Board of Commissioners for foreign Missions in Foochow; the Rev. Samuel Clarke, China Inland Mission in Kweiyang; the Rev. Arthur Gostick Shorrock, English Baptist Mission in Taiyuan; and the Rev. Griffith John, London Mission Society in Hankow. These missionaries were generally outraged over the British government's Royal Commission on Opium visiting India but not China. Accordingly, the missionaries first organized the Anti-Opium League in China among their colleagues in every mission station in China. American missionary Hampden Coit DuBose acted as first president. This organization, which had elected national officers and held an annual national meeting, was instrumental in gathering data from every Western-trained medical doctor in China, which was then published as William Hector Park compiled "Opinions of Over 100 Physicians on the Use of Opium in China" (Shanghai: American Presbyterian Mission Press, 1899). The vast majority of these medical doctors were missionaries; the survey also included doctors who were in private practices, particularly in Shanghai and Hong Kong, as well as Chinese who had been trained in medical schools in Western countries. In England, the home director of the China Inland Mission, Benjamin Broomhall, was an active opponent of the opium trade, writing two books to promote the banning of opium smoking: "The Truth about Opium Smoking" and "The Chinese Opium Smoker". In 1888, Broomhall formed and became secretary of the Christian Union for the Severance of the British Empire with the Opium Traffic and editor of its periodical, "National Righteousness". He lobbied the British Parliament to stop the opium trade. He and James Laidlaw Maxwell appealed to the London Missionary Conference of 1888 and the Edinburgh Missionary Conference of 1910 to condemn the continuation of the trade. When Broomhall was dying, his son Marshall read to him from "The Times" the welcome news that an agreement had been signed ensuring the end of the opium trade within two years.

Official Chinese resistance to opium was renewed on September 20, 1906, with an antiopium initiative intended to eliminate the drug problem within 10 years. The program relied on the turning of public sentiment against opium, with mass meetings at which opium paraphernalia were publicly burned, as well as coercive legal action and the granting of police powers to organizations such as the Fujian Anti-Opium Society. Smokers were required to register for licenses for gradually reducing rations of the drug. Action against opium farmers centred upon a highly repressive incarnation of law enforcement in which rural populations had their property destroyed, their land confiscated and/or were publicly tortured, humiliated and executed. Addicts sometimes turned to missionaries for treatment for their addiction, though many associated these foreigners with the drug trade. The program was counted as a substantial success, with a cessation of direct British opium exports to China (but not Hong Kong) and most provinces declared free of opium production. Nonetheless, the success of the program was only temporary, with opium use rapidly increasing during the disorder following the death of Yuan Shikai in 1916. Opium farming also increased, peaking in 1930 when the League of Nations singled China out as the primary source of illicit opium in East and Southeast Asia. Many local powerholders facilitated the trade during this period to finance conflicts over territory and political campaigns. In some areas food crops were eradicated to make way for opium, contributing to famines in Kweichow and Shensi Provinces between 1921 and 1923, and food deficits in other provinces.

Beginning in 1915, Chinese nationalist groups came to describe the period of military losses and Unequal Treaties as the "Century of National Humiliation", later defined to end with the conclusion of the Chinese Civil War in 1949.

In the northern provinces of Ningxia and Suiyuan in China, Chinese Muslim General Ma Fuxiang both prohibited and engaged in the opium trade. It was hoped that Ma Fuxiang would have improved the situation, since Chinese Muslims were well known for opposition to smoking opium. Ma Fuxiang officially prohibited opium and made it illegal in Ningxia, but the Guominjun reversed his policy; by 1933, people from every level of society were abusing the drug, and Ningxia was left in destitution. In 1923, an officer of the Bank of China from Baotou found out that Ma Fuxiang was assisting the drug trade in opium which helped finance his military expenses. He earned from taxing those sales in 1923. General Ma had been using the bank, a branch of the Government of China's exchequer, to arrange for silver currency to be transported to Baotou to use it to sponsor the trade.

The opium trade under the Chinese Communist Party was important to its finances in the 1940s. Peter Vladimirov's diary provided a first hand account. Chen Yung-Fa provided a detailed historical account of how the opium trade was essential to the economy of Yan'an during this period. Mitsubishi and Mitsui were involved in the opium trade during the Japanese occupation of China.

The Mao Zedong government is generally credited with eradicating both consumption and production of opium during the 1950s using unrestrained repression and social reform. Ten million addicts were forced into compulsory treatment, dealers were executed, and opium-producing regions were planted with new crops. Remaining opium production shifted south of the Chinese border into the Golden Triangle region. The remnant opium trade primarily served Southeast Asia, but spread to American soldiers during the Vietnam War, with 20 percent of soldiers regarding themselves as addicted during the peak of the epidemic in 1971. In 2003, China was estimated to have four million regular drug users and one million registered drug addicts.

There were no legal restrictions on the importation or use of opium in the United States until the San Francisco Opium Den Ordinance, which banned dens for public smoking of opium in 1875, a measure fueled by anti-Chinese sentiment and the perception that whites were starting to frequent the dens. This was followed by an 1891 California law requiring that narcotics carry warning labels and that their sales be recorded in a registry; amendments to the California Pharmacy and Poison Act in 1907 made it a crime to sell opiates without a prescription, and bans on possession of opium or opium pipes in 1909 were enacted.

At the US federal level, the legal actions taken reflected constitutional restrictions under the enumerated powers doctrine prior to reinterpretation of the commerce clause, which did not allow the federal government to enact arbitrary prohibitions, but did permit arbitrary taxation. Beginning in 1883, opium importation was taxed at to per pound, until the Opium Exclusion Act of 1909 prohibited the importation of opium altogether. In a similar manner, the Harrison Narcotics Tax Act of 1914, passed in fulfillment of the International Opium Convention of 1912, nominally placed a tax on the distribution of opiates, but served as a "de facto" prohibition of the drugs. Today, opium is regulated by the Drug Enforcement Administration under the Controlled Substances Act.

Following passage of a Colonial Australian law in 1895, Queensland's Aboriginals Protection and Restriction of the Sale of Opium Act 1897 addressed opium addiction among Aboriginal people, though it soon became a general vehicle for depriving them of basic rights by administrative regulation. By 1905 all Australian states and territories had passed similar laws making prohibitions to Opium sale. Smoking and possession was prohibited in 1908.

Hardening of Canadian attitudes toward Chinese opium users and fear of a spread of the drug into the white population led to the effective criminalization of opium for nonmedical use in Canada between 1908 and the mid-1920s.

In 1909, the International Opium Commission was founded, and by 1914, 34 nations had agreed that the production and importation of opium should be diminished. In 1924, 62 nations participated in a meeting of the Commission. Subsequently, this role passed to the League of Nations, and all signatory nations agreed to prohibit the import, sale, distribution, export, and use of all narcotic drugs, except for medical and scientific purposes. This role was later taken up by the International Narcotics Control Board of the United Nations under of the Single Convention on Narcotic Drugs, and subsequently under the Convention on Psychotropic Substances. Opium-producing nations are required to designate a government agency to take physical possession of licit opium crops as soon as possible after harvest and conduct all wholesaling and exporting through that agency.

Before the 1920s, regulation in Britain was controlled by the pharmacists. Pharmacists that were found to have prescribed opium for illegitimate causes and anyone found to have sold opium without proper qualifications would be prosecuted. Due to the passing of the Rolleston Act in Britain in 1926, doctors could prescribe opiates such as morphine and heroin on their own accord if they felt their patients demonstrated a medical need. Because addiction was viewed as a medical problem rather than an indulgence, doctors were permitted to allow patients to wean themselves off opiates rather than cutting off any opiate use altogether. The passing of the Rolleston Act put the control of opium use in the hands of medical doctors instead of pharmacists. Later in the 20th century, the addiction to opiates, especially heroin in young people, continued to rise and so the sale and prescription of opiates was limited to doctors in treatment centers. If these doctors were found to be prescribing opiates without just cause, then they could lose their license to practice or prescribe drugs.
The abuse of opium in the United States began in the late 19th century and was largely stigmatized with Chinese immigrants. During this time the use of opium had little negative connotation and was used freely until 1882 when a law was passed to confine opium smoking to specific dens. Until the full ban on opium based products came into effect just after the turn of the century, physicians in the US considered opium a miracle drug that could help with many ailments. Therefore, the ban on said products was more a result of negative connotations towards its use and distribution by Chinese immigrants who were heavily persecuted during this particular period in history. As the 19th century progressed however, there was a doctor by the name of Hamilton Wright that worked to decrease the use of opium in the US by submitting the Harrison Act to congress. This act put taxes and restrictions on the sale and prescription of opium, as well as trying to stigmatize the opium poppy and its derivatives as "demon drugs", to try and scare people away from them. This act and the stigma of a demon drug on opium, led to the criminalization of people that used opium-based products. It made the use and possession of opium and any of its derivatives illegal. The restrictions were recently redefined by the Federal Controlled Substances Act of 1970.

During the Communist era in Eastern Europe, poppy stalks sold in bundles by farmers were processed by users with household chemicals to make "kompot" ("Polish heroin"), and poppy seeds were used to produce "koknar", an opiate.

Globally, opium has gradually been superseded by a variety of purified, semi-synthetic, and synthetic opioids with progressively stronger effects, and by other general anesthetics. This process began in 1804, when Friedrich Wilhelm Adam Sertürner first isolated morphine from the opium poppy. The process continued until 1817, when Sertürner published the isolation of pure morphine from opium after at least thirteen years of research and a nearly disastrous trial on himself and three boys. The great advantage of purified morphine was that a patient could be treated with a known dose—whereas with raw plant material, as Gabriel Fallopius once lamented, "if soporifics are weak they do not help; if they are strong they are exceedingly dangerous." Morphine was the first pharmaceutical isolated from a natural product, and this success encouraged the isolation of other alkaloids: by 1820, isolations of noscapine, strychnine, veratrine, colchicine, caffeine, and quinine were reported. Morphine sales began in 1827, by Heinrich Emanuel Merck of Darmstadt, and helped him expand his family pharmacy into the Merck KGaA pharmaceutical company.

Codeine was isolated in 1832 by Pierre Jean Robiquet.

The use of diethyl ether and chloroform for general anesthesia began in 1846–1847, and rapidly displaced the use of opiates and tropane alkaloids from Solanaceae due to their relative safety.

Heroin, the first semi-synthetic opioid, was first synthesized in 1874, but was not pursued until its rediscovery in 1897 by Felix Hoffmann at the Bayer pharmaceutical company in Elberfeld, Germany. From 1898 to 1910 heroin was marketed as a non-addictive morphine substitute and cough medicine for children. Because the lethal dose of heroin was viewed as a hundred times greater than its effective dose, heroin was advertised as a safer alternative to other opioids. By 1902, sales made up 5 percent of the company's profits, and "heroinism" had attracted media attention. Oxycodone, a thebaine derivative similar to codeine, was introduced by Bayer in 1916 and promoted as a less-addictive analgesic. Preparations of the drug such as oxycodone with paracetamol and extended release oxycodone remain popular to this day.

A range of synthetic opioids such as methadone (1937), pethidine (1939), fentanyl (late 1950s), and derivatives thereof have been introduced, and each is preferred for certain specialized applications. Nonetheless, morphine remains the drug of choice for American combat medics, who carry packs of syrettes containing 16 milligrams each for use on severely wounded soldiers. No drug has been found that can match the painkilling effect of opioids without also duplicating much of their addictive potential.

Opium poppies are popular and attractive garden plants, whose flowers vary greatly in color, size and form. A modest amount of domestic cultivation in private gardens is not usually subject to legal controls. In part, this tolerance reflects variation in addictive potency. A cultivar for opium production, "Papaver somniferum L. elite", contains 91.2 percent morphine, codeine, and thebaine in its latex alkaloids, whereas in the latex of the condiment cultivar "Marianne", these three alkaloids total only 14.0 percent. The remaining alkaloids in the latter cultivar are primarily narcotoline and noscapine.

Seed capsules can be dried and used for decorations, but they also contain morphine, codeine, and other alkaloids. These pods can be boiled in water to produce a bitter tea that induces a long-lasting intoxication "(See Poppy tea)". If allowed to mature, poppy pods (poppy straw) can be crushed and used to produce lower quantities of morphinans. In poppies subjected to mutagenesis and selection on a mass scale, researchers have been able to use poppy straw to obtain large quantities of oripavine, a precursor to opioids and antagonists such as naltrexone. Although millennia older, the production of poppy head decoctions can be seen as a quick-and-dirty variant of the Kábáy poppy straw process, which since its publication in 1930 has become the major method of obtaining licit opium alkaloids worldwide, as discussed in Morphine.

Poppy seeds are a common and flavorsome topping for breads and cakes. One gram of poppy seeds contains up to 33 micrograms of morphine and 14 micrograms of codeine, and the Substance Abuse and Mental Health Services Administration in the United States formerly mandated that all drug screening laboratories use a standard cutoff of 300 nanograms per milliliter in urine samples. A single poppy seed roll (0.76 grams of seeds) usually did not produce a positive drug test, but a positive result was observed from eating two rolls. A slice of poppy seed cake containing nearly five grams of seeds per slice produced positive results for 24 hours. Such results are viewed as false positive indications of drug use and were the basis of a legal defense. On November 30, 1998, the standard cutoff was increased to 2000 nanograms (two micrograms) per milliliter. Confirmation by gas chromatography-mass spectrometry will distinguish amongst opium and variants including poppy seeds, heroin, and morphine and codeine pharmaceuticals by measuring the morphine:codeine ratio and looking for the presence of noscapine and acetylcodeine, the latter of which is only found in illicitly produced heroin, and heroin metabolites such as 6-monoacetylmorphine.

When grown for opium production, the skin of the ripening pods of these poppies is scored by a sharp blade at a time carefully chosen so that rain, wind, and dew cannot spoil the exudation of white, milky latex, usually in the afternoon. Incisions are made while the pods are still raw, with no more than a slight yellow tint, and must be shallow to avoid penetrating hollow inner chambers or "loculi" while cutting into the lactiferous vessels. In Indian Subcontinent, Afghanistan, Central Asia and Iran, the special tool used to make the incisions is called a "nushtar" or "nishtar" (from Persian, meaning a lancet) and carries three or four blades three millimeters apart, which are scored upward along the pod. Incisions are made three or four times at intervals of two to three days, and each time the "poppy tears", which dry to a sticky brown resin, are collected the following morning. One acre harvested in this way can produce three to five kilograms of raw opium. In the Soviet Union, pods were typically scored horizontally, and opium was collected three times, or else one or two collections were followed by isolation of opiates from the ripe capsules. Oil poppies, an alternative strain of "P. somniferum", were also used for production of opiates from their capsules and stems. A traditional Chinese method of harvesting opium latex involved cutting off the heads and piercing them with a coarse needle then collecting the dried opium 24 to 48 hours later.

Raw opium may be sold to a merchant or broker on the black market, but it usually does not travel far from the field before it is refined into morphine base, because pungent, jelly-like raw opium is bulkier and harder to smuggle. Crude laboratories in the field are capable of refining opium into morphine base by a simple acid-base extraction. A sticky, brown paste, morphine base is pressed into bricks and sun-dried, and can either be smoked, prepared into other forms or processed into heroin.

Other methods of preparation (besides smoking), include processing into regular opium tincture ("tinctura opii"), laudanum, paregoric ("tinctura opii camphorata"), herbal wine (e.g., "vinum opii"), opium powder ("pulvis opii"), opium sirup ("sirupus opii") and opium extract ("extractum opii"). Vinum opii is made by combining sugar, white wine, cinnamon, and cloves. Opium syrup is made by combining 97.5 part sugar syrup with 2.5 parts opium extract. Opium extract ("extractum opii") finally can be made by macerating raw opium with water. To make opium extract, 20 parts water are combined with 1 part raw opium which has been boiled for 5 minutes (the latter to ease mixing).

Heroin is widely preferred because of increased potency. One study in postaddicts found heroin to be approximately 2.2 times more potent than morphine by weight with a similar duration; at these relative quantities, they could distinguish the drugs subjectively but had no preference. Heroin was also found to be twice as potent as morphine in surgical anesthesia. Morphine is converted into heroin by a simple chemical reaction with acetic anhydride, followed by purification. Especially in Mexican production, opium may be converted directly to "black tar heroin" in a simplified procedure. This form predominates in the U.S. west of the Mississippi. Relative to other preparations of heroin, it has been associated with a dramatically decreased rate of HIV transmission among intravenous drug users (4 percent in Los Angeles vs. 40 percent in New York) due to technical requirements of injection, although it is also associated with greater risk of venous sclerosis and necrotizing fasciitis.

Opium production has fallen since 1906, when 41,000 tons were produced, but because 39,000 tons of that year's opium were consumed in China, overall usage in the rest of the world was much lower. These figures from 1906 have been criticized as overestimates. In 1980, 2,000 tons of opium supplied all legal and illegal uses. Recently, opium production has increased considerably, surpassing 5,000 tons in 2002 and reaching 8,600 tons in Afghanistan and 840 tons in the Golden Triangle in 2014. Production is expected to increase in 2015 as new, improved seeds have been brought into Afghanistan. The World Health Organization has estimated that current production of opium would need to increase fivefold to account for total global medical need.

In 2002, the price for one kilogram of opium was for the farmer, for purchasers in Afghanistan, and on the streets of Europe before conversion into heroin.

Afghanistan is currently the primary producer of the drug. After regularly producing 70 percent of the world's opium, Afghanistan decreased production to 74 tons per year under a ban by the Taliban in 2000, a move which cut production by 94 percent. A year later, after American and British troops invaded Afghanistan, removed the Taliban and installed the interim government, the land under cultivation leapt back to , with Afghanistan supplanting Burma to become the world's largest opium producer once more. Opium production in that country has increased rapidly since, reaching an all-time high in 2006. According to DEA statistics, Afghanistan's production of oven-dried opium increased to 1,278 tons in 2002, more than doubled by 2003, and nearly doubled again during 2004. In late 2004, the U.S. government estimated that 206,000 hectares were under poppy cultivation, 4.5 percent of the country's total cropland, and produced 4,200 metric tons of opium, 76 percent of the world's supply, yielding 60 percent of Afghanistan's gross domestic product. In 2006, the UN Office on Drugs and Crime estimated production to have risen 59 percent to in cultivation, yielding 6,100 tons of opium, 82 percent of the world's supply. The value of the resulting heroin was estimated at , of which Afghan farmers were estimated to have received in revenue. For farmers, the crop can be up to ten times more profitable than wheat. The price of opium is around per kilo. Opium production has led to rising tensions in Afghan villages. Though direct conflict has yet to occur, the opinions of the new class of young rich men involved in the opium trade are at odds with those of the traditional village leaders.

An increasingly large fraction of opium is processed into morphine base and heroin in drug labs in Afghanistan. Despite an international set of chemical controls designed to restrict availability of acetic anhydride, it enters the country, perhaps through its Central Asian neighbors which do not participate. A counternarcotics law passed in December 2005 requires Afghanistan to develop registries or regulations for tracking, storing, and owning acetic anhydride.

Besides Afghanistan, smaller quantities of opium are produced in Pakistan, the Golden Triangle region of Southeast Asia (particularly Burma), Colombia, Guatemala, and Mexico.

Chinese production mainly trades with and profits from North America. In 2002, they were seeking to expand through eastern United States. In the post 9/11 era, trading between borders became difficult and because new international laws were set into place, the opium trade became more diffused. Power shifted from remote to high-end smugglers and opium traders. Outsourcing became a huge factor for survival for many smugglers and opium farmers.

In South American countries, opium poppies are technically illegal, but nonetheless appear in some nurseries as ornamentals.

Legal opium production is allowed under the United Nations Single Convention on Narcotic Drugs and other international drug treaties, subject to strict supervision by the law enforcement agencies of individual countries. The leading legal production method is the Gregory process, whereby the entire poppy, excluding roots and leaves, is mashed and stewed in dilute acid solutions. The alkaloids are then recovered via acid-base extraction and purified. This process was developed in the UK during World War II, when wartime shortages of many essential drugs encouraged innovation in pharmaceutical processing.

Legal opium production in India is much more traditional. As of 2008, opium was collected by farmers who were licensed to grow of opium poppies, who to maintain their licences needed to sell 56 kilograms of unadulterated raw opium paste. The price of opium paste is fixed by the government according to the quality and quantity tendered. The average is around 1500 rupees () per kilogram. Some additional money is made by drying the poppy heads and collecting poppy seeds, and a small fraction of opium beyond the quota may be consumed locally or diverted to the black market. The opium paste is dried and processed into government opium and alkaloid factories before it is packed into cases of 60 kilograms for export. Purification of chemical constituents is done in India for domestic production, but typically done abroad by foreign importers.

Legal opium importation from India and Turkey is conducted by Mallinckrodt, Noramco, Abbott Laboratories, Purdue Pharma, and Cody Laboratories Inc. in the United States, and legal opium production is conducted by GlaxoSmithKline, Johnson & Johnson, Johnson Matthey, and Mayne in Tasmania, Australia; Sanofi Aventis in France; Shionogi Pharmaceutical in Japan; and MacFarlan Smith in the United Kingdom. The UN treaty requires that every country submit annual reports to the International Narcotics Control Board, stating that year's actual consumption of many classes of controlled drugs as well as opioids and projecting required quantities for the next year. This is to allow trends in consumption to be monitored and production quotas allotted.

A recent proposal from the European Senlis Council hopes to solve the problems caused by the large quantity of opium produced illegally in Afghanistan, most of which is converted to heroin and smuggled for sale in Europe and the United States. This proposal is to license Afghan farmers to produce opium for the world pharmaceutical market, and thereby solve another problem, that of chronic underuse of potent analgesics where required within developing nations. Part of the proposal is to overcome the "80–20 rule" that requires the U.S. to purchase 80 percent of its legal opium from India and Turkey to include Afghanistan, by establishing a second-tier system of supply control that complements the current INCB regulated supply and demand system by providing poppy-based medicines to countries who cannot meet their demand under the current regulations. Senlis arranged a conference in Kabul that brought drug policy experts from around the world to meet with Afghan government officials to discuss internal security, corruption issues, and legal issues within Afghanistan.
In June 2007, the Council launched a "Poppy for Medicines" project that provides a technical blueprint for the implementation of an integrated control system within Afghan village-based poppy for medicine projects: the idea promotes the economic diversification by redirecting proceeds from the legal cultivation of poppy and production of poppy-based medicines (See Senlis Council). There has been criticism of the Senlis report findings by Macfarlan Smith, who argue that though they produce morphine in Europe, they were never asked to contribute to the report.

In late 2006, the British government permitted the pharmaceutical company MacFarlan Smith (a Johnson Matthey company) to cultivate opium poppies in England for medicinal reasons, after Macfarlan Smith's primary source, India, decided to increase the price of export opium latex. This move is well received by British farmers, with a major opium poppy field located in Didcot, England. The British government has contradicted the Home Office's suggestion that opium cultivation can be legalized in Afghanistan for exports to the United Kingdom, helping lower poverty and internal fighting while helping the NHS to meet the high demand for morphine and heroin. Opium poppy cultivation in the United Kingdom does not need a licence, but a licence is required for those wishing to extract opium for medicinal products.

In the industrialized world, the United States is the world's biggest consumer of prescription opioids, with Italy one of the lowest because of tighter regulations on prescribing narcotics for pain relief. Most opium imported into the United States is broken down into its alkaloid constituents, and whether legal or illegal, most current drug use occurs with processed derivatives such as heroin rather than with unrefined opium.

Intravenous injection of opiates is most used: by comparison with injection, "dragon chasing" (heating of heroin with barbital on a piece of foil), and madak and "ack ack" (smoking of cigarettes containing tobacco mixed with heroin powder) are only 40 percent and 20 percent efficient, respectively. One study of British heroin addicts found a 12-fold excess mortality ratio (1.8 percent of the group dying per year). Most heroin deaths result not from overdose "per se", but combination with other depressant drugs such as alcohol or benzodiazepines.

The smoking of opium does not involve the burning of the material as might be imagined. Rather, the prepared opium is indirectly heated to temperatures at which the active alkaloids, chiefly morphine, are vaporized. In the past, smokers would use a specially designed opium pipe which had a removable knob-like pipe-bowl of fired earthenware attached by a metal fitting to a long, cylindrical stem. A small "pill" of opium about the size of a pea would be placed on the pipe-bowl, which was then heated by holding it over an opium lamp, a special oil lamp with a distinct funnel-like chimney to channel heat into a small area. The smoker would lie on his or her side in order to guide the pipe-bowl and the tiny pill of opium over the stream of heat rising from the chimney of the oil lamp and inhale the vaporized opium fumes as needed. Several pills of opium were smoked at a single session depending on the smoker's tolerance to the drug. The effects could last up to twelve hours.

In Eastern culture, opium is more commonly used in the form of paregoric to treat diarrhea. This is a weaker solution than laudanum, an alcoholic tincture which was prevalently used as a pain medication and sleeping aid. Tincture of opium has been prescribed for, among other things, severe diarrhea. Taken thirty minutes prior to meals, it significantly slows intestinal motility, giving the intestines greater time to absorb fluid in the stool.

Despite the historically negative view of opium as a cause of addiction, the use of morphine and other derivatives isolated from opium in the treatment of chronic pain has been reestablished. If given in controlled doses, modern opiates can be an effective treatment for neuropathic pain and other forms of chronic pain.

Opium contains two main groups of alkaloids. Phenanthrenes such as morphine, codeine, and thebaine are the main psychoactive constituents. Isoquinolines such as papaverine and noscapine have no significant central nervous system effects. Morphine is the most prevalent and important alkaloid in opium, consisting of 10–16 percent of the total, and is responsible for most of its harmful effects such as lung edema, respiratory difficulties, coma, or cardiac or respiratory collapse. Morphine binds to and activates mu opioid receptor in the brain, spinal cord, stomach and intestine. Regular use can lead to drug tolerance or physical dependence. Chronic opium addicts in 1906 China or modern-day Iran consume an average of eight grams of opium daily.

Both analgesia and drug addiction are functions of the mu opioid receptor, the class of opioid receptor first identified as responsive to morphine. Tolerance is associated with the superactivation of the receptor, which may be affected by the degree of endocytosis caused by the opioid administered, and leads to a superactivation of cyclic AMP signaling. Long-term use of morphine in palliative care and the management of chronic pain always entails a risk that the patient develops tolerance or physical dependence. There are many kinds of rehabilitation treatment, including pharmacologically based treatments with naltrexone, methadone, or ibogaine.

Some slang terms for opium include: "Big O", "Shanghai Sally", "dope", "hop", "midnight oil", "O.P.", and "tar". "Dope" and "tar" can also refer to heroin. The traditional opium pipe is known as a "dream stick". The term "dope" entered the English language in the early nineteenth century, originally referring to viscous liquids, particularly sauces or gravy.  It has been used to refer to opiates since at least 1888, and this usage arose because opium, when prepared for smoking, is viscous.





</doc>
<doc id="22716" url="https://en.wikipedia.org/wiki?curid=22716" title="Online algorithm">
Online algorithm

In computer science, an online algorithm is one that can process its input piece-by-piece in a serial fashion, i.e., in the order that the input is fed to the algorithm, without having the entire input available from the start.

In contrast, an offline algorithm is given the whole problem data from the beginning and is required to output an answer which solves the problem at hand. In operations research, the area in which online algorithms are developed is called online optimization.

As an example, consider the sorting algorithms selection sort and insertion sort: selection sort repeatedly selects the minimum element from the unsorted remainder and places it at the front, which requires access to the entire input; it is thus an offline algorithm. On the other hand, insertion sort considers one input element per iteration and produces a partial solution without considering future elements. Thus insertion sort is an online algorithm.

Note that insertion sort produces the optimum result, i.e., a correctly sorted list. For many problems, online algorithms cannot match the performance of offline algorithms. If the ratio between the performance of an online algorithm and an optimal offline algorithm is bounded, the online algorithm is called competitive.

Not every "online algorithm" has an "offline" counterpart.
Because it does not know the whole input, an online algorithm is forced to make decisions that may later turn out not to be optimal, and the study of online algorithms has focused on the quality of decision-making that is possible in this setting. Competitive analysis formalizes this idea by comparing the relative performance of an online and offline algorithm for the same problem instance. Specifically, the competitive ratio of an algorithm, is defined as the worst-case ratio of its cost divided by the optimal cost, over all possible inputs. The competitive ratio of an online problem is the best competitive ratio achieved by an online algorithm. Intuitively, the competitive ratio of an algorithm gives a measure on the quality of solutions produced by this algorithm, while the competitive ratio of a problem shows the importance of knowing the future for this problem.

For other points of view on "online inputs to algorithms", see 

Some "online algorithms":


A problem exemplifying the concepts of online algorithms is the Canadian Traveller Problem. The goal of this problem is to minimize the cost of reaching a target in a weighted graph where some of the edges are unreliable and may have been removed from the graph. However, that an edge has been removed ("failed") is only revealed to "the traveller" when she/he reaches one of the edge's endpoints. The worst case for this problem is simply that all of the unreliable edges fail and the problem reduces to the usual Shortest Path Problem. An alternative analysis of the problem can be made with the help of competitive analysis. For this method of analysis, the offline algorithm knows in advance which edges will fail and the goal is to minimize the ratio between the online and offline algorithms' performance. This problem is PSPACE-complete.

There are many formal problems that offer more than one "online algorithm" as solution:




</doc>
<doc id="22717" url="https://en.wikipedia.org/wiki?curid=22717" title="Origin">
Origin

Origin, origins, or original may refer to:





















[[ja:オリジン (曖昧さ回避)]]

</doc>
<doc id="22718" url="https://en.wikipedia.org/wiki?curid=22718" title="Ozone">
Ozone

Ozone , or trioxygen, is an inorganic molecule with the chemical formula . It is a pale blue gas with a distinctively pungent smell. It is an allotrope of oxygen that is much less stable than the diatomic allotrope , breaking down in the lower atmosphere to or dioxygen. Ozone is formed from dioxygen by the action of ultraviolet light (UV) and electrical discharges within the Earth's atmosphere. It is present in very low concentrations throughout the latter, with its highest concentration high in the ozone layer of the stratosphere, which absorbs most of the Sun's ultraviolet (UV) radiation.

Ozone's odour is reminiscent of chlorine, and detectable by many people at concentrations of as little as in air. Ozone's O structure was determined in 1865. The molecule was later proven to have a bent structure and to be diamagnetic. In standard conditions, ozone is a pale blue gas that condenses at progressively cryogenic temperatures to a dark blue liquid and finally a violet-black solid. Ozone's instability with regard to more common dioxygen is such that both concentrated gas and liquid ozone may decompose explosively at elevated temperatures or fast warming to the boiling point.
It is therefore used commercially only in low concentrations.

Ozone is a powerful oxidant (far more so than dioxygen) and has many industrial and consumer applications related to oxidation. This same high oxidising potential, however, causes ozone to damage mucous and respiratory tissues in animals, and also tissues in plants, above concentrations of about . While this makes ozone a potent respiratory hazard and pollutant near ground level, a higher concentration in the ozone layer (from two to eight ppm) is beneficial, preventing damaging UV light from reaching the Earth's surface.

The trivial name "ozone" is the most commonly used and preferred IUPAC name. The systematic names "2λ-trioxidiene" and "catena-trioxygen", valid IUPAC names, are constructed according to the substitutive and additive nomenclatures, respectively. The name "ozone" derives from "ozein" (ὄζειν), the Greek verb for smell, referring to ozone's distinctive smell.

In appropriate contexts, ozone can be viewed as trioxidane with two hydrogen atoms removed, and as such, "trioxidanylidene" may be used as a context-specific systematic name, according to substitutive nomenclature. By default, these names pay no regard to the radicality of the ozone molecule. In even more specific context, this can also name the non-radical singlet ground state, whereas the diradical state is named "trioxidanediyl".

"Trioxidanediyl" (or "ozonide") is used, non-systematically, to refer to the substituent group (-OOO-). Care should be taken to avoid confusing the name of the group for the context-specific name for ozone given above.

In 1785, the Dutch chemist Martinus van Marum was conducting experiments involving electrical sparking above water when he noticed an unusual smell, which he attributed to the electrical reactions, failing to realize that he had in fact created ozone. 

A half century later, Christian Friedrich Schönbein noticed the same pungent odour and recognized it as the smell often following a bolt of lightning. In 1839, he succeeded in isolating the gaseous chemical and named it "ozone", from the Greek word "" () meaning "to smell". 
For this reason, Schönbein is generally credited with the discovery of ozone. The formula for ozone, O, was not determined until 1865 by Jacques-Louis Soret and confirmed by Schönbein in 1867.

For much of the second half of the nineteenth century and well into the twentieth, ozone was considered a healthy component of the environment by naturalists and health-seekers. Beaumont, California had as its official slogan "Beaumont: Zone of Ozone", as evidenced on postcards and Chamber of Commerce letterhead. Naturalists working outdoors often considered the higher elevations beneficial because of their ozone content. "There is quite a different atmosphere [at higher elevation] with enough ozone to sustain the necessary energy [to work]", wrote naturalist Henry Henshaw, working in Hawaii. Seaside air was considered to be healthy because of its believed ozone content; but the smell giving rise to this belief is in fact that of halogenated seaweed metabolites.

Much of ozone's appeal seems to have resulted from its "fresh" smell, which evoked associations with purifying properties. Scientists, however, noted its harmful effects. In 1873 James Dewar and John Gray McKendrick documented that frogs grew sluggish, birds gasped for breath, and rabbits’ blood showed decreased levels of oxygen after exposure to "ozonized air", which "exercised a destructive action".
Schönbein himself reported that chest pains, irritation of the mucous membranes and difficulty breathing occurred as a result of inhaling ozone, and small mammals died.
In 1911, Leonard Hill and Martin Flack stated in the "Proceedings of the Royal Society B" that ozone’s healthful effects “have, by mere iteration, become part and parcel of common belief; and yet exact physiological evidence in favour of its good effects has been hitherto almost entirely wanting... The only thoroughly well-ascertained knowledge concerning the physiological effect of ozone, so far attained, is that it causes irritation and œdema of the lungs, and death if inhaled in relatively strong concentration for any time.”

During World War I, ozone was tested at Queen Alexandra's Military Hospital in London as a possible disinfectant for wounds. The gas was applied directly to wounds for as long as 15 minutes. This resulted in damage to both bacterial cells and human tissue. Other sanitizing techniques, such as irrigation with antiseptics, were found preferable.

Ozone is colourless or pale blue gas (blue when liquefied), slightly soluble in water and much more soluble in inert non-polar solvents such as carbon tetrachloride or fluorocarbons, in which it forms a blue solution. At , it condenses to form a dark blue liquid. It is dangerous to allow this liquid to warm to its boiling point, because both concentrated gaseous ozone and liquid ozone can detonate. At temperatures below , it forms a violet-black solid.

Most people can detect about 0.01 μmol/mol of ozone in air where it has a very specific sharp odour somewhat resembling chlorine bleach. Exposure of 0.1 to 1 μmol/mol produces headaches, burning eyes and irritation to the respiratory passages.
Even low concentrations of ozone in air are very destructive to organic materials such as latex, plastics and animal lung tissue.

Ozone is diamagnetic, with all its electrons paired. In contrast, O is paramagnetic, containing two unpaired electrons.

According to experimental evidence from microwave spectroscopy, ozone is a bent molecule, with C symmetry (similar to the water molecule). The O – O distances are . The O – O – O angle is 116.78°. The central atom is "sp"² hybridized with one lone pair. Ozone is a polar molecule with a dipole moment of 0.53 D. The molecule can be represented as a resonance hybrid with two contributing structures, each with a single bond on one side and double bond on the other. The arrangement possesses an overall bond order of 1.5 for both sides.

Ozone is among the most powerful oxidizing agents known, far stronger than O. It is also unstable at high concentrations, decaying into ordinary oxygen. Its half-life varies with atmospheric conditions such as temperature, humidity, and air movement. In a sealed chamber with a fan that moves the gas, ozone has a half-life of approximately one day at room temperature. Some unverified claims assert that ozone can have a half life of as short as thirty minutes under atmospheric conditions.

This reaction proceeds more rapidly with increasing temperature. Deflagration of ozone can be triggered by a spark and can occur in ozone concentrations of 10 wt% or higher.

Ozone can also be produced from oxygen at the anode of an electrochemical cell. This reaction can create smaller quantities of ozone for research purposes.

(g) + 2H + 2e ←→ (g) + E°= 2.075V 

This can be observed as an unwanted reaction in a Hoffman gas apparatus during the electrolysis of water when the voltage is set above the necessary voltage.

Ozone will oxidise most metals (except gold, platinum, and iridium) to oxides of the metals in their highest oxidation state. For example:

Ozone also oxidizes nitric oxide to nitrogen dioxide:
This reaction is accompanied by chemiluminescence. The can be further oxidized:
The formed can react with to form .

Solid nitronium perchlorate can be made from NO, ClO, and gases:

Ozone does not react with ammonium salts, but it oxidizes ammonia to ammonium nitrate:

Ozone reacts with carbon to form carbon dioxide, even at room temperature:

Ozone oxidises sulfides to sulfates. For example, lead(II) sulfide is oxidised to lead(II) sulfate:

Sulfuric acid can be produced from ozone, water and either elemental sulfur or sulfur dioxide:

In the gas phase, ozone reacts with hydrogen sulfide to form sulfur dioxide:

In an aqueous solution, however, two competing simultaneous reactions occur, one to produce elemental sulfur, and one to produce sulfuric acid:

Alkenes can be oxidatively cleaved by ozone, in a process called ozonolysis, giving alcohols, aldehydes, ketones, and carboxylic acids, depending on the second step of the workup.

Ozone can also cleave alkynes to form a acid anhydride or diketone product. If the reaction is performed in the presence of water, the anhydride hydrolyzes to give two carboxylic acids.

Usually ozonolysis is carried out in a solution of dichloromethane, at a temperature of −78C. After a sequence of cleavage and rearrangement, an organic ozonide is formed. With reductive workup (e.g. zinc in acetic acid or dimethyl sulfide), ketones and aldehydes will be formed, with oxidative workup (e.g. aqueous or alcoholic hydrogen peroxide), carboxylic acids will be formed.

All three atoms of ozone may also react, as in the reaction of tin(II) chloride with hydrochloric acid and ozone:
Iodine perchlorate can be made by treating iodine dissolved in cold anhydrous perchloric acid with ozone:

Ozone can be used for combustion reactions and combustible gases; ozone provides higher temperatures than burning in dioxygen (O). The following is a reaction for the combustion of carbon subnitride which can also cause higher temperatures:

Ozone can react at cryogenic temperatures. At , atomic hydrogen reacts with liquid ozone to form a hydrogen superoxide radical, which dimerizes:

Reduction of ozone gives the ozonide anion, O. Derivatives of this anion are explosive and must be stored at cryogenic temperatures. Ozonides for all the alkali metals are known. KO, RbO, and CsO can be prepared from their respective superoxides:
Although KO can be formed as above, it can also be formed from potassium hydroxide and ozone:
NaO and LiO must be prepared by action of CsO in liquid NH on an ion exchange resin containing Na or Li ions:
A solution of calcium in ammonia reacts with ozone to give to ammonium ozonide and not calcium ozonide:

Ozone can be used to remove iron and manganese from water, forming a precipitate which can be filtered:
Ozone will also oxidize dissolved hydrogen sulfide in water to sulfurous acid:

These three reactions are central in the use of ozone based well water treatment.

Ozone will also detoxify cyanides by converting them to cyanates.

Ozone will also completely decompose urea:

Ozone is a bent triatomic molecule with three vibrational modes: the symmetric stretch (1103.157 cm), bend (701.42 cm) and antisymmetric stretch (1042.096 cm). The symmetric stretch and bend are weak absorbers, but the antisymmetric stretch is strong and responsible for ozone being an important minor greenhouse gas. This IR band is also used to detect ambient and atmospheric ozone although UV based measurements are more common.

The electronic spectrum of ozone is quite complex. An overview can be seen at the MPI Mainz UV/VIS Spectral Atlas of Gaseous Molecules of Atmospheric Interest.

All of the bands are dissociative, meaning that the molecule falls apart to after absorbing a photon. The most important absorption is the Hartley band, extending from slightly above 300 nm down to slightly above 200 nm. It is this band that is responsible for absorbing UV C in the stratosphere.

On the high wavelength side, the Hartley band transitions to the so-called Huggins band, which falls off rapidly until disappearing by ~360 nm. Above 400 nm, extending well out into the NIR, are the Chappius and Wulf bands. There, unstructured absorption bands are useful for detecting high ambient concentrations of ozone, but are so weak that they do not have much practical effect.

There are additional absorption bands in the far UV, which increase slowly from 200 nm down to reaching a maximum at ~120 nm.

The standard way to express total ozone levels (the amount of ozone in a given vertical column) in the atmosphere is by using Dobson units. Point measurements are reported as mole fractions in nmol/mol (parts per billion, ppb) or as concentrations in μg/m. The study of ozone concentration in the atmosphere started in the 1920s.

The highest levels of ozone in the atmosphere are in the stratosphere, in a region also known as the ozone layer between about 10 km and 50 km above the surface (or between about 6 and 31 miles). However, even in this "layer", the ozone concentrations are only two to eight parts per million, so most of the oxygen there is dioxygen, O, at about 210,000 parts per million by volume.

Ozone in the stratosphere is mostly produced from short-wave ultraviolet rays between 240 and 160 nm. Oxygen starts to absorb weakly at 240 nm in the Herzberg bands, but most of the oxygen is dissociated by absorption in the strong Schumann–Runge bands between 200 and 160 nm where ozone does not absorb. While shorter wavelength light, extending to even the X-Ray limit, is energetic enough to dissociate molecular oxygen, there is relatively little of it, and, the strong solar emission at Lyman-alpha, 121 nm, falls at a point where molecular oxygen absorption is a minimum.

The process of ozone creation and destruction is called the Chapman cycle and starts with the photolysis of molecular oxygen

followed by reaction of the oxygen atom with another molecule of oxygen to form ozone.

where "M" denotes the third body that carries off the excess energy of the reaction. The ozone molecule can then absorb a UV-C photon and dissociate

The excess kinetic energy heats the stratosphere when the O atoms and the molecular oxygen fly apart and collide with other molecules. This conversion of UV light into kinetic energy warms the stratosphere. The oxygen atoms produced in the photolysis of ozone then react back with other oxygen molecule as in the previous step to form more ozone. In the clear atmosphere, with only nitrogen and oxygen, ozone can react with the atomic oxygen to form two molecules of O

An estimate of the rate of this termination step to the cycling of atomic oxygen back to ozone can be found simply by taking the ratios of the concentration of O to O. The termination reaction is catalysed by the presence of certain free radicals, of which the most important are hydroxyl (OH), nitric oxide (NO) and atomic chlorine (Cl) and bromine (Br). In the second half of the 20th Century the amount of ozone in the stratosphere was discovered to be declining, mostly because of increasing concentrations of chlorofluorocarbons (CFC) and similar chlorinated and brominated organic molecules. The concern over the health effects of the decline led to the 1987 Montreal Protocol, the ban on the production of many ozone depleting chemicals and in the first and second decade of the 21st Century the beginning of the recovery of stratospheric ozone concentrations.

Ozone in the ozone layer filters out sunlight wavelengths from about 200 nm UV rays to 315 nm, with ozone peak absorption at about 250 nm. This ozone UV absorption is important to life, since it extends the absorption of UV by ordinary oxygen and nitrogen in air (which absorb all wavelengths < 200 nm) through the lower UV-C (200–280 nm) and the entire UV-B band (280–315 nm). The small unabsorbed part that remains of UV-B after passage through ozone causes sunburn in humans, and direct DNA damage in living tissues in both plants and animals. Ozone's effect on mid-range UV-B rays is illustrated by its effect on UV-B at 290 nm, which has a radiation intensity 350 million times as powerful at the top of the atmosphere as at the surface. Nevertheless, enough of UV-B radiation at similar frequency reaches the ground to cause some sunburn, and these same wavelengths are also among those responsible for the production of vitamin D in humans.

The ozone layer has little effect on the longer UV wavelengths called UV-A (315–400 nm), but this radiation does not cause sunburn or direct DNA damage, and while it probably does cause long-term skin damage in certain humans, it is not as dangerous to plants and to the health of surface-dwelling organisms on Earth in general (see ultraviolet for more information on near ultraviolet).

Low level ozone (or tropospheric ozone) is an atmospheric pollutant. It is not emitted directly by car engines or by industrial operations, but formed by the reaction of sunlight on air containing hydrocarbons and nitrogen oxides that react to form ozone directly at the source of the pollution or many kilometers down wind.

Ozone reacts directly with some hydrocarbons such as aldehydes and thus begins their removal from the air, but the products are themselves key components of smog. Ozone photolysis by UV light leads to production of the hydroxyl radical HO• and this plays a part in the removal of hydrocarbons from the air, but is also the first step in the creation of components of smog such as peroxyacyl nitrates, which can be powerful eye irritants. The atmospheric lifetime of tropospheric ozone is about 22 days; its main removal mechanisms are being deposited to the ground, the above-mentioned reaction giving HO•, and by reactions with OH and the peroxy radical HO•.

There is evidence of significant reduction in agricultural yields because of increased ground-level ozone and pollution which interferes with photosynthesis and stunts overall growth of some plant species. The United States Environmental Protection Agency is proposing a secondary regulation to reduce crop damage, in addition to the primary regulation designed for the protection of human health.

Certain examples of cities with elevated ozone readings are Houston, Texas, and Mexico City, Mexico. Houston has a reading of around 41 nmol/mol, while Mexico City is far more hazardous, with a reading of about 125 nmol/mol.

Ozone gas attacks any polymer possessing olefinic or double bonds within its chain structure, such as natural rubber, nitrile rubber, and styrene-butadiene rubber. Products made using these polymers are especially susceptible to attack, which causes cracks to grow longer and deeper with time, the rate of crack growth depending on the load carried by the rubber component and the concentration of ozone in the atmosphere. Such materials can be protected by adding antiozonants, such as waxes, which bond to the surface to create a protective film or blend with the material and provide long term protection. Ozone cracking used to be a serious problem in car tires for example, but the problem is now seen only in very old tires. On the other hand, many critical products, like gaskets and O-rings, may be attacked by ozone produced within compressed air systems. Fuel lines made of reinforced rubber are also susceptible to attack, especially within the engine compartment, where some ozone is produced by electrical components. Storing rubber products in close proximity to a DC electric motor can accelerate ozone cracking. The commutator of the motor generates sparks which in turn produce ozone.

Although ozone was present at ground level before the Industrial Revolution, peak concentrations are now far higher than the pre-industrial levels, and even background concentrations well away from sources of pollution are substantially higher. Ozone acts as a greenhouse gas, absorbing some of the infrared energy emitted by the earth. Quantifying the greenhouse gas potency of ozone is difficult because it is not present in uniform concentrations across the globe. However, the most widely accepted scientific assessments relating to climate change (e.g. the Intergovernmental Panel on Climate Change Third Assessment Report) suggest that the radiative forcing of tropospheric ozone is about 25% that of carbon dioxide.

The annual global warming potential of tropospheric ozone is between 918–1022 tons carbon dioxide equivalent/tons tropospheric ozone. This means on a per-molecule basis, ozone in the troposphere has a radiative forcing effect roughly 1,000 times as strong as carbon dioxide. However, tropospheric ozone is a short-lived greenhouse gas, which decays in the atmosphere much more quickly than carbon dioxide. This means that over a 20-year span, the global warming potential of tropospheric ozone is much less, roughly 62 to 69 tons carbon dioxide equivalent / ton tropospheric ozone.

Because of its short-lived nature, tropospheric ozone does not have strong global effects, but has very strong radiative forcing effects on regional scales. In fact, there are regions of the world where tropospheric ozone has a radiative forcing up to 150% of carbon dioxide.

Ozone precursors are a group of pollutants, predominantly those emitted during the combustion of fossil fuels. Ground-level ozone pollution (tropospheric ozone) is created near the Earth's surface by the action of daylight UV rays on these precursors. The ozone at ground level is primarily from fossil fuel precursors, but methane is a natural precursor, and the very low natural background level of ozone at ground level is considered safe. This section examines the health impacts of fossil fuel burning, which raises ground level ozone far above background levels.

There is a great deal of evidence to show that ground-level ozone can harm lung function and irritate the respiratory system. Exposure to ozone (and the pollutants that produce it) is linked to premature death, asthma, bronchitis, heart attack, and other cardiopulmonary problems.

Long-term exposure to ozone has been shown to increase risk of death from respiratory illness. A study of 450,000 people living in United States cities saw a significant correlation between ozone levels and respiratory illness over the 18-year follow-up period. The study revealed that people living in cities with high ozone levels, such as Houston or Los Angeles, had an over 30% increased risk of dying from lung disease.

Air quality guidelines such as those from the World Health Organization, the United States Environmental Protection Agency (EPA) and the European Union are based on detailed studies designed to identify the levels that can cause measurable ill health effects.

According to scientists with the US EPA, susceptible people can be adversely affected by ozone levels as low as 40 nmol/mol. In the EU, the current target value for ozone concentrations is 120 µg/m which is about 60 nmol/mol. This target applies to all member states in accordance with Directive 2008/50/EC. Ozone concentration is measured as a maximum daily mean of 8 hour averages and the target should not be exceeded on more than 25 calendar days per year, starting from January 2010. Whilst the directive requires in the future a strict compliance with 120 µg/m limit (i.e. mean ozone concentration not to be exceeded on any day of the year), there is no date set for this requirement and this is treated as a long-term objective.

In the USA, the Clean Air Act directs the EPA to set National Ambient Air Quality Standards for several pollutants, including ground-level ozone, and counties out of compliance with these standards are required to take steps to reduce their levels. In May 2008, under a court order, the EPA lowered its ozone standard from 80 nmol/mol to 75 nmol/mol. The move proved controversial, since the Agency's own scientists and advisory board had recommended lowering the standard to 60 nmol/mol. Many public health and environmental groups also supported the 60 nmol/mol standard, and the World Health Organization recommends 51 nmol/mol.

On January 7, 2010, the U.S. Environmental Protection Agency (EPA) announced proposed revisions to the National Ambient Air Quality Standard (NAAQS) for the pollutant ozone, the principal component of smog:

On October 26, 2015, the EPA published a final rule with an effective date of December 28, 2015 that revised the 8-hour primary NAAQS from 0.075 ppm to 0.070 ppm.

The EPA has developed an Air Quality Index (AQI) to help explain air pollution levels to the general public. Under the current standards, eight-hour average ozone mole fractions of 85 to 104 nmol/mol are described as "unhealthy for sensitive groups", 105 nmol/mol to 124 nmol/mol as "unhealthy", and 125 nmol/mol to 404 nmol/mol as "very unhealthy".

Ozone can also be present in indoor air pollution, partly as a result of electronic equipment such as photocopiers. A connection has also been known to exist between the increased pollen, fungal spores, and ozone caused by thunderstorms and hospital admissions of asthma sufferers.

In the Victorian era, one British folk myth held that the smell of the sea was caused by ozone. In fact, the characteristic "smell of the sea" is caused by dimethyl sulfide, a chemical generated by phytoplankton. Victorian British folk considered the resulting smell "bracing".

Ozone production rises during heat waves, because plants absorb less ozone. It is estimated that curtailed ozone absorption by plants was responsible for the loss of 460 lives in the UK in the hot summer of 2006. A similar investigation to assess the joint effects of ozone and heat during the European heat waves in 2003, concluded that these appear to be additive.

Ozone, along with reactive forms of oxygen such as superoxide, singlet oxygen, hydrogen peroxide, and hypochlorite ions, is produced by white blood cells and other biological systems (such as the roots of marigolds) as a means of destroying foreign bodies. Ozone reacts directly with organic double bonds. Also, when ozone breaks down to dioxygen it gives rise to oxygen free radicals, which are highly reactive and capable of damaging many organic molecules. Moreover, it is believed that the powerful oxidizing properties of ozone may be a contributing factor of inflammation. The cause-and-effect relationship of how the ozone is created in the body and what it does is still under consideration and still subject to various interpretations, since other body chemical processes can trigger some of the same reactions. A team headed by Paul Wentworth Jr. of the Department of Chemistry at the Scripps Research Institute has shown evidence linking the antibody-catalyzed water-oxidation pathway of the human immune response to the production of ozone. In this system, ozone is produced by antibody-catalyzed production of trioxidane from water and neutrophil-produced singlet oxygen.

When inhaled, ozone reacts with compounds lining the lungs to form specific, cholesterol-derived metabolites that are thought to facilitate the build-up and pathogenesis of atherosclerotic plaques (a form of heart disease). These metabolites have been confirmed as naturally occurring in human atherosclerotic arteries and are categorized into a class of secosterols termed "atheronals", generated by ozonolysis of cholesterol's double bond to form a 5,6 secosterol as well as a secondary condensation product via aldolization.

Ozone has been implicated to have an adverse effect on plant growth: "... ozone reduced total chlorophylls, carotenoid and carbohydrate concentration, and increased 1-aminocyclopropane-1-carboxylic acid (ACC) content and ethylene production. In treated plants, the ascorbate leaf pool was decreased, while lipid peroxidation and solute leakage were significantly higher than in ozone-free controls. The data indicated that ozone triggered protective mechanisms against oxidative stress in citrus."

Because of the strongly oxidizing properties of ozone, ozone is a primary irritant, affecting especially the eyes and respiratory systems and can be hazardous at even low concentrations. The Canadian Centre for Occupation Safety and Health reports that: "Even very low concentrations of ozone can be harmful to the upper respiratory tract and the lungs. The severity of injury depends on both by the concentration of ozone and the duration of exposure. Severe and permanent lung injury or death could result from even a very short-term exposure to relatively low concentrations." To protect workers potentially exposed to ozone, U.S. Occupational Safety and Health Administration has established a permissible exposure limit (PEL) of 0.1 μmol/mol (29 CFR 1910.1000 table Z-1), calculated as an 8-hour time weighted average. Higher concentrations are especially hazardous and NIOSH has established an Immediately Dangerous to Life and Health Limit (IDLH) of 5 μmol/mol. Work environments where ozone is used or where it is likely to be produced should have adequate ventilation and it is prudent to have a monitor for ozone that will alarm if the concentration exceeds the OSHA PEL. Continuous monitors for ozone are available from several suppliers.

Elevated ozone exposure can occur on passenger aircraft, with levels depending on altitude and atmospheric turbulence. United States Federal Aviation Authority regulations set a limit of 250 nmol/mol with a maximum four-hour average of 100 nmol/mol. Some planes are equipped with ozone converters in the ventilation system to reduce passenger exposure.

Ozone generators are used to produce ozone for cleaning air or removing smoke odours in unoccupied rooms. These ozone generators can produce over 3 g of ozone per hour. Ozone often forms in nature under conditions where O will not react. Ozone used in industry is measured in μmol/mol (ppm, parts per million), nmol/mol (ppb, parts per billion), μg/m, mg/h (milligrams per hour) or weight percent. The regime of applied concentrations ranges from 1% to 5% (in air) and from 6% to 14% (in oxygen) for older generation methods. New electrolytic methods can achieve up 20% to 30% dissolved ozone concentrations in output water.

Temperature and humidity play a large role in how much ozone is being produced using traditional generation methods (such as corona discharge and ultraviolet light). Old generation methods will produce less than 50% of nominal capacity if operated with humid ambient air, as opposed to very dry air. New generators, using electrolytic methods, can achieve higher purity and dissolution through using water molecules as the source of ozone production.

This is the most common type of ozone generator for most industrial and personal uses. While variations of the "hot spark" coronal discharge method of ozone production exist, including medical grade and industrial grade ozone generators, these units usually work by means of a corona discharge tube. They are typically cost-effective and do not require an oxygen source other than the ambient air to produce ozone concentrations of 3–6%. Fluctuations in ambient air, due to weather or other environmental conditions, cause variability in ozone production. However, they also produce nitrogen oxides as a by-product. Use of an air dryer can reduce or eliminate nitric acid formation by removing water vapor and increase ozone production. Use of an oxygen concentrator can further increase the ozone production and further reduce the risk of nitric acid formation by removing not only the water vapor, but also the bulk of the nitrogen.

UV ozone generators, or vacuum-ultraviolet (VUV) ozone generators, employ a light source that generates a narrow-band ultraviolet light, a subset of that produced by the Sun. The Sun's UV sustains the ozone layer in the stratosphere of Earth.

UV ozone generators use ambient air for ozone production, no air prep systems are used (air dryer or oxygen concentrator), therefore these generators tend to be less expensive. However UV ozone generators usually produce ozone with a concentration of about 0.5% or lower which limits the potential ozone production rate. Another disadvantage of this method is that it requires the ambient air (oxygen) to be exposed to the UV source for a longer amount of time, and any gas that is not exposed to the UV source will not be treated. This makes UV generators impractical for use in situations that deal with rapidly moving air or water streams (in-duct air sterilization, for example). Production of ozone is one of the potential dangers of ultraviolet germicidal irradiation. VUV ozone generators are used in swimming pool and spa applications ranging to millions of gallons of water. VUV ozone generators, unlike corona discharge generators, do not produce harmful nitrogen by-products and also unlike corona discharge systems, VUV ozone generators work extremely well in humid air environments. There is also not normally a need for expensive off-gas mechanisms, and no need for air driers or oxygen concentrators which require extra costs and maintenance.

In the cold plasma method, pure oxygen gas is exposed to a plasma created by dielectric barrier discharge. The diatomic oxygen is split into single atoms, which then recombine in triplets to form ozone.

Cold plasma machines utilize pure oxygen as the input source and produce a maximum concentration of about 5% ozone. They produce far greater quantities of ozone in a given space of time compared to ultraviolet production. However, because cold plasma ozone generators are very expensive, they are found less frequently than the previous two types.

The discharges manifest as filamentary transfer of electrons (micro discharges) in a gap between two electrodes. In order to evenly distribute the micro discharges, a dielectric insulator must be used to separate the metallic electrodes and to prevent arcing.

Some cold plasma units also have the capability of producing short-lived allotropes of oxygen which include O, O, O, O, etc. These species are even more reactive than ordinary .

Electrolytic ozone generation (EOG) splits water molecules into H, O, and O.
In most EOG methods, the hydrogen gas will be removed to leave oxygen and ozone as the only reaction products. Therefore, EOG can achieve higher dissolution in water without other competing gases found in corona discharge method, such as nitrogen gases present in ambient air. This method of generation can achieve concentrations of 20–30% and is independent of air quality because water is used as the source material. Production of ozone electrolytically is typically unfavorable because of the high overpotential required to produce ozone as compared to oxygen. This is why ozone is not produced during typical water electrolysis. However, it is possible to increase the overpotential of oxygen by careful catalyst selection such that ozone is preferentially produced under electrolysis. Catalysts typically chosen for this approach are lead dioxide or boron-doped diamond.

Ozone cannot be stored and transported like other industrial gases (because it quickly decays into diatomic oxygen) and must therefore be produced on site. Available ozone generators vary in the arrangement and design of the high-voltage electrodes. At production capacities higher than 20 kg per hour, a gas/water tube heat-exchanger may be utilized as ground electrode and assembled with tubular high-voltage electrodes on the gas-side. The regime of typical gas pressures is around absolute in oxygen and absolute in air. Several megawatts of electrical power may be installed in large facilities, applied as one phase AC current at 50 to 8000 Hz and peak voltages between 3,000 and 20,000 volts. Applied voltage is usually inversely related to the applied frequency.

The dominating parameter influencing ozone generation efficiency is the gas temperature, which is controlled by cooling water temperature and/or gas velocity. The cooler the water, the better the ozone synthesis. The lower the gas velocity, the higher the concentration (but the lower the net ozone produced). At typical industrial conditions, almost 90% of the effective power is dissipated as heat and needs to be removed by a sufficient cooling water flow.

Because of the high reactivity of ozone, only a few materials may be used like stainless steel (quality 316L), titanium, aluminium (as long as no moisture is present), glass, polytetrafluorethylene, or polyvinylidene fluoride. Viton may be used with the restriction of constant mechanical forces and absence of humidity (humidity limitations apply depending on the formulation). Hypalon may be used with the restriction that no water come in contact with it, except for normal atmospheric levels. Embrittlement or shrinkage is the common mode of failure of elastomers with exposure to ozone. Ozone cracking is the common mode of failure of elastomer seals like O-rings.

Silicone rubbers are usually adequate for use as gaskets in ozone concentrations below 1 wt%, such as in equipment for accelerated aging of rubber samples.

Ozone may be formed from by electrical discharges and by action of high energy electromagnetic radiation. Unsuppressed arcing in electrical contacts, motor brushes, or mechanical switches breaks down the chemical bonds of the atmospheric oxygen surrounding the contacts [ → 2O]. Free radicals of oxygen in and around the arc recombine to create ozone []. Certain electrical equipment generate significant levels of ozone. This is especially true of devices using high voltages, such as ionic air purifiers, laser printers, photocopiers, tasers and arc welders. Electric motors using brushes can generate ozone from repeated sparking inside the unit. Large motors that use brushes, such as those used by elevators or hydraulic pumps, will generate more ozone than smaller motors.

Ozone is similarly formed in the Catatumbo lightning storms phenomenon on the Catatumbo River in Venezuela, though ozone's instability makes it dubious that it has any effect on the ozonosphere.
It is the world's largest single natural generator of ozone, lending calls for it to be designated a UNESCO World Heritage Site.

In the laboratory, ozone can be produced by electrolysis using a 9 volt battery, a pencil graphite rod cathode, a platinum wire anode and a 3 molar sulfuric acid electrolyte. The half cell reactions taking place are:

In the net reaction, three equivalents of water are converted into one equivalent of ozone and three equivalents of hydrogen. Oxygen formation is a competing reaction.

It can also be generated by a high voltage arc. In its simplest form, high voltage AC, such as the output of a Neon-sign transformer is connected to two metal rods with the ends placed sufficiently close to each other to allow an arc. The resulting arc will convert atmospheric oxygen to ozone.

It is often desirable to contain the ozone. This can be done with an apparatus consisting of two concentric glass tubes sealed together at the top with gas ports at the top and bottom of the outer tube. The inner core should have a length of metal foil inserted into it connected to one side of the power source. The other side of the power source should be connected to another piece of foil wrapped around the outer tube. A source of dry is applied to the bottom port. When high voltage is applied to the foil leads, electricity will discharge between the dry dioxygen in the middle and form and which will flow out the top port. The reaction can be summarized as follows:

The largest use of ozone is in the preparation of pharmaceuticals, synthetic lubricants, and many other commercially useful organic compounds, where it is used to sever carbon-carbon bonds. It can also be used for bleaching substances and for killing microorganisms in air and water sources. Many municipal drinking water systems kill bacteria with ozone instead of the more common chlorine. Ozone has a very high oxidation potential. Ozone does not form organochlorine compounds, nor does it remain in the water after treatment. Ozone can form the suspected carcinogen bromate in source water with high bromide concentrations. The U.S. Safe Drinking Water Act mandates that these systems introduce an amount of chlorine to maintain a minimum of 0.2 μmol/mol residual free chlorine in the pipes, based on results of regular testing. Where electrical power is abundant, ozone is a cost-effective method of treating water, since it is produced on demand and does not require transportation and storage of hazardous chemicals. Once it has decayed, it leaves no taste or odour in drinking water.

Although low levels of ozone have been advertised to be of some disinfectant use in residential homes, the concentration of ozone in dry air required to have a rapid, substantial effect on airborne pathogens exceeds safe levels recommended by the U.S. Occupational Safety and Health Administration and Environmental Protection Agency. Humidity control can vastly improve both the killing power of the ozone and the rate at which it decays back to oxygen (more humidity allows more effectiveness). Spore forms of most pathogens are very tolerant of atmospheric ozone in concentrations at which asthma patients start to have issues.

Industrially, ozone is used to:

Ozone is a reagent in many organic reactions in the laboratory and in industry. Ozonolysis is the cleavage of an alkene to carbonyl compounds.

Many hospitals around the world use large ozone generators to decontaminate operating rooms between surgeries. The rooms are cleaned and then sealed airtight before being filled with ozone which effectively kills or neutralizes all remaining bacteria.

Ozone is used as an alternative to chlorine or chlorine dioxide in the bleaching of wood pulp. It is often used in conjunction with oxygen and hydrogen peroxide to eliminate the need for chlorine-containing compounds in the manufacture of high-quality, white paper.

Ozone can be used to detoxify cyanide wastes (for example from gold and silver mining) by oxidising cyanide to cyanate and eventually to carbon dioxide.

Devices generating high levels of ozone, some of which use ionization, are used to sanitize and deodorize uninhabited buildings, rooms, ductwork, woodsheds, boats and other vehicles.

In the U.S., air purifiers emitting low levels of ozone have been sold. This kind of air purifier is sometimes claimed to imitate nature's way of purifying the air without filters and to sanitize both it and household surfaces. The United States Environmental Protection Agency (EPA) has declared that there is "evidence to show that at concentrations that do not exceed public health standards, ozone is not effective at removing many odor-causing chemicals" or "viruses, bacteria, mold, or other biological pollutants". Furthermore, its report states that "results of some controlled studies show that concentrations of ozone considerably higher than these [human safety] standards are possible even when a user follows the manufacturer’s operating instructions".

Ozonated water is used to launder clothes and to sanitize food, drinking water, and surfaces in the home. According to the U.S. Food and Drug Administration (FDA), it is "amending the food additive regulations to provide for the safe use of ozone in gaseous and aqueous phases as an antimicrobial agent on food, including meat and poultry." Studies at California Polytechnic University demonstrated that 0.3 μmol/mol levels of ozone dissolved in filtered tapwater can produce a reduction of more than 99.99% in such food-borne microorganisms as salmonella, "E. coli" 0157:H7 and "Campylobacter". This quantity is 20,000 times the WHO-recommended limits stated above.
Ozone can be used to remove pesticide residues from fruits and vegetables.

Ozone is used in homes and hot tubs to kill bacteria in the water and to reduce the amount of chlorine or bromine required by reactivating them to their free state. Since ozone does not remain in the water long enough, ozone by itself is ineffective at preventing cross-contamination among bathers and must be used in conjunction with halogens. Gaseous ozone created by ultraviolet light or by corona discharge is injected into the water.

Ozone is also widely used in treatment of water in aquariums and fish ponds. Its use can minimize bacterial growth, control parasites, eliminate transmission of some diseases, and reduce or eliminate "yellowing" of the water. Ozone must not come in contact with fish's gill structures. Natural salt water (with life forms) provides enough "instantaneous demand" that controlled amounts of ozone activate bromide ion to hypobromous acid, and the ozone entirely decays in a few seconds to minutes. If oxygen fed ozone is used, the water will be higher in dissolved oxygen, fish's gill structures will atrophy and they will become dependent on higher dissolved oxygen levels.

Ozonation – a process of infusing water with ozone – can be used in aquaculture to facilitate organic breakdown. Ozone is also added to recirculating systems to reduce nitrite levels through conversion into nitrate. If nitrite levels in the water are high, nitrites will also accumulate in the blood and tissues of fish, where it interferes with oxygen transport (it causes oxidation of the heme-group of haemoglobin from ferrous () to ferric (), making haemoglobin unable to bind ). Despite these apparent positive effects, ozone use in recirculation systems has been linked to reducing the level of bioavailable iodine in salt water systems, resulting in iodine deficiency symptoms such as goitre and decreased growth in Senegalese sole ("Solea senegalensis") larvae.

Ozonate seawater is used for surface disinfection of haddock and Atlantic halibut eggs against nodavirus. Nodavirus is a lethal and vertically transmitted virus which causes severe mortality in fish. Haddock eggs should not be treated with high ozone level as eggs so treated did not hatch and died after 3–4 days.

Ozone application on freshly cut pineapple and banana shows increase in flavonoids and total phenol contents when exposure is up to 20 minutes. Decrease in ascorbic acid (one form of vitamin C) content is observed but the positive effect on total phenol content and flavonoids can overcome the negative effect. Tomatoes upon treatment with ozone shows an increase in β-carotene, lutein and lycopene. However, ozone application on strawberries in pre-harvest period shows decrease in ascorbic acid content.

Ozone facilitates the extraction of some heavy metals from soil using EDTA. EDTA forms strong, water-soluble coordination compounds with some heavy metals (Pb, Zn) thereby making it possible to dissolve them out from contaminated soil. If contaminated soil is pre-treated with ozone, the extraction efficacy of Pb, Am and Pu increases by 11.0–28.9%, 43.5% and 50.7% respectively.

Various therapeutic uses for ozone have been proposed, but are not supported by high quality evidence and generally considered alternative medicine.





</doc>
<doc id="22719" url="https://en.wikipedia.org/wiki?curid=22719" title="Orchidaceae">
Orchidaceae

The Orchidaceae are a diverse and widespread family of flowering plants, with blooms that are often colourful and fragrant, commonly known as the orchid family.

Along with the Asteraceae, they are one of the two largest families of flowering plants. The Orchidaceae have about 28,000 currently accepted species, distributed in about 763 genera. The determination of which family is larger is still under debate, because verified data on the members of such enormous families are continually in flux. Regardless, the number of orchid species nearly equals the number of bony fishes and is more than twice the number of bird species, and about four times the number of mammal species. 

The family encompasses about 6–11% of all seed plants. The largest genera are "Bulbophyllum" (2,000 species), "Epidendrum" (1,500 species), "Dendrobium" (1,400 species) and "Pleurothallis" (1,000 species). It also includes "Vanilla"–the genus of the vanilla plant, the type genus "Orchis", and many commonly cultivated plants such as "Phalaenopsis" and "Cattleya". Moreover, since the introduction of tropical species into cultivation in the 19th century, horticulturists have produced more than 100,000 hybrids and cultivars.

Orchids are easily distinguished from other plants, as they share some very evident, shared derived characteristics, or synapomorphies. Among these are: bilateral symmetry of the flower (zygomorphism), many resupinate flowers, a nearly always highly modified petal (labellum), fused stamens and carpels, and extremely small seeds.

All orchids are perennial herbs that lack any permanent woody structure. They can grow according to two patterns:
Terrestrial orchids may be rhizomatous or form corms or tubers. The root caps of terrestrial orchids are smooth and white.

Some sympodial terrestrial orchids, such as "Orchis" and "Ophrys", have two subterranean tuberous roots. One is used as a food reserve for wintry periods, and provides for the development of the other one, from which visible growth develops.

In warm and constantly humid climates, many terrestrial orchids do not need pseudobulbs.

Epiphytic orchids, those that grow upon a support, have modified aerial roots that can sometimes be a few meters long. In the older parts of the roots, a modified spongy epidermis, called velamen, has the function to absorb humidity. It is made of dead cells and can have a silvery-grey, white or brown appearance. In some orchids, the velamen includes spongy and fibrous bodies near the passage cells, called tilosomes.

The cells of the root epidermis grow at a right angle to the axis of the root to allow them to get a firm grasp on their support. Nutrients for epiphytic orchids mainly come from mineral dust, organic detritus, animal droppings and other substances collecting among on their supporting surfaces.
The base of the stem of sympodial epiphytes, or in some species essentially the entire stem, may be thickened to form a pseudobulb that contains nutrients and water for drier periods.

The pseudobulb has a smooth surface with lengthwise grooves, and can have different shapes, often conical or oblong. Its size is very variable; in some small species of "Bulbophyllum", it is no longer than two millimeters, while in the largest orchid in the world, "Grammatophyllum speciosum" (giant orchid), it can reach three meters. Some "Dendrobium" species have long, canelike pseudobulbs with short, rounded leaves over the whole length; some other orchids have hidden or extremely small pseudobulbs, completely included inside the leaves.

With ageing, the pseudobulb sheds its leaves and becomes dormant. At this stage, it is often called a backbulb. Backbulbs still hold nutrition for the plant, but then a pseudobulb usually takes over, exploiting the last reserves accumulated in the backbulb, which eventually dies off, too. A pseudobulb typically lives for about five years. Orchids without noticeable pseudobulbs are also said to have growths, an individual component of a sympodial plant.

Like most monocots, orchids generally have simple leaves with parallel veins, although some Vanilloideae have reticulate venation. Leaves may be ovate, lanceolate, or orbiculate, and very variable in size on the individual plant. Their characteristics are often diagnostic. They are normally alternate on the stem, often folded lengthwise along the centre ("plicate"), and have no stipules. Orchid leaves often have siliceous bodies called stegmata in the vascular bundle sheaths (not present in the Orchidoideae) and are fibrous.

The structure of the leaves corresponds to the specific habitat of the plant. Species that typically bask in sunlight, or grow on sites which can be occasionally very dry, have thick, leathery leaves and the laminae are covered by a waxy cuticle to retain their necessary water supply. Shade-loving species, on the other hand, have long, thin leaves.

The leaves of most orchids are perennial, that is, they live for several years, while others, especially those with plicate leaves as in "Catasetum", shed them annually and develop new leaves together with new pseudobulbs.

The leaves of some orchids are considered ornamental. The leaves of the "Macodes sanderiana", a semiterrestrial or rock-hugging ("lithophyte") orchid, show a sparkling silver and gold veining on a light green background. The cordate leaves of "Psychopsis limminghei" are light brownish-green with maroon-puce markings, created by flower pigments. The attractive mottle of the leaves of lady's slippers from tropical and subtropical Asia ("Paphiopedilum"), is caused by uneven distribution of chlorophyll. Also, "Phalaenopsis schilleriana" is a pastel pink orchid with leaves spotted dark green and light green. The jewel orchid ("Ludisia discolor") is grown more for its colorful leaves than its white flowers.

Some orchids, as "Dendrophylax lindenii" (ghost orchid), "Aphyllorchis" and "Taeniophyllum" depend on their green roots for photosynthesis and lack normally developed leaves, as do all of the heterotrophic species.

Orchids of the genus "Corallorhiza" (coralroot orchids) lack leaves altogether and instead wrap their roots around the roots of mature trees and use specialized fungi to harvest sugars.

The Orchidaceae are well known for the many structural variations in their flowers.

Some orchids have single flowers, but most have a racemose inflorescence, sometimes with a large number of flowers. The flowering stem can be basal, that is, produced from the base of the tuber, like in "Cymbidium", apical, meaning it grows from the apex of the main stem, like in "Cattleya", or axillary, from the leaf axil, as in "Vanda".

As an apomorphy of the clade, orchid flowers are primitively zygomorphic (bilaterally symmetrical), although in some genera like "Mormodes", "Ludisia", and "Macodes", this kind of symmetry may be difficult to notice.

The orchid flower, like most flowers of monocots, has two whorls of sterile elements. The outer whorl has three sepals and the inner whorl has three petals. The sepals are usually very similar to the petals (thus called tepals, 1), but may be completely distinct.

The medial petal, called the labellum or lip (6), which is always modified and enlarged, is actually the upper medial petal; however, as the flower develops, the inferior ovary (7) or the pedicel usually rotates 180°, so that the labellum arrives at the lower part of the flower, thus becoming suitable to form a platform for pollinators. This characteristic, called resupination, occurs primitively in the family and is considered apomorphic, a derived characteristic all Orchidaceae share. The torsion of the ovary is very evident from the longitudinal section shown ("below right"). Some orchids have secondarily lost this resupination, e.g. "Epidendrum secundum".

The normal form of the sepals can be found in "Cattleya", where they form a triangle. In "Paphiopedilum" (Venus slippers), the lower two sepals are fused into a synsepal, while the lip has taken the form of a slipper. In "Masdevallia", all the sepals are fused.

Orchid flowers with abnormal numbers of petals or lips are called peloric. Peloria is a genetic trait, but its expression is environmentally influenced and may appear random.

Orchid flowers primitively had three stamens, but this situation is now limited to the genus "Neuwiedia". "Apostasia" and the Cypripedioideae have two stamens, the central one being sterile and reduced to a staminode. All of the other orchids, the clade called "Monandria", retain only the central stamen, the others being reduced to staminodes (4). The filaments of the stamens are always adnate (fused) to the style to form cylindrical structure called the gynostemium or column (2). In the primitive Apostasioideae, this fusion is only partial; in the Vanilloideae, it is more deep; in Orchidoideae and Epidendroideae, it is total. The stigma (9) is very asymmetrical, as all of its lobes are bent towards the centre of the flower and lie on the bottom of the column.

Pollen is released as single grains, like in most other plants, in the Apostasioideae, Cypripedioideae, and Vanilloideae. In the other subfamilies, which comprise the great majority of orchids, the anther (3) carries two pollinia.

A pollinium is a waxy mass of pollen grains held together by the glue-like alkaloid viscin, containing both cellulosic strands and mucopolysaccharides. Each pollinium is connected to a filament which can take the form of a caudicle, as in "Dactylorhiza" or "Habenaria", or a "stipe", as in "Vanda". Caudicles or stipes hold the pollinia to the viscidium, a sticky pad which sticks the pollinia to the body of pollinators.

At the upper edge of the stigma of single-anthered orchids, in front of the anther cap, is the rostellum (5), a slender extension involved in the complex pollination mechanism.

As mentioned, the ovary is always inferior (located behind the flower). It is three-carpelate and one or, more rarely, three-partitioned, with parietal placentation (axile in the Apostasioideae).

In 2011, "Bulbophyllum nocturnum" was discovered to flower nocturnally.

The complex mechanisms which orchids have evolved to achieve cross-pollination were investigated by Charles Darwin and described in "Fertilisation of Orchids" (1862). Orchids have developed highly specialized pollination systems, thus the chances of being pollinated are often scarce, so orchid flowers usually remain receptive for very long periods, rendering unpollinated flowers long-lasting in cultivation. Most orchids deliver pollen in a single mass. Each time pollination succeeds, thousands of ovules can be fertilized.

Pollinators are often visually attracted by the shape and colours of the labellum. However, some "Bulbophyllum" species attract male fruit flies ("Bactrocera" spp.) solely via a floral chemical which simultaneously acts as a floral reward (e.g. methyl eugenol, raspberry ketone, or zingerone) to perform pollination. The flowers may produce attractive odours. Although absent in most species, nectar may be produced in a spur of the labellum (8 in the illustration above), or on the point of the sepals, or in the septa of the ovary, the most typical position amongst the Asparagales.

In orchids that produce pollinia, pollination happens as some variant of the following sequence: when the pollinator enters into the flower, it touches a viscidium, which promptly sticks to its body, generally on the head or abdomen. While leaving the flower, it pulls the pollinium out of the anther, as it is connected to the viscidium by the caudicle or stipe. The caudicle then bends and the pollinium is moved forwards and downwards. When the pollinator enters another flower of the same species, the pollinium has taken such position that it will stick to the stigma of the second flower, just below the rostellum, pollinating it. The possessors of orchids may be able to reproduce the process with a pencil, small paintbrush, or other similar device.
Some orchids mainly or totally rely on self-pollination, especially in colder regions where pollinators are particularly rare. The caudicles may dry up if the flower has not been visited by any pollinator, and the pollinia then fall directly on the stigma. Otherwise, the anther may rotate and then enter the stigma cavity of the flower (as in "Holcoglossum amesianum").

The slipper orchid "Paphiopedilum parishii" reproduces by self-fertilization. This occurs when the anther changes from a solid to a liquid state and directly contacts the stigma surface without the aid of any pollinating agent or floral assembly.

The labellum of the Cypripedioideae is poke bonnet-shaped, and has the function of trapping visiting insects. The only exit leads to the anthers that deposit pollen on the visitor.

In some extremely specialized orchids, such as the Eurasian genus "Ophrys", the labellum is adapted to have a colour, shape, and odour which attracts male insects via mimicry of a receptive female. Pollination happens as the insect attempts to mate with flowers.

Many neotropical orchids are pollinated by male orchid bees, which visit the flowers to gather volatile chemicals they require to synthesize pheromonal attractants. Males of such species as "Euglossa imperialis" or "Eulaema meriana" have been observed to leave their territories periodically to forage for aromatic compounds, such as cineole, to synthesize pheromone for attracting and mating with females. Each type of orchid places the pollinia on a different body part of a different species of bee, so as to enforce proper cross-pollination.

A rare achlorophyllous saprophytic orchid growing entirely underground in Australia, "Rhizanthella slateri", is never exposed to light, and depends on ants and other terrestrial insects to pollinate it.

"Catasetum", a genus discussed briefly by Darwin, actually launches its viscid pollinia with explosive force when an insect touches a seta, knocking the pollinator off the flower.

After pollination, the sepals and petals fade and wilt, but they usually remain attached to the ovary.

Some species, such as "Phalaenopsis", "Dendrobium", and "Vanda", produce offshoots or plantlets formed from one of the nodes along the stem, through the accumulation of growth hormones at that point. These shoots are known as "keiki".

The ovary typically develops into a capsule that is dehiscent by three or six longitudinal slits, while remaining closed at both ends.

The seeds are generally almost microscopic and very numerous, in some species over a million per capsule. After ripening, they blow off like dust particles or spores. They lack endosperm and must enter symbiotic relationships with various mycorrhizal basidiomyceteous fungi that provide them the necessary nutrients to germinate, so all orchid species are mycoheterotrophic during germination and reliant upon fungi to complete their lifecycles.

As the chance for a seed to meet a suitable fungus is very small, only a minute fraction of all the seeds released grow into adult plants. In cultivation, germination typically takes weeks.

Horticultural techniques have been devised for germinating orchid seeds on an artificial nutrient medium, eliminating the requirement of the fungus for germination and greatly aiding the propagation of ornamental orchids. The usual medium for the sowing of orchids in artificial conditions is agar agar gel combined with a carbohydrate energy source. The carbohydrate source can be combinations of discrete sugars or can be derived from other sources such as banana, pineapple, peach, or even tomato puree or coconut water. After the preparation of the agar agar medium, it is poured into test tubes or jars which are then autoclaved (or cooked in a pressure cooker) to sterilize the medium. After cooking, the medium begins to gel as it cools.

The taxonomy of this family is in constant flux, as new studies continue to clarify the relationships between species and groups of species, allowing more taxa at several ranks to be recognized. The Orchidaceae is currently placed in the order Asparagales by the APG III system of 2009.

Five subfamilies are recognised. The cladogram below was made according to the APG system of 1998. It represents the view that most botanists had held up to that time. It was supported by morphological studies, but never received strong support in molecular phylogenetic studies.

In 2015, a phylogenetic study showed strong statistical support for the following topology of the orchid tree, using 9 kb of plastid and nuclear DNA from 7 genes, a topology that was confirmed by a phylogenomic study in the same year.

A study in the scientific journal "Nature" has hypothesised that the origin of orchids goes back much longer than originally expected. An extinct species of stingless bee, "Proplebeia dominicana", was found trapped in Miocene amber from about 15-20 million years ago. The bee was carrying pollen of a previously unknown orchid taxon, "Meliorchis caribea", on its wings. This find is the first evidence of fossilised orchids to date and shows insects were active pollinators of orchids then. This extinct orchid, "M. caribea", has been placed within the extant tribe Cranichideae, subtribe Goodyerinae (subfamily Orchidoideae). An even older orchid species, "Succinanthera baltica", was described from the Eocene Baltic amber by Poinar & Rasmussen (2017).

Genetic sequencing indicates orchids may have arisen earlier, 76 to 84 million years ago during the Late Cretaceous. According to Mark W. Chase "et al." (2001), the overall biogeography and phylogenetic patterns of Orchidaceae show they are even older and may go back roughly 100 million years.

Using the molecular clock method, it was possible to determine the age of the major branches of the orchid family. This also confirmed that the subfamily Vanilloideae is a branch at the basal dichotomy of the monandrous orchids, and must have evolved very early in the evolution of the family. Since this subfamily occurs worldwide in tropical and subtropical regions, from tropical America to tropical Asia, New Guinea and West Africa, and the continents began to split about 100 million years ago, significant biotic exchange must have occurred after this split (since the age of "Vanilla" is estimated at 60 to 70 million years).

Genome duplication occurred prior to the divergence of this taxon.

The following are amongst the most notable genera of the orchid family:

The type genus (i.e. the genus after which the family is named) is "Orchis". The genus name comes from the Ancient Greek (""), literally meaning "testicle", because of the shape of the twin tubers in some species of "Orchis". The term "orchid" was introduced in 1845 by John Lindley in "School Botany", as a shortened form of "Orchidaceae".

In Middle English, the name "bollockwort" was used for some orchids, based on "bollock" meaning testicle and "wort" meaning plant.

Orchidaceae are cosmopolitan, occurring in almost every habitat apart from glaciers. The world's richest diversity of orchid genera and species is found in the tropics, but they are also found above the Arctic Circle, in southern Patagonia, and two species of "Nematoceras" on Macquarie Island at 54° south.

The following list gives a rough overview of their distribution:

A majority of orchids are perennial epiphytes, which grow anchored to trees or shrubs in the tropics and subtropics. Species such as "Angraecum sororium" are lithophytes, growing on rocks or very rocky soil. Other orchids (including the majority of temperate Orchidaceae) are terrestrial and can be found in habitat areas such as grasslands or forest.

Some orchids, such as "Neottia" and "Corallorhiza", lack chlorophyll, so are unable to photosynthesise. Instead, these species obtain energy and nutrients by parasitising soil fungi through the formation of orchid mycorrhizas. The fungi involved include those that form ectomycorrhizas with trees and other woody plants, parasites such as "Armillaria", and saprotrophs. These orchids are known as myco-heterotrophs, but were formerly (incorrectly) described as saprophytes as it was believed they gained their nutrition by breaking down organic matter. While only a few species are achlorophyllous holoparasites, all orchids are myco-heterotrophic during germination and seedling growth, and even photosynthetic adult plants may continue to obtain carbon from their mycorrhizal fungi.

The scent of orchids is frequently analysed by perfumers (using headspace technology and gas-liquid chromatography/mass spectrometry) to identify potential fragrance chemicals.

The other important use of orchids is their cultivation for the enjoyment of the flowers. Most cultivated orchids are tropical or subtropical, but quite a few which grow in colder climates can be found on the market. Temperate species available at nurseries include "Ophrys apifera" (bee orchid), "Gymnadenia conopsea" (fragrant orchid), "Anacamptis pyramidalis" (pyramidal orchid) and "Dactylorhiza fuchsii" (common spotted orchid).

Orchids of all types have also often been sought by collectors of both species and hybrids. Many hundreds of societies and clubs worldwide have been established. These can be small, local clubs, or larger, national organisations such as the American Orchid Society. Both serve to encourage cultivation and collection of orchids, but some go further by concentrating on conservation or research.

The term "botanical orchid" loosely denotes those small-flowered, tropical orchids belonging to several genera that do not fit into the "florist" orchid category. A few of these genera contain enormous numbers of species. Some, such as "Pleurothallis" and "Bulbophyllum", contain approximately 1700 and 2000 species, respectively, and are often extremely vegetatively diverse. The primary use of the term is among orchid hobbyists wishing to describe unusual species they grow, though it is also used to distinguish naturally occurring orchid species from horticulturally created hybrids.

New orchids are registered with the International Orchid Register, maintained by the Royal Horticultural Society.

The dried seed pods of one orchid genus, "Vanilla" (especially "Vanilla planifolia"), are commercially important as a flavouring in baking, for perfume manufacture and aromatherapy.

The underground tubers of terrestrial orchids [mainly "Orchis mascula" (early purple orchid)] are ground to a powder and used for cooking, such as in the hot beverage "salep" or in the Turkish frozen treat "dondurma". The name "salep" has been claimed to come from the Arabic expression ', "fox testicles", but it appears more likely the name comes directly from the Arabic name '. The similarity in appearance to testes naturally accounts for "salep" being considered an aphrodisiac.

The dried leaves of "Jumellea fragrans" are used to flavour rum on Reunion Island.

Some saprophytic orchid species of the group "Gastrodia" produce potato-like tubers and were consumed as food by native peoples in Australia and can be successfully cultivated, notably "Gastrodia sesamoides". Wild stands of these plants can still be found in the same areas as early aboriginal settlements, such as Ku-ring-gai Chase National Park in Australia. Aboriginal peoples located the plants in habitat by observing where bandicoots had scratched in search of the tubers after detecting the plants underground by scent.

Orchids have been used in traditional medicine in an effort to treat many diseases and ailments. They have been used as a source of herbal remedies in China since 2800 BC. "Gastrodia elata" is one of the three orchids listed in the earliest known Chinese Materia Medica ("Shennon bencaojing") (c. 100 AD). Theophrastus mentions orchids in his "Enquiry into Plants" (372–286 BC).

Orchids have many associations with symbolic values. For example, the orchid is the City Flower of Shaoxing, China. "Cattleya mossiae" is the national Venezuelan flower, while "Cattleya trianae" is the national flower of Colombia. "Vanda" 'Miss Joaquim' is the national flower of Singapore, "Guarianthe skinneri" is the national flower of Costa Rica and "Rhyncholaelia digbyana" is the national flower of Honduras. "Prosthechea cochleata" is the national flower of Belize, where it is known as the "black orchid". "Lycaste skinneri" has a white variety (alba) which is the national flower of Guatemala, commonly known as "Monja Blanca" (White Nun). Panama's national flower is the "Holy Ghost orchid" ("Peristeria elata"), or 'the flor del Espiritu Santo'.

Orchids native to the Mediterranean are depicted on the "Ara Pacis" in Rome, until now the only known instance of orchids in ancient art, and the earliest in European art.



</doc>
<doc id="22721" url="https://en.wikipedia.org/wiki?curid=22721" title="Obsidian">
Obsidian

Obsidian is a naturally occurring volcanic glass formed as an extrusive igneous rock.

Obsidian is produced when felsic lava extruded from a volcano cools rapidly with minimal crystal growth. It is commonly found within the margins of rhyolitic lava flows known as obsidian flows, where the chemical composition (high silica content) induces a high degree of viscosity and polymerization of the lava. The inhibition of atomic diffusion through this highly viscous and polymerized lava explains the lack of crystal growth. Obsidian is hard and brittle and therefore fractures with very sharp edges. In the past it was used to manufacture cutting and piercing tools and it has been used experimentally as surgical scalpel blades.

The translation into English of "Natural History" written by Pliny the Elder of Rome shows a few sentences on the subject of a volcanic glass called obsidian ("lapis obsidianus"), discovered in Ethiopia by Obsidius, a Roman explorer.

Obsidian is the rock formed as a result of quickly cooled lava, which is the parent material. Extrusive formation of obsidian may occur when felsic lava cools rapidly at the edges of a felsic lava flow or volcanic dome or when lava cools during sudden contact with water or air. Intrusive formation of obsidian may occur when felsic lava cools along the edges of a dike.

Tektites were once thought by many to be obsidian produced by lunar volcanic eruptions, though few scientists now adhere to this hypothesis.

Obsidian is mineral-like, but not a true mineral because as a glass it is not crystalline; in addition, its composition is too variable to be classified as a mineral. It is sometimes classified as a mineraloid. Though obsidian is usually dark in color, similar to mafic rocks such as basalt, obsidian's composition is extremely felsic. Obsidian consists mainly of SiO (silicon dioxide), usually 70% or more. Crystalline rocks with obsidian's composition include granite and rhyolite. Because obsidian is metastable at the Earth's surface (over time the glass becomes fine-grained mineral crystals), no obsidian has been found that is older than Cretaceous age. This breakdown of obsidian is accelerated by the presence of water. Having a low water content when newly formed, typically less than 1% water by weight, obsidian becomes progressively hydrated when exposed to groundwater, forming perlite.

Pure obsidian is usually dark in appearance, though the color varies depending on the presence of impurities. Iron and other transition elements may give the obsidian a dark brown to black color. Very few samples are nearly colorless. In some stones, the inclusion of small, white, radially clustered crystals of cristobalite in the black glass produce a blotchy or snowflake pattern ("snowflake obsidian"). Obsidian may contain patterns of gas bubbles remaining from the lava flow, aligned along layers created as the molten rock was flowing before being cooled. These bubbles can produce interesting effects such as a golden sheen ("sheen obsidian"). An iridescent, rainbow-like sheen ("rainbow obsidian") is caused by inclusions of magnetite nanoparticles.

Obsidian can be found in locations which have experienced rhyolitic eruptions. It can be found in Argentina, Armenia, Azerbaijan, Australia, Canada, Chile, Georgia, Greece, El Salvador, Guatemala, Iceland, Italy, Japan, Kenya, Mexico, New Zealand, Papua New Guinea, Peru, Scotland, Turkey and the United States. Obsidian flows which may be hiked on are found within the calderas of Newberry Volcano and Medicine Lake Volcano in the Cascade Range of western North America, and at Inyo Craters east of the Sierra Nevada in California. Yellowstone National Park has a mountainside containing obsidian located between Mammoth Hot Springs and the Norris Geyser Basin, and deposits can be found in many other western U.S. states including Arizona, Colorado, New Mexico, Texas, Utah, Washington, Oregon and Idaho. Obsidian can also be found in the eastern U.S. states of Virginia, as well as Pennsylvania and North Carolina.

There are only four major deposit areas in the central Mediterranean: Lipari, Pantelleria, Palmarola and Monte Arci.

Ancient sources in the Aegean were Milos and Gyali.

Acıgöl town and the Göllü Dağ volcano were the most important sources in central Anatolia, one of the more important source areas in the prehistoric Near East.

The first known archaeological evidence of usage was in Kariandusi and other sites of the Acheulian age (beginning 1.5 million years BP) dated 700,000 BC, although the number of objects found at these sites were very low relative to the Neolithic. Use of obsidian in pottery of the Neolithic in the area around Lipari was found to be significantly less at a distance representing two weeks journeying. Anatolian sources of obsidian are known to have been the material used in the Levant and modern-day Iraqi Kurdistan from a time beginning sometime about 12,500 BC. The first attested civilized use is dated to the late fifth millennium BC, known from excavations at Tell Brak. Obsidian was valued in Stone Age cultures because, like flint, it could be fractured to produce sharp blades or arrowheads. Like all glass and some other types of naturally occurring rocks, obsidian breaks with a characteristic conchoidal fracture. It was also polished to create early mirrors. Modern archaeologists have developed a relative dating system, obsidian hydration dating, to calculate the age of obsidian artifacts.

In the Ubaid in the 5th millennium BC, blades were manufactured from obsidian extracted from outcrops located in modern-day Turkey. Ancient Egyptians used obsidian imported from the eastern Mediterranean and southern Red Sea regions. Obsidian was also used in ritual circumcisions because of its deftness and sharpness. In the eastern Mediterranean area the material was used to make tools, mirrors and decorative objects.

Obsidian has also been found in Gilat, a site in the western Negev in Israel. Eight obsidian artifacts dating to the Chalcolithic Age found at this site were traced to obsidian sources in Anatolia. Neutron activation analysis (NAA) on the obsidian found at this site helped to reveal trade routes and exchange networks previously unknown.

Lithic analysis can be instrumental in understanding prehispanic groups in Mesoamerica. A careful analysis of obsidian in a culture or place can be of considerable use to reconstruct commerce, production, distribution and thereby understand economic, social and political aspects of a civilization. This is the case in Yaxchilán, a Maya city where even warfare implications have been studied linked with obsidian use and its debris. Another example is the archeological recovery at coastal Chumash sites in California indicating considerable trade with the distant site of Casa Diablo, California in the Sierra Nevada Mountains.
Pre-Columbian Mesoamericans' use of obsidian was extensive and sophisticated; including carved and worked obsidian for tools and decorative objects. Mesoamericans also made a type of sword with obsidian blades mounted in a wooden body. Called a "macuahuitl", the weapon was capable of inflicting terrible injuries, combining the sharp cutting edge of an obsidian blade with the ragged cut of a serrated weapon. The pole arm version of this weapon was called "tepoztopilli".
Native American people traded obsidian throughout the Americas. Each volcano and in some cases each volcanic eruption produces a distinguishable type of obsidian, making it possible for archaeologists to trace the origins of a particular artifact. Similar tracing techniques have allowed obsidian to be identified in Greece also as coming from Milos, Nisyros or Gyali, islands in the Aegean Sea. Obsidian cores and blades were traded great distances inland from the coast.

In Chile obsidian tools from Chaitén Volcano have been found as far away as in Chan-Chan north of the volcano and also in sites 400 km south of it.

Obsidian was also used on Rapa Nui (Easter Island) for edged tools such as "Mataia" and the pupils of the eyes of their Moai (statues), which were encircled by rings of bird bone. Obsidian was used to inscribe the Rongorongo glyphs.

Obsidian can be used to make extremely sharp knives, and obsidian blades are a type of glass knife made using naturally occurring obsidian instead of manufactured glass. Obsidian is used by some surgeons for scalpel blades, although this is not approved by the US Food and Drug Administration (FDA) for use on humans. Well-crafted obsidian blades, as with any glass knife, can have a cutting edge many times sharper than high-quality steel surgical scalpels, the cutting edge of the blade being only about 3 nanometers thick. Even the sharpest metal knife has a jagged, irregular blade when viewed under a strong enough microscope; when examined even under an electron microscope an obsidian blade is still smooth and even. One study found that obsidian incisions produced fewer inflammatory cells and less granulation tissue at seven days, in a group of rats, although no differences were found after 21 days. Don Crabtree produced obsidian blades for surgery and other purposes, and has written articles on the subject. Obsidian scalpels may currently be purchased for surgical use on research animals.
Obsidian is also used for ornamental purposes and as a gemstone. It presents a different appearance depending on how it is cut: in one direction it is jet black, while in another it is glistening gray. "Apache tears" are small rounded obsidian nuggets often embedded within a grayish-white perlite matrix.

Plinths for audio turntables have been made of obsidian since the 1970s; e.g. the grayish-black SH-10B3 plinth by Technics.




</doc>
<doc id="22722" url="https://en.wikipedia.org/wiki?curid=22722" title="Otaku">
Otaku

Otaku subculture is a central theme of various anime and manga works, documentaries and academic research. The subculture began in the 1980s as changing social mentalities and the nurturing of otaku traits by Japanese schools combined with the resignation of such individuals to become social outcasts. The subculture's birth coincided with the anime boom, after the release of works such as "Mobile Suit Gundam" before it branched into Comic Market. The definition of otaku subsequently became more complex, and numerous classifications of otaku emerged. In 2005, the Nomura Research Institute divided otaku into twelve groups and estimated the size and market impact of each of these groups. Other institutions have split it further or focus on a single otaku interest. These publications classify distinct groups including anime, manga, camera, automobile, idol and electronics otaku. The economic impact of otaku has been estimated to be as high as ¥2 trillion ($18 billion).
"Otaku" is derived from a Japanese term for another person's house or family ( "otaku"). This word is often used metaphorically, as an honorific second-person pronoun. In this usage, its literal translation is "you". For example, early in the anime "Macross", first aired in 1982, the characters Hikaru Ichijyo and Lynn Minmay use the term this way to address one another, until they get to know each other better. The modern slang form, which is distinguished from the older usage by being written only in hiragana (おたく), katakana (オタク or, less frequently, ヲタク) or rarely in rōmaji, first appeared in public discourse in the 1980s, through the work of humorist and essayist Akio Nakamori. His 1983 series , printed in the lolicon magazine "Manga Burikko", applied the term to unpleasant fans in caricature. Animators Haruhiko Mikimoto and Shōji Kawamori had used the term between themselves as an honorific second-person pronoun since the late 1970s. Supposedly, some fans used it past the point in their relationships where others would have moved on to a less formal style. Because this misuse indicated social awkwardness, Nakamori chose the word itself to label the fans. Morikawa Kaichirō, an author and lecturer at Meiji University, identified this as the origin of its contemporary usage.

Another claim for the origin of the term comes from the works of science fiction author Motoko Arai, who used the word in her novels as a second-person pronoun and the readers adopted the term for themselves. However, a different claim points to a 1981 "Variety" magazine essay.

In 1989, the case of Tsutomu Miyazaki, "The Otaku Murderer", brought the fandom, very negatively, to national attention. Miyazaki, who randomly chose and murdered four girls, had a collection of 5,763 video tapes, some containing anime and slasher films that were found interspersed with videos and pictures of his victims. Later that year, the contemporary knowledge magazine "Bessatsu Takarajima" dedicated its 104th issue to the topic of otaku. It was called and delved into the subculture of otaku with 19 articles by otaku insiders, among them Akio Nakamori. This publication has been claimed by scholar Rudyard Pesimo to have popularized the term.

In modern Japanese slang, the term "otaku" is mostly equivalent to "geek" or "nerd", but in a more derogatory manner than used in the West. However, it can relate to any fan of any particular theme, topic, hobby or form of entertainment. "When these people are referred to as otaku, they are judged for their behaviors - and people suddenly see an “otaku” as a person unable to relate to reality". The word entered English as a loanword from the Japanese language. It is typically used to refer to a fan of anime/manga but can also refer to Japanese video games or Japanese culture in general. The American magazine "Otaku USA" popularizes and covers these aspects. The usage of the word is a source of contention among some fans, owing to its negative connotations and stereotyping of the fandom. Widespread English exposure to the term came in 1988 with the release of "Gunbuster", which referred to anime fans as otaku. Gunbuster was released officially in English in March 1990. The term's usage spread throughout rec.arts.anime with discussions about Otaku no Video's portrayal of otaku before its 1994 English release. Positive and negative aspects, including the pejorative usage, were intermixed. The term was also popularized by William Gibson's 1996 novel "Idoru", which references otaku.

Morikawa Kaichirō identifies the subculture as distinctly Japanese, a product of the school system and society. Japanese schools have a class structure which functions as a caste system, but clubs are an exception to the social hierarchy. In these clubs, a student's interests will be recognized and nurtured, catering to the interests of otaku. Secondly, the vertical structure of Japanese society identifies the value of individuals by their success. Until the late 1980s, unathletic and unattractive males focused on academics, hoping to secure a good job and marry to raise their social standing. Those unable to succeed socially focused instead on their interests, often into adulthood, with their lifestyle centering on those interests, furthering the creation of the otaku subculture.

Even prior to the coinage of the term, the stereotypical traits of the subculture were identified in a 1981 issue of "Fan Rōdo" (Fan road) about "culture clubs". These individuals were drawn to anime, a counter-culture, with the release of hard science fiction works such as "Mobile Suit Gundam". These works allowed a congregation and development of obsessive interests that turned anime into a medium for unpopular students, catering to obsessed fans. After these fans discovered Comic Market, the term was used as a self-confirming and self-mocking collective identity.

The 1989 "Otaku Murderer" case gave a negative connotation to the fandom from which it has not fully recovered. The usage of "(interest) otaku", however, is used for teasing or self-deprecation, but the unqualified term remains negative. The identification of otaku turned negative in late 2004 when Kaoru Kobayashi kidnapped, sexually assaulted, and murdered a seven-year-old first-grade student. Japanese journalist Akihiro Ōtani suspected that Kobayashi's crime was committed by a member of the figure moe zoku even before his arrest. Although Kobayashi was not an otaku, the degree of social hostility against otaku increased. Otaku were seen by law enforcement as possible suspects for sex crimes, and local governments called for stricter laws controlling the depiction of eroticism in otaku materials.

Not all attention has been negative. In his book, "Otaku", Hiroki Azuma observed: "Between 2001 and 2007, the otaku forms and markets quite rapidly won social recognition in Japan", citing the fact that "[i]n 2003, Hayao Miyazaki won the Academy Award for his "Spirited Away"; around the same time Takashi Murakami achieved recognition for otaku-like designs; in 2004, the Japanese pavilion in the 2004 International Architecture exhibition of the Venice Biennale (Biennale Architecture) featured “otaku”. In 2005, the word "moe" - one of the keywords of the present volume - was chosen as one of the top ten “buzzwords of the year." The former Prime Minister of Japan Taro Aso has also claimed to be an otaku, using this subculture to promote Japan in foreign affairs. In 2013, a Japanese study of 137,734 people found that 42.2% self-identify as a type of otaku. This study suggests that the stigma of the word has vanished, and the term has been embraced by many.

The district of Akihabara in Tokyo, where there are maid cafés featuring waitresses who dress up and act like maids or anime characters, is a notable attraction center for otaku. Akihabara also has dozens of stores specializing in anime, manga, retro video games, figurines, card games and other collectibles. Another popular location is Otome Road in Ikebukuro, Tokyo. In Nagoya, students from Nagoya City University started a project on ways to help promote hidden tourist attractions related to the otaku culture to attract more otaku to the city.

There are specific terms for different types of otaku, including , a self-mockingly pejorative Japanese term for female fans of yaoi, which focuses on homosexual male relationships. "Reki-jo" are female otaku who are interested in Japanese history. Some terms refer to a location, such as "Akiba-kei", a slang term meaning "Akihabara-style" which applies to those familiar with Akihabara's culture. Another is , a type of cheering that is part of Akiba-kei. Other terms, such as , literally "painful car", describe vehicles who are decorated with fictional characters, especially bishōjo game or eroge characters.

Otaku often participate in self-mocking through the production or interest in humor directed at their subculture. Anime and manga otaku are the subject of numerous self-critical works, such as Otaku no Video, which contains a live-interview mockumentary that pokes fun at the otaku subculture and includes Gainax's own staff as the interviewees. Other works depict otaku subculture less critically, such as "Genshiken" and "Comic Party". A well-known novel-cum-manga-cum-anime is "Welcome to the N.H.K.", which focuses on the subcultures popular with otaku and highlights other social outcasts such as the hikikomori and NEETs. Works that focus on an otaku character include "WataMote", the story of an unattractive and unsociable otome game otaku who exhibits delusions about her social status. Watamote is a self-mocking insight that follows the heroine's delusion and attempts to reform herself only by facing reality with comedic results on the path to popularity. An American documentary, "Otaku Unite!", focuses on the American side of the otaku culture.

The Nomura Research Institute (NRI) has made two major studies into otaku, the first in 2004 and a revised study with a more specific definition in 2005. The 2005 study defines twelve major fields of otaku interests. Of these groups, manga (Japanese comics) was the largest, with 350,000 individuals and ¥83 billion market scale. Idol otaku were the next largest group, with 280,000 individuals and ¥61 billion. Travel otaku with 250,000 individuals and ¥81 billion. PC otaku with 190,000 individuals and ¥36 billion. Video game otaku with 160,000 individuals and ¥21 billion. Automobile otaku with 140,000 individuals and ¥54 billion. Animation (anime) otaku with 110,000 individuals and ¥20 billion. The remaining five categories include Mobile IT equipment otaku, with 70,000 individuals and ¥8 billion; Audio-visual equipment otaku, with 60,000 individuals and ¥12 billion; camera otaku, with 50,000 individuals and ¥18 billion; fashion otaku, with 40,000 individuals and ¥13 billion; and railway otaku, with 20,000 individuals and ¥4 billion. These values were partially released with a much higher estimation in 2004, but this definition focused on the consumerism and not the "unique psychological characteristics" of otaku used in the 2005 study.

NRI's 2005 study also put forth five archetypes of otaku. The first is the family-oriented otaku, who has broad interests and is more mature than other otaku; their object of interest is secretive and they are "closet otaku". The second is the serious "leaving my own mark on the world" otaku, with interests in mechanical or business personality fields. The third type is the "media-sensitive multiple interest" otaku, whose diverse interests are shared with others. The fourth type is the "outgoing and assertive otaku", who gain recognition by promoting their hobby. The last is the "fan magazine-obsessed otaku", which is predominately female with a small group of males being the "moe type"; the secret hobby is focused on the production or interest in fan works. The Hamagin Research Institute found that moe-related content was worth ¥88.8 billion ($807 million) in 2005, and one analyst estimated the market could be as much as ¥2 trillion ($18 billion). Japan based "Tokyo Otaku Mode" a place for news relating to Otaku has been liked on Facebook almost 10 million times.

Other classifications of otaku interests include vocaloid, cosplay, figures and professional wrestling as categorized by the Yano Research Institute. Yano Research reports and tracks market growth and trends in sectors heavily influenced by otaku consumerism. In 2012, it noted around 30% growth in dating sim and online gaming otaku, while vocaloid, cosplay, idols and maid services grew by 10%, confirming its 2011 predictions.




</doc>
<doc id="22723" url="https://en.wikipedia.org/wiki?curid=22723" title="Object-modeling language">
Object-modeling language

An object-modeling language is a standardized set of symbols used to model a software system using an object-oriented framework. The symbols can be either informal or formal ranging from predefined graphical templates to formal object models defined by grammars and specifications.

A modeling language is usually associated with a methodology for object-oriented development. The modeling language defines the elements of the model. E.g., that a model has classes, methods, object properties, etc. The methodology defines the steps developers and users need to take to develop and maintain a software system. Steps such as "Define requirements", "Develop code", and "Test system". 

It is common to equate the modeling language and the modeling methodology. For example, the Booch method may refer to Grady Booch's standard for diagramming, his methodology, or both. Or the Rumbaugh Object Modeling Technique is both a set of diagrams and a process model for developing object-oriented systems.

In the early years of the object-oriented community there were several competing modeling and methodology standards. Booch and Rumbaugh were two of the most popular. Ivar Jacobson's Objectory, Shlaer-Mellor,and Yourdon-Coad were also popular.

However, the object-oriented community values re-use and standardization. As shown in the graphic there were efforts starting in the mid '90's to reconcile the leading models and focus on one unified specification. The graphic shows the evolution of one of the most important object modeling language standards: the Unified Modeling Language (UML).

The UML began as an attempt by some of the major thought leaders in the community to define a standard language at the OOPSLA '95 Conference. Originally, Grady Booch and James Rumbaugh merged their models into a unified model. This was followed by Booch's company Rational Software purchasing Ivar Jacobson's Objectory company and merging their model into the UML. At the time Rational and Objectory were two of the dominant players in the small world of independent vendors of Object-Oriented tools and methods.

The Object Management Group then picked up and took over ownership of the UML. The OMG is one of the most influential standards organizations in the object-oriented world. The UML is both a formal metamodel and a collection of graphical templates. The meta-model defines the elements in an object-oriented model such as classes and properties. It is essentially the same thing as the meta-model in object-oriented languages such as Smalltalk or CLOS. However, in those cases the meta-model is meant primarily to be used by developers at run time to dynamically inspect and modify an application object model. The UML meta-model provides a mathematical formal foundation for the various graphic views used by the modeling language to describe an emerging system.

The following diagram illustrates the class hierarchy of the various graphic templates defined by the UML. "Structure diagrams" define the static structure of an object: its place in the class hierarchy, its relation to other objects, etc. "Behavior diagrams" specify the dynamic aspects of the model, business process logic, coordination and timing of distributed objects, etc. 


</doc>
<doc id="22725" url="https://en.wikipedia.org/wiki?curid=22725" title="On Fairy-Stories">
On Fairy-Stories

"On Fairy-Stories" is an essay by J. R. R. Tolkien which discusses the fairy-story as a literary form. It was initially written (and entitled simply "Fairy Stories") for presentation by Tolkien as the Andrew Lang lecture at the University of St Andrews, Scotland, in 1939.

In the lecture, Tolkien chose to focus on Andrew Lang’s work as a folklorist and collector of fairy tales. He disagreed with Lang's broad inclusion in his Fairy Books collection (1889–1910), of traveller's tales, beast fables, and other types of stories. Tolkien held a narrower perspective, viewing fairy stories as those that took place in "Faerie", an enchanted realm, with or without fairies as characters. He disagreed with both Max Müller and Andrew Lang in their respective theories of the development of fairy stories, which he viewed as the natural development of the interaction of human imagination and human language.

The essay first appeared in print, with some enhancement, in 1947, in a festschrift volume, "Essays Presented to Charles Williams", compiled by C. S. Lewis. Charles Williams, a friend of Lewis's, had been relocated with the Oxford University Press staff from London to Oxford during the London blitz in World War II. This allowed him to participate in gatherings of the Inklings with Lewis and Tolkien. The volume of essays was intended to be presented to Williams upon the return of the OUP staff to London with the ending of the war. However, Williams died suddenly on 15 May 1945, and the book was published as a memorial volume. "Essays Presented to Charles Williams" received little attention, and was out of print by 1955.

"On Fairy-Stories" received much more attention beginning in 1964 when it was published in "Tree and Leaf". Since then "Tree and Leaf" has been reprinted several times, and "On Fairy-Stories" itself has been reprinted in other compilations of Tolkien's works, such as "The Tolkien Reader" in 1966 and "The Monsters and the Critics, and Other Essays" in 1983 (see #Publication history below). "On Fairy Stories" was published on its own in an expanded edition in 2008. The length of the essay, as it appears in "Tree and Leaf", is 60 pages, including about ten pages of notes.

The essay is significant because it contains Tolkien's explanation of his philosophy on fantasy and thoughts on mythopoiesis. Moreover, the essay is an early analysis of speculative fiction by one of the most important authors in the genre.

Tolkien had not intended to write a sequel to "The Hobbit". The Lang lecture was important as it brought him to clarify for himself his view of fairy stories as a legitimate literary genre, and one not intended exclusively for children. "It is a deeply perceptive commentary on the interdependence of language and human consciousness."

Tolkien was among the pioneers of the genre that we would now call fantasy writing. In particular, his stories—together with those of C. S. Lewis—were among the first to establish the convention of an alternative world or universe as the setting for speculative fiction. Most earlier works with styles similar to Tolkien's, such as the science fiction of H. G. Wells or the Gothic romances of Mary Shelley, were set in a world that is recognisably that of the author and introduced only a single fantastic element—or at most a fantastic milieu within the author's world, as with Lovecraft or Howard. Tolkien departed from this; his work was nominally part of the history of our own world, but did not have the close linkage to history or contemporary times that his precursors had.

The essay "On Fairy-Stories" is an attempt to explain and defend the genre of fairy tales or "Märchen". It distinguishes "Märchen" from "traveller's tales" (such as "Gulliver's Travels"), science fiction (such as H. G. Wells's "The Time Machine"), beast tales (such as Aesop's Fables and "Peter Rabbit"), and dream stories (such as "Alice in Wonderland"). One touchstone of the authentic fairy tale is that it is presented as wholly credible. "It is at any rate essential to a genuine fairy-story, as distinct from the employment of this form for lesser or debased purposes, that it should be presented as 'true.' ...But since the fairy-story deals with 'marvels,' it cannot tolerate any frame or machinery suggesting that the whole framework in which they occur is a figment or illusion."

Tolkien emphasises that through the use of fantasy, which he equates with imagination, the author can bring the reader to experience a world which is consistent and rational, under rules other than those of the normal world. He calls this "a rare achievement of Art," and notes that it was important to him as a reader: "It was in fairy-stories that I first divined the potency of the words, and the wonder of things, such as stone, and wood, and iron; tree and grass; house and fire; bread and wine."

Tolkien suggests that fairy stories allow the reader to review his own world from the "perspective" of a different world. Tolkien calls this "recovery", in the sense that one's unquestioned assumptions might be recovered and changed by an outside perspective. Second, he defends fairy stories as offering escapist pleasure to the reader, justifying this analogy: a prisoner is not obliged to think of nothing but cells and wardens. And third, Tolkien suggests that fairy stories can provide moral or emotional consolation, through their happy ending, which he terms a "eucatastrophe".

In conclusion and as expanded upon in an epilogue, Tolkien asserts that a truly good and representative fairy story is marked by joy: "Far more powerful and poignant is the effect [of joy] in a serious tale of Faerie. In such stories, when the sudden turn comes, we get a piercing glimpse of joy, and heart's desire, that for a moment passes outside the frame, rends indeed the very web of story, and lets a gleam come through." Tolkien sees Christianity as partaking in and fulfilling the overarching mythological nature of the cosmos: "I would venture to say that approaching the Christian story from this perspective, it has long been my feeling (a joyous feeling) that God redeemed the corrupt making-creatures, men, in a way fitting to this aspect, as to others, of their strange nature. The Gospels contain a fairy-story, or a story of a larger kind which embraces all the essence of fairy-stories. ...and among its marvels is the greatest and most complete conceivable eucatastrophe. The Birth of Christ is the eucatastrophe of Man's history. The Resurrection is the eucatastrophe of the story of the Incarnation."

In his essay, Tolkien cites a large range of works by other authors: fiction, mythology and academic works. The fiction and mythology includes:
Tolkien also quotes from his own poem "Mythopoeia".




</doc>
<doc id="22727" url="https://en.wikipedia.org/wiki?curid=22727" title="Otaku no Video">
Otaku no Video

The story begins in "Otaku no Video 1982", where the main character is an everyman character, Ken Kubo, living with his girlfriend Yoshiko and as a member of his college's tennis team, until introduced by his former friend Tanaka to a club of enthusiasts: a female illustrator, an information geek, a martial artist, and a weapons collector. Kubo soon joins them; and when Yoshiko abandons him, makes the wish to become the supreme enthusiast, under the name of "Otaking".

Kubo's quest continues in "More Otaku no Video 1985", set three years later, in which he creates his model kits, opens shops, and builds a factory in China. Later, he loses his fortune when one of his rivals (now married to Yoshiko) takes control of his enterprise; but Kubo and Tanaka, with hard-working artist Misuzu, gradually take over the anime industry with a 'magical girl' show, "Misty May". At the peak of their ambitions, Ken and Tanaka create Otakuland in 1999: the equivalent of Disneyland for otaku (the story suggests Otakuland to be located in the same city of Urayasu, Chiba Prefecture, as the original Tokyo Disneyland.)

Many years later, Ken and Tanaka return to Otakuland in a post-apocalyptic submerged Japan and find its central structure, a giant robot, converted into a functional spaceship piloted by their old friends. Miraculously rejuvenated, they fly into space in search of "The Planet of Otaku".

A controversial and humorous part of "Otaku no Video" was the inclusion of live-action documentary excerpts, titled "A Portrait of an Otaku". In these segments, the documentary crew would interview an anonymous otaku, typically ashamed at being a fan and whose face are censored with a mosaic and have their voices digitally masked. The mock documentary segments serve as a counterpoint to the anime: while the anime emphasizes the camaradrie, creativity, and dreams of mainstream acceptance of otaku, the mock interviews exaggerate its negative qualities. The subjects run the gamut of the otaku subculture: the interviews cover a cosplayer who now works as a computer programmer and outright denies his cosplay days, even when presented with photographic evidence, but keeps his Char Aznable helmet in his desk drawer, an airsoft otaku, a garage kit otaku, and a shut-in who videorecords television programs for trade, but has not actually watched anything he's recorded. The interviews also contain fans who engage in a range of illicit or unsavory activities, such as cel thieves, a pornography fan attempting to manufacture glasses to defeat the mosaic censorship common in Japanese porn videos and who is shown masturbating during the interview, and a computer gamer—famous Gainax member Hideaki Anno—obsessed with a character in a hentai computer game (Noriko from "Gunbuster"—one of Anno's works—who makes a cameo in Gainax's own hentai game, "Cybernetic High School").

It is believed that all the subjects in the Portrait of an Otaku segments were Gainax employees or connected to Gainax at the time of filming. The first otaku interviewed bore a remarkable resemblance to Toshio Okada, a principal founder in Gainax, in both background and physical appearance. The gaijin otaku, Shon Hernandez, has been confirmed to have been Craig York, who with Shon Howell and Lea Hernandez, whose names were borrowed for the character, were the main staff of General Products USA, an early western branch of Gainax's merchandising enterprise in the early 1990s. The interview with "Shon Hernandez" has been a point of contention with Lea Hernandez, who, in an interview with "PULP" magazine, noted that the interview was unscripted and that Craig York had been fairly sincere in his thoughts and had felt that Gainax insulted their American members. In the interview, the words spoken by Shon Hernandez in the background are noticeably different from what is shown on screen via subtitle (which is based on the Japanese voice-over "translation").

At FanimeCon 2003, Hiroshi Sato, an animator and another Gainax member, mentioned that he had been in one of the interviews in "Otaku no Video". In "Otaku no Video", the garage kit otaku was given the pseudonym "Sato Hiroshi" for the interview.

Since "Otaku no Video" was partially based in the personal life of the original creators of Gainax, who started their careers as otaku during the late seventies and the beginning of the eighties, many anime titles from that period are shown as footage or referenced in the OVA (in costumes, cosplay or other related material). Among them are "Gatchaman", "Uchuu Senkan Yamato", "Urusei Yatsura", "Captain Harlock", "Mobile Suit Gundam", "Dirty Pair", "Space Adventure Cobra", "Phoenix 2772", "Silent Möbius", "Magical Princess Minky Momo", "The Super Dimension Fortress Macross", "", "Genesis Climber Mospeada", "The Wings of Honneamise", "Top o Nerae!" and the "Daicon III and IV Opening Animations".

"Otaku no Video" was released with subtitles on VHS in North America on March 17, 1993, on DVD on April 2, 2002, and on Blu-ray Disc on June 24, 2016, all by AnimEigo.




</doc>
<doc id="22735" url="https://en.wikipedia.org/wiki?curid=22735" title="Original sin">
Original sin

Original sin, also called "ancestral sin", is a Christian belief of the state of sin in which humanity exists since the fall of man, stemming from Adam and Eve's rebellion in Eden, namely the sin of disobedience in consuming the forbidden fruit from the tree of the knowledge of good and evil. This condition has been characterized in many ways, ranging from something as insignificant as a slight deficiency, or a tendency toward sin yet without collective guilt, referred to as a "sin nature", to something as drastic as total depravity or automatic guilt of all humans through collective guilt.

The concept of original sin was first alluded to in the 2nd century by Irenaeus, Bishop of Lyon in his controversy with certain dualist Gnostics. Other church fathers such as Augustine also developed the doctrine, seeing it as based on the New Testament teaching of Paul the Apostle (Romans and 1 Corinthians ) and the Old Testament verse of Psalms . Tertullian, Cyprian, Ambrose and Ambrosiaster considered that humanity shares in Adam's sin, transmitted by human generation. Augustine's formulation of original sin was popular among Protestant reformers, such as Martin Luther and John Calvin, who equated original sin with concupiscence (or "hurtful desire"), affirming that it persisted even after baptism and completely destroyed freedom, although Augustine said that free will was weakened but not destroyed by original sin. The Jansenist movement, which the Catholic Church declared to be heretical, also maintained that original sin destroyed freedom of will. Instead the Catholic Church declares "Baptism, by imparting the life of Christ's grace, erases original sin and turns a man back towards God, but the consequences for nature, weakened and inclined to evil, persist in man and summon him to spiritual battle." "Weakened and diminished by Adam's fall, free will is yet not destroyed in the race"

The doctrine of ancestral fault ( "progonikon hamartema"), i.e. the sins of the forefathers leading to punishment of their descendants, was presented as a tradition of immemorial antiquity in ancient Greek religion by Celsus in his "True Doctrine", a polemic "attacking" Christianity.
Celsus is quoted as attributing to "a priest of Apollo or of Zeus" the saying that "the mills of the gods grind slowly, even to children's children, and to those who are born after them". The idea of divine justice taking the form of collective punishment is also ubiquitous in the Hebrew Bible.

St Paul's idea of redemption hinged upon the contrast between the sin of Adam and the death and resurrection of Jesus. "Therefore, just as sin entered the world through one man, and death through sin, and in this way death came to all people, because all sinned." "For as in Adam all die, so in Christ all will be made alive." Up till then the transgression in the Garden of Eden had not been given great significance. As the Jesus scholar, Geza Vermes has said:

The formalized Christian doctrine of original sin was first developed in the 2nd century by Irenaeus, the Bishop of Lyon, in his struggle against Gnosticism. Irenaeus contrasted their doctrine with the view that the Fall was a step in the wrong direction by Adam, with whom, Irenaeus believed, his descendants had some solidarity or identity. However, Irenaeus did not believe that Adam's sin had tremendously grave consequences for humanity as the later tradition would hold, nor that his sin was the source of universal human sinfulness. That all human beings participate in Adam's sin and share his guilt are totally foreign concepts for Irenaeus; Adam's sin belonged to Adam alone. Adam in his transgression is likened to a child who merely partook of the tree ahead of his time. For Irenaeus, knowing good an evil was an integral aspect of human nature; the 'sin' of Adam was snatching at the fruit of the tree rather than waiting for it as a gift from God.

Other Greek Fathers would come to emphasize the cosmic dimension of the Fall, namely that since Adam human beings are born into a fallen world, but held fast to belief that man, though fallen, is free. They thus did not teach that human beings are deprived of free will and involved in total depravity, which is one understanding of original sin among the leaders of the Reformation. During this period the doctrines of human depravity and the inherently sinful nature of human flesh were taught by Gnostics, and orthodox Christian writers took great pains to counter them. Christian apologists insisted that God's future judgment of humanity implied humanity must have the ability to live righteously.

Historian Robin Lane Fox argues that the foundation of the doctrine of original sin as accepted by the Church was ultimately based on a mistranslation of Paul the Apostle's Epistle to the Romans () by Augustine, in his "On the Grace of Christ, and on Original Sin"".

The original sin doctrine can be found fourth Book of Esdras, which refers Adam being responsible for the Fall of man whose offspring inherited the disease and evil.
O Adam, what have you done? For though it was you who sinned, the fall was not yours alone, but ours also who are your descendants.─ 4 Esdras 7:48(118)
For the first Adam, burdened with an evil heart, transgressed and was overcome, as were also all who were descended from him. Thus the disease became permanent; the law was in the hearts of the people along with the evil root; but what was good departed, and the evil remained. ─ 4 Esdras 3:21-22
For a grain of evil seed was sown in Adam’s heart from the beginning, and how much ungodliness it has produced until now—and will produce until the time of threshing comes! ─ 4 Esdras 4:30

Augustine of Hippo (354–430) taught that Adam's sin is transmitted by concupiscence, or "hurtful desire", resulting in humanity becoming a "massa damnata" (mass of perdition, condemned crowd), with much enfeebled, though not destroyed, freedom of will. When Adam sinned, human nature was thenceforth transformed. Adam and Eve, via sexual reproduction, recreated human nature. Their descendants now live in sin, in the form of concupiscence, a term Augustine used in a metaphysical, not a psychological sense. Augustine insisted that concupiscence was not "a being" but a "bad quality", the privation of good or a wound. He admitted that sexual concupiscence ("libido") might have been present in the perfect human nature in paradise, and that only later it became disobedient to human will as a result of the first couple's disobedience to God's will in the original sin. In Augustine's view (termed "Realism"), all of humanity was really present in Adam when he sinned, and therefore all have sinned. Original sin, according to Augustine, consists of the guilt of Adam which all humans inherit. Justo Gonzalez interprets Augustine's teaching that humans are utterly depraved in nature and grace is irresistible, results in conversion, and leads to perseverance.

Augustine articulated his explanation in reaction to Pelagianism, which insisted that humans have of themselves, without the necessary help of God's grace, the ability to lead a morally good life, and thus denied both the importance of baptism and the teaching that God is the giver of all that is good. Pelagius claimed that the influence of Adam on other humans was merely that of bad example. Augustine held that the effects of Adam's sin are transmitted to his descendants not by example but by the very fact of generation from that ancestor. A wounded nature comes to the soul and body of the new person from his/her parents, who experience "libido" (or "concupiscence"). Augustine's view was that human procreation was the way the transmission was being effected. He did not blame, however, the sexual passion itself, but the spiritual "concupiscence" present in human nature, soul and body, even after baptismal regeneration. Christian parents transmit their wounded nature to children, because they give them birth, not the "re-birth". Augustine used Ciceronian Stoic concept of passions, to interpret St. Paul's doctrine of universal sin and redemption. In that view, also sexual desire itself as well as other bodily passions were consequence of the original sin, in which pure affections were wounded by vice and became disobedient to human reason and will. As long as they carry a threat to the dominion of reason over the soul they constitute moral evil, but since they do not presuppose consent, one cannot call them sins. Humanity will be liberated from passions, and pure affections will be restored only when all sin has been washed away and ended, that is in the resurrection of the dead.

Augustine believed that unbaptized infants go to hell as a consequence of original sin. The Latin Church Fathers who followed Augustine adopted his position, which became a point of reference for Latin theologians in the Middle Ages. In the later medieval period, some theologians continued to hold Augustine's view, others held that unbaptized infants suffered no pain at all: unaware of being deprived of the beatific vision, they enjoyed a state of natural, not supernatural happiness. Starting around 1300, unbaptized infants were often said to inhabit the "limbo of infants". The "Catechism of the Catholic Church", 1261 declares: "As regards children who have died without Baptism, the Church can only entrust them to the mercy of God, as she does in her funeral rites for them. Indeed, the great mercy of God who desires that all men should be saved, and Jesus' tenderness toward children which caused him to say: 'Let the children come to me, do not hinder them', allow us to hope that there is a way of salvation for children who have died without Baptism. All the more urgent is the Church's call not to prevent little children coming to Christ through the gift of holy Baptism." But the theory of Limbo, while it "never entered into the dogmatic definitions of the Magisterium ... remains ... a possible theological hypothesis".

In the works of John Cassian (c. 360 – 435), "Conference" XIII recounts how the wise monk Chaeremon, of whom he is writing, responded to puzzlement caused by his own statement that "man even though he strive with all his might for a good result, yet cannot become master of what is good unless he has acquired it simply by the gift of Divine bounty and not by the efforts of his own toil" (chapter 1). In chapter 11, Cassian presents Chaeremon as speaking of the cases of Paul the persecutor and Matthew the publican as difficulties for those who say "the beginning of free will is in our own power", and the cases of Zaccheus and the good thief on the cross as difficulties for those who say "the beginning of our free will is always due to the inspiration of the grace of God", and as concluding: "These two then; viz., the grace of God and free will seem opposed to each other, but really are in harmony, and we gather from the system of goodness that we ought to have both alike, lest if we withdraw one of them from man, we may seem to have broken the rule of the Church's faith: for when God sees us inclined to will what is good, He meets, guides, and strengthens us: for 'At the voice of thy cry, as soon as He shall hear, He will answer thee'; and: 'Call upon Me', He says, 'in the day of tribulation and I will deliver thee, and thou shalt glorify Me'. And again, if He finds that we are unwilling or have grown cold, He stirs our hearts with salutary exhortations, by which a good will is either renewed or formed in us."

Cassian did not accept the idea of total depravity, on which Martin Luther was to insist. He taught that human nature is fallen or depraved, but not totally. Augustine Casiday states that, at the same time, Cassian "baldly asserts that God's grace, not human free will, is responsible for 'everything which pertains to salvation' – even faith". Cassian pointed out that people still have moral freedom and one has the option to choose to follow God. Colm Luibhéid says that, according to Cassian, there are cases where the soul makes the first little turn, but in Cassian's view, according to Casiday, any sparks of goodwill that may exist, not "directly" caused by God, are totally inadequate and only "direct" divine intervention ensures spiritual progress; and Lauren Pristas says that "for Cassian, salvation is, from beginning to end, the effect of God's grace".

Opposition to Augustine's ideas about original sin, which he had developed in reaction to Pelagianism, arose rapidly. After a long and bitter struggle several councils, especially the Second Council of Orange in 529, confirmed the general principles of Augustine's teaching within Western Christianity. However, while the western Church condemned Pelagius, it did not endorse Augustine entirely and, while Augustine's authority was accepted, he was interpreted in the light of writers such as Cassian. Some of the followers of Augustine identified original sin with concupiscence in the psychological sense, but Saint Anselm of Canterbury challenged this identification in the 11th-century, defining original sin as "privation of the righteousness that every man ought to possess", thus separating it from concupiscence. In the 12th century the identification of original sin with concupiscence was supported by Peter Lombard and others, but was rejected by the leading theologians in the next century, most notably by Thomas Aquinas. Aquinas distinguished the supernatural gifts of Adam before the Fall from what was merely natural, and said that it was the former that were lost, privileges that enabled man to keep his inferior powers in submission to reason and directed to his supernatural end. Even after the fall, man thus kept his natural abilities of reason, will and passions. Rigorous Augustine-inspired views persisted among the Franciscans, though the most prominent Franciscan theologians, such as Duns Scotus and William of Ockham, eliminated the element of concupiscence and identified original sin with the loss of sanctifying grace.

Eastern Orthodox theology has questioned Western Christianity's ideas on original sin from the outset and does not promote the idea of inherited guilt.

Martin Luther (1483–1546) asserted that humans inherit Adamic guilt and are in a state of sin from the moment of conception. The second article in Lutheranism's Augsburg Confession presents its doctrine of original sin in summary form:

Luther, however, also agreed with the Roman Catholic doctrine of the Immaculate Conception (that Mary was conceived free from original sin) by saying:
Protestant Reformer John Calvin (1509–1564) developed a systematic theology of Augustinian Protestantism by interpretation of Augustine of Hippo's notion of original sin. Calvin believed that humans inherit Adamic guilt and are in a state of sin from the moment of conception. This inherently sinful nature (the basis for the Calvinistic doctrine of "total depravity") results in a complete alienation from God and the total inability of humans to achieve reconciliation with God based on their own abilities. Not only do individuals inherit a sinful nature due to Adam's fall, but since he was the federal head and representative of the human race, all whom he represented inherit the guilt of his sin by imputation. Redemption by Jesus Christ is the only remedy.

John Calvin defined original sin in his "Institutes of the Christian Religion" as follows:

The Council of Trent (1545–1563), while not pronouncing on points disputed among Catholic theologians, condemned the teaching that in baptism the whole of what belongs to the essence of sin is not taken away, but is only cancelled or not imputed, and declared the concupiscence that remains after baptism not truly and properly "sin" in the baptized, but only to be called sin in the sense that it is of sin and inclines to sin.

In 1567, soon after the close of the Council of Trent, Pope Pius V went beyond Trent by sanctioning Aquinas's distinction between nature and supernature in Adam's state before the Fall, condemned the identification of original sin with concupiscence, and approved the view that the unbaptized could have right use of will. The Catholic Encyclopedia refers: "Whilst original sin is effaced by baptism concupiscence still remains in the person baptized; therefore original sin and concupiscence cannot be one and the same thing, as was held by the early Protestants (see Council of Trent, Sess. V, can. v).".

The "Catechism of the Catholic Church" says:
By his sin Adam, as the first man, lost the original holiness and justice he had received from God, not only for himself but for all humans.

Adam and Eve transmitted to their descendants human nature wounded by their own first sin and hence deprived of original holiness and justice; this deprivation is called "original sin".

As a result of original sin, human nature is weakened in its powers, subject to ignorance, suffering and the domination of death, and inclined to sin (this inclination is called "concupiscence").
St. Anselm refers: "the sin of Adam was one thing but the sin of children at their birth is quite another, the former was the cause, the latter is the effect" In a child original sin is distinct from the fault of Adam, it is one of its effects. The effects of Adam's sin according to the Catholic Encyclopedia are:

The Catholic Church teaches that every human person born on this earth is made in the image of God. Within man "is both the powerful surge toward the good because we are made in the image of God, and the darker impulses toward evil because of the effects of Original Sin". Furthermore, it explicitly denies that we inherit "guilt" from anyone, maintaining that instead we inherit our fallen nature. In this it differs from the Calvinist/Protestant position that each person actually inherits Adam's guilt, and teaches instead that "original sin does not have the character of a personal fault in any of Adam's descendants ... but the consequences for nature, weakened and inclined to evil, persist in man". "In other words, human beings do not bear any 'original guilt' from Adam and Eve's particular sin."

The Church has always held baptism to be for the remission of sins including the original sin, and, as mentioned in "Catechism of the Catholic Church", 403, infants too have traditionally been baptized, though not guilty of any actual personal sin. The sin that through baptism is remitted for them could only be original sin. Baptism confers original sanctifying grace which erases original sin and any actual personal sin. The first comprehensive theological explanation of this practice of baptizing infants, guilty of no actual personal sin, was given by Saint Augustine of Hippo, not all of whose ideas on original sin have been adopted by the Catholic Church. Indeed, the Church has condemned the interpretation of some of his ideas by certain leaders of the Protestant Reformation.

The "Catechism of the Catholic Church" explains that in "yielding to the tempter, Adam and Eve committed a "personal sin", but this sin affected "the human nature" that they would then transmit in a "fallen state" ... original sin is called "sin" only in an analogical sense: it is a sin "contracted" and not "committed"—a state and not an act" ("Catechism of the Catholic Church", 404). This "state of deprivation of the original holiness and justice ... transmitted to the descendants of Adam along with human nature" ("Compendium of the Catechism of the Catholic Church", 76) involves no personal responsibility or personal guilt on their part (cf. "Catechism of the Catholic Church", 405). Personal responsibility and guilt were Adam's, who because of his sin, was unable to pass on to his descendants a human nature with the holiness with which it would otherwise have been endowed, in this way implicating them in his sin. The doctrine of original sin thus does not impute the sin of the father to his children, but merely states that they inherit from him a "human nature deprived of original holiness and justice", which is "transmitted by propagation to all mankind".

In the theology of the Catholic Church, original sin is the absence of original holiness and justice into which humans are born, distinct from the actual sins that a person commits. The absence of sanctifying grace or holiness in the new-born child is an effect of the first sin, for Adam, having received holiness and justice from God, lost it not only for himself but also for us. This teaching explicitly states that "original sin does not have the character of a personal fault in any of Adam's descendants". In other words, human beings do not bear any "original guilt" from Adam's particular sin, which is his alone. The prevailing view, also held in Eastern Orthodoxy, is that human beings bear no guilt for the sin of Adam. The Catholic Church teaches: "By our first parents' sin, the devil has acquired a certain domination over man, even though "man remains free"."

The Catholic doctrine of the Immaculate Conception of Mary is that Mary was conceived free from original sin: "the most Blessed Virgin Mary was, from the first moment of her conception, by a singular grace and privilege of almighty God and by virtue of the merits of Jesus Christ, Savior of the human race, preserved immune from all stain of original sin". The doctrine sees her as an exception to the general rule that human beings are not immune from the reality of original sin.

Soon after the Second Vatican Council, biblical theologian Herbert Haag raised the question: "Is original sin in Scripture?" According to his exegesis, Genesis would indicate that Adam and Eve were created from the beginning naked of the divine grace, an originary grace that, then, they would never have had and even less would have lost due to the subsequent events narrated. On the other hand, while supporting a continuity in the Bible about the absence of preternatural gifts () with regard to the ophitic event, Haag never makes any reference to the discontinuity of the loss of access to the tree of life.

The Eastern Orthodox version of "original sin" is the view that sin originates with the Devil, "for the devil sinneth from the beginning (1 John iii. 8)". They acknowledge that the introduction of ancestral sin into the human race affected the subsequent environment for humanity (see also traducianism). However, they never accepted Augustine of Hippo's notions of original sin and hereditary guilt.

Orthodox Churches accept the teachings of John Cassian, as do Catholic Churches eastern and western, in rejecting the doctrine of total depravity, by teaching that human nature is "fallen", that is, depraved, but not totally. Augustine Casiday states that Cassian "baldly asserts that God's grace, not human free will, is responsible for 'everything which pertains to salvation' – even faith". Cassian points out that people still have moral freedom and one has the option to choose to follow God. Colm Luibhéid says that, according to Cassian, there are cases where the soul makes the first little turn, while Augustine Casiday says that, in Cassian's view, any sparks of goodwill that may exist, not "directly" caused by God, are totally inadequate and only "direct" divine intervention ensures spiritual progress. and Lauren Pristas says that "for Cassian, salvation is, from beginning to end, the effect of God's grace".

Eastern Orthodoxy accepts the doctrine of ancestral sin: "Original sin is hereditary. It did not remain only Adam and Eve's. As life passes from them to all of their descendants, so does original sin." "As from an infected source there naturally flows an infected stream, so from a father infected with sin, and consequently mortal, there naturally proceeds a posterity infected like him with sin, and like him mortal."

The Orthodox Church in America makes clear the distinction between "fallen nature" and "fallen man" and this is affirmed in the early teaching of the Church whose role it is to act as the catalyst that leads to true or inner redemption. Every human person born on this earth bears the image of God undistorted within themselves. In the Orthodox Christian understanding, they explicitly deny that humanity inherited "guilt" from anyone. Rather, they maintain that we inherit our fallen nature. While humanity does bear the consequences of the original, or first, sin, humanity does not bear the personal guilt associated with this sin. Adam and Eve are guilty of their willful action; we bear the consequences, chief of which is death."

The view of the Eastern Orthodox Church varies on whether Mary is free of all actual sin or concupiscence. Some Patristic sources imply that she was cleansed from sin at the Annunciation, while the liturgical references are unanimous that she is all-holy from the time of her conception.

The original formularies of the Church of England also continue in the Reformation understanding of original sin. In the Thirty-Nine Articles, Article IX "Of Original or Birth-sin" states:
However, more recent doctrinal statements (e.g. the 1938 report "Doctrine in the Church of England") permit a greater variety of understandings of this doctrine. The 1938 report summarizes:
The Methodist Church upholds Article VII in the Articles of Religion in the "Book of Discipline of the United Methodist Church":

Seventh-day Adventists believe that humans are inherently sinful due to the fall of Adam, but they do not totally accept the Augustinian/Calvinistic understanding of original sin, taught in terms of original guilt, but hold more to what could be termed the "total depravity" tradition. Seventh-day Adventists have historically preached a doctrine of inherited weakness, but not a doctrine of inherited guilt. According to Augustine and Calvin, humanity inherits not only Adam's depraved nature but also the actual guilt of his transgression, and Adventists look more toward the Wesleyan model.

In part, the Adventist position on original sin reads:

Early Adventists Pioneers (such as George Storrs and Uriah Smith) tended to de-emphasise the morally corrupt nature inherited from Adam, while stressing the importance of actual, personal sins committed by the individual. They thought of the "sinful nature" in terms of physical mortality rather than moral depravity. Traditionally, Adventists look at sin in terms of willful transgressions, and that Christ triumphed over sin. 

Though believing in the concept of inherited sin from Adam, there is no dogmatic Adventist position on original sin.

According to the theology of the Christian Congregation of Jehovah's Witnesses, all humans are born sinners, because of inheriting sin, corruption, and death from Adam. They teach that Adam was originally created perfect and sinless, but with free will; that the Devil, who was originally a perfect angel, but later developed feelings of pride and self-importance, seduced Eve, and then through her, persuaded Adam to disobey God, and to obey the Devil instead, rebelling against God's sovereignty, thereby making themselves sinners, and because of that, transmitting a sinful nature to all of their future offspring. Instead of destroying the Devil right away, as well as destroying the disobedient couple, God decided to test the loyalty of the rest of humankind, and to prove that man cannot be independent of God successfully, that man is lost without God's laws and standards, and can never bring peace to the earth, and that Satan was a deceiver, murderer, and liar.

Jehovah's Witnesses believe that all men possess "inherited sin" from the "one man" Adam and they teach that verses such as Romans 5:12-22, Psalm 51:5, Job 14:4, and 1st Corinthians 15:22 show that man is born corrupt, and dies because of inherited sin and imperfection, that inherited sin is the reason and cause for sickness and suffering, made worse by the Devil's wicked influence. They believe Jesus is the "second Adam", being the sinless Son of God and the Messiah, and that he came to undo Adamic sin; and that salvation and everlasting life can only be obtained through faith and obedience to the second Adam. They believe that "sin" is "missing the mark" of God's standard of perfection, and that everyone is born a sinner, due to being the offspring of sinner Adam.

The Book of Mormon, a text sacred to Mormonism, explains that the opportunity to live here in a world where we can learn good and bad is a gift from God, and not a punishment for Adam's and Eve's choice. As Mormon founder Joseph Smith taught, humans had an essentially godlike nature, and were not only holy in a premortal state, but had the potential to progress eternally to become like God. He wrote as one of his church's Articles of Faith, "We believe that men will be punished for their own sins, and not for Adam’s transgression." Later Mormons took this creed as a rejection of the doctrine of original sin and any notion of inherited sinfulness. Thus, while modern Mormons will agree that the fall of Adam brought consequences to the world, including the possibility of sin, they generally reject the idea that any culpability is automatically transmitted to Adam and Eve's offspring. Children under the age of eight are regarded as free of all sin and therefore do not require baptism. Children who die prior to age eight are believed to be saved in the highest degree of heaven.

In Swedenborgianism, exegesis of the first 11 chapters of Genesis from "The First Church", has a view that Adam is not an individual person. Rather, he is a symbolic representation of the "Most Ancient Church", having a more direct contact with heaven than all other successive churches. Swedenborg's view of original sin is referred to as "hereditary evil", which passes from generation to generation. It cannot be completely abolished by an individual man, but can be tempered when someone reforms their own life, and are thus held accountable only for their own sins.

Most Quakers (also known as the Religious Society of Friends), including the founder of Quakerism, George Fox, believe in the doctrine of Inward light, a doctrine which states that there is "that of God in everyone". This has led to a common belief among many liberal and universalist Quakers affiliated with the Friends General Conference and Britain Yearly Meeting, based on the ideas of Quaker Rufus Jones among others, that rather than being burdened by original sin, human beings are inherently good, and the doctrine of universal reconciliation, that is, that all people will eventually be saved and reconciled with God.

However, this rejection of the doctrine of original sin or the necessity of salvation is not something that most conservative or evangelical Quakers affiliated with Friends United Meeting or Evangelical Friends Church International tend to agree with. Although the more conservative and evangelical Quakers also believe in the doctrine of inward light, they interpret it in a manner consistent with the doctrine of original sin, namely, that people may or may not listen to the voice of God within them and be saved, and people who do not listen will not be saved.

The doctrine of "inherited sin" is not found in most of mainstream Judaism. Although some in Orthodox Judaism place blame on Adam for overall corruption of the world, and though there were some Jewish teachers in Babylon who believed that mortality was a punishment brought upon humanity on account of Adam's sin, that is not the dominant view in most of Judaism today. Modern Judaism generally teaches that humans are born sin-free and untainted, and choose to sin later and bring suffering to themselves.

Jewish theologians are divided in regard to the cause of what is called "original sin". Others teach that it was due to Adam's yielding to temptation in eating of the forbidden fruit and has been inherited by his descendants; the majority of chazalic opinions, however, do not hold Adam responsible for the sins of humanity, teaching that, in Genesis 8:21 and 6:5-8, God recognized that Adam did not willfully sin. However, Adam is recognized by some as having brought death into the world by his disobedience. Because of his sin, his descendants will live a mortal life, which will end in death of their bodies. According to book "Legends of the Jews", in Judgement Day, Adam will disavow any complaints of all men who accuse him as the cause of death of every human on earth. Instead, Adam will reproach their mortality because of their sins.

The concept of inherited sin does not exist in Islam. Islam teaches that Adam and Eve sinned, but then sought forgiveness and thus were forgiven by God. 

The Qur'an says that after Adam and Eve sinned, they were sent down to the earth for a temporary life as a consequence. In their earthly life, they received words from God, through which God granted them repentance.

The Qur'an further says about individual responsibility:




</doc>
<doc id="22738" url="https://en.wikipedia.org/wiki?curid=22738" title="Operation Enduring Freedom">
Operation Enduring Freedom

Operation Enduring Freedom (OEF) is the official name used by the U.S. government for the Global War on Terrorism. On October 7, 2001, in response to the September 11 attacks, President George W. Bush announced that airstrikes targeting Al Qaeda and the Taliban had begun in Afghanistan. Operation Enduring Freedom primarily refers to the War in Afghanistan, but it is also affiliated with counterterrorism operations in other countries, such as OEF-Philippines and OEF-Trans Sahara.

After 13 years, on December 28, 2014, President Barack Obama announced the end of Operation Enduring Freedom in Afghanistan. Continued operations in Afghanistan by the United States' military forces, both non-combat and combat, now occur under the name Operation Freedom's Sentinel.

Operation Enduring Freedom most commonly refers to the U.S.-led combat mission in Afghanistan, which is a NATO military alliance between the United States, United Kingdom and Afghanistan. OEF is also affiliated with counterterrorism operations in other countries targeting Al Qaeda and remnants of the Taliban, such as OEF-Philippines and OEF-Trans Sahara, primarily through government funding vehicles.

The U.S. government used the term "Operation Enduring Freedom – Afghanistan" to officially describe the War in Afghanistan, from the period between October 2001 and December 2014. Continued operations in Afghanistan by the United States' military forces, both non-combat and combat, now occur under the name Operation Freedom's Sentinel.

The operation was originally called "Operation Infinite Justice", but as similar phrases have been used by adherents of several religions as an exclusive description of God, it is believed to have been changed to avoid offense to Muslims, who are the majority religion in Afghanistan. In September 2001, U.S. President George W. Bush's remark that "this crusade, this war on terrorism, is going to take a while", which prompted widespread criticism from the Islamic world, may also have contributed to the renaming of the operation.

The term "OEF-A" typically refers to the phase of the War in Afghanistan from 2001 to 2014. Other operations, such as the Georgia Train and Equip Program, are only loosely or nominally connected, such as through government funding vehicles. All the operations, however, have a focus on counterterrorism activities.

Operation Enduring Freedom – Afghanistan, which was a joint U.S., U.K., and Afghan operation, was separate from the International Security Assistance Force (ISAF), which was an operation of North Atlantic Treaty Organization nations including the U.S. and the U.K. The two operations ran in parallel, although it had been suggested that they merge.

In response to the attacks of 11 September, the early combat operations that took place on 7 October 2001 to include a mix of strikes from land-based B-1 Lancer, B-2 Spirit and B-52 Stratofortress bombers, carrier-based F-14 Tomcat and F/A-18 Hornet fighters, and Tomahawk cruise missiles launched from both U.S. and British ships and submarines signaled the start of Operation Enduring Freedom – Afghanistan (OEF-A).

The initial military objectives of OEF-A, as articulated by President George W. Bush in his 20 September Address to a Joint Session of Congress and his 7 October address to the country, included the destruction of terrorist training camps and infrastructure within Afghanistan, the capture of al-Qaeda leaders, and the cessation of terrorist activities in Afghanistan.

In January 2002, over 1,200 soldiers from the United States Special Operations Command Pacific (SOCPAC) deployed to the Philippines to support the Armed Forces of the Philippines (AFP) in their push to uproot terrorist forces on the island of Basilan. Of those groups included are Abu Sayyaf Group (ASG), al-Qaeda and Jemaah Islamiyah. The operation consisted of training the AFP in counter-terrorist operations as well as supporting the local people with humanitarian aid in Operation Smiles.

In October 2002, the Combined Task Force 150 and United States military Special Forces established themselves in Djibouti at Camp Lemonnier. The stated goals of the operation were to provide humanitarian aid and patrol the Horn of Africa to reduce the abilities of terrorist organizations in the region. Similar to OEF-P, the goal of humanitarian aid was emphasised, ostensibly to prevent militant organizations from being able to take hold amongst the population as well as reemerge after being removed.

The military aspect involves coalition forces searching and boarding ships entering the region for illegal cargo as well as providing training and equipment to the armed forces in the region. The humanitarian aspect involves building schools, clinics and water wells to enforce the confidence of the local people.

Since 2001, the cumulative expenditure by the U.S. government on Operation Enduring Freedom has exceeded $150 billion.

The operation continues, with military direction mostly coming from United States Central Command.

Seizing upon a power vacuum after the Soviets withdrew from Afghanistan after their invasion, the Taliban had the role of government from 1996–2001. Their extreme interpretation of Islamic law prompted them to ban music, television, sports, and dancing, and enforce harsh judicial penalties (See Human rights in Afghanistan). Amputation was an accepted form of punishment for stealing, and public executions could often be seen at the Kabul football stadium. Women's rights groups around the world were frequently critical as the Taliban banned women from appearing in public or holding many jobs outside the home. They drew further criticism when they destroyed the Buddhas of Bamyan, historical statues nearly 1500 years old, because the Buddhas were considered idols.

In 1996, Saudi dissident Osama bin Laden moved to Afghanistan upon the invitation of the Northern Alliance leader Abdur Rabb ur Rasool Sayyaf. When the Taliban came to power, bin Laden was able to forge an alliance between the Taliban and his al-Qaeda organization. It is understood that al-Qaeda-trained fighters known as the 055 Brigade were integrated with the Taliban army between 1997 and 2001. It has been suggested that the Taliban and bin Laden had very close connections.

On 20 September 2001, the U.S. stated that Osama bin Laden was behind the 11 September attacks in 2001. The US made a five-point ultimatum to the Taliban:

On 21 September 2001, the Taliban rejected this ultimatum, stating there was no evidence in their possession linking bin Laden to the 11 September attacks.

On 22 September 2001 the United Arab Emirates and later Saudi Arabia withdrew their recognition of the Taliban as the legal government of Afghanistan, leaving neighboring Pakistan as the only remaining country with diplomatic ties.

On 4 October 2001, it was reported that the Taliban covertly offered to turn bin Laden over to Pakistan for trial in an international tribunal that operated according to Islamic shar'ia law. On 7 October 2001, the Taliban proposed to try bin Laden in Afghanistan in an Islamic court. This proposition was immediately rejected by the US. Later on the same day, United States and British forces initiated military action against the Taliban, bombing Taliban forces and al-Qaeda terrorist training camps.

On 14 October 2001, the Taliban proposed to hand bin Laden over to a third country for trial, but only if they were given evidence of bin Laden's involvement in the events of 11 September 2001. The US rejected this proposal and military operations ensued.

The UN Security Council, on 16 January 2002, unanimously established an arms embargo and the freezing of identifiable assets belonging to bin Laden, al-Qaeda, and the remaining Taliban.

On Sunday 7 October 2001, American and British forces began an aerial bombing campaign targeting Taliban forces and al-Qaeda.

The Northern Alliance, aided by Joint Special Operations teams consisting of Green Berets from the 5th Special Forces Group, aircrew members from the 160th Special Operations Aviation Regiment (SOAR), and Air Force Combat Controllers, fought against the Taliban. Aided by U.S. bombing and massive defections, they captured Mazar-i-Sharif on 9 November. They then rapidly gained control of most of northern Afghanistan, and took control of Kabul on 13 November after the Taliban unexpectedly fled the city. The Taliban were restricted to a smaller and smaller region, with Kunduz, the last Taliban-held city in the north, captured on 26 November. Most of the Taliban fled to Pakistan.

The war continued in the south of the country, where the Taliban retreated to Kandahar. After Kandahar fell in December, remnants of the Taliban and al-Qaeda continued to mount resistance. Meanwhile, in November 2001 the U.S. military and its allied forces established their first ground base in Afghanistan to the south west of Kandahar, known as FOB Rhino.

The Battle of Tora Bora, involving U.S., British and Northern Alliance forces took place in December 2001 to further destroy the Taliban and suspected al-Qaeda in Afghanistan. In early March 2002 the United States military, along with allied Afghan military forces, conducted a large operation to destroy al-Qaeda in an operation code-named Operation Anaconda.

The operation was carried out by elements of the United States 10th Mountain Division, 101st Airborne Division, the U.S. special forces groups TF 11, TF Bowie, TF Dagger, TF K-Bar, British Royal Marines, the Norwegian "Forsvarets Spesialkommando" (FSK), "Hærens Jegerkommando" and "Marinejegerkommandoen", Canada's 3rd Battalion Princess Patricia's Canadian Light Infantry, Canada's Joint Task Force 2, the German KSK, and elements of the Australian Special Air Service Regiment and of the New Zealand Special Air Service and the Afghan National Army.

After managing to evade U.S. forces throughout the summer of 2002, the remnants of the Taliban gradually began to regain their confidence. A U.S. and Canadian led operation (supported by British and Dutch forces), Operation Mountain Thrust was launched in May 2006 to counter renewed Taliban insurgency.

Since January 2006, the NATO International Security Assistance Force undertook combat duties from Operation Enduring Freedom in southern Afghanistan, the NATO force chiefly made up of British, Canadian and Dutch forces (and some smaller contributions from Denmark, Romania and Estonia and air support from Norway as well as air and artillery support from the U.S.) ("see the article Coalition combat operations in Afghanistan in 2006"). The United States military also conducts military operations separate from NATO as part of Operation Enduring Freedom in other parts of Afghanistan, in areas such as Kandahar, Bagram, and Kabul (including Camp Eggers and Camp Phoenix.)

The United States was supported by during Operation Enduring Freedom (OEF) in Afghanistan in 2001–2003 and in subsequent coalition operations directly or indirectly in support of OEF. See the article Afghanistan War order of battle for the current disposition of coalition forces in Afghanistan.

The U.S.-led coalition initially removed the Taliban from power and seriously crippled al-Qaeda and associated militants in Afghanistan. However, success in quelling the Taliban insurgency since the 2001 invasion has been mixed. Many believe the Taliban cannot be defeated as long as it has sanctuary in neighboring Pakistan and that Operation Enduring Freedom has transformed into a continuing full-fledged war with no end in sight.

On 9 October 2004, Afghanistan elected Hamid Karzai president in its first direct elections. The following year, Afghans conducted the Afghan parliamentary election, 2005 on 18 September. Since the invasion, hundreds of schools and mosques have been constructed, millions of dollars in aid have been distributed, and the occurrence of violence has been reduced.

While military forces interdict insurgents and assure security, Provincial reconstruction teams are tasked with infrastructure building, such as constructing roads and bridges, assisting during floods, and providing food and water to refugees. Many warlords have participated in an allegiance program, recognizing the legitimacy of the government of Afghanistan, and surrendering their soldiers and weapons; however, subsequent actions have led to questions about their true loyalties.

The Afghan National Army, Afghan National Police, and Afghan Border Police are being trained to assume the task of securing their nation.

On 31 December 2014, Operation Enduring Freedom - Afghanistan concluded, and was succeeded by Operation Freedom's Sentinel on 1 January 2015.

AFP, reporting on a news story in the Sunday, 3 April 2004, issue of "The New Yorker", wrote that retired Army Colonel Hy Rothstein, "who served in the Army Special Forces for more than 20 years, ...commissioned by The Pentagon to examine the war in Afghanistan concluded the conflict created conditions that have given 'warlordism, banditry and opium production a new lease on life'..."

The conduct of U.S. forces was criticised in a report entitled "Enduring Freedom – Abuses by U.S. Forces in Afghanistan" by U.S.-based human rights group Human Rights Watch in 2004. Some Pakistani scholars, such as Masood Ashraf Raja, editor of , have also provided a more specific form of criticism that relates to the consequences of the Global War on Terrorism on the region.

The Abu Sayyaf Group (ASG) Al Harakat Al Islamiyya, is deemed a "foreign terrorist organization" by the United States government. Specifically, it is an Islamist separatist group based in and around the southern islands of the Republic of the Philippines, primarily Jolo, Basilan, and Mindanao.

Since inception in the early 1990s, the group has carried out bombings, assassinations, kidnappings, and extortion in their fight for an independent Islamic state in western Mindanao and the Sulu Archipelago. Its claimed overarching goal is to create a Pan-Islamic superstate across the "Malay" portions of Southeast Asia, spanning, from east to west, the large island of Mindanao, the Sulu Archipelago (Basilan and Jolo islands), the large island of Borneo (Malaysia and Indonesia), the South China Sea, and the Malay Peninsula (Peninsular Malaysia, Thailand and Myanmar).

Jemaah Islamiyah is a militant Islamic terrorist organization dedicated to the establishment of a fundamentalist Islamic theocracy in Southeast Asia, in particular Indonesia, Singapore, Brunei, Malaysia, the south of Thailand and the Philippines. Jemaah Islamiyah originally used peaceful means to achieve its goals, but later resorted to terrorism because of its connections with al-Qaeda.

Financial links between Jemaah Islamiyah and other terrorist groups, such as Abu Sayyaf and al-Qaeda, have been found to exist. Jemaah Islamiyah means "Islamic Group" or "Islamic Community" and is often abbreviated JI.

Jemaah Islamiyah is thought to have killed hundreds of civilians. Also, it is suspected of carrying out the Bali car bombing on 12 October 2002, in which suicide bombers attacked a nightclub killing 202 people and wounding many more. Most of the casualties were Australian tourists. After this attack, the U.S. State Department designated Jemaah Islamiyah as a Foreign Terrorist Organization. Jemaah Islamiyah is also suspected of carrying out the Zamboanga bombings, the Metro Manila bombings, the 2004 Australian embassy bombing and the 2005 Bali terrorist bombing.

In January 2002, 1,200 members of United States Special Operations Command, Pacific (SOCPAC) were deployed to the Philippines to assist the Armed Forces of the Philippines (AFP) in uprooting al-Qaeda, Jemaah Islamiyah and Abu Sayyaf. The members of SOCPAC were assigned to assist in military operations against the terrorist forces as well as humanitarian operations for the island of Basilan, where most of the conflict was expected to take place.

The United States Special Forces (SF) unit trained and equipped special forces and scout rangers of the AFP, creating the Light Reaction Company (LRC). The LRC and elements of SOCPAC deployed to Basilan on completion of their training. The stated goals of the deployment were denying the ASG sanctuary, surveiling, controlling, and denying ASG routes, surveiling supporting villages and key personnel, conducting local training to overcome AFP weaknesses and sustain AFP strengths, supporting operations by the AFP "strike force" (LRC) in the area of responsibility (AOR), conducting and supporting civil affairs operations in the AOR.

The desired result was for the AFP to gain sufficient capability to locate and destroy the ASG, to recover hostages and to enhance the legitimacy of the Philippine government. Much of the operation was a success: the ASG was driven from Basilan and one U.S. hostage was recovered. The Abu Sayyaf Group's ranks, which once counted more than 800 members, was reduced to less than 100. The humanitarian portion of the operation, Operation Smiles, created 14 schools, 7 clinics, 3 hospitals and provided medical care to over 18,000 residents of Basilan. Humanitarian groups were able to continue their work without fear of further kidnappings and terrorists attacks by the Abu Sayyaf Group.

Unlike other operations contained in Operation Enduring Freedom, OEF-HOA does not have a specific terrorist organization as a target. OEF-HOA instead focuses its efforts to disrupt and detect terrorist activities in the region and to work with host nations to deny the reemergence of terrorist cells and activities. Operations began in mid-2002 at Camp Lemonnier by a Combined Joint Special Operations Task Force (CJSOTF) augmented by support forces from Fort Stewart, Fort Hood, and Fort Story. In October 2002, the Combined Joint Task Force, Horn of Africa (CJTF-HOA) was established at Djibouti at Camp Lemonnier, taking over responsibilities from the CJSOTF. CJTF-HOA comprised approximately 2,000 personnel including U.S. military and Special Operations Forces (SOF), and coalition force members, Combined Task Force 150 (CTF-150). The coalition force consists of ships from Australia, Canada, France, Germany, Netherlands, India, Italy, Pakistan, New Zealand, Spain, Turkey and the United Kingdom. The primary goal of the coalition forces is to monitor, inspect, board and stop suspected shipments from entering the Horn of Africa region. Since 2003, the U.S. Military also conducts operations targeting Al-Qaeda-linked fighters in Somalia, these operations had reportedly killed between 113 and 136 militants by early 2016. On 7 March 2016, a further 150 were killed in U.S. airstrikes on an al Shabaab training camp north of Mogadishu.

CJTF-HOA has devoted the majority of its efforts to train selected armed forces units of the countries of Djibouti, Kenya and Ethiopia in counterterrorism and counterinsurgency tactics. Humanitarian efforts conducted by CJTF-HOA include the rebuilding of schools and medical clinics, as well as providing medical services to those countries whose forces are being trained. The program expands as part of the Trans-Saharan Counter Terrorism Initiative as CJTF personnel also assist in training the forces of Chad, Niger, Mauritania and Mali.

"Operation Enduring Freedom"

Anti-piracy operations were undertaken by the coalition throughout 2006 with a battle fought in March when US vessels were attacked by pirates. In January 2007, during the war in Somalia, an AC-130 airstrike was conducted against al-Qaeda members embedded with forces of the Islamic Courts Union (ICU) operating in southern Somalia near Ras Kamboni. US naval forces, including the aircraft carrier USS "Dwight D. Eisenhower", were positioned off the coast of Somalia to provide support and to prevent any al-Qaeda forces escaping by sea. Actions against pirates also occurred in June and October 2007 with varying amounts of success.

"Operation's Resolute Support/Freedom Sentinel"

Effective 1 January 2015, Secretary of Defense Hagel announced that the new U.S. mission in Afghanistan will focus on training, advising, and assisting Afghan security forces and designated as Operation Freedom's Sentinel.
19 About 13,500 U.S. troops are expected in Afghanistan through
2015 and will be assisted by troops from NATO allies.

Since 2002, the United States military has created military awards and decorations related to Operation Enduring Freedom

NATO also created a military decoration related to Operation Enduring Freedom:




</doc>
<doc id="22739" url="https://en.wikipedia.org/wiki?curid=22739" title="Obfuscation (software)">
Obfuscation (software)

In software development, obfuscation is the deliberate act of creating source or machine code that is difficult for humans to understand. Like obfuscation in natural language, it may use needlessly roundabout expressions to compose statements.
Programmers may deliberately obfuscate code to conceal its purpose (security through obscurity) or its logic or implicit values embedded in it, primarily, in order to prevent tampering, deter reverse engineering, or even as a puzzle or recreational challenge for someone reading the source code. This can be done manually or by using an automated tool, the latter being the preferred technique in industry.

The architecture and characteristics of some languages may make them easier to obfuscate than others. C, C++, and the Perl programming language are some examples of languages easy to obfuscate.

Writing and reading obfuscated source code can be a brain teaser. A number of programming contests reward the most creatively obfuscated code, such as the International Obfuscated C Code Contest and the Obfuscated Perl Contest.

Types of obfuscations include simple keyword substitution, use or non-use of whitespace to create artistic effects, and self-generating or heavily compressed programs.

According to Nick Montfort, techniques may include: 

Short obfuscated Perl programs may be used in signatures of Perl programmers. These are JAPHs ("Just another Perl hacker").

This is a winning entry from the International Obfuscated C Code Contest written by Ian Phillipps in 1988 and subsequently reverse engineered by Thomas Ball.
It is a C program that when compiled and run will generate the 12 verses of "The 12 Days of Christmas". It contains all the strings required for the poem in an encoded form within the code.

A non-winning entry from the same year, this next example illustrates creative use of whitespace; it generates mazes of arbitrary length:
Modern C compilers don't allow constant strings to be overwritten, which can be avoided by changing "*M" to "M[3]" and omitting "M=".

The following example by Óscar Toledo Gutiérrez, Best of Show entry in the 19th IOCCC, implements an 8080 emulator complete with terminal and disk controller, capable of booting CP/M-80 and running CP/M applications:

An example of a JAPH:

This slowly displays the text "Just another Perl / Unix hacker", multiple characters at a time, with delays. An explanation can be found here.

Some Python examples can be found in the official Python programming FAQ and elsewhere.

There are several advantages of automated code obfuscation that have made it popular and widely useful across many platforms. On some platforms (such as Java., Android, and .NET) a decompiler can reverse-engineer source code from an executable or library. 
A main advantage of automated code obfuscation is that it helps protect the trade secrets (intellectual property) contained within software by making reverse-engineering a program difficult and economically unfeasible. Other advantages might include helping to protect licensing mechanisms and unauthorized access, and shrinking the size of the executable. Decompilation is sometimes called a man-at-the-end attack, based on the traditional cryptographic attack known as "man-in-the-middle".

While obfuscation can make reading, writing, and reverse-engineering a program difficult and time-consuming, it will not necessarily make it impossible. Some anti-virus software, such as AVG AntiVirus, will also alert their users when they land on a site with code that is manually obfuscated, as one of the purposes of obfuscation can be to hide malicious code. However, some developers may employ code obfuscation for the purpose of reducing file size or increasing security. The average user may not expect their antivirus software to provide alerts about an otherwise harmless piece of code, especially from trusted corporations, so such a feature may actually deter users from using legitimate software.

A variety of tools exist to perform or assist with code obfuscation.
These include experimental research tools created by academics, hobbyist tools,
commercial products written by professionals, and open-source software.
There also exist deobfuscation tools that attempt to perform the reverse
transformation.

Although the majority of commercial obfuscation solutions work by transforming
either program source
code, or platform-independent bytecode as used by
Java and
.NET, there are also some that work directly on compiled binaries.

There has been debate on whether it is illegal to skirt copyleft software licenses by releasing source code in obfuscated form, such as in cases in which the author is less willing to make the source code available. The issue is addressed in the GNU General Public License by defining source code as the "preferred" version of the source code be made available. The GNU website states "Obfuscated 'source code' is not real source code and does not count as source code." 





</doc>
<doc id="22742" url="https://en.wikipedia.org/wiki?curid=22742" title="Ötzi">
Ötzi

Ötzi (; also called the Iceman, the Similaun Man, the Man from Hauslabjoch, the Tyrolean Iceman, and the Hauslabjoch mummy) is the well-preserved natural mummy of a man who lived between 3400 and 3100 BCE. The mummy was found in September 1991 in the Ötztal Alps, hence the nickname "Ötzi", near Similaun mountain and Hauslabjoch on the border between Austria and Italy. He is Europe's oldest known natural human mummy, and has offered an unprecedented view of Chalcolithic (Copper Age) Europeans. His body and belongings are displayed in the South Tyrol Museum of Archaeology in Bolzano, South Tyrol, Italy.

Ötzi was found on 19 September 1991 by two German tourists, at an elevation of on the east ridge of the Fineilspitze in the Ötztal Alps on the Austrian–Italian border. The tourists, Helmut and Erika Simon, were walking off the path between the mountain passes Hauslabjoch and Tisenjoch. They believed that the body was of a recently deceased mountaineer. The next day, a mountain gendarme and the keeper of the nearby Similaunhütte first attempted to remove the body, which was frozen in ice below the torso, using a pneumatic drill and ice-axes, but had to give up due to bad weather. The next day, eight groups visited the site, among whom happened to be the famous mountaineers Hans Kammerlander and Reinhold Messner. The body was semi-officially extracted on 22 September and officially salvaged the following day. It was transported to the office of the medical examiner in Innsbruck, together with other objects found. On 24 September the find was examined there by archaeologist Konrad Spindler of the University of Innsbruck. He dated the find to be "about four thousand years old", based on the typology of an axe among the retrieved objects.

At the Treaty of Saint-Germain-en-Laye of 1919, the border between North and South Tyrol was defined as the watershed of the rivers Inn and Etsch. However, near Tisenjoch the (now withdrawn) glacier complicated establishing the watershed at the time and the border was established too far north. Therefore, although Ötzi's find site drains to the Austrian side, surveys in October 1991 showed that the body had been located inside Italian territory as delineated in 1919. The province of South Tyrol therefore claimed property rights, but agreed to let Innsbruck University finish its scientific examinations. Since 1998, it has been on display at the South Tyrol Museum of Archaeology in Bolzano, the capital of South Tyrol.

The corpse has been extensively examined, measured, X-rayed, and dated. Tissues and intestinal contents have been examined microscopically, as have the items found with the body. In August 2004, frozen bodies of three Austro-Hungarian soldiers killed during the Battle of San Matteo (1918) were found on the mountain Punta San Matteo in Trentino. One body was sent to a museum in the hope that research on how the environment affected its preservation would help unravel Ötzi's past.

By current estimates, at the time of his death Ötzi was approximately tall, weighed about and was about 45 years of age. When his body was found, it weighed . Because the body was covered in ice shortly after his death, it had only partially deteriorated. Initial reports claimed that his penis and most of his scrotum were missing, but this was later shown to be unfounded. Analysis of pollen, dust grains and the isotopic composition of his tooth enamel indicates that he spent his childhood near the present village of Feldthurns, north of Bolzano, but later went to live in valleys about 50 kilometres farther north.
In 2009, a CAT scan revealed that the stomach had shifted upward to where his lower lung area would normally be. Analysis of the contents revealed the partly digested remains of ibex meat, confirmed by DNA analysis, suggesting he had a meal less than two hours before his death. Wheat grains were also found. It is believed that Ötzi most likely had a few slices of a dried, fatty meat, probably bacon which came from a wild goat in South Tyrol, Italy. Analysis of Ötzi's intestinal contents showed two meals (the last one consumed about eight hours before his death), one of chamois meat, the other of red deer and herb bread. Both were eaten with grain as well as roots and fruits. The grain from both meals was a highly processed einkorn wheat bran, quite possibly eaten in the form of bread. In the proximity of the body, and thus possibly originating from the Iceman's provisions, chaff and grains of einkorn and barley, and seeds of flax and poppy were discovered, as well as kernels of sloes (small plumlike fruits of the blackthorn tree) and various seeds of berries growing in the wild. Hair analysis was used to examine his diet from several months before.

Pollen in the first meal showed that it had been consumed in a mid-altitude conifer forest, and other pollens indicated the presence of wheat and legumes, which may have been domesticated crops. Pollen grains of hop-hornbeam were also discovered. The pollen was very well preserved, with the cells inside remaining intact, indicating that it had been fresh (estimated about two hours old) at the time of Ötzi's death, which places the event in the spring, or early summer. Einkorn wheat is harvested in the late summer, and sloes in the autumn; these must have been stored from the previous year.

High levels of both copper particles and arsenic were found in Ötzi's hair. This, along with Ötzi's copper axe blade, which is 99.7% pure copper, has led scientists to speculate that Ötzi was involved in copper smelting.

By examining the proportions of Ötzi's tibia, femur and pelvis, Christopher Ruff has determined that Ötzi's lifestyle included long walks over hilly terrain. This degree of mobility is not characteristic of other Copper Age Europeans. Ruff proposes that this may indicate that Ötzi was a high-altitude shepherd.

Using modern 3-D technology, a facial reconstruction has been created for the South Tyrol Museum of Archaeology in Bolzano, Italy. It shows Ötzi looking old for his 45 years, with deep-set brown eyes, a beard, a furrowed face, and sunken cheeks. He is depicted looking tired and ungroomed.

Ötzi apparently had whipworm ("Trichuris trichiura"), an intestinal parasite. During CT scans, it was observed that three or four of his right ribs had been cracked when he had been lying face down after death, or where the ice had crushed his body. One of his fingernails (of the two found) shows three Beau's lines indicating he was sick three times in the six months before he died. The last incident, two months before he died, lasted about two weeks. Also, it was found that his epidermis, the outer skin layer, was missing, a natural process from his mummification in ice. Ötzi's teeth showed considerable internal deterioration from cavities. These oral pathologies may have been brought about by his grain-heavy, high carbohydrate diet. DNA analysis in February 2012 revealed that Ötzi was lactose intolerant, supporting the theory that lactose intolerance was still common at that time, despite the increasing spread of agriculture and dairying.

Ötzi had a total of 61 tattoos (or Soot tattoos), consisting of 19 groups of black lines ranging from 1 to 3 mm in thickness and 7 to 40 mm long. These include groups of parallel lines running along the longitudinal axis of his body and to both sides of the lumbar spine, as well as a cruciform mark behind the right knee and on the right ankle, and parallel lines around the left wrist. The greatest concentration of markings is found on his legs, which together exhibit 12 groups of lines. A microscopic examination of samples collected from these tattoos revealed that they were created from pigment manufactured out of fireplace ash or soot.

Radiological examination of Ötzi's bones showed "age-conditioned or strain-induced degeneration" corresponding to many tattooed areas, including osteochondrosis and slight spondylosis in the lumbar spine and wear-and-tear degeneration in the knee and especially in the ankle joints. It has been speculated that these tattoos may have been related to pain relief treatments similar to acupressure or acupuncture. If so, this is at least 2,000 years before their previously known earliest use in China (c. 1000 BCE). Recent research into archaeological evidence for ancient tattooing has confirmed that Ötzi is the oldest tattooed human mummy yet discovered.

Ötzi's clothes were sophisticated. He wore a cloak made of woven grass and a coat, a belt, a pair of leggings, a loincloth and shoes, all made of leather of different skins. He also wore a bearskin cap with a leather chin strap. The shoes were waterproof and wide, seemingly designed for walking across the snow; they were constructed using bearskin for the soles, deer hide for the top panels, and a netting made of tree bark. Soft grass went around the foot and in the shoe and functioned like modern socks. The coat, belt, leggings and loincloth were constructed of vertical strips of leather sewn together with sinew. His belt had a pouch sewn to it that contained a cache of useful items: a scraper, drill, flint flake, bone awl and a dried fungus.

The shoes have since been reproduced by a Czech academic, who said that "because the shoes are actually quite complex, I'm convinced that even 5,300 years ago, people had the equivalent of a cobbler who made shoes for other people". The reproductions were found to constitute such excellent footwear that it was reported that a Czech company offered to purchase the rights to sell them. However, a more recent hypothesis by British archaeologist Jacqui Wood says that Ötzi's "shoes" were actually the upper part of snowshoes. According to this theory, the item currently interpreted as part of a "backpack" is actually the wood frame and netting of one snowshoe and animal hide to cover the face.

The leather loincloth and hide coat were made from sheepskin. Genetic analysis showed that the sheep species was nearer to modern domestic European sheep than to wild sheep; the items were made from the skins of at least four animals. Part of the coat was made from domesticated goat belonging to a mitochondrial haplogroup (a common female ancestor) that inhabits central Europe today.

The coat was made from several animals from two different species and was stitched together with hides available at the time.

The leggings were made from domesticated goat leather. A similar set of 6,500-year-old leggings discovered in Switzerland were made from goat leather which may indicate the goat leather was specifically chosen.

Shoelaces were made from the European genetic population of cattle. The quiver was made from wild roe deer, the fur hat was made from a genetic lineage of brown bear which lives in the region today. Writing in the journal "Scientific Reports", researchers from Ireland and Italy reported their analysis of mitochondrial DNA, that was extracted from nine fragments from six of his garments, including his loin cloth and fur cap.

Other items found with the Iceman were a copper axe with a yew handle, a chert-bladed knife with an ash handle and a quiver of 14 arrows with viburnum and dogwood shafts. Two of the arrows, which were broken, were tipped with flint and had fletching (stabilizing fins), while the other 12 were unfinished and untipped. The arrows were found in a quiver with what is presumed to be a bow string, an unidentified tool, and an antler tool which might have been used for sharpening arrow points. There was also an unfinished yew longbow that was long.

In addition, among Ötzi's possessions were berries, two birch bark baskets, and two species of polypore mushrooms with leather strings through them. One of these, the birch fungus, is known to have anthelmintic properties, and was probably used for medicinal purposes. The other was a type of tinder fungus, included with part of what appeared to be a complex firelighting kit. The kit featured pieces of over a dozen different plants, in addition to flint and pyrite for creating sparks.

Ötzi's copper axe was of particular interest. The axe's haft is long and made from carefully worked yew with a right-angled crook at the shoulder, leading to the blade. The long axe head is made of almost pure copper, produced by a combination of casting, cold forging, polishing, and sharpening. Despite the fact that copper ore sources in the Alpines are known to have been exploited at the time, a study indicated that the copper in the axe came from southern Tuscany. It was let into the forked end of the crook and fixed there using birch-tar and tight leather lashing. The blade part of the head extends out of the lashing and shows clear signs of having been used to chop and cut. At the time, such an axe would have been a valuable possession, important both as a tool and as a status symbol for the bearer.

Ötzi's full genome has been sequenced; the report on this was published on 28 February 2012. The Y-DNA of Ötzi belongs to a subclade of G defined by the SNPs M201, P287, P15, L223 and L91 (G-L91, ISOGG G2a2b, former "G2a4"). He was not typed for any of the subclades downstreaming from G-L91. G-L91 is now mostly found in South Corsica.

Analysis of his mitochondrial DNA showed that Ötzi belongs to the K1 subclade, but cannot be categorized into any of the three modern branches of that subclade (K1a, K1b or K1c). The new subclade has provisionally been named "K1ö" for "Ötzi". Multiplex assay study was able to confirm that the Iceman's mtDNA belongs to a previously unknown European mtDNA clade with a very limited distribution among modern data sets.

By autosomal DNA, Ötzi is most closely related to southern Europeans, especially to geographically isolated populations like Corsicans and Sardinians.

DNA analysis also showed him at high risk of atherosclerosis and lactose intolerance, with the presence of the DNA sequence of "Borrelia burgdorferi", possibly making him the earliest known human with Lyme disease. A later analysis suggested the sequence may have been a different "Borrelia" species.

A 2012 paper by paleoanthropologist John Hawks suggests that Ötzi had a higher degree of Neanderthal ancestry than modern Europeans.

In October 2013, it was reported that 19 modern Tyrolean men were related to Ötzi. Scientists from the Institute of Legal Medicine at Innsbruck Medical University had analysed the DNA of over 3,700 Tyrolean male blood donors and found 19 who shared a particular genetic mutation with the 5,300-year-old man.

In May 2012, scientists announced the discovery that Ötzi still had intact blood cells. These are the oldest complete human blood cells ever identified. In most bodies this old, the blood cells are either shrunken or mere remnants, but Ötzi's have the same dimensions as living red blood cells and resembled a modern-day sample.

In 2016, researchers reported on a study from the extraction of twelve samples from the gastrointestinal tract of Ötzi to analyze the origins of the "Helicobacter pylori" in his gut. The "H. pylori" strain found in his gastrointestinal tract was, surprisingly, the hpAsia2 strain, a strain today found primarily in South Asian and Central Asian populations, with extremely rare occurrences in modern European populations. The strain found in Ötzi's gut is most similar to three modern individuals from Northern India; the strain itself is, of course, older than the modern Northern Indian strain.

The cause of death remained uncertain until 10 years after the discovery of the body. It was initially believed that Ötzi died from exposure during a winter storm. Later it was speculated that Ötzi might have been a victim of a ritual sacrifice, perhaps for being a chieftain. This explanation was inspired by theories previously advanced for the first millennium BCE bodies recovered from peat bogs such as the Tollund Man and the Lindow Man.

In 2001 X-rays and a CT scan revealed that Ötzi had an arrowhead lodged in his left shoulder when he died, and a matching small tear on his coat. The discovery of the arrowhead prompted researchers to theorize Ötzi died of blood loss from the wound, which would probably have been fatal even if modern medical techniques had been available. Further research found that the arrow's shaft had been removed before death, and close examination of the body found bruises and cuts to the hands, wrists and chest and cerebral trauma indicative of a blow to the head. One of the cuts was to the base of his thumb that reached down to the bone but had no time to heal before his death. Currently, it is believed that Ötzi bled to death after the arrow shattered the scapula and damaged nerves and blood vessels before lodging near the lung.

Recent DNA analyses claim they revealed traces of blood from at least four other people on his gear: one from his knife, two from the same arrowhead, and a fourth from his coat. Interpretations of these findings were that Ötzi killed two people with the same arrow, and was able to retrieve it on both occasions, and the blood on his coat was from a wounded comrade he may have carried over his back. Ötzi's posture in death (frozen body, face down, left arm bent across the chest) could support a theory that before death occurred and rigor mortis set in, the Iceman was turned onto his stomach in the effort to remove the arrow shaft.

In 2010, it was proposed that Ötzi died at a much lower altitude and was buried higher in the mountains, as posited by archaeologist Alessandro Vanzetti of the Sapienza University of Rome and his colleagues. According to their study of the items found near Ötzi and their locations, it is possible that the iceman may have been placed above what has been interpreted as a stone burial mound but was subsequently moved with each thaw cycle that created a flowing watery mix driven by gravity before being re-frozen. While archaeobotanist Klaus Oeggl of the University of Innsbruck agrees that the natural process described probably caused the body to move from the ridge that includes the stone formation, he pointed out that the paper provided no compelling evidence to demonstrate that the scattered stones constituted a burial platform. Moreover, biological anthropologist Albert Zink argues that the iceman's bones display no dislocations that would have resulted from a downhill slide and that the intact blood clots in his arrow wound would show damage if the body had been moved up the mountain. In either case, the burial theory does not contradict the possibility of a violent cause of death.

Italian law entitled the Simons to a finders' fee from the South Tyrolean provincial government of 25% of the value of Ötzi. In 1994 the authorities offered a "symbolic" reward of 10 million lire (€5,200), which the Simons turned down. In 2003, the Simons filed a lawsuit which asked a court in Bolzano to recognize their role in Ötzi's discovery and declare them his "official discoverers". The court decided in the Simons' favor in November 2003, and at the end of December that year the Simons announced that they were seeking US$300,000 as their fee. The provincial government decided to appeal.

In addition, two people came forward to claim that they were part of the same mountaineering party that came across Ötzi and discovered the body first:


In 2005 the rival claims were heard by a Bolzano court. The legal case angered Mrs. Simon, who alleged that neither woman was present on the mountain that day. In 2005, Mrs. Simon's lawyer said: "Mrs. Simon is very upset by all this and by the fact that these two new claimants have decided to appear 14 years after Ötzi was found." In 2008, however, Jarc stated for a Slovene newspaper that she wrote twice to the Bolzano court in regard to her claim but received no reply whatsoever.

In 2004, Helmut Simon died. Two years later, in June 2006, an appeals court affirmed that the Simons had indeed discovered the Iceman and were therefore entitled to a finder's fee. It also ruled that the provincial government had to pay the Simons' legal costs. After this ruling, Mrs. Erika Simon reduced her claim to €150,000. The provincial government's response was that the expenses it had incurred to establish a museum and the costs of preserving the Iceman should be considered in determining the finder's fee. It insisted it would pay no more than €50,000. In September 2006, the authorities appealed the case to Italy's highest court, the Court of Cassation.

On 29 September 2008 it was announced that the provincial government and Mrs. Simon had reached a settlement of the dispute, under which she would receive €150,000 in recognition of Ötzi's discovery by her and her late husband and the tourist income that it attracts.

Influenced by the "Curse of the pharaohs" and the media theme of cursed mummies, claims have been made that Ötzi is cursed. The allegation revolves around the deaths of several people connected to the discovery, recovery and subsequent examination of Ötzi. It is alleged that they have died under mysterious circumstances. These persons include co-discoverer Helmut Simon and Konrad Spindler, the first examiner of the mummy in Austria in 1991. To date, the deaths of seven people, of which four were accidental, have been attributed to the alleged curse. In reality hundreds of people were involved in the recovery of Ötzi and are still involved in studying the body and the artifacts found with it. The fact that a small percentage of them have died over the years has not been shown to be statistically significant.






</doc>
<doc id="22743" url="https://en.wikipedia.org/wiki?curid=22743" title="Operation Deadlight">
Operation Deadlight

Operation "Deadlight" was the code name for the Royal Navy operation to scuttle German U-boats surrendered to the Allies after the defeat of Germany near the end of World War II.

Of the 156 U-boats that surrendered to the allies at the end of the war, 116 were scuttled as part of Operation "Deadlight". The operation was carried out by the Royal Navy and it was planned to tow the submarines to three areas about 100 miles north-west of Ireland and sink them. The areas were codenamed XX, YY and ZZ. The intention was to use XX as the main area for scuttling while 36 boats would be towed to ZZ for use as targets for aerial attack. YY was to be a reserve position where, if the weather was good enough, submarines could be diverted from XX to be sunk by naval forces. In the case of those submarines not being used as targets, the plan was to sink them with explosive charges, with naval gunfire as a fall-back option if that failed.

When Operation "Deadlight" was activated, it was found that many of the U-boats were in an extremely poor condition as a result of being moored in exposed harbours while awaiting disposal. Combined with poor weather, this meant that 56 of the boats sank before reaching the designated scuttling areas, and those which did, were generally sunk by gunfire rather than explosive charges. The first sinking took place on 17 November 1945 and the last on 11 February 1946.

Several U-boats escaped Operation "Deadlight". Some were claimed as prizes by Britain, France, Norway and the Soviet Union. Four were in East Asia when Germany surrendered and were commandeered by Japan ( was renamed "I-501", – "I-506", – "I-505", – "I-502", and a fifth boat, , had been sold to Japan in 1943 and renamed "RO-500"). Two U-boats that survived Operation "Deadlight" are today museum ships. was earmarked for scuttling, but American Rear Admiral Daniel V. Gallery argued successfully that she did not fall under Operation "Deadlight". United States Navy Task Group 22.3, under then-Captain Gallery, had captured "U-505" in battle on 4 June 1944. Having been captured, not surrendered at the end of the war, she survived to become a war memorial at the Museum of Science and Industry in Chicago. was transferred to Norway by Britain in October 1948 and became the Norwegian "Kaura". She was returned to Germany in 1965, to become a museum ship in 1971.

In the late 1990s, an approach was made to the British Ministry of Defence for salvage rights to the Operation "Deadlight" U-boats, by a firm which planned to raise up to a hundred of them. Because the U-boats were constructed in the pre-atomic age, the wrecks contain metals which are not radioactively tainted, and which are therefore valuable for certain research purposes. No salvage award was made, due to objections from Russia and the U.S. and potentially from Great Britain.

Between 2001 and 2003, nautical archaeologist Innes McCartney discovered and surveyed fourteen of the U-boat wrecks; including the rare Type XXI U-boat "U-2506", once under the command of Horst von Schroeter; the successful Type IXC U-boat, commanded by Adolf Piening and the "U-778", which was the most promising salvage.

In 2007, Derry City Council announced plans to raise the "U-778" to be the main exhibit of a new maritime museum. On 3 October 2007, an Irish diver, Michael Hanrahan, died whilst filming the wreck as part of the salvage project. In November 2009, a spokesman from the council's heritage museum service announced the salvage project had been cancelled for cost reasons.




</doc>
<doc id="22746" url="https://en.wikipedia.org/wiki?curid=22746" title="Order of the Eastern Star">
Order of the Eastern Star

The Order of the Eastern Star is a Masonic appendant body open to both men and women. It was established in 1850 by lawyer and educator Rob Morris, a noted Freemason, but was only adopted and approved as an appendant body of the Masonic Fraternity in 1873. The order is based on teachings from the Bible, but is open to people of all religious beliefs. It has approximately 10,000 chapters in twenty countries and approximately 500,000 members under its General Grand Chapter.

Members of the Order of the Eastern Star are aged 18 and older; men must be Master Masons and women must have specific relationships with Masons. Originally, a woman would have to be the daughter, widow, wife, sister, or mother of a master Mason, but the Order now allows other relatives as well as allowing Job's Daughters, Rainbow Girls, Members of the Organization of Triangles (NY only) and members of the Constellation of Junior Stars (NY only) to become members when of age.

The Order was created by Rob Morris in 1850 when he was teaching at the Eureka Masonic College in Richland, Mississippi. While confined by illness, he set down the principles of the order in his "Rosary of the Eastern Star". By 1855, he had organized a "Supreme Constellation" in New York, which chartered chapters throughout the United States.

In 1866, Dr. Morris started working with Robert Macoy, and handed the Order over to him while Morris was traveling in the Holy Land. Macoy organized the current system of Chapters, and modified Dr. Morris' "Rosary" into a "Ritual".

On December 1, 1874, Queen Esther Chapter No. 1 became the first Prince Hall Affiliate chapter of the Order of the Eastern Star when it was established in Washington, D.C. by Thornton Andrew Jackson.

The "General Grand Chapter" was formed in Indianapolis, Indiana on November 6, 1876. Committees formed at that time created the "Ritual of the Order of the Eastern Star" in more or less its current form.

The emblem of the Order is a five-pointed star with the white ray of the star pointing downwards towards the manger. The letters FATAL surrounding the center pentagon in the emblem stands for "Fairest Among Thousands, Altogether Lovely," a reference to Song of Songs. In the Chapter room, the downward-pointing white ray points to the West. The character-building lessons taught in the Order are stories inspired by Biblical figures:

There are 18 main officers in a full chapter:

Traditionally, a woman who is elected Associate Conductress will be elected to Conductress the following year, then the next year Associate Matron, and the next year Worthy Matron. A man elected Associate Patron will usually be elected Worthy Patron the following year. Usually the woman who is elected to become Associate Matron will let it be known who she wishes to be her Associate Patron, so the next year they will both go to the East together as Worthy Matron and Worthy Patron. There is no male counterpart to the Conductress and Associate Conductress. Only women are allowed to be Matrons, Conductresses, and the Star Points (Adah, Ruth, etc.) and only men can be Patrons.

Once a member has served a term as Worthy Matron or Worthy Patron, they may use the post-nominal letters, PM or PP respectively.

The General Grand Chapter headquarters, the International Temple, is located in the Dupont Circle neighborhood of Washington, D.C., in the Perry Belmont Mansion. The mansion was built in 1909 for the purpose of entertaining the guests of Perry Belmont. They included Britain's Prince of Wales in 1919. General Grand Chapter purchased the building in 1935. The secretary of General Grand Chapter lives there while serving his or her term of office. The mansion features works of art from around the world, most of which were given as gifts from various international Eastern Star chapters.

The Order has a charitable foundation and from 1986-2001 contributed $513,147 to Alzheimer's disease research, juvenile diabetes research, and juvenile asthma research. It also provides bursaries to students of theology and religious music, as well as other scholarships that differ by jurisdiction. In 2000 over $83,000 was donated. Many jurisdictions support a Masonic and/or Eastern Star retirement center or nursing home for older members; some homes are also open to the public. The Elizabeth Bentley OES Scholarship Fund was started in 1947.




</doc>
<doc id="22747" url="https://en.wikipedia.org/wiki?curid=22747" title="OSI model">
OSI model

The Open Systems Interconnection model (OSI model) is a conceptual model that characterizes and standardizes the communication functions of a telecommunication or computing system without regard to its underlying internal structure and technology. Its goal is the interoperability of diverse communication systems with standard protocols. The model partitions a communication system into abstraction layers. The original version of the model defined seven layers.

A layer serves the layer above it and is served by the layer below it. For example, a layer that provides error-free communications across a network provides the path needed by applications above it, while it calls the next lower layer to send and receive packets that comprise the contents of that path. Two instances at the same layer are visualized as connected by a "horizontal" connection in that layer.

The model is a product of the Open Systems Interconnection project at the International Organization for Standardization (ISO), maintained by the identification ISO/IEC 7498-1.

In the late 1970s, one project was administered by the International Organization for Standardization (ISO), while another was undertaken by the International Telegraph and Telephone Consultative Committee (CCITT, from French: Comité Consultatif International Téléphonique et Télégraphique). These two international standards bodies each developed a document that defined similar networking models.

In 1983, these two documents were merged to form a standard called The Basic Reference Model for Open Systems Interconnection. The standard is usually referred to as the Open Systems Interconnection Reference Model, the OSI Reference Model, or simply the OSI model. It was published in 1984 by both the ISO, as standard ISO 7498, and the renamed CCITT (now called the Telecommunications Standardization Sector of the International Telecommunication Union or ITU-T) as standard X.200.

OSI had two major components, an "abstract model" of networking, called the Basic Reference Model or seven-layer model, and a set of specific protocols.

The concept of a seven-layer model was provided by the work of Charles Bachman at Honeywell Information Services. Various aspects of OSI design evolved from experiences with the ARPANET, NPLNET, EIN, CYCLADES network and the work in IFIP WG6.1. The new design was documented in ISO 7498 and its various addenda. In this model, a networking system was divided into layers. Within each layer, one or more entities implement its functionality. Each entity interacted directly only with the layer immediately beneath it, and provided facilities for use by the layer above it.

Protocols enable an entity in one host to interact with a corresponding entity at the same layer in another host. Service definitions abstractly described the functionality provided to an (N)-layer by an (N-1) layer, where N was one of the seven layers of protocols operating in the local host.

The OSI standards documents are available from the ITU-T as the X.200-series of recommendations. Some of the protocol specifications were also available as part of the ITU-T X series. The equivalent ISO and ISO/IEC standards for the OSI model were available from ISO. Not all are free of charge.

The recommendation X.200 describes seven layers, labeled 1 to 7. Layer 1 is the lowest layer in this model.

At each level "N", two entities at the communicating devices (layer N "peers") exchange protocol data units (PDUs) by means of a layer N "protocol". Each PDU contains a payload, called the service data unit (SDU), along with protocol-related headers or footers.

Data processing by two communicating OSI-compatible devices is done as such:

Some orthogonal aspects, such as management and security, involve all of the layers (See ITU-T X.800 Recommendation). These services are aimed at improving the CIA triad - confidentiality, integrity, and availability - of the transmitted data. In practice, the availability of a communication service is determined by the interaction between network design and network management protocols. Appropriate choices for both of these are needed to protect against denial of service.

The physical layer is responsible for transmission and reception of unstructured raw data between a device and a physical transmission medium. Layer specifications define characteristics such as voltage levels, timing of voltage changes, physical data rates, maximum transmission distances, and physical connectors. This includes the layout of pins, voltages, line impedance, cable specifications, signal timing and frequency for wireless devices. Bit rate control is done at the physical layer and may define transmission mode as simplex, half duplex, and full duplex. The components of a physical layer can be described in terms of a network topology. Examples of protocols using the physical layer include Bluetooth, Ethernet, and USB.

The data link layer provides node-to-node data transfer—a link between two directly connected nodes. It detects and possibly corrects errors that may occur in the physical layer.
It defines the protocol to establish and terminate a connection between two physically connected devices. It also defines the protocol for flow control between them.

IEEE 802 divides the data link layer into two sublayers:

The MAC and LLC layers of IEEE 802 networks such as 802.3 Ethernet, 802.11 Wi-Fi, and 802.15.4 ZigBee operate at the data link layer.

The Point-to-Point Protocol (PPP) is a data link layer protocol that can operate over several different physical layers, such as synchronous and asynchronous serial lines.

The ITU-T G.hn standard, which provides high-speed local area networking over existing wires (power lines, phone lines and coaxial cables), includes a complete data link layer that provides both error correction and flow control by means of a selective-repeat sliding-window protocol.

The network layer provides the functional and procedural means of transferring variable length data sequences (called packets) from one node to another connected in "different networks". A network is a medium to which many nodes can be connected, on which every node has an "address" and which permits nodes connected to it to transfer messages to other nodes connected to it by merely providing the content of a message and the address of the destination node and letting the network find the way to deliver the message to the destination node, possibly routing it through intermediate nodes. If the message is too large to be transmitted from one node to another on the data link layer between those nodes, the network may implement message delivery by splitting the message into several fragments at one node, sending the fragments independently, and reassembling the fragments at another node. It may, but does not need to, report delivery errors.

Message delivery at the network layer is not necessarily guaranteed to be reliable; a network layer protocol may provide reliable message delivery, but it need not do so.

A number of layer-management protocols, a function defined in the "management annex", ISO 7498/4, belong to the network layer. These include routing protocols, multicast group management, network-layer information and error, and network-layer address assignment. It is the function of the payload that makes these belong to the network layer, not the protocol that carries them.

The transport layer provides the functional and procedural means of transferring variable-length data sequences from a source to a destination host, while maintaining the quality of service functions.

The transport layer controls the reliability of a given link through flow control, segmentation/desegmentation, and error control. Some protocols are state- and connection-oriented. This means that the transport layer can keep track of the segments and re-transmit those that fail delivery. The transport layer also provides the acknowledgement of the successful data transmission and sends the next data if no errors occurred. The transport layer creates segments out of the message received from the application layer. Segmentation is the process of dividing a long message into smaller messages.

OSI defines five classes of connection-mode transport protocols ranging from class 0 (which is also known as TP0 and provides the fewest features) to class 4 (TP4, designed for less reliable networks, similar to the Internet). Class 0 contains no error recovery, and was designed for use on network layers that provide error-free connections. Class 4 is closest to TCP, although TCP contains functions, such as the graceful close, which OSI assigns to the session layer. Also, all OSI TP connection-mode protocol classes provide expedited data and preservation of record boundaries. Detailed characteristics of TP0-4 classes are shown in the following table:

An easy way to visualize the transport layer is to compare it with a post office, which deals with the dispatch and classification of mail and parcels sent. A post office inspects only the outer envelope of mail to determine its delivery. Higher layers may have the equivalent of double envelopes, such as cryptographic presentation services that can be read by the addressee only. Roughly speaking, tunneling protocols operate at the transport layer, such as carrying non-IP protocols such as IBM's SNA or Novell's IPX over an IP network, or end-to-end encryption with IPsec. While Generic Routing Encapsulation (GRE) might seem to be a network-layer protocol, if the encapsulation of the payload takes place only at the endpoint, GRE becomes closer to a transport protocol that uses IP headers but contains complete Layer 2 frames or Layer 3 packets to deliver to the endpoint. L2TP carries PPP frames inside transport segments.

Although not developed under the OSI Reference Model and not strictly conforming to the OSI definition of the transport layer, the Transmission Control Protocol (TCP) and the User Datagram Protocol (UDP) of the Internet Protocol Suite are commonly categorized as layer-4 protocols within OSI.

The session layer controls the dialogues (connections) between computers. It establishes, manages and terminates the connections between the local and remote application. It provides for full-duplex, half-duplex, or simplex operation, and establishes checkpointing, adjournment, termination, and restart procedures. The OSI model made this layer responsible for graceful close of sessions, which is a property of the Transmission Control Protocol, and also for session checkpointing and recovery, which is not usually used in the Internet Protocol Suite. The session layer is commonly implemented explicitly in application environments that use remote procedure calls.

The presentation layer establishes context between application-layer entities, in which the application-layer entities may use different syntax and semantics if the presentation service provides a mapping between them. If a mapping is available, presentation protocol data units are encapsulated into session protocol data units and passed down the protocol stack.

This layer provides independence from data representation by translating between application and network formats. The presentation layer transforms data into the form that the application accepts. This layer formats data to be sent across a network. It is sometimes called the syntax layer. The presentation layer can include compression functions. The Presentation Layer negotiates the Transfer Syntax.

The original presentation structure used the Basic Encoding Rules of Abstract Syntax Notation One (ASN.1), with capabilities such as converting an EBCDIC-coded text file to an ASCII-coded file, or serialization of objects and other data structures from and to XML. ASN.1 effectively makes an application protocol invariant with respect to syntax.

The application layer is the OSI layer closest to the end user, which means both the OSI application layer and the user interact directly with the software application. This layer interacts with software applications that implement a communicating component. Such application programs fall outside the scope of the OSI model. Application-layer functions typically include identifying communication partners, determining resource availability, and synchronizing communication. When identifying communication partners, the application layer determines the identity and availability of communication partners for an application with data to transmit. The most important distinction in the application layer is the distinction between the application-entity and the application. For example, a reservation website might have two application-entities: one using HTTP to communicate with its users, and one for a remote database protocol to record reservations. Neither of these protocols have anything to do with reservations. That logic is in the application itself. The application layer per se has no means to determine the availability of resources in the network.

Cross-layer functions are services that are not tied to a given layer, but may affect more than one layer. Examples include the following:

Neither the OSI Reference Model nor OSI protocols specify any programming interfaces, other than deliberately abstract service specifications. Protocol specifications precisely define the interfaces between different computers, but the software interfaces inside computers, known as network sockets are implementation-specific.

For example, Microsoft Windows' Winsock, and Unix's Berkeley sockets and System V Transport Layer Interface, are interfaces between applications (layer 5 and above) and the transport (layer 4). NDIS and ODI are interfaces between the media (layer 2) and the network protocol (layer 3).

Interface standards, except for the physical layer to media, are approximate implementations of OSI service specifications.

The design of protocols in the TCP/IP model of the Internet does not concern itself with strict hierarchical encapsulation and layering. RFC 3439 contains a section entitled "Layering considered harmful". TCP/IP does recognize four broad layers of functionality which are derived from the operating scope of their contained protocols: the scope of the software application; the end-to-end transport connection; the internetworking range; and the scope of the direct links to other nodes on the local network.

Despite using a different concept for layering than the OSI model, these layers are often compared with the OSI layering scheme in the following way:
These comparisons are based on the original seven-layer protocol model as defined in ISO 7498, rather than refinements in such things as the internal organization of the network layer document.

The presumably strict layering of the OSI model as it is usually described does not present contradictions in TCP/IP, as it is permissible that protocol usage does not follow the hierarchy implied in a layered model. Such examples exist in some routing protocols (for example OSPF), or in the description of tunneling protocols, which provide a link layer for an application, although the tunnel host protocol might well be a transport or even an application-layer protocol in its own right.



</doc>
<doc id="22751" url="https://en.wikipedia.org/wiki?curid=22751" title="Original Sin (2001 film)">
Original Sin (2001 film)

Original Sin is a 2001 erotic thriller film starring Antonio Banderas and Angelina Jolie. It is based on the novel "Waltz into Darkness" by Cornell Woolrich, and is a remake of the 1969 François Truffaut film "Mississippi Mermaid". The movie was produced by actress Michelle Pfeiffer's production company, Via Rosa Productions. 

"Original Sin" is set in the late 19th century Cuba during the Spanish rule, and flashes back and forth from the scene of a woman awaiting her execution by garrote while telling her story to a priest, to the actual events of that story. 

Luis Vargas (Antonio Banderas) sends for American Julia Russell (Angelina Jolie) from Delaware to sail to his country Cuba to be his mail-order bride. Julia alights from the ship, looking nothing like the photos she had sent prior to her voyage. Julia explains she wants more than a man who is only interested in a pretty face and that is why she has been deceptive—substituting a plain-looking woman's photo in place of her own picture. Luis also admits to a deception; he has been misleading her into believing that he is a poor clerk in a coffee export house, instead of being the rich owner of that coffee export house. On hearing this, Julia says that they both have something in common and that is that both are not to be trusted. But they assure each other that they would make efforts in understanding and trusting each other in life. 

Luis and Julia wed in the church within hours of her setting foot in Cuba. Luis falls desperately in love with his new wife, and they passionately make love.

Meanwhile, Julia's sister Emily has been trying to contact her, worried about her after such a long trip to a strange land. She sends an emotional letter to Julia asking about her welfare. Luis forces Julia to write back, fearing that if Julia continues to ignore Emily's letters, Emily will assume something terrible has befallen her sister and she might send the authorities to check on her welfare. Holding off as long as possible, Julia finally pens a letter to her sister.

In order to assure that his wife has everything she wants, Luis adds Julia to his business and personal bank accounts, giving her free rein to spend as she pleases. A detective, Walter Downs (Thomas Jane), arrives from Wilmington and tells Luis that he has been hired by Emily to find her sister Julia and would like to see her on the coming Sunday. Luis informs Julia about this and she gets upset. Emily arrives in Cuba to meet Luis, and shows the letter Julia wrote to her. She informs Luis that she believes Julia to be an impostor and that her sister may be dead. Luis discovers that Julia has taken nearly all of his fortune and disconsolate, teams up with Walter to look for her.

Luis finds Julia and discovers she is actually working with Walter and that she and Luis are staying at the same hotel. Luis believes she loves him and lies to Walter, but when confronted, a fight breaks out and Luis shoots Walter. Julia coldly tells Luis to go and buy them tickets home, but the minute he leaves, Walter gets to his feet; he had loaded the gun with blanks. Julia appears to love Luis, but Walter has too much control over her, so she continues to work for him as she and Luis run off to live in secret, with the supposedly dead Walter in pursuit. Walter turns out to be Julia (Bonny's) old lover and partner Billy.

Luis throws away his promising future and opens himself to living a lie with Julia. One night, Luis follows Julia/Bonny and discovers Walter/Billy is alive and that the two are still working together; she is apparently going to poison her husband that very night. He returns home to wait for her, and when she arrives, he reveals that he knows about the plan, confesses his love for her once more and swallows the poisoned drink though she desperately tries to stop him. Julia flees with the dying Luis, with Walter close behind. They run into him at a train station; Walter is furious that Julia has betrayed him. As Walter holds a knife to her throat, Luis shoots and wounds him, with Julia finishing him off.

Back in the "mise en scene", Julia finishes her story and asks the priest to pray with her. The next morning the guards come to her cell to take her to her execution, only to find the priest kneeling in her clothing.

In Morocco, Julia is watching a card game. She walks around the table occupied by gamblers—including Luis—and thanks them for allowing her to watch. As Julia signals Luis about the other players' cards, he begins telling them the story of how they got there.

"Original Sin" was poorly received by critics. It currently holds a 12% rating on Rotten Tomatoes based on 91 reviews with the consensus stating: "Laughably melodramatic, "Original Sin" features bad acting, poor dialogue and even worse plotting."

Film critic Roger Ebert gave the movie a positive review and said about Jolie's performance, "Jolie continues to stalk through pictures entirely on her own terms. Her presence is like a dare-ya for a man. There's dialogue in this movie so overwrought, it's almost literally unspeakable, and she survives it by biting it off contemptuously and spitting it out."

Angelina Jolie was nominated for a Golden Raspberry Award for Worst Actress for her work in both this film and "", but lost the trophy to Mariah Carey for "Glitter".




</doc>
<doc id="22753" url="https://en.wikipedia.org/wiki?curid=22753" title="Oscar Hammerstein II">
Oscar Hammerstein II

Oscar Greeley Clendenning Hammerstein II (; July 12, 1895 – August 23, 1960) was an American librettist, theatrical producer, and (usually uncredited) theatre director of musicals for almost forty years. Hammerstein won eight Tony Awards and two Academy Awards for Best Original Song. Many of his songs are standard repertoire for vocalists and jazz musicians. He co-wrote 850 songs. Hammerstein was the lyricist and playwright in his partnerships; his collaborators wrote the music. Hammerstein collaborated with numerous composers, such as Jerome Kern, with whom he wrote "Show Boat", Vincent Youmans, Rudolf Friml, Richard A. Whiting and Sigmund Romberg; but he is best known for his collaborations with Richard Rodgers, as the duo Rodgers and Hammerstein which include "Oklahoma!", "Carousel", "South Pacific", "The King and I", and "The Sound of Music".

Oscar Greeley Clendenning Hammerstein II was born in New York City, the son of Alice Hammerstein (née Nimmo) and theatrical manager William Hammerstein. His grandfather was the German theatre impresario Oscar Hammerstein I. His father was from a Jewish family, and his mother was the daughter of Scottish and English parents. He was raised Episcopalian.

Although Hammerstein's father managed the Victoria Theatre for his father and was a producer of vaudeville shows, he was opposed to his son's desire to participate in the arts. Hammerstein attended Columbia University (1912–1916) and studied at Columbia Law School until 1917. As a student, he maintained high grades and engaged in numerous extracurricular activities. These included playing first base on the baseball team, performing in the Varsity Show and becoming an active member of Pi Lambda Phi, a mostly Jewish fraternity.

When he was 19, and still a student at Columbia, his father died of Bright's disease, June 10, 1914, symptoms of which doctors originally attributed to scarlet fever. On the train trip to the funeral with his brother, he read the headlines in the "New York Herald": "Hammerstein's Death a Shock to the Theater Circle." The "New York Times" wrote, "Hammerstein, the Barnum of Vaudeville, Dead at Forty." When he and his brother arrived home, they attended their father's funeral with their grandfather, and more than a thousand others, at Temple Israel in Harlem, and took part in the ceremonies held in the Jewish tradition. Two hours later, "taps was sounded over Broadway," writes biographer Hugh Fordin.

After his father's death, he participated in his first play with the Varsity Show, entitled "On Your Way". Throughout the rest of his college career, Hammerstein wrote and performed in several Varsity Shows.

After quitting law school to pursue theatre, Hammerstein began his first professional collaboration, with Herbert Stothart, Otto Harbach and Frank Mandel. He began as an apprentice and went on to form a 20-year collaboration with Harbach. Out of this collaboration came his first musical, "Always You", for which he wrote the book and lyrics. It opened on Broadway in 1920. In 1921 Hammerstein joined The Lambs club.

Throughout the next forty years, Hammerstein teamed with many other composers, including Jerome Kern, with whom Hammerstein enjoyed a highly successful collaboration. In 1927, Kern and Hammerstein had their biggest hit, "Show Boat", which is often revived and is still considered one of the masterpieces of the American musical theatre. "Here we come to a completely new genre — the musical play as distinguished from musical comedy. Now ... the play was the thing, and everything else was subservient to that play. Now ... came complete integration of song, humor and production numbers into a single and inextricable artistic entity." Many years later, Hammerstein's wife Dorothy bristled when she heard a remark that Jerome Kern had written "Ol' Man River." "Indeed not," she retorted. "Jerome Kern wrote 'dum, dum, dum-dum.' My husband wrote 'Ol' Man River'."

Other Kern-Hammerstein musicals include "Sweet Adeline", "Music in the Air", "Three Sisters", and "Very Warm for May". Hammerstein also collaborated with Vincent Youmans ("Wildflower"), Rudolf Friml ("Rose-Marie"), and Sigmund Romberg ("The Desert Song" and "The New Moon").

Hammerstein's most successful and sustained collaboration began when he teamed up with Richard Rodgers to write a musical adaptation of the play "Green Grow the Lilacs". Rodgers' first partner, Lorenz Hart, originally planned to collaborate with Rodgers on this piece, but his alcoholism had become out of control, and he was unable to write. Hart was also not certain that the idea had much merit, and the two therefore separated. The adaptation became the first Rodgers and Hammerstein collaboration, entitled "Oklahoma!", which opened on Broadway in 1943. It furthered the revolution begun by "Show Boat", by thoroughly integrating all the aspects of musical theatre, with the songs and dances arising out of and further developing the plot and characters. William A. Everett and Paul R. Laird wrote that this was a "show, that, like 'Show Boat', became a milestone, so that later historians writing about important moments in twentieth-century theatre would begin to identify eras according to their relationship to 'Oklahoma.'" After "Oklahoma!", Rodgers and Hammerstein were the most important contributors to the musical-play form – with such masterworks as "Carousel", "The King and I" and "South Pacific". The examples they set in creating vital plays, often rich with social thought, provided the necessary encouragement for other gifted writers to create musical plays of their own".

The partnership went on to produce these and other Broadway musicals such as "Allegro", "Me and Juliet", "Pipe Dream", "Flower Drum Song", and "The Sound of Music", as well as the musical film "State Fair" (and its stage adaptation of the same name), and the television musical "Cinderella", all featured in the revue "A Grand Night for Singing". Hammerstein also wrote the book and lyrics for "Carmen Jones", an adaptation of Georges Bizet's opera "Carmen" with an all-black cast that became a 1943 Broadway musical and a 1954 film.

An active advocate for writers's rights within the theatre industry, Hammerstein was a member of the Dramatists Guild of America. In 1956, he was elected as the eleventh president of the non-profit organization. He continued his presidency at the Guild until 1960. 

Hammerstein died of stomach cancer on August 23, 1960, at his home Highland Farm in Doylestown, Pennsylvania, at 65, shortly after the opening of "The Sound of Music" on Broadway. The final song he wrote was "Edelweiss", which was added near the end of the second act during rehearsal. This was not an Austrian folk song, but had been written specifically for the musical. After his death, "The Sound of Music" was made into the hit 1965 film adaptation, which won the Academy Award for Best Picture.

The lights of Times Square were turned off for one minute, and London's West End lights were dimmed in recognition of his contribution to the musical. He was cremated, and his ashes were buried at the Ferncliff Cemetery in Hartsdale, New York. A memorial plaque was unveiled at Southwark Cathedral, England, on May 24, 1961. He was survived by his second wife, Dorothy, his three children and two stepchildren.

Hammerstein married his second wife, the Australian-born Dorothy (Blanchard) Jacobson (1899–1987), on May 13, 1929. He had three children: William Hammerstein (1918–2001) and Alice Hammerstein Mathias by his first wife, Myra Finn, and James Hammerstein by Blanchard. By Dorothy he also had a stepdaughter, Susan Blanchard (whose four husbands included Henry Fonda and Richard Widmark), and a stepson, Henry Jacobson.

Hammerstein was one of the most important "book writers" in Broadway history – he made the story, not the songs or the stars, central to the musical and brought musical theater to full maturity as an art form. According to Stephen Sondheim, "What few people understand is that Oscar's big contribution to the theater was as a theoretician, as a Peter Brook, as an innovator. People don't understand how experimental "Show Boat" and "Oklahoma!" felt at the time they were done. Oscar is not about the 'lark that is learning to pray' – that's easy to make fun of. He's about "Allegro"."

His reputation for being sentimental is based largely on the movie versions of the musicals, especially "The Sound of Music", in which a song sung by those in favor of reaching an accommodation with the Nazis, "No Way to Stop It", was cut. As recent revivals of "Show Boat", "Oklahoma!", "Carousel", and "The King and I" in London and New York show, Hammerstein was one of the more tough-minded and socially conscious American musical theater artists. According to Richard Kislan, "The shows of Rodgers and Hammerstein were the product of sincerity. In the light of criticism directed against them and their universe of sweetness and light, it is important to understand that they believed sincerely in what they wrote." According to Marc Bauch, "The Rodgers and Hammerstein musicals are romantic musical plays. Love is important."

According to "The Rodgers and Hammerstein Story" by Stanley Green, "For three minutes, on the night of September first, the entire Times Square area in New York City was blacked out in honor of the man who had done so much to light up that particular part of the world. From 8:57 to 9:00 p.m., every neon sign and every light bulb was turned off and all traffic was halted between 42nd Street and 53rd Street, and between 8th Ave and the Avenue of the Americas. A crowd of 5,000 people, many with heads bowed, assembled at the base of the statue of Father Duffy on Times Square where two trumpeters blew taps. It was the most complete blackout on Broadway since World War II, and the greatest tribute of its kind ever paid to one man."

Hammerstein contributed the lyrics to 850 songs, according to "The Complete Lyrics of Oscar Hammerstein II", edited by Amy Asch. Some well-known songs are "Ol' Man River", "Can't Help Lovin' That Man" and "Make Believe" from "Show Boat"; "Indian Love Call" from "Rose-Marie"; "People Will Say We're in Love" and "Oklahoma" (which has been the official state song of Oklahoma since 1953) from "Oklahoma!"; "Some Enchanted Evening", from "South Pacific"; "Getting to Know You" and "Shall We Dance" from "The King and I"; and the title song as well as "Climb Ev'ry Mountain" from "The Sound of Music".

Several albums of Hammerstein's musicals were named to the "Songs of the Century" list as compiled by the Recording Industry Association of America (RIAA), the National Endowment for the Arts, and Scholastic Corporation:

Hammerstein won two Oscars for best original song—in 1941 for "The Last Time I Saw Paris" in the film "Lady Be Good", and in 1945 for "It Might as Well Be Spring" in "State Fair." In 1950, the team of Rodgers and Hammerstein received The Hundred Year Association of New York's Gold Medal Award "in recognition of outstanding contributions to the City of New York."

Hammerstein won eight Tony Awards, six for lyrics or book, and two as producer of the Best Musical ("South Pacific" and "The Sound of Music"). Rodgers and Hammerstein began writing together before the era of the Tonys: "Oklahoma!" opened in 1943 and "Carousel" in 1945, and the Tony Awards were not awarded until 1947. They won a special Pulitzer Prize in 1944 for "Oklahoma!" and, with Joshua Logan, the annual Pulitzer Prize for Drama in 1950 for "South Pacific". The Oscar Hammerstein II Center for Theater Studies at Columbia University was established in 1981 with a $1-million gift from his family.

His advice and work influenced Stephen Sondheim, a friend of the Hammerstein family from childhood. Sondheim has attributed his success in theater, and especially as a lyricist, directly to Hammerstein's influence and guidance.

The Oscar Hammerstein Award for Lifetime Achievement in Musical Theatre is presented annually. The York Theatre Company in New York City is the Administrator of the award. The 2009 winners were Jerry Bock and Sheldon Harnick. Past awardees are composers such as Stephen Sondheim and performers such as Carol Channing. The 2010 award went to Thomas Meehan.

Oscar Hammerstein was a member of the American Theater Hall of Fame.




</doc>
<doc id="22756" url="https://en.wikipedia.org/wiki?curid=22756" title="Otto Jespersen">
Otto Jespersen

Jens Otto Harry Jespersen (; 16 July 1860 – 30 April 1943) was a Danish linguist who specialized in the grammar of the English language.

Otto Jespersen was born in Randers in Jutland. He was inspired by the work of Danish philologist Rasmus Rask as a boy, and with the help of Rask's grammars taught himself some Icelandic, Italian, and Spanish. He entered the University of Copenhagen in 1877 when he was 17, initially studying law but not forgetting his language studies. In 1881 he shifted his focus completely to languages, and in 1887 earned his master's degree in French, with English and Latin as his secondary languages. He supported himself during his studies through part-time work as a schoolteacher and as a shorthand reporter in the Danish parliament. In 1887–1888, he traveled to England, Germany and France, meeting linguists like Henry Sweet and Paul Passy and attending lectures at institutions like Oxford University. Following the advice of his mentor Vilhelm Thomsen, he returned to Copenhagen in August 1888 and began work on his doctoral dissertation on the English case system. He successfully defended his dissertation in 1891.

Jespersen was a professor of English at the University of Copenhagen from 1893 to 1925, and served as Rector of the university in 1920–21. His early work focused primarily on language teaching reform and on phonetics, but he is best known for his later work on syntax and on language development.

He advanced the theories of "Rank" and "Nexus" in Danish in two papers: "Sprogets logik" (1913) and "De to hovedarter af grammatiske forbindelser" (1921). Jespersen in this theory of ranks removes the parts of speech from the syntax, and differentiates between primaries, secondaries, and tertiaries; e.g. in ""well honed phrase"," "phrase" is a primary, this being defined by a secondary, "honed", which again is defined by a tertiary "well". The term "Nexus" is applied to sentences, structures similar to sentences and sentences in formation, in which two concepts are expressed in one unit; e.g., "it rained, he ran indoors". This term is qualified by a further concept called a "junction" which represents one idea, expressed by means of two or more elements, whereas a nexus combines two ideas. Junction and nexus proved valuable in bringing the concept of context to the forefront of the attention of the world of linguistics.

He was most widely recognized for some of his books. "Language: Its Nature, Development and Origin" (1922) is considered by many to be his masterpiece. "Modern English Grammar on Historical Principles" (1909–1949), concentrated on morphology and syntax, and "Growth and Structure of the English Language" (1905) is a comprehensive view of English by someone with another native language, and still in print, over 70 years after his death and more than 100 years after publication. Late in his life he published "Analytic Syntax" (1937), in which he presents his views on syntactic structure using an idiosyncratic shorthand notation. In "The Philosophy of Grammar" (1924) he challenged the accepted views of common concepts in Grammar and proposed corrections to the basic definitions of grammatical case, pronoun, object, voice etc., and developed further his notions of "Rank" and "Nexus". In the 21st century this book is still used as one of the basic texts in modern Structural linguistics. "Mankind, Nation and Individual: from a linguistic point of view" (1925) is one of the pioneering works on Sociolinguistics.

Jespersen visited the United States twice: he lectured at the Congress of Arts and Sciences in St. Louis in 1904, and in 1909–1910 he visited both the University of California and Columbia University. While in the U.S., he took occasion to study the country's educational system. His autobiography (see below) was published in English translation as recently as 1995.

Jespersen was a proponent of phonosemanticism and wrote: “Is there really much more logic in the opposite extreme which denies any kind of sound symbolism (apart from the small class of evident echoisms and ‘onomatopoeia’) and sees in our words only a collection of accidental and irrational associations of sound and meaning? ...There is no denying that there are words which we feel instinctively to be adequate to express the ideas they stand for.”

After his retirement in 1925, Jespersen remained active in the international linguistic community. In addition to continuing to write, he convened and chaired the first International Meeting on Linguistic Research in Geneva in 1930, and acted as president of the Fourth International Congress of Linguists in Copenhagen in 1936.

Jespersen was an important figure in the international language movement. He was an early supporter of the Esperanto offshoot Ido and in 1927 published his own project Novial. He also worked with the International Auxiliary Language Association.

Jespersen received honorary degrees from Columbia University in New York (1910), St. Andrews University in Scotland (1925), and the Sorbonne in Paris (1927). He was one of the first six international scholars to be elected as honorary members of the Linguistic Society of America.






</doc>
<doc id="22758" url="https://en.wikipedia.org/wiki?curid=22758" title="List of object-oriented programming languages">
List of object-oriented programming languages

This is a list of notable programming languages with object-oriented programming (OOP) features, which are also listed in . Note that, in some contexts, the definition of an "object-oriented programming language" is not exactly the same as that of a "programming language with object-oriented features". For example, C++ is a multi-paradigm language including object-oriented paradigm; however, it is less object-oriented than some other languages such as Python and Ruby. Therefore, someone considers C++ as an OOP language, while others do not or prefer to name it as "semi-object-oriented programming language".




</doc>
<doc id="22759" url="https://en.wikipedia.org/wiki?curid=22759" title="OOP">
OOP

OOP, Oop, or oop may refer to:





</doc>
<doc id="22760" url="https://en.wikipedia.org/wiki?curid=22760" title="Occidental">
Occidental

Occidental may refer to:






</doc>
<doc id="22761" url="https://en.wikipedia.org/wiki?curid=22761" title="Occidental language">
Occidental language

The language Occidental, later Interlingue, is a planned international auxiliary language created by the Balto-German naval officer and teacher Edgar de Wahl, and published in 1922. The vocabulary is based on already existing words from various languages. The language is thereby naturalistic, at the same time as it is constructed to be regular. Occidental was quite popular in the years before the Second World War, but declined in the years thereafter.

Occidental is devised so that many of its derived word forms reflect the similar forms common to a number of Western European languages, primarily those in the Romance family. This was done through application of de Wahl's rule which is a set of rules for converting verb infinitives into derived nouns and adjectives. The result is a language easy to understand at first sight for individuals acquainted with several Western European languages. Coupled with a simplified grammar, this made Occidental exceptionally popular in Europe during the 15 years before World War II.

In "The Esperanto Book", Don Harlow says that Occidental had an intentional emphasis on European forms, and that some of its leading followers espoused a Eurocentric philosophy, which may have hindered its spread. Still, Occidental gained adherents in many nations including Asian nations. According to the Occidental magazine "Cosmoglotta" in 1928, a majority of Ido adherents took up Occidental in place of Ido.

Occidental survived World War II, undergoing a name change to "Interlingue", but faded into insignificance following the appearance in the early 1950s of a competing naturalistic project, Interlingua, which attracted among others the notable Occidentalist Ric Berger.

Occidental is written with 26 Latin letters: a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z. The letters of the alphabet are pronounced as "a, be, ce, de, e, ef, ge, ha, i, jot, ka, el, em, en, o, pe, qu, er, es, te, u, ve, duplic ve, ix, ypsilon", and "zet".

The vowels "a, e, i, o", and "u" have a continental pronunciation and are all sounded. The "y" (initial and medial) are pronounced as in "yes", "ey" (final) as in "they", and "eu" as éh-oo.

The consonants are pronounced as in English, with the following exceptions:

Like English, Interlingue has a definite article and an indefinite article. The definite article (the) is "li", and the indefinite (a, an) is "un". Plural of a noun is made by adding "-s" after a vowel, or "-es" after a consonant.

Interlingue has two forms for the personal pronouns: one for the subject form (nominative), and one for the object form (accusative or dative). In short, the personal pronouns in the subject form are:

The variants "illa" and "ella" both exist for third person singular feminine. The pronoun expressing politeness is "vu", which behaves like second person plural. The indefinite personal pronoun "one" is "on" in Occidental. If necessary, one can specify the gender of third person plural by using "illos" (masculine) or "illas" (feminine).

In the object form the pronouns are: "me, te, le, la, it, nos, vos", and "les" (with "los" and "las" as specific masculine and feminine forms, respectively). The possessive pronouns are "mi, tui, su" (his/her/its), "nor, vor" and "lor".

Translation: "Material civilization, science, and even art unify themselves more and more. The educated European feels himself almost at home in all lands that have European civilization, that is, more and more, in the entire world. Today almost all states war with the same armaments. Without pause the modes of intercommunication improve, and in consequence from that the world seems to decrease. A Parisian is now closer to an Englishman or a German than he was a hundred years before to a French peasant."






</doc>
<doc id="22763" url="https://en.wikipedia.org/wiki?curid=22763" title="Osiris">
Osiris

Osiris (, from Egyptian "wsjr", Coptic ) is an Egyptian god, identified as the god of the afterlife, the underworld, and rebirth. He was classically depicted as a green-skinned deity with a pharaoh's beard, partially mummy-wrapped at the legs, wearing a distinctive crown with two large ostrich feathers at either side, and holding a symbolic crook and flail.
Osiris was at times considered the eldest son of the god Geb, though other sources state his father is the sun-god Ra, and the sky goddess Nut, as well as being brother and husband of Isis, with Horus being considered his posthumously begotten son. He was also associated with the epithet Khenti-Amentiu, meaning "Foremost of the Westerners", a reference to his kingship in the land of the dead. As ruler of the dead, Osiris was also sometimes called "king of the living": ancient Egyptians considered the blessed dead "the living ones". Through syncretism with Iah, he is also the god of the Moon.

Osiris was considered the brother of Isis, Set, Nephthys, and Horus the Elder, and father of Horus the Younger. The first evidence of the worship of Osiris was found in the middle of the Fifth dynasty of Egypt, although it is likely that he was worshiped much earlier; the Khenti-Amentiu epithet dates to at least the first dynasty, and was also used as a pharaonic title. Most information available on the myths of Osiris is derived from allusions contained in the Pyramid Texts at the end of the Fifth Dynasty, later New Kingdom source documents such as the Shabaka Stone and the "Contending of Horus and Seth", and much later, in narrative style from the writings of Greek authors including Plutarch and Diodorus Siculus.

Osiris was the judge of the dead and the underworld agency that granted all life, including sprouting vegetation and the fertile flooding of the Nile River. He was described as the "Lord of love", "He Who is Permanently Benign and Youthful" and the "Lord of Silence". The Kings of Egypt were associated with Osiris in death – as Osiris rose from the dead so would they in union with him, and inherit eternal life through a process of imitative magic. By the New Kingdom all people, not just pharaohs, were believed to be associated with Osiris at death, if they incurred the costs of the assimilation rituals.

Through the hope of new life after death, Osiris began to be associated with the cycles observed in nature, in particular vegetation and the annual flooding of the Nile, through his links with the heliacal rising of Orion and Sirius at the start of the new year. Osiris was widely worshipped as Lord of the Dead until the suppression of the Egyptian religion during the rise of Christianity in the Roman Empire.

"Osiris" is a Latin transliteration of the Ancient Greek , which in turn is the Greek adaptation of the original theonym in the Egyptian language. In Egyptian hieroglyphs the name appears as "wsjr", which some Egyptologists instead choose to transliterate "ꜣsjr" or "jsjrj". Since hieroglyphic writing lacks vowels, Egyptologists have vocalized the name in various ways as Asar, Yasar, Aser, Asaru, Ausar, Ausir, Wesir, Usir, Usire or Ausare.

Several proposals have been made for the etymology and meaning of the original name; as Egyptologist Mark J. Smith notes, none are fully convincing. Most take "wsjr" as the accepted transliteration, following Adolf Erman:


However, recently alternative transliterations have been proposed:


Osiris is represented in his most developed form of iconography wearing the "Atef" crown, which is similar to the White crown of Upper Egypt, but with the addition of two curling ostrich feathers at each side (see also Atef crown (hieroglyph)). He also carries the crook and flail. The crook is thought to represent Osiris as a shepherd god. The symbolism of the flail is more uncertain with shepherds whip, fly-whisk, or association with the god Andjety of the ninth nome of Lower Egypt proposed.

He was commonly depicted as a pharaoh with a complexion of either green (the color of rebirth) or black (alluding to the fertility of the Nile floodplain) in mummiform (wearing the trappings of mummification from chest downward).

The Pyramid Texts describe early conceptions of an afterlife in terms of eternal travelling with the sun god amongst the stars. Amongst these mortuary texts, at the beginning of the 4th dynasty, is found: ""An offering the king gives and Anubis"". By the end of the 5th dynasty, the formula in all tombs becomes ""An offering the king gives and Osiris"".

Osiris is the mythological father of the god Horus, whose conception is described in the Osiris myth (a central myth in ancient Egyptian belief). The myth describes Osiris as having been killed by his brother, Set, who wanted Osiris' throne. His wife, Isis finds the body of Osiris and hides it in the reeds where it is found and dismembered by Set. Isis retrieves and joins the fragmented pieces of Osiris, then briefly brings Osiris back to life by use of magic. This spell gives her time to become pregnant by Osiris before he again dies. Isis later gives birth to Horus. As such, since Horus was born after Osiris' resurrection, Horus became thought of as a representation of new beginnings and the vanquisher of the usurper Set.

"Ptah-Seker" (who resulted from the identification of Creator god Ptah with Seker) thus gradually became identified with Osiris, the two becoming Ptah-Seker-Osiris. As the sun was thought to spend the night in the underworld, and was subsequently "reborn" every morning, Ptah-Seker-Osiris was identified as king of the underworld, god of the afterlife, life, death, and regeneration.

Osiris' soul, or rather his "Ba", was occasionally worshipped in its own right, almost as if it were a distinct god, especially in the Delta city of Mendes. This aspect of Osiris was referred to as "Banebdjedet", which is grammatically feminine (also spelt ""Banebded"" or ""Banebdjed""), literally "the "ba" of the lord of the "djed", which roughly means "The soul of the lord of the pillar of continuity". The "djed", a type of pillar, was usually understood as the backbone of Osiris.

The Nile supplying water, and Osiris (strongly connected to the vegetable regeneration) who died only to be resurrected, represented continuity and stability. As "Banebdjed", Osiris was given epithets such as "Lord of the Sky" and "Life of the (sun god) Ra", since Ra, when he had become identified with Atum, was considered Osiris' ancestor, from whom his regal authority is inherited. "Ba" does not mean "soul" in the western sense, and has to do with power, reputation, force of character, especially in the case of a god.

Since the "ba" was associated with power, and also happened to be a word for ram in Egyptian, Banebdjed was depicted as a ram, or as Ram-headed. A living, sacred ram was kept at Mendes and worshipped as the incarnation of the god, and upon death, the rams were mummified and buried in a ram-specific necropolis. Banebdjed was consequently said to be Horus' father, as Banebdjed was an aspect of Osiris.

Regarding the association of Osiris with the ram, the god's traditional crook and flail are the instruments of the shepherd, which has suggested to some scholars also an origin for Osiris in herding tribes of the upper Nile. The crook and flail were originally symbols of the minor agricultural deity Andjety, and passed to Osiris later. From Osiris, they eventually passed to Egyptian kings in general as symbols of divine authority.

The cult of Osiris (who was a god chiefly of regeneration and rebirth) had a particularly strong interest in the concept of immortality. Plutarch recounts one version of the myth in which Set (Osiris' brother), along with the Queen of Ethiopia, conspired with 72 accomplices to plot the assassination of Osiris.
Set fooled Osiris into getting into a box, which Set then shut, sealed with lead, and threw into the Nile. Osiris' wife, Isis, searched for his remains until she finally found him embedded in a tamarisk tree trunk, which was holding up the roof of a palace in Byblos on the Phoenician coast. She managed to remove the coffin and retrieve her husband's body.

In one version of the myth, Isis used a spell to briefly revive Osiris so he could impregnate her. After embalming and burying Osiris, Isis conceived and gave birth to their son, Horus. Thereafter Osiris lived on as the god of the underworld. Because of his death and resurrection, Osiris was associated with the flooding and retreating of the Nile and thus with the yearly growth and death of crops along the Nile valley.

Diodorus Siculus gives another version of the myth in which Osiris was described as an ancient king who taught the Egyptians the arts of civilization, including agriculture, then travelled the world with his sister Isis, the satyrs, and the nine muses, before finally returning to Egypt. Osiris was then murdered by his evil brother Typhon, who was identified with Set. Typhon divided the body into twenty-six pieces, which he distributed amongst his fellow conspirators in order to implicate them in the murder. Isis and Hercules (Horus) avenged the death of Osiris and slew Typhon. Isis recovered all the parts of Osiris' body, except the phallus, and secretly buried them. She made replicas of them and distributed them to several locations, which then became centres of Osiris worship.

Annual ceremonies were performed in honor of Osiris in various places across Egypt. These ceremonies were fertility rites which symbolised the resurrection of Osiris. E.A. Wallis Budge stated "Osiris is closely connected with the germination of wheat; the grain which is put into the ground is the dead Osiris, and the grain which has germinated is the Osiris who has once again renewed his life."

Plutarch and others have noted that the sacrifices to Osiris were "gloomy, solemn, and mournful..." (Isis and Osiris, 69) and that the great mystery festival, celebrated in two phases, began at Abydos commemorating the death of the god, on the same day that grain was planted in the ground (Isis and Osiris, 13). The annual festival involved the construction of "Osiris Beds" formed in shape of Osiris, filled with soil and sown with seed.

The germinating seed symbolized Osiris rising from the dead. An almost pristine example was found in the tomb of Tutankhamun by Howard Carter.

The first phase of the festival was a public drama depicting the murder and dismemberment of Osiris, the search of his body by Isis, his triumphal return as the resurrected god, and the battle in which Horus defeated Set.

According to Julius Firmicus Maternus of the fourth century, this play was re-enacted each year by worshippers who "beat their breasts and gashed their shoulders... When they pretend that the mutilated remains of the god have been found and rejoined...they turn from mourning to rejoicing." ("De Errore Profanorum").

The passion of Osiris was reflected in his name 'Wenennefer" ("the one who continues to be perfect"), which also alludes to his post mortem power.

Much of the extant information about the rites of Osiris can be found on the Ikhernofret Stela at Abydos erected in the 12th Dynasty by Ikhernofret (also I-Kher-Nefert), possibly a priest of Osiris or other official (the titles of Ikhernofret are described in his stela from Abydos) during the reign of Senwosret III (Pharaoh Sesostris, about 1875 BC). The ritual reenactment of Osiris's funeral rites were held in the last month of the inundation (the annual Nile flood), coinciding with Spring, and held at Abydos/Abedjou which was the traditional place where the body of Osiris/Wesir drifted ashore after having been drowned in the Nile.

The part of the myth recounting the chopping up of the body into 14 pieces by Set is not recounted in this particular stela. Although it is attested to be a part of the rituals by a version of the Papyrus Jumilhac, in which it took Isis 12 days to reassemble the pieces, coinciding with the festival of ploughing. Some elements of the ceremony were held in the temple, while others involved public participation in a form of theatre. The Stela of I-Kher-Nefert recounts the programme of events of the public elements over the five days of the Festival:

Contrasting with the public "theatrical" ceremonies sourced from the I-Kher-Nefert stele (from the Middle Kingdom), more esoteric ceremonies were performed inside the temples by priests witnessed only by chosen initiates. Plutarch mentions that (for much later period) two days after the beginning of the festival "the priests bring forth a sacred chest containing a small golden coffer, into which they pour some potable water...and a great shout arises from the company for joy that Osiris is found (or resurrected). Then they knead some fertile soil with the water...and fashion therefrom a crescent-shaped figure, which they cloth and adorn, this indicating that they regard these gods as the substance of Earth and Water." ("Isis and Osiris," 39). Yet his accounts were still obscure, for he also wrote, "I pass over the cutting of the wood" – opting not to describe it, since he considered it as a most sacred ritual ("Ibid." 21).

In the Osirian temple at Denderah, an inscription (translated by Budge, Chapter XV, Osiris and the Egyptian Resurrection) describes in detail the making of wheat paste models of each dismembered piece of Osiris to be sent out to the town where each piece is discovered by Isis. At the temple of Mendes, figures of Osiris were made from wheat and paste placed in a trough on the day of the murder, then water was added for several days, until finally the mixture was kneaded into a mold of Osiris and taken to the temple to be buried (the sacred grain for these cakes were grown only in the temple fields). Molds were made from the wood of a red tree in the forms of the sixteen dismembered parts of Osiris, the cakes of 'divine' bread were made from each mold, placed in a silver chest and set near the head of the god with "the inward parts of Osiris" as described in the Book of the Dead (XVII).

The idea of divine justice being exercised after death for wrongdoing during life is first encountered during the Old Kingdom in a 6th dynasty tomb containing fragments of what would be described later as the Negative Confessions performed in front of the 42 Assessors of Ma'at.
With the rise of the cult of Osiris during the Middle Kingdom the ""democratization of religion"" offered to even his humblest followers the prospect of eternal life, with moral fitness becoming the dominant factor in determining a person's suitability.

At death a person faced judgment by a tribunal of forty-two divine judges. If they led a life in conformance with the precepts of the goddess Ma'at, who represented truth and right living, the person was welcomed into the kingdom of Osiris. If found guilty, the person was thrown to a ""devourer"" (such as the soul-eating demon Ammit) and did not share in eternal life.

The person who is taken by the devourer is subject first to terrifying punishment and then annihilated. These depictions of punishment may have influenced medieval perceptions of the inferno in hell via early Christian and Coptic texts.

Purification for those who are considered justified may be found in the descriptions of ""Flame Island"", where they experience the triumph over evil and rebirth. For the damned, complete destruction into a state of non-being awaits, but there is no suggestion of eternal torture.

Divine pardon at judgement was always a central concern for the ancient Egyptians.

During the reign of Seti I, Osiris was also invoked in royal decrees to pursue the living when wrongdoing was observed, but kept secret and not reported.

The early Ptolemaic kings promoted a new god, Serapis, who combined traits of Osiris with those of various Greek gods and was portrayed in a Hellenistic form. Serapis was often treated as the consort of Isis and became the patron deity of the Ptolemies' capital, Alexandria. Serapis's origins are not known. Some ancient authors claim the cult of Serapis was established at Alexandria by Alexander the Great himself, but most who discuss the subject of Serapis's origins give a story similar to that by Plutarch. Writing about 400 years after the fact, Plutarch claimed that Ptolemy I established the cult after dreaming of a colossal statue at Sinope in Anatolia. His councillors identified as a statue of the Greek god Pluto and said that the Egyptian name for Pluto was Serapis. This name may have been a Hellenization of "Osiris-Apis". Osiris-Apis was a patron deity of the Memphite Necropolis and the father of the Apis bull who was worshipped there, and texts from Ptolemaic times treat "Serapis" as the Greek translation of "Osiris-Apis". But little of the early evidence for Serapis's cult comes from Memphis, and much of it comes from the Mediterranean world with no reference to an Egyptian origin for Serapis, so Mark Smith expresses doubt that Serapis originated as a Greek form of Osiris-Apis's name and leaves open the possibility that Serapis originated outside Egypt.

The cult of Isis and Osiris continued at Philae until at least the 450s CE, long after the imperial decrees of the late 4th century that ordered the closing of temples to "pagan" gods. Philae was the last major ancient Egyptian temple to be closed.




</doc>
<doc id="22764" url="https://en.wikipedia.org/wiki?curid=22764" title="Orthodox Bahá'í Faith">
Orthodox Bahá'í Faith

The Orthodox Bahá'í Faith is a small Bahá'í sect that formed in 1960 by Mason Remey, and subsequently was the name used by Joel Marangella after he claimed to be Remey's successor. The basis of the dispute is over the identity of the Guardian, a term referring to the appointed head of the religion, an executive hereditary office held by Shoghi Effendi from 1921 to 1957.

Other than on the matter of leadership and organization, there are few differences between the orthodox and mainstream Bahá'ís in matters of doctrine. As a group who believe that Mason Remey was the second Guardian of the Bahá'í Faith, they are considered heretical Covenant-breakers by the majority of Bahá'ís who follow the leadership of the Universal House of Justice. 

Membership data of the Orthodox Bahá'ís is scarce. One source estimated them at no more than 100 members as of 1988. Memorandums from an Illinois court case in 2007 state their membership in the United States at 40. Websites claiming to represent the Orthodox community indicate followers in the United States and India. Joel Marangella died in San Diego, California on Sept 1, 2013.

Following the unexpected death of the Bahá'í Faith's first Guardian Shoghi Effendi in 1957, the 27 living Hands of the Cause, having the responsibility to acknowledge any appointment of a successor, gathered and decided that he had died "without having appointed his successor," and that the Universal House of Justice would decide on the situation after its first election. Charles Mason Remey, one of the Hands, declared himself the successor to Shoghi Effendi in 1960. His claim was rejected by the 26 remaining Hands, on the basis that he was not a descendant of Bahá'u'lláh, nor was he appointed to the position by Shoghi Effendi. Remey based his claim on his being the president of the International Bahá'í Council appointed by Shoghi Effendi in 1951. The result was that Remey was unanimously expelled from the Bahá'í community by the Hands of the Cause.

In 1962 Remey asked his supporters in the United States to organize themselves and elect a "National Spiritual Assembly Under the Hereditary Guardianship" (NSAUHG), first elected in 1963. The Assembly of 9 members was incorporated in New Mexico in 1964.

In 1964 the NSAUHG filed a lawsuit against the National Spiritual Assembly (NSA) of the Bahá'ís of the United States to receive the legal title to the Bahá'í House of Worship in Illinois, and all other property owned by the NSA. The NSA counter-sued, and in August 1966 Remey instructed the NSAUHG to withdraw from any action in the matter "regardless of the consequences." Later that year, Remey asked the NSAUHG to dissolve, as well as the International Bahá'í Council that he had appointed with Joel Marangella as president, residing in France. Marangella previously served on the National Spiritual Assembly of France in 1961, and was declared a Covenant-breaker when he accepted Mason Remey as the next Guardian.

Over the years following 1966 the followers of Mason Remey were not organized; with some of his followers concluding that Remey was suffering from dementia, until several of the individuals involved began forming their own groups based on different understandings of succession.

In 1962 Remey gave Marangella a sealed envelope, with instructions to open it when the time was right. In 1965 Mason Remey called for the International Bahá'í Council, of which Marangella was president, to become active. Marangella then opened the sealed letter, which was a hand-written note by Remey appointing Marangella as his successor. Marangella looks upon that time as the time of his official appointment. Remey then changed his mind, deactivated the International Bahá'í Council in 1966, and in 1969 Marangella announced that he was the third Guardian. All of the members of the 1966 NSAUHG accepted Marangella's claim. 

In 1970 Marangella appointed members to a "National Bureau of the Orthodox Bahá'ís in New York", which two years later was moved to New Mexico, and subsequently changed its name to "Mother Bahá'í Council of the United States" (1978) and "Provisional National Bahá'í Council" (2000), with all members appointed by Joel Marangella.. Marangella died in San Diego, CA on Sept 1, 2013.

The third Guardian of the Baha'i Faith Joel Bray Marangella in his Proclamation of 22 September 2006 appointed a Persian Nosrat'u'llah Bahremand the eldest son of Hassan Bahremand, a prominent member of the Orthodox Baha'i Faith as the Fourth Guardian of Orthodox Baha'i Faith. 



</doc>
<doc id="22770" url="https://en.wikipedia.org/wiki?curid=22770" title="1">
1

1 (one, also called unit, unity, and (multiplicative) identity) is a number, numeral, and glyph. It represents a single entity, the unit of counting or measurement. For example, a line segment of "unit length" is a line segment of length 1. It is also the first of the infinite sequence of natural numbers, followed by 2.

The word "one" can be used as a noun, an adjective and a pronoun.

It comes from the English word "an", which comes from the Proto-Germanic root "*ainaz". The Proto-Germanic root "*ainaz" comes from the Proto-Indo-European root "*oi-no-".

Compare the Proto-Germanic root "*ainaz" to Old Frisian "an", Gothic "ains", Danish "en", Dutch "een", German "eins" and Old Norse "einn".

Compare the Proto-Indo-European root "*oi-no-" (which means "one, single") to Greek "oinos" (which means "ace" on dice), Latin "unus" (one), Old Persian "aivam", Old Church Slavonic "-inu" and "ino-", Lithuanian "vienas", Old Irish "oin" and Breton "un" (one).

One, sometimes referred to as unity, is the first non-zero natural number. It is thus the integer before two and after zero, and the first positive odd number.

Any number multiplied by one remains that number, as one is the identity for multiplication. As a result, 1 is its own factorial, its own square, its own cube, and so on. One is also the result of the empty product, as any number multiplied by one is itself. It is also the only natural number that is neither composite nor prime with respect to division, but instead considered a unit (meaning of ring theory).

The glyph used today in the Western world to represent the number 1, a vertical line, often with a serif at the top and sometimes a short horizontal line at the bottom, traces its roots back to the Indians, who wrote 1 as a horizontal line, much like the Chinese character . The Gupta wrote it as a curved line, and the Nagari sometimes added a small circle on the left (rotated a quarter turn to the right, this 9-look-alike became the present day numeral 1 in the Gujarati and Punjabi scripts). The Nepali also rotated it to the right but kept the circle small. This eventually became the top serif in the modern numeral, but the occasional short horizontal line at the bottom probably originates from similarity with the Roman numeral . In some countries, the little serif at the top is sometimes extended into a long upstroke, sometimes as long as the vertical line, which can lead to confusion with the glyph for seven in other countries. Where the 1 is written with a long upstroke, the number 7 has a horizontal stroke through the vertical line.

While the shape of the 1 character has an ascender in most modern typefaces, in typefaces with text figures, the character usually is of x-height, as, for example, in .
Many older typewriters do not have a separate symbol for "1" and use the lowercase letter "l" instead. It is possible to find cases when the uppercase "J" is used, while it may be for decorative purposes.

Mathematically, 1 is:

Tallying is often referred to as "base 1", since only one mark – the tally itself – is needed. This is more formally referred to as a unary numeral system. Unlike base 2 or base 10, this is not a positional notation.

Since the base 1 exponential function (1) always equals 1, its inverse does not exist (which would be called the logarithm base 1 if it did exist).

There are two ways to write the real number 1 as a recurring decimal: as 1.000..., and as 0.999...

Formalizations of the natural numbers have their own representations of 1:

In a multiplicative group or monoid, the identity element is sometimes denoted 1, but "e" (from the German "Einheit", "unity") is also traditional. However, 1 is especially common for the multiplicative identity of a ring, i.e., when an addition and 0 are also present. When such a ring has characteristic "n" not equal to 0, the element called 1 has the property that (where this 0 is the additive identity of the ring). Important examples are finite fields.

1 is the first figurate number of every kind, such as triangular number, pentagonal number and centered hexagonal number, to name just a few.

In many mathematical and engineering problems, numeric values are typically "normalized" to fall within the unit interval from 0 to 1, where 1 usually represents the maximum possible value in the range of parameters. Likewise, vectors are often normalized to give unit vectors, that is vectors of magnitude one, because these often have more desirable properties. Functions, too, are often normalized by the condition that they have integral one, maximum value one, or square integral one, depending on the application.

Because of the multiplicative identity, if "f"("x") is a multiplicative function, then "f"(1) must equal 1.

It is also the first and second number in the Fibonacci sequence (0 is the zeroth) and is the first number in many other mathematical sequences.

1 is neither a prime number nor a composite number, but a unit (meaning of ring theory), like −1 and, in the Gaussian integers, "i" and −"i". The fundamental theorem of arithmetic guarantees unique factorization over the integers only up to units. (For example, , but if units are included, is also equal to, say, among infinitely many similar "factorizations".)

The definition of a field requires that 1 must not be equal to 0. Thus, there are no fields of characteristic 1. Nevertheless, abstract algebra can consider the field with one element, which is not a singleton and is not a set at all.

1 is the only positive integer divisible by exactly one positive integer (whereas prime numbers are divisible by exactly two positive integers, composite numbers are divisible by more than two positive integers, and zero is divisible by all positive integers). 1 was formerly considered prime by some mathematicians, using the definition that a prime is divisible only by 1 and itself. However, this complicates the fundamental theorem of arithmetic, so modern definitions exclude units.

By definition, 1 is the magnitude, absolute value, or norm of a unit complex number, unit vector, and a unit matrix (more usually called an identity matrix). Note that the term "unit matrix" is sometimes used to mean something quite different.

By definition, 1 is the probability of an event that is almost certain to occur.

1 is the most common leading digit in many sets of data, a consequence of Benford's law.

1 is the only known Tamagawa number for a simply connected algebraic group over a number field.

The generating function that has all coefficients 1 is given by

formula_1

This power series converges and has finite value if and only if, formula_2.

In category theory, 1 is sometimes used to denote the terminal object of a category.

In number theory, 1 is the value of Legendre's constant, which was introduced in 1808 by Adrien-Marie Legendre in expressing the asymptotic behavior of the prime-counting function. Legendre's constant was originally conjectured to be approximately 1.08366, but was proven to equal exactly 1 in 1899.



In the philosophy of Plotinus and a number of other neoplatonists, The One is the ultimate reality and source of all existence. Philo of Alexandria (20 BC – AD 50) regarded the number one as God's number, and the basis for all numbers ("De Allegoriis Legum," ii.12 [i.66]).








</doc>
<doc id="22773" url="https://en.wikipedia.org/wiki?curid=22773" title="Oxidative phosphorylation">
Oxidative phosphorylation

Oxidative phosphorylation (or OXPHOS in short) (UK , US ) is the metabolic pathway in which cells use enzymes to oxidize nutrients, thereby releasing energy which is used to produce adenosine triphosphate (ATP). In most eukaryotes, this takes place inside mitochondria. Almost all aerobic organisms carry out oxidative phosphorylation. This pathway is probably so pervasive because it is a highly efficient way of releasing energy, compared to alternative fermentation processes such as anaerobic glycolysis.

During oxidative phosphorylation, electrons are transferred from electron donors to electron acceptors such as oxygen, in redox reactions. These redox reactions release energy, which is used to form ATP. In eukaryotes, these redox reactions are carried out by a series of protein complexes within the inner membrane of the cell's mitochondria, whereas, in prokaryotes, these proteins are located in the cells' intermembrane space. These linked sets of proteins are called electron transport chains. In eukaryotes, five main protein complexes are involved, whereas in prokaryotes many different enzymes are present, using a variety of electron donors and acceptors.

The energy released by electrons flowing through this electron transport chain is used to transport protons across the inner mitochondrial membrane, in a process called "electron transport". This generates potential energy in the form of a pH gradient and an electrical potential across this membrane. This store of energy is tapped when protons flow back across the membrane and down the potential energy gradient, through a large enzyme called ATP synthase; this process is known as chemiosmosis. The ATP synthase uses the energy to transform adenosine diphosphate (ADP) into adenosine triphosphate, in a phosphorylation reaction. The reaction is driven by the proton flow, which forces the rotation of a part of the enzyme; the ATP synthase is a rotary mechanical motor.

Although oxidative phosphorylation is a vital part of metabolism, it produces reactive oxygen species such as superoxide and hydrogen peroxide, which lead to propagation of free radicals, damaging cells and contributing to disease and, possibly, aging (senescence). The enzymes carrying out this metabolic pathway are also the target of many drugs and poisons that inhibit their activities.

It is the terminal process of cellular respiration in eukaryotes and accounts for high ATP yield.

Oxidative phosphorylation works by using energy-releasing chemical reactions to drive energy-requiring reactions: The two sets of reactions are said to be "coupled". This means one cannot occur without the other. The flow of electrons through the electron transport chain, from electron donors such as NADH to electron acceptors such as oxygen, is an exergonic process – it releases energy, whereas the synthesis of ATP is an endergonic process, which requires an input of energy. Both the electron transport chain and the ATP synthase are embedded in a membrane, and energy is transferred from electron transport chain to the ATP synthase by movements of protons across this membrane, in a process called "chemiosmosis". In practice, this is like a simple electric circuit, with a current of protons being driven from the negative N-side of the membrane to the positive P-side by the proton-pumping enzymes of the electron transport chain. These enzymes are like a battery, as they perform work to drive current through the circuit. The movement of protons creates an electrochemical gradient across the membrane, which is often called the "proton-motive force". It has two components: a difference in proton concentration (a H gradient, ΔpH) and a difference in electric potential, with the N-side having a negative charge.

ATP synthase releases this stored energy by completing the circuit and allowing protons to flow down the electrochemical gradient, back to the N-side of the membrane. The electrochemical gradient drives the rotation of part of the enzyme's structure and couples this motion to the synthesis of ATP.

The two components of the proton-motive force are thermodynamically equivalent: In mitochondria, the largest part of energy is provided by the potential; in alkaliphile bacteria the electrical energy even has to compensate for a counteracting inverse pH difference. Inversely, chloroplasts operate mainly on ΔpH. However, they also require a small membrane potential for the kinetics of ATP synthesis. In the case of the fusobacterium "Propionigenium modestum" it drives the counter-rotation of subunits a and c of the F motor of ATP synthase.

The amount of energy released by oxidative phosphorylation is high, compared with the amount produced by anaerobic fermentation. Glycolysis produces only 2 ATP molecules, but somewhere between 30 and 36 ATPs are produced by the oxidative phosphorylation of the 10 NADH and 2 succinate molecules made by converting one molecule of glucose to carbon dioxide and water, while each cycle of beta oxidation of a fatty acid yields about 14 ATPs. These ATP yields are theoretical maximum values; in practice, some protons leak across the membrane, lowering the yield of ATP.

The electron transport chain carries both protons and electrons, passing electrons from donors to acceptors, and transporting protons across a membrane. These processes use both soluble and protein-bound transfer molecules. In mitochondria, electrons are transferred within the intermembrane space by the water-soluble electron transfer protein cytochrome c. This carries only electrons, and these are transferred by the reduction and oxidation of an iron atom that the protein holds within a heme group in its structure. Cytochrome c is also found in some bacteria, where it is located within the periplasmic space.

Within the inner mitochondrial membrane, the lipid-soluble electron carrier coenzyme Q10 (Q) carries both electrons and protons by a redox cycle. This small benzoquinone molecule is very hydrophobic, so it diffuses freely within the membrane. When Q accepts two electrons and two protons, it becomes reduced to the "ubiquinol" form (QH); when QH releases two electrons and two protons, it becomes oxidized back to the "ubiquinone" (Q) form. As a result, if two enzymes are arranged so that Q is reduced on one side of the membrane and QH oxidized on the other, ubiquinone will couple these reactions and shuttle protons across the membrane. Some bacterial electron transport chains use different quinones, such as menaquinone, in addition to ubiquinone.

Within proteins, electrons are transferred between flavin cofactors, iron–sulfur clusters, and cytochromes. There are several types of iron–sulfur cluster. The simplest kind found in the electron transfer chain consists of two iron atoms joined by two atoms of inorganic sulfur; these are called [2Fe–2S] clusters. The second kind, called [4Fe–4S], contains a cube of four iron atoms and four sulfur atoms. Each iron atom in these clusters is coordinated by an additional amino acid, usually by the sulfur atom of cysteine. Metal ion cofactors undergo redox reactions without binding or releasing protons, so in the electron transport chain they serve solely to transport electrons through proteins. Electrons move quite long distances through proteins by hopping along chains of these cofactors. This occurs by quantum tunnelling, which is rapid over distances of less than 1.4 m.

Many catabolic biochemical processes, such as glycolysis, the citric acid cycle, and beta oxidation, produce the reduced coenzyme NADH. This coenzyme contains electrons that have a high transfer potential; in other words, they will release a large amount of energy upon oxidation. However, the cell does not release this energy all at once, as this would be an uncontrollable reaction. Instead, the electrons are removed from NADH and passed to oxygen through a series of enzymes that each release a small amount of the energy. This set of enzymes, consisting of complexes I through IV, is called the electron transport chain and is found in the inner membrane of the mitochondrion. Succinate is also oxidized by the electron transport chain, but feeds into the pathway at a different point.

In eukaryotes, the enzymes in this electron transport system use the energy released from the oxidation of NADH to pump protons across the inner membrane of the mitochondrion. This causes protons to build up in the intermembrane space, and generates an electrochemical gradient across the membrane. The energy stored in this potential is then used by ATP synthase to produce ATP. Oxidative phosphorylation in the eukaryotic mitochondrion is the best-understood example of this process. The mitochondrion is present in almost all eukaryotes, with the exception of anaerobic protozoa such as "Trichomonas vaginalis" that instead reduce protons to hydrogen in a remnant mitochondrion called a hydrogenosome.

NADH-coenzyme Q oxidoreductase, also known as "NADH dehydrogenase" or "complex I", is the first protein in the electron transport chain. Complex I is a giant enzyme with the mammalian complex I having 46 subunits and a molecular mass of about 1,000 kilodaltons (kDa). The structure is known in detail only from a bacterium; in most organisms the complex resembles a boot with a large "ball" poking out from the membrane into the mitochondrion. The genes that encode the individual proteins are contained in both the cell nucleus and the mitochondrial genome, as is the case for many enzymes present in the mitochondrion.

The reaction that is catalyzed by this enzyme is the two electron oxidation of NADH by coenzyme Q10 or "ubiquinone" (represented as Q in the equation below), a lipid-soluble quinone that is found in the mitochondrion membrane:

The start of the reaction, and indeed of the entire electron chain, is the binding of a NADH molecule to complex I and the donation of two electrons. The electrons enter complex I via a prosthetic group attached to the complex, flavin mononucleotide (FMN). The addition of electrons to FMN converts it to its reduced form, FMNH. The electrons are then transferred through a series of iron–sulfur clusters: the second kind of prosthetic group present in the complex. There are both [2Fe–2S] and [4Fe–4S] iron–sulfur clusters in complex I.

As the electrons pass through this complex, four protons are pumped from the matrix into the intermembrane space. Exactly how this occurs is unclear, but it seems to involve conformational changes in complex I that cause the protein to bind protons on the N-side of the membrane and release them on the P-side of the membrane. Finally, the electrons are transferred from the chain of iron–sulfur clusters to a ubiquinone molecule in the membrane. Reduction of ubiquinone also contributes to the generation of a proton gradient, as two protons are taken up from the matrix as it is reduced to ubiquinol (QH).

Succinate-Q oxidoreductase, also known as "complex II" or "succinate dehydrogenase", is a second entry point to the electron transport chain. It is unusual because it is the only enzyme that is part of both the citric acid cycle and the electron transport chain. Complex II consists of four protein subunits and contains a bound flavin adenine dinucleotide (FAD) cofactor, iron–sulfur clusters, and a heme group that does not participate in electron transfer to coenzyme Q, but is believed to be important in decreasing production of reactive oxygen species. It oxidizes succinate to fumarate and reduces ubiquinone. As this reaction releases less energy than the oxidation of NADH, complex II does not transport protons across the membrane and does not contribute to the proton gradient.

In some eukaryotes, such as the parasitic worm "Ascaris suum", an enzyme similar to complex II, fumarate reductase (menaquinol:fumarate
oxidoreductase, or QFR), operates in reverse to oxidize ubiquinol and reduce fumarate. This allows the worm to survive in the anaerobic environment of the large intestine, carrying out anaerobic oxidative phosphorylation with fumarate as the electron acceptor. Another unconventional function of complex II is seen in the malaria parasite "Plasmodium falciparum". Here, the reversed action of complex II as an oxidase is important in regenerating ubiquinol, which the parasite uses in an unusual form of pyrimidine biosynthesis.

Electron transfer flavoprotein-ubiquinone oxidoreductase (ETF-Q oxidoreductase), also known as "electron transferring-flavoprotein dehydrogenase", is a third entry point to the electron transport chain. It is an enzyme that accepts electrons from electron-transferring flavoprotein in the mitochondrial matrix, and uses these electrons to reduce ubiquinone. This enzyme contains a flavin and a [4Fe–4S] cluster, but, unlike the other respiratory complexes, it attaches to the surface of the membrane and does not cross the lipid bilayer.

In mammals, this metabolic pathway is important in beta oxidation of fatty acids and catabolism of amino acids and choline, as it accepts electrons from multiple acetyl-CoA dehydrogenases. In plants, ETF-Q oxidoreductase is also important in the metabolic responses that allow survival in extended periods of darkness.

Q-cytochrome c oxidoreductase is also known as "cytochrome c reductase", "cytochrome bc complex", or simply "complex III". In mammals, this enzyme is a dimer, with each subunit complex containing 11 protein subunits, an [2Fe-2S] iron–sulfur cluster and three cytochromes: one cytochrome c and two b cytochromes. A cytochrome is a kind of electron-transferring protein that contains at least one heme group. The iron atoms inside complex III’s heme groups alternate between a reduced ferrous (+2) and oxidized ferric (+3) state as the electrons are transferred through the protein.

The reaction catalyzed by complex III is the oxidation of one molecule of ubiquinol and the reduction of two molecules of cytochrome c, a heme protein loosely associated with the mitochondrion. Unlike coenzyme Q, which carries two electrons, cytochrome c carries only one electron.

As only one of the electrons can be transferred from the QH donor to a cytochrome c acceptor at a time, the reaction mechanism of complex III is more elaborate than those of the other respiratory complexes, and occurs in two steps called the Q cycle. In the first step, the enzyme binds three substrates, first, QH, which is then oxidized, with one electron being passed to the second substrate, cytochrome c. The two protons released from QH pass into the intermembrane space. The third substrate is Q, which accepts the second electron from the QH and is reduced to Q, which is the ubisemiquinone free radical. The first two substrates are released, but this ubisemiquinone intermediate remains bound. In the second step, a second molecule of QH is bound and again passes its first electron to a cytochrome c acceptor. The second electron is passed to the bound ubisemiquinone, reducing it to QH as it gains two protons from the mitochondrial matrix. This QH is then released from the enzyme.

As coenzyme Q is reduced to ubiquinol on the inner side of the membrane and oxidized to ubiquinone on the other, a net transfer of protons across the membrane occurs, adding to the proton gradient. The rather complex two-step mechanism by which this occurs is important, as it increases the efficiency of proton transfer. If, instead of the Q cycle, one molecule of QH were used to directly reduce two molecules of cytochrome c, the efficiency would be halved, with only one proton transferred per cytochrome c reduced.

Cytochrome c oxidase, also known as "complex IV", is the final protein complex in the electron transport chain. The mammalian enzyme has an extremely complicated structure and contains 13 subunits, two heme groups, as well as multiple metal ion cofactors – in all, three atoms of copper, one of magnesium and one of zinc.

This enzyme mediates the final reaction in the electron transport chain and transfers electrons to oxygen, while pumping protons across the membrane. The final electron acceptor oxygen, which is also called the "terminal electron acceptor", is reduced to water in this step. Both the direct pumping of protons and the consumption of matrix protons in the reduction of oxygen contribute to the proton gradient. The reaction catalyzed is the oxidation of cytochrome c and the reduction of oxygen:

Many eukaryotic organisms have electron transport chains that differ from the much-studied mammalian enzymes described above. For example, plants have alternative NADH oxidases, which oxidize NADH in the cytosol rather than in the mitochondrial matrix, and pass these electrons to the ubiquinone pool. These enzymes do not transport protons, and, therefore, reduce ubiquinone without altering the electrochemical gradient across the inner membrane.

Another example of a divergent electron transport chain is the "alternative oxidase", which is found in plants, as well as some fungi, protists, and possibly some animals. This enzyme transfers electrons directly from ubiquinol to oxygen.

The electron transport pathways produced by these alternative NADH and ubiquinone oxidases have lower ATP yields than the full pathway. The advantages produced by a shortened pathway are not entirely clear. However, the alternative oxidase is produced in response to stresses such as cold, reactive oxygen species, and infection by pathogens, as well as other factors that inhibit the full electron transport chain. Alternative pathways might, therefore, enhance an organisms' resistance to injury, by reducing oxidative stress.

The original model for how the respiratory chain complexes are organized was that they diffuse freely and independently in the mitochondrial membrane. However, recent data suggest that the complexes might form higher-order structures called supercomplexes or "respirasomes". In this model, the various complexes exist as organized sets of interacting enzymes. These associations might allow channeling of substrates between the various enzyme complexes, increasing the rate and efficiency of electron transfer. Within such mammalian supercomplexes, some components would be present in higher amounts than others, with some data suggesting a ratio between complexes I/II/III/IV and the ATP synthase of approximately 1:1:3:7:4. However, the debate over this supercomplex hypothesis is not completely resolved, as some data do not appear to fit with this model.

In contrast to the general similarity in structure and function of the electron transport chains in eukaryotes, bacteria and archaea possess a large variety of electron-transfer enzymes. These use an equally wide set of chemicals as substrates. In common with eukaryotes, prokaryotic electron transport uses the energy released from the oxidation of a substrate to pump ions across a membrane and generate an electrochemical gradient. In the bacteria, oxidative phosphorylation in "Escherichia coli" is understood in most detail, while archaeal systems are at present poorly understood.

The main difference between eukaryotic and prokaryotic oxidative phosphorylation is that bacteria and archaea use many different substances to donate or accept electrons. This allows prokaryotes to grow under a wide variety of environmental conditions. In "E. coli", for example, oxidative phosphorylation can be driven by a large number of pairs of reducing agents and oxidizing agents, which are listed below. The midpoint potential of a chemical measures how much energy is released when it is oxidized or reduced, with reducing agents having negative potentials and oxidizing agents positive potentials.

As shown above, "E. coli" can grow with reducing agents such as formate, hydrogen, or lactate as electron donors, and nitrate, DMSO, or oxygen as acceptors. The larger the difference in midpoint potential between an oxidizing and reducing agent, the more energy is released when they react. Out of these compounds, the succinate/fumarate pair is unusual, as its midpoint potential is close to zero. Succinate can therefore be oxidized to fumarate if a strong oxidizing agent such as oxygen is available, or fumarate can be reduced to succinate using a strong reducing agent such as formate. These alternative reactions are catalyzed by succinate dehydrogenase and fumarate reductase, respectively.

Some prokaryotes use redox pairs that have only a small difference in midpoint potential. For example, nitrifying bacteria such as "Nitrobacter" oxidize nitrite to nitrate, donating the electrons to oxygen. The small amount of energy released in this reaction is enough to pump protons and generate ATP, but not enough to produce NADH or NADPH directly for use in anabolism. This problem is solved by using a nitrite oxidoreductase to produce enough proton-motive force to run part of the electron transport chain in reverse, causing complex I to generate NADH.

Prokaryotes control their use of these electron donors and acceptors by varying which enzymes are produced, in response to environmental conditions. This flexibility is possible because different oxidases and reductases use the same ubiquinone pool. This allows many combinations of enzymes to function together, linked by the common ubiquinol intermediate. These respiratory chains therefore have a modular design, with easily interchangeable sets of enzyme systems.

In addition to this metabolic diversity, prokaryotes also possess a range of isozymes – different enzymes that catalyze the same reaction. For example, in "E. coli", there are two different types of ubiquinol oxidase using oxygen as an electron acceptor. Under highly aerobic conditions, the cell uses an oxidase with a low affinity for oxygen that can transport two protons per electron. However, if levels of oxygen fall, they switch to an oxidase that transfers only one proton per electron, but has a high affinity for oxygen.

ATP synthase, also called "complex V", is the final enzyme in the oxidative phosphorylation pathway. This enzyme is found in all forms of life and functions in the same way in both prokaryotes and eukaryotes. The enzyme uses the energy stored in a proton gradient across a membrane to drive the synthesis of ATP from ADP and phosphate (P). Estimates of the number of protons required to synthesize one ATP have ranged from three to four, with some suggesting cells can vary this ratio, to suit different conditions.

This phosphorylation reaction is an equilibrium, which can be shifted by altering the proton-motive force. In the absence of a proton-motive force, the ATP synthase reaction will run from right to left, hydrolyzing ATP and pumping protons out of the matrix across the membrane. However, when the proton-motive force is high, the reaction is forced to run in the opposite direction; it proceeds from left to right, allowing protons to flow down their concentration gradient and turning ADP into ATP. Indeed, in the closely related vacuolar type H+-ATPases, the hydrolysis reaction is used to acidify cellular compartments, by pumping protons and hydrolysing ATP.

ATP synthase is a massive protein complex with a mushroom-like shape. The mammalian enzyme complex contains 16 subunits and has a mass of approximately 600 kilodaltons. The portion embedded within the membrane is called F and contains a ring of c subunits and the proton channel. The stalk and the ball-shaped headpiece is called F and is the site of ATP synthesis. The ball-shaped complex at the end of the F portion contains six proteins of two different kinds (three α subunits and three β subunits), whereas the "stalk" consists of one protein: the γ subunit, with the tip of the stalk extending into the ball of α and β subunits. Both the α and β subunits bind nucleotides, but only the β subunits catalyze the ATP synthesis reaction. Reaching along the side of the F portion and back into the membrane is a long rod-like subunit that anchors the α and β subunits into the base of the enzyme.

As protons cross the membrane through the channel in the base of ATP synthase, the F proton-driven motor rotates. Rotation might be caused by changes in the ionization of amino acids in the ring of c subunits causing electrostatic interactions that propel the ring of c subunits past the proton channel. This rotating ring in turn drives the rotation of the central axle (the γ subunit stalk) within the α and β subunits. The α and β subunits are prevented from rotating themselves by the side-arm, which acts as a stator. This movement of the tip of the γ subunit within the ball of α and β subunits provides the energy for the active sites in the β subunits to undergo a cycle of movements that produces and then releases ATP.

This ATP synthesis reaction is called the "binding change mechanism" and involves the active site of a β subunit cycling between three states. In the "open" state, ADP and phosphate enter the active site (shown in brown in the diagram). The protein then closes up around the molecules and binds them loosely – the "loose" state (shown in red). The enzyme then changes shape again and forces these molecules together, with the active site in the resulting "tight" state (shown in pink) binding the newly produced ATP molecule with very high affinity. Finally, the active site cycles back to the open state, releasing ATP and binding more ADP and phosphate, ready for the next cycle.

In some bacteria and archaea, ATP synthesis is driven by the movement of sodium ions through the cell membrane, rather than the movement of protons. Archaea such as "Methanococcus" also contain the AA synthase, a form of the enzyme that contains additional proteins with little similarity in sequence to other bacterial and eukaryotic ATP synthase subunits. It is possible that, in some species, the AA form of the enzyme is a specialized sodium-driven ATP synthase, but this might not be true in all cases.

Molecular oxygen is an ideal terminal electron acceptor because it is a strong oxidizing agent. The reduction of oxygen does involve potentially harmful intermediates. Although the transfer of four electrons and four protons reduces oxygen to water, which is harmless, transfer of one or two electrons produces superoxide or peroxide anions, which are dangerously reactive.

These reactive oxygen species and their reaction products, such as the hydroxyl radical, are very harmful to cells, as they oxidize proteins and cause mutations in DNA. This cellular damage might contribute to disease and is proposed as one cause of aging.

The cytochrome c oxidase complex is highly efficient at reducing oxygen to water, and it releases very few partly reduced intermediates; however small amounts of superoxide anion and peroxide are produced by the electron transport chain. Particularly important is the reduction of coenzyme Q in complex III, as a highly reactive ubisemiquinone free radical is formed as an intermediate in the Q cycle. This unstable species can lead to electron "leakage" when electrons transfer directly to oxygen, forming superoxide. As the production of reactive oxygen species by these proton-pumping complexes is greatest at high membrane potentials, it has been proposed that mitochondria regulate their activity to maintain the membrane potential within a narrow range that balances ATP production against oxidant generation. For instance, oxidants can activate uncoupling proteins that reduce membrane potential.

To counteract these reactive oxygen species, cells contain numerous antioxidant systems, including antioxidant vitamins such as vitamin C and vitamin E, and antioxidant enzymes such as superoxide dismutase, catalase, and peroxidases, which detoxify the reactive species, limiting damage to the cell.

There are several well-known drugs and toxins that inhibit oxidative phosphorylation. Although any one of these toxins inhibits only one enzyme in the electron transport chain, inhibition of any step in this process will halt the rest of the process. For example, if oligomycin inhibits ATP synthase, protons cannot pass back into the mitochondrion. As a result, the proton pumps are unable to operate, as the gradient becomes too strong for them to overcome. NADH is then no longer oxidized and the citric acid cycle ceases to operate because the concentration of NAD falls below the concentration that these enzymes can use.

Not all inhibitors of oxidative phosphorylation are toxins. In brown adipose tissue, regulated proton channels called uncoupling proteins can uncouple respiration from ATP synthesis. This rapid respiration produces heat, and is particularly important as a way of maintaining body temperature for hibernating animals, although these proteins may also have a more general function in cells' responses to stress.

The field of oxidative phosphorylation began with the report in 1906 by Arthur Harden of a vital role for phosphate in cellular fermentation, but initially only sugar phosphates were known to be involved. However, in the early 1940s, the link between the oxidation of sugars and the generation of ATP was firmly established by Herman Kalckar, confirming the central role of ATP in energy transfer that had been proposed by Fritz Albert Lipmann in 1941. Later, in 1949, Morris Friedkin and Albert L. Lehninger proved that the coenzyme NADH linked metabolic pathways such as the citric acid cycle and the synthesis of ATP. The term "oxidative phosphorylation" was coined by in 1939.

For another twenty years, the mechanism by which ATP is generated remained mysterious, with scientists searching for an elusive "high-energy intermediate" that would link oxidation and phosphorylation reactions. This puzzle was solved by Peter D. Mitchell with the publication of the chemiosmotic theory in 1961. At first, this proposal was highly controversial, but it was slowly accepted and Mitchell was awarded a Nobel prize in 1978. Subsequent research concentrated on purifying and characterizing the enzymes involved, with major contributions being made by David E. Green on the complexes of the electron-transport chain, as well as Efraim Racker on the ATP synthase. A critical step towards solving the mechanism of the ATP synthase was provided by Paul D. Boyer, by his development in 1973 of the "binding change" mechanism, followed by his radical proposal of rotational catalysis in 1982. More recent work has included structural studies on the enzymes involved in oxidative phosphorylation by John E. Walker, with Walker and Boyer being awarded a Nobel Prize in 1997.







</doc>
<doc id="22775" url="https://en.wikipedia.org/wiki?curid=22775" title="Old Fashioned">
Old Fashioned

The Old Fashioned is a cocktail made by muddling sugar with bitters, then adding alcohol, originally whiskey but now sometimes brandy, and finally a twist of citrus rind. It is traditionally served in a short, round, tumbler-like glass, which is called an Old Fashioned glass, named after the drink.

The Old Fashioned, developed during the 19th century and given its name in the 1880s, is an IBA Official Cocktail. It is also one of six basic drinks listed in David A. Embury's "The Fine Art of Mixing Drinks".

The first documented definition of the word "cocktail" was in response to a reader's letter asking to define the word in the May 6, 1806 issue of "The Balance and Columbian Repository" in Hudson, New York. In the May 13, 1806 issue, the paper's editor wrote that it was a potent concoction of spirits, bitters, water, and sugar; it was also referred to at the time as a bittered sling.
J.E. Alexander describes the cocktail similarly in 1833, as he encountered it in New York City, as being rum, gin, or brandy, significant water, bitters, and sugar, though he includes a nutmeg garnish as well.

By the 1860s, it was common for orange curaçao, absinthe, and other liqueurs to be added to the cocktail. The original concoction, albeit in different proportions, came back into vogue, and was referred to as "old-fashioned". The most popular of the in-vogue "old-fashioned" cocktails were made with whiskey, according to a Chicago barman, quoted in the "Chicago Daily Tribune" in 1882, with rye being more popular than Bourbon. The recipe he describes is a similar combination of spirits, bitters, water and sugar of seventy-six years earlier.

The first use of the name "Old Fashioned" for a Bourbon whiskey cocktail was said to have been at the Pendennis Club, a gentlemen's club founded in 1881 in Louisville, Kentucky. The recipe was said to have been invented by a bartender at that club in honor of Colonel James E. Pepper, a prominent bourbon distiller, who brought it to the Waldorf-Astoria Hotel bar in New York City.

With its conception rooted in the city's history, in 2015 the city of Louisville named the Old Fashioned as its official cocktail. Each year, during the first two weeks of June, Louisville celebrates "Old Fashioned Fortnight" which encompasses bourbon events, cocktail specials and National Bourbon Day which is always celebrated on June 14.

George Kappeler provides several of the earliest published recipes for Old Fashioned cocktails in his 1895 book. Recipes are given for Whiskey, Brandy, Holland gin, and Old Tom gin. The Whiskey Old Fashioned recipe specifies the following (with a jigger being ):

By the 1860s, as illustrated by Jerry Thomas' 1862 book, basic cocktail recipes included Curaçao, or other liqueurs. These liqueurs were not mentioned in the early 19th century descriptions, nor the "Chicago Daily Tribune" descriptions of the "Old Fashioned" cocktails of the early 1880s; they were absent from Kappeler's Old Fashioned recipes as well. The differences of the Old Fashioned cocktail recipes from the cocktail recipes of the late 19th Century are mainly preparation method, the use of sugar and water in lieu of simple or gomme syrup, and the absence of additional liqueurs. These Old Fashioned cocktail recipes are literally for cocktails done the old-fashioned way.

A book by David Embury published in 1948 provides a slight variation, specifying 12 parts American whiskey, 1 part simple syrup, 1-3 dashes Angostura bitters, a twist of lemon peel over the top, and serve garnished with the lemon peel.

Two additional recipes from the 1900s vary in the precise ingredients, but omit the cherry which was introduced after 1930 as well as the soda water which the occasional recipe calls for. Orange bitters were a popular ingredient in the late 19th century.

The original Old Fashioned recipe would have showcased the whiskey available in America in the 19th century: Irish, Bourbon or rye whiskey. But in some regions, especially Wisconsin, brandy is substituted for whiskey (sometimes called a Brandy Old Fashioned). Eventually the use of other spirits became common, such as a gin recipe becoming popularized in the late 1940s.

Common garnishes for an Old Fashioned include an orange slice or a maraschino cherry or both, although these modifications came around 1930, some time after the original recipe was invented. While some recipes began making sparse use of the orange zest for flavor, the practice of muddling orange and other fruit gained prevalence as late as the 1990s.

The Old Fashioned is the cocktail of choice of Don Draper, the lead character on the "Mad Men" television series, set in the 1960s. The use of the drink in the series coincided with a renewed interest in this and other classic cocktails in the 2000s.

In the movie It's a Mad Mad Mad Mad World (1963), pilot Tyler Fitzgerald (Jim Backus) directs passenger Dingy Bell (Mickey Rooney) to the aircraft's bar to "make us some Old Fashioneds." Annoyed by suggestions that he should limit drinking while piloting an airplane, and finding Bell's Old Fashioneds too sweet, Fitzgerald turns the controls over to Bell's sidekick Benjy Benjamin (Buddy Hackett) and retires to the back of the plane to "make some Old Fashioneds the Old Fashioned way, the way dear old dad used to." When Benjamin asks what if something happens, Fitzgerald replies, "What could happen to an Old Fashioned?"





</doc>
<doc id="22776" url="https://en.wikipedia.org/wiki?curid=22776" title="Omnipotence">
Omnipotence

Omnipotence is the quality of having unlimited power. Monotheistic religions generally attribute omnipotence to only the deity of their faith. In the monotheistic philosophies of Abrahamic religions, omnipotence is often listed as one of a deity's characteristics among many, including omniscience, omnipresence, and omnibenevolence. The presence of all these properties in a single entity has given rise to considerable theological debate, prominently including the problem of theodicy, the question of why such a deity would permit the manifestation of evil.

The term omnipotent has been used to connote a number of different positions. These positions include, but are not limited to, the following:

Under many philosophical definitions of the term "deity", senses 2, 3 and 4 can be shown to be equivalent. However, on all understandings of omnipotence, it is generally held that a deity is able to intervene in the world by superseding the laws of physics, since they are not part of its nature, but the principles on which it has created the physical world. However many modern scholars (such as John Polkinghorne) hold that it is part of a deity's nature to be consistent and that it would be inconsistent for a deity to go against its own laws unless there were an overwhelming reason to do so.

The word "Omnipotence" derives from the Latin term ""Omni Potens"", meaning "All-Powerful" instead of "Infinite Power" implied by its English counterpart. The term could be applied to both deities and Roman Emperors. Being the one with "All the power", it was not uncommon for nobles to attempt to prove their Emperor's ""Omni Potens"" to the people, by demonstrating his effectiveness at leading the Empire.

St. Thomas Aquinas, OP acknowledged difficulty in comprehending the Deity's power: "All confess that God is omnipotent; but it seems difficult to explain in what His omnipotence precisely consists: for there may be doubt as to the precise meaning of the word 'all' when we say that God can do all things. If, however, we consider the matter aright, since power is said in reference to possible things, this phrase, 'God can do all things,' is rightly understood to mean that God can do all things that are possible; and for this reason He is said to be omnipotent." In the scholastic understanding, omnipotence is generally understood to be compatible with certain limitations or restrictions. A proposition that is necessarily true is one whose negation is self-contradictory.

St. Thomas explains that:

Omnipotence is all-sufficient power. The adaptation of means to ends in the universe does not argue, as J. S. Mill would have it, that the power of the designer is limited, but only that God has willed to manifest his glory by a world so constituted rather than by another. Indeed, the production of secondary causes, capable of accomplishing certain effects, requires greater power than the direct accomplishment of these same effects. On the other hand, even though no creature existed, God's power would not be barren, for "creatures are not an end to God." Regarding the Deity's power, medieval theologians contended that there are certain things that even an omnipotent deity cannot do. The statement "a deity can do anything" is only sensible with an assumed suppressed clause, "that implies the perfection of true power". This standard scholastic answer allows that acts of creatures such as walking can be performed by humans but not by a deity. Rather than an advantage in power, human acts such as walking, sitting, or giving birth were possible only because of a "defect" in human power. The capacity to sin, for example, is not a power but a defect or infirmity. In response to questions of a deity performing impossibilities, e.g. making square circles, St. Thomas says that "everything that does not imply a contradiction in terms, is numbered amongst those possible things, in respect of which God is called omnipotent: whereas whatever implies contradiction does not come within the scope of divine omnipotence, because it cannot have the aspect of possibility. Hence it is better to say that such things cannot be done, than that God cannot do them. Nor is this contrary to the word of the angel, saying: 'No word shall be impossible with God.' For whatever implies a contradiction cannot be a word, because no intellect can possibly conceive such a thing."

In recent times, C. S. Lewis has adopted a scholastic position in the course of his work "The Problem of Pain". Lewis follows Aquinas' view on contradiction:
Early Freudianism saw a feeling of omnipotence as intrinsic to early childhood. 'As Freud and Ferenczi have shown, the child lives in a sort of megalomania for a long period...the "fiction of omnipotence"'. At birth. 'the baby "is" everything "as far as he knows" - "all powerful"...every step he takes towards establishing his own limits and boundaries will be painful because he'll have to lose this original God-like feeling of omnipotence'.

Freud considered that in a neurotic 'the "omnipotence" which he ascribed to his thoughts and feelings...is a frank acknowledgement of a relic of the old megalomania of infancy'. In some narcissists, the 'period of primary narcissism which subjectively did not need any objects and was entirely independent...may be retained or regressively regained..."omnipotent" behavior'.

D. W. Winnicott took a more positive view of a belief in early omnipotence, seeing it as essential to the child's well-being; and "good-enough" mothering as essential to enable the child to 'cope with the immense shock of loss of omnipotence' - as opposed to whatever 'prematurely forces it out of its narcissistic universe'.

Omnipotence can be a reaction to extreme threat. For example, after the attacks on the World Trade Center in New York City on September 11, 2001, a man dreamt: "I am a prisoner of war in a German camp during World War II. Single-handedly, I escape to another camp, which I liberate by tying a bungee cord around it. I see Hogan from Hogan’s Heroes, except that Hogan is a German officer, and I must evade him, since Hogan knows all of my tricks."

Some monotheists reject the view that a deity is or could be omnipotent, or take the view that, by choosing to create creatures with freewill, a deity has chosen to limit divine omnipotence. In Conservative and Reform Judaism, and some movements within Protestant Christianity, including open theism, deities are said to act in the world through persuasion, and not by coercion (this is a matter of choice—a deity could act miraculously, and perhaps on occasion does so—while for process theism it is a matter of necessity—creatures have inherent powers that a deity cannot, even in principle, override). Deities are manifested in the world through inspiration and the creation of possibility, not necessarily by miracles or violations of the laws of nature.

The rejection of omnipotence often follows from either philosophical or scriptural considerations, discussed below.

Process theology rejects unlimited omnipotence on a philosophical basis, arguing that omnipotence as classically understood would be less than perfect, and is therefore incompatible with the idea of a perfect deity. The idea is grounded in Plato's oft-overlooked statement that "being is power."
From this premise, Charles Hartshorne argues further that:
The argument can be stated as follows:

For example, though someone might control a lump of jelly-pudding almost completely, the inability of that pudding to stage any resistance renders that person's power rather unimpressive. Power can only be said to be great if it is over something that has defenses and its own agenda. If a deity's power is to be great, it must therefore be over beings that have at least some of their own defenses and agenda. Thus, if a deity does not have absolute power, it must therefore embody some of the characteristics of power, and some of the characteristics of persuasion. This view is known as dipolar theism.

The most popular works espousing this point are from Harold Kushner (in Judaism). The need for a modified view of omnipotence was also articulated by Alfred North Whitehead in the early 20th century and expanded upon by the aforementioned philosopher Charles Hartshorne. Hartshorne proceeded within the context of the theological system known as process theology.

In the Authorized King James Version of the Bible, as well as several other versions, in Revelation 19:6 it is stated "...the Lord God omnipotent reigneth" (the original Greek word is παντοκράτωρ, "all-mighty").

All the above stated claims of power are each based on scriptural grounds and upon empirical human perception. This perception is limited to our senses. The power of a deity is related to its existence.There are however other ways of perception like: reason, intuition, revelation, divine inspiration, religious experience, mystical states, and historical testimony.

According to the Hindu philosophy the essence of God or Brahman can never be understood or known since Brahman is beyond both existence and non-existence, transcending and including time, causation and space, and thus can never be known in the same material sense as one traditionally 'understands' a given concept or object.

So presuming there is a god-like entity consciently taking actions, we cannot comprehend the limits of a deity's powers.

Since the current laws of physics are only known to be valid in this universe, it is possible that the laws of physics are different in parallel universes, giving a God-like entity more power. If the number of universes is unlimited, then the power of a certain God-like entity is also unlimited, since the laws of physics may be different in other universes, and accordingly making this entity omnipotent. Unfortunately concerning a multiverse there is a lack of empirical correlation. To the extreme there are theories about realms beyond this multiverse (Nirvana, Chaos, Nothingness).

Also trying to develop a theory to explain, assign or reject omnipotence on grounds of logic has little merit, since being omnipotent, in a Cartesian sense, would mean the omnipotent being is above logic, a view supported by René Descartes. He issues this idea in his Meditations on First Philosophy. This view is called universal possibilism.

Allowing assumption that a deity exists, further debate may be provoked that said deity is consciously taking actions. It could be concluded from an emanationism point of view, that all actions and creations by a deity are simply flows of divine energy (the flowing Tao in conjunction with qi is often seen as a river; Dharma (Buddhism) the law of nature discovered by Buddha has no beginning or end.)
Pantheism and pandeism see the universe/multiverse itself as God (or, at least, the current state of God), while panentheism sees the universe/multiverse as 'the body of God', making 'God' everybody and everything. So if one does something, actually 'God' is doing it. We are 'God's' means according to this view.

In the Taoist religious or philosophical tradition, the Tao is in some ways equivalent to a deity or the logos. The Tao is understood to have inexhaustible power, yet that power is simply another aspect of its weakness.





</doc>
<doc id="22780" url="https://en.wikipedia.org/wiki?curid=22780" title="Octopus">
Octopus

The octopus is a soft-bodied, eight-limbed mollusc of the order Octopoda. Around 300 species are recognised and the order is grouped within the class Cephalopoda with squids, cuttlefish and nautiloids. Like other cephalopods, the octopus is bilaterally symmetric with two eyes and a beak, with its mouth at the centre point of the eight limbs (traditionally called "arms", sometimes mistakenly called "tentacles"). The soft body can rapidly alter its shape, enabling octopuses to squeeze through small gaps. They trail their eight appendages behind them as they swim. The siphon is used both for respiration and for locomotion, by expelling a jet of water. Octopuses have a complex nervous system and excellent sight, and are among the most intelligent and behaviourally diverse of all invertebrates.

Octopuses inhabit various regions of the ocean, including coral reefs, pelagic waters, and the seabed; some live in the intertidal zone and others at abyssal depths. Most species grow fast, mature early and are short-lived. During breeding, the male uses a specially adapted arm to deliver a bundle of sperm directly into the female's mantle cavity, after which he becomes senescent and dies. The female deposits fertilised eggs in a den and cares for them until they hatch, after which she also dies. Strategies to defend themselves against predators include the expulsion of ink, the use of camouflage and threat displays, their abilities to jet quickly through the water and hide, and even through deceit. All octopuses are venomous, but only the blue-ringed octopuses are known to be deadly to humans.

Octopuses appear in mythology as sea monsters like the Kraken of Norway and the Akkorokamui of the Ainu, and probably the Gorgon of ancient Greece. A battle with an octopus appears in Victor Hugo's book "Toilers of the Sea", inspiring other works such as Ian Fleming's "Octopussy". Octopuses appear in Japanese erotic art, "shunga". They are eaten and considered a delicacy by humans in many parts of the world, especially the Mediterranean and the Asian seas.

The scientific Latin term "octopus" was derived from Ancient Greek ὀκτώπους, a compound form of ὀκτώ ("oktō", "eight") and πούς ("pous", "foot"), itself a variant form of ὀκτάπους, a word used for example by Alexander of Tralles (c. 525–605) for the common octopus. The standard pluralised form of "octopus" in English is "octopuses"; the Ancient Greek plural ὀκτώποδες, "octopodes" (), has also been used historically. The alternative plural "octopi" – which wrongly assumes it is a Latin second declension "-us" noun or adjective when, in either Greek or Latin, it is a third declension one – is considered grammatically incorrect, but is used frequently enough to be acknowledged by the descriptivist "Merriam-Webster 11th Collegiate Dictionary" and "Webster's New World College Dictionary". The "Oxford English Dictionary" lists "octopuses", "octopi", and "octopodes", in that order, reflecting frequency of use, calling "octopodes" rare and noting that "octopi" is based on a misunderstanding. The "New Oxford American Dictionary" (3rd Edition, 2010) lists "octopuses" as the only acceptable pluralisation, and indicates that "octopodes" is still occasionally used, but that "octopi" is incorrect.

The giant Pacific octopus "(Enteroctopus dofleini)" is often cited as the largest known octopus species. Adults usually weigh around 15 kg (33 lb), with an arm span of up to 4.3 m (14 ft). The largest specimen of this species to be scientifically documented was an animal with a live mass of 71 kg (156.5 lb). Much larger sizes have been claimed for the giant Pacific octopus: one specimen was recorded as 272 kg (600 lb) with an arm span of 9 m (30 ft).
A carcass of the seven-arm octopus, "Haliphron atlanticus", weighed 61 kg (134 lb) and was estimated to have had a live mass of 75 kg (165 lb). The smallest species is "Octopus wolfi", which is around and weighs less than .

The octopus is bilaterally symmetrical along its dorso-ventral axis; the head and foot are at one end of an elongated body and function as the anterior (front) of the animal. The head includes the mouth and brain. The foot has evolved into a set of flexible, prehensile appendages, known as "arms", that surround the mouth and are attached to each other near their base by a webbed structure. The arms can be described based on side and sequence position (such as L1, R1, L2, R2) and divided into four pairs. The two rear appendages are generally used to walk on the sea floor, while the other six are used to forage for food, hence some biologists refer to the animals has having six "arms" and two "legs". The bulbous and hollow mantle is fused to the back of the head and is known as the visceral hump; it contains most of the vital organs. The mantle cavity has muscular walls and contains the gills; it is connected to the exterior by a funnel or siphon. The mouth of an octopus, located underneath the arms, has a sharp hard beak.

The skin consists of a thin outer epidermis with mucous cells and sensory cells, and a connective tissue dermis consisting largely of collagen fibres and various cells allowing colour change. Most of the body is made of soft tissue allowing it to lengthen, contract, and contort itself. The octopus can squeeze through tiny gaps; even the larger species can pass through an opening close to in diameter. Lacking skeletal support, the arms work as muscular hydrostats and contain longitudinal, transverse and circular muscles around a central axial nerve. They can extend and contract, twist to left or right, bend at any place in any direction or be held rigid.

The interior surfaces of the arms are covered with circular, adhesive suckers. The suckers allow the octopus to anchor itself or to manipulate objects. Each sucker is usually circular and bowl-like and has two distinct parts: an outer shallow cavity called an infundibulum and a central hollow cavity called an acetabulum, both of which are thick muscles covered in a protective chitinous cuticle. When a sucker attaches to a surface, the orifice between the two structures is sealed. The infundibulum provides adhesion while the acetabulum remains free, and muscle contractions allow for attachment and detachment.
The eyes of the octopus are large and are at the top of the head. They are similar in structure to those of a fish and are enclosed in a cartilaginous capsule fused to the cranium. The cornea is formed from a translucent epidermal layer and the slit-shaped pupil forms a hole in the iris and lies just behind. The lens is suspended behind the pupil and photoreceptive retinal cells cover the back of the eye. The pupil can be adjusted in size and a retinal pigment screens incident light in bright conditions.

Some species differ in form from the typical octopus body shape. Members of the suborder Cirrina have stout gelatinous bodies with webbing that reaches near the tip of their arms, and two large fins above the eyes, supported by an internal shell. Fleshy papillae or cirri are found along the bottom of the arms, and the eyes are more developed.

Octopuses have a closed circulatory system, where the blood remains inside blood vessels. Octopuses have three hearts; a systemic heart that circulates blood round the body and two branchial hearts that pump it through each of the two gills. The systemic heart is inactive when the animal is swimming and thus it tires quickly and prefers to crawl.
Octopus blood contains the copper-rich protein haemocyanin to transport oxygen. This makes the blood very viscous and it requires considerable pressure to pump it round the body; octopuses' blood pressures can exceed 75 mmHg. In cold conditions with low oxygen levels, haemocyanin transports oxygen more efficiently than haemoglobin. The haemocyanin is dissolved in the plasma instead of being carried within blood cells, and gives the blood a bluish colour.

The systemic heart has muscular contractile walls and consists of a single ventricle and two atria, one for each side of the body. The blood vessels consist of arteries, capillaries and veins and are lined with a cellular endothelium which is quite unlike that of most other invertebrates. The blood circulates through the aorta and capillary system, to the vena cavae, after which the blood is pumped through the gills by the auxiliary hearts and back to the main heart. Much of the venous system is contractile, which helps circulate the blood.

Respiration involves drawing water into the mantle cavity through an aperture, passing it through the gills, and expelling it through the siphon. The ingress of water is achieved by contraction of radial muscles in the mantle wall, and flapper valves shut when strong circular muscles force the water out through the siphon. Extensive connective tissue lattices support the respiratory muscles and allow them to expand the respiratory chamber. The lamella structure of the gills allows for a high oxygen uptake; up to 65% in water at . Water flow over the gills correlates with locomotion, and an octopus can propel its body when it expels water out of its siphon.

The thin skin of the octopus absorbs additional oxygen. When resting, around 41% of an octopus's oxygen absorption is through the skin. This decreases to 33% when it swims, as more water flows over the gills; skin oxygen uptake also increases. When it is resting after a meal, absorption through the skin can drop to 3% of its total oxygen uptake.

The digestive system of the octopus begins with the buccal mass which consists of the mouth, pharynx, radula and salivary glands. The radula is a spiked, tongue-like organ made of chitin. Food is broken down and is forced into the oesophagus by two lateral extensions of the esophageal side walls in addition to the radula. From there it is transferred to the gastrointestinal tract, which is mostly suspended from the roof of the mantle cavity by numerous membranes. The tract consists of a crop, where the food is stored; a stomach, where food is ground down; a caecum where the now sludgy food is sorted into fluids and particles and which plays an important role in absorption; the digestive gland, where liver cells break down and absorb the fluid and become "brown bodies"; and the intestine, where the accumulated waste is turned into faecal ropes by secretions and blown out of the funnel via the rectum.

During osmoregulation, fluid is added to the pericardia of the branchial hearts. The octopus has two nephridia (equivalent to vertebrate kidneys) which are associated with the branchial hearts; these and their associated ducts connect the pericardial cavities with the mantle cavity. Before reaching the branchial heart, each branch of the vena cava expands to form renal appendages which are in direct contact with the thin-walled nephridium. The urine is first formed in the pericardial cavity, and is modified by excretion, chiefly of ammonia, and selective absorption from the renal appendages, as it is passed along the associated duct and through the nephridiopore into the mantle cavity.

The octopus (along with cuttlefish) has the highest brain-to-body mass ratios of all invertebrates; it is also greater than that of many vertebrates. It has a highly complex nervous system, only part of which is localised in its brain, which is contained in a cartilaginous capsule. Two-thirds of an octopus's neurons are found in the nerve cords of its arms, which show a variety of complex reflex actions that persist even when they have no input from the brain. Unlike vertebrates, the complex motor skills of octopuses are not organised in their brain via an internal somatotopic map of its body, instead using a nonsomatotopic system unique to large-brained invertebrates.

Like other cephalopods, octopuses can distinguish the polarisation of light. Colour vision appears to vary from species to species, for example being present in "O. aegina" but absent in "O. vulgaris". Researchers believe that opsins in the skin can sense different wavelengths of light and help the creatures choose a coloration that camouflages them, in addition to light input from the eyes. Other researchers hypothesise that cephalopod eyes in species which only have a single photoreceptor protein may use chromatic aberration to turn monochromatic vision into colour vision, though this sacrifices image quality. This would explain pupils shaped like the letter U, the letter W, or a dumbbell, as well as explaining the need for colourful mating displays.

Attached to the brain are two special organs called statocysts (sac-like structures containing a mineralised mass and sensitive hairs), that allow the octopus to sense the orientation of its body. They provide information on the position of the body relative to gravity and can detect angular acceleration. An autonomic response keeps the octopus's eyes oriented so that the pupil is always horizontal. Octopuses may also use the statocyst to hear sound. The common octopus can hear sounds between 400 Hz and 1000 Hz, and hears best at 600 Hz.

Octopuses also have an excellent sense of touch. The octopus's suction cups are equipped with chemoreceptors so the octopus can taste what it touches. Octopus arms do not become tangled or stuck to each other because the sensors recognise octopus skin and prevent self-attachment.

The arms contain tension sensors so the octopus knows whether its arms are stretched out, but this is not sufficient for the brain to determine the position of the octopus's body or arms. As a result, the octopus does not possess stereognosis; that is, it does not form a mental image of the overall shape of the object it is handling. It can detect local texture variations, but cannot integrate the information into a larger picture. The neurological autonomy of the arms means the octopus has great difficulty learning about the detailed effects of its motions. It has a poor proprioceptive sense, and it knows what exact motions were made only by observing the arms visually.

The ink sac of an octopus is located under the digestive gland. A gland attached to the sac produces the ink, and the sac stores it. The sac is close enough to the funnel for the octopus to shoot out the ink with a water jet. Before it leaves the funnel, the ink passes through glands which mix it with mucus, creating a thick, dark blob which allows the animal to escape from a predator. The main pigment in the ink is melanin, which gives it its black colour. Cirrate octopuses lack the ink sac.

Octopuses are gonochoric and have a single, posteriorly-located gonad which is associated with the coelom. The testis in males and the ovary in females bulges into the gonocoel and the gametes are released here. The gonocoel is connected by the gonoduct to the mantle cavity, which it enters at the gonopore. An optic gland creates hormones that cause the octopus to mature and age and stimulate gamete production. The gland may be triggered by environmental conditions such as temperature, light and nutrition, which thus control the timing of reproduction and lifespan.

When octopuses reproduce, the male uses a specialised arm called a hectocotylus to transfer spermatophores (packets of sperm) from the terminal organ of the reproductive tract (the cephalopod "penis") into the female's mantle cavity. The hectocotylus in benthic octopuses is usually the third right arm, which has a spoon-shaped depression and modified suckers near the tip. In most species, fertilisation occurs in the mantle cavity.

The reproduction of octopuses has been studied in only a few species. One such species is the giant Pacific octopus, in which courtship is accompanied, especially in the male, by changes in skin texture and colour. The male may cling to the top or side of the female or position himself beside her. There is some speculation that he may first use his hectocotylus to remove any spermatophore or sperm already present in the female. He picks up a spermatophore from his spermatophoric sac with the hectocotylus, inserts it into the female's mantle cavity, and deposits it in the correct location for the species, which in the giant Pacific octopus is the opening of the oviduct. Two spermatophores are transferred in this way; these are about one metre (yard) long, and the empty ends may protrude from the female's mantle. A complex hydraulic mechanism releases the sperm from the spermatophore, and it is stored internally by the female.

About forty days after mating, the female giant Pacific octopus attaches strings of small fertilised eggs (10,000 to 70,000 in total) to rocks in a crevice or under an overhang. Here she guards and cares for them for about five months (160 days) until they hatch. In colder waters, such as those off of Alaska, it may take as much as 10 months for the eggs to completely develop. The female aerates the eggs and keeps them clean; if left untended, many eggs will not hatch. She does not feed during this time and dies soon afterwards. Males become senescent and die a few weeks after mating.

The eggs have large yolks; cleavage (division) is superficial and a germinal disc develops at the pole. During gastrulation, the margins of this grow down and surround the yolk, forming a yolk sac, which eventually forms part of the gut. The dorsal side of the disc grows upwards and forms the embryo, with a shell gland on its dorsal surface, gills, mantle and eyes. The arms and funnel develop as part of the foot on the ventral side of the disc. The arms later migrate upwards, coming to form a ring around the funnel and mouth. The yolk is gradually absorbed as the embryo develops.

Most young octopuses hatch as paralarvae and are planktonic for weeks to months, depending on the species and water temperature. They feed on copepods, arthropod larvae and other zooplankton, eventually settling on the ocean floor and developing directly into adults with no distinct metamorphoses that are present in other groups of mollusc larvae. Octopus species that produce larger eggs – including the southern blue-ringed, Caribbean reef, California two-spot, "Eledone moschata" and deep sea octopuses – do not have a paralarval stage, but hatch as benthic animals similar to the adults.

In the argonaut (paper nautilus), the female secretes a fine, fluted, papery shell in which the eggs are deposited and in which she also resides while floating in mid-ocean. In this she broods the young, and it also serves as a buoyancy aid allowing her to adjust her depth. The male argonaut is minute by comparison and has no shell.

Octopuses have a relatively short life expectancy; some species live for as little as six months. The giant Pacific octopus, one of the two largest species of octopus, may live for as much as five years. Octopus lifespan is limited by reproduction: males can live for only a few months after mating, and females die shortly after their eggs hatch. Octopus reproductive organs mature due to the hormonal influence of the optic gland but result in the inactivation of their digestive glands, typically causing the octopus to die from starvation. Experimental removal of both optic glands after spawning was found to result in the cessation of broodiness, the resumption of feeding, increased growth, and greatly extended lifespans.

Octopuses live in every ocean, and different species have adapted to different marine habitats. As juveniles, common octopuses inhabit shallow tide pools. The Hawaiian day octopus ("Octopus cyanea") lives on coral reefs; argonauts drift in pelagic waters. "Abdopus aculeatus" mostly lives in near-shore seagrass beds. Some species are adapted to the cold, ocean depths. The spoon-armed octopus ("Bathypolypus arcticus") is found in abyssal plains at depths of , and "Vulcanoctopus hydrothermalis" lives near hydrothermal vents at . The cirrate species are often free-swimming and live in deep-water habitats. No species are known to live in fresh water.

Most species are solitary when not mating, though a few are known to occur in high densities and with frequent interactions, signaling, mate defending and eviction of individuals from dens. This is likely the result of abundant food supplies combined with limited den sites. Octopuses hide in dens, which are typically crevices in rocky outcrops or other hard structures, though some species burrow into sand or mud. Octopuses are not territorial but generally remain in a home range; they may leave the area in search of food. They can use navigation skills to return to a den without having to retrace their outward route. They are not known to be migratory.

Octopuses bring captured prey back to the den where they can eat it safely. Sometimes the octopus catches more prey than it can eat, and the den is often surrounded by a midden of dead and uneaten food items. Other creatures, such as fish, crabs, molluscs and echinoderms, often share the den with the octopus, either because they have arrived as scavengers, or because they have survived capture.

Nearly all octopuses are predatory; bottom-dwelling octopuses eat mainly crustaceans, polychaete worms, and other molluscs such as whelks and clams; open-ocean octopuses eat mainly prawns, fish and other cephalopods. Major items in the diet of the giant Pacific octopus include bivalve molluscs such as the cockle "Clinocardium nuttallii", clams and scallops, and crustaceans such as crabs and spider crabs. Prey that it is likely to reject include moon snails, because they are too large, and limpets, rock scallops, chitons and abalone, because they are too securely fixed to the rock.

A bottom-dwelling octopus typically moves among the rocks and feels through the crevices. The creature may make a jet-propelled pounce on prey and pull it towards the mouth with its arms, the suckers restraining it. Small prey may be completely trapped by the webbed structure. Octopuses usually inject crustaceans like crabs with a paralysing saliva then dismember them with their beaks. Octopuses feed on shelled molluscs either by forcing the valves apart, or by drilling a hole in the shell to inject a nerve toxin. It used to be thought that the hole was drilled by the radula, but it has now been shown that minute teeth at the tip of the salivary papilla are involved, and an enzyme in the toxic saliva is used to dissolve the calcium carbonate of the shell. It takes about three hours for "O. vulgaris" to create a hole. Once the shell is penetrated, the prey dies almost instantaneously, its muscles relax, and the soft tissues are easy for the octopus to remove. Crabs may also be treated in this way; tough-shelled species are more likely to be drilled, and soft-shelled crabs are torn apart.

Some species have other modes of feeding. "Grimpoteuthis" has a reduced or non-existent radula and swallows prey whole. In the deep-sea genus "Stauroteuthis", some of the muscle cells that control the suckers in most species have been replaced with photophores which are believed to fool prey by directing them towards the mouth, making them one of the few bioluminescent octopuses.

Octopuses mainly move about by relatively slow crawling, with some swimming in a head-first position. Jet propulsion, or backwards swimming, is their fastest means of locomotion, followed by swimming and crawling. When in no hurry, they usually crawl on either solid or soft surfaces. Several arms are extended forwards, some of the suckers adhere to the substrate and the animal hauls itself forwards with its powerful arm muscles, while other arms may push rather than pull. As progress is made, other arms move ahead to repeat these actions and the original suckers detach. During crawling, the heart rate nearly doubles, and the animal requires ten or fifteen minutes to recover from relatively minor exercise.

Most octopuses swim by expelling a jet of water from the mantle through the siphon into the sea. The physical principle behind this is that the force required to accelerate the water through the orifice produces a reaction that propels the octopus in the opposite direction. The direction of travel depends on the orientation of the siphon. When swimming, the head is at the front and the siphon is pointed backwards, but when jetting, the visceral hump leads, the siphon points towards the head and the arms trail behind, with the animal presenting a fusiform appearance. In an alternative method of swimming, some species flatten themselves dorso-ventrally, and swim with the arms held out sideways, and this may provide lift and be faster than normal swimming. Jetting is used to escape from danger, but is physiologically inefficient, requiring a mantle pressure so high as to stop the heart from beating, resulting in a progressive oxygen deficit.

Cirrate octopuses cannot produce jet propulsion and rely on their fins for swimming. They have neutral buoyancy and drift through the water with the fins extended. They can also contract their arms and surrounding web to make sudden moves known as "take-offs". Another form of locomotion is "pumping", which involves symmetrical contractions of muscles in their webs producing peristaltic waves. This moves the body slowly.

In 2005, "Adopus aculeatus" and veined octopus ("Amphioctopus marginatus") were found to walk on two arms, while at the same time mimicking plant matter. This form of locomotion allows these octopuses to move quickly away from a potential predator without being recognised. A study of this behaviour led to the suggestion that the two rearmost appendages may be more accurately termed "legs" rather than "arms". Some species of octopus can crawl out of the water briefly, which they may do between tide pools while hunting crustaceans or gastropods or to escape predators. "Stilt walking" is used by the veined octopus when carrying stacked coconut shells. The octopus carries the shells underneath it with two arms, and progresses with an ungainly gait supported by its remaining arms held rigid.

Octopuses are highly intelligent; the extent of their intelligence and learning capability are not well defined. Maze and problem-solving experiments have shown evidence of a memory system that can store both short- and long-term memory. It is not known precisely what contribution learning makes to adult octopus behaviour. Young octopuses learn nothing from their parents, as adults provide no parental care beyond tending to their eggs until the young octopuses hatch.

In laboratory experiments, octopuses can be readily trained to distinguish between different shapes and patterns. They have been reported to practise observational learning, although the validity of these findings is contested. Octopuses have also been observed in what has been described as play: repeatedly releasing bottles or toys into a circular current in their aquariums and then catching them. Octopuses often break out of their aquariums and sometimes into others in search of food. They have even boarded fishing boats and opened holds to eat crabs. The veined octopus collects discarded coconut shells, then uses them to build a shelter, an example of tool use.

Octopuses use camouflage when hunting, and to avoid predators. To do this they use specialised skin cells which change the appearance of the skin by adjusting its colour, opacity, or reflectivity. Chromatophores contain yellow, orange, red, brown, or black pigments; most species have three of these colours, while some have two or four. Other colour-changing cells are reflective iridophores and white leucophores. This colour-changing ability is also used to communicate with or warn other octopuses.

Octopuses can create distracting patterns with waves of dark coloration across the body, a display known as the "passing cloud". Muscles in the skin change the texture of the mantle to achieve greater camouflage. In some species, the mantle can take on the spiky appearance of algae; in others, skin anatomy is limited to relatively uniform shades of one colour with limited skin texture. Octopuses that are diurnal and live in shallow water have evolved more complex skin than their nocturnal and deep-sea counterparts.

A "moving rock" trick involves the octopus mimicking a rock and them inching across the open space in a speed matching the movement of in the surrounding water, allowing it to move in plain sight of a predator.

Aside from humans, octopuses may be preyed on by fishes, seabirds, sea otters, pinnipeds, cetaceans, and other cephalopods. Octopuses typically hide or disguise themselves by camouflage and mimicry; some have conspicuous warning coloration (aposematism) or deimatic behaviour. An octopus may spend 40% of its time hidden away in its den. When the octopus is approached, it may extend an arm to investigate. 66% of "Enteroctopus dolfleini" in one study had scars, with 50% having amputated arms. The blue rings of the highly venomous blue-ringed octopus are hidden in muscular skin folds which contract when the animal is threatened, exposing the iridescent warning. The Atlantic white-spotted octopus ("Callistoctopus macropus") turns bright brownish red with oval white spots all over in a high contrast display. Displays are often reinforced by stretching out the animal's arms, fins or web to make it look as big and threatening as possible.

Once they have been seen by a predator, they commonly try to escape but can also use distraction with an ink cloud ejected from the ink sac. The ink is thought to reduce the efficiency of olfactory organs, which would aid evasion from predators that employ smell for hunting, such as sharks. Ink clouds of some species might act as pseudomorphs, or decoys that the predator attacks instead.

When under attack, some octopuses can perform arm autotomy, in a manner similar to the way skinks and other lizards detach their tails. The crawling arm may distract would-be predators. Such severed arms remain sensitive to stimuli and move away from unpleasant sensations. Octopuses can replace lost limbs.

Some octopuses, such as the mimic octopus, can combine their highly flexible bodies with their colour-changing ability to mimic other, more dangerous animals, such as lionfish, sea snakes, and eels.

The diseases and parasites that affect octopuses have been little studied, but cephalopods are known to be the intermediate or final hosts of various parasitic cestodes, nematodes and copepods; 150 species of protistan and metazoan parasites have been recognised. The Dicyemidae are a family of tiny worms that are found in the renal appendages of many species; it is unclear whether they are parasitic or are endosymbionts. Coccidians in the genus "Aggregata" living in the gut cause severe disease to the host. Octopuses have an innate immune system, and the haemocytes respond to infection by phagocytosis, encapsulation, infiltration or cytotoxic activities to destroy or isolate the pathogens. The haemocytes play an important role in the recognition and elimination of foreign bodies and wound repair. Captive animals have been found to be more susceptible to pathogens than wild ones. A gram-negative bacterium, "Vibrio lentus", has been found to cause skin lesions, exposure of muscle and death of octopuses in extreme cases.

Cephalopods have existed for 500 million years and octopus ancestors were in the Carboniferous seas 300 million years ago. The oldest known octopus fossil is "Pohlsepia", which lived 296 million years ago. Researchers have identified impressions of eight arms, two eyes, and possibly an ink sac. Octopuses are mostly soft tissue, and so fossils are relatively rare. Octopuses, squids and cuttlefish belong to the clade Coleoidea. They are known as "soft-bodied" cephalopods, lacking the external shell of most molluscs and other cephalopods like the Nautiloids and the extinct Ammonoidea. Octopuses have eight limbs like other coleoids but lack the extra specialised feeding appendages known as tentacles which are longer and thinner with suckers only at their club-like ends. The vampire squid ("Vampyroteuthis") also lacks tentacles but has sensory filaments.

Two possible extant cephalopod phylogenies, based on genetics studies by Strugnell "et al." 2007, are shown in the possible cladograms.

The scientific name Octopoda was first coined and given as the order of octopuses in 1818 by English biologist William Elford Leach, who classified them as Octopoida the previous year. The Octopoda consists of around 300 known species and can be divided into two suborders, the Incirrina and the Cirrina. The incirrate octopuses (the majority of species) lack the cirri and paired swimming fins of the cirrates. In addition, the internal shell of incirrates is either present as a pair of stylets or absent altogether.

Octopuses and other coleoid cephalopods are capable of greater RNA editing (which involves changes to the nucleic acid sequence of the primary transcript of RNA molecules) than any other organisms. Editing is concentrated in the nervous system and affects proteins involved in neural excitability and neuronal morphology. More than 60% of RNA transcripts for coleoid brains are recoded by editing, compared to less than 1% for a human or fruit fly. Coleoids rely mostly on ADAR enzymes for RNA editing, which requires large double-stranded RNA structures to flank the editing sites. Both the structures and editing sites are conserved in the coleoid genome and the mutation rates for the sites are severely hampered. Hence, greater transcriptome plasticity has come as the cost of slower genome evolution. High levels of RNA editing do not appear to be present in more basal cephalopods or other molluscs.

Ancient seafaring people were aware of the octopus, as evidenced by certain artworks and designs. For example, a stone carving found in the archaeological recovery from Bronze Age Minoan Crete at Knossos (1900–1100 BC) has a depiction of a fisherman carrying an octopus. The terrifyingly powerful Gorgon of Greek mythology has been thought to have been inspired by the octopus or squid, the octopus itself representing the severed head of Medusa, the beak as the protruding tongue and fangs, and its tentacles as the snakes. The Kraken are legendary sea monsters of giant proportions said to dwell off the coasts of Norway and Greenland, usually portrayed in art as a giant octopus attacking ships. Linnaeus included it in the first edition of his 1735 "Systema Naturae". A Hawaiian creation myth says that the present cosmos is the last of a series which arose in stages from the ruins of the previous universe. In this account, the octopus is the lone survivor of the previous, alien universe. The Akkorokamui is a gigantic octopus-like monster from Ainu folklore.
A battle with an octopus plays a significant role in Victor Hugo's book "Travailleurs de la mer" ("Toilers of the Sea"), relating to his time in exile on Guernsey.
Ian Fleming's 1966 short story collection "Octopussy and The Living Daylights", and the 1983 "James Bond" film were partly inspired by Hugo's book.

Japanese erotic art, "shunga", includes ukiyo-e woodblock prints such as Katsushika Hokusai's 1814 print "Tako to ama" (The Dream of the Fisherman's Wife), in which an ama diver is sexually intertwined with a large and a small octopus. The print is a forerunner of tentacle erotica. The biologist P. Z. Myers noted in his science blog, Pharyngula, that octopuses appear in "extraordinary" graphic illustrations involving women, tentacles, and bare breasts.

Since it has numerous arms emanating from a common centre, the octopus is often used as a symbol for a powerful and manipulative organisation, usually negatively.

Octopuses generally avoid humans, but incidents have been verified. For example, a Pacific octopus, said to be nearly perfectly camouflaged, "lunged" at a diver and "wrangled" over his camera before it let go. Another diver recorded the encounter on video. 

All species are venomous, but only blue-ringed octopuses have venom that is lethal to humans. Bites are reported each year across the animals' range from Australia to the eastern Indo-Pacific Ocean. They bite only when provoked or accidentally stepped upon; bites are small and usually painless. The venom appears to be able to penetrate the skin without a puncture, given prolonged contact. It contains tetrodotoxin, which causes paralysis by blocking the transmission of nerve impulses to the muscles. This causes death by respiratory failure leading to cerebral anoxia. No antidote is known, but if breathing can be kept going artificially, patients recover within 24 hours. Bites have been recorded from captive octopuses of other species; they leave swellings which disappear in a day or two.

Octopus fisheries exist around the world with total catches varying between 245,320 and 322,999 metric tons from 1986 to 1995. The world catch peaked in 2007 at 380,000 tons, and fell by a tenth by 2012. Methods to capture octopuses include pots, traps, trawls, snares, drift fishing, spearing, hooking and hand collection. Octopus is eaten in many cultures and is a common food on the Mediterranean and Asian coasts. The arms and sometimes other body parts are prepared in various ways, often varying by species or geography. Live octopuses are eaten in several countries around the world, including the US. Animal welfare groups have objected to this practice on the basis that octopuses can experience pain. Octopuses have a food conversion efficiency greater than that of chickens, making octopus aquaculture a possibility.

In classical Greece, Aristotle (384–322 BC) commented on the colour-changing abilities of the octopus, both for camouflage and for signalling, in his "Historia animalium": "The octopus ... seeks its prey by so changing its colour as to render it like the colour of the stones adjacent to it; it does so also when alarmed." Aristotle noted that the octopus had a hectocotyl arm and suggested it might be used in sexual reproduction. This claim was widely disbelieved until the 19th century. It was described in 1829 by the French zoologist Georges Cuvier, who supposed it to be a parasitic worm, naming it as a new species, "Hectocotylus octopodis". Other zoologists thought it a spermatophore; the German zoologist Heinrich Müller believed it was "designed" to detach during copulation. In 1856 the Danish zoologist Japetus Steenstrup demonstrated that it is used to transfer sperm, and only rarely detaches.

Octopuses offer many possibilities in biological research, including their ability to regenerate limbs, change the colour of their skin, behave intelligently with a distributed nervous system, and make use of 168 kinds of protocadherins (humans have 58), the proteins that guide the connections neurons make with each other. The California two-spot octopus has had its genome sequenced, allowing exploration of its molecular adaptations. Having independently evolved mammal-like intelligence, octopuses have been compared to hypothetical intelligent extraterrestrials. Their problem-solving skills, along with their mobility and lack of rigid structure enable them to escape from supposedly secure tanks in laboratories and public aquariums.

Due to their intelligence, octopuses are listed in some countries as experimental animals on which surgery may not be performed without anesthesia, a protection usually extended only to vertebrates. In the UK from 1993 to 2012, the common octopus ("Octopus vulgaris") was the only invertebrate protected under the Animals (Scientific Procedures) Act 1986. In 2012, this legislation was extended to include all cephalopods in accordance with a general EU directive.

Some robotics research is exploring biomimicry of octopus features. Octopus arms can move and sense largely autonomously without intervention from the animal's central nervous system. In 2015 a team in Italy built soft-bodied robots able to crawl and swim, requiring only minimal computation. In 2017 a German company made an arm with a soft pneumatically controlled silicone gripper fitted with two rows of suckers. It is able to grasp objects such as a metal tube, a magazine, or a ball, and to fill a glass by pouring water from a bottle.




</doc>
<doc id="22781" url="https://en.wikipedia.org/wiki?curid=22781" title="Omniscience">
Omniscience

Omniscience () is the capacity to know everything that there is to know. In monotheistic religions, such as Sikhism and the Abrahamic religions, this is an attribute of God. In some other religions that do not include a supreme deity, such as Buddhism and Jainism, omniscience is an attribute that any individual can eventually attain.

The topic of omniscience has been much debated in various Indian traditions, but no more so than by the Buddhists. After Dharmakirti's excursions into the subject of what constitutes a valid cognition, Śāntarakṣita and his student Kamalaśīla thoroughly investigated the subject in the Tattvasamgraha and its commentary the Panjika. The arguments in the text can be broadly grouped into four sections:

Some modern Christian theologians argue that God's omniscience is inherent rather than total, and that God chooses to limit his omniscience in order to preserve the freewill and dignity of his creatures. John Calvin, among other theologians of the 16th century, comfortable with the definition of God as being omniscient in the total sense, in order for worthy beings' abilities to choose freely, embraced the doctrine of predestination.

In Jainism, omniscience is considered the highest type of perception. In the words of a Jain scholar,
"The perfect manifestation of the innate nature of the self, arising on the complete annihilation of the obstructive veils, is called omniscience."

Jainism views infinite knowledge as an inherent capability of every soul. "Arihanta" is the word used by Jains to refer to those human beings who have conquered all inner passions (like attachment, greed, pride, anger) and possess "Kevala Jnana" (infinite knowledge). They are said to be of two kinds:

Whether omniscience, particularly regarding the choices that a human will make, is compatible with free will has been debated by theologians and philosophers. The argument that divine foreknowledge is not compatible with free will is known as theological fatalism. It is argued that if humans are free to choose between alternatives, God could not know what this choice will be.

A question arises: if an omniscient entity knows everything, even about its own decisions in the future, does it therefore forbid any free will to that entity? William Lane Craig states that the question subdivides into two:

However, this kind of argument fails to recognize its use of the modal fallacy. It is possible to show that the first premise of arguments like these is fallacious. 





</doc>
<doc id="22784" url="https://en.wikipedia.org/wiki?curid=22784" title="Original Chip Set">
Original Chip Set

The Original Chip Set (OCS) is a chipset used in the earliest Commodore Amiga computers and defined the Amiga's graphics and sound capabilities. It was succeeded by the slightly improved Enhanced Chip Set (ECS) and greatly improved Advanced Graphics Architecture (AGA).

The original chipset appeared in Amiga models built between 1985 and 1990: the Amiga 1000, Amiga 2000, Amiga CDTV, and Amiga 500.

The chipset which gave the Amiga its unique graphics features consists of three main "custom" chips; "Agnus", "Denise", and "Paula". Both the original chipset and the enhanced chipset were manufactured using NMOS logic technology by Commodore's chip manufacturing subsidiary, MOS Technology. According to Jay Miner, the OCS chipset was fabricated in 5 µm manufacturing process while AGA Lisa was implemented in 1.5 µm process. All three custom chips were originally packaged in 48-pin DIPs; later versions of Agnus, known as Fat Agnus, were packaged in an 84-pin PLCC.

Agnus is the central chip in the design. It controls all access to chip RAM from both the central 68000 processor and the other custom chips, using a complicated priority system. Agnus includes sub-components known as the "blitter" (fast transfer of data in memory without the intervention of the processor) and the "Copper" (video-synchronized co-processor). The original Agnus can address of chip RAM. Later revisions, dubbed 'Fat Agnus', added pseudo-fast RAM, which for ECS was changed to 1 MB (sometimes called 'Fatter Agnus') and subsequently to 2 MB chip RAM.

Denise is the main video processor. Without using overscan, the Amiga's graphics display is 320 or 640 pixels wide by 200 (NTSC) or 256 (PAL) pixels tall. Denise also supports interlacing, which doubles the vertical resolution, at the cost of intrusive flickering on typical monitors of that era. Planar bitmap graphics are used, which splits the individual bits per pixel into separate areas of memory, called bitplanes. In normal operation, Denise allows between one and five bitplanes, giving two to 32 unique colors. These colors are selected from a palette of 4096 colors (four bits per RGB component). A 6th bitplane is available for two special video modes: Halfbrite mode and Hold-And-Modify (HAM) mode. Denise also supports eight sprites, single pixel scrolling, and a "dual-playfield" mode. Denise also handles mouse and digital joystick input.

Paula is primarily the audio chip, with four independent hardware-mixed 8-bit PCM sound channels, each of which supports 65 volume levels (no sound to maximum volume) and waveform output rates from roughly 20 samples per second to almost 29,000 samples per second. Paula also handles interrupts and various I/O functions including the floppy disk drive, the serial port, and analog joysticks.

There are many similarities both in overall functionality and in the division of functionality into the three component chips between the OCS chipset and the much earlier and simpler chipset of the Atari 8-bit family of home computers, consisting of the ANTIC, GTIA and POKEY chips; both chipsets were conceptually designed by Jay Miner, which explains the similarity.

The Agnus chip is in overall control of the entire chipset's operation. All operations are synchronised to the position of the video beam. This includes access to the built-in RAM, known as chip RAM because the chipset has access to it. Both the central 68000 processor and other members of the chipset have to arbitrate for access to chip RAM via "Agnus". In computing architecture terms, this is Direct Memory Access (DMA), where Agnus is the DMA Controller (DMAC).

Agnus has a complex and priority-based memory access policy that attempts to best coordinate requests for memory access among competing resources. For example, bitplane data fetches are prioritized over blitter transfers as the immediate display of frame buffer data is considered more important than the processing of memory by the blitter. Agnus also attempts to order accesses in such a way so as to overlap CPU bus cycles with DMA cycles. As the original 68000 processor in Amigas tended only to access memory on every second available memory cycle, Agnus operates a system where "odd" memory access cycles are allocated first and as needed to time-critical custom chip DMA while any remaining cycles are available to the CPU, thus the CPU does not generally get locked out of memory access and does not appear to slow down. However, non-time-critical custom chip access, such as "blitter" transfers, can use up any spare odd or even cycles and, if the "BLITHOG" (blitter hog) flag is set, Agnus can lock out the even cycles from the CPU in deference to the "blitter".

Agnus's timings are measured in "color clocks" of 280 ns. This is equivalent to two low resolution (140 ns) pixels or four high resolution (70 ns) pixels. Like Denise, these timings were designed for display on household TVs, and can be synchronized to an external clock source.

The "blitter" is a sub-component of Agnus. "Blit" is shorthand for "block image transfer" or bit blit. The blitter is a highly parallel memory transfer and logic operation unit. It has three modes of operation: copying blocks of memory, filling blocks (e.g. polygon filling) and line drawing.

The blitter allows the rapid copying of video memory, meaning that the CPU can be freed for other tasks. The blitter was primarily used for drawing and redrawing graphics images on the screen, called "bobs", short for "blitter objects".

The blitter's block copying mode takes zero to three data sources in memory, called A, B and C, performs a programmable boolean function on the data sources and writes the result to a destination area, D. Any of these four areas can overlap. The blitter runs either from the start of the block to the end, known as "ascending" mode, or in reverse, "descending" mode.

Blocks are "rectangular"; they have a "width" in multiples of 16 bits, a height measured in "lines", and a "stride" distance to move from the end of one line to the next. This allows the blitter to operate on any video resolution up to 1,024×1,024 pixels. The copy automatically performs a per-pixel logical operation. These operations are described generically using minterms. This is most commonly used to do direct copies (D = A), or apply a pixel mask around blitted objects (D = (C AND B) OR A). The copy can also barrel shift each line by 0 to 15 pixels. This allows the blitter to draw at pixel offsets that are not exactly multiples of 16.

These functions allow the Amiga to move GUI windows around the screen rapidly as each is represented in graphical memory space as a rectangular block of memory which may be shifted to any required screen memory location at will.

The blitter's line mode draws single-pixel thick lines using Bresenham's line algorithm. It can also apply a 16-bit repeating pattern to the line. The line mode can also be used to draw rotated bobs: each line of bob data is used as line pattern while the line mode draws the tilted bob line by line.

The blitter's filling mode is used to fill per-line horizontal spans. On each span, it reads each pixel in turn from right to left. Whenever it reads a set pixel, it toggles filling mode on or off. When filling mode is on, it sets every pixel until filling mode is turned off or the line ends. Together, these modes allow the blitter to draw individual flat-shaded polygons. Later Amigas tended to use a combination of a faster CPU and blitter for many operations.

The "Copper" is another sub-component of Agnus; The name is short for "co-processor". The Copper is a programmable finite state machine that executes a programmed instruction stream, synchronized with the video hardware.

When it is turned on, the Copper has three states; either reading an instruction, executing it, or waiting for a specific video beam position. The Copper runs a program called the "Copper list" in parallel with the main CPU. The Copper runs in sync with the video beam, and it can be used to perform various operations which require video synchronization. Most commonly it is used to control video output, but it can write to most of the chipset registers and thus can be used to initiate blits, set audio registers, or interrupt the CPU.

The Copper list has three kinds of instructions, each one being a pair of two bytes, four bytes in total:


The length of the Copper list program is limited by execution time. The Copper restarts executing the Copper list at the start of each new video frame. There is no explicit "end" instruction; instead, the WAIT instruction is used to wait for a location which is never reached.


Under normal circumstances, the Amiga generates its own video timings, but Agnus also supports synchronising the system to an external signal so as to achieve genlocking with external video hardware. There is also a 1-bit output on this connector that indicates whether the Amiga is outputting background color or not, permitting easy overlaying of Amiga video onto external video. This made the Amiga particularly attractive as a character generator for titling videos and broadcast work, as it avoided the use and expense of AB roll and chromakey units that would be required without the genlock support. The support of overscan, interlacing and genlocking capabilities, and the fact that the display timing was very close to broadcast standards (NTSC or PAL), made the Amiga the first ideal computer for video purposes, and indeed, it was used in many studios for digitizing video data (sometimes called frame-grabbing), subtitling and interactive video news.

Denise is programmed to fetch planar video data from one to five bitplanes and translate that into a color lookup. The number of bitplanes is arbitrary, thus if 32 colors are not needed, 2, 4, 8 or 16 can be used instead. The number of bitplanes (and resolution) can be changed on the fly, usually by the Copper. This allows for very economical use of RAM, and balancing of CPU processing speed vs graphical sophistication when executing from Chip RAM (as modes beyond 4bpp in lorez, or 2bpp in hires, use extra DMA channels that can slow or temporarily halt the CPU in addition to the usual non-conflicting channels). There can also be a sixth bitplane, which can be used in three special graphics modes:

In Extra-HalfBrite (EHB), if a pixel is set on the sixth bitplane, the brightness of the regular 32 color pixel is halved. Early versions of the Amiga 1000 sold in the United States did not have the Extra-HalfBrite mode.

In Hold-and-Modify (HAM) mode, each 6-bit pixel is interpreted as two control bits and four data bits. The four possible permutations of control bits are "set", "modify red", "modify green" and "modify blue". With "set", the four data bits act like a regular 16-color display look up. With one of the "modify"s, the red, green or blue component of the previous pixel is modified to the data value, and the other two components are held from the previous pixel. This allows all 4096 colors on screen at once and is an example of lossy image compression in hardware.

In dual-playfield mode, instead of acting as a single screen, two "playfields" of eight colors each (three bitplanes each) are drawn on top of each other. They are independently scrollable and the background color of the top playfield "shines through" to the underlying playfield.

There are two horizontal graphics resolutions, "lowres" with 140 ns pixels and "hires" with 70 ns pixels, with a default of 320 or 640 horizontal pixels wide without using overscan. As the pixel output is regulated by the main system clock, which is based directly on the NTSC colorburst clock, these sizes very nearly fill the width of a standard television with only a thin "underscan" border between the graphics and the screen border when compared to many other contemporary home computers, for an appearance closer to a games console but with finer detail. On top of this, Denise supports reasonably extensive overscan; technically modes with enough data for up to 400 or 800 pixels (+25%) may be specified, although this is only actually useful for scrolling and special effects that involve partial display of large graphics, as a separate hardware limit is met at 368 (or 736) pixels, which is the maximum that will fit between the end of one blanking period and the start of the next - although it is unlikely that even this many pixels will be visible on any display other than a dedicated monitor that allows adjustment of horizontal scan width, as much of the image will, by design, disappear seamlessly behind the screen bezel (or, on LCDs, be cropped off at the edge of the panel). Because of the highly regular structure of the Amiga's timing in relation to scanlines and allocation of DMA resources to various uses besides normal "playfield" graphics, increased horizontal resolution is also a tradeoff between number of pixels and how many hardware sprites are available, as increasing the DMA slots dedicated to playfield video ends up stealing some (from 1 to 7 of the total 8) the sprite engine.. Vertical resolution, without overscan, is 200 pixels for a 60 Hz NTSC Amiga or 256 for a 50 Hz PAL Amiga. This can be doubled using an interlaced display, and, as with horizontal resolution, increased using overscan, to a maximum of 241 (or 483) for NTSC, and 283 (567) for PAL (interlaced modes gaining one extra line as the maximum is determined by how many lines are taken from the available total by blanking and sync, and the total scanlines in non-interlaced modes are half the original, broadcast-spec odd-numbered interlaced counts, rounded down).

Denise can composite up to eight 16 pixel wide sprites per scan line (in automatic mode) on top, underneath, or between playfields, and detect collisions between sprites and the playfields or between sprites. These sprites have three visible colors and one transparent color. Optionally, adjacent pairs of sprites can be "attached" to make a single 15 color sprite. Using Copper or CPU register manipulations, each sprite 'channel' can be reused multiple times in a single frame to increase the total sprites per frame. Sprite "position" registers may also be changed during a scanline, increasing the total number of sprites on a single scanline. However, the sprite "data", or shape, is only fetched a single time per scanline and can't change. The first Amiga game to utilize the sprite re-position registers during a scanline was Hybris released in 1988.

Finally, Denise is responsible for handling mouse/joystick X/Y inputs.

The Paula chip, from MOS Technology, includes logic for audio playback, floppy disk drive control, serial port input/output and mouse/joystick buttons two and three signals. The logic remained functionally identical across all Amiga models from Commodore.

Paula has four DMA-driven 8-bit PCM sample sound channels. Two sound channels are mixed into the left audio output, and the other two are mixed into the right output, producing stereo audio output. The only supported hardware sample format is signed linear 8-bit two's complement. Each sound channel has an independent frequency and a 6-bit volume control (64 levels). Internally, the audio hardware is implemented by four state machines, each having eight different states.

Additionally the hardware allows one channel in a channel pair to modulate the other channel's period or amplitude. It is rarely used on the Amiga due to both frequency and volume being controllable in better ways, but could be used to achieve different kinds of tremolo and vibrato, and even rudimentary FM synthesis effects.

Audio may be output using two methods. Most often, DMA-driven audio is used. As explained in the discussion of Agnus, memory access is prioritized and one DMA slot per scan line is available for each of the four sound channels. On a regular NTSC or PAL display, DMA audio playback is limited to a maximum output rate of 28867 values per channel (PAL: 28837) per second totaling 57674 (PAL: 57734) values per second on each stereo output. This rate can be increased with the ECS and AGA chipsets by using a video mode with higher horizontal scan rate.

Alternately, Paula may signal the CPU to load a new sample into any of the four audio output buffers by generating an interrupt when a new sample is needed. This allows for output rates that exceed 57 kHz per channel and increases the number of possible voices (simultaneous sounds) through software mixing.

The Amiga contains an analog low-pass filter (reconstruction filter) which is external to Paula. The filter is a 12 dB/oct Butterworth low-pass filter at approximately 3.3 kHz. The filter can only be applied globally to all four channels. In models after the Amiga 1000 (excluding the very first revision of the Amiga 500), the brightness of the power LED is used to indicate the status of the filter. The filter is active when the LED is at normal brightness, and deactivated when dimmed (on early Amiga 500 models the LED went completely off). Models released before Amiga 1200 also have a static "tone knob" type low-pass filter that is enabled regardless of the optional "LED filter". This filter is a 6 dB/oct low-pass filter with cutoff frequency at 4.5 or 5 kHz.

A software technique was later developed which can play back 14-bit audio by combining two channels set at different volumes. This results in two 14-bit channels instead of four 8-bit channels. This is achieved by playing the high byte of a 16-bit sample at maximum volume, and the low byte at minimum volume (both ranges overlap, so the low byte needs to be shifted right two bits). The bit shift operation requires a small amount of CPU or blitter overhead, whereas conventional 8-bit playback is almost entirely DMA driven. This technique was incorporated into the retargetable audio subsystem AHI, allowing compatible applications to use this mode transparently.

The floppy controller is unusually flexible. It can read and write raw bit sequences directly from and to the disk via DMA or programmed I/O at 500 (double density) or 250 kbit/s (single density or GCR). MFM or GCR were the two most commonly used formats though in theory any run-length limited code could be used. It also provides a number of convenient features, such as sync-on-word (in MFM coding, $4489 is usually used as the sync word). MFM encoding/decoding is usually done with the blitter — one pass for decode, three passes for encode. Normally the entire track is read or written in one shot, rather than sector-by-sector; this made it possible to get rid of most of the inter-sector gaps that most floppy disk formats need to safely prevent the "bleeding" of a written sector into the previously-existing header of the next sector due to speed variations of the drive. If all sectors and their headers are always written in one go, such bleeding is only an issue at the end of the track (which still must not bleed back into its beginning), so that only one gap per track is needed. This way, for the native Amiga disk format, the raw storage capacity of 3.5 inch DD disks was increased from the typical 720 KB to 880 KB, although the less-than-ideal file system of the earlier Amiga models reduced this again to approximately 830 KB of actual payload data.

In addition to the native 880 KB 3.5-inch disk format, the controller can handle many foreign formats, such as:


The Amiga 3000 introduced a special, dual-speed floppy drive that also allowed to use high density disks with double capacity without any change to Paula's floppy controller.

The serial port is rudimentary, using programmed input/output only and lacking a FIFO buffer. However, virtually any bit rate can be selected, including all standard rates, MIDI rate, as well as extremely high custom rates.





</doc>
<doc id="22786" url="https://en.wikipedia.org/wiki?curid=22786" title="Optic neuritis">
Optic neuritis

Optic neuritis is a demyelinating inflammation of the optic nerve. It is also known as optic papillitis (when the head of the optic nerve is involved) and retrobulbar neuritis (when the posterior part of the nerve is involved). It is most often associated with multiple sclerosis, and it may lead to complete or partial loss of vision in one or both eyes.

Partial, transient vision loss (lasting less than one hour) can be an indication of early onset multiple sclerosis. Other possible diagnoses include: diabetes mellitus, low phosphorus levels, or hyperkalaemia.

Major symptoms are sudden loss of vision (partial or complete), sudden blurred or "foggy" vision, and pain on movement of the affected eye. Early symptoms that require investigation include symptoms from multiple sclerosis (twitching, lack of coordination, slurred speech, frequent episodes of partial vision loss or blurred vision), episodes of "disturbed/blackened" rather than blurry indicate moderate stage and require immediate medical attention to prevent further loss of vision. Other early symptoms are reduced night vision, photophobia and red eyes. Many patients with optic neuritis may lose some of their color vision in the affected eye (especially red), with colors appearing subtly washed out compared to the other eye. Patients may also experience difficulties judging movement in depth which can be particular troublesome during driving or sport (Pulfrich effect). Likewise transient worsening of vision with increase of body temperature (Uhthoff's phenomenon) and glare disability are a frequent complaint. However, several case studies in children have demonstrated the absence of pain in more than half of cases (approximately 60%) in their pediatric study population, with the most common symptom reported simply as "blurriness." Other remarkable differences between the presentation of adult optic neuritis as compared to pediatric cases include more often unilateral optic neuritis in adults, while children much predominantly present with bilateral involvement.

On medical examination the head of the optic nerve can easily be visualized by a slit lamp with a high positive lens or by using direct ophthalmoscopy; however, frequently there is no abnormal appearance of the nerve head in optic neuritis (in cases of retrobulbar optic neuritis), though it may be swollen in some patients (anterior papillitis or more extensive optic neuritis). In many cases, only one eye is affected and patients may not be aware of the loss of color vision until they are asked to close or cover the healthy eye.

The optic nerve comprises axons that emerge from the retina of the eye and carry visual information to the primary visual nuclei, most of which is relayed to the occipital cortex of the brain to be processed into vision. Inflammation of the optic nerve causes loss of vision, usually because of the swelling and destruction of the myelin sheath covering the optic nerve.

The most common cause is multiple sclerosis or ischemic optic neuropathy (Blood Clot). Blood Clot that supplies the optic nerve. Up to 50% of patients with MS will develop an episode of optic neuritis, and 20-30% of the time optic neuritis is the presenting sign of MS. The presence of demyelinating white matter lesions on brain MRI at the time of presentation of optic neuritis is the strongest predictor for developing clinically definite MS. Almost half of the patients with optic neuritis have white matter lesions consistent with multiple sclerosis.

Some other common causes of optic neuritis include infection (e.g. Tooth Abscess in upper jaw, syphilis, Lyme disease, herpes zoster), autoimmune disorders (e.g. lupus, neurosarcoidosis, neuromyelitis optica), Pinch in Optic Nerve, Methanol poisoning, B12 deficiency and diabetes . Injury to the eye, which usually does not heal by itself.

Less common causes are: papilledema, brain tumor or abscess in occipitalregion, Cerebral trauma or hemorrhage, Meningitis Arachnoidal adhesions, sinus thrombosis, Liver Dysfunction or, Late Stage Kidney.

The repetition of an idiopathic optic neuritis is considered a distinct clinical condition, and when it shows demyelination, it has been found to be associated to anti-MOG and AQP4-negative neuromyelitis optica

When an inflammatory recurrent optic neuritis is not demyelinating, it is called "Chronic relapsing inflammatory optic neuropathy" (CRION)

When it is anti-MOG related, it is demyelinating and it is considered inside the anti-MOG associated inflammatory demyelinating diseases.

In most MS-associated optic neuritis, visual function spontaneously improves over 2–3 months, and there is evidence that corticosteroid treatment does not affect the long term outcome. However, for optic neuritis that is not MS-associated (or atypical optic neuritis) the evidence is less clear and therefore the threshold for treatment with intravenous corticosteroids is lower. Intravenous corticosteroids also reduce the risk of developing MS in the following two years in patients with MRI lesions; but this effect disappears by the third year of follow up.

Paradoxically, oral administration of corticosteroids in this situation may lead to more recurrent attacks than in non-treated patients (though oral steroids are generally prescribed after the intravenous course, to wean the patient off the medication). This effect of corticosteroids seems to be limited to optic neuritis and has not been observed in other diseases treated with corticosteroids.

A Cochrane Systematic Review studied the effect of corticosteroids for treating people with acute optic neuritis. Specific corticosteroids studied included intravenous and oral methylprednisone, and oral prednisone. The authors conclude that current evidence does not show a benefit of either intravenous or oral corticosteroids for rate of recovery of vision (in terms of visual acuity, contrast sensitivity, or visual fields)..

Optic neuritis typically affects young adults ranging from 18–45 years of age, with a mean age of 30–35 years. There is a strong female predominance. The annual incidence is approximately 5/100,000, with a prevalence estimated to be 115/100,000.

In the season five episode of Dr. Quinn, Medicine Woman, "Season of Miracles", Reverend Timothy Johnson is struck blind by optic neuritis on Christmas Day, 1872. He remains blind for the duration of the series.
In Charles Dickens' "Bleak House" the main character, Esther Summerville suffers from a transient episode of visual loss with symptoms also observed during the course of optic neuritis. Sir William Searle Holdsworth suggested "Bleak House" to have taken place in 1827.



</doc>
<doc id="22787" url="https://en.wikipedia.org/wiki?curid=22787" title="List of organizations with .int domain names">
List of organizations with .int domain names

This is a list of organizations with INT domain names, in alphabetical order of the second-level domain name. The list is not comprehensive. As of June 2012, the INT domain consists of 166 subdomain delegations.

These organizations are generally international organizations established by treaty. Some however (such as YMCA) do not meet current restrictions and were grandfathered in from prior acceptance.


<nowiki>*</nowiki> Bodies which are not international treaty organizations or (arguably) related to Internet infrastructure, and so do not meet current .INT requirements.

<nowiki>**</nowiki> Domains connected with Internet infrastructure; these are not international intergovernmental organizations.

<nowiki>***</nowiki> Domains connected with treaties but which may not have the full status of intergovernmental organization (sites for treaties or treaty secretariats).


</doc>
