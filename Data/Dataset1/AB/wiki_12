<doc id="14117" url="https://en.wikipedia.org/wiki?curid=14117" title="History of Christianity">
History of Christianity

The history of Christianity concerns the Christian religion, Christendom, and the Church with its various denominations, from the 1st century to the present.

Roman Catholic and Eastern Orthodox Christianity spread to all of Europe in the Middle Ages. Christianity expanded throughout the world and became the world's largest religion. Today there are more than two billion Christians worldwide.

During its early history, Christianity grew from a 1st-century Jewish following to a religion that existed across the entire Greco-Roman world and beyond.

Early Christianity may be divided into 2 distinct phases: the apostolic period, when the first apostles were alive and led the Church, and the post-apostolic period, when an early episcopal structure developed, and persecution was periodically intense. The Roman persecution of Christians ended in AD 324 when Constantine the Great decreed tolerance for the religion. He then called the First Council of Nicaea in AD 325, beginning the period of the First seven Ecumenical Councils.

The Apostolic Church was the community led by the apostles, and to some degree, Jesus's relatives. In his "Great Commission", the resurrected Jesus commanded that his teachings be spread to all the world. While the historical reliability of the Acts of the Apostles is disputed by critics, the "Acts of the Apostles" is the major primary source of information for this period. Acts gives a history of the Church from this commission in to the spread of the religion among the Gentiles and the eastern Mediterranean by Paul and others.

The first Christians were essentially all ethnically Jewish or Jewish proselytes. In other words, Jesus preached to the Jewish people and called from them his first disciples, see for example Matthew 10. However, the Great Commission is specifically directed at "all nations", and an early difficulty arose concerning the matter of Gentile (non-Jewish) converts as to whether they had to "become Jewish" (usually referring to circumcision and adherence to dietary law), as part of becoming Christian. Circumcision in particular was considered repulsive by Greeks and Hellenists while circumcision advocates were labelled Judaisers, see Jewish background to the circumcision controversy for details. The actions of Peter, at the conversion of Cornelius the Centurion, seemed to indicate that circumcision and food laws did not apply to Gentiles, and this was agreed to at the apostolic Council of Jerusalem. Related issues are still debated today.

The doctrines of the apostles brought the Early Church into conflict with some Jewish religious authorities. This eventually led to their expulsion from the synagogues, according to one theory of the Council of Jamnia. Acts records the martyrdom of the Christian leaders, Stephen and James of Zebedee. Thus, Christianity acquired an identity distinct from Rabbinic Judaism, but this distinction was not recognised all at once by the Roman Empire, see Split of early Christianity and Judaism for details. The name "Christian" (Greek ) was first applied to the disciples in Antioch, as recorded in Acts 11:26. Some contend that the term "Christian" was first coined as a derogatory term, meaning "little Christs", and was meant as a mockery, a term of derision for those that followed the teachings of Jesus.

The sources for the beliefs of the apostolic community include the Gospels and New Testament epistles. The very earliest accounts of belief are contained in these texts, such as early creeds and hymns, as well as accounts of the Passion, the empty tomb, and Resurrection appearances; some of these are dated to the 30s or 40s AD, originating within the Jerusalem Church. According to a tradition recorded by Eusebius and Epiphanius, the Jerusalem church fled to Pella at the outbreak of the First Jewish–Roman War (66–73 AD).

The post-apostolic period concerns the time after the death of the apostles (roughly 100 AD) until persecutions ended with the legalisation of Christian worship under Emperors Constantine the Great and Licinius.

According to the New Testament, Christians were subject to various persecutions from the beginning. This involved even death for Christians such as Stephen () and James, son of Zebedee (). Larger-scale persecutions followed at the hands of the authorities of the Roman Empire, beginning with the year 64, when, as reported by the Roman historian Tacitus, the Emperor Nero blamed them for that year's Great Fire of Rome.

According to Church tradition, it was under Nero's persecution that Peter and Paul were each martyred in Rome. Similarly, several of the New Testament writings mention persecutions and stress endurance through them.

Early Christians suffered sporadic persecutions as the result of local pagan populations putting pressure on the imperial authorities to take action against the Christians in their midst, who were thought to bring misfortune by their refusal to honour the gods.
The last and most severe persecution organised by the imperial authorities was the Diocletianic Persecution, 303–311.

In spite of these sometimes intense persecutions, the Christian religion continued its spread throughout the Mediterranean Basin. There is no agreement on how Christianity managed to spread so successfully prior to the Edict of Milan and the establishment of Christianity as the state religion of the Roman Empire. In "The Rise of Christianity", Rodney Stark argues that Christianity triumphed over paganism chiefly because it improved the lives of its adherents in various ways.

Another factor was the way in which Christianity combined its promise of a general resurrection of the dead with the traditional Greek belief that true immortality depended on the survival of the body, with Christianity adding practical explanations of how this was going to actually happen at the end of the world. For Mosheim, the rapid progression of Christianity was explained by two factors: translations of the New Testament and the Apologies composed in defence of Christianity.

Edward Gibbon in his "The History of the Decline and Fall of the Roman Empire" discusses the topic in considerable detail in his famous Chapter Fifteen, summarizing the historical causes of the early success of Christianity as follows: "(1) The inflexible, and, if we may use the expression, the intolerant zeal of the Christians, derived, it is true, from the Jewish religion, but purified from the narrow and unsocial spirit which, instead of inviting, had deterred the Gentiles from embracing the law of Moses. (2) The doctrine of a future life, improved by every additional circumstance which could give weight and efficacy to that important truth. (3) The miraculous powers ascribed to the primitive church. (4) The pure and austere morals of the Christians. (5) The union and discipline of the Christian republic, which gradually formed an independent and increasing state in the heart of the Roman empire."

In the post-Apostolic church, bishops emerged as overseers of urban Christian populations, and a hierarchy of clergy gradually took on the form of "episkopos" (overseers, in-spectors; and the origin of the term bishop) and "presbyters" (elders; and the origin of the term priest), and then "deacons" (servants). But this emerged slowly and at different times for different locations. Clement, a 1st-century bishop of Rome, refers to the leaders of the Corinthian church in his epistle to Corinthians as bishops and presbyters interchangeably. The New Testament writers also use the terms overseer and elders interchangeably and as synonyms.

Post-apostolic bishops of importance include Polycarp of Smyrna, Clement of Rome, and Ignatius of Antioch. These men reportedly knew and studied under the apostles personally and are therefore called Apostolic Fathers. Each Christian community also had presbyters, as was the case with Jewish communities, who were also ordained and assisted the bishop. As Christianity spread, especially in rural areas, the presbyters exercised more responsibilities and took distinctive shape as priests. Lastly, deacons also performed certain duties, such as tending to the poor and sick. In the 2nd century, an episcopal structure becomes more visible, and in that century this structure was supported by teaching on apostolic succession, where a bishop becomes the spiritual successor of the previous bishop in a line tracing back to the apostles themselves.

The diversity of early Christianity can be documented from the New Testament record itself. The Book of Acts admits conflicts between Hebrews and Hellenists, and Jewish Christians and Gentile Christians, and Aramaic speakers and Greek speakers. The letters of Paul, Peter, John, and Jude all testify to intra-Church conflicts over both leadership and theology. In a response to the Gnostic teaching, Irenaeus created the first document describing what is now called apostolic succession.

As Christianity spread, it acquired certain members from well-educated circles of the Hellenistic world; they sometimes became bishops, but not always. They produced two sorts of works: theological and "apologetic", the latter being works aimed at defending the faith by using reason to refute arguments against the veracity of Christianity. These authors are known as the Church Fathers, and study of them is called patristics. Notable early Fathers include Ignatius of Antioch, Polycarp, Justin Martyr, Irenaeus of Lyons, Tertullian, Clement of Alexandria, and Origen of Alexandria.

Christian art only emerged relatively late, and the first known Christian images emerge from about 200 AD, though there is some literary evidence that small domestic images were used earlier. The oldest known Christian paintings are from the Roman Catacombs, dated to about AD 200, and the oldest Christian sculptures are from sarcophagi, dating to the beginning of the 3rd century.

Although many Hellenised Jews seem, as at the Dura-Europos synagogue, to have had images of religious figures, the traditional Mosaic prohibition of "graven images" no doubt retained some effect, although never proclaimed by theologians. This early rejection of images, and the necessity to hide Christian practise from persecution, leaves us with few archaeological records regarding early Christianity and its evolution.

The New Testament itself speaks of the importance of maintaining correct (orthodox) doctrine and refuting heresies, showing the antiquity of the concern. Because of the biblical proscription against false prophets, Christianity has always been occupied with the orthodox interpretation of the faith. Indeed, one of the main roles of the bishops in the early Church was to determine and retain important correct beliefs, and refute contrarian opinions, known as heresies. As there were sometimes differing opinions among the bishops on new questions, defining orthodoxy would occupy the Church for some time.

The earliest controversies were often Christological in nature; that is, they were related to Jesus's divinity or humanity. Docetism held that Jesus's humanity was merely an illusion, thus denying the Incarnation (God becoming human). Arianism held that Jesus, while not merely mortal, was not eternally divine and was, therefore, of lesser status than the Father. Trinitarianism held that the Father, Son, and the Holy Spirit were all strictly one being with three hypostases or persons. Many groups held dualistic beliefs, maintaining that reality was composed into two radically opposing parts: matter, seen as evil, and spirit, seen as good. Such views gave rise to some theology of the "incarnation" that were declared heresies. Most scholars agree that the Bible teaches that both the material and the spiritual worlds were created by God and were therefore both good.

The development of doctrine, the position of orthodoxy, and the relationship between the various opinions is a matter of continuing academic debate. Since most Christians today subscribe to the doctrines established by the Nicene Creed, modern Christian theologians tend to regard the early debates as a unified orthodox position against a minority of heretics. Other scholars, drawing upon distinctions between Jewish Christians, Pauline Christianity, and other groups such as Marcionites, argue that early Christianity was always fragmented, with contemporaneous competing beliefs.

The Biblical canon is the set of books Christians regard as divinely inspired and thus constituting the Christian Bible. Though the Early Church used the Old Testament according to the canon of the Septuagint (LXX), the apostles did not otherwise leave a defined set of new scriptures; instead the New Testament developed over time.

The writings attributed to the apostles circulated amongst the earliest Christian communities. The Pauline epistles were circulating in collected form by the end of the 1st century AD. Justin Martyr, in the early 2nd century, mentions the "memoirs of the apostles", which Christians called "gospels" and which were regarded as on par with the Old Testament, which was written in narrative form where "in the biblical story God is the protagonist, Satan (or evil people/powers) are the antagonists, and God's people are the agonists".

A four gospel canon (the "Tetramorph") was in place by the time of Irenaeus, c. 160, who refers to it directly. By the early 3rd century, Origen of Alexandria may have been using the same 27 books as in the modern New Testament, though there were still disputes over the canonicity of Hebrews, James, II Peter, II and III John, and Revelation Such works that were sometimes "spoken against" were called "Antilegomena". In contrast, the major writings and most of what is now the New Testament were "Homologoumena", or universally acknowledged for a long time, since the middle of the 2nd century or before. Likewise the Muratorian fragment shows that by 200 there existed a set of Christian writings similar to the current New Testament.

In his Easter letter of 367, Athanasius, Bishop of Alexandria, gave the earliest preserved list of exactly the books that would become the New Testament canon. The African Synod of Hippo, in 393, approved the New Testament, as it stands today, together with the Septuagint books, a decision that was repeated by the Council of Carthage (397) and the Council of Carthage (419). These councils were under the authority of St. Augustine, who regarded the canon as already closed. Likewise, Damasus' commissioning of the Latin Vulgate edition of the Bible, c. 383, was instrumental in the fixation of the canon in the West. In 405, Pope Innocent I sent a list of the sacred books to Exuperius, a Gallic bishop.

When these bishops and councils spoke on the matter, however, they were not defining something new, but instead "were ratifying what had already become the mind of the Church." Thus, by the 4th century, there existed unanimity in the West concerning the New Testament canon, and by the 5th century the East, with a few exceptions, had come to accept the Book of Revelation and thus had come into harmony on the matter of the canon. Nonetheless, a full dogmatic articulation of the canon was not made until the 1546 Council of Trent for Roman Catholicism, the 1563 Thirty-Nine Articles for the Church of England, the 1647 Westminster Confession of Faith for Calvinism, and the 1672 Synod of Jerusalem for Greek Orthodoxy.

Galerius, who had previously been one of the leading figures in persecution, in 311 issued an edict which ended the Diocletian persecution of Christianity. After halting the persecutions of the Christians, Galerius reigned for another 2 years. He was then succeeded by an emperor with distinctively "pro" Christian leanings, Constantine the Great.

The Emperor Constantine I was exposed to Christianity by his mother, Helena. At the Battle of Milvian Bridge in 312, Constantine commanded his troops to adorn their shields with the Christian symbol in accordance with a vision that he had had the night before. After winning the battle, Constantine was able to claim the emperorship in the West. In 313, he issued the Edict of Milan, officially legalizing Christian worship.

How much Christianity Constantine adopted at this point is difficult to discern. The Roman coins minted up to eight years subsequent to the battle still bore the images of Roman gods. Nonetheless, the accession of Constantine was a turning point for the Christian Church. After his victory, Constantine supported the Church financially, built various basilicas, granted privileges (e.g., exemption from certain taxes) to clergy, promoted Christians to some high-ranking offices, and returned property confiscated during the Great Persecution of Diocletian.

Between 324 and 330, Constantine built, virtually from scratch, a new imperial capital that came to be named for him: Constantinople. It had overtly Christian architecture, contained churches within the city walls, and had no pagan temples. In accordance with a prevailing custom, Constantine was baptised on his deathbed.
Constantine also played an active role in the leadership of the Church. In 316, he acted as a judge in a North African dispute concerning the Donatist controversy. More significantly, in 325 he summoned the Council of Nicaea, the first Ecumenical Council. Constantine thus established a precedent for the emperor as responsible to God for the spiritual health of their subjects, and thus with a duty to maintain orthodoxy. The emperor was to enforce doctrine, root out heresy, and uphold ecclesiastical unity.

Constantine's son's successor, known as Julian the Apostate, was a philosopher who upon becoming emperor renounced Christianity and embraced a Neo-platonic and mystical form of paganism shocking the Christian establishment. He began reopening pagan temples and, intent on re-establishing the prestige of the old pagan beliefs, he modified them to resemble Christian traditions such as the episcopal structure and public charity (previously unknown in Roman paganism). Julian's short reign ended when he died while campaigning in the East.

Later Church Fathers wrote volumes of theological texts, including Augustine, Gregory Nazianzus, Cyril of Jerusalem, Ambrose of Milan, Jerome, and others. Some of these fathers, such as John Chrysostom and Athanasius, suffered exile, persecution, or martyrdom from Arian Byzantine Emperors. Many of their writings are translated into English in the compilations of Nicene and Post-Nicene Fathers.

A popular doctrine of the 4th century was Arianism, the denial of the divinity of Christ, as propounded by Arius. Though this doctrine was condemned as heresy and eventually eliminated by the Roman Church it remained popular underground for some time. In the late 4th century Ulfilas, a Roman bishop and an Arian, was appointed as the first bishop to the Goths, the Germanic peoples in much of Europe at the borders of and within the Empire. Ulfilas spread Arian Christianity among the Goths firmly establishing the faith among many of the Germanic tribes, thus helping to keep them culturally distinct.

During this age, the first Ecumenical Councils were convened. They were mostly concerned with Christological disputes. The First Council of Nicaea (325) and the First Council of Constantinople (381) resulted in condemning Arian teachings as heresy and producing the Nicene Creed.

On 27 February 380, with the Edict of Thessalonica put forth under Theodosius I, the Roman Empire officially adopted Trinitarian Christianity as its state religion. Prior to this date, Constantius II (337-361) and Valens (364–378) had personally favoured Arian or Semi-Arian forms of Christianity, but Valens' successor Theodosius I supported the Trinitarian doctrine as expounded in the Nicene Creed.

After its establishment, the Church adopted the same organisational boundaries as the Empire: geographical provinces, called dioceses, corresponding to imperial governmental territorial division. The bishops, who were located in major urban centres as per pre-legalisation tradition, thus oversaw each diocese. The bishop's location was his "seat", or "see". Among the sees, five came to hold special eminence: Rome, Constantinople, Jerusalem, Antioch, and Alexandria. The prestige of most of these sees depended in part on their apostolic founders, from whom the bishops were therefore the spiritual successors. Though the bishop of Rome was still held to be the First among equals, Constantinople was second in precedence as the new capital of the empire.

Theodosius I decreed that others not believing in the preserved "faithful tradition", such as the Trinity, were to be considered to be practitioners of illegal heresy, and in 385, this resulted in the first case of capital punishment of a heretic, namely Priscillian.

During the early 5th century the School of Edessa had taught a Christological perspective stating that Christ's divine and human nature were distinct persons. A particular consequence of this perspective was that Mary could not be properly called the mother of God, but could only be considered the mother of Christ. The most widely known proponent of this viewpoint was the Patriarch of Constantinople Nestorius. Since referring to Mary as the mother of God had become popular in many parts of the Church this became a divisive issue.

The Roman Emperor Theodosius II called for the Council of Ephesus (431), with the intention of settling the issue. The councils ultimately rejected Nestorius' view. Many churches who followed the Nestorian viewpoint broke away from the Roman Church, causing a major schism. The Nestorian churches were persecuted and many followers fled to the Sasanian Empire where they were accepted.

The Sasanian (Persian) Empire had many Christian converts early in its history tied closely to the Syriac branch of Christianity. The Empire was officially Zoroastrian and maintained a strict adherence to this faith in part to distinguish itself from the religion of the Roman Empire (originally the pagan Roman religion and then Christianity). Christianity became tolerated in the Sasanian Empire and as the Roman Empire increasingly exiled "heretics" during the 4th and 6th centuries, the Sasanian Christian community grew rapidly. By the end of the 5th century the Persian Church was firmly established and had become independent of the Roman Church. This church evolved into what is today known as the Church of the East.

In 451 the Council of Chalcedon was held to further clarify the Christological issues surrounding Nestorianism. The council ultimately stated that Christ's divine and human nature were separate but both part of a single entity, a viewpoint rejected by many churches who called themselves miaphysites. The resulting schism created a communion of churches, including the Armenian, Syrian, and Egyptian churches. Though efforts were made at reconciliation in the next few centuries the schism remained permanent resulting in what is today known as Oriental Orthodoxy.

Monasticism is a form of asceticism whereby one renounces worldly pursuits and goes off alone as a hermit or joins a tightly organized community. It began early in the Church as a family of similar traditions, modelled upon Scriptural examples and ideals, and with roots in certain strands of Judaism. John the Baptist is seen as an archetypical monk, and monasticism was also inspired by the organisation of the Apostolic community as recorded in Acts 2.

Eremetic monks, or hermits, live in solitude, whereas cenobitics live in communities, generally in a monastery, under a rule (or code of practice) and are governed by an abbot. Originally, all Christian monks were hermits, following the example of Anthony the Great. However, the need for some form of organised spiritual guidance lead Pachomius in 318 to organise his many followers in what was to become the first monastery. Soon, similar institutions were established throughout the Egyptian desert as well as the rest of the eastern half of the Roman Empire. Women were especially attracted to the movement.

Central figures in the development of monasticism were Basil the Great in the East and, in the West, Benedict, who created the famous Rule of Saint Benedict, which would become the most common rule throughout the Middle Ages, and starting point for other monastic rules.

The transition into the Middle Ages was a gradual and localised process. Rural areas rose as power centres whilst urban areas declined. Although a greater number of Christians remained in the East (Greek areas), important developments were underway in the West (Latin areas) and each took on distinctive shapes.

The Bishops of Rome, the Popes, were forced to adapt to drastically changing circumstances. Maintaining only nominal allegiance to the Emperor, they were forced to negotiate balances with the "barbarian rulers" of the former Roman provinces. In the East the Church maintained its structure and character and evolved more slowly.

The stepwise loss of Western Roman Empire dominance, replaced with foederati and Germanic kingdoms, coincided with early missionary efforts into areas not controlled by the collapsing empire. Already as early as in the 5th century, missionary activities from Roman Britain into the Celtic areas (current Scotland, Ireland and Wales) produced competing early traditions of Celtic Christianity, that was later reintegrated under the Church in Rome.

Prominent missionaries were Saints Patrick, Columba and Columbanus. The Anglo-Saxon tribes that invaded southern Britain some time after the Roman abandonment, were initially pagan, but converted to Christianity by Augustine of Canterbury on the mission of Pope Gregory the Great. Soon becoming a missionary centre, missionaries such as Wilfrid, Willibrord, Lullus and Boniface would begin converting their Saxon relatives in Germania.

The largely Christian Gallo-Roman inhabitants of Gaul (modern France) were overrun by the Franks in the early 5th century. The native inhabitants were persecuted until the Frankish king Clovis I converted from paganism to Roman Catholicism in 496. Clovis insisted that his fellow nobles follow suit, strengthening his newly established kingdom by uniting the faith of the rulers with that of the ruled.

After the rise of the Frankish Kingdom and the stabilizing political conditions, the Western part of the Church increased the missionary activities, supported by the Merovingian kingdom as a means to pacify troublesome neighbour peoples. After the foundation of a church in Utrecht by Willibrord, backlashes occurred when the pagan Frisian king Radbod destroyed many Christian centres between 716 and 719. In 717, the English missionary Boniface was sent to aid Willibrord, re-establishing churches in Frisia continuing missions in Germany.

Following a series of heavy military reverses against the Muslims, the Iconoclasm emerged in the early 8th century. In the 720s the Byzantine Emperor Leo III the Isaurian banned the pictorial representation of Christ, saints, and biblical scenes. In the West, Pope Gregory III held two synods at Rome and condemned Leo's actions. The Byzantine Iconoclast Council at Hieria in 754, ruled that holy portraits were heretical.

The movement destroyed much of the Christian church's early artistic history. The iconoclastic movement itself was later defined as heretical in 787 under the Seventh Ecumenical council, but enjoyed a brief resurgence between 815 and 842.

The Carolingian Renaissance was a period of intellectual and cultural revival of literature, arts, and scriptural studies during the late 8th and 9th centuries, mostly during the reigns of Charlemagne and Louis the Pious, Frankish rulers. To address the problems of illiteracy among clergy and court scribes, Charlemagne founded schools and attracted the most learned men from all of Europe to his court.

From the 6th century onward most of the monasteries in the West were of the Benedictine Order. Owing to the stricter adherence to a reformed Benedictine rule, the abbey of Cluny became the acknowledged leader of western monasticism from the later 10th century. Cluny created a large, federated order in which the administrators of subsidiary houses served as deputies of the abbot of Cluny and answered to him. The Cluniac spirit was a revitalising influence on the Norman church, at its height from the second half of the 10th centuries through the early 12th.

The next wave of monastic reform came with the Cistercian Movement. The first Cistercian abbey was founded in 1098, at Cîteaux Abbey. The keynote of Cistercian life was a return to a literal observance of the Benedictine rule, rejecting the developments of the Benedictines. The most striking feature in the reform was the return to manual labour, and especially to field-work.

Inspired by Bernard of Clairvaux, the primary builder of the Cistercians, they became the main force of technological diffusion in medieval Europe. By the end of the 12th century the Cistercian houses numbered 500, and at its height in the 15th century the order claimed to have close to 750 houses. Most of these were built in wilderness areas, and played a major part in bringing such isolated parts of Europe into economic cultivation

A third level of monastic reform was provided by the establishment of the Mendicant orders. Commonly known as friars, mendicants live under a monastic rule with traditional vows of poverty, chastity, and obedience, but they emphasise preaching, missionary activity, and education, in a secluded monastery. Beginning in the 12th century, the Franciscan order was instituted by the followers of Francis of Assisi, and thereafter the Dominican order was begun by St. Dominic.

The Investiture Controversy, or Lay investiture controversy, was the most significant conflict between secular and religious powers in medieval Europe. It began as a dispute in the 11th century between the Holy Roman Emperor Henry IV, and Pope Gregory VII concerning who would appoint bishops (investiture). The end of lay investiture threatened to undercut the power of the Empire and the ambitions of noblemen for the benefit of Church reform.

Bishops collected revenues from estates attached to their bishopric. Noblemen who held lands (fiefdoms) hereditarily passed those lands on within their family. However, because bishops had no legitimate children, when a bishop died it was the king's right to appoint a successor. So, while a king had little recourse in preventing noblemen from acquiring powerful domains via inheritance and dynastic marriages, a king could keep careful control of lands under the domain of his bishops.

Kings would bestow bishoprics to members of noble families whose friendship he wished to secure. Furthermore, if a king left a bishopric vacant, then he collected the estates' revenues until a bishop was appointed, when in theory he was to repay the earnings. The infrequent nature of this repayment was an obvious source of dispute. The Church wanted to end this lay investiture because of the potential corruption, not only from vacant sees but also from other practices such as simony. Thus, the Investiture Contest was part of the Church's attempt to reform the episcopate and provide better pastoral care.

Pope Gregory VII issued the "Dictatus Papae", which declared that the pope alone could appoint or depose bishops, or translate them to other sees. Henry IV's rejection of the decree lead to his excommunication and a ducal revolt. Eventually Henry received absolution after dramatic public penance barefoot in Alpine snow and cloaked in a hair-shirt (see Road to Canossa), though the revolt and conflict of investiture continued.

Likewise, a similar controversy occurred in England between King Henry I and St. Anselm, Archbishop of Canterbury, over investiture and ecclesiastical revenues collected by the king during an episcopal vacancy. The English dispute was resolved by the Concordat of London, 1107, where the king renounced his claim to invest bishops but continued to require an oath of fealty from them upon their election.

This was a partial model for the Concordat of Worms ("Pactum Calixtinum"), which resolved the Imperial investiture controversy with a compromise that allowed secular authorities some measure of control but granted the selection of bishops to their cathedral canons. As a symbol of the compromise, lay authorities invested bishops with their secular authority symbolised by the lance, and ecclesiastical authorities invested bishops with their spiritual authority symbolised by the ring and the staff.

The Medieval Inquisition is a series of Inquisitions (Roman Catholic Church bodies charged with suppressing heresy) from around 1184, including the Episcopal Inquisition (1184–1230s) and later the Papal Inquisition (1230s). It was in response to movements within Europe considered apostate or heretical to Western Catholicism, in particular the Cathars and the Waldensians in southern France and northern Italy. These were the first inquisition movements of many that would follow. The inquisitions in combination with the Albigensian Crusade were fairly successful in ending heresy. Historian Thomas F. Madden has written about popular myths regarding the Inquisition.

Early evangelisation in Scandinavia was begun by Ansgar, Archbishop of Bremen, "Apostle of the North". Ansgar, a native of Amiens, was sent with a group of monks to Jutland Denmark in around 820 at the time of the pro-Christian Jutish king Harald Klak. The mission was only partially successful, and Ansgar returned two years later to Germany, after Harald had been driven out of his kingdom.

In 829 Ansgar went to Birka on Lake Mälaren, Sweden, with his aide friar Witmar, and a small congregation was formed in 831 which included the king's own steward Hergeir. Conversion was slow, however, and most Scandinavian lands were only completely Christianised at the time of rulers such as Saint Canute IV of Denmark and Olaf I of Norway in the years following AD 1000.

Though by 800 Western Europe was ruled entirely by Christian kings, East and Central Europe remained an area of missionary activity. For example, in the 9th century SS. Cyril and Methodius had extensive missionary success in the region among the Slavic peoples, translating the Bible and liturgy into Slavonic. The Baptism of Kiev in 988 spread Christianity throughout Kievan Rus', establishing Christianity among Ukraine, Belarus and Russia.

In the 9th and 10th centuries, Christianity made great inroads into Eastern Europe, including Bulgaria and Kievan Rus'. The evangelisation, or Christianisation, of the Slavs was initiated by one of Byzantium's most learned churchmen—the Patriarch Photios I of Constantinople (Photius). The Byzantine emperor Michael III chose Cyril and Methodius in response to a request from Rastislav, the king of Moravia who wanted missionaries that could minister to the Moravians in their own language.

The two brothers spoke the local Slavonic vernacular and translated the Bible and many of the prayer books. As the translations prepared by them were copied by speakers of other dialects, the hybrid literary language Old Church Slavonic was created.

Methodius later went on to convert the Serbs. Some of the disciples returned to Bulgaria where they were welcomed by the Bulgarian Knyaz Boris I who viewed the Slavonic liturgy as a way to counteract Byzantine influence in the country. In a short time the disciples of Cyril and Methodius managed to prepare and instruct the future Slavic clergy into the Glagolitic alphabet and the biblical texts.

Bulgaria was officially recognised as a patriarchate by Constantinople in 927, Serbia in 1346, and Russia in 1589. All these nations, however, had been converted long before these dates.

The missionaries to the East and South Slavs had great success in part because they used the people's native language rather than Latin as the Roman priests did, or Greek.

When king Rastislav of Moravia asked Byzantium for teachers who could minister to the Moravians in their own language, Byzantine emperor Michael III chose two brothers, Cyril and Methodius. As their mother was a Slav from the hinterlands of Thessaloniki, the two brothers had been raised speaking the local Slavonic vernacular. Once commissioned, they immediately set about creating an alphabet, the Glagolitic alphabet. They then translated the Scripture and the liturgy into Slavonic.

This Slavic dialect became the basis of Old Church Slavonic which later evolved into Church Slavonic which is the common liturgical language still used by the Russian Orthodox Church and other Slavic Orthodox Christians. The missionaries to the East and South Slavs had great success in part because they used the people's native language rather than Latin or Greek. In Great Moravia, Constantine and Methodius encountered Frankish missionaries from Germany, representing the western or Latin branch of the Church, and more particularly representing the Holy Roman Empire as founded by Charlemagne, and committed to linguistic, and cultural uniformity. They insisted on the use of the Latin liturgy, and they regarded Moravia and the Slavic peoples as part of their rightful mission field.

When friction developed, the brothers, unwilling to be a cause of dissension among Christians, travelled to Rome to see the Pope, seeking an agreement that would avoid quarrelling between missionaries in the field. Constantine entered a monastery in Rome, taking the name Cyril, by which he is now remembered. However, he died only a few weeks thereafter.

Pope Adrian II gave Methodius the title of Archbishop of Sirmium (now Sremska Mitrovica in Serbia) and sent him back in 869, with jurisdiction over all of Moravia and Pannonia, and authorisation to use the Slavonic Liturgy. Soon, however, Prince Ratislav, who had originally invited the brothers to Moravia, died, and his successor did not support Methodius. In 870 the Frankish king Louis and his bishops deposed Methodius at a synod at Ratisbon, and imprisoned him for a little over two years. Pope John VIII secured his release, but instructed him to stop using the Slavonic Liturgy.

In 878, Methodius was summoned to Rome on charges of heresy and using Slavonic. This time Pope John was convinced by the arguments that Methodius made in his defence and sent him back cleared of all charges, and with permission to use Slavonic. The Carolingian bishop who succeeded him, Witching, suppressed the Slavonic Liturgy and forced the followers of Methodius into exile. Many found refuge with Knyaz Boris of Bulgaria, under whom they reorganised a Slavic-speaking Church. Meanwhile, Pope John's successors adopted a Latin-only policy which lasted for centuries.

Bulgaria was a pagan country since its establishment in 681 until 864 when Boris I (852–889) converted to Christianity. The reasons for that decision were complex; the most important factors were that Bulgaria was situated between two powerful Christian empires, Byzantium and East Francia; Christian doctrine particularly favoured the position of the monarch as God's representative on Earth, while Boris also saw it as a way to overcome the differences between Bulgars and Slavs.

In 885 some of the disciples of Cyril and Methodius, including Clement of Ohrid, Naum of Preslav and Angelaruis, returned to Bulgaria where they were welcomed by Boris I who viewed the Slavonic liturgy as a way to counteract Byzantine influence in the country. In a short time they managed to prepare and instruct the future Bulgarian clergy into the Glagolitic alphabet and the biblical texts. As a result of the Council of Preslav in AD 893, Bulgaria expelled its Greek clergy and proclaimed the Old Bulgarian language as the official language of the church and the state.

The success of the conversion of the Bulgarians facilitated the conversion of other East Slavic peoples, most notably the Rus', predecessors of Belarusians, Russians, and Ukrainians, as well as Rusyns. By the beginning of the 11th century most of the pagan Slavic world, including Rus', Bulgaria and Serbia, had been converted to Byzantine Christianity. The traditional event associated with the conversion of Rus' is the baptism of Vladimir of Kiev in 989. However, Christianity is documented to have predated this event in the city of Kiev and in Georgia. Today the Russian Orthodox Church is the largest of the Orthodox Churches.

The cracks and fissures in Christian unity which led to the East-West Schism started to become evident as early as the 4th century. Cultural, political, and linguistic differences were often mixed with the theological, leading to schism.

The transfer of the Roman capital to Constantinople inevitably brought mistrust, rivalry, and even jealousy to the relations of the two great sees, Rome and Constantinople. It was easy for Rome to be jealous of Constantinople at a time when it was rapidly losing its political prominence. Estrangement was also helped along by the German invasions in the West, which effectively weakened contacts. The rise of Islam with its conquest of most of the Mediterranean coastline (not to mention the arrival of the pagan Slavs in the Balkans at the same time) further intensified this separation by driving a physical wedge between the two worlds. The once homogeneous unified world of the Mediterranean was fast vanishing. Communication between the Greek East and Latin West by the 7th century had become dangerous and practically ceased.

Two basic problems were involved: the nature of the primacy of the bishop of Rome and the theological implications of adding a clause to the Nicene Creed, known as the "Filioque" clause. These doctrinal issues were first openly discussed in Photius's patriarchate.

By the 5th century, Christendom was divided into a pentarchy of five sees with Rome accorded a primacy. The four Eastern sees of the pentarchy considered this determined by canonical decision and not entailing hegemony of any one local church or patriarchate over the others. However, Rome began to interpret her primacy in terms of sovereignty, as a God-given right involving universal jurisdiction in the Church. The collegial and conciliar nature of the Church, in effect, was gradually abandoned in favour of supremacy of unlimited papal power over the entire Church. These ideas were finally given systematic expression in the West during the Gregorian Reform movement of the 11th century.

The Eastern churches viewed Rome's understanding of the nature of episcopal power as being in direct opposition to the Church's essentially conciliar structure and thus saw the two ecclesiologies as mutually antithetical. For them, specifically, Simon Peter's primacy could never be the exclusive prerogative of any one bishop. All bishops must, like St. Peter, confess Jesus as the Christ and, as such, all are Peter's successors. The churches of the East gave the Roman See primacy but not supremacy, the Pope being the first among equals but not infallible and not with absolute authority.

The other major irritant to Eastern Christendom was the Western use of the "Filioque" clause—meaning "and the Son"—in the Nicene Creed . This too developed gradually and entered the Creed over time. The issue was the addition by the West of the Latin clause "Filioque" to the Creed, as in "the Holy Spirit... who proceeds from the Father "and the Son"", where the original Creed, sanctioned by the councils and still used today by the Eastern Orthodox simply states "the Holy Spirit, the Lord and Giver of Life, who proceeds from the Father." The Eastern Church argued that the phrase had been added unilaterally, and therefore illegitimately, since the East had never been consulted.

In the final analysis, only another ecumenical council could introduce such an alteration. Indeed, the councils, which drew up the original Creed, had expressly forbidden any subtraction or addition to the text. In addition to this ecclesiological issue, the Eastern Church also considered the" Filioque" clause unacceptable on dogmatic grounds. Theologically, the Latin interpolation was unacceptable since it implied that the Spirit now had two sources of origin and procession, the Father and the Son, rather than the Father alone.

In the 9th century AD, a controversy arose between Eastern (Byzantine, Greek Orthodox) and Western (Latin, Roman Catholic) Christianity that was precipitated by the opposition of the Roman Pope John VII to the appointment by the Byzantine emperor Michael III of Photios I to the position of patriarch of Constantinople. Photios was refused an apology by the pope for previous points of dispute between the East and West. Photios refused to accept the supremacy of the pope in Eastern matters or accept the "Filioque" clause. The Latin delegation at the council of his consecration pressed him to accept the clause in order to secure their support.

The controversy also involved Eastern and Western ecclesiastical jurisdictional rights in the Bulgarian church, as well as a doctrinal dispute over the "Filioque" ("and from the Son") clause. That had been added to the Nicene Creed by the Latin church, which was later the theological breaking point in the ultimate Great East-West Schism in the 11th century.

Photios did provide concession on the issue of jurisdictional rights concerning Bulgaria and the papal legates made do with his return of Bulgaria to Rome. This concession, however, was purely nominal, as Bulgaria's return to the Byzantine rite in 870 had already secured for it an autocephalous church. Without the consent of Boris I of Bulgaria, the papacy was unable to enforce any of its claims.

The East-West Schism, or Great Schism, separated the Church into Western (Latin) and Eastern (Greek) branches, i.e., Western Catholicism and Eastern Orthodoxy. It was the first major division since certain groups in the East rejected the decrees of the Council of Chalcedon (see Oriental Orthodoxy), and was far more significant. Though normally dated to 1054, the East-West Schism was actually the result of an extended period of estrangement between Latin and Greek Christendom over the nature of papal primacy and certain doctrinal matters like the "Filioque", but intensified by cultural and linguistic differences.

The "official" schism in 1054 was the excommunication of Patriarch Michael Cerularius of Constantinople, followed by his excommunication of papal legates. Attempts at reconciliation were made in 1274 (by the Second Council of Lyon) and in 1439 (by the Council of Basel), but in each case the eastern hierarchs who consented to the unions were repudiated by the Orthodox as a whole, though reconciliation was achieved between the West and what are now called the "Eastern Rite Catholic Churches". More recently, in 1965 the mutual excommunications were rescinded by the Pope and the Patriarch of Constantinople, though schism remains.

Both groups are descended from the Early Church, both acknowledge the apostolic succession of each other's bishops, and the validity of each other's sacraments. Though both acknowledge the primacy of the Bishop of Rome, Eastern Orthodoxy understands this as a primacy of honour with limited or no ecclesiastical authority in other dioceses.

The Orthodox East perceived the Papacy as taking on monarchical characteristics that were not in line with the church's tradition.

The final breach is often considered to have arisen after the capture and sacking of Constantinople by the Fourth Crusade in 1204. Crusades against Christians in the East by Roman Catholic crusaders was not exclusive to the Mediterranean though (see also the Northern Crusades and the Battle of the Ice). The sacking of Constantinople and the Church of Holy Wisdom and establishment of the Latin Empire as a seeming attempt to supplant the Orthodox Byzantine Empire in 1204 is viewed with some rancour to the present day.

Many in the East saw the actions of the West as a prime determining factor in the weakening of Byzantium. This led to the Empire's eventual conquest and fall to Islam. In 2004, Pope John Paul II extended a formal apology for the sacking of Constantinople in 1204; the apology was formally accepted by Patriarch Bartholomew of Constantinople. Many things that were stolen during this time: holy relics, riches, and many other items, are still held in various Western European cities, particularly Venice, Italy.

Generally, the Crusades refer to the campaigns in the Holy Land against Muslim forces sponsored by the Papacy. There were other crusades against Islamic forces in southern Spain, southern Italy, and Sicily, as well as the campaigns of Teutonic knights against pagan strongholds in North-eastern Europe (see Northern Crusades). A few crusades such as the Fourth Crusade were waged within Christendom against groups that were considered heretical and schismatic (also see the Battle of the Ice and the Albigensian Crusade).

The Holy Land had been part of the Roman Empire, and thus Byzantine Empire, until the Islamic conquests of the 7th and 8th centuries. Thereafter, Christians had generally been permitted to visit the sacred places in the Holy Land until 1071, when the Seljuk Turks closed Christian pilgrimages and assailed the Byzantines, defeating them at the Battle of Manzikert.

Emperor Alexius I asked for aid from Pope Urban II (1088–1099) for help against Islamic aggression. He probably expected money from the pope for the hiring of mercenaries. Instead, Urban II called upon the knights of Christendom in a speech made at the Council of Clermont on 27 November 1095, combining the idea of pilgrimage to the Holy Land with that of waging a holy war against infidels.

The First Crusade captured Antioch in 1099 and then Jerusalem. The Second Crusade occurred in 1145 when Edessa was retaken by Islamic forces. Jerusalem would be held until 1187 and the Third Crusade, famous for the battles between Richard the Lionheart and Saladin. The Fourth Crusade, begun by Innocent III in 1202, intended to retake the Holy Land but was soon subverted by Venetians who used the forces to sack the Christian city of Zara.

Eventually, the crusaders arrived in Constantinople. Rather than proceed to the Holy Land the crusaders instead sacked Constantinople and other parts of Asia Minor effectively establishing the Latin Empire of Constantinople in Greece and Asia Minor. This was effectively the last crusade sponsored by the papacy, with later crusades being sponsored by individuals.

Jerusalem was held by the crusaders for nearly a century, and other strongholds in the Near East would remain in Christian possession much longer. The crusades in the Holy Land ultimately failed to establish permanent Christian kingdoms. Islamic expansion into Europe would renew and remain a threat for centuries culminating in the campaigns of Suleiman the Magnificent in the 16th century.

Crusades in southern Spain, southern Italy, and Sicily eventually lead to the demise of Islamic power in Europe. The Teutonic knights expanded Christian domains in Eastern Europe, and the much less frequent crusades within Christendom, such as the Albigensian Crusade, achieved their goal of maintaining doctrinal unity.

In 1337 Hesychasm—a mystical teaching at Mount Athos came under attack from Barlaam of Calabria, an abbot in Constantinople. Barlaam propounded a more intellectual and propositional approach to the knowledge of God than the Hesychasts taught. Hesychasm is a form of constant purposeful prayer or experiential prayer, explicitly referred to as contemplation focusing on the idea of stillness and the characteristic mystical idea of light as the vehicle for knowing God.

Gregory Palamas, afterwards Archbishop of Thessalonica, defended Hesychasm. Several synods took one position or the other until in 1351 at a synod under the presidency of the Emperor John VI Cantacuzenus, Hesychast doctrine was established as the doctrine of the Orthodox Church. the theology was especially attractive in the East because it validated the use of icons as a vehicle for contemplation of divine light.

In 1453, Constantinople fell to the Ottoman Empire. By this time Egypt had been under Muslim control for some seven centuries, but Orthodoxy was very strong in Russia which had recently acquired an autocephalous status; and thus Moscow called itself the Third Rome, as the cultural heir of Constantinople.

Under Ottoman rule, the Greek Orthodox Church acquired substantial power as an autonomous "millet". The ecumenical patriarch was the religious and administrative ruler of the entire "Greek Orthodox nation" (Ottoman administrative unit), which encompassed all the Eastern Orthodox subjects of the Empire.

Eastern Christians fleeing Constantinople, and the Greek manuscripts they carried with them, is one of the factors that prompted the literary renaissance in the West at about this time.

As a result of the Ottoman conquest of the Byzantine Empire in 1453, and the Fall of Constantinople, the entire Orthodox communion of the Balkans and the Near East became suddenly isolated from the West. For the next four hundred years, it would be confined within a hostile Islamic world, with which it had little in common religiously or culturally. The Russian Orthodox Church was the only part of the Orthodox communion which remained outside the control of the Ottoman Empire.

It is, in part, due to this geographical and intellectual confinement that the voice of Eastern Orthodoxy was not heard during the Reformation in 16th-century Europe. As a result, this important theological debate often seems strange and distorted to the Orthodox. They never took part in it and thus neither Reformation nor Counter-Reformation is part of their theological framework.

The new Ottoman government that conquered the Byzantine Empire followed Islamic law when dealing with the conquered Christian population. Christians were officially tolerated as People of the Book. As such, the Church's canonical and hierarchical organisation were not significantly disrupted and its administration continued to function. One of the first things that Mehmet the Conqueror did was to allow the Church to elect a new patriarch, Gennadius Scholarius.

Because Islamic law makes no distinction between nationality and religion, all Christians, regardless of their language or nationality, were considered a single millet, or nation. The patriarch, as the highest-ranking hierarch, was thus invested with civil and religious authority and made ethnarch, head of the entire Christian Orthodox population. This meant that all Orthodox Churches within Ottoman territory were under the control of Constantinople. However, these rights and privileges, including freedom of worship and religious organisation, were often established in principle but seldom corresponded to reality. Christians were viewed as second-class citizens, and the legal protections they depended upon were subject to the whims of the Sultan and the Sublime Porte.

Under Ottoman occupation the Church could no longer bear witness to Christ. Christian missionary work among Muslims was illegal and dangerous, whereas conversion to Islam was entirely legal and permissible. Converts to Islam who returned to Orthodoxy were put to death as apostates. No new churches could be built and even the ringing of church bells was prohibited. The Hagia Sophia and the Parthenon, which had been Christian churches for nearly a millennium, were converted into mosques. Education of the clergy and the Christian population either ceased altogether or was reduced to the most rudimentary elements. Violent persecutions of Christians were common, and reached their climax in the Armenian, Assyrian, and Greek genocides.

The Orthodox Church found itself subject to the Turkish system of corruption. The patriarchal throne was frequently sold to the highest bidder, while new patriarchal investiture was accompanied by heavy payment to the government. In order to recoup their losses, patriarchs and bishops taxed the local parishes and their clergy.

Few patriarchs between the 15th and the 19th centuries died a natural death while in office. The forced abdications, exiles, hangings, drownings, and poisonings of patriarchs are well documented. The hierarchy's positions were often dangerous as well. The hanging of patriarch Gregory V from the gate of the patriarchate on Easter Sunday 1821 was accompanied by the execution of two metropolitans and twelve bishops.

The Avignon Papacy, sometimes referred to as the Babylonian Captivity, was a period from 1309 to 1378 during which seven Popes resided in Avignon, in modern-day France. The period was one of conflict and controversy during which French Kings held considerable sway over the Papacy and rulers across Europe felt sidelined by the new French-centric papacy.

Troubles reached their peak in 1378 when, Gregory XI died while visiting Rome. A papal conclave met in Rome and elected Urban VI, an Italian. Urban soon alienated the French cardinals, and they held a second conclave electing Robert of Geneva to succeed Gregory XI, beginning the Western Schism.

The Western Schism, or Papal Schism, was a prolonged period of crisis in Latin Christendom from 1378 to 1416, when there were two or more claimants to the See of Rome and there was conflict concerning the rightful holder of the papacy. The conflict was political, rather than doctrinal, in nature.

In 1309, Pope Clement V, due to political considerations, moved to Avignon in southern France and exercised his pontificate there. For sixty-nine years popes resided in Avignon rather than Rome. This was not only an obvious source of confusion but of political animosity as the prestige and influence of city of Rome waned without a resident pontiff. Though Pope Gregory XI, a Frenchman, returned to Rome in 1378, the strife between Italian and French factions intensified, especially following his subsequent death.

In 1378 the conclave, elected an Italian from Naples, Pope Urban VI; his intransigence in office soon alienated the French cardinals, who withdrew to a conclave of their own, asserting the previous election was invalid since its decision had been made under the duress of a riotous mob. They elected one of their own, Robert of Geneva, who took the name Pope Clement VII. By 1379, he was back in the palace of popes in Avignon, while Urban VI remained in Rome.

For nearly forty years, there were two papal curias and two sets of cardinals, each electing a new pope for Rome or Avignon when death created a vacancy. Each pope lobbied for support among kings and princes who played them off against each other, changing allegiance according to political advantage. In 1409, a council was convened at Pisa to resolve the issue. The council declared both existing popes to be schismatic (Gregory XII from Rome, Benedict XIII from Avignon) and appointed a new one, Alexander V. The existing popes refused to resign and thus there were three papal claimants. Another council was convened in 1414, the Council of Constance.

In March 1415 the Pisan pope John XXIII fled from Constance in disguise. He was brought back a prisoner and deposed in May. The Roman pope, Gregory XII, resigned voluntarily in July. The Avignon pope, Benedict XIII, refused to come to Constance, nor would he consider resignation. The council deposed him in July 1417. The council in Constance elected Pope Martin V as pope in November, having finally cleared the field of popes and antipopes, .

John Wycliffe (or Wyclif) (1330–1384) was an English scholar and heretic best known for denouncing the corruptions of the Church, and his sponsoring the first translation of the Bible from Latin into English. He was a precursor of the Protestant Reformation. He emphasized the supremacy of the Bible, and called for a direct relationship between man and God, without interference by priests and bishops. His followers, called Lollards, faced persecution by the Church of England. They went underground for over a century and played a role in the English Reformation.

Jan Hus (or Huss) (1369?–1415) a Czech theologian in Prague, was influenced by Wycliffe and spoke out against the corruptions he saw in the Church; his continued defiance led to his excommunication and condemnation by the Council of Constance, which also condemned John Wycliff. Hus was executed in 1415, but his followers organized a peasants' war, 1419–1436, that was put down by the Empire with great brutality. Hus was a forerunner of the Protestant Reformation and his memory has become a powerful symbol of Czech culture in Bohemia.

The Renaissance was a period of great cultural change and achievement, marked in Italy by a classical orientation and an increase of wealth through mercantile trade. The City of Rome, the Papacy, and the Papal States were all affected by the Renaissance. On the one hand, it was a time of great artistic patronage and architectural magnificence, where the Church pardoned such artists as Michelangelo, Brunelleschi, Bramante, Raphael, Fra Angelico, Donatello, and da Vinci. On the other hand, wealthy Italian families often secured episcopal offices, including the papacy, for their own members, some of whom were known for immorality, such as Alexander VI and Sixtus IV.

In addition to being the head of the Church, the Pope became one of Italy's most important secular rulers, and pontiffs such as Julius II often waged campaigns to protect and expand their temporal domains. Furthermore, the popes, in a spirit of refined competition with other Italian lords, spent lavishly both on private luxuries but also on public works, repairing or building churches, bridges, and a magnificent system of aqueducts in Rome that still function today.

From 1505 to 1626, St. Peter's Basilica, perhaps the most recognised Christian church, was built on the site of the old Constantinian basilica. It was also a time of increased contact with Greek culture, opening up new avenues of learning, especially in the fields of philosophy, poetry, classics, rhetoric, and political science, fostering a spirit of humanism–all of which would influence the Church.

In the early 16th century, movements were begun by two theologians, Martin Luther and Huldrych Zwingli, that aimed to reform the Church; these reformers are distinguished from previous ones in that they considered the root of corruptions to be doctrinal (rather than simply a matter of moral weakness or lack of ecclesiastical discipline) and thus they aimed to change contemporary doctrines to accord with what they perceived to be the "true gospel." The word "Protestant" is derived from the Latin "protestatio" meaning "declaration" which refers to the letter of protestation by Lutheran princes against the decision of the Diet of Speyer in 1529, which reaffirmed the edict of the Diet of Worms against the Reformation. Since that time, the term has been used in many different senses, but most often as a general term refers to Western Christianity that is not subject to papal authority. The term "Protestant" was not originally used by Reformation era leaders; instead, they called themselves "evangelical", emphasising the "return to the true gospel (Greek: "euangelion")."

The beginning of the Protestant Reformation is generally identified with Martin Luther and the posting of the 95 Theses on the castle church in Wittenberg, Germany. Early protest was against corruptions such as simony, episcopal vacancies, and the sale of indulgences. The Protestant position, however, would come to incorporate doctrinal changes such as sola scriptura and sola fide. The three most important traditions to emerge directly from the Protestant Reformation were the Lutheran, Reformed (Calvinist, Presbyterian, etc.), and Anglican traditions, though the latter group identifies as both "Reformed" and "Catholic", and some subgroups reject the classification as "Protestant."

The Protestant Reformation may be divided into two distinct but basically simultaneous movements, the Magisterial Reformation and the Radical Reformation. The Magisterial Reformation involved the alliance of certain theological teachers (Latin: "magistri") such as Luther, Zwingli, Calvin, Cranmer, etc. with secular magistrates who cooperated in the reformation of Christendom. Radical Reformers, besides forming communities outside state sanction, often employed more extreme doctrinal change, such as the rejection of tenets of the Councils of Nicaea and Chalcedon. Often the division between magisterial and radical reformers was as or more violent than the general Catholic and Protestant hostilities.

The Protestant Reformation spread almost entirely within the confines of Northern Europe, but did not take hold in certain northern areas such as Ireland and parts of Germany. By far the magisterial reformers were more successful and their changes more widespread than the radical reformers. The Catholic response to the Protestant Reformation is known as the Counter Reformation, or Catholic Reformation, which resulted in a reassertion of traditional doctrines and the emergence of new religious orders aimed at both moral reform and new missionary activity. The Counter Reformation reconverted approximately 33% of Northern Europe to Catholicism and initiated missions in South and Central America, Africa, Asia, and even China and Japan. Protestant expansion outside of Europe occurred on a smaller scale through colonisation of North America and areas of Africa.

Martin Luther was an Augustinian friar and professor at the University of Wittenberg. In 1517, he published a list of 95 Theses, or points to be debated, concerning the illicitness of selling indulgences. Luther had a particular disdain for Aristotelian philosophy, and as he began developing his own theology, he increasingly came into conflict with Thomistic scholars, most notably Cardinal Cajetan. Soon, Luther had begun to develop his theology of justification, or process by which one is "made right" (righteous) in the eyes of God. In Catholic theology, one is made righteous by a progressive infusion of grace accepted through faith and cooperated with through good works. Luther's doctrine of justification differed from Catholic theology in that justification rather meant "the declaring of one to be righteous", where God imputes the merits of Christ upon one who remains without inherent merit. In this process, good works are more of an unessential byproduct that contribute nothing to one's own state of righteousness. Conflict between Luther and leading theologians lead to his gradual rejection of authority of the Church hierarchy. In 1520, he was condemned for heresy by the papal bull "Exsurge Domine", which he burned at Wittenberg along with books of canon law.

Ulrich Zwingli was a Swiss scholar and parish priest who was likewise influential in the beginnings of the Protestant Reformation. Zwingli claimed that his theology owed nothing to Luther, and that he had developed it in 1516, before Luther's famous protest, though his doctrine of justification was remarkably similar to that of the German friar. In 1518, Zwingli was given a post at the wealthy collegiate church of the Grossmünster in Zürich, where he would remain until his death at a relatively young age. Soon he had risen to prominence in the city, and when political tension developed between most of Switzerland and the Catholic Habsburg Emperor Charles V. In this environment, Zwingli began preaching his version of reform, with certain points as the aforementioned doctrine of justification, but others (with which Luther vehemently disagreed) such as the position that veneration of icons was actually idolatry and thus a violation of the first commandment, and the denial of the real presence in the Eucharist. Soon the city council had accepted Zwingli's doctrines and Zurich became a focal point of more radical reforming movements, and certain admirers and followers of Zwingli pushed his message and reforms far further than even he had intended, such as rejecting infant baptism. This split between Luther and Zwingli formed the essence of the Protestant division between Lutheran and Reformed theology. Meanwhile, political tensions increased; Zwingli and the Zurich leadership imposed an economic blockade on the inner Catholic states of Switzerland, which led to a battle in which Zwingli, in full armor, was slain along with his troops.

John Calvin was a French cleric and doctor of law turned Protestant reformer. He belonged to the second generation of the Reformation, publishing his theological tome, the "Institutes of the Christian Religion", in 1536 (later revised), and establishing himself as a leader of the Reformed church in Geneva, which became an "unofficial capital" of Reformed Christianity in the second half of the 16th century. He exerted a remarkable amount of authority in the city and over the city council, such that he has (rather ignominiously) been called a "Protestant pope." Calvin established an eldership together with a "consistory", where pastors and the elders established matters of religious discipline for the Genevan population. Calvin's theology is best known for his doctrine of (double) predestination, which held that God had, from all eternity, providentially foreordained who would be saved (the elect) and likewise who would be damned (the reprobate). Predestination was not the dominant idea in Calvin's works, but it would seemingly become so for many of his Reformed successors.

Unlike other reform movements, the English Reformation began by royal influence. Henry VIII considered himself a thoroughly Catholic King, and in 1521 he defended the papacy against Luther in a book he commissioned entitled, "The Defence of the Seven Sacraments", for which Pope Leo X awarded him the title "Fidei Defensor" (Defender of the Faith). However, the king came into conflict with the papacy when he wished to annul his marriage with Catherine of Aragon, for which he needed papal sanction. Catherine, among many other noble relations, was the aunt of Emperor Charles V, the papacy's most significant secular supporter. The ensuing dispute eventually lead to a break from Rome and the declaration of the King of England as head of the English Church. England would later experience periods of frenetic and eclectic reforms contrasted by periods led by staunch conservatives. Monarchs such as Edward VI, Mary I, Elizabeth I, and Archbishops of Canterbury such as Thomas Cranmer and William Laud pushed the Church of England in many directions over the course of only a few generations. What emerged was a state church that considered itself both "Reformed" and "Catholic" but not "Roman" (and hesitated from the title "Protestant"), and other "unofficial" more radical movements such as the Puritans.

The Counter-Reformation, or Catholic Reformation, was the response of the Catholic Church to the Protestant Reformation. The essence of the Counter-Reformation was a renewed conviction in traditional practices and the upholding of Catholic doctrine as the source of ecclesiastic and moral reform, and the answer to halt the spread of Protestantism. Thus it experienced the founding of new religious orders, such as the Jesuits, the establishment of seminaries for the proper training of priests, renewed worldwide missionary activity, and the development of new yet orthodox forms of spirituality, such as that of the Spanish mystics and the French school of spirituality. The entire process was spearheaded by the Council of Trent, which clarified and reasserted doctrine, issued dogmatic definitions, and produced the "Roman Catechism".

Though Ireland, Spain, France, and elsewhere featured significantly in the Counter-Reformation, its heart was Italy and the various popes of the time, who established the "Index Librorum Prohibitorum" (the list of prohibited books) and the Roman Inquisition, a system of juridical tribunals that prosecuted heresy and related offences. The Papacy of St. Pius V (1566–1572) was known not only for its focus on halting heresy and worldly abuses within the Church, but also for its focus on improving popular piety in a determined effort to stem the appeal of Protestantism. Pius began his pontificate by giving large alms to the poor, charity, and hospitals, and the pontiff was known for consoling the poor and sick, and supporting missionaries. The activities of these pontiffs coincided with a rediscovery of the ancient Christian catacombs in Rome. As Diarmaid MacCulloch stated, "Just as these ancient martyrs were revealed once more, Catholics were beginning to be martyred afresh, both in mission fields overseas and in the struggle to win back Protestant northern Europe: the catacombs proved to be an inspiration for many to action and to heroism."

The Council of Trent (1545–1563), initiated by Pope Paul III (1534–1549) addressed issues of certain ecclesiastical corruptions such as simony, absenteeism, nepotism, and other abuses, as well as the reassertion of traditional practices and the dogmatic articulation of the traditional doctrines of the Church, such as the episcopal structure, clerical celibacy, the seven Sacraments, transubstantiation (the belief that during mass the consecrated bread and wine truly become the body and blood of Christ), the veneration of relics, icons, and saints (especially the Blessed Virgin Mary), the necessity of both faith and good works for salvation, the existence of purgatory and the issuance (but not the sale) of indulgences, etc. In other words, all Protestant doctrinal objections and changes were uncompromisingly rejected. The Council also fostered an interest in education for parish priests to increase pastoral care. Milan's Archbishop Saint Charles Borromeo (1538–1584) set an example by visiting the remotest parishes and instilling high standards.

Catholic missions were carried to new places beginning with the new Age of Discovery, and the Roman Catholic Church established a number of Missions in the Americas and other colonies in order to spread Christianity in the New World and to convert the indigenous peoples. At the same time, missionaries such as Francis Xavier as well as other Jesuits, Augustinians, Franciscans and Dominicans were moving into Asia and the Far East. The Portuguese sent missions into Africa. While some of these missions were associated with imperialism and oppression, others (notably Matteo Ricci's Jesuit mission to China) were relatively peaceful and focused on integration rather than cultural imperialism.

The Galileo affair, in which Galileo Galilei came into conflict with the Roman Catholic Church over his support of Copernican astronomy, is often considered a defining moment in the history of the relationship between religion and science.

In 1610, Galileo published his "Sidereus Nuncius (Starry Messenger)", describing the surprising observations that he had made with the new telescope. These and other discoveries exposed major difficulties with the understanding of the Heavens that had been held since antiquity, and raised new interest in radical teachings such as the heliocentric theory of Copernicus.

In reaction, many scholars maintained that the motion of the Earth and immobility of the Sun were heretical, as they contradicted some accounts given in the Bible as understood at that time. Galileo's part in the controversies over theology, astronomy and philosophy culminated in his trial and sentencing in 1633, on a grave suspicion of heresy.

The most famous colonisation by Protestants in the New World was that of English Puritans in North America. Unlike the Spanish or French, the English colonists made surprisingly little effort to evangelise the native peoples. The Puritans, or Pilgrims, left England so that they could live in an area with Puritanism established as the exclusive civic religion. Though they had left England because of the suppression of their religious practice, most Puritans had thereafter originally settled in the Low Countries but found the licentiousness there, where the state hesitated from enforcing religious practice, as unacceptable, and thus they set out for the New World and the hopes of a Puritan utopia.

This is the period from the Industrial revolution and the French Revolution until the mid 19th century.

See the French Republican Calendar and anti-clerical measures. See also the Holy League, the Battle of Vienna, Cardinal Richelieu, and Louis XIV of France.

Revivalism refers to the Calvinist and Wesleyan revival, called the Great Awakening, in North America which saw the development of evangelical Congregationalist, Presbyterian, Baptist, and new Methodist churches.

The First Great Awakening was a wave of religious enthusiasm among Protestants in the American colonies c. 1730–1740, emphasising the traditional Reformed virtues of Godly preaching, rudimentary liturgy, and a deep sense of personal guilt and redemption by Christ Jesus. Historian Sydney E. Ahlstrom saw it as part of a "great international Protestant upheaval" that also created Pietism in Germany, the Evangelical Revival, and Methodism in England. It centred on reviving the spirituality of established congregations, and mostly affected Congregational, Presbyterian, Dutch Reformed, German Reformed, Baptist, and Methodist churches, while also spreading within the slave population. The Second Great Awakening (1800–1830s), unlike the first, focused on the unchurched and sought to instil in them a deep sense of personal salvation as experienced in revival meetings. It also sparked the beginnings of groups such as the Mormons, the Restoration Movement and the Holiness movement. The Third Great Awakening began from 1857 and was most notable for taking the movement throughout the world, especially in English speaking countries. The final group to emerge from the "great awakenings" in North America was Pentecostalism, which had its roots in the Methodist, Wesleyan, and Holiness movements, and began in 1906 on Azusa Street, in Los Angeles. Pentecostalism would later lead to the Charismatic movement.

Restorationism refers to the belief that a purer form of Christianity should be restored using the early church as a model. In many cases, restorationist groups believed that contemporary Christianity, in all its forms, had deviated from the true, original Christianity, which they then attempted to "Reconstruct", often using the Book of Acts as a "guidebook" of sorts. Restorationists do not usually describe themselves as "reforming" a Christian church continuously existing from the time of Jesus, but as "restoring" the Church that they believe was lost at some point. "Restorationism" is often used to describe the Stone-Campbell Restoration Movement.

The term "Restorationist" is also used to describe the Jehovah's Witness Movement, founded in the late 1870s by Charles Taze Russell

The term "Restorationist" is also used to describe the Latter Day Saint movement, including The Church of Jesus Christ of Latter-day Saints (LDS Church), the Community of Christ and numerous other Latter Day Saints sects. Latter Day Saints, also known as Mormons, believe that Joseph Smith was chosen to restore the original organization established by Jesus, now "in its fullness", rather than to reform the church.

The history of the Church from the mid 19th century around period of the revolutions of 1848 to today.

The Russian Orthodox Church held a privileged position in the Russian Empire, expressed in the motto of the late Empire from 1833: Orthodoxy, Autocracy, and Populism. Nevertheless, the Church reform of Peter I in the early 18th century had placed the Orthodox authorities under the control of the Tsar. An official (titled Ober-Procurator) appointed by the Tsar himself ran the committee which governed the Church between 1721 and 1918: the Most Holy Synod.

The Church became involved in the various campaigns of russification, and was accused of involvement in anti-Jewish pogroms. In the case of anti-Semitism and the anti-Jewish pogroms, no evidence is given of the direct participation of the Church, and many Russian Orthodox clerics, including senior hierarchs, openly defended persecuted Jews, at least from the second half of the 19th century.
Also, the Church has no official position on Judaism as such.

The Bolsheviks and other Russian revolutionaries saw the Church, like the Tsarist state, as an enemy of the people.

The Russian Orthodox Church collaborated with the White Army in the Russian Civil War (see White movement) after the October Revolution. This may have further strengthened the Bolshevik animus against the church.

After the October Revolution of 7 November 1917 (25 October Old Calendar) there was a movement within the Soviet Union to unite all of the people of the world under Communist rule (see Communist International). This included the Eastern European bloc countries as well as the Balkan States. Since some of these Slavic states tied their ethnic heritage to their ethnic churches, both the peoples and their church where targeted by the Soviet. Criticism of atheism was strictly forbidden and sometimes lead to imprisonment.

The Soviet Union was the first state to have as an ideological objective the elimination of religion. Toward that end, the Communist regime confiscated church property, ridiculed religion, harassed believers, and propagated anti-religious atheistic propaganda in the schools. Actions toward particular religions, however, were determined by State interests, and most organised religions were never outlawed.
Some actions against Orthodox priests and believers along with execution included torture being sent to prison camps, labour camps or mental hospitals.
The result of state atheism was to transform the Church into a persecuted and martyred Church. In the first five years after the Bolshevik revolution, 28 bishops and 1,200 priests were executed. This included people like the Grand Duchess Elizabeth Fyodorovna who was at this point a monastic. Along with her murder was Grand Duke Sergei Mikhailovich Romanov; the Princes Ioann Konstantinovich, Konstantin Konstantinovich, Igor Konstantinovich and Vladimir Pavlovich Paley; Grand Duke Sergei's secretary, Fyodor Remez; and Varvara Yakovleva, a sister from the Grand Duchess Elizabeth's convent. They were herded into the forest, pushed into an abandoned mineshaft and grenades were then hurled into the mineshaft. Her remains were buried in Jerusalem, in the Church of Maria Magdalene.
The main target of the anti-religious campaign in the 1920s and 1930s was the Russian Orthodox Church, which had the largest number of faithful. Nearly its entire clergy, and many of its believers, were shot or sent to labor camps. Theological schools were closed, and church publications were prohibited. In the period between 1927 and 1940, the number of Orthodox Churches in the Russian Republic fell from 29,584 to fewer than 500. Between 1917 and 1940, 130,000 Orthodox priests were arrested. Father Pavel Florensky was one of the New-martyrs of this particular period.

After Nazi Germany's attack on the Soviet Union in 1941, Joseph Stalin revived the Russian Orthodox Church to intensify patriotic support for the war effort. By 1957 about 22,000 Russian Orthodox churches had become active. But in 1959 Nikita Khrushchev initiated his own campaign against the Russian Orthodox Church and forced the closure of about 12,000 churches. By 1985 fewer than 7,000 churches remained active.

In the Soviet Union, in addition to the methodical closing and destruction of churches, the charitable and social work formerly done by ecclesiastical authorities was taken over by the state. As with all private property, Church owned property was confiscated into public use. The few places of worship left to the Church were legally viewed as state property which the government permitted the church to use. After the advent of state funded universal education, the Church was not permitted to carry on educational, instructional activity for children. For adults, only training for church-related occupations was allowed. Outside of sermons during the celebration of the divine liturgy it could not instruct or evangelise to the faithful or its youth. Catechism classes, religious schools, study groups, Sunday schools and religious publications were all illegal and or banned. This persecution continued, even after the death of Stalin until the dissolution of the Soviet Union in 1991. This caused many religious tracts to be circulated as illegal literature or samizdat.
Since the fall of the Soviet Union there have been many New-martyrs added as Saints from the yoke.

One of the most striking developments in modern historical Orthodoxy is the dispersion of Orthodox Christians to the West. Emigration from Greece and the Near East in the last hundred years has created a sizable Orthodox diaspora in Western Europe, North and South America, and Australia. In addition, the Bolshevik Revolution forced thousands of Russian exiles westward. As a result, Orthodoxy's traditional frontiers have been profoundly modified. Millions of Orthodox are no longer geographically "eastern" since they live permanently in their newly adopted countries in the West. Nonetheless, they remain Eastern Orthodox in their faith and practice.

Liberal Christianity, sometimes called liberal theology, is an umbrella term covering diverse, philosophically informed religious movements and moods within late 18th, 19th and 20th-century Christianity. The word "liberal" in liberal Christianity does not refer to a leftist "political" agenda or set of beliefs, but rather to the freedom of dialectic process associated with continental philosophy and other philosophical and religious paradigms developed during the Age of Enlightenment.

Fundamentalist Christianity, is a movement that arose mainly within British and American Protestantism in the late 19th century and early 20th century in reaction to modernism and certain liberal Protestant groups that denied doctrines considered fundamental to Christianity yet still called themselves "Christian." Thus, fundamentalism sought to re-establish tenets that could not be denied without relinquishing a Christian identity, the "fundamentals": inerrancy of the Bible, Sola Scriptura, the Virgin Birth of Jesus, the doctrine of substitutionary atonement, the bodily Resurrection of Jesus, and the imminent return of Jesus Christ.

The position of Christians affected by Nazism is highly complex.

Regarding the matter, historian Derek Holmes wrote, "There is no doubt that the Catholic districts, resisted the lure of National Socialism [Nazism] far better than the Protestant ones." Pope Pius XI declared – "Mit brennender Sorge" – that Fascist governments had hidden "pagan intentions" and expressed the irreconcilability of the Catholic position and Totalitarian Fascist State Worship, which placed the nation above God, fundamental human rights and dignity. His declaration that "Spiritually, [Christians] are all Semites" prompted the Nazis to give him the title "Chief Rabbi of the Christian World."

Catholic priests were executed in concentration camps alongside Jews; for example, 2,600 Catholic Priests were imprisoned in Dachau, and 2,000 of them were executed. A further 2,700 Polish priests were executed (a quarter of all Polish priests), and 5,350 Polish nuns were either displaced, imprisoned, or executed. Many Catholic laymen and clergy played notable roles in sheltering Jews during the Holocaust, including Pope Pius XII (1876–1958). The head rabbi of Rome became a Catholic in 1945 and, in honour of the actions the Pope undertook to save Jewish lives, he took the name Eugenio (the pope's first name). A former Israeli consul in Italy claimed: "The Catholic Church saved more Jewish lives during the war than all the other churches, religious institutions, and rescue organisations put together."

The relationship between Nazism and Protestantism, especially the German Lutheran Church, was complex. Though many Protestant church leaders in Germany supported the Nazis' growing anti-Jewish activities, some, such as Dietrich Bonhoeffer (a Lutheran pastor) were strongly opposed to the Nazis. Bonhoeffer was later found guilty in the conspiracy to assassinate Hitler and executed.

On 11 October 1962, Pope John XXIII opened the Second Vatican Council, the 21st ecumenical council of the Catholic Church. The council was "pastoral" in nature, emphasising and clarifying already defined dogma, revising liturgical practices, and providing guidance for articulating traditional Church teachings in contemporary times. The council is perhaps best known for its instructions that the Mass may be celebrated in the vernacular as well as in Latin.

Ecumenism broadly refers to movements between Christian groups to establish a degree of unity through dialogue. ""Ecumenism"" is derived from Greek (oikoumene), which means "the inhabited world", but more figuratively something like "universal oneness." The movement can be distinguished into Catholic and Protestant movements, with the latter characterised by a redefined ecclesiology of "denominationalism" (which the Catholic Church, among others, rejects).

Over the last century, a number of moves have been made to reconcile the schism between the Catholic Church and the Eastern Orthodox churches. Although progress has been made, concerns over papal primacy and the independence of the smaller Orthodox churches has blocked a final resolution of the schism.

On 30 November 1894, Pope Leo XIII published the Apostolic Letter "Orientalium Dignitas" (On the Churches of the East) safeguarding the importance and continuance of the Eastern traditions for the whole Church. On 7 December 1965, a Joint Catholic-Orthodox Declaration of Pope Paul VI and the Ecumenical Patriarch Athenagoras I was issued lifting the mutual excommunications of 1054.

Some of the most difficult questions in relations with the ancient Eastern Churches concern some doctrine (i.e. "Filioque", Scholasticism, functional purposes of asceticism, the essence of God, Hesychasm, Fourth Crusade, establishment of the Latin Empire, Uniatism to note but a few) as well as practical matters such as the concrete exercise of the claim to papal primacy and how to ensure that ecclesiastical union would not mean mere absorption of the smaller Churches by the Latin component of the much larger Catholic Church (the most numerous single religious denomination in the world), and the stifling or abandonment of their own rich theological, liturgical and cultural heritage.

With respect to Catholic relations with Protestant communities, certain commissions were established to foster dialogue and documents have been produced aimed at identifying points of doctrinal unity, such as the Joint Declaration on the Doctrine of Justification produced with the Lutheran World Federation in 1999.

The final Great Awakening (1904 onwards) had its roots in the Holiness movement which had developed in the late 19th century. The Pentecostal revival movement began, out of a passion for more power and a greater outpouring of the Spirit. In 1902, the American evangelists Reuben Archer Torrey and Charles M. Alexander conducted meetings in Melbourne, Australia, resulting in more than 8,000 converts. News of this revival travelled fast, igniting a passion for prayer and an expectation that God would work in similar ways elsewhere.

Torrey and Alexander were involved in the beginnings of the great Welsh revival (1904) which led Jessie Penn-Lewis to witness the working of Satan during times of revival, and write her book "War on the Saints". In 1906, the modern Pentecostal Movement was born on Azusa Street in Los Angeles.

Another noteworthy development in 20th-century Christianity was the rise of the modern Pentecostal movement. Although its roots predate the year 1900, its actual birth is commonly attributed to the 20th century. Sprung from Methodist and Wesleyan roots, it arose out of the meetings at an urban mission on Azusa Street in Los Angeles. From there it spread around the world, carried by those who experienced what they believed to be miraculous moves of God there. These Pentecost-like manifestations have steadily been in evidence throughout the history of Christianity—such as seen in the two Great Awakenings that started in the United States. However, Azusa Street is widely accepted as the fount of the modern Pentecostal movement. Pentecostalism, which in turn birthed the Charismatic movement within already established denominations, continues to be an important force in western Christianity.

In reaction to these developments, Christian fundamentalism was a movement to reject the radical influences of philosophical humanism, as this was affecting the Christian religion. Especially targeting critical approaches to the interpretation of the Bible, and trying to blockade the inroads made into their churches by atheistic scientific assumptions, the fundamentalists began to appear in various denominations as numerous independent movements of resistance to the drift away from historic Christianity. Over time, the Fundamentalist Evangelical movement has divided into two main wings, with the label Fundamentalist following one branch, while Evangelical has become the preferred banner of the more moderate movement. Although both movements primarily originated in the English-speaking world, the majority of Evangelicals now live elsewhere in the world.

Ecumenical movements within Protestantism have focused on determining a list of doctrines and practices essential to being Christian and thus extending to all groups which fulfil these basic criteria a (more or less) co-equal status, with perhaps one's own group still retaining a "first among equal" standing. This process involved a redefinition of the idea of "the Church" from traditional theology. This ecclesiology, known as denominationalism, contends that each group (which fulfils the essential criteria of "being Christian") is a sub-group of a greater "Christian Church", itself a purely abstract concept with no direct representation, i.e., no group, or "denomination", claims to be "the Church." This ecclesiology is at variance with other groups that indeed consider themselves to be "the Church." The "essential criteria" generally consist of belief in the Trinity, belief that Jesus Christ is the only way to have forgiveness and eternal life, and that He died and rose again bodily.



The following links give an overview of the history of Christianity:

The following links provide quantitative data related to Christianity and other major religions, including rates of adherence at different points in time:


</doc>
<doc id="14121" url="https://en.wikipedia.org/wiki?curid=14121" title="Hertz">
Hertz

The hertz (symbol: Hz) is the derived unit of frequency in the International System of Units (SI) and is defined as one cycle per second. It is named for Heinrich Rudolf Hertz, the first person to provide conclusive proof of the existence of electromagnetic waves. Hertz are commonly expressed in multiples: kilohertz (10 Hz, kHz), megahertz (10 Hz, MHz), gigahertz (10 Hz, GHz), and terahertz (10 Hz, THz).

Some of the unit's most common uses are in the description of sine waves and musical tones, particularly those used in radio- and audio-related applications. It is also used to describe the speeds at which computers and other electronics are driven.

The hertz is defined as one cycle per second. The International Committee for Weights and Measures defined the second as "the duration of 9 192 631 770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom"

In English, "hertz" is also used as the plural form. As an SI unit, Hz can be prefixed; commonly used multiples are kHz (kilohertz, 10 Hz), MHz (megahertz, 10 Hz), GHz (gigahertz, 10 Hz) and THz (terahertz, 10 Hz). One hertz simply means "one cycle per second" (typically that which is being counted is a complete cycle); 100 Hz means "one hundred cycles per second", and so on. The unit may be applied to any periodic event—for example, a clock might be said to tick at 1 Hz, or a human heart might be said to beat at 1.2 Hz. The occurrence rate of aperiodic or stochastic events is expressed in reciprocal second or inverse second (1/s or s) in general or, in the specific case of radioactive decay, in becquerels. Whereas 1 Hz is 1 cycle per second, 1 Bq is 1 aperiodic radionuclide event per second.

Even though angular velocity, angular frequency and the unit hertz all have the dimension 1/s, angular velocity and angular frequency are not expressed in hertz, but rather in an appropriate angular unit such as radians per second. Thus a disc rotating at 60 revolutions per minute (rpm) is said to be rotating at either 2 rad/s "or" 1 Hz, where the former measures the angular velocity and the latter reflects the number of "complete" revolutions per second. The conversion between a frequency "f" measured in hertz and an angular velocity "ω" measured in radians per second is

The hertz is named after the German physicist Heinrich Hertz (1857–1894), who made important scientific contributions to the study of electromagnetism. The name was established by the International Electrotechnical Commission (IEC) in 1930. It was adopted by the General Conference on Weights and Measures (CGPM) ("Conférence générale des poids et mesures") in 1960, replacing the previous name for the unit, "cycles per second" (cps), along with its related multiples, primarily "kilocycles per second" (kc/s) and "megacycles per second" (Mc/s), and occasionally "kilomegacycles per second" (kMc/s). The term "cycles per second" was largely replaced by "hertz" by the 1970s. One hobby magazine, "Electronics Illustrated", declared their intention to stick with the traditional kc., Mc., etc. units.

Sound is a traveling longitudinal wave which is an oscillation of pressure. Humans perceive frequency of sound waves as pitch. Each musical note corresponds to a particular frequency which can be measured in hertz. An infant's ear is able to perceive frequencies ranging from 20 Hz to 20,000 Hz; the average adult human can hear sounds between 20 Hz and 16,000 Hz. The range of ultrasound, infrasound and other physical vibrations such as molecular and atomic vibrations extends from a few femtohertz into the terahertz range and beyond.

Electromagnetic radiation is often described by its frequency—the number of oscillations of the perpendicular electric and magnetic fields per second—expressed in hertz.

Radio frequency radiation is usually measured in kilohertz (kHz), megahertz (MHz), or gigahertz (GHz). Light is electromagnetic radiation that is even higher in frequency, and has frequencies in the range of tens (infrared) to thousands (ultraviolet) of terahertz. Electromagnetic radiation with frequencies in the low terahertz range (intermediate between those of the highest normally usable radio frequencies and long-wave infrared light) is often called terahertz radiation. Even higher frequencies exist, such as that of gamma rays, which can be measured in exahertz. (For historical reasons, the frequencies of light and higher frequency electromagnetic radiation are more commonly specified in terms of their wavelengths or photon energies: for a more detailed treatment of this and the above frequency ranges, see electromagnetic spectrum.)

In computers, most central processing units (CPU) are labeled in terms of their clock rate expressed in megahertz (10 Hz) or gigahertz (10 Hz). This specification refers to the frequency of the CPU's master clock signal. This signal is a square wave, which is an electrical voltage that switches between low and high logic values at regular intervals. As the hertz has become the primary unit of measurement accepted by the general populace to determine the performance of a CPU, many experts have criticized this approach, which they claim is an easily manipulable benchmark. Some processors use multiple clock periods to perform a single operation, while others can perform multiple operations in a single cycle. For personal computers, CPU clock speeds have ranged from approximately 1 MHz in the late 1970s (Atari, Commodore, Apple computers) to up to 6 GHz in IBM POWER microprocessors.

Various computer buses, such as the front-side bus connecting the CPU and northbridge, also operate at various frequencies in the megahertz range.

Higher frequencies than the International System of Units provides prefixes for are believed to occur naturally in the frequencies of the quantum-mechanical vibrations of high-energy, or, equivalently, massive particles, although these are not directly observable and must be inferred from their interactions with other phenomena. By convention, these are typically not expressed in hertz, but in terms of the equivalent quantum energy, which is proportional to the frequency by the factor of Planck's constant.




</doc>
<doc id="14123" url="https://en.wikipedia.org/wiki?curid=14123" title="Heroic couplet">
Heroic couplet

A heroic couplet is a traditional form for English poetry, commonly used in epic and narrative poetry, and consisting of a rhyming pair of lines in iambic pentameter. Use of the heroic couplet was pioneered by Geoffrey Chaucer in the "Legend of Good Women" and the "Canterbury Tales", and generally considered to have been perfected by John Dryden and Alexander Pope in the Restoration Age and early 18th century respectively. 

A frequently-cited example illustrating the use of heroic couplets is this passage from "Cooper's Hill" by John Denham, part of his description of the Thames:

The term "heroic couplet" is sometimes reserved for couplets that are largely "closed" and self-contained, as opposed to the enjambed couplets of poets like John Donne. The heroic couplet is often identified with the English Baroque works of John Dryden and Alexander Pope, who used the form for their translations of the epics of Virgil and Homer, respectively. Major poems in the closed couplet, apart from the works of Dryden and Pope, are Samuel Johnson's "The Vanity of Human Wishes", Oliver Goldsmith's "The Deserted Village", and John Keats's "Lamia". The form was immensely popular in the 18th century. The looser type of couplet, with occasional enjambment, was one of the standard verse forms in medieval narrative poetry, largely because of the influence of the Canterbury Tales.

English heroic couplets, especially in Dryden and his followers, are sometimes varied by the use of the occasional alexandrine, or hexameter line, and triplet. Often these two variations are used together to heighten a climax. The breaking of the regular pattern of rhyming pentameter pairs brings about a sense of poetic closure. Here are two examples from Book IV of Dryden's translation of the "Aeneid".
Twentieth-century authors have occasionally made use of the heroic couplet, often as an allusion to the works of poets of previous centuries. An example of this is Vladimir Nabokov's novel "Pale Fire", the second section of which is a 999-line, 4-canto poem largely written in loose heroic couplets with frequent enjambment. Here is an example from the first canto:


</doc>
<doc id="14127" url="https://en.wikipedia.org/wiki?curid=14127" title="Höðr">
Höðr

Höðr (Old Norse: "Hǫðr" [hɔðr]; often anglicized as Hod, Hoder, or Hodur) is a blind god, the twin brother of Baldr and a son of Odin and Frigg in Norse mythology. Tricked and guided by Loki, he shot the mistletoe arrow which was to slay the otherwise invulnerable Baldr.

According to the "Prose Edda" and the "Poetic Edda", the goddess Frigg, Baldr's mother, made everything in existence swear never to harm Baldr, except for the mistletoe, which she found too unimportant to ask (alternatively, which she found too young to demand an oath from). The gods amused themselves by trying weapons on Baldr and seeing them fail to do any harm. Loki, the mischief-maker, upon finding out about Baldr's one weakness, made a spear from mistletoe, and helped Höðr shoot it at Baldr. In reaction to this, Odin and the giantess Rindr gave birth to Váli, who grew to adulthood within a day and slew Höðr.

The Danish historian Saxo Grammaticus recorded an alternative version of this myth in his "Gesta Danorum". In this version, the mortal hero Høtherus and the demi-god "Balderus" compete for the hand of Nanna. Ultimately, Høtherus slays Balderus.

In the "Gylfaginning" part of Snorri Sturluson's Prose Edda Höðr is introduced in an ominous way.

Höðr is not mentioned again until the prelude to Baldr's death is described. All things except
the mistletoe (believed to be harmless) have sworn an oath not to harm Baldr, so the Æsir throw missiles at him for sport.

The "Gylfaginning" does not say what happens to Höðr after this. In fact it specifically states that Baldr cannot be avenged, at least not immediately.

It does seem, however, that Höðr ends up in Hel one way or another for the last mention of him in "Gylfaginning" is in the description of the post-Ragnarök world.

Snorri's source of this knowledge is clearly "Völuspá" as quoted below.

In the "Skáldskaparmál" section of the Prose Edda several kennings for Höðr are related.

None of those kennings, however, are actually found in surviving skaldic poetry. Neither are Snorri's kennings for Váli, which are also of interest in this context.

It is clear from this that Snorri was familiar with the role of Váli as Höðr's slayer, even though he does not relate that myth in the "Gylfaginning" prose. Some scholars have speculated that he found it distasteful since Höðr is essentially innocent in his version of the story.

Höðr is referred to several times in the Poetic Edda, always in the context of Baldr's death. The following strophes are from "Völuspá".

This account seems to fit well with the information in the Prose Edda, but here the role of Baldr's avenging brother is emphasized.

Baldr and Höðr are also mentioned in "Völuspá"'s description of the world after Ragnarök.

The poem "Vafþrúðnismál" informs us that the gods who survive Ragnarök are Viðarr, Váli, Móði and Magni with no mention of Höðr and Baldr.

The myth of Baldr's death is also referred to in another Eddic poem, "Baldrs draumar".

Höðr is not mentioned again by name in the Eddas. He is, however, referred to in "Völuspá in skamma".

The name of Höðr occurs several times in skaldic poetry as a part of warrior-kennings. Thus "Höðr brynju", "Höðr of byrnie", is a warrior and so is "Höðr víga", "Höðr of battle". Some scholars have found the fact that the poets should want to compare warriors with Höðr to be incongruous with Snorri's description of him as a blind god, unable to harm anyone without assistance. It is possible that this indicates that some of the poets were familiar with other myths about Höðr than the one related in "Gylfaginning" - perhaps some where Höðr has a more active role. On the other hand, the names of many gods occur in kennings and the poets might not have been particular in using any god name as a part of a kenning.

In "Gesta Danorum" Hotherus is a human hero of the Danish and Swedish royal lines. He is gifted in swimming, archery, fighting and music and Nanna, daughter of King Gevarus falls in love with him. But at the same time Balderus, son of Othinus, has caught sight of Nanna bathing and fallen violently in love with her. He resolves to slay Hotherus, his rival.

Out hunting, Hotherus is led astray by a mist and meets wood-maidens who control the fortunes of war. They warn him that Balderus has designs on Nanna but also tell him that he shouldn't attack him in battle since he is a demigod. Hotherus goes to consult with King Gevarus and asks him for his daughter. The king replies that he would gladly favour him but that Balderus has already made a like request and he does not want to incur his wrath.

Gevarus tells Hotherus that Balderus is invincible but that he knows of one weapon which can defeat him, a sword kept by Mimingus, the satyr of the woods. Mimingus also has another magical artifact, a bracelet that increases the wealth of its owner. Riding through a region of extraordinary cold in a carriage drawn by reindeer, Hotherus captures the satyr with a clever ruse and forces him to yield his artifacts.

Hearing about Hotherus's artifacts, Gelderus, king of Saxony, equips a fleet to attack him. Gevarus warns Hotherus of this and tells him where to meet Gelderus in battle. When the battle is joined, Hotherus and his men save their missiles while defending themselves against those of the enemy with a testudo formation. With his missiles exhausted, Gelderus is forced to sue for peace. He is treated mercifully by Hotherus and becomes his ally. Hotherus then gains another ally with his eloquent oratory by helping King Helgo of Hålogaland win a bride.

Meanwhile, Balderus enters the country of king Gevarus armed and sues for Nanna. Gevarus tells him to learn Nanna's own mind. Balderus addresses her with cajoling words but is refused. Nanna tells him that because of the great difference in their nature and stature, since he is a demigod, they are not suitable for marriage.

As news of Balderus's efforts reaches Hotherus, he and his allies resolve to attack Balderus. A great naval battle ensues where the gods fight on the side of Balderus. Thoro in particular shatters all opposition with his mighty club. When the battle seems lost, Hotherus manages to hew Thoro's club off at the haft and the gods are forced to retreat. Gelderus perishes in the battle and Hotherus arranges a funeral pyre of vessels for him. After this battle Hotherus finally marries Nanna.

Balderus is not completely defeated and shortly afterwards returns to defeat Hotherus in the field. But Balderus's victory is without fruit for he is still without Nanna. Lovesick, he is harassed by phantoms in Nanna's likeness and his health deteriorates so that he cannot walk but has himself drawn around in a cart.

After a while Hotherus and Balderus have their third battle and again Hotherus is forced to retreat. Weary of life because of his misfortunes, he plans to retire and wanders into the wilderness. In a cave he comes upon the same maidens he had met at the start of his career. Now they tell him that he can defeat Balderus if he gets a taste of some extraordinary food which had been devised to increase the strength of Balderus.

Encouraged by this, Hotherus returns from exile and once again meets Balderus in the field. After a day of inconclusive fighting, he goes out during the night to spy on the enemy. He finds where Balderus's magical food is prepared and plays the lyre for the maidens preparing it. While they don't want to give him the food, they bestow on him a belt and a girdle which secure victory.

Heading back to his camp, Hotherus meets Balderus and plunges his sword into his side. After three days, Balderus dies from his wound. Many years later, Bous, the son of Othinus and Rinda, avenges his brother by killing Hotherus in a duel.

There are also two lesser-known DanishLatin chronicles, the "Chronicon Lethrense" and the "Annales Lundenses", of which the latter is included in the former. These two sources provide a second euhemerized account of Höðr's slaying of Balder.

It relates that Hother was the king of the Saxons, son of Hothbrod, the daughter of Hadding. Hother first slew Othen's (i.e., Odin's) son Balder in battle and then chased Othen and Thor. Finally, Othen's son Both killed Hother. Hother, Balder, Othen, and Thor were incorrectly considered to be gods.

According to the Swedish mythologist and romantic poet Viktor Rydberg, the story of Baldr's death was taken from Húsdrápa, a poem composed by Ulfr Uggason around 990 AD at a feast thrown by the Icelandic Chief Óláfr Höskuldsson to celebrate the finished construction of his new home, Hjarðarholt, the walls of which were filled with symbolic representations of the Baldr myth among others. Rydberg suggested that Höðr was depicted with eyes closed and Loki guiding his aim to indicate that Loki was the true cause of Baldr's death and Höðr was only his "blind tool." Rydberg theorized that the author of the "Gylfaginning" then mistook the description of the symbolic artwork in the Húsdrápa as the actual tale of Baldr's death.




</doc>
<doc id="14128" url="https://en.wikipedia.org/wiki?curid=14128" title="Herat">
Herat

Herat (; ,"Harât" ",Herât"; ; , "Alexándreia hē en Aríois"; ) is the third-largest city of Afghanistan. It has a population of about 436,300, and serves as the capital of Herat Province, situated in the fertile valley of the Hari River. It is linked with Kandahar and Mazar-e-Sharif via Highway 1 or the ring road. It is further linked to the city of Mashhad in neighboring Iran through the border town of Islam Qala, and to Turkmenistan through the border town of Torghundi, both about away.

Herat dates back to the Avestan times and was traditionally known for its wine. The city has a number of historic sites, including the Herat Citadel and the Musallah Complex. During the Middle Ages Herat became one of the important cities of Khorasan, as it was known as the "Pearl of Khorasan". It has been governed by various Afghan rulers since the early 18th century. In 1717, the city was invaded by the Hotaki forces until they were expelled by the Afsharids in 1729. After Nader Shah's death and Ahmad Shah Durrani's rise to power in 1747, Herat became part of Afghanistan. It witnessed some political disturbances and military invasions during the early half of the 19th century but the 1857 Treaty of Paris ended hostilities of the Anglo-Persian War.

Herat lies on the ancient trade routes of the Middle East, Central and South Asia, and today is a regional hub in western Afghanistan. The roads from Herat to Iran, Turkmenistan, and other parts of Afghanistan are still strategically important. As the gateway to Iran, it collects high amount of customs revenue for Afghanistan. It also has an international airport. The city has high residential density clustered around the core of the city. However, vacant plots account for a higher percentage of the city (21%) than residential land use (18%) and agricultural is the largest percentage of total land use (36%). Today the city is considered to be relatively safe.

Herat dates back to ancient times (its exact age remains unknown). During the period of the Achaemenid Empire (ca. 550-330 BC), the surrounding district was known as "Haraiva" (in Old Persian), and in classical sources the region was correspondingly known as Aria (Areia). In the Zoroastrian Avesta, the district is mentioned as "Haroiva". The name of the district and its main town is derived from that of the chief river of the region, the Herey River (Old Dari "Hereyrud", "Silken Water"), which traverses the district and passes some south of modern Herāt. Herey is mentioned in Sanskrit as yellow or golden color equivalent to Persian "Zard" meaning Gold (yellow). The naming of a region and its principal town after the main river is a common feature in this part of the world—compare the adjoining districts/rivers/towns of Arachosia and Bactria.

The district "Aria" of the Achaemenid Empire is mentioned in the provincial lists that are included in various royal inscriptions, for instance, in the Behistun inscription of Darius I (ca. 520 BC). Representatives from the district are depicted in reliefs, e.g., at the royal Achaemenid tombs of Naqsh-e Rustam and Persepolis. They are wearing Scythian-style dress (with a tunic and trousers tucked into high boots) and a twisted Bashlyk that covers their head, chin and neck.

Hamdallah Mustawfi, composer of the 14th century work "The Geographical Part of the Nuzhat-al-Qulub" writes that:

Herodotus described Herat as "the bread-basket of Central Asia". At the time of Alexander the Great in 330 BC, Aria was obviously an important district. It was administered by a satrap called Satibarzanes, who was one of the three main Persian officials in the East of the Empire, together with the satrap Bessus of Bactria and Barsaentes of Arachosia. In late 330 BC, Alexander captured the Arian capital that was called Artacoana. The town was rebuilt and the citadel was constructed. Afghanistan became part of the Seleucid Empire after Alexander died, which formed an alliance with the Indian Maurya Empire. Roman Historian Strabo writes that the Seleucids later gave the area south of the Hindu Kush to the Mauryas after a treaty was made.

However, most sources suggest that Herat was predominantly Zoroastrian. It became part of the Parthian Empire in 167 BC. In the Sasanian period (226-652), "Harēv" is listed in an inscription on the Ka'ba-i Zartosht at Naqsh-e Rustam; and "Hariy" is mentioned in the Pahlavi catalogue of the provincial capitals of the empire. In around 430, the town is also listed as having a Christian community, with a Nestorian bishop.

In the last two centuries of Sasanian rule, Aria (Herat) had great strategic importance in the endless wars between the Sasanians, the Chionites and the Hephthalites who had been settled in the northern section of Afghanistan since the late 4th century.

At the time of the Arab invasion in the middle of the 7th century, the Sasanian central power seemed already largely nominal in the province in contrast with the role of the Hephthalites tribal lords, who were settled in the Herat region and in the neighboring districts, mainly in pastoral Bādghis and in Qohestān. It must be underlined, however, that Herat remained one of the three Sasanian mint centers in the east, the other two being Balkh and Marv. The Hephthalites from Herat and some unidentified Turks opposed the Arab forces in a battle of Qohestān in 651-52 AD, trying to block their advance on Nishāpur, but they were defeated

When the Arab armies appeared in Khorāsān in the 650s AD, Herāt was counted among the twelve capital towns of the Sasanian Empire. The Arab army under the general command of Ahnaf ibn Qais in its conquest of Khorāsān in 652 seems to have avoided Herāt, but it can be assumed that the city eventually submitted to the Arabs, since shortly afterwards an Arab governor is mentioned there. A treaty was drawn in which the regions of Bādghis and Bushanj were included. As did many other places in Khorāsān, Herāt rebelled and had to be re-conquered several times.

Another power that was active in the area in the 650s was Tang dynasty China which had embarked on a campaign that culminated in the Conquest of the Western Turks. By 659-661, the Tang claimed a tenuous suzerainty over Herat, the westernmost point of Chinese power in its long history. This hold however would be ephemeral with local Turkish tribes rising in rebellion in 665 and driving out the Tang.

In 702 AD Yazid ibn al-Muhallab defeated certain Arab rebels, followers of Ibn al-Ash'ath, and forced them out of Herat. The city was the scene of conflicts between different groups of Muslims and Arab tribes in the disorders leading to the establishment of the Abbasid Caliphate. Herat was also a centre of the followers of Ustadh Sis.

In 870 AD, Yaqub ibn Layth Saffari, a local ruler of the Saffarid dynasty conquered Herat and the rest of the nearby regions in the name of Islam.

The region of Herāt was under the rule of King Nuh III, the seventh of the Samanid line—at the time of Sebük Tigin and his older son, Mahmud of Ghazni. The governor of Herāt was a noble by the name of "Faik", who was appointed by Nuh III. It is said that Faik was a powerful, but insubordinate governor of Nuh III; and had been punished by Nuh III. Faik made overtures to Bogra Khan and Ughar Khan of Khorasan. Bogra Khan answered Faik's call, came to Herāt and became its ruler. The Samanids fled, betrayed at the hands of Faik to whom the defence of Herāt had been entrusted by Nuh III. In 994, Nuh III invited Alp Tigin to come to his aid. Alp Tigin, along with Mahmud of Ghazni, defeated Faik and annexed Herāt, Nishapur and Tous.

Herat was a great trading centre strategically located on trade routes from Mediterranean to India or to China. The city was noted for its textiles during the Abbasid Caliphate, according to many references by geographers. Herāt also had many learned sons such as Ansārī. The city is described by Estakhri and Ibn Hawqal in the 10th century as a prosperous town surrounded by strong walls with plenty of water sources, extensive suburbs, an inner citadel, a congregational mosque, and four gates, each gate opening to a thriving market place. The government building was outside the city at a distance of about a mile in a place called Khorāsānābād. A church was still visible in the countryside northeast of the town on the road to Balkh, and farther away on a hilltop stood a flourishing fire temple, called Sereshk, or Arshak according to Mustawfi.

Herat was a part of the Taherid dominion in Khorāsān until the rise of the Saffarids in Sistān under Ya'qub-i Laith in 861, who, in 862, started launching raids on Herat before besieging and capturing it on 16 August 867, and again in 872. The Saffarids succeeded in expelling the Taherids from Khorasan in 873.

The Sāmānid dynasty was established in Transoxiana by three brothers, Nuh, Yahyā, and Ahmad. Ahmad Sāmāni opened the way for the Samanid dynasty to the conquest of Khorāsān, including Herāt, which they were to rule for one century. The centralized Samanid administration served as a model for later dynasties. The Samanid power was destroyed in 999 by the Qarakhanids, who were advancing on Transoxiana from the northeast, and by the Ghaznavids, former Samanid retainers, attacking from the southeast.

Sultan Maḥmud of Ghazni officially took control of Khorāsān in 998. Herat was one of the six Ghaznavid mints in the region. In 1040, Herat was captured by the Seljuk Empire. Yet, in 1175, it was captured by the Ghurids of Ghor and then came under the Khawarazm Empire in 1214. According to the account of Mustawfi, Herat flourished especially under the Ghurid dynasty in the 12th century. Mustawfi reported that there were "359 colleges in Herat, 12,000 shops all fully occupied, 6,000 bath-houses; besides caravanserais and mills, also a darwish convent and a fire temple". There were about 444,000 houses occupied by a settled population. The men were described as "warlike and carry arms", and they were Sunni Muslims. The great mosque of Herāt was built by Ghiyas ad-Din Ghori in 1201. In this period Herāt became an important center for the production of metal goods, especially in bronze, often decorated with elaborate inlays in precious metals.

Herat was invaded and destroyed by Genghis Khan's Mongol army in 1221. The city was destroyed a second time and remained in ruins from 1222 to about 1236. In 1244 a local prince Shams al-Din Kart was named ruler of Herāt by the Mongol governor of Khorāsān and in 1255 he was confirmed in his rule by the founder of the Il-Khan dynasty Hulagu. Shams al-Din founded a new dynasty and his successors, especially Fakhr-al-Din and Ghiyath al-Din, built many mosques and other buildings. The members of this dynasty were great patrons of literature and the arts. By this time Herāt became known as the "pearl of Khorasan".

Timur took Herat in 1380 and he brought the Kartid dynasty to an end a few years later. The city reached its greatest glory under the Timurid princes, especially Sultan Husayn Bayqara who ruled Herat from 1469 until May 4, 1506. His chief minister, the poet and author in Persian and Turkish, Mir Ali-Shir Nava'i was a great builder and patron of the arts. Under the Timurids, Herat assumed the role of the main capital of an empire that extended in the West as far as central Persia. As the capital of the Timurid empire, it boasted many fine religious buildings and was famous for its sumptuous court life and musical performance and its tradition of miniature paintings. On the whole, the period was one of relative stability, prosperity, and development of economy and cultural activities. It began with the nomination of Shahrokh, the youngest son of Timur, as governor of Herat in 1397. The reign of Shahrokh in Herat was marked by intense royal patronage, building activities, and promotion of manufacturing and trade, especially through the restoration and enlargement of the Herat's bāzār. The present Musallah Complex, and many buildings such as the madrasa of Goharshad, Ali Shir mahāl, many gardens, and others, date from this time. The village of Gazar Gah, over two km northeast of Herat, contained a shrine which was enlarged and embellished under the Timurids. The tomb of the poet and mystic Khwājah Abdullāh Ansārī (d. 1088), was first rebuilt by Shahrokh about 1425, and other famous men were buried in the shrine area. Herat was shortly captured by Kara Koyunlu between 1458–1459.

In 1507 Herat was occupied by the Uzbeks but after much fighting the city was taken by Shah Isma'il, the founder of the Safavid dynasty, in 1510 and the Shamlu Qizilbash assumed the governorship of the area. Under the Safavids, Herat was again relegated to the position of a provincial capital, albeit one of a particular importance. At the death of Shah Isma'il the Uzbeks again took Herat and held it until Shah Tahmasp retook it in 1528. The Persian king, Abbas was born in Herat, and in Safavid texts, Herat is referred to as "a'zam-i bilād-i īrān", meaning "the greatest of the cities of Iran". In the 16th century, all future Safavid rulers, from Tahamsp I to Abbas I, were governors of Herat in their youth.

By the early 18th century Herat was governed by various Hotaki and Abdali Afghans. After Nader Shah's death in 1747, Ahmad Shah Durrani took possession of the city and became part of the Durrani Empire.

In 1824, Herat became independent for several years when the Afghan Empire was split between the Durranis and Barakzais. The Persians invaded the city in 1838, but the British helped the Afghans in repelling them. In 1856, they invaded again, and briefly managed to retake the city; it led directly to the Anglo-Persian War. In 1857 hostilities between the Persians and the British ended after the Treaty of Paris was signed, and the Persian troops withdrew from Herat.

One of the greatest tragedies for the Afghans and Muslims was the British invasion of, and subsequent destruction of the Islamic Musallah complex in Herat in 1885. The officially stated reason was to get a good line of sight for their artillery against Russian invaders who never came. This was but one small sidetrack in the Great Game, a century-long conflict between the British Empire and the Russian Empire in 19th century.

In the 1960s, engineers from the United States built Herat Airport, which was used by the Soviet forces during the Democratic Republic of Afghanistan in the 1980s. Even before the Soviet invasion at the end of 1979, there was a substantial presence of Soviet advisors in the city with their families.

Between March 10 and March 20, 1979, the Afghan Army in Herāt under the control of commander Ismail Khan mutinied. Thousands of protesters took to the streets against the Khalq communist regime's oppression led by Nur Mohammad Taraki. The new rebels led by Khan managed to oust the communists and take control of the city for 3 days, with some protesters murdering any Soviet advisers. This shocked the government, who blamed the new administration of Iran following the Iranian Revolution for influencing the uprising. Reprisals by the government followed, and between 3,000 and 24,000 people (according to different sources) were killed, in what is called the 1979 Herat uprising, or in Persian as the "Qiam-e Herat". The city itself was recaptured with tanks and airborne forces, but at the cost of thousands of civilians killed. This massacre was the first of its kind since the country's independence in 1919, and was the bloodiest event preceding the Soviet–Afghan War.

Herat received damage during the Soviet–Afghan War in the 1980s, especially its western side. The province as a whole was one of the worst-hit. In April 1983, a series of Soviet bombings damaged half of the city and killed around 3,000 civilians, described as "extremely heavy, brutal and prolonged". Ismail Khan was the leading mujahideen commander in Herāt fighting against the Soviet-backed government.

After the communist government's collapse in 1992, Khan joined the new government and he became governor of Herat Province. The city was relatively safe and it was recovering and rebuilding from the damage caused in the Soviet–Afghan War. However, on September 5, 1995, the city was captured by the Taliban without much resistance, forcing Khan to flee. Herat became the first Persian-speaking city to be captured by the Taliban. The Taliban's strict enforcement of laws confining women at home and closing girls' schools alienated Heratis who are traditionally more liberal and educated, like the Kabulis, than other urban populations in the country. Two days of anti-Taliban protests occurred in December 1996 which was violently dispersed and led to the imposition of a curfew.

After the U.S. invasion of Afghanistan, on November 12, 2001, it was captured from the Taliban by forces loyal to the Northern Alliance and Ismail Khan returned to power (see Battle of Herat). In 2004, Mirwais Sadiq, Aviation Minister of Afghanistan and the son of Ismail Khan, was ambushed and killed in Herāt by a local rival group. More than 200 people were arrested under suspicion of involvement.

In 2005, the International Security Assistance Force (ISAF) began establishing bases in and around the city. Its main mission was to train the Afghan National Security Forces (ANSF) and help with the rebuilding process of the country. Regional Command West, led by Italy, assisted the Afghan National Army (ANA) 207th Corps. Herat was one of the first seven areas that transitioned security responsibility from NATO to Afghanistan. In July 2011, the Afghan security forces assumed security responsibility from NATO.

Due to their close relations, Iran began investing in the development of Herat's power, economy and education sectors. In the meantime, the United States built a consulate in Herat to help further strengthen its relations with Afghanistan. In addition to the usual services, the consulate works with the local officials on development projects and with security issues in the region.

Herat has a cold semi-arid climate (Köppen climate classification "BSk"). Precipitation is very low, and mostly falls in winter. Although Herāt is approximately lower than Kandahar, the summer climate is more temperate, and the climate throughout the year is far from disagreeable, although winter temperatures are comparably lower. From May to September, the wind blows from the northwest with great force. The winter is tolerably mild; snow melts as it falls, and even on the mountains does not lie long. Three years out of four it does not freeze hard enough for the people to store ice. The eastern reaches of the Hari River, including the rapids, are frozen hard in the winter, and people travel on it as on a road.

India, Iran and Pakistan operate their consulate here for trade, military and political links.



Of the more than dozen minarets that once stood in Herāt, many have been toppled from war and neglect over the past century. Recently, however, everyday traffic threatens many of the remaining unique towers by shaking the very foundations they stand on. Cars and trucks that drive on a road encircling the ancient city rumble the ground every time they pass these historic structures. UNESCO personnel and Afghan authorities have been working to stabilize the Fifth Minaret.


The population of Herat numbers approximately 436,300 as of 2013. It is a multi-ethnic society with Persian-speakers as the majority. There is no current data on the precise ethnic make-over but according to a 2003 map found in the National Geographic Magazine, the percentage figure of ethnic groups was given as follows: 85% Tajik, 10% Pashtuns, 2% Hazaras, 2% Uzbeks and 1% Turkmens.

Persian serves as the lingua franca of the city. It is the native language of Herat and the local dialect – known by natives as "Herātī" – belongs to the "Khorāsānī" cluster within Persian. It is akin to the Persian dialects of eastern Iran, notably those of Mashhad and Khorasan Province. The second language that is understood by many is Pashto, which is the native language of the Pashtuns. The local Pashto dialect spoken in Herat is a variant of western Pashto, which is also spoken in Kandahar and southern and western Afghanistan. Religiously, Sunni Islam is practiced by the majority while Shias make up the minority.

The city once had a Jewish community. About 280 families lived in Herat as of 1948 but most moved to Israel that year, and the community disappeared by 1992. There are four former synagogues in the city's old quarter, which were neglected but in the late 2000s renovated by the Aga Khan Trust for Culture, three of them turning into nurseries and schools. The Jewish cemetery is being taken care of by Jalilahmed Abdelaziz.


Herat International Airport was built by engineers from the United States in the 1960s and was used by the Soviet Armed Forces during the Soviet–Afghan War in the 1980s. It was bombed in late 2001 during Operation Enduring Freedom but had been rebuilt within the next decade. The runway of the airport has been extended and upgraded and as of August 2014 there were regularly scheduled direct flights to Delhi, Dubai, Mashad, and various airports in Afghanistan. At least five airlines operated regularly scheduled direct flights to Kabul.

Rail connections to and from Herat were proposed many times, during "The Great Game" of the 19th century and again in the 1970s and 1980s, but nothing came to life. In February 2002, Iran and the Asian Development Bank announced funding for a railway connecting Torbat-e Heydarieh in Iran to Herat. This was later changed to begin in Khaf in Iran, a railway for both cargo and passengers, with work on the Iranian side of the border starting in 2006. Construction is underway in the Afghan side and it is estimated to be completed by March 2018. There is also the prospect of an extension across Afghanistan to Sher Khan Bandar.

The AH76 highway connects Herat to Maymana and the north. The AH77 connects it east towards Chaghcharan and north towards Mary in Turkmenistan. Highway 1 (part of Asian highway AH1) links it to Mashhad in Iran to the northwest, and south via the Kandahar–Herat Highway to Delaram.





 


</doc>
<doc id="14130" url="https://en.wikipedia.org/wiki?curid=14130" title="Hedeby">
Hedeby

Hedeby (, Old Norse "Heiðabýr", German "Haithabu") was an important Viking Age (8th to the 11th centuries) trading settlement near the southern end of the Jutland Peninsula, now in the Schleswig-Flensburg district of Schleswig-Holstein, Germany. It is the most important archaeological site in Schleswig-Holstein. 

The settlement developed as a trading centre at the head of a narrow, navigable inlet known as the Schlei, which connects to the Baltic Sea. The location was favorable because there is a short portage of less than 15 km to the Treene River, which flows into the Eider with its North Sea estuary, making it a convenient place where goods and ships could be pulled on a corduroy road overland for an almost uninterrupted seaway between the Baltic and the North Sea and avoid a dangerous and time-consuming circumnavigation of Jutland, providing Hedeby with a role similar to later Lübeck.
Hedeby was the second largest Nordic town during the Viking Age, after Uppåkra in present-day southern Sweden,
The city of Schleswig was later founded on the other side of the Schlei. Hedeby was abandoned after its destruction in 1066.

Hedeby was rediscovered in the late 19th century and excavations began in 1900. The Haithabu Museum was opened next to the site in 1985.

The Old Norse name "Heiða-býr" simply translates to "heath-settlement" ("heiðr" "heath" and "býr" = "yard; settlement, village, town"). The name is recorded in numerous spelling variants.


The old name of the nearby town of Schleswig is "Sliesthorp" (later "Sliaswich", c.f. "-thorp" vs. "-wich"). It is possible that the two names were used interchangeably for the same settlement, depending on which language was being used (Old Saxon vs. Old Norse).

Hedeby is first mentioned in the Frankish chronicles of Einhard (804) who was in the service of Charlemagne,
but was probably founded around 770. In 808 the Danish king Godfred (Lat. Godofredus) destroyed a competing Slav trade centre named Reric, and it is recorded in the Frankish chronicles that he moved the merchants from there to Hedeby. This may have provided the initial impetus for the town to develop. The same sources record that Godfred strengthened the Danevirke, an earthen wall that stretched across the south of the Jutland peninsula. The Danevirke joined the defensive walls of Hedeby to form an east-west barrier across the peninsula, from the marshes in the west to the Schlei inlet leading into the Baltic in the east.

The town itself was surrounded on its three landward sides (north, west, and south) by earthworks. At the end of the 9th century the northern and southern parts of the town were abandoned for the central section. Later a 9-metre (29-ft) high semi-circular wall was erected to guard the western approaches to the town. On the eastern side, the town was bordered by the innermost part of the Schlei inlet and the bay of Haddebyer Noor.

Hedeby became a principal marketplace because of its geographical location on the major trade routes between the Frankish Empire and Scandinavia (north-south), and between the Baltic and the North Sea (east-west). Between 800 and 1000 the growing economic power of the Vikings led to its dramatic expansion as a major trading centre.

The following indicate the importance achieved by the town:

A Swedish dynasty founded by Olof the Brash is said to have ruled Hedeby during the last decades of the 9th century and the first part of the 10th century. This was told to Adam of Bremen by the Danish king Sweyn Estridsson, and it is supported by three runestones found in Denmark. Two of them were raised by the mother of Olof's grandson Sigtrygg Gnupasson. The third runestone, discovered in 1796, is from Hedeby, the "Stone of Eric" (). It is inscribed with Norwegian-Swedish runes. It is, however, possible that Danes also occasionally wrote with this version of the younger futhark.

Life was short and crowded in Hedeby. The small houses were clustered tightly together in a grid, with the east-west streets leading down to jetties in the harbour. People rarely lived beyond 30 or 40, and archaeological research shows that their later years were often painful due to crippling diseases such as tuberculosis. Yet make-up for men and rights for women provide surprises to the modern understanding.

Al-Tartushi, a late 10th-century traveller from al-Andalus, provides one of the most colourful and often quoted descriptions of life in Hedeby. Al-Tartushi was from Cordoba in Spain, which had a significantly more wealthy and comfortable lifestyle than Hedeby. While Hedeby may have been significant by Scandinavian standards, Al-Tartushi was unimpressed:

The town was sacked in 1050 by King Harald Hardrada of Norway during a conflict with King Sweyn II of Denmark. He set the town on fire by sending several burning ships into the harbour, the charred remains of which were found at the bottom of the Schlei during recent excavations. A Norwegian "skald", quoted by Snorri Sturluson, describes the sack as follows:

In 1066 the town was sacked and burned by West Slavs. Following the destruction, Hedeby was slowly abandoned. People moved across the Schlei inlet, which separates the two peninsulas of Angeln and Schwansen, and founded the town of Schleswig.

After the settlement was abandoned, rising waters contributed to the complete disappearance of all visible structures on the site. It was even forgotten where the settlement had been. This proved to be fortunate for later archaeological work at the site.

Archaeological work began at the site in 1900 after the rediscovery of the settlement. Excavations were conducted for the next 15 years. Further excavations were carried out between 1930 and 1939. Archaeological work on the site was productive for two main reasons: that the site had never been built on since its destruction some 840 years earlier, and that the permanently waterlogged ground had preserved wood and other perishable materials. After the Second World War, in 1959, archaeological work was started again and has continued intermittently ever since. The embankments surrounding the settlement were excavated, and the harbour was partially dredged, during which the wreck of a Viking ship was discovered. Despite all this work, only 5% of the settlement (and only 1% of the harbour) has as yet been investigated.

The most important finds resulting from the excavations are now on display in the adjoining Haithabu Museum.

In 2005 an ambitious archaeological reconstruction program was initiated on the original site. Based on the results of archaeological analyses, exact copies of some of the original Viking houses have been built.





</doc>
<doc id="14131" url="https://en.wikipedia.org/wiki?curid=14131" title="Hazaras">
Hazaras

The Hazaras (, ) are an ethnic group native to the region of Hazarajat in central Afghanistan, speaking the Hazaragi variant of Dari, itself an eastern variety of Persian and one of the two official languages of Afghanistan.

They are overwhelmingly Twelver Shia Muslims and make up the third largest ethnic group in Afghanistan. They also make up a significant minority group in the neighboring Pakistan, with a population of over 650,000–900,000, largely living in the region of Quetta.

Babur, founder of the Mughal Empire in the early 16th century, records the name "Hazara" in his autobiography. He referred to the populace of a region called "Hazaristan", located west of the Kabulistan region, north of Ghazna, and southwest of Ghor.

The conventional theory is that the name "Hazara" derives from the Persian word for "thousand" ( ). It may be the translation of the Mongol word (or ), a military unit of 1,000 soldiers at the time of Genghis Khan. With time, the term "Hazar" could have been substituted for the Mongol word and now stands for the group of people.

The origins of the Hazara have not been fully reconstructed. Significant inner Asian descent—in historical context, Turkic and Mongol—is impossible to rule out because the Hazara's physical attributes, facial bone structures and parts of their culture and language resemble those of Mongolians and Central Asian Turks. Thus, it is widely and popularly believed that Hazara have Mongolian ancestry. Genetic analysis of the Hazara indicate partial Mongolian ancestry. Invading Mongols and Turco-Mongols mixed with the local Iranian population, forming a distinct group.
For example, Nikudari Mongols settled in what is now Afghanistan and mixed with native populations who spoke Dari Persian. A second wave of mostly Chagatai Mongols came from Central Asia and were followed by other Mongolic groups, associated with the Ilkhanate and the Timurids, all of whom settled in Hazarajat and mixed with the local, mostly Dari-speaking population, forming a distinct group.

The Hazara identity in Afghanistan is believed by many to have originated in the aftermath of the 1221 Siege of Bamyan. The first mention of Hazara are made by Babur in the early 16th century and later by the court historians of Shah Abbas of the Safavid dynasty. It is reported that they embraced Shia Islam between the end of the 16th and the beginning of the 17th century, during the Safavid period.

Hazara men along with tribes of other ethnic groups had been recruited and added to the army of Ahmad Shah Durrani in the 18th century. Some claim that in the mid‑18th century Hazara were forced out of Helmand and the Arghandab District of Kandahar Province.

During the second reign of Dost Mohammad Khan in the 19th century, Hazara from Hazarajat began to be taxed for the first time. However, for the most part they still managed to keep their regional autonomy until the subjugation of Abdur Rahman Khan began in the late 19th century.
When the Treaty of Gandomak was signed and the Second Anglo-Afghan War ended in 1880, Abdur Rahman Khan set out a goal to bring Hazarajat and Kafiristan under his control. He launched several campaigns in Hazarajat due to resistance from the Hazara in which his forces committed atrocities. The southern part of Hazarajat was spared as they accepted his rule, while the other parts of Hazarajat rejected Abdur Rahman and instead supported his uncle, Sher Ali Khan. In response to this Abdur Rahman waged a war against tribal leaders who rejected his policies and rule. Abdur Rahman arrested Syed Jafar, chief of the Sheikh Ali Hazara tribe, and jailed him in Mazar-i-Sharif.

The 1888–1893 Uprisings of Hazaras occurred when the Treaty of Gandomak was signed and the Second Anglo-Afghan War ended in 1880, causing Abdur Rahman Khan to set out on a goal to bring Hazarajat and Kafiristan under his control. He launched several campaigns in Hazarajat due to resistance from the Hazara in which his forces committed atrocities. The southern part of Hazarajat was spared as they accepted his rule, while the other parts of Hazarajat rejected Abdur Rahman and instead supported his uncle, Sher Ali Khan. In response to this Abdur Rahman waged a war against tribal leaders who rejected his policies and rule. Abdur Rahman arrested Syed Jafar, chief of the Sheikh Ali Hazara tribe, and jailed him in Mazar-i-Sharif. These campaigns had a catastrophic impact on the demographics of Hazaras causing 60% of them to perish or become displaced.

In 1901, Habibullah Khan, Abdur Rahman's successor, granted amnesty to all people who were exiled by his predecessor. However, the division between the Afghan government and the Hazara people was already made too deep under Abdur Rahman. Hazara continued to face severe social, economic and political discrimination through most of the 20th century. In 1933 King Mohammed Nadir Khan was assassinated by Abdul Khaliq Hazara. The Afghan government captured and executed him later, along with several of his innocent family members.

Mistrust of the central government by the Hazaras and local uprisings continued. In particular, in the 1940s, during Zahir Shah's rule, a revolt took place against new taxes that were exclusively imposed on the Hazara. The Kuchi nomads meanwhile not only were exempted from taxes, but also received allowances from the Afghan government. The angry rebels began capturing and killing government officials. In response, the central government sent a force to subdue the region and later removed the taxes.
During the Soviet–Afghan War, the Hazarajat region did not see as much heavy fighting as other regions of Afghanistan. However, rival Hazara political factions fought. The division was between the "Tanzáim-i nasl-i naw-i Hazara", a party based in Quetta, of Hazara nationalists and secular intellectuals, and the pro-Khomeini Islamist parties backed by the new "Islamic Republic of Iran". By 1979, the Iran-backed Islamist groups liberated Hazarajat from the central Soviet-backed Afghan government and later took entire control of Hazarajat away from the secularists. By 1984, after severe fighting, the secularist groups lost all their power to the Islamists.

As the Soviets withdrew in 1989, the Islamist groups felt the need to broaden their political appeal and turned their focus to Hazara ethnic nationalism. This led to establishment of the Hizb-i-Wahdat, an alliance of all the Hazara resistance groups (except the "Harakat-i Islami"). In 1992 with the fall of Kabul, the "Harakat-i Islami" took sides with Burhanuddin Rabbani's government while the Hizb-i-Wahdat took sides with the opposition. The Hizb-i-Wahdat was eventually forced out of Kabul in 1995 when the Taliban movement captured and killed their leader Abdul Ali Mazari. With the Taliban's capture of Kabul in 1996, all the Hazara groups united with the new Northern Alliance against the common new enemy. However, it was too late and despite the fierce resistance Hazarajat fell to the Taliban by 1998. The Taliban had Hazarajat totally isolated from the rest of the world going as far as not allowing the United Nations to deliver food to the provinces of Bamiyan, Ghor, Wardak, and Daykundi.

Though Hazara played a role in the anti-Soviet movement, other Hazara participated in the new communist government, which actively courted Afghan minorities. Sultan Ali Kishtmand, a Hazara, served as prime minister of Afghanistan from 1981–1990 (with one brief interruption in 1988). The Ismaili Hazara of Baghlan Province likewise supported the communists, and their "pir" (religious leader) Jaffar Naderi led a pro-Communist militia in the region.

During the years that followed, Hazara suffered severe oppression and many ethnic massacres, genocides and pogroms were carried out by the predominantly ethnic Pashtun Taliban and are documented by such groups the Human Rights Watch. These human rights abuses not only occurred in Hazarajat, but across all districts controlled by the Taliban. Particularly after their capture of Mazar-i-Sharif in 1998, where after a massive killing of some 8,000 civilians, the Taliban openly declared that the Hazara would be targeted.

Following the 11 September 2001 attacks in the United States, British and American forces invaded Afghanistan. Many Hazara have become leaders in today's newly emerging Afghanistan. Hazara have also pursued higher education, enrolled in the army, and many have top government positions. For example, Mohammad Mohaqiq, a Hazara from the Hizb-i-Wahdat party, ran in the 2004 presidential election in Afghanistan, and Karim Khalili became the Vice President of Afghanistan. A number of ministers and governors are Hazara, including Sima Samar, Habiba Sarabi, Sarwar Danish, Sayed Hussein Anwari, Abdul Haq Shafaq, Sayed Anwar Rahmati, Qurban Ali Oruzgani and many others. The mayor of Nili in Daykundi Province is Azra Jafari, who became the first female mayor in Afghanistan. The National Assembly of Afghanistan (Parliament) is 25% made up of ethnic Hazara, which represents 61 members.
Although Afghanistan has been historically one of the poorest countries in the world, the Hazarajat region has been kept even more poor from development by past governments. Since ousting the Taliban in late 2001, billions of dollars have poured into Afghanistan for reconstruction and several large-scale reconstruction projects took place in Afghanistan from August 2012. For example, there have been more than 5000 kilometers of road pavement completed across Afghanistan, of which little was done in central Afghanistan Hazarajat. On the other hand, the Band-e Amir in the Bamyan Province became the first national park of Afghanistan. The road from Kabul to Bamyan was also built, along with new police stations, government institutions, hospitals, and schools in the Bamyan Province, Daykundi Province, and the others. The first ski resort of Afghanistan was also established in Bamyan Province.

An indication of discrimination is that Kuchis (Afghan nomads who have historically been migrating from region to region depending on the season) are allowed to use Hazarajat pastures during the summer season. It is believed that allowing the Kuchis to use some of the grazing land in Hazarajat began during the rule of Abdur Rahman Khan. Living in mountainous Hazarajat, where little farm land exists, Hazara people rely on these pasture lands for their livelihood during the long and harsh winters. In 2007 some Kuchi nomads entered into parts of Hazarajat to graze their livestock, and when the local Hazara resisted, a clash took place and several people on both sides died using assault rifles. Such events continue to occur, even after the central government was forced to intervene, including President Hamid Karzai. In late July 2012, a Hazara police commander in Uruzgan province reportedly rounded up and killed 9 Pashtun civilians in revenge for the death of two local Hazara. The matter is being investigated by the Afghan government.

The drive by President Hamid Karzai after the Peace Jirga to strike a deal with Taliban leaders caused deep unease in Afghanistan's minority communities, who fought the Taliban the longest and suffered the most during their rule. The leaders of the Tajik, Uzbek and Hazara communities, which together make up around 65% of the country's population, vowed to resist any return of the Taliban to power, referring to the large-scale massacres of Hazara civilians during the Taliban period.

Genetically, the Hazara are a mixture of western Eurasian and eastern Eurasian components. While it has been found that "at least third to half of their chromosomes are of East Asian origin, PCA places them between East Asia and Caucasus/Middle East/Europe clusters". Genetic research suggests that the Hazaras of Afghanistan cluster closely with the Uzbek population of the country, while both groups are at a notable distance from Afghanistan's Tajik and Pashtun populations. There is evidence of both a patrimonial and maternal relation to Turkic Peoples and Mongols.

Mongol male and female ancestry is supported by studies in genetic genealogy as well, which have identified a particular lineage of the Y‑chromosome characteristic of people of Mongolian descent ("the Y-chromosome of Genghis Khan"). This chromosome is virtually absent outside the limits of the Mongol Empire except among the Hazara, where it reaches its highest frequency anywhere. These results indicate that the Hazara are also characterized by very high frequencies of eastern Eurasian mtDNAs at 35%, which are virtually absent from bordering populations, suggesting that the male descendants of Genghis Khan, or other Mongols, were accompanied by women of East Asian ancestry. Women of Non-eastern Eurasian mtDNA in Hazaras are at 65% most which are West Eurasians and some South Asian.

The most frequent paternal Haplogroup type found amongst the Pakistani Hazara was haplogroup C-M217 at 40%(10/25) with Haplogroup O3 (Y-DNA) at 8% (2/25) both which are Eastern Eurasian males ancestry associated with the Mongoloid ethnicity.

In one mtDNA study of Hazara, mtDNA Haplogroup L which is of African origin was detected at a frequency of 7.5% Also in one Y-DNA study of Hazara people, two haplogroups regarded as extreme outliers geographically have also been identified at low levels among the Hazara:

The vast majority of Hazaras live in central Afghanistan, and significant numbers are also found in major cities and towns. Many Hazara men leave Hazarjat to work in cities, including in neighboring countries or abroad. The latest World Factbook estimates show that Hazara make up nine percent of the total Afghan population but some sources claim that they are about 20 percent. However, they fail to cite a reference. In the 1970s, they were estimated by Louis Dupree at approximately 1,000,000.

Alessandro Monsutti argues, in his recent anthropological book, that migration is the traditional way of life of the Hazara people, referring to the seasonal and historical migrations which have never ceased and do not seem to be dictated only by emergency situations such as war. Due to the decades of war in Afghanistan and the sectarian violence in Pakistan, many Hazaras left their communities and have settled in Australia, New Zealand, Canada, the United States, the United Kingdom and particularly the Northern European countries such as Sweden and Denmark. Some go to these countries as exchange students while others through human smuggling, which sometimes cost them their lives. Since 2001, about 1,000 people have died in the ocean while trying to reach Australia by boats from Indonesia. Many of these were Hazaras, including women and small children who could not swim. The notable case was the Tampa affair in which a shipload of refugees, mostly Hazara, was rescued by the Norwegian freighter MV "Tampa" and subsequently sent to Nauru. New Zealand agreed to take some of the refugees and all but one of those were granted stay.

During the British expansion in the 19th century, Hazaras worked during the winter months in coal mines, road construction and in other menial labor jobs in some cities of what is now Pakistan. The earliest record of Hazara in the areas of Pakistan is found in Broadfoot's Sappers company from 1835 in Quetta. This company had also participated in the First Anglo-Afghan War. Some Hazara also worked in the agriculture farms in Sindh and construction of Sukkur barrage. Haider Ali Karmal Jaghori was a prominent political thinker of the Hazara people in Pakistan, writing about the political history of Hazara people. His work "Hazaraha wa Hazarajat Bastan Dar Aiyna-i-Tarikh" was published in Quetta in 1992, and another work by Aziz Tughyan Hazara "Tarikh Milli Hazara" was published in 1984 in Quetta.

Most Pakistani Hazaras today live in the city of Quetta, in Balochistan, Pakistan. Localities in the city of Quetta with prominent Hazara populations include Hazara Town and Mehr Abad and Hazara tribes such as the "Sardar" are exclusively Pakistani. Literacy level among the Hazara community in Pakistan is relatively high compare to the Hazaras of Afghanistan, and they have integrated well into the social dynamics of the local society. Saira Batool, a Hazara woman, was one of the first female pilots in Pakistan Air Force. Other notable Hazara include Qazi Mohammad Esa, General Musa Khan Hazara, who served as Commander in Chief of the Pakistani Army from 1958 to 1968, Air Marshal Sharbat Ali Changezi, Hussain Ali Yousafi, the slain chairman of the Hazara Democratic Party, Syed Nasir Ali Shah, MNA from Quetta and his father Haji Sayed Hussain Hazara who was a senator and member of majlis shora during the Zia-ul-Haq era.

Despite all of this, Hazaras are often targeted by militant groups such as the Lashkar-e-Jhangvi and others. "Activists say at least 800-1,000 Hazaras have been killed since 1999 and the pace is quickening. More than one hundred have been murdered in and around Quetta since January, according to Human Rights Watch." The political representation of the community is served by Hazara Democratic Party, a secular liberal democratic party, headed by Abdul Khaliq Hazara.

Over the many years as a result of political unrest in Afghanistan some Hazaras have migrated to Iran. The local Hazara population has been estimated at 500,000 people of which at least one third have spent more than half their life in Iran.

They have complained of discrimination in Iran. In March 2011, "Eurasia Daily Monitor" reported that representatives of Hazara community in Iran have asked Mongolia to intervene in supporting their case with Iranian government and prevent Iranian forced repatriation to Afghanistan.

The Hazara, outside of Hazarajat, have adopted the cultures of the cities where they dwell, and in many cases they have become Pashtunized and Persianized, resembling customs and traditions of the Afghan Tajiks and Pashtuns. Traditionally the Hazara are highland farmers and although sedentary, in the Hazarajat, they have retained many of their own customs and traditions, some of which are more closely related to those of Central Asia than to those of the Afghan Tajiks. For instance, many Hazara musicians are widely hailed as being skilled in playing the dambura, a regional and native instrument, a lute instrument similarly found in other Central Asian nations such as Tajikistan and Uzbekistan. The Hazara live in houses rather than tents; Aimaq people in tents rather than houses.

Hazara people living in Hazarajat (Hazaristan) areas speak the Hazaragi language of Afghanistan, which is infused with a significant number of Mongolic loan words. The primary differences between Dari and Hazaragi are the accent and Hazaragi's greater array of some Mongolic loanwords. Despite these differences, Hazaragi is mutually intelligible with Dari, one of the official languages of Afghanistan.

Many of the urban Hazara in the larger cities such as Kabul and Mazar-i-Sharif no longer speak Hazaragi but speak standard literary Dari (usually the "Kābolī" dialect) or other regional varieties of Dari (for example the "Khorāsānī" dialect in the western region of Herat).

Until recently, a very small number of Sunni Hazara in Herat Province most known as Moghol people still spoke the Moghol language, a Mongolic language once spoken by rebels against the Mongol armies of the Il-Khanat.

Hazara are predominantly Shi'a Muslims, mostly of the Twelver sect and some Ismaili. Since the majority of Afghans practice Sunni Islam, this may have contributed to the discrimination against the Hazara. Hazara probably converted to Shi'ism during the first part of the 16th century, in the early days of the Safavid Dynasty. Nonetheless, a small number of Hazara are Sunni, such as the Aimaq Hazaras. Sunni Hazara have been attached to non-Hazara tribes (such as Taimuris), while the Ismaili Hazara have always been kept separate from the rest of the Hazara on account of religious beliefs and political purposes.

The Hazara people have been organized by various tribes. The Daizangi are the largest tribe, representing 57.2% of the Hazara population. However, more recently and since the inclusion of the Hazara into the "Afghan state", tribal affiliations have been disappearing and former tribal names Sheikh Ali, Jaghori, Ghaznichi, Behsud, Uruzgani, and Daizangi are also disappearing. There are smaller tribes such as Sarcheshmae, Kolokheshgi, Turkmani, Daemirdadi, Wazirgi that have remained a minority among the Hazara tribes. The different tribes come from regions such as Parwan, Bamiyan, Ghazni and Maydan-Shahr and have spread outwards from Hazarajat (Main City) into Kabul and other parts of Afghanistan.
Many Hazara are engaged in different sports. Rohullah Nikpai won a bronze medal in Taekwondo in the Beijing Olympics 2008, beating world champion Juan Antonio Ramos of Spain 4–1 in a play-off final. It was Afghanistan's first-ever Olympic medal. He then won a second Olympic medal for Afghanistan in the London 2012 games. Afghanistan's first female Olympic athlete Friba Razayee competed in judo at the 2004 Athens Olympics, but was eliminated in the first round of competition.

Other famous Hazara athletes are Syed Abdul Jalil Waiz (Badminton) and Ali Hazara (Football). Syed Abdul Jalil Waiz was the first ever badminton player representing Afghanistan in Asian Junior Championships in 2005 where he produced the first win for his country against Iraq, with 15–13, 15–1. He participated in several international championships since 2005 and achieved victories against Australia, Philippines and Mongolia. Hamid Rahimi is a new boxer from Afghanistan and lives in Germany. Zohib Islam Amiri the former captain of Afghanistan's national football team is also of Hazara descent.

A Pakistani Hazara named Syed Abrar Hussain Shah, a former Olympic boxer served as deputy director general of the Pakistan Sports Board. Shah represented Pakistan three times at the Olympics and won a gold medal at the 1990 Asian Games in Beijing. Some Hazara from Pakistan have also excelled in sports and have received numerous awards particularly in boxing, football and in field hockey. Qayum Changezi, a legendary Pakistani football player, was a Hazara.
New Hazara youngsters are seen to appear in many sports in Pakistan including boys and girls from Hazara Town and Mehr Abad. Rajab Ali Hazara who is leading under 16 Pakistan Football team as captain and many other youngsters representing Hazaras in sports in Pakistan.





</doc>
<doc id="14132" url="https://en.wikipedia.org/wiki?curid=14132" title="Hawala">
Hawala

Hawala or hewala (, meaning "transfer" or sometimes "trust"), also known as havaleh in Persian, or hundi or—in Somali, xawala or xawilaad—is a popular and informal value transfer system based not on the movement of cash, or on telegraph or computer network wire transfers between banks, but instead on the performance and honour of a huge network of money brokers (known as "hawaladars"). While hawaladars are spread throughout the world, they are primarily located in the Middle East, North Africa, the Horn of Africa, and the Indian subcontinent, operating outside of, or parallel to, traditional banking, financial channels, and remittance systems. Hawala follows Islamic traditions but its use is not limited to Muslims.

The hawala system has existed since the 8th century between Arabic and Muslim traders alongside the Silk Road and beyond as a protection against theft. It is believed to have arisen in the financing of long-distance trade around the emerging capital trade centers in the early medieval period. In South Asia, it appears to have developed into a fully-fledged money market instrument, which was only gradually replaced by the instruments of the formal banking system in the first half of the 20th century.

"Hawala" itself influenced the development of the agency in common law and in civil laws, such as the "aval" in French law and the "avallo" in Italian law. The words "aval" and "avallo" were themselves derived from "hawala". The transfer of debt, which was "not permissible under Roman law but became widely practiced in medieval Europe, especially in commercial transactions", was due to the large extent of the "trade conducted by the Italian cities with the Muslim world in the Middle Ages". The agency was also "an institution unknown to Roman law" as no "individual could conclude a binding contract on behalf of another as his agent". In Roman law, the "contractor himself was considered the party to the contract and it took a second contract between the person who acted on behalf of a principal and the latter in order to transfer the rights and the obligations deriving from the contract to him". On the other hand, Islamic law and the later common law "had no difficulty in accepting agency as one of its institutions in the field of contracts and of obligations in general".

Today, hawala is probably used mostly for migrant workers' remittances to their countries of origin.

In the most basic variant of the hawala system, money is transferred via a network of hawala brokers, or "hawaladars". It is the transfer of money without actually moving it. In fact, a successful definition of the hawala system that is used is "money transfer without money movement". According to author Sam Vaknin, while there are large hawaladar operators with networks of middlemen in cities across many countries, most hawaladars are small businesses who work at Hawala as a sideline or moonlighting operation.
The figure shows how hawala works: (1) a customer ("A", left-hand side) approaches a hawala broker ("X") in one city and gives a sum of money (red arrow) that is to be transferred to a recipient ("B", right-hand side) in another, usually foreign, city. Along with the money, he usually specifies something like a password that will lead to the money being paid out (blue arrows). (2b) The hawa calls another hawala broker "M" in the recipient's city, and informs "M" about the agreed password, or gives other disposition of the funds. Then, the intended recipient ("B"), who also has been informed by "A" about the password (2a), now approaches "M" and tells him the agreed password (3a). If the password is correct, then "M" releases the transferred sum to "B" (3b), usually minus a small commission. "X" now basically owes "M" the money that "M" had paid out to "B"; thus "M" has to trust "X"s promise to settle the debt at a later date.

The unique feature of the system is that no promissory instruments are exchanged between the hawala brokers; the transaction takes place entirely on the honour system. As the system does not depend on the legal enforceability of claims, it can operate even in the absence of a legal and juridical environment. Trust and extensive use of connections are the components that distinguish it from other remittance systems. Hawaladars networks are often based on membership in the same family, village, clan, or ethnic group, and cheating is punished by effective ex-communication and "loss of honour"—leading to severe economic hardship.

Informal records are produced of individual transactions, and a running tally of the amount owed by one broker to another is kept. Settlements of debts between hawala brokers can take a variety of forms (such as goods, services, properties, transfers of employees, etc.), and need not take the form of direct cash transactions.

In addition to commissions, hawala brokers often earn their profits through bypassing official exchange rates. Generally, the funds enter the system in the source country's currency and leave the system in the recipient country's currency. As settlements often take place without any foreign exchange transactions, they can be made at other than official exchange rates.

Hawala is attractive to customers because it provides a fast and convenient transfer of funds, usually with a far lower commission than that charged by banks. Its advantages are most pronounced when the receiving country applies unprofitable exchange rate regulations or when the banking system in the receiving country is less complex (e.g., due to differences in legal environment in places such as Afghanistan, Yemen, Somalia). Moreover, in some parts of the world it is the only option for legitimate fund transfers, and has even been used by aid organizations in areas where it is the best-functioning institution.

Dubai has been prominent for decades as a welcoming hub for hawala transactions worldwide.

The "hundi" is a financial instrument that developed on the Indian sub-continent for use in trade and credit transactions. Hundis are used as a form of remittance instrument to transfer money from place to place, as a form of credit instrument or IOU to borrow money and as a bill of exchange in trade transactions. The Reserve Bank of India describes the Hundi as "an unconditional order in writing made by a person directing another to pay a certain sum of money to a person named in the order."

The word "angadia" means courier in Hindi, but also designates those who act as hawaladars within India. These people mostly act as a parallel banking system for businessmen. They charge a commission of around 0.2–0.5% per transaction from transferring money from one city to another.

According to the CIA, with the dissolution of Somalia's formal banking system, many informal money transfer operators, arose to fill the void. It estimates that such "hawaladars", "xawilaad" or "xawala" brokers, are now responsible for the transfer of up to $1.6 billion per year in remittances to the country, most coming from working Somalis outside Somalia. Such funds have in turn had a stimulating effect on local business activity.

The 2012 Tuareg rebellion left Northern Mali without an official money transfer service for months. The coping mechanisms that appeared were patterned on the hawala system.

Some government officials assert that hawala can be used to facilitate money laundering, avoid taxation, and move wealth anonymously. As a result, it is illegal in some U.S. states, India, Pakistan, and some other countries.

After the September 11 terrorist attacks, the American government suspected that some hawala brokers may have helped terrorist organizations transfer money to fund their activities, and the 9/11 Commission Report stated that "Al Qaeda frequently moved the money it raised by hawala". As a result of intense pressure from the U.S. authorities to introduce systematic anti-money laundering initiatives on a global scale, a number of hawala networks were closed down and a number of hawaladars were successfully prosecuted for money laundering. However, there is little evidence that these actions brought the authorities any closer to identifying and arresting a significant number of terrorists or drug smugglers. Experts emphasized that the overwhelming majority of those who used these informal networks were doing so for legitimate purposes, and simply chose to use a transaction medium other than state-supported banking systems. Today, the hawala system in Afghanistan is instrumental in providing financial services for the delivery of emergency relief and humanitarian and developmental aid for the majority of international and domestic NGOs, donor organizations, and development aid agencies.

In November 2001, the Bush administration froze the assets of Al-Barakat, a Somali remittance hawala company used primarily by a large number of Somali immigrants. Many of its agents in several countries were initially arrested, though later freed after no concrete evidence against them was found. In August 2006 the last Al-Barakat representatives were taken off the U.S. terror list, though some assets remain frozen. The mass media has speculated that pirates from Somalia use the hawala system to move funds internationally, for example into neighboring Kenya, where these transactions are neither taxed nor recorded.

In January 2010, the Kabul office of New Ansari Exchange, Afghanistan's largest hawala money transfer business, was closed following a raid by the Sensitive Investigative Unit, the country's national anti-political corruption unit, allegedly because this company was involved in laundering profits from the illicit opium trade and the moving of cash earned by government allied warlords through extortion and drug trafficking. Thousands of records were seized, from which links were found between money transfers by this company and political and business figures and NGOs in the country, including relatives of President Hamid Karzai. In August 2010, Karzai took control of the task force that staged the raid, and the US-advised anti-corruption group, the Major Crimes Task Force. He ordered a commission to review scores of past and current anti-corruption inquests.




</doc>
<doc id="14133" url="https://en.wikipedia.org/wiki?curid=14133" title="Hydroponics">
Hydroponics

Hydroponics is a subset of hydroculture, the method of growing plants without soil, using mineral nutrient solutions in a water solvent. Terrestrial plants may be grown with only their roots exposed to the mineral solution, or the roots may be supported by an inert medium, such as perlite or gravel. The nutrients in hydroponics can come from an array of different sources; these can include but are not limited to byproduct from fish waste, duck manure, or "commercial fertilisers".

The earliest published work on growing terrestrial plants without soil was the 1627 book "Sylva Sylvarum" or 'A Natural History' by Francis Bacon, printed a year after his death. Water culture became a popular research technique after that. In 1699, John Woodward published his water culture experiments with spearmint. He found that plants in less-pure water sources grew better than plants in distilled water. By 1842, a list of nine elements believed to be essential for plant growth had been compiled, and the discoveries of German botanists Julius von Sachs and Wilhelm Knop, in the years 1859–1875, resulted in a development of the technique of soilless cultivation. Growth of terrestrial plants without soil in mineral nutrient solutions was called solution culture. It quickly became a standard research and teaching technique and is still widely used. Solution culture is, now considered, a type of hydroponics where there is no inert medium.

In 1929, William Frederick Gericke of the University of California at Berkeley began publicly promoting that solution culture be used for agricultural crop production. He first termed it aquaculture but later found that aquaculture was already applied to culture of aquatic organisms. Gericke created a sensation by growing tomato vines high in his back yard in mineral nutrient solutions rather than soil. He introduced the term hydroponics, water culture, in 1937, proposed to him by , a phycologist with an extensive education in the classics. Hydroponics is derived from neologism υδρωπονικά, constructed in analogy to γεωπονικά, geoponica, that which concerns agriculture, replacing, γεω-, earth, with ὑδρο-, water.

Reports of Gericke's work and his claims that hydroponics would revolutionize plant agriculture prompted a huge number of requests for further information. Gericke had been denied use of the University's greenhouses for his experiments due to the administration's skepticism, and when the University tried to compel him to release his preliminary nutrient recipes developed at home he requested greenhouse space and time to improve them using appropriate research facilities. While he was eventually provided greenhouse space, the University assigned Hoagland and Arnon to re-develop Gericke's formula and show it held no benefit over soil grown plant yields, a view held by Hoagland. In 1940, Gericke published the book, "Complete Guide to Soil less Gardening," after leaving his academic position in a climate that was politically unfavorable.

Two other plant nutritionists at the University of California were asked to research Gericke's claims. Dennis R. Hoagland and Daniel I. Arnon wrote a classic 1938 agricultural bulletin, "The Water Culture Method for Growing Plants Without Soil,". Hoagland and Arnon claimed that hydroponic crop yields were no better than crop yields with good-quality soils. Crop yields were ultimately limited by factors other than mineral nutrients, especially light. This research, however, overlooked the fact that hydroponics has other advantages including the fact that the roots of the plant have constant access to oxygen and that the plants have access to as much or as little water as they need. This is important as one of the most common errors when growing is over- and under- watering; and hydroponics prevents this from occurring as large amounts of water can be made available to the plant and any water not used, drained away, recirculated, or actively aerated, eliminating anoxic conditions, which drown root systems in soil. In soil, a grower needs to be very experienced to know exactly how much water to feed the plant. Too much and the plant will be unable to access oxygen; too little and the plant will lose the ability to transport nutrients, which are typically moved into the roots while in solution. These two researchers developed several formulas for mineral nutrient solutions, known as Hoagland solution. Modified Hoagland solutions are still in use.

One of the earliest successes of hydroponics occurred on Wake Island, a rocky atoll in the Pacific Ocean used as a refueling stop for Pan American Airlines. Hydroponics was used there in the 1930s to grow vegetables for the passengers. Hydroponics was a necessity on Wake Island because there was no soil, and it was prohibitively expensive to airlift in fresh vegetables.

In the 1960s, Allen Cooper of England developed the Nutrient film technique. The Land Pavilion at Walt Disney World's EPCOT Center opened in 1982 and prominently features a variety of hydroponic techniques.

In recent decades, NASA has done extensive hydroponic research for its Controlled Ecological Life Support System (CELSS). Hydroponics intended to take place on Mars are using LED lighting to grow in a different color spectrum with much less heat. Ray Wheeler, a plant physiologist at Kennedy Space Center’s Space Life Science Lab, believes that hydroponics will create advances within space travel, as a bioregenerative life support system.

In 2007, Eurofresh Farms in Willcox, Arizona, sold more than 200 million pounds of hydroponically grown tomatoes. Eurofresh has under glass and represents about a third of the commercial hydroponic greenhouse area in the U.S. Eurofresh tomatoes were pesticide-free, grown in rockwool with top irrigation. Eurofresh declared bankruptcy, and the greenhouses were acquired by NatureSweet Ltd. in 2013.

As of 2017, Canada had hundreds of acres of large-scale commercial hydroponic greenhouses, producing tomatoes, peppers and cucumbers.

Due to technological advancements within the industry and numerous economic factors, the global hydroponics market is forecast to grow from $226.45 million USD in 2016 to $724.87 million USD by 2023.

There are two main variations for each medium, sub-irrigation and top irrigation. For all techniques, most hydroponic reservoirs are now built of plastic, but other materials have been used including concrete, glass, metal, vegetable solids, and wood. The containers should exclude light to prevent algae and fungal growth in the nutrient solution.

In static solution culture, plants are grown in containers of nutrient solution, such as glass Mason jars (typically, in-home applications), plastic buckets, tubs, or tanks. The solution is usually gently aerated but may be un-aerated. If un-aerated, the solution level is kept low enough that enough roots are above the solution so they get adequate oxygen. A hole is cut in the lid of the reservoir for each plant. A single reservoir can be dedicated to a single plant, or to various plants. Reservoir size can be increased as plant size increases. A home made system can be constructed from plastic food containers or glass canning jars with aeration provided by an aquarium pump, aquarium airline tubing and aquarium valves. Clear containers are covered with aluminium foil, butcher paper, black plastic, or other material to exclude light, thus helping to eliminate the formation of algae. The nutrient solution is changed either on a schedule, such as once per week, or when the concentration drops below a certain level as determined with an electrical conductivity meter. Whenever the solution is depleted below a certain level, either water or fresh nutrient solution is added. A Mariotte's bottle, or a float valve, can be used to automatically maintain the solution level. In raft solution culture, plants are placed in a sheet of buoyant plastic that is floated on the surface of the nutrient solution. That way, the solution level never drops below the roots.

In continuous-flow solution culture, the nutrient solution constantly flows past the roots. It is much easier to automate than the static solution culture because sampling and adjustments to the temperature and nutrient concentrations can be made in a large storage tank that has potential to serve thousands of plants. A popular variation is the nutrient film technique or NFT, whereby a very shallow stream of water containing all the dissolved nutrients required for plant growth is recirculated past the bare roots of plants in a watertight thick root mat, which develops in the bottom of the channel and has an upper surface that, although moist, is in the air. Subsequent to this, an abundant supply of oxygen is provided to the roots of the plants. A properly designed NFT system is based on using the right channel slope, the right flow rate, and the right channel length. The main advantage of the NFT system over other forms of hydroponics is that the plant roots are exposed to adequate supplies of water, oxygen, and nutrients. In all other forms of production, there is a conflict between the supply of these requirements, since excessive or deficient amounts of one results in an imbalance of one or both of the others. NFT, because of its design, provides a system where all three requirements for healthy plant growth can be met at the same time, provided that the simple concept of NFT is always remembered and practised. The result of these advantages is that higher yields of high-quality produce are obtained over an extended period of cropping. A downside of NFT is that it has very little buffering against interruptions in the flow (e.g., power outages). But, overall, it is probably one of the more productive techniques.

The same design characteristics apply to all conventional NFT systems. While slopes along channels of 1:100 have been recommended, in practice it is difficult to build a base for channels that is sufficiently true to enable nutrient films to flow without ponding in locally depressed areas. As a consequence, it is recommended that slopes of 1:30 to 1:40 are used. This allows for minor irregularities in the surface, but, even with these slopes, ponding and water logging may occur. The slope may be provided by the floor, benches or racks may hold the channels and provide the required slope. Both methods are used and depend on local requirements, often determined by the site and crop requirements.

As a general guide, flow rates for each gully should be 1 liter per minute. At planting, rates may be half this and the upper limit of 2 L/min appears about the maximum. Flow rates beyond these extremes are often associated with nutritional problems. Depressed growth rates of many crops have been observed when channels exceed 12 metres in length. On rapidly growing crops, tests have indicated that, while oxygen levels remain adequate, nitrogen may be depleted over the length of the gully. As a consequence, channel length should not exceed 10–15 metres. In situations where this is not possible, the reductions in growth can be eliminated by placing another nutrient feed halfway along the gully and halving the flow rates through each outlet.

Aeroponics is a system wherein roots are continuously or discontinuously kept in an environment saturated with fine drops (a mist or aerosol) of nutrient solution. The method requires no substrate and entails growing plants with their roots suspended in a deep air or growth chamber with the roots periodically wetted with a fine mist of atomized nutrients. Excellent aeration is the main advantage of aeroponics.

Aeroponic techniques have proven to be commercially successful for propagation, seed germination, seed potato production, tomato production, leaf crops, and micro-greens. Since inventor Richard Stoner commercialized aeroponic technology in 1983, aeroponics has been implemented as an alternative to water intensive hydroponic systems worldwide. The limitation of hydroponics is the fact that of water can only hold of air, no matter whether aerators are utilized or not.

Another distinct advantage of aeroponics over hydroponics is that any species of plants can be grown in a true aeroponic system because the micro environment of an aeroponic can be finely controlled. The limitation of hydroponics is that certain species of plants can only survive for so long in water before they become waterlogged. The advantage of aeroponics is that suspended aeroponic plants receive 100% of the available oxygen and carbon dioxide to the roots zone, stems, and leaves, thus accelerating biomass growth and reducing rooting times. NASA research has shown that aeroponically grown plants have an 80% increase in dry weight biomass (essential minerals) compared to hydroponically grown plants. Aeroponics used 65% less water than hydroponics. NASA also concluded that aeroponically grown plants requires ¼ the nutrient input compared to hydroponics. Unlike hydroponically grown plants, aeroponically grown plants will not suffer transplant shock when transplanted to soil, and offers growers the ability to reduce the spread of disease and pathogens.
Aeroponics is also widely used in laboratory studies of plant physiology and plant pathology. Aeroponic techniques have been given special attention from NASA since a mist is easier to handle than a liquid in a zero-gravity environment.

Fogponics is a derivation of aeroponics wherein the nutrient solution is aerosolized by a diaphragm vibrating at ultrasonic frequencies. Solution droplets produced by this method tend to be 5–10 µm in diameter, smaller than those produced by forcing a nutrient solution through pressurized nozzles, as in aeroponics. The smaller size of the droplets allows them to diffuse through the air more easily, and deliver nutrients to the roots without limiting their access to oxygen.

Passive sub-irrigation, also known as passive hydroponics, semi-hydroponics, or hydroculture, is a method wherein plants are grown in an inert porous medium that transports water and fertilizer to the roots by capillary action from a separate reservoir as necessary, reducing labor and providing a constant supply of water to the roots. In the simplest method, the pot sits in a shallow solution of fertilizer and water or on a capillary mat saturated with nutrient solution. The various hydroponic media available, such as expanded clay and coconut husk, contain more air space than more traditional potting mixes, delivering increased oxygen to the roots, which is important in epiphytic plants such as orchids and bromeliads, whose roots are exposed to the air in nature. Additional advantages of passive hydroponics are the reduction of root rot and the additional ambient humidity provided through evaporations.

Hydroculture compared to traditional farming in terms of crops yield per area in a controlled environment was roughly 10 times more efficient than traditional farming, uses 13 times less water in one crop cycle then traditional farming, but on average uses 100 times more kilojoules per kilogram of energy than traditional farming.

In its simplest form, there is a tray above a reservoir of nutrient solution. Either the tray is filled with growing medium (clay granules being the most common) and then plant directly or place the pot over medium, stand in the tray. At regular intervals, a simple timer causes a pump to fill the upper tray with nutrient solution, after which the solution drains back down into the reservoir. This keeps the medium regularly flushed with nutrients and air. Once the upper tray fills past the drain stop, it begins recirculating the water until the timer turns the pump off, and the water in the upper tray drains back into the reservoirs.

In a run-to-waste system, nutrient and water solution is periodically applied to the medium surface. The method was invented in Bengal in 1946; for this reason it is sometimes referred to as "The Bengal System".

This method can be set up in various configurations. In its simplest form, a nutrient-and-water solution is manually applied one or more times per day to a container of inert growing media, such as rockwool, perlite, vermiculite, coco fibre, or sand. In a slightly more complex system, it is automated with a delivery pump, a timer and irrigation tubing to deliver nutrient solution with a delivery frequency that is governed by the key parameters of plant size, plant growing stage, climate, substrate, and substrate conductivity, pH, and water content.

In a commercial setting, watering frequency is multi-factorial and governed by computers or PLCs.

Commercial hydroponics production of large plants like tomatoes, cucumber, and peppers uses one form or another of run-to-waste hydroponics.

In environmentally responsible uses, the nutrient-rich waste is collected and processed through an on-site filtration system to be used many times, making the system very productive.

Some bonsai are also grown in soil-free substrates (typically consisting of akadama, grit, diatomaceous earth and other inorganic components) and have their water and nutrients provided in a run-to-waste form.

The hydroponic method of plant production by means of suspending the plant roots in a solution of nutrient-rich, oxygenated water. Traditional methods favor the use of plastic buckets and large containers with the plant contained in a net pot suspended from the centre of the lid and the roots suspended in the nutrient solution.
The solution is oxygen saturated by an air pump combined with porous stones. With this method, the plants grow much faster because of the high amount of oxygen that the roots receive.

"Top-fed" deep water culture is a technique involving delivering highly oxygenated nutrient solution direct to the root zone of plants. While deep water culture involves the plant roots hanging down into a reservoir of nutrient solution, in top-fed deep water culture the solution is pumped from the reservoir up to the roots (top feeding). The water is released over the plant's roots and then runs back into the reservoir below in a constantly recirculating system. As with deep water culture, there is an airstone in the reservoir that pumps air into the water via a hose from outside the reservoir. The airstone helps add oxygen to the water. Both the airstone and the water pump run 24 hours a day.

The biggest advantage of top-fed deep water culture over standard deep water culture is increased growth during the first few weeks. With deep water culture, there is a time when the roots have not reached the water yet. With top-fed deep water culture, the roots get easy access to water from the beginning and will grow to the reservoir below much more quickly than with a deep water culture system. Once the roots have reached the reservoir below, there is not a huge advantage with top-fed deep water culture over standard deep water culture. However, due to the quicker growth in the beginning, grow time can be reduced by a few weeks.

A rotary hydroponic garden is a style of commercial hydroponics created within a circular frame which rotates continuously during the entire growth cycle of whatever plant is being grown.

While system specifics vary, systems typically rotate once per hour, giving a plant 24 full turns within the circle each 24-hour period. Within the center of each rotary hydroponic garden can be a high intensity grow light, designed to simulate sunlight, often with the assistance of a mechanized timer.

Each day, as the plants rotate, they are periodically watered with a hydroponic growth solution to provide all nutrients necessary for robust growth. Due to the plants continuous fight against gravity, plants typically mature much more quickly than when grown in soil or other traditional hydroponic growing systems. Due to the small foot print a rotary hydroponic system has, it allows for more plant material to be grown per square foot of floor space than other traditional hydroponic systems.

One of the most obvious decisions hydroponic farmers have to make is which medium they should use. Different media are appropriate for different growing techniques.

Baked clay pellets are suitable for hydroponic systems in which all nutrients are carefully controlled in water solution. The clay pellets are inert, pH neutral and do not contain any nutrient value.

The clay is formed into round pellets and fired in rotary kilns at . This causes the clay to expand, like popcorn, and become porous. It is light in weight, and does not compact over time. The shape of an individual pellet can be irregular or uniform depending on brand and manufacturing process. The manufacturers consider expanded clay to be an ecologically sustainable and re-usable growing medium because of its ability to be cleaned and sterilized, typically by washing in solutions of white vinegar, chlorine bleach, or hydrogen peroxide (), and rinsing completely.

Another view is that clay pebbles are best not re-used even when they are cleaned, due to root growth that may enter the medium. Breaking open a clay pebble after a crop has been shown to reveal this growth.

Growstones, made from glass waste, have both more air and water retention space than perlite and peat. This aggregate holds more water than parboiled rice hulls. Growstones by volume consist of 0.5 to 5% calcium carbonate – for a standard 5.1 kg bag of Growstones that corresponds to 25.8 to 258 grams of calcium carbonate. The remainder is soda-lime glass.

Coco peat, also known as coir or coco, is the leftover material after the fibres have been removed from the outermost shell (bolster) of the coconut. Coir is a 100% natural grow and flowering medium. Coconut coir is colonized with trichoderma fungi, which protects roots and stimulates root growth. It is extremely difficult to over-water coir due to its perfect air-to-water ratio; plant roots thrive in this environment. Coir has a high cation exchange, meaning it can store unused minerals to be released to the plant as and when it requires it. Coir is available in many forms; most common is coco peat, which has the appearance and texture of soil but contains no mineral content.

Parboiled rice husks (PBH) are an agricultural byproduct that would otherwise have little use. They decay over time, and allow drainage, and even retain less water than growstones. A study showed that rice husks did not affect the effects of plant growth regulators.

Perlite is a volcanic rock that has been superheated into very lightweight expanded glass pebbles. It is used loose or in plastic sleeves immersed in the water. It is also used in potting soil mixes to decrease soil density. Perlite has similar properties and uses to vermiculite but, in general, holds more air and less water and is buoyant.

Like perlite, vermiculite is a mineral that has been superheated until it has expanded into light pebbles. Vermiculite holds more water than perlite and has a natural "wicking" property that can draw water and nutrients in a passive hydroponic system. If too much water and not enough air surrounds the plants roots, it is possible to gradually lower the medium's water-retention capability by mixing in increasing quantities of perlite.

Like perlite, pumice is a lightweight, mined volcanic rock that finds application in hydroponics.

Sand is cheap and easily available. However, it is heavy, does not hold water very well, and it must be sterilized between uses.

The same type that is used in aquariums, though any small gravel can be used, provided it is washed first. Indeed, plants growing in a typical traditional gravel filter bed, with water circulated using electric powerhead pumps, are in effect being grown using gravel hydroponics. Gravel is inexpensive, easy to keep clean, drains well and will not become waterlogged. However, it is also heavy, and, if the system does not provide continuous water, the plant roots may dry out.

Wood fibre, produced from steam friction of wood, is a very efficient organic substrate for hydroponics. It has the advantage that it keeps its structure for a very long time. Wood wool (i.e. wood slivers) have been used since the earliest days of the hydroponics research. However, more recent research suggests that wood fibre may have detrimental effects on "plant growth regulators".

Wool from shearing sheep is a little-used yet promising renewable growing medium. In a study comparing wool with peat slabs, coconut fibre slabs, perlite and rockwool slabs to grow cucumber plants, sheep wool had a greater air capacity of 70%, which decreased with use to a comparable 43%, and water capacity that increased from 23% to 44% with use. Using sheep wool resulted in the greatest yield out of the tested substrates, while application of a biostimulator consisting of humic acid, lactic acid and Bacillus subtilis improved yields in all substrates.

Rock wool (mineral wool) is the most widely used medium in hydroponics. Rock wool is an inert substrate suitable for both run-to-waste and recirculating systems. Rock wool is made from molten rock, basalt or 'slag' that is spun into bundles of single filament fibres, and bonded into a medium capable of capillary action, and is, in effect, protected from most common microbiological degradation. Rock wool is typically used only for the seedling stage, or with newly cut clones, but can remain with the plant base for its lifetime. Rock wool has many advantages and some disadvantages. The latter being the possible skin irritancy (mechanical) whilst handling (1:1000). Flushing with cold water usually brings relief. Advantages include its proven efficiency and effectiveness as a commercial hydroponic substrate. Most of the rock wool sold to date is a non-hazardous, non-carcinogenic material, falling under Note Q of the European Union Classification Packaging and Labeling Regulation (CLP).

Mineral wool products can be engineered to hold large quantities of water and air that aid root growth and nutrient uptake in hydroponics; their fibrous nature also provides a good mechanical structure to hold the plant stable. The naturally high pH of mineral wool makes them initially unsuitable to plant growth and requires "conditioning" to produce a wool with an appropriate, stable pH.

Brick shards have similar properties to gravel. They have the added disadvantages of possibly altering the pH and requiring extra cleaning before reuse.

Polystyrene packing peanuts are inexpensive, readily available, and have excellent drainage. However, they can be too lightweight for some uses. They are used mainly in closed-tube systems. Note that non-biodegradable polystyrene peanuts must be used; biodegradable packing peanuts will decompose into a sludge. Plants may absorb styrene and pass it to their consumers; this is a possible health risk.

The formulation of hydroponic solutions is an application of plant nutrition, with nutrient deficiency symptoms mirroring those found in traditional soil based agriculture. However, the underlying chemistry of hydroponic solutions can differ from soil chemistry in many significant ways. Important differences include:

As in conventional agriculture, nutrients should be adjusted to satisfy Liebig's law of the minimum for each specific plant variety. Nevertheless, generally acceptable concentrations for nutrient solutions exist, with minimum and maximum concentration ranges for most plants being somewhat similar. Most nutrient solutions are mixed to have concentrations between 1,000 and 2,500 ppm. Acceptable concentrations for the individual nutrient ions, which comprise that total ppm figure, are summarized in the following table. For essential nutrients, concentrations below these ranges often lead to nutrient deficiencies while exceeding these ranges can lead to nutrient toxicity. Optimum nutrition concentrations for plant varieties are found empirically by experience and/or by plant tissue tests.

Organic fertilizers can be used to supplement or entirely replace the inorganic compounds used in conventional hydroponic solutions. However, using organic fertilizers introduces a number of challenges that are not easily resolved. Examples include:

Nevertheless, if precautions are taken, organic fertilizers can be used successfully in hydroponics.

Examples of suitable materials, with their average nutritional contents tabulated in terms of percent dried mass, are listed in the following table.
Micronutrients can be sourced from organic fertilizers as well. For example, composted pine bark is high in manganese and is sometimes used to fulfill that mineral requirement in hydroponic solutions. To satisfy requirements for National Organic Programs, pulverized, unrefined minerals (e.g. Gypsum, Calcite, and glauconite) can also be added to satisfy a plant's nutritional needs.

In addition to chelating agents, humic acids can be added to increase nutrient uptake.

Managing nutrient concentrations and pH values within acceptable ranges is essential for successful hydroponic horticulture. Common tools used to manage hydroponic solutions include:

Advanced equipment can also be used to perform accurate chemical analyses of nutrient solutions. Examples include:

Using advanced equipment for hydroponic solutions can be beneficial to growers of any background because nutrient solutions are often reusable. Because nutrient solutions are virtually never completely depleted, and should never be due to the unacceptably low osmotic pressure that would result, re-fortification of old solutions with new nutrients can save growers money and can control point source pollution, a common source for the eutrophication of nearby lakes and streams.

Although pre-mixed concentrated nutrient solutions are generally purchased from commercial nutrient manufacturers by hydroponic hobbyists and small commercial growers, several tools exist to help anyone prepare their own solutions without extensive knowledge about chemistry. The free and open source tools HydroBuddy and HydroCal have been created by professional chemists to help any hydroponics grower prepare their own nutrient solutions. The first program is available for Windows, Mac and Linux while the second one can be used through a simple JavaScript interface. Both programs allow for basic nutrient solution preparation although HydroBuddy provides added functionality to use and save custom substances, save formulations and predict electrical conductivity values.

Often mixing hydroponic solutions using individual salts is impractical for hobbyists and/or small-scale commercial growers because commercial products are available at reasonable prices. However, even when buying commercial products, multi-component fertilizers are popular. Often these products are bought as three part formulas which emphasize certain nutritional roles. For example, solutions for vegetative growth (i.e. high in nitrogen), flowering (i.e. high in potassium and phosphorus), and micronutrient solutions (i.e. with trace minerals) are popular. The timing and application of these multi-part fertilizers should coincide with a plant's growth stage. For example, at the end of an annual plant's life cycle, a plant should be restricted from high nitrogen fertilizers. In most plants, nitrogen restriction inhibits vegetative growth and helps induce flowering.

With pest problems reduced and nutrients constantly fed to the roots, productivity in hydroponics is high; however, growers can further increase yield by manipulating a plant's environment by constructing sophisticated growrooms.

To increase yield further, some sealed greenhouses inject CO into their environment to help improve growth and plant fertility.



</doc>
<doc id="14134" url="https://en.wikipedia.org/wiki?curid=14134" title="Humanist (disambiguation)">
Humanist (disambiguation)

Humanist may refer to:




</doc>
<doc id="14135" url="https://en.wikipedia.org/wiki?curid=14135" title="Henry Purcell">
Henry Purcell

Henry Purcell ( or ; September 1659 – 21 November 1695) was an English composer. Although incorporating Italian and French stylistic elements into his compositions, Purcell's legacy was a uniquely English form of Baroque music. He is generally considered to be one of the greatest English composers; no later native-born English composer approached his fame until Edward Elgar, Ralph Vaughan Williams, William Walton and Benjamin Britten in the 20th century.

Purcell was born in St Ann's Lane, Old Pye Street, Westminster – the area of London later known as Devil's Acre – in 1659. Henry Purcell Senior, whose older brother, Thomas Purcell, (died 1682) was a musician, was a gentleman of the Chapel Royal and sang at the coronation of King Charles II of England. Henry the elder had three sons: Edward, Henry and Daniel. Daniel Purcell (died 1717), the youngest of the brothers, was also a prolific composer who wrote the music for much of the final act of "The Indian Queen" after Henry Purcell's death. Henry Purcell's family lived just a few hundred yards west of Westminster Abbey from 1659 onwards.

After his father's death in 1664, Purcell was placed under the guardianship of his uncle Thomas, who showed him great affection and kindness. Thomas was himself a gentleman of His Majesty's Chapel, and arranged for Henry to be admitted as a chorister. Henry studied first under Captain Henry Cooke (died 1672), Master of the Children, and afterwards under Pelham Humfrey (died 1674), Cooke's successor. Henry was a chorister in the Chapel Royal until his voice broke in 1673, when he became assistant to the organ-builder John Hingston, who held the post of keeper of wind instruments to the King.

Purcell is said to have been composing at nine years old, but the earliest work that can be certainly identified as his is an ode for the King's birthday, written in 1670. (The dates for his compositions are often uncertain, despite considerable research.) It is assumed that the three-part song "Sweet tyranness, I now resign" was written by him as a child. After Humfrey's death, Purcell continued his studies under Dr John Blow. He attended Westminster School and in 1676 was appointed copyist at Westminster Abbey. Henry Purcell's earliest anthem "Lord, who can tell" was composed in 1678. It is a psalm that is prescribed for Christmas Day and also to be read at morning prayer on the fourth day of the month.

In 1679, he wrote songs for John Playford's "Choice Ayres, Songs and Dialogues" and an anthem, the name of which is unknown, for the Chapel Royal. From an extant letter written by Thomas Purcell we learn that this anthem was composed for the exceptionally fine voice of the Rev. John Gostling, then at Canterbury, but afterwards a gentleman of His Majesty's Chapel. Purcell wrote several anthems at different times for Gostling's extraordinary basso profondo voice, which is known to have had a range of at least two full octaves, from D below the bass staff to the D above it. The dates of very few of these sacred compositions are known; perhaps the most notable example is the anthem "They that go down to the sea in ships." In gratitude for the providential escape of King Charles II from shipwreck, Gostling, who had been of the royal party, put together some verses from the Psalms in the form of an anthem and requested Purcell to set them to music. The challenging work opens with a passage which traverses the full extent of Gostling's range, beginning on the upper D and descending two octaves to the lower.

In 1679, Blow, who had been appointed organist of Westminster Abbey 10 years before, resigned his office in favour of Purcell. Purcell now devoted himself almost entirely to the composition of sacred music, and for six years severed his connection with the theatre. However, during the early part of the year, probably before taking up his new office, he had produced two important works for the stage, the music for Nathaniel Lee's "Theodosius", and Thomas d'Urfey's "Virtuous Wife". Between 1680 and 1688 Purcell wrote music for seven plays. The composition of his chamber opera "Dido and Aeneas", which forms a very important landmark in the history of English dramatic music, has been attributed to this period, and its earliest production may well have predated the documented one of 1689. It was written to a libretto furnished by Nahum Tate, and performed in 1689 in cooperation with Josias Priest, a dancing master and the choreographer for the Dorset Garden Theatre. Priest's wife kept a boarding school for young gentlewomen, first in Leicester Fields and afterwards at Chelsea, where the opera was performed. It is occasionally considered the first genuine English opera, though that title is usually given to Blow's "Venus and Adonis": as in Blow's work, the action does not progress in spoken dialogue but in Italian-style recitative. Each work runs to less than one hour. At the time, "Dido and Aeneas" never found its way to the theatre, though it appears to have been very popular in private circles. It is believed to have been extensively copied, but only one song was printed by Purcell's widow in "Orpheus Britannicus", and the complete work remained in manuscript until 1840, when it was printed by the Musical Antiquarian Society under the editorship of Sir George Macfarren. The composition of "Dido and Aeneas" gave Purcell his first chance to write a sustained musical setting of a dramatic text. It was his only opportunity to compose a work in which the music carried the entire drama. The story of "Dido and Aeneas" derives from the original source in Virgil's epic the "Aeneid".

Soon after Purcell's marriage, in 1682, on the death of Edward Lowe, he was appointed organist of the Chapel Royal, an office which he was able to hold simultaneously with his position at Westminster Abbey. His eldest son was born in this same year, but he was short-lived. His first printed composition, "Twelve Sonatas", was published in 1683. For some years after this, he was busy in the production of sacred music, odes addressed to the king and royal family, and other similar works. In 1685, he wrote two of his finest anthems, "I was glad" and "My heart is inditing," for the coronation of King James II. In 1690 he composed a setting of the birthday ode for Queen Mary, "Arise, my muse" and four years later wrote one of his most elaborate, important and magnificent works – a setting for another birthday ode for the Queen, written by Nahum Tate, entitled "Come Ye Sons of Art".
In 1687, he resumed his connection with the theatre by furnishing the music for John Dryden's tragedy "Tyrannick Love". In this year, Purcell also composed a march and passepied called "Quick-step", which became so popular that Lord Wharton adapted the latter to the fatal verses of "Lillibullero"; and in or before January 1688, Purcell composed his anthem "Blessed are they that fear the Lord" by express command of the King. A few months later, he wrote the music for D'Urfey's play, "The Fool's Preferment". In 1690, he composed the music for Betterton's adaptation of Fletcher and Massinger's "Prophetess" (afterwards called "Dioclesian") and Dryden's "Amphitryon". In 1691, he wrote the music for what is sometimes considered his dramatic masterpiece, "King Arthur", or "The British Worthy ". In 1692, he composed "The Fairy-Queen" (an adaptation of Shakespeare's "A Midsummer Night's Dream"), the score of which (his longest for theatre) was rediscovered in 1901 and published by the Purcell Society. "The Indian Queen" followed in 1695, in which year he also wrote songs for Dryden and Davenant's version of Shakespeare's "The Tempest" (recently, this has been disputed by music scholars), probably including "Full fathom five" and "Come unto these yellow sands". "The Indian Queen" was adapted from a tragedy by Dryden and Sir Robert Howard. In these semi-operas (another term for which at the time was "dramatic opera"), the main characters of the plays do not sing but speak their lines: the action moves in dialogue rather than recitative. The related songs are sung "for" them by singers, who have minor dramatic roles.
Purcell's "Te Deum" and "Jubilate Deo" were written for Saint Cecilia's Day, 1694, the first English "Te Deum" ever composed with orchestral accompaniment. This work was annually performed at St Paul's Cathedral until 1712, after which it was performed alternately with Handel's "Utrecht Te Deum and Jubilate" until 1743, when both works were replaced by Handel's "Dettingen Te Deum".

He composed an anthem and two elegies for Queen Mary II's funeral. Besides the operas and semi-operas already mentioned, Purcell wrote the music and songs for Thomas d'Urfey's "The Comical History of Don Quixote", "Bonduca", "The Indian Queen" and others, a vast quantity of sacred music, and numerous odes, cantatas, and other miscellaneous pieces. The quantity of his instrumental chamber music is minimal after his early career, and his keyboard music consists of an even more minimal number of harpsichord suites and organ pieces. In 1693, Purcell composed music for two comedies: "The Old Bachelor", and "The Double Dealer". Purcell also composed for five other plays within the same year. In July 1695, Purcell composed an ode for the Duke of Gloucester for his sixth birthday. The ode is titled "Who can from joy refrain?" Purcell's four-part sonatas were issued in 1697. In the final six years of his life, Purcell wrote music for forty-two plays.

Purcell died in 1695 at his home in Marsham Street, at the height of his career. He is believed to have been 35 or 36 years old at the time. The cause of his death is unclear: one theory is that he caught a chill after returning home late from the theatre one night to find that his wife had locked him out. Another is that he succumbed to tuberculosis. The beginning of Purcell's will reads:

Purcell is buried adjacent to the organ in Westminster Abbey. The music that he had earlier composed for Queen Mary's funeral was performed during his funeral as well. Purcell was universally mourned as "a very great master of music."  Following his death, the officials at Westminster honoured him by unanimously voting that he be buried with no expense in the north aisle of the Abbey. His epitaph reads: "Here lyes Henry Purcell Esq., who left this life and is gone to that Blessed Place where only His harmony can be exceeded."

Purcell fathered six children by his wife Frances, four of whom died in infancy. His wife, as well as his son Edward (1689–1740) and daughter Frances, survived him. Frances the elder died in 1706, having published a number of her husband's works, including the now famous collection called "Orpheus Britannicus", in two volumes, printed in 1698 and 1702, respectively. Edward was appointed organist of St Clement Eastcheap, London, in 1711 and was succeeded by his son Edward Henry Purcell (died 1765). Both men were buried in St Clement's near the organ gallery.

Purcell worked in many genres, both in works closely linked to the court, such as symphony song, to the Chapel Royal, such as the symphony anthem, and the theatre.

After his death, Purcell was honoured by many of his contemporaries, including his old friend John Blow, who wrote "An Ode, on the Death of Mr. Henry Purcell (Mark how the lark and linnet sing)" with text by his old collaborator, John Dryden. William Croft's 1724 setting for the Burial Service, was written in the style of "the great Master". Croft preserved Purcell's setting of "Thou knowest Lord" (Z 58) in his service, for reasons "obvious to any artist"; it has been sung at every British state funeral ever since. More recently, the English poet Gerard Manley Hopkins wrote a famous sonnet entitled simply "Henry Purcell", with a headnote reading: "The poet wishes well to the divine genius of Purcell and praises him that, whereas other musicians have given utterance to the moods of man's mind, he has, beyond that, uttered in notes the very make and species of man as created both in him and in all men generally."

Purcell also had a strong influence on the composers of the English musical renaissance of the early 20th century, most notably Benjamin Britten, who created and performed a realisation of "Dido and Aeneas" and whose "The Young Person's Guide to the Orchestra" is based on a theme from Purcell's "Abdelazar". Stylistically, the aria "I know a bank" from Britten's opera "A Midsummer Night's Dream" is clearly inspired by Purcell's aria "Sweeter than Roses", which Purcell originally wrote as part of incidental music to Richard Norton's "Pausanias, the Betrayer of His Country".

Purcell is honoured together with Johann Sebastian Bach and George Frideric Handel with a feast day on the liturgical calendar of the Episcopal Church (USA) on 28 July. In a 1940 interview Ignaz Friedman stated that he considered Purcell as great as Bach and Beethoven. In Victoria Street, Westminster, England, there is a bronze monument to Purcell, sculpted by Glynn Williams and erected in 1994.

Purcell's works have been catalogued by Franklin Zimmerman, who gave them a number preceded by Z.

A Purcell Club was founded in London in 1836 for promoting the performance of his music, but was dissolved in 1863. In 1876 a Purcell Society was founded, which published new editions of his works. A modern-day Purcell Club has been created, and provides guided tours and concerts in support of Westminster Abbey.

So strong was his reputation that a popular wedding processional was incorrectly attributed to Purcell for many years. The so-called "Purcell's Trumpet Voluntary" was in fact written around 1700 by a British composer named Jeremiah Clarke as the "Prince of Denmark's March".

The 1973 "Rolling Stone" review of Jethro Tull's "A Passion Play" compared the musical style of the album with the one of Purcell.
In 2009 Pete Townshend of The Who, an English rock band that established itself in the 1960s, identified Purcell's harmonies, particularly the use of suspension and resolution that Townshend had learned from producer Kit Lambert, as an influence on the band's music (in songs such as "Won't Get Fooled Again" (1971), "I Can See for Miles" (1967) and the very Purcellian intro to "Pinball Wizard"). The processional section from Purcell's Music for the Funeral of Queen Mary was adapted for the synthesiser by Wendy Carlos to serve as the theme music for the 1971 film "A Clockwork Orange"; the music is also featured in the final scene of the 1995 film "The Young Poisoner's Handbook." Noted cult New Wave artist Klaus Nomi regularly performed "The Cold Song" from "King Arthur" during his career, including a version on his debut self-titled album, "Klaus Nomi", from 1981; his last public performance before his untimely death was an interpretation of the piece done with a full orchestra in December 1982 in Munich. Purcell wrote the song for a bass, but numerous countertenors have performed the piece in homage to Nomi. Sting recorded it on his 2009 album "If on a Winter's Night...".

In the 1995 film "England, My England", the life of Purcell (played by Michael Ball) was depicted as seen through the eyes of a playwright in the 1960s who is trying to write a play about him.

In the 21st century, the soundtrack of the 2005 film version of "Pride and Prejudice" features a dance titled "A Postcard to Henry Purcell." This is a version by composer Dario Marianelli of Purcell's "Abdelazar" theme. In the German-language 2004 movie, "Downfall", the music of Dido's Lament is used repeatedly as the end of the Third Reich culminates. The 2012 film "Moonrise Kingdom" contains Benjamin Britten's version of the Rondeau in Purcell's "Abdelazar" created for his 1946 "The Young Person's Guide to the Orchestra". In 2013, the Pet Shop Boys released their single "Love Is a Bourgeois Construct" incorporating one of the same ground basses from "King Arthur" used by Nyman in his "Draughtsman's Contract" score. Olivia Chaney performs her adaptation of "There's Not a Swain" on her CD "The Longest River."

Bibliography



</doc>
<doc id="14136" url="https://en.wikipedia.org/wiki?curid=14136" title="Hydrophobe">
Hydrophobe

In chemistry, hydrophobicity is the physical property of a molecule (known as a hydrophobe) that is seemingly repelled from a mass of water. (Strictly speaking, there is no repulsive force involved; it is an absence of attraction.) In contrast, hydrophiles are attracted to water.

Hydrophobic molecules tend to be nonpolar and, thus, prefer other neutral molecules and nonpolar solvents. Because water molecules are polar, hydrophobes do not dissolve well among them. Hydrophobic molecules in water often cluster together, forming micelles. Water on hydrophobic surfaces will exhibit a high contact angle.

Examples of hydrophobic molecules include the alkanes, oils, fats, and greasy substances in general. Hydrophobic materials are used for oil removal from water, the management of oil spills, and chemical separation processes to remove non-polar substances from polar compounds.

Hydrophobic is often used interchangeably with lipophilic, "fat-loving". However, the two terms are not synonymous. While hydrophobic substances are usually lipophilic, there are exceptions, such as the silicones and fluorocarbons.

The term "hydrophobe" comes from the Ancient Greek ὑδρόφοβος, "having a horror of water", constructed from ὕδωρ, "water", and φόβος, "fear".

The hydrophobic interaction is mostly an entropic effect originating from the disruption of the highly dynamic hydrogen bonds between molecules of liquid water by the nonpolar solute forming a clathrate-like structure around the non-polar molecules. This structure formed is more highly ordered than free water molecules due to the water molecules arranging themselves to interact as much as possible with themselves, and thus results in a higher entropic state which causes non-polar molecules to clump together to reduce the surface area exposed to water and decrease the entropy of the system. Thus, the 2 immiscible phases (hydrophilic vs. hydrophobic) will change so that their corresponding interfacial area will be minimal. This effect can be visualized in the phenomenon called phase separation.

Superhydrophobic surfaces, such as the leaves of the lotus plant, are those that are extremely difficult to wet. The contact angles of a water droplet exceeds 150°. This is referred to as the lotus effect, and is primarily a physical property related to interfacial tension, rather than a chemical property.

In 1805, Thomas Young defined the contact angle "θ" by analyzing the forces acting on a fluid droplet resting on a solid surface surrounded by a gas.

where

"θ" can be measured using a contact angle goniometer.

Wenzel determined that when the liquid is in intimate contact with a microstructured surface, "θ" will change to "θ"

where "r" is the ratio of the actual area to the projected area. Wenzel's equation shows that microstructuring a surface amplifies the natural tendency of the surface. A hydrophobic surface (one that has an original contact angle greater than 90°) becomes more hydrophobic when microstructured – its new contact angle becomes greater than the original. However, a hydrophilic surface (one that has an original contact angle less than 90°) becomes more hydrophilic when microstructured – its new contact angle becomes less than the original.
Cassie and Baxter found that if the liquid is suspended on the tops of microstructures, "θ" will change to "θ":

where φ is the area fraction of the solid that touches the liquid. Liquid in the Cassie–Baxter state is more mobile than in the Wenzel state.

We can predict whether the Wenzel or Cassie–Baxter state should exist by calculating the new contact angle with both equations. By a minimization of free energy argument, the relation that predicted the smaller new contact angle is the state most likely to exist. Stated in mathematical terms, for the Cassie–Baxter state to exist, the following inequality must be true.

A recent alternative criterion for the Cassie–Baxter state asserts that the Cassie–Baxter state exists when the following 2 criteria are met:1) Contact line forces overcome body forces of unsupported droplet weight and 2) The microstructures are tall enough to prevent the liquid that bridges microstructures from touching the base of the microstructures.

A new criterion for the switch between Wenzel and Cassie-Baxter states has been developed recently based on surface roughness and surface energy. The criterion focuses on the air-trapping capability under liquid droplets on rough surfaces, which could tell whether Wenzel's model or Cassie-Baxter's model should be used for certain combination of surface roughness and energy.

Contact angle is a measure of static hydrophobicity, and contact angle hysteresis and slide angle are dynamic measures. Contact angle hysteresis is a phenomenon that characterizes surface heterogeneity. When a pipette injects a liquid onto a solid, the liquid will form some contact angle. As the pipette injects more liquid, the droplet will increase in volume, the contact angle will increase, but its three-phase boundary will remain stationary until it suddenly advances outward. The contact angle the droplet had immediately before advancing outward is termed the advancing contact angle. The receding contact angle is now measured by pumping the liquid back out of the droplet. The droplet will decrease in volume, the contact angle will decrease, but its three-phase boundary will remain stationary until it suddenly recedes inward. The contact angle the droplet had immediately before receding inward is termed the receding contact angle. The difference between advancing and receding contact angles is termed contact angle hysteresis and can be used to characterize surface heterogeneity, roughness, and mobility. Surfaces that are not homogeneous will have domains that impede motion of the contact line. The slide angle is another dynamic measure of hydrophobicity and is measured by depositing a droplet on a surface and tilting the surface until the droplet begins to slide. In general, liquids in the Cassie–Baxter state exhibit lower slide angles and contact angle hysteresis than those in the Wenzel state.

Dettre and Johnson discovered in 1964 that the superhydrophobic lotus effect phenomenon was related to rough hydrophobic surfaces, and they developed a theoretical model based on experiments with glass beads coated with paraffin or TFE telomer. The self-cleaning property of superhydrophobic micro-nanostructured surfaces was reported in 1977. Perfluoroalkyl, perfluoropolyether, and RF plasma -formed superhydrophobic materials were developed, used for electrowetting and commercialized for bio-medical applications between 1986 and 1995. Other technology and applications have emerged since the mid 1990s. A durable superhydrophobic hierarchical composition, applied in one or two steps, was disclosed in 2002 comprising nano-sized particles ≤ 100 nanometers overlaying a surface having micrometer-sized features or particles ≤ 100 micrometers. The larger particles were observed to protect the smaller particles from mechanical abrasion.

In recent research, superhydrophobicity has been reported by allowing alkylketene dimer (AKD) to solidify into a nanostructured fractal surface. Many papers have since presented fabrication methods for producing superhydrophobic surfaces including particle deposition, sol-gel techniques, plasma treatments, vapor deposition, and casting techniques. Current opportunity for research impact lies mainly in fundamental research and practical manufacturing. Debates have recently emerged concerning the applicability of the Wenzel and Cassie–Baxter models. In an experiment designed to challenge the surface energy perspective of the Wenzel and Cassie–Baxter model and promote a contact line perspective, water drops were placed on a smooth hydrophobic spot in a rough hydrophobic field, a rough hydrophobic spot in a smooth hydrophobic field, and a hydrophilic spot in a hydrophobic field. Experiments showed that the surface chemistry and geometry at the contact line affected the contact angle and contact angle hysteresis, but the surface area inside the contact line had no effect. An argument that increased jaggedness in the contact line enhances droplet mobility has also been proposed.

Many hydrophobic materials found in nature rely on Cassie's law and are biphasic on the submicrometer level with one component air. The lotus effect is based on this principle. Inspired by it, many functional superhydrophobic surfaces have been prepared.

An example of a bionic or biomimetic superhydrophobic material in nanotechnology is nanopin film.

One study presents a vanadium pentoxide surface that switches reversibly between superhydrophobicity and superhydrophilicity under the influence of UV radiation. According to the study, any surface can be modified to this effect by application of a suspension of rose-like VO particles, for instance with an inkjet printer. Once again hydrophobicity is induced by interlaminar air pockets (separated by 2.1 nm distances). The UV effect is also explained. UV light creates electron-hole pairs, with the holes reacting with lattice oxygen, creating surface oxygen vacancies, while the electrons reduce V to V. The oxygen vacancies are met by water, and it is this water absorbency by the vanadium surface that makes it hydrophilic. By extended storage in the dark, water is replaced by oxygen and hydrophilicity is once again lost.

Hydrophobic concrete has been produced since the mid-20th century.

Active recent research on superhydrophobic materials might eventually lead to more industrial applications.

A simple routine of coating cotton fabric with silica or titania particles by sol-gel technique has been reported, which protects the fabric from UV light and makes it superhydrophobic.

An efficient routine has been reported for making polyethylene superhydrophobic and thus self-cleaning. —99% of dirt absorbed on such surface is easily washed away.

Patterned superhydrophobic surfaces also have promise for lab-on-a-chip microfluidic devices and can drastically improve surface-based bioanalysis.




</doc>
<doc id="14142" url="https://en.wikipedia.org/wiki?curid=14142" title="Harley-Davidson">
Harley-Davidson

Harley-Davidson, Inc. (H-D), or Harley, is an American motorcycle manufacturer, founded in Milwaukee, Wisconsin in 1903.

One of two major American motorcycle manufacturers to survive the Great Depression (along with Indian), the company has survived numerous ownership arrangements, subsidiary arrangements (e.g., Aermacchi 1974-1978 and Buell 1987-2009), periods of poor economic health and product quality, as well as intense global competition, to become one of the world's largest motorcycle manufacturers and an iconic brand widely known for its loyal following. There are owner clubs and events worldwide as well as a company-sponsored brand-focused museum.

Noted for a style of customization that gave rise to the chopper motorcycle style, Harley-Davidson traditionally marketed heavyweight, air-cooled cruiser motorcycles with engine displacements greater than 700 cm³ and has broadened its offerings to include its more contemporary VRSC (2002) and middle-weight Street (2015) platforms.

Harley-Davidson manufactures its motorcycles at factories in York, Pennsylvania; Milwaukee, Wisconsin; Kansas City, Missouri (closing); Manaus, Brazil; and Bawal, India. Construction of a new plant in Thailand is scheduled to begin in late 2018. The company markets its products worldwide.

Besides motorcycles, the company licenses and markets merchandise under the Harley-Davidson brand, among them apparel, home decor and ornaments, accessories, toys, and scale figures of its motorcycles, and video games based on its motorcycle line and the community.

In 1901, -year-old William S. Harley drew up plans for a small engine with a displacement of 7.07 cubic inches (116 cc³) and four-inch (102 mm) flywheels. The engine was designed for use in a regular pedal-bicycle frame. Over the next two years, Harley and his childhood friend Arthur Davidson worked on their motor-bicycle using the northside Milwaukee machine shop at the home of their friend, Henry Melk. It was finished in 1903 with the help of Arthur's brother, Walter Davidson. Upon testing their power-cycle, Harley and the Davidson brothers found it unable to climb the hills around Milwaukee without pedal assistance. They quickly wrote off their first motor-bicycle as a valuable learning experiment.

Work immediately began on a new and improved second-generation machine. This first "real" Harley-Davidson motorcycle had a bigger engine of 24.74 cubic inches (405 cc³) with flywheels weighing . The machine's advanced loop-frame pattern was similar to the 1903 Milwaukee Merkel motorcycle (designed by Joseph Merkel, later of Flying Merkel fame). The bigger engine and loop-frame design took it out of the motorized bicycle category and marked the path to future motorcycle designs. The boys also received help with their bigger engine from outboard motor pioneer Ole Evinrude, who was then building gas engines of his own design for automotive use on Milwaukee's Lake Street.
The prototype of the new loop-frame Harley-Davidson was assembled in a shed in the Davidson family backyard. Most of the major parts, however, were made elsewhere, including some probably fabricated at the West Milwaukee railshops where oldest brother William A. Davidson was then toolroom foreman. This prototype machine was functional by September 8, 1904, when it competed in a Milwaukee motorcycle race held at State Fair Park. It was ridden by Edward Hildebrand and placed fourth. This is the first documented appearance of a Harley-Davidson motorcycle in the historical record.

In January 1905, small advertisements were placed in the "Automobile and Cycle Trade Journal" offering bare Harley-Davidson engines to the do-it-yourself trade. By April, complete motorcycles were in production on a very limited basis. That year, the first Harley-Davidson dealer, Carl H. Lang of Chicago, sold three bikes from the five built in the Davidson backyard shed. Years later the original shed was taken to the Juneau Avenue factory where it would stand for many decades as a tribute to the Motor Company's humble origins until it was accidentally destroyed by contractors cleaning the factory yard in the early 1970s.

In 1906, Harley and the Davidson brothers built their first factory on Chestnut Street (later Juneau Avenue), at the current location of Harley-Davidson's corporate headquarters. The first Juneau Avenue plant was a single-story wooden structure. The company produced about 50 motorcycles that year.

In 1907, William S. Harley graduated from the University of Wisconsin–Madison with a degree in mechanical engineering. That year additional factory expansion came with a second floor and later with facings and additions of Milwaukee pale yellow ("cream") brick. With the new facilities production increased to 150 motorcycles in 1907. The company was officially incorporated that September. They also began selling their motorcycles to police departments around this time, a market that has been important to them ever since.

In 1907 William A. Davidson, brother to Arthur and Walter Davidson, quit his job as tool foreman for the Milwaukee Road railroad and joined the Motor Company.

Production in 1905 and 1906 were all single-cylinder models with 26.84 cubic inch (440 cm³) engines. In February 1907 a prototype model with a 45-degree V-Twin engine was displayed at the Chicago Automobile Show. Although shown and advertised, very few V-Twin models were built between 1907 and 1910. These first V-Twins displaced 53.68 cubic inches (880 cm³) and produced about . This gave about double the power of the first singles. Top speed was about . Production jumped from 450 motorcycles in 1908 to 1,149 machines in 1909.

By 1911, some 150 makes of motorcycles had already been built in the United States – although just a handful would survive the 1910s.

In 1911, an improved V-Twin model was introduced. The new engine had mechanically operated intake valves, as opposed to the "automatic" intake valves used on earlier V-Twins that opened by engine vacuum. With a displacement of 49.48 cubic inches (811 cm³), the 1911 V-Twin was smaller than earlier twins, but gave better performance. After 1913 the majority of bikes produced by Harley-Davidson would be V-Twin models.

In 1912, Harley-Davidson introduced their patented "Ful-Floteing Seat", which was suspended by a coil spring inside the seat tube. The spring tension could be adjusted to suit the rider's weight. More than of travel was available. Harley-Davidson would use seats of this type until 1958.

By 1913, the yellow brick factory had been demolished and on the site a new 5-story structure had been built. Begun in 1910, the factory with its many additions would take up two blocks along Juneau Avenue and around the corner on 38th Street. Despite the competition, Harley-Davidson was already pulling ahead of Indian and would dominate motorcycle racing after 1914. Production that year swelled to 16,284 machines.
In 1917, the United States entered World War I and the military demanded motorcycles for the war effort. Harleys had already been used by the military in the Pancho Villa Expedition but World War I was the first time the motorcycle had been adopted for military issue, first with the British Model H, produced by British Triumph Motorcycles Ltd in 1915. After the U.S. entry into the war, the U.S. military purchased over 20,000 motorcycles from Harley-Davidson.

Harley-Davidson launched a line of bicycles in 1917 in hopes of recruiting customers for its motorcycles. Besides the traditional diamond frame men's bicycle, models included a step-through frame 3-18 "Ladies Standard" and a 5-17 "Boy Scout" for youth. The effort was discontinued in 1923 because of disappointing sales.

The bicycles were built for Harley-Davidson in Dayton, Ohio, by the Davis Machine Company from 1917 to 1921, when Davis stopped manufacturing bicycles.

By 1920, Harley-Davidson was the largest motorcycle manufacturer in the world, with 28,189 machines produced, and dealers in 67 countries.

In 1921, a Harley-Davidson, ridden by Otto Walker, was the first motorcycle ever to win a race at an average speed greater than .

During the 1920s, several improvements were put in place, such as a new 74 cubic inch (1,212.6  cm³) V-Twin, introduced in 1921, and the "teardrop" gas tank in 1925. A front brake was added in 1928 although notably only on the J/JD models.

In the late summer of 1929, Harley-Davidson introduced its 45 cubic inches (737 cm³) flathead V-Twin to compete with the Indian 101 Scout and the Excelsior Super X. This was the "D" model, produced from 1929 to 1931. Riders of Indian motorcycles derisively referred to this model as the "three cylinder Harley" because the generator was upright and parallel to the front cylinder.

The Great Depression began a few months after the introduction of their 45 cubic inch (737 cm³) model. Harley-Davidson's sales fell from 21,000 in 1929 to 3,703 in 1933. Despite this, Harley-Davidson unveiled a new lineup for 1934, which included a flathead engine and Art Deco styling.

In order to survive the remainder of the Depression, the company manufactured industrial powerplants based on their motorcycle engines. They also designed and built a three-wheeled delivery vehicle called the Servi-Car, which remained in production until 1973.

In the mid-1930s, Alfred Rich Child opened a production line in Japan with the 74-cubic-inch (1,210 cm³) VL. The Japanese license-holder, Sankyo Seiyaku Corporation, severed its business relations with Harley-Davidson in 1936 and continued manufacturing the VL under the Rikuo name.

An 80-cubic-inch (1,300 cm³) flathead engine was added to the line in 1935, by which time the single-cylinder motorcycles had been discontinued.

In 1936, the 61E and 61EL models with the "Knucklehead" OHV engines were introduced. Valvetrain problems in early Knucklehead engines required a redesign halfway through its first year of production and retrofitting of the new valvetrain on earlier engines.

By 1937, all Harley-Davidson flathead engines were equipped with dry-sump oil recirculation systems similar to the one introduced in the "Knucklehead" OHV engine. The revised 74-cubic-inch (1,212 cc ) V and VL models were renamed U and UL, the 80-cubic-inch (1,300 cc³) VH and VLH to be renamed UH and ULH, and the 45-cubic-inch (740 cc³) R to be renamed W.

In 1941, the 74-cubic-inch (1,210 cm³) "Knucklehead" was introduced as the F and the FL. The 80-cubic-inch (1,300 cc³) flathead UH and ULH models were discontinued after 1941, while the 74 inch (1210 cm³) U & UL flathead models were produced up to 1948.

One of only two American cycle manufacturers to survive the Great Depression, Harley-Davidson again produced large numbers of motorcycles for the US Army in World War II and resumed civilian production afterwards, producing a range of large V-twin motorcycles that were successful both on racetracks and for private buyers.

Harley-Davidson, on the eve of World War II, was already supplying the Army with a military-specific version of its 45 cubic inches (740 cm³) WL line, called the WLA. The A in this case stood for "Army". Upon the outbreak of war, the company, along with most other manufacturing enterprises, shifted to war work. More than 90,000 military motorcycles, mostly WLAs and WLCs (the Canadian version) were produced, many to be provided to allies. Harley-Davidson received two Army-Navy ‘E’ Awards, one in 1943 and the other in 1945, which were awarded for Excellence in Production.
Shipments to the Soviet Union under the Lend-Lease program numbered at least 30,000. The WLAs produced during all four years of war production generally have 1942 serial numbers. Production of the WLA stopped at the end of World War II, but was resumed from 1950 to 1952 for use in the Korean War.

The U.S. Army also asked Harley-Davidson to produce a new motorcycle with many of the features of BMW's side-valve and shaft-driven R71. Harley largely copied the BMW engine and drive train and produced the shaft-driven 750 cc 1942 Harley-Davidson XA. This shared no dimensions, no parts and no design concepts (except side valves) with any prior Harley-Davidson engine. Due to the superior cooling of the flat-twin engine with the cylinders across the frame, Harley's XA cylinder heads ran 100 °F (56 °C) cooler than its V-twins. The XA never entered full production: the motorcycle by that time had been eclipsed by the Jeep as the Army's general purpose vehicle, and the WLA—already in production—was sufficient for its limited police, escort, and courier roles. Only 1,000 were made and the XA never went into full production. It remains the only shaft-driven Harley-Davidson ever made.

As part of war reparations, Harley-Davidson acquired the design of a small German motorcycle, the DKW RT 125, which they adapted, manufactured, and sold from 1948 to 1966. Various models were made, including the Hummer from 1955 to 1959, but they are all colloquially referred to as "Hummers" at present. BSA in the United Kingdom took the same design as the foundation of their BSA Bantam.
In 1960, Harley-Davidson consolidated the Model 165 and Hummer lines into the Super-10, introduced the Topper scooter, and bought fifty percent of Aermacchi's motorcycle division. Importation of Aermacchi's 250 cc horizontal single began the following year. The bike bore Harley-Davidson badges and was marketed as the Harley-Davidson Sprint. The engine of the Sprint was increased to 350 cc in 1969 and would remain that size until 1974, when the four-stroke Sprint was discontinued.

After the Pacer and Scat models were discontinued at the end of 1965, the Bobcat became the last of Harley-Davidson's American-made two-stroke motorcycles. The Bobcat was manufactured only in the 1966 model year.

Harley-Davidson replaced their American-made lightweight two-stroke motorcycles with the Italian Aermacchi-built two-stroke powered M-65, M-65S, and Rapido. The M-65 had a semi-step-through frame and tank. The M-65S was a M-65 with a larger tank that eliminated the step-through feature. The Rapido was a larger bike with a 125 cc engine. The Aermacchi-built Harley-Davidsons became entirely two-stroke powered when the 250 cc two-stroke SS-250 replaced the four-stroke 350 cc Sprint in 1974.

Harley-Davidson purchased full control of Aermacchi's motorcycle production in 1974 and continued making two-stroke motorcycles there until 1978, when they sold the facility to Cagiva, owned by the Castiglioni family.

Established in 1918, the oldest continuously operating Harley-Davidson dealership outside of the United States is in Australia. Sales in Japan started in 1912 then in 1929, Harley-Davidsons were produced in Japan under license to the company Rikuo (Rikuo Internal Combustion Company) under the name of Harley-Davidson and using the company's tooling, and later under the name Rikuo. Production continued until 1958.

In 1952, following their application to the U.S. Tariff Commission for a 40 percent tax on imported motorcycles, Harley-Davidson was charged with restrictive practices.
In 1969, American Machine and Foundry (AMF) bought the company, streamlined production, and slashed the workforce. This tactic resulted in a labor strike and lower-quality bikes. The bikes were expensive and inferior in performance, handling, and quality to Japanese motorcycles. Sales and quality declined, and the company almost went bankrupt. The "Harley-Davidson" name was mocked as "Hardly Ableson", "Hardly Driveable," and "Hogly Ferguson",
and the nickname "Hog" became pejorative.

In 1977, following the successful manufacture of the Liberty Edition to commemorate America's bicentennial in 1976, Harley-Davidson produced what has become one of its most controversial models, the Harley-Davidson Confederate Edition. The bike was essentially a stock Harley with Confederate-specific paint and details.

In 1981, AMF sold the company to a group of 13 investors led by Vaughn Beals and Willie G. Davidson for $80 million. Inventory was strictly controlled using the just-in-time system.

In the early eighties, Harley-Davidson claimed that Japanese manufacturers were importing motorcycles into the US in such volume as to harm or threaten to harm domestic producers. After an investigation by the U.S. International Trade Commission, President Reagan in 1983 imposed a 45 percent tariff on imported bikes with engine capacities greater than 700 cc. Harley-Davidson subsequently rejected offers of assistance from Japanese motorcycle makers.<ref name="7/83 US IMPOSES 45% TARIFF ON IMPORTED MOTORCYCLES"> – 7/83 US Imposes 45% Tariff on Imported Motorcycles</ref> However, the company did offer to drop the request for the tariff in exchange for loan guarantees from the Japanese.

Rather than trying to match the Japanese, the new management deliberately exploited the "retro" appeal of the machines, building motorcycles that deliberately adopted the look and feel of their earlier machines and the subsequent customizations of owners of that era. Many components such as brakes, forks, shocks, carburetors, electrics and wheels were outsourced from foreign manufacturers and quality increased, technical improvements were made, and buyers slowly returned.

Harley-Davidson bought the "Sub Shock" cantilever-swingarm rear suspension design from Missouri engineer Bill Davis and developed it into its Softail series of motorcycles, introduced in 1984 with the FXST Softail.

In response to possible motorcycle market loss due to the aging of baby-boomers, Harley-Davidson bought luxury motorhome manufacturer Holiday Rambler in 1986. In 1996, the company sold Holiday Rambler to the Monaco Coach Corporation.

The "Sturgis" model, boasting a dual belt-drive, was introduced initially in 1980 and was made for three years. This bike was then brought back as a commemorative model in 1991.
By 1990, with the introduction of the "Fat Boy", Harley once again became the sales leader in the heavyweight (over 750 cc³) market. At the time of the Fat Boy model introduction, a story rapidly spread that its silver paint job and other features were inspired by the B-29; and Fat Boy was a combination of the names of the atomic bombs Fat Man and Little Boy. However, the Urban Legend Reference Pages lists this story as an urban legend.

1993 and 1994 saw the replacement of FXR models with the Dyna (FXD), which became the sole rubber mount FX Big Twin frame in 1994. The FXR was revived briefly from 1999 to 2000 for special limited editions (FXR, FXR & FXR).

Construction started on the $75 million, 130,000 square-foot (12,000 m) Harley-Davidson Museum in the Menomonee Valley on June 1, 2006. It opened in 2008 and houses the company's vast collection of historic motorcycles and corporate archives, along with a restaurant, café and meeting space.

Harley-Davidson's association with sportbike manufacturer Buell Motorcycle Company began in 1987 when they supplied Buell with fifty surplus XR1000 engines. Buell continued to buy engines from Harley-Davidson until 1993, when Harley-Davidson bought 49 percent of the Buell Motorcycle Company. Harley-Davidson increased its share in Buell to ninety-eight percent in 1998, and to complete ownership in 2003.

In an attempt to attract newcomers to motorcycling in general and to Harley-Davidson in particular, Buell developed a low-cost, low-maintenance motorcycle. The resulting single-cylinder Buell Blast was introduced in 2000, and was made through 2009, which, according to Buell, was to be the final year of production. The Buell Blast was the training vehicle for the Harley-Davidson Rider's Edge New Rider Course from 2000 until May 2014, when the company re-branded the training academy and started using the Harley-Davidson Street 500 motorcycles. In those 14 years, more than 350,000 participants in the course learned to ride on the Buell Blast.

On October 15, 2009, Harley-Davidson Inc. issued an official statement that it would be discontinuing the Buell line and ceasing production immediately. The stated reason was to focus on the Harley-Davidson brand. The company refused to consider selling Buell. Founder Erik Buell subsequently established Erik Buell Racing and continued to manufacture and develop the company's 1125RR racing motorcycle.

In 1998 the first Harley-Davidson factory outside the US opened in Manaus, Brazil, taking advantage of the free economic zone there. The location was positioned to sell motorcycles in the southern hemisphere market.

During its period of peak demand, during the late 1990s and early first decade of the 21st century, Harley-Davidson embarked on a program of expanding the number of dealerships throughout the country. At the same time, its current dealers typically had waiting lists that extended up to a year for some of the most popular models. Harley-Davidson, like the auto manufacturers, records a sale not when a consumer buys their product, but rather when it is delivered to a dealer. Therefore, it is possible for the manufacturer to inflate sales numbers by requiring dealers to accept more inventory than desired in a practice called channel stuffing. When demand softened following the unique 2003 model year, this news led to a dramatic decline in the stock price. In April 2004 alone, the price of HOG shares dropped from more than $60 to less than $40. Immediately prior to this decline, retiring CEO Jeffrey Bleustein profited $42 million on the exercise of employee stock options. Harley-Davidson was named as a defendant in numerous class action suits filed by investors who claimed they were intentionally defrauded by Harley-Davidson's management and directors. By January 2007, the price of Harley-Davidson shares reached $70.

Starting around 2000, several police departments started reporting problems with high speed instability on the Harley-Davidson Touring motorcycles. A Raleigh, North Carolina police officer, Charles Paul, was killed when his 2002 police touring motorcycle crashed after reportedly experiencing a high speed wobble. The California Highway Patrol conducted testing of the Police Touring motorcycles in 2006. The CHP test riders reported experiencing wobble or weave instability while operating the motorcycles on the test track.

On February 2, 2007, upon the expiration of their union contract, about 2,700 employees at Harley-Davidson Inc.'s largest manufacturing plant in York, Pennsylvania went on strike after failing to agree on wages and health benefits. During the pendency of the strike, the company refused to pay for any portion of the striking employees' health care.

The day before the strike, after the union voted against the proposed contract and to authorize the strike, the company shut down all production at the plant. The York facility employs more than 3,200 workers, both union and non-union.

Harley-Davidson announced on February 16, 2007, that it had reached a labor agreement with union workers at its largest manufacturing plant, a breakthrough in the two-week-old strike. The strike disrupted Harley-Davidson's national production and was felt in Wisconsin, where 440 employees were laid off, and many Harley suppliers also laid off workers because of the strike.

On July 11, 2008 Harley-Davidson announced they had signed a definitive agreement to acquire the MV Agusta Group for $109M USD (€70M). MV Agusta Group contains two lines of motorcycles: the high-performance MV Agusta brand and the lightweight Cagiva brand. The acquisition was completed on August 8.

On October 15, 2009, Harley-Davidson announced that it would divest its interest in MV Agusta. Harley-Davidson Inc. sold Italian motorcycle maker MV Agusta to Claudio Castiglioni - a member of the family that had purchased Aermacchi from H-D in 1978 - for a reported 3 euros, ending the transaction in the first week of August 2010. Castiglioni was MV Agusta's former owner, and had been MV Agusta's chairman since Harley-Davidson bought it in 2008. As part of the deal, Harley-Davidson put $26M into MV Agusta's accounts, essentially giving Castiglioni $26M to take the brand.

In August 2009, Harley-Davidson announced plans to enter the market in India, and started selling motorcycles there in 2010. The company established a subsidiary, Harley-Davidson India, in Gurgaon, near Delhi, in 2011, and created an Indian dealer network.

According to Interbrand, the value of the Harley-Davidson brand fell by 43 percent to $4.34 billion in 2009. The fall in value is believed to be connected to the 66 percent drop in the company profits in two quarters of the previous year. On April 29, 2010, Harley-Davidson stated that they must cut $54 million in manufacturing costs from its production facilities in Wisconsin, and that they would explore alternative U.S. sites to accomplish this. The announcement came in the wake of a massive company-wide restructuring, which began in early 2009 and involved the closing of two factories, one distribution center, and the planned elimination of nearly 25 percent of its total workforce (around 3,500 employees). The company announced on September 14, 2010 that it would remain in Wisconsin.

The classic Harley-Davidson engines are V-twin engines, with a 45° angle between the cylinders. The crankshaft has a single pin, and both pistons are connected to this pin through their connecting rods.

This 45° angle is covered under several United States patents and is an engineering tradeoff that allows a large, high-torque engine in a relatively small space. It causes the cylinders to fire at uneven intervals and produces the choppy "potato-potato" sound so strongly linked to the Harley-Davidson brand.

To simplify the engine and reduce costs, the V-twin ignition was designed to operate with a single set of points and no distributor. This is known as a dual fire ignition system, causing both spark plugs to fire regardless of which cylinder was on its compression stroke, with the other spark plug firing on its cylinder's exhaust stroke, effectively "wasting a spark". The exhaust note is basically a throaty growling sound with some popping.
The 45° design of the engine thus creates a plug firing sequencing as such: The first cylinder fires, the second (rear) cylinder fires 315° later, then there is a 405° gap until the first cylinder fires again, giving the engine its unique sound.

Harley-Davidson has used various ignition systems throughout its history – be it the early points and condenser system, (Big Twin up to 1978 and Sportsters up to 1978), magneto ignition system used on some 1958 to 1969 Sportsters, early electronic with centrifugal mechanical advance weights, (all models 1978 and a half to 1979), or the late electronic with transistorized ignition control module, more familiarly known as the black box or the brain, (all models 1980 to present).

Starting in 1995, the company introduced Electronic Fuel Injection (EFI) as an option for the 30th anniversary edition Electra Glide. EFI became standard on all Harley-Davidson motorcycles, including Sportsters, upon the introduction of the 2007 product line.

In 1991, Harley-Davidson began to participate in the Sound Quality Working Group, founded by Orfield Labs, Bruel and Kjaer, TEAC, Yamaha, Sennheiser, SMS and Cortex. This was the nation's first group to share research on psychological acoustics. Later that year, Harley-Davidson participated in a series of sound quality studies at Orfield Labs, based on recordings taken at the Talladega Superspeedway, with the objective to lower the sound level for EU standards while analytically capturing the "Harley Sound". This research resulted in the bikes that were introduced in compliance with EU standards for 1998.

On February 1, 1994, the company filed a sound trademark application for the distinctive sound of the Harley-Davidson motorcycle engine: "The mark consists of the exhaust sound of applicant's motorcycles, produced by V-twin, common crankpin motorcycle engines when the goods are in use". Nine of Harley-Davidson's competitors filed comments opposing the application, arguing that cruiser-style motorcycles of various brands use a single-crankpin V-twin engine which produce a similar sound. These objections were followed by litigation. In June 2000, the company dropped efforts to federally register its trademark.



The Revolution engine is based on the VR-1000 Superbike race program, co-developed by Harley-Davidson's Powertrain Engineering team and Porsche Engineering in Stuttgart, Germany. It is a liquid cooled, dual overhead cam, internally counterbalanced 60 degree V-twin engine with a displacement of 69 cubic inch (1,130 cm³), producing at 8,250 rpm at the crank, with a redline of 9,000 rpm. It was introduced for the new VRSC (V-Rod) line in 2001 for the 2002 model year, starting with the single VRSCA (V-Twin Racing Street Custom) model. The Revolution marks Harley's first collaboration with Porsche since the V4 Nova project, which, like the V-Rod, was a radical departure from Harley's traditional lineup until it was cancelled by AMF in 1981 in favor of the Evolution engine.

A 1,250 cc Screamin' Eagle version of the Revolution engine was made available for 2005 and 2006, and was present thereafter in a single production model from 2005 to 2007. In 2008, the 1,250 cc Revolution Engine became standard for the entire VRSC line. Harley-Davidson claims at the crank for the 2008 VRSCAW model. The VRXSE "Destroyer" is equipped with a stroker (75 mm crank) Screamin' Eagle 79 cubic inch (1,300 cm³) Revolution Engine, producing more than .

750 cc and 500 cc versions of the Revolution engine are used in Harley-Davidson's Street line of light cruisers. These motors, named the Revolution X, use a single overhead cam, screw and locknut valve adjustment, a single internal counterbalancer, and vertically split crankcases; all of these changes making it different from the original Revolution design.

An extreme endurance test of the Revolution engine was performed in a dynometer installation, simulating the German Autobahn (highways without general speed limit) between the Porsche research and development center in Weissach, near Stuttgart to Düsseldorf. Uncounted samples of engines failed, until an engine successfully passed the 500 hour nonstop run. This was the benchmark for the engineers to approve the start of production for the Revolution engine, which was documented in the Discovery channel special Harley-Davidson: Birth of the V-Rod, October 14, 2001.

The first Harley-Davidson motorcycles were powered by single-cylinder IOE engines with the inlet valve operated by engine vacuum, based on the DeDion-Bouton pattern. Singles of this type continued to be made until 1913, when a pushrod and rocker system was used to operate the overhead inlet valve on the single, a similar system having been used on their V-twins since 1911. Single-cylinder motorcycle engines were discontinued in 1918.

Single-cylinder engines were reintroduced in 1925 as 1926 models. These singles were available either as flathead engines or as overhead valve engines until 1930, after which they were only available as flatheads. The flathead single-cylinder motorcycles were designated Model A for engines with magneto systems only and Model B for engines with battery and coil systems, while overhead valve versions were designated Model AA and Model BA respectively, and a magneto-only racing version was designated Model S. This line of single-cylinder motorcycles ended production in 1934.

Modern Harley-branded motorcycles fall into one of six model families: Touring, Softail, Dyna, Sportster, Vrod and Street. These model families are distinguished by the frame, engine, suspension, and other characteristics.

Touring models use Big-Twin engines and large-diameter telescopic forks. All Touring designations begin with the letters FL, "e.g.", FLHR (Road King) and FLTR (Road Glide).

The touring family, also known as "dressers" or "baggers", includes Road King, Road Glide, Street Glide and Electra Glide models offered in various trims. The Road Kings have a "retro cruiser" appearance and are equipped with a large clear windshield. Road Kings are reminiscent of big-twin models from the 1940s and 1950s. Electra Glides can be identified by their full front fairings. Most Electra Glides sport a fork-mounted fairing referred to as the "Batwing" due to its unmistakable shape. The Road Glide and Road Glide Ultra Classic have a frame-mounted fairing, referred to as the "Sharknose". The Sharknose includes a unique, dual front headlight.

Touring models are distinguishable by their large saddlebags, rear coil-over air suspension and are the only models to offer full fairings with radios and CBs. All touring models use the same frame, first introduced with a Shovelhead motor in 1980, and carried forward with only modest upgrades until 2009, when it was extensively redesigned. The frame is distinguished by the location of the steering head in front of the forks and was the first H-D frame to rubber mount the drivetrain to isolate the rider from the vibration of the big V-twin.

The frame was modified for the 1994 model year when the oil tank went under the transmission and the battery was moved inboard from under the right saddlebag to under the seat. In 1997, the frame was again modified to allow for a larger battery under the seat and to lower seat height. In 2007, Harley-Davidson introduced the Twin Cam 96 engine, as well the six-speed transmission to give the rider better speeds on the highway.

In 2006, Harley introduced the FLHX Street Glide, a bike designed by Willie G. Davidson to be his personal ride, to its touring line.

In 2008, Harley added anti-lock braking systems and cruise control as a factory installed option on all touring models (standard on CVO and Anniversary models). Also new for 2008 is the fuel tank for all touring models. 2008 also brought throttle-by-wire to all touring models.

For the 2009 model year, Harley-Davidson redesigned the entire touring range with several changes, including a new frame, new swingarm, a completely revised engine-mounting system, front wheels for all but the FLHRC Road King Classic, and a 2–1–2 exhaust. The changes result in greater load carrying capacity, better handling, a smoother engine, longer range and less exhaust heat transmitted to the rider and passenger.
Also released for the 2009 model year is the FLHTCUTG Tri-Glide Ultra Classic, the first three-wheeled Harley since the Servi-Car was discontinued in 1973. The model features a unique frame and a 103-cubic-inch (1,690 cm³) engine exclusive to the trike.

In 2014, Harley-Davidson released a redesign for specific touring bikes and called it "Project Rushmore". Changes include a new 103CI High Output engine, one handed easy open saddlebags and compartments, a new Boom! Box Infotainment system with either 4.3 inch (10 cm) or 6.5 inch (16.5 cm) screens featuring touchscreen functionality [6.5 inch (16.5 cm) models only], Bluetooth (media and phone with approved compatible devices), available GPS and SiriusXM, Text-to-Speech functionality (with approved compatible devices) and USB connectivity with charging. Other features include ABS with Reflex linked brakes, improved styling, Halogen or LED lighting and upgraded passenger comfort.

These big-twin motorcycles capitalize on Harley's strong value on tradition. With the rear-wheel suspension hidden under the transmission, they are visually similar to the "hardtail" choppers popular in the 1960s and 1970s, as well as from their own earlier history. In keeping with that tradition, Harley offers Softail models with "Heritage" styling that incorporate design cues from throughout their history and used to offer "Springer" front ends on these Softail models from the factory.

Softail models utilize the big-twin engine (F) and the Softail chassis (ST).

Dyna-frame motorcycles were developed in the 1980s and early 1990s and debuted in the 1991 model year with the FXDB Sturgis offered in limited edition quantities. In 1992 the line continued with the limited edition FXDB Daytona and a production model FXD Super Glide. The new DYNA frame featured big-twin engines and traditional styling. They can be distinguished from the Softail by the traditional coil-over suspension that connects the swingarm to the frame, and from the Sportster by their larger engines. On these models, the transmission also houses the engine's oil reservoir.

Prior to 2006, Dyna models typically featured a narrow, XL-style 39mm front fork and front wheel, as well as footpegs which the manufacturer included the letter "X" in the model designation to indicate. This lineup traditionally included the Super Glide (FXD), Super Glide Custom (FXDC), Street Bob (FXDB), and Low Rider (FXDL). One exception was the Wide Glide (FXDWG), which featured thicker 41mm forks and a narrow front wheel, but positioned the forks on wider triple-trees that give a beefier appearance. In 2008, the Dyna Fat Bob (FXDF) was introduced to the Dyna lineup, featuring aggressive styling like a new 2–1–2 exhaust, twin headlamps, a 180 mm rear tire, and, for the first time in the Dyna lineup, a 130 mm front tire. For the 2012 model year, the Dyna Switchback (FLD) became the first Dyna to break the tradition of having an FX model designation with floorboards, detachable painted hard saddlebags, touring windshield, headlight nacelle and a wide front tire with full fender. The new front end resembled the big-twin FL models from 1968-1971.

The Dyna family used the 88-cubic-inch (1,440 cm³) twin cam from 1999 to 2006. In 2007, the displacement was increased to 96 cubic inches (1,570 cm³) as the factory increased the stroke to . For the 2012 model year, the manufacturer began to offer Dyna models with the 103-cubic-inch (1,690 cm³) upgrade. All Dyna models use a rubber-mounted engine to isolate engine vibration. Harley discontinued the Dyna platform in 2017 for the 2018 model year, having been replaced by a completely-redesigned Softail chassis; some of the existing models previously released by the company under the Dyna nameplate have since been carried over to the new Softail line.
Dyna models utilize the big-twin engine (F), footpegs noted as (X) with the exception of the 2012 FLD Switchback, a Dyna model which used floorboards as featured on the Touring (L) models, and the Dyna chassis (D). Therefore, except for the FLD from 2012 to 2016, all Dyna models have designations that begin with FXD, "e.g.", FXDWG (Dyna Wide Glide) and FXDL (Dyna Low Rider).

Introduced in 1957, the Sportster family were conceived as racing motorcycles, and were popular on dirt and flat-track race courses through the 1960s and 1970s. Smaller and lighter than the other Harley models, contemporary Sportsters make use of 883 cc or 1,200 cc Evolution engines and, though often modified, remain similar in appearance to their racing ancestors.

Up until the 2003 model year, the engine on the Sportster was rigidly mounted to the frame. The 2004 Sportster received a new frame accommodating a rubber-mounted engine. This made the bike heavier and reduced the available lean angle, while it reduced the amount of vibration transmitted to the frame and the rider, providing a smoother ride for rider and passenger.

In the 2007 model year, Harley-Davidson celebrated the 50th anniversary of the Sportster and produced a limited edition called the XL50, of which only 2000 were made for sale worldwide. Each motorcycle was individually numbered and came in one of two colors, Mirage Pearl Orange or Vivid Black. Also in 2007, electronic fuel injection was introduced to the Sportster family, and the Nightster model was introduced in mid-year. In 2009, Harley-Davidson added the Iron 883 to the Sportster line, as part of the Dark Custom series.
In the 2008 model year, Harley-Davidson released the XR1200 Sportster in Europe, Africa, and the Middle East. The XR1200 had an Evolution engine tuned to produce , four-piston dual front disc brakes, and an aluminum swing arm. "Motorcyclist" featured the XR1200 on the cover of its July 2008 issue and was generally positive about it in their "First Ride" story, in which Harley-Davidson was repeatedly asked to sell it in the United States.
One possible reason for the delayed availability in the United States was the fact that Harley-Davidson had to obtain the "XR1200" naming rights from Storz Performance, a Harley customizing shop in Ventura, Calif. The XR1200 was released in the United States in 2009 in a special color scheme including Mirage Orange highlighting its dirt-tracker heritage. The first 750 XR1200 models in 2009 were pre-ordered and came with a number 1 tag for the front of the bike, autographed by Kenny Coolbeth and Scott Parker and a thank you/welcome letter from the company, signed by Bill Davidson. The XR1200 was discontinued in model year 2013.
Except for the street-going XR1000 of the 1980s and the XR1200, most Sportsters made for street use have the prefix XL in their model designation. For the Sportster Evolution engines used since the mid-1980s, there have been two engine sizes. Motorcycles with the smaller engine are designated XL883, while those with the larger engine were initially designated XL1100. When the size of the larger engine was increased from 1,100 cc to 1,200 cc, the designation was changed accordingly from XL1100 to XL1200. Subsequent letters in the designation refer to model variations within the Sportster range, e.g. the XL883C refers to an 883 cc Sportster Custom, while the XL1200S designates the now-discontinued 1200 Sportster Sport.

Introduced in 2001 and produced until 2017,the VRSC muscle bike family bears little resemblance to Harley's more traditional lineup. Competing against Japanese and American muscle bikes in the upcoming muscle bike/power cruiser segment, the "V-Rod" makes use of an engine developed jointly with Porsche that, for the first time in Harley history, incorporates overhead cams and liquid cooling. The V-Rod is visually distinctive, easily identified by the 60-degree V-Twin engine, the radiator and the hydroformed frame members that support the round-topped air cleaner cover. The VRSC platform was also used for factory drag-racing motorcycles.

In 2008, Harley added the anti-lock braking system as a factory installed option on all VRSC models. Harley also increased the displacement of the stock engine from , which had only previously been available from Screamin' Eagle, and added a slipper clutch as standard equipment.

VRSC models include: 

VRSCA: V-Rod (2002–2006), VRSCAW: V-Rod (2007–2010), VRSCB: V-Rod (2004–2005), VRSCD: Night Rod (2006–2008), VRSCDX: Night Rod Special (2007–2014), VRSCSE: Screamin' Eagle CVO V-Rod (2005), VRSCSE2: Screamin' Eagle CVO V-Rod (2006), VRSCR: Street Rod (2006–2007), VRSCX: Screamin' Eagle Tribute V-Rod (2007), VRSCF: V-Rod Muscle (2009–2014).

VRSC models utilize the Revolution engine (VR), and the street versions are designated Street Custom (SC). After the VRSC prefix common to all street Revolution bikes, the next letter denotes the model, either A (base V-Rod: discontinued), AW (base V-Rod + W for Wide with a 240 mm rear tire), B (discontinued), D (Night Rod: discontinued), R (Street Rod: discontinued), SE and SEII(CVO Special Edition), or X (Special edition). Further differentiation within models are made with an additional letter, "e.g.", VRSCDX denotes the Night Rod Special.

The VRXSE V-Rod Destroyer is Harley-Davidson's production drag racing motorcycle, constructed to run the quarter mile in less than ten seconds. It is based on the same revolution engine that powers the VRSC line, but the VRXSE uses the Screamin' Eagle 1,300 cc "stroked" incarnation, featuring a 75 mm crankshaft, 105 mm Pistons, and 58 mm throttle bodies.

The V-Rod Destroyer is not a street legal motorcycle. As such, it uses "X" instead of "SC" to denote a non-street bike. "SE" denotes a CVO Special Edition.

The Street, Harley-Davidson's newest platform and their first all new platform in thirteen years, was designed to appeal to younger riders looking for a lighter bike at a cheaper price. The Street 750 model was launched in India at the 2014 Indian Auto Expo, Delhi-NCR on February 5, 2014. The Street 750 weighs 218 kg and has a ground clearance of 144 mm giving it the lowest weight and the highest ground clearance of Harley-Davidson motorcycles currently available.

The Street 750 uses an all-new, liquid-cooled, 60° V-twin engine called the Revolution X. In the Street 750, the engine displaces and produces 65 Nm at 4,000 rpm. A six speed transmission is used.

The Street 750 and the smaller-displacement Street 500 has been available since late 2014. Street series motorcycles for the North American market will be built in Harley-Davidson's Kansas City, Missouri plant, while those for other markets around the world will be built completely in their plant in Bawal, India.

Custom Vehicle Operations (CVO) is a team within Harley-Davidson that produces limited-edition customizations of Harley's stock models. Every year since 1999, the team has selected two to five of the company's base models and added higher-displacement engines, performance upgrades, special-edition paint jobs, more chromed or accented components, audio system upgrades, and electronic accessories to create high-dollar, premium-quality customizations for the factory custom market. The models most commonly upgraded in such a fashion are the Ultra Classic Electra Glide, which has been selected for CVO treatment every year from 2006 to the present, and the Road King, which was selected in 2002, 2003, 2007, and 2008. The Dyna, Softail, and VRSC families have also been selected for CVO customization.

The Environmental Protection Agency conducted emissions-certification and representative emissions test in Ann Arbor, Michigan, in 2005. Subsequently, Harley-Davidson produced an "environmental warranty". The warranty ensures each owner that the vehicle is designed and built free of any defects in materials and workmanship that would cause the vehicle to not meet EPA standards. In 2005, the EPA and the Pennsylvania Department of Environmental Protection (PADEP) confirmed Harley-Davidson to be the first corporation to voluntarily enroll in the One Clean-Up Program. This program is designed for the clean-up of the affected soil and groundwater at the former York Naval Ordnance Plant. The program is backed by the state and local government along with participating organizations and corporations.

Paul Gotthold, Director of Operations for the EPA, congratulated the motor company:

Harley-Davidson also purchased most of Castalloy, a South Australian producer of cast motorcycle wheels and hubs. The South Australian government has set forth "protection to the purchaser (Harley-Davidson) against environmental risks".

In August 2016 Harley-Davidson settled with the EPA for $12 million, without admitting wrongdoing, over the sale of after-market "super tuners". Super tuners were devices, marketed for competition, which enabled increased performance of Harley-Davidson products. However, the devices also modified the emission control systems, producing increased hydrocarbon and nitrogen oxide. Harley-Davidson is required to buy back and destroy any super tuners which do not meet Clean Air Act requirements and spend $3 million on air pollution mitigation.

According to a recent Harley-Davidson study, in 1987 half of all Harley riders were under age 35. Now, only 15 percent of Harley buyers are under 35, and as of 2005, the median age had risen to 46.7. In 2008, Harley-Davidson stopped disclosing the average age of riders; at this point it was 48 years old. 

In 1987, the median household income of a Harley-Davidson rider was $38,000. By 1997, the median household income for those riders had more than doubled, to $83,000.

Many Harley-Davidson Clubs exist nowadays around the world, the oldest one, founded in 1928, is in Prague.

Harley-Davidson attracts a loyal brand community, with licensing of the Harley-Davidson logo accounting for almost 5 percent of the company's net revenue ($41 million in 2004). Harley-Davidson supplies many American police forces with their motorcycle fleets.

From its founding, Harley-Davidson had worked to brand its motorcycles as respectable and refined products, with ads that showed what motorcycling writer Fred Rau called "refined-looking ladies with parasols, and men in conservative suits as the target market". The 1906 Harley-Davidson's effective, and polite, muffler was emphasized in advertisements with the nickname "The Silent Gray Fellow". That began to shift in the 1960s, partially in response to the clean-cut motorcyclist portrayed in Honda's "You meet the nicest people on a Honda" campaign, when Harley-Davidson sought to draw a contrast with Honda by underscoring the more working-class, macho, and even a little anti-social attitude associated with motorcycling's dark side. With the 1971 FX Super Glide, the company embraced, rather than distanced, itself from chopper style, and the counterculture custom Harley scene. Their marketing cultivated the "bad boy" image of biker and motorcycle clubs, and to a point, even outlaw or one-percenter motorcycle clubs.

Beginning in 1920, a team of farm boys, including Ray Weishaar, who became known as the "hog boys", consistently won races. The group had a live hog as their mascot. Following a win, they would put the hog on their Harley and take a victory lap. In 1983, the Motor Company formed a club for owners of its product taking advantage of the long-standing nickname by turning "hog" into the acronym HOG., for Harley Owners Group. Harley-Davidson attempted to trademark "hog", but lost a case against an independent Harley-Davidson specialist, The Hog Farm of West Seneca, New York, in 1999 when the appellate panel ruled that "hog" had become a generic term for large motorcycles and was therefore unprotectable as a trademark.

On August 15, 2006, Harley-Davidson Inc. had its NYSE ticker symbol changed from HDI to HOG.

Harley-Davidson FL "big twins" normally had heavy steel fenders, chrome trim, and other ornate and heavy accessories. After World War II, riders wanting more speed would often shorten the fenders or take them off completely to reduce the weight of the motorcycle. These bikes were called "bobbers" or sometimes "choppers" because parts considered unnecessary were chopped off. Those who made or rode choppers and bobbers, especially members of outlaw bike gangs like the Hells Angels, referred to stock FLs as "garbage wagons".

Harley-Davidson established the Harley Owners Group (HOG) in 1983 to build on the loyalty of Harley-Davidson enthusiasts as a means to promote a lifestyle alongside its products. The HOG also opened new revenue streams for the company, with the production of tie-in merchandise offered to club members, numbering more than one million. Other motorcycle brands,
and other and consumer brands outside motorcycling, have also tried to create factory-sponsored community marketing clubs of their own.
HOG members typically spend 30 percent more than other Harley owners, on such items as clothing and Harley-Davidson-sponsored events.

In 1991, HOG went international, with the first official European HOG Rally in Cheltenham, England.
Today, more than one million members and more than 1400 chapters worldwide make HOG the largest factory-sponsored motorcycle organization in the world.

HOG benefits include organized group rides, exclusive products and product discounts, insurance discounts, and the Hog Tales newsletter. A one-year full membership is included with the purchase of a new, unregistered Harley-Davidson.

In 2008, HOG celebrated its 25th anniversary in conjunction with the Harley 105th in Milwaukee, Wisconsin.

3rd Southern HOG Rally set to bring together largest gathering of Harley-Davidson owners in South India. More than 600 Harley-Davidson Owners expected to ride to Hyderabad from across 13 HOG Chapters 

Harley-Davidson offers factory tours at four of its manufacturing sites, and the Harley-Davidson Museum, which opened in 2008, exhibits Harley-Davidson's history, culture, and vehicles, including the motor company's corporate archives.

Due to the consolidation of operations, the Capitol Drive Tour Center in Wauwatosa, Wisconsin was closed in 2009.

Beginning with Harley-Davidson's 90th anniversary in 1993, Harley-Davidson has had celebratory rides to Milwaukee called the "Ride Home". This new tradition has continued every five years, and is referred to unofficially as "Harleyfest", in line with Milwaukee's other festivals (Summerfest, German fest, Festa Italiana, etc.). This event brings Harley riders from all around the world. The 105th anniversary celebration was held on August 28–31, 2008, and included events in Milwaukee, Waukesha, Racine, and Kenosha counties, in Southeast Wisconsin. The 110th anniversary celebration was held on August 29–31, 2013. The 115th anniversary was held in Prague, Czech Republic, the home country of the first Harley Davidson Club, on July 5-8, 2018 and attracted more than 100.000 visitors and 60.000 bikes.

William S. Harley, Arthur Davidson, William A. Davidson and Walter Davidson, Sr. were inducted into the Labor Hall of Fame for their accomplishments for the H-D company and its workforce.

The company's origins were dramatized in a 2016 miniseries entitled "Harley and the Davidsons", starring Robert Aramayo as William Harley, Bug Hall as Arthur Davidson and Michiel Huisman as Walter Davidson, and premiered on the Discovery Channel as a "three-night event series" on September 5, 2016.




</doc>
<doc id="14144" url="https://en.wikipedia.org/wiki?curid=14144" title="Hiberno-English">
Hiberno-English

Hiberno‐English (from Latin "Hibernia": "Ireland") or Irish English is the set of English dialects natively written and spoken within the island of Ireland (including both the Republic of Ireland and Northern Ireland).

English was brought to Ireland as a result of the Norman invasion of Ireland of the late 12th century. Initially, it was mainly spoken in an area known as the Pale around Dublin, with mostly Irish spoken throughout the rest of the country. By the Tudor period, Irish culture and language had regained most of the territory lost to the invaders: even in the Pale, "all the common folk… for the most part are of Irish birth, Irish habit, and of Irish language". Some small pockets remained predominantly English-speaking; because of their sheer isolation their dialects developed into later (now extinct) dialects known as Yola in Wexford and Fingallian in Fingal, Dublin. These were no longer mutually intelligible with other English varieties. However, the Tudor conquest and colonisation of Ireland in the 16th century marked a revival in the use of English. By the mid-19th century, English was the majority language spoken in the country. It has retained this status to the present day, with even those whose first language is Irish being fluent in English as well. Today, there is only a little more than one percent of the population that speaks Irish natively. English is one of two official languages, along with Irish, of the Republic of Ireland, and is the country's working language.

Hiberno-English's spelling and pronunciation standards align with British rather than American English. However, Hiberno-English's diverse accents and some of its grammatical structures are unique, with some influence by the Irish language and a tendency to be phonologically conservative, retaining older features no longer common in the accents of England or North America.

Phonologists today often divide Hiberno-English into four or five overarching classes of dialects or accents: Ulster accents, West and South-West Region accents (including, for example, the Cork accent), various Dublin accents, and a supraregional accent developing since only the last quarter of the twentieth century.
Ulster English (or Northern Irish English) here refers collectively to the varieties of the Ulster province, including Northern Ireland and neighbouring counties outside of Northern Ireland, which has been influenced by Ulster Irish as well as the Scots language, brought over by Scottish settlers during the Plantation of Ulster. Its main subdivisions are Mid-Ulster English , South Ulster English and Ulster Scots English, the latter of which is more directly and strongly influenced by the Scots language. All Ulster English has more obvious pronunciation similarities with Scottish English than other Irish English dialects do.

Ulster varieties distinctly pronounce:


West and South-West Irish English here refers to broad varieties of Ireland's West and South-West Regions. Accents of both regions are known for:

South-West Irish English (often known, by specific county, as Cork English, Kerry English, or Limerick English) also features two major defining characteristics of its own: the raising of to [ɪ] when before /n/ or /m/ (as in "again" or "pen"), and the noticeable intonation pattern of a slightly higher pitch followed by a significant drop in pitch on stressed long-vowel syllables (across multiple syllables or even within a single one), which is popularly heard, in rapid conversation, as a kind of undulating "sing-song" pattern.


Dublin English is highly internally diverse and refers collectively to the Irish English varieties of eastern Ireland (the province of Leinster). Modern-day Dublin English largely lies on a phonological continuum, ranging from a more traditional, lower-prestige, local urban accent on the one end to a more recently developing, higher-prestige, non-local (regional and even supraregional) accent on the other end, whose most advanced characteristics only first emerged in the late 1980s and 1990s. The accent that most strongly uses the traditional working-class features is often called local Dublin English. Most speakers from Dublin and its suburbs, however, have accent features falling variously along the entire middle as well as newer end of the spectrum, which together form what is called non-local Dublin English, spoken by middle- and upper-class natives of Dublin and the greater eastern Irish region surrounding the city. A subset of this variety, whose middle-class speakers mostly range in the middle of the continuum, is called mainstream Dublin English. Mainstream Dublin English has become the basis of an accent that has otherwise become supraregional (see more below) everywhere except in the north of the country. The majority of Dubliners born since the 1980s (led particularly by females) has shifted towards the most innovative non-local accent, here called new Dublin English, which has gained ground over mainstream Dublin English and which is the most extreme variety in rejecting the local accent's traditional features. The varieties at either extreme of the spectrum, local and new Dublin English, are both discussed in further detail below. In the most general terms, all varieties of Dublin English have the following identifying sounds that are often distinct from the rest of Ireland, pronouncing:

Local Dublin English (or popular Dublin English) here refers to a traditional, broad, working-class variety spoken in the Republic of Ireland's capital city of Dublin. It is the only Irish English variety that in earlier history was non-rhotic; however, it is today weakly rhotic, and it uniquely pronounces:

The local Dublin accent is also known for a phenomenon called "vowel breaking", in which the vowel sounds , , , and in closed syllables are "broken" into two syllables, approximating , , , and , respectively.


Evolving as a fashionable outgrowth of the mainstream non-local Dublin English, new Dublin English (also, advanced Dublin English and, formerly, fashionable Dublin English) is a youthful variety that originally began in the early 1990s among the "avant-garde" and now those aspiring to a non-local "urban sophistication". New Dublin English itself, first associated with affluent and middle-class inhabitants of southside Dublin, is probably now spoken by a majority of Dubliners born since the 1980s. It has replaced (yet was largely influenced by) moribund D4 English (often known as "Dublin 4" or "DART speak" or, mockingly, "Dortspeak"), which originated around the 1970s from Dubliners who rejected traditional notions of Irishness, regarding themselves as more trendy and sophisticated; however, particular aspects of the D4 accent became quickly noticed and ridiculed as sounding affected, causing these features to fall out of fashion by the 1990s.

This "new mainstream" accent of Dublin's youth, rejecting traditional working-class Dublin, pronounces:


Supraregional southern Irish English (sometimes, simply, supraregional Irish English or supraregional Hiberno-English) here refers to a variety crossing regional boundaries throughout all of the Republic of Ireland, except the north. As mentioned earlier, mainstream Dublin English of the early- to mid-1900s is the direct influence and catalyst for this variety. Most speakers born in the 1980s or later are showing fewer features of the twentieth-century mainstream supraregional form and more characteristics of an advanced supraregional variety that aligns clearly with the rapidly spreading new Dublin accent (see more above, under "Non-local Dublin English").

Ireland's surparegional dialect pronounces:

The following charts list the vowels typical of each Irish English dialect as well as the several distinctive consonants of Irish English. Phonological characteristics of overall Irish English are given as well as categorisations into five major divisions of Hiberno-English: northern Ireland (or Ulster); West & South-West Ireland; local Dublin; new Dublin; and supraregional (southern) Ireland. Features of mainstream non-local Dublin English fall on a range between "local Dublin" and "new Dublin".

The defining monophthongs of Irish English:

The following pure vowel sounds are defining characteristics of Irish English:

All pure vowels of various Hiberno-English dialects:

Footnotes:
In southside Dublin's once-briefly fashionable "Dublin 4" (or "Dortspeak") accent, the " and broad " set becomes rounded as [ɒː].

Unstressed syllable-final /iː/ or /ɪ/ is realised in Ulster accents uniquely as [e~ɪ].

Other notes:


The defining diphthongs of Hiberno-English:

The following gliding vowel (diphthong) sounds are defining characteristics of Irish English:

All diphthongs of various Hiberno-English dialects:

Footnotes:'
Due to the local Dublin accent's phenomenon of "vowel breaking", may be realised in that accent as [əjə] in a closed syllable, and, in the same environment, may be realised as [ɛwə].

The defining "r"-coloured vowels of Hiberno-English:

The following "r"-coloured vowel features are defining characteristics of Hiberno-English: 

All "r"-coloured vowels of various Hiberno-English dialects:

Footnotes:

Every major accent of Irish English is rhotic (pronounces "r" after a vowel sound). The local Dublin accent is the only one that during an earlier time was non-rhotic, though it usually very lightly rhotic today, with a few minor exceptions. The rhotic consonant in this and most other Irish accents is an approximant [ɹ̠].

The "r" sound of the mainstream non-local Dublin accent is more precisely a velarised approximant [ɹˠ], while the "r" sound of the more recently emerging non-local Dublin (or "new Dublin") accent is more precisely a retroflex approximant [ɻ].

In southside Dublin's once-briefly fashionable "Dublin 4" (or "Dortspeak") accent, /ɑr/ is realised as [ɒːɹ].

In non-local Dublin's more recently emerging (or "new Dublin") accent, /ɛər/ and /ɜr/ may both be realised more rounded as [øːɻ].

In local Dublin, West/South-West, and other very conservative and traditional Irish English varieties ranging from the south to the north, the phoneme /ɜr/ is split into two distinct phonemes depending on spelling and preceding consonants, which have sometimes been represented as /ɛr/ versus /ʊr/, and often more precisely pronounced as [ɛːɹ] versus [ʊːɹ]. As an example, the words "earn" and "urn" are not pronounced the same, as they are in most dialects of English around the world. In the local Dublin and West/South-West accents, /ɜr/ when after a labial consonant (e.g. "fern"), when spelled as "ur" or "or" (e.g. "word"), or when spelled as "ir" after an alveolar stop (e.g. "dirt") are pronounced as [ʊːɹ]; in all other situations, /ɜr/ is pronounced as [ɛːɹ]. Example words include:
/ɛr/

/ʊr/

In non-local Dublin, younger, and supraregional Irish accents, this split is seldom preserved, with both of the /ɜr/ phonemes typically merged as [ɚː].

In rare few local Dublin varieties that are non-rhotic, is either lowered to or backed and raised to .

The distinction between and is widely preserved in Ireland, so that, for example, "horse" and "hoarse" are not merged in most Irish English dialects; however, they are usually merged in Belfast and new Dublin.

In local Dublin, due to the phenomenon of "vowel breaking" may in fact be realised as .

The defining consonants of Hiberno-English:

The consonants of Hiberno-English mostly align to the typical English consonant sounds. However, a few Irish English consonants have distinctive, varying qualities. The following consonant features are defining characteristics of Hiberno-English: 

Unique consonants in various Hiberno-English dialects:

Footnotes:

In traditional, conservative Ulster English, /k/ and /g/ are palatalised before a low front vowel.

Local Dublin also undergoes cluster simplification, so that stop consonant sounds occurring after fricatives or sonorants may be left unpronounced, resulting, for example, in "poun(d)" and "las(t)".

Rhoticity: Every major accent of Irish English is strongly rhotic (pronounces "r" after a vowel sound), though to a weaker degree with the local Dublin accent. The accents of local Dublin and some smaller eastern towns like Drogheda were historically non-rhotic and now only very lightly rhotic or variably rhotic, with the rhotic consonant being an alveolar approximant, [ɹ]. In extremely traditional and conservative accents (exemplified, for instance, in the speech of older speakers throughout the country, even in South-West Ireland, such as Mícheál Ó Muircheartaigh and Jackie Healy-Rae), the rhotic consonant, before a vowel sound, can also be an alveolar tap, [ɾ]. The rhotic consonant for the northern Ireland and new Dublin accents is a retroflex approximant, [ɻ]. Dublin's retroflex approximant has no precedent outside of northern Ireland and is a genuine innovation of the past two decades. A guttural/uvular is found in north-east Leinster. Otherwise, the rhotic consonant of virtually all other Irish accents is the postalveolar approximant, [ɹ].

The symbol [θ̠] is used here to represent the voiceless alveolar non-sibilant fricative, sometimes known as a "slit fricative", whose articulation is described as being apico-alveolar.

Overall, and are being increasingly merged in supraregional Irish English, for example, making "wine" and "whine" homophones, as in most varieties of English around the world.

Other phonological characteristics of Irish English include that consonant clusters ending in before are distinctive:

The naming of the letter "H" as "haitch" is standard.

Due to Gaelic influence, an epenthetic schwa is sometimes inserted, perhaps as a feature of older and less careful speakers, e.g. "film" [ˈfɪləm] and "form" [ˈfɒːɹəm].

A number of Irish-language loan words are used in Hiberno-English, particularly in an official state capacity. For example, the head of government is the Taoiseach, the deputy head is the Tánaiste, the parliament is the Oireachtas and its lower house is Dáil Éireann. Less formally, people also use loan words in day-to-day speech, although this has been on the wane in recent decades and among the young.

Another group of Hiberno-English words are those "derived" from the Irish language. Some are words in English that have entered into general use, while others are unique to Ireland. These words and phrases are often Anglicised versions of words in Irish or direct translations into English. In the latter case, they often give a meaning to a word or phrase that is generally not found in wider English use.

Another class of vocabulary found in Hiberno-English are words and phrases common in Old and Middle English, but which have since become obscure or obsolete in the modern English language generally. Hiberno-English has also developed particular meanings for words that are still in common use in English generally.

In addition to the three groups above, there are also additional words and phrases whose origin is disputed or unknown. While this group may not be unique to Ireland, their usage is not widespread, and could be seen as characteristic of the language in Ireland.

The syntax of the Irish language is quite different from that of English. Various aspects of Irish syntax have influenced Hiberno-English, though many of these idiosyncrasies are disappearing in suburban areas and among the younger population.

The other major influence on Hiberno-English that sets it apart from modern English in general is the retention of words and phrases from Old- and Middle-English.

Reduplication is an alleged trait of Hiberno-English strongly associated with Stage Irish and Hollywood films.


Irish lacks words that directly translate as "yes" or "no", and instead repeats the verb used in the question, negated if necessary, to answer. Hiberno-English uses "yes" and "no" less frequently than other English dialects as speakers can repeat the verb, positively or negatively, instead of (or in redundant addition to) using "yes" or "no".


This is not limited only to the verb "to be": it is also used with "to have" when used as an auxiliary; and, with other verbs, the verb "to do" is used. This is most commonly used for intensification, especially in Ulster English.

Irish indicates recency of an action by adding "after" to the present continuous (a verb ending in "-ing"), a construction known as the "hot news perfect" or "after perfect". The idiom for "I had done X when I did Y" is "I was after doing X when I did Y", modelled on the Irish usage of the compound prepositions "i ndiaidh", "tar éis", and "in éis": "bhí mé tar éis/i ndiaidh/in éis X a dhéanamh, nuair a rinne mé Y".
A similar construction is seen where exclamation is used in describing a recent event:

When describing less astonishing or significant events, a structure resembling the German perfect can be seen:

This correlates with an analysis of "H1 Irish" proposed by Adger & Mitrovic, in a deliberate parallel to the status of German as a V2 language.

The reflexive version of pronouns is often used for emphasis or to refer indirectly to a particular person, etc., according to context. "Herself", for example, might refer to the speaker's boss or to the woman of the house. Use of "herself" or "himself" in this way often indicates that the speaker attributes some degree of arrogance or selfishness to the person in question. Note also the indirectness of this construction relative to, for example, "She's coming now"

There are some language forms that stem from the fact that there is no verb "to have" in Irish. Instead, possession is indicated in Irish by using the preposition "at", (in Irish, "ag."). To be more precise, Irish uses a prepositional pronoun that combines "ag" "at" and "mé" "me" to create "agam".
In English, the verb "to have" is used, along with a "with me" or "on me" that derives from "Tá … agam." This gives rise to the frequent
Somebody who can speak a language "has" a language, in which Hiberno-English has borrowed the grammatical form used in Irish.

When describing something, many Hiberno-English speakers use the term "in it" where "there" would usually be used. This is due to the Irish word "ann" (pronounced "oun" or "on") fulfilling both meanings.

Another idiom is this thing or that thing described as "this man here" or "that man there", which also features in Newfoundland English in Canada.

Conditionals have a greater presence in Hiberno-English due to the tendency to replace the simple present tense with the conditional (would) and the simple past tense with the conditional perfect (would have).

Bring and take: Irish use of these words differs from that of British English because it follows the Irish grammar for "beir" and "tóg". English usage is determined by direction; person determines Irish usage. So, in English, one takes ""from" here "to" there", and brings it ""to" here "from" there". In Irish, a person takes only when accepting a transfer of possession of the object from someone elseand a person brings at all other times, irrespective of direction (to or from).

The Irish equivalent of the verb "to be" has two present tenses, one (the present tense proper or "aimsir láithreach") for cases which are generally true or are true at the time of speaking and the other (the habitual present or "aimsir ghnáthláithreach") for repeated actions. Thus, "you are [now, or generally]" is "tá tú", but "you are [repeatedly]" is "bíonn tú". Both forms are used with the verbal noun (equivalent to the English present participle) to create compound tenses. This is similar to the distinction between "ser" and "estar" in Spanish.

The corresponding usage in English is frequently found in rural areas, especially Mayo/Sligo in the west of Ireland and Wexford in the south-east, Inner-City Dublin along with border areas of the North and Republic. In this form, the verb "to be" in English is similar to its use in Irish, with a "does be/do be" (or "bees", although less frequently) construction to indicate the continuous, or habitual, present:

This construction also surfaces in African American Vernacular English, as the famous habitual be.

In old-fashioned usage, "it is" can be freely abbreviated "’tis", even as a standalone sentence. This also allows the double contraction "’tisn’t", for "it is not".

Irish has separate forms for the second person singular ("tú") and the second person plural ("sibh").
Mirroring Irish, and almost every other Indo European language, the plural "you" is also distinguished from the singular in Hiberno-English, normally by use of the otherwise archaic English word "ye" ; the word "yous" (sometimes written as "youse") also occurs, but primarily only in Dublin and across Ulster. In addition, in some areas in Leinster, north Connacht and parts of Ulster, the hybrid word "ye-s", pronounced "yiz", may be used. The pronunciation differs with that of the northwestern being and the Leinster pronunciation being .


The word "ye", "yis" or "yous", otherwise archaic, is still used in place of "you" for the second-person plural. "Ye'r", "Yisser" or "Yousser" are the possessive forms, e.g. "Where are yous going?"

The verb "mitch" is very common in Ireland, indicating being truant from school. This word appears in Shakespeare (though he wrote in Early Modern English rather than Middle English), but is seldom heard these days in British English, although pockets of usage persist in some areas (notably South Wales, Devon, and Cornwall). In parts of Connacht and Ulster the "mitch" is often replaced by the verb "scheme", while in Dublin it is often replaced by "on the hop/bounce".

Another usage familiar from Shakespeare is the inclusion of the second person pronoun after the imperative form of a verb, as in "Wife, go you to her ere you go to bed" (Romeo and Juliet, Act III, Scene IV). This is still common in Ulster: "Get youse your homework done or you're no goin' out!" In Munster, you will still hear children being told, "Up to bed, let ye" .

For influence from Scotland, see Ulster Scots and Ulster English.

Now is often used at the end of sentences or phrases as a semantically empty word, completing an utterance without contributing any apparent meaning. Examples include "Bye now" (= "Goodbye"), "There you go now" (when giving someone something), "Ah now!" (expressing dismay), "Hold on now" (= "wait a minute"), "Now then" as a mild attention-getter, etc. This usage is universal among English dialects, but occurs more frequently in Hiberno-English. It is also used in the manner of the Italian 'prego' or German 'bitte', for example a barman might say "Now, Sir." when delivering drinks.

So is often used for emphasis ("I can speak Irish, so I can"), or it may be tacked onto the end of a sentence to indicate agreement, where "then" would often be used in Standard English ("Bye so", "Let's go so", "That's fine so", "We'll do that so"). The word is also used to contradict a negative statement ("You're not pushing hard enough" – "I am so!"). (This contradiction of a negative is also seen in American English, though not as often as "I am too", or "Yes, I am".) The practice of indicating emphasis with "so" and including reduplicating the sentence's subject pronoun and auxiliary verb (is, are, have, has, can, etc.) such as in the initial example, is particularly prevalent in more northern dialects such as those of Sligo, Mayo and the counties of Ulster.

Sure is often used as a tag word, emphasising the obviousness of the statement, roughly translating as but/and/well. Can be used as "to be sure", the famous Irish stereotype phrase. (But note that the other stereotype of "Sure and …" is not actually used in Ireland.) Or "Sure, I can just go on Wednesday", "I will not, to be sure." The word is also used at the end of sentences (primarily in Munster), for instance "I was only here five minutes ago, sure!" and can express emphasis or indignation.

To is often omitted from sentences where it would exist in British English. For example, "I'm not let go out tonight", instead of "I'm not allowed "to" go out tonight".

Will is often used where British English would use "shall" or American English "should" (as in "Will I make us a cup of tea?"). The distinction between "shall" (for first-person simple future, and second- and third-person emphatic future) and "will" (second- and third-person simple future, first-person emphatic future), maintained by many in England, does not exist in Hiberno-English, with "will" generally used in all cases.

Once is sometimes used in a different way from how it is used in other dialects; in this usage, it indicates a combination of logical and causal conditionality: "I have no problem laughing at myself once the joke is funny." Other dialects of English would probably use "if" in this situation.




</doc>
<doc id="14147" url="https://en.wikipedia.org/wiki?curid=14147" title="Harmonic analysis">
Harmonic analysis

Harmonic analysis is a branch of mathematics concerned with the representation of functions or signals as the superposition of basic waves, and the study of and generalization of the notions of Fourier series and Fourier transforms (i.e. an extended form of Fourier analysis). In the past two centuries, it has become a vast subject with applications in areas as diverse as number theory, representation theory, signal processing, quantum mechanics, tidal analysis and neuroscience.

The term "harmonics" originated as the Ancient Greek word "harmonikos", meaning "skilled in music". In physical eigenvalue problems, it began to mean waves whose frequencies are integer multiples of one another, as are the frequencies of the harmonics of music notes, but the term has been generalized beyond its original meaning.

The classical Fourier transform on R is still an area of ongoing research, particularly concerning Fourier transformation on more general objects such as tempered distributions. For instance, if we impose some requirements on a distribution "f", we can attempt to translate these requirements in terms of the Fourier transform of "f". The Paley–Wiener theorem is an example of this. The Paley–Wiener theorem immediately implies that if "f" is a nonzero distribution of compact support (these include functions of compact support), then its Fourier transform is never compactly supported. This is a very elementary form of an uncertainty principle in a harmonic-analysis setting. See also: Convergence of Fourier series.

Fourier series can be conveniently studied in the context of Hilbert spaces, which provides a connection between harmonic analysis and functional analysis.

Many applications of harmonic analysis in science and engineering begin with the idea or hypothesis that a phenomenon or signal is composed of a sum of individual oscillatory components. Ocean tides and vibrating strings are common and simple examples. The theoretical approach is often to try to describe the system by a differential equation or system of equations to predict the essential features, including the amplitude, frequency, and phases of the oscillatory components. The specific equations depend on the field, but theories generally try to select equations that represent major principles that are applicable.

The experimental approach is usually to acquire data that accurately quantifies the phenomenon. For example, in a study of tides, the experimentalist would acquire samples of water depth as a function of time at closely enough spaced intervals to see each oscillation and over a long enough duration that multiple oscillatory periods are likely included. In a study on vibrating strings, it is common for the experimentalist to acquire a sound waveform sampled at a rate at least twice that of the highest frequency expected and for a duration many times the period of the lowest frequency expected.

For example, the top signal at the right is a sound waveform of a bass guitar playing an open string corresponding to an A note with a fundamental frequency of 55 Hz. The waveform appears oscillatory, but it is more complex than a simple sine wave, indicating the presence of additional waves. The different wave components contributing to the sound can be revealed by applying a mathematical analysis technique known as the Fourier transform, the result of which is shown in the lower figure. Note that there is a prominent peak at 55 Hz, but that there are other peaks at 110 Hz, 165 Hz, and at other frequencies corresponding to integer multiples of 55 Hz. In this case, 55 Hz is identified as the fundamental frequency of the string vibration, and the integer multiples are known as harmonics.

One of the most modern branches of harmonic analysis, having its roots in the mid-20th century, is analysis on topological groups. The core motivating ideas are the various Fourier transforms, which can be generalized to a transform of functions defined on Hausdorff locally compact topological groups.

The theory for abelian locally compact groups is called Pontryagin duality.

Harmonic analysis studies the properties of that duality and Fourier transform and attempts to extend those features to different settings, for instance, to the case of non-abelian Lie groups.

For general non-abelian locally compact groups, harmonic analysis is closely related to the theory of unitary group representations. For compact groups, the Peter–Weyl theorem explains how one may get harmonics by choosing one irreducible representation out of each equivalence class of representations. This choice of harmonics enjoys some of the useful properties of the classical Fourier transform in terms of carrying convolutions to pointwise products, or otherwise showing a certain understanding of the underlying group structure. See also: Non-commutative harmonic analysis.

If the group is neither abelian nor compact, no general satisfactory theory is currently known ("satisfactory" means at least as strong as the Plancherel theorem). However, many specific cases have been analyzed, for example SL. In this case, representations in infinite dimensions play a crucial role.





</doc>
<doc id="14148" url="https://en.wikipedia.org/wiki?curid=14148" title="Home run">
Home run

In baseball, a home run (abbreviated HR) is scored when the ball is hit in such a way that the batter is able to circle the bases and reach home 
safely in one play without any errors being committed by the defensive team in the process. In modern baseball, the feat is typically achieved by hitting the ball over the outfield fence between the foul poles (or making contact with either foul pole) without first touching the ground, resulting in an automatic home run. There is also the "inside-the-park" home run where the batter reaches home safely while the baseball is in play on the field.

When a home run is scored, the batter is also credited with a hit and a run scored, and an RBI for each runner that scores, including himself. Likewise, the pitcher is recorded as having given up a hit, and a run for each runner that scores including the batter.

Home runs are among the most popular aspects of baseball and, as a result, prolific home run hitters are usually the most popular among fans and consequently the highest paid by teams—hence the old saying, variously attributed to slugger Ralph Kiner, or to a teammate talking about Kiner, "Home run hitters drive Cadillacs, and singles hitters drive Fords."

In modern times a home run is most often scored when the ball is hit over the outfield wall between the foul poles (in fair territory) before it touches the ground (in flight), and without being caught or deflected back onto the field by a fielder. A batted ball is also a home run if it touches either foul pole or its attached screen before touching the ground, as the foul poles are by definition in fair territory. Additionally, many major-league ballparks have ground rules stating that a batted ball in flight that strikes a specified location or fixed object is a home run; this usually applies to objects that are beyond the outfield wall but are located such that it may be difficult for an umpire to judge.

A batted ball that goes over the outfield wall "after" touching the ground is not a home run, but an automatic double for the batter (often colloquially called a "ground rule double").

A fielder is allowed to reach over the wall to attempt to catch the ball as long as his feet are on or over the field during the attempt, and if the fielder successfully catches the ball while it is in flight the batter is out, even if the ball had already passed the vertical plane of the wall. However, since the fielder is not part of the field, a ball that bounces off a fielder (including his glove) and over the wall without touching the ground is still a home run. A fielder may not deliberately throw his glove, cap, or any other equipment or apparel to stop or deflect a fair ball, and an umpire may award a home run to the batter if a fielder does so on a ball that, in the umpire's judgment, would have otherwise been a home run (this is rare in modern professional baseball).

A home run accomplished in any of the above manners is an automatic home run. The ball is dead, even if it rebounds back onto the field (e.g., from striking a foul pole), and the batter and any preceding runners cannot be put out at any time while running the bases. However, if one or more runners fail to touch a base or one runner passes another before reaching home plate, that runner or runners can be called out on appeal, though in the case of not touching a base a runner can go back and touch it if doing so won't cause them to be passed by another preceding runner and they have not yet touched the next base (or home plate in the case of missing third base). This stipulation is in Approved Ruling (2) of Rule 7.10(b).

An inside-the-park home run occurs when a batter hits the ball into play and is able to circle the bases before the fielders can put him out. Unlike with an outside-the-park home run, the batter-runner and all preceding runners are liable to be put out by the defensive team at any time while running the bases. This can only happen if the ball does not leave the ballfield.

In the early days of baseball, outfields were relatively much more spacious, reducing the likelihood of an over-the-fence home run, while increasing the likelihood of an inside-the-park home run, as a ball getting past an outfielder had more distance that it could roll before a fielder could track it down.

Modern outfields are much less spacious and more uniformly designed than in the game's early days, therefore inside-the-park home runs are now a rarity. They usually occur when a fast runner hits the ball deep into the outfield and the ball bounces in an unexpected direction away from the nearest outfielder (e.g., off a divot in the grass or off the outfield wall), or an outfielder misjudges the flight of the ball in a way that he cannot quickly recover from the mistake (e.g., by diving and missing). The speed of the runner is crucial as even triples are relatively rare in most modern ballparks.

If any defensive play on an inside-the-park home run is labeled an error by the official scorer, a home run is not scored; instead, it is scored as a single, double, or triple, and the batter-runner and any applicable preceding runners are said to have taken all additional bases on error. All runs scored on such a play, however, still count.

An example of an unexpected bounce occurred during the 2007 Major League Baseball All-Star Game at AT&T Park in San Francisco on July 10, 2007. Ichiro Suzuki of the American League team hit a fly ball that caromed off the right-center field wall in the opposite direction from where National League right fielder Ken Griffey, Jr. was expecting it to go. By the time the ball was relayed, Ichiro had already crossed the plate standing up. This was the first inside-the-park home run in All-Star Game history, and led to Suzuki being named the game's Most Valuable Player.

Home runs are often characterized by the number of runners on base at the time. A home run hit with the bases empty is seldom called a "one-run homer", but rather a solo home run, solo homer, or "solo shot". With one runner on base, two runs are scored (the baserunner and the batter) and thus the home run is often called a two-run homer or two-run shot. Similarly, a home runs with two runners on base is a three-run homer or three-run shot.

The term "four-run homer" is seldom used; instead, it is nearly always called a "grand slam". Hitting a grand slam is the best possible result for the batter's turn at bat and the worst possible result for the pitcher and his team.

A grand slam occurs when the bases are "loaded" (that is, there are base runners standing at first, second, and third base) and the batter hits a home run. According to "The Dickson Baseball Dictionary", the term originated in the card game of contract bridge. An inside-the-park grand slam is a grand slam that is also an inside-the-park home run, a home run without the ball leaving the field, and it is very rare, due to the relative rarity of loading the bases along with the significant rarity (nowadays) of inside-the-park home runs.

On July 25, 1956, Roberto Clemente became the only MLB player to have ever scored a walk-off inside-the-park grand slam in a 9–8 Pittsburgh Pirates win over the Chicago Cubs, at Forbes Field.

On April 23, 1999, Fernando Tatís made history by hitting two grand slams in one inning, both against Chan Ho Park of the Los Angeles Dodgers. With this feat, Tatís also set a Major League record with 8 RBI in one inning.

On July 29, 2003 against the Texas Rangers, Bill Mueller of the Boston Red Sox became the only player in major league history to hit two grand slams in one game from opposite sides of the plate. In fact, he hit three home runs in that game, and his two grand slams were in consecutive at-bats.

On August 25, 2011 the New York Yankees became the first team to hit three grand slams in one game vs the Oakland A's. The Yankees eventually went on to win the game 22–9, after trailing 7–1.

These types of home runs are characterized by the specific game situation in which they occur, and can theoretically occur on either an outside-the-park or inside-the-park home run.

A walk-off home run is a home run hit by the home team in the bottom of the ninth inning, any extra inning, or other scheduled final inning, which gives the home team the lead and thereby ends the game. The term is attributed to Hall of Fame relief pitcher Dennis Eckersley, so named because after the run is scored, the losing team has to "walk off" the field.

Two World Series have ended via the "walk-off" home run. The first was the 1960 World Series when Bill Mazeroski of the Pittsburgh Pirates hit a 9th inning solo home run in the 7th game of the series off New York Yankees pitcher Ralph Terry to give the Pirates the World Championship. The second time was the 1993 World Series when Joe Carter of the Toronto Blue Jays hit a 9th inning 3-run home run off Philadelphia Phillies pitcher Mitch Williams in Game 6 of the series, to help the Toronto Blue Jays capture their second World Series Championship in a row.

Such a home run can also be called a "sudden death" or "sudden victory" home run. That usage has lessened as "walk-off home run" has gained favor. Along with Mazeroski's 1960 shot, the most famous walk-off or sudden-death homer would probably be the "Shot Heard 'Round the World" hit by Bobby Thomson to win the 1951 National League pennant for the New York Giants, along with many other game-ending home runs that famously ended some of the most important and suspenseful baseball games.

A walk-off home run over the fence is an exception to baseball's one-run rule. Normally if the home team is tied or behind in the ninth or extra innings the game ends as soon as the home team scores enough runs to achieve a lead. If the home team has two outs in the inning, and the game is tied, the game will officially end either the moment the batter successfully reaches 1st base or the moment the runner touches home plate—whichever happens last. However, this is superseded by the "ground rule", which provides automatic doubles (when a ball-in-play hits the ground first then leaves the playing field) and home runs (when a ball-in-play leaves the playing field without ever touching the ground). In the latter case, all base runners including the batter are allowed to cross the plate.

A lead-off home run is a home run hit by the first batter of a team, the leadoff hitter of the first inning of the game. In MLB, Rickey Henderson holds the record with 81 lead-off home runs.

In 1996, Brady Anderson set a Major League record by hitting a lead-off home run in four consecutive games.

When two consecutive batters each hit a home run, this is described as back-to-back home runs. It is still considered back-to-back even if both batters hit their home runs off different pitchers. A third batter hitting a home run is commonly referred to as back-to-back-to-back.

Four home runs in a row by consecutive batters has only occurred nine times in the history of Major League Baseball. Following convention, this is called back-to-back-to-back-to-back. The most recent occurrence was on July 27, 2017, when the Washington Nationals hit four in a row against the Milwaukee Brewers in Nationals Park as Brian Goodwin, Wilmer Difo, Bryce Harper and Ryan Zimmerman homered off pitcher Michael Blazek. Blazek became the fourth pitcher to surrender back-to-back-to-back-to-back home runs, following Paul Foytack on July 31, 1963, Chase Wright on April 22, 2007, and Dave Bush.

On August 14, 2008, the Chicago White Sox defeated the Kansas City Royals 9-2. In this game, Jim Thome, Paul Konerko, Alexei Ramírez, and Juan Uribe hit back-to-back-to-back-to-back home runs in that order. Thome, Konerko, and Ramirez blasted their homers off of Joel Peralta, while Uribe did it off of Rob Tejeda. The next batter, veteran backstop Toby Hall, tried aimlessly to hit the ball as far as possible, but his effort resulted in a strike out.

On April 22, 2007 the Boston Red Sox were trailing the New York Yankees 3–0 when Manny Ramirez, J. D. Drew, Mike Lowell and Jason Varitek hit back-to-back-to-back-to-back home runs to put them up 4–3. They eventually went on to win the game 7–6 after a three-run home run by Mike Lowell in the bottom of the 7th inning. On September 18, 2006 trailing 9–5 to the San Diego Padres in the 9th inning, Jeff Kent, J. D. Drew, Russell Martin, and Marlon Anderson of the Los Angeles Dodgers hit back-to-back-to-back-to-back home runs to tie the game. After giving up a run in the top of the 10th, the Dodgers won the game in the bottom of the 10th, on a walk-off two run home run by Nomar Garciaparra. J. D. Drew has been part of two different sets of back-to-back-to-back-to-back home runs. In both occurrences, his homer was the second of the four.

On September 30, 1997, in the sixth inning of Game One of the American League Division Series between the New York Yankees and Cleveland Indians, Tim Raines, Derek Jeter and Paul O'Neill hit back-to-back-to-back home runs for the Yankees. Raines' home run tied the game. New York went on to win 8–6. This was the first occurrence of three home runs in a row ever in postseason play. The Boston Red Sox repeated the feat in Game Four of the 2007 American League Championship Series, also against the Indians.

Twice in MLB history have two brothers hit back-to-back home runs. On April 23, 2013, brothers Melvin Upton, Jr. (formerly B.J. Upton) and Justin Upton hit back-to-back home runs. The first time was on September 15, 1938, when Lloyd Waner and Paul Waner performed the feat.

Simple back-to-back home runs are a relatively frequent occurrence. If a pitcher gives up a homer, he might have his concentration broken and might alter his normal approach in an attempt to "make up for it" by striking out the next batter with some fastballs. Sometimes the next batter will be expecting that and will capitalize on it. A notable back-to-back home run of that type in World Series play involved "Babe Ruth's called shot" in 1932, which was accompanied by various Ruthian theatrics, yet the pitcher, Charlie Root, was allowed to stay in the game. He delivered just one more pitch, which Lou Gehrig drilled out of the park for a back-to-back shot, after which Root was removed from the game.

In Game 3 of the 1976 NLCS, George Foster and Johnny Bench hit back-to-back homers in the last of the ninth off Ron Reed to tie the game. The Series-winning run was scored later in the inning.

Another notable pair of back-to-back home runs occurred on September 14, 1990, when Ken Griffey, Sr. and Ken Griffey, Jr. hit back-to-back home runs, off Kirk McCaskill, the only father-and-son duo to do so in Major League history.

On May 2, 2002, Bret Boone and Mike Cameron of the Seattle Mariners hit back-to-back home runs off of starter Jon Rauch in the first inning of a game against the Chicago White Sox. The Mariners batted around in the inning, and Boone and Cameron came up to bat against reliever Jim Parque with two outs, again hitting back-to-back home runs and becoming the only pair of teammates to hit back-to-back home runs twice in the same inning.

On June 19, 2012, José Bautista and Colby Rasmus hit back-to-back home runs and back-to-back-to-back home runs with Edwin Encarnación for a lead change in each instance.

On July 23, 2017 Whit Merrifield, Jorge Bonifacio, and Eric Hosmer of the Kansas City Royals hit back to back to back home runs in the fourth inning against the Chicago White Sox. The Royals went on to win the game 5-4.

On June 20, 2018 George Springer, Alex Bregman, and José Altuve of the Houston Astros hit back to back to back home runs in the sixth inning against the Tampa Bay Rays. The Astros went on to win the game 5-1.

The record for consecutive home runs by a batter under any circumstances is 4. Of the sixteen players (through 2012) who have hit 4 in one game, six have hit them consecutively. Twenty-eight other batters have hit four consecutive across two games.

Bases on balls do not count as at-bats, and Ted Williams holds the record for consecutive home runs across the most games, 4 in four games played, during September 17–22, 1957, for the Red Sox. Williams hit a pinch-hit homer on the 17th; walked as a pinch-hitter on the 18th; there was no game on the 19th; hit another pinch-homer on the 20th; homered and then was lifted for a pinch-runner after at least one walk, on the 21st; and homered after at least one walk on the 22nd. All in all, he had 4 walks interspersed among his 4 homers.

In World Series play, Reggie Jackson hit a record three in one Series game, the final game (Game 6) in 1977. But those three were a part of a much more impressive feat. He walked on four pitches in the second inning of game 6. Then he hit his three home runs on the first pitch of his next three at bats, off of three different pitchers (4th inning- Hooten, 5th inning- Sosa, 8th inning- Hough). He had also hit one in his last at bat of the previous game, giving him four home runs on four consecutive swings. (His home run in game 5 was also hit on the first pitch, although this did not add to any significant streak.) The four in a row set the record for consecutive homers across two Series games.

In Game 3 of the World Series in 2011, Albert Pujols hit three home runs to tie the record with Babe Ruth and Reggie Jackson. The St. Louis Cardinals went on to win the World Series in Game 7 at Busch Stadium. In Game 1 of the World Series in 2012, Pablo Sandoval of the San Francisco Giants hit three home runs on his first three at-bats of the Series, also tying the record with Pujols, Jackson, and Ruth.

Nomar Garciaparra holds the record for consecutive home runs in the shortest time in terms of innings: 3 homers in 2 innings, on July 23, 2002, for the Boston Red Sox.

An offshoot of hitting for the cycle, a "home run cycle" is where a player hits a solo, 2-run, 3-run, and grand slam all in one game. This is an extremely rare feat, as it requires the batter to not only hit four home runs in a game (which itself has only occurred 18 times in the Major Leagues), but also to hit those home runs with the specific number of runners already on base. Although it is a rare accomplishment, it is largely dependent on circumstances outside the player's control, such as his preceding teammates' ability to get on base, as well as the order in which he comes to bat in any particular inning.

Another variant of the home run cycle would be the "natural home run cycle", which would require a batter to hit a solo, 2-run, 3-run, and grand slam in that order.

Though multiple home run cycles have been recorded in collegiate baseball, the only home run cycle in a professional baseball game belongs to Tyrone Horne, who stroked four long balls for the minor league, Double-A Arkansas Travelers in a game against the San Antonio Missions on July 27, 1998.

A major league player has come close to hitting for the home run cycle several times. Recent examples include:

In the early days of the game, when the ball was less lively and the ballparks generally had very large outfields, most home runs were of the inside-the-park variety. The first home run ever hit in the National League was by Ross Barnes of the Chicago White Stockings (now known as the Chicago Cubs), in 1876. The home "run" was literally descriptive. Home runs over the fence were rare, and only in ballparks where a fence was fairly close. Hitters were discouraged from trying to hit home runs, with the conventional wisdom being that if they tried to do so they would simply fly out. This was a serious concern in the 19th century, because in baseball's early days a ball caught after one bounce was still an out. The emphasis was on place-hitting and what is now called "manufacturing runs" or "small ball".

The home run's place in baseball changed dramatically when the live-ball era began after World War I. First, the materials and manufacturing processes improved significantly, making the now-mass-produced, cork-centered ball somewhat more lively. Batters such as Babe Ruth and Rogers Hornsby took full advantage of rules changes that were instituted during the 1920s, particularly prohibition of the spitball, and the requirement that balls be replaced when worn or dirty. These changes resulted in the baseball being easier to see and hit, and easier to hit out of the park. Meanwhile, as the game's popularity boomed, more outfield seating was built, shrinking the size of the outfield and increasing the chances of a long fly ball resulting in a home run. The teams with the sluggers, typified by the New York Yankees, became the championship teams, and other teams had to change their focus from the "inside game" to the "power game" in order to keep up.

Before 1931, a ball that bounced over an outfield fence during a major league game was considered a home run. The rule was changed to require the ball to clear the fence on the fly, and balls that reached the seats on a bounce became ground rule doubles in most parks. A carryover of the old rule is that if a player deflects a ball over the outfield fence without it touching the ground, it is a home run.

Also, until approximately that time, the ball had to not only go over the fence in fair territory, but to land in the bleachers in fair territory or to still be visibly fair when disappearing behind a wall. The rule stipulated "fair when last seen" by the umpires. Photos from that era in ballparks, such as the Polo Grounds and Yankee Stadium, show ropes strung from the foul poles to the back of the bleachers, or a second "foul pole" at the back of the bleachers, in a straight line with the foul line, as a visual aid for the umpire. Ballparks still use a visual aid much like the ropes; a net or screen attached to the foul poles on the fair side has replaced ropes. As with American football, where a touchdown once required a literal "touch down" of the ball in the end zone but now only requires the "breaking of the [vertical] plane" of the goal line, in baseball the ball need only "break the plane" of the fence in fair territory (unless the ball is caught by a player who is in play, in which case the batter is called out).

Babe Ruth's 60th home run in 1927 was somewhat controversial, because it landed barely in fair territory in the stands down the right field line. Ruth lost a number of home runs in his career due to the when-last-seen rule. Bill Jenkinson, in "The Year Babe Ruth Hit 104 Home Runs", estimates that Ruth lost at least 50 and as many as 78 in his career due to this rule.

Further, the rules once stipulated that an over-the-fence home run in a sudden-victory situation would only count for as many bases as was necessary to "force" the winning run home. For example, if a team trailed by two runs with the bases loaded, and the batter hit a fair ball over the fence, it only counted as a triple, because the runner immediately ahead of him had technically already scored the game-winning run. That rule was changed in the 1920s as home runs became increasingly frequent and popular. Babe Ruth's career total of 714 would have been one higher had that rule not been in effect in the early part of his career.

Major League Baseball keeps running totals of all-time home runs by team, including teams no longer active (prior to 1900) as well as by individual players. Gary Sheffield hit the 250,000th home run in MLB history with a grand slam on September 8, 2008. Sheffield had hit MLB's 249,999th home run against Gio González in his previous at-bat.

The all-time, verified professional baseball record for career home runs for one player, excluding the U. S. Negro Leagues during the era of segregation, is held by Sadaharu Oh. Oh spent his entire career playing for the Yomiuri Giants in Japan's Nippon Professional Baseball, later managing the Giants, the Fukuoka SoftBank Hawks and the 2006 World Baseball Classic Japanese team. Oh holds the all-time home run world record, having hit 868 home runs in his career.

In Major League Baseball, the career record is 762, held by Barry Bonds, who broke Hank Aaron's record on August 7, 2007, when he hit his 756th home run at AT&T Park off pitcher Mike Bacsik. Only eight other major league players have hit as many as 600: Hank Aaron (755), Babe Ruth (714), Alex Rodriguez (696), Willie Mays (660), Ken Griffey, Jr. (630), Albert Pujols (630) and Jim Thome (612), Sammy Sosa (609).

The single season record is 73, set by Barry Bonds in 2001. Other notable single season records were achieved by Babe Ruth who hit 60 in 1927, Roger Maris, with 61 home runs in 1961, and Mark McGwire, who hit 70 in 1998.

Negro League slugger Josh Gibson's Baseball Hall of Fame plaque says he hit "almost 800" home runs in his career. The "Guinness Book of World Records" lists Gibson's lifetime home run total at 800. Ken Burns' award-winning series, "Baseball", states that his actual total may have been as high as 950. Gibson's true total is not known, in part due to inconsistent record keeping in the Negro Leagues. The 1993 edition of the MacMillan "Baseball Encyclopedia" attempted to compile a set of Negro League records, and subsequent work has expanded on that effort. Those records demonstrate that Gibson and Ruth were of comparable power. The 1993 book had Gibson hitting 146 home runs in the 501 "official" Negro League games they were able to account for in his 17-year career, about 1 homer every 3.4 games. Babe Ruth, in 22 seasons (several of them in the dead-ball era), hit 714 in 2503 games, or 1 homer every 3.5 games. The large gap in the numbers for Gibson reflect the fact that Negro League clubs played relatively far fewer league games and many more "barnstorming" or exhibition games during the course of a season, than did the major league clubs of that era.

Other legendary home run hitters include Jimmie Foxx, Mel Ott, Ted Williams, Mickey Mantle (who on September 10, 1960, mythically hit "the longest home run ever" at an estimated distance of , although this was measured after the ball stopped rolling), Reggie Jackson, Harmon Killebrew, Ernie Banks, Mike Schmidt, Dave Kingman, Sammy Sosa (who hit 60 or more home runs in a season 3 times), Ken Griffey, Jr. and Eddie Mathews. In 1987, Joey Meyer of the Denver Zephyrs hit the longest verifiable home run in professional baseball history. The home run was measured at a distance of and was hit inside Denver's Mile High Stadium. Major League Baseball's longest verifiable home run distance is about , by Babe Ruth, to straightaway center field at Tiger Stadium (then called Navin Field and before the double-deck), which landed nearly across the intersection of Trumbull and Cherry.

The location of where Hank Aaron's record 755th home run landed has been monumented in Milwaukee. The spot sits outside Miller Park, where the Milwaukee Brewers currently play. Similarly, the point where Aaron's 715th homer landed, upon breaking Ruth's career record in 1974, is marked in the Turner Field parking lot. A red-painted seat in Fenway Park marks the landing place of the 502-ft home run Ted Williams hit in 1946, the longest measured homer in Fenway's history; a red stadium seat mounted on the wall of the Mall of America in Bloomington, Minnesota, marks the landing spot of Harmon Killebrew's record 520-foot shot in old Metropolitan Stadium.

Replays "to get the call right" have been used extremely sporadically in the past, but the use of instant replay to determine "boundary calls"—home runs and foul balls—was not officially allowed until 2008.

In a game on May 31, 1999, involving the St. Louis Cardinals and Florida Marlins, a hit by Cliff Floyd of the Marlins was initially ruled a double, then a home run, then was changed back to a double when umpire Frank Pulli decided to review video of the play. The Marlins protested that video replay was not allowed, but while the National League office agreed that replay was not to be used in future games, it declined the protest on the grounds it was a judgment call, and the play stood.

In November 2007, the general managers of Major League Baseball voted in favor of implementing instant replay reviews on boundary home run calls. The proposal limited the use of instant replay to determining whether a boundary/home run call is:

On August 28, 2008, instant replay review became available in MLB for reviewing calls in accordance with the above proposal. It was first utilized on September 3, 2008 in a game between the New York Yankees and the Tampa Bay Rays at Tropicana Field. Alex Rodriguez of the Yankees hit what appeared to be a home run, but the ball hit a catwalk behind the foul pole. It was at first called a home run, until Tampa Bay manager Joe Maddon argued the call, and the umpires decided to review the play. After 2 minutes and 15 seconds, the umpires came back and ruled it a home run.

About two weeks later, on September 19, also at Tropicana Field, a boundary call was overturned for the first time. In this case, Carlos Peña of the Rays was given a ground rule double in a game against the Minnesota Twins after an umpire believed a fan reached into the field of play to catch a fly ball in right field. The umpires reviewed the play, determined the fan did not reach over the fence, and reversed the call, awarding Peña a home run.

Aside from the two aforementioned reviews at Tampa Bay, replay was used four more times in the 2008 MLB regular season: twice at Houston, once at Seattle, and once at San Francisco. The San Francisco incident is perhaps the most unusual. Bengie Molina, the Giants' catcher, hit what was first called a single. Molina then was replaced in the game by Emmanuel Burriss, a pinch-runner, before the umpires re-evaluated the call and ruled it a home run. In this instance though, Molina was not allowed to return to the game to complete the run, as he had already been replaced. Molina was credited with the home run, and two RBIs, but not for the run scored which went to Burriss instead.

On October 31, 2009, in the fourth inning of Game 3 of the World Series, Alex Rodriguez hit a long fly ball that appeared to hit a camera protruding over the wall and into the field of play in deep left field. The ball ricocheted off the camera and re-entered the field, initially ruled a double. However, after the umpires consulted with each other after watching the instant replay, the hit was ruled a home run, marking the first time an instant replay home run was hit in a playoff game.


Career achievements



</doc>
<doc id="14149" url="https://en.wikipedia.org/wiki?curid=14149" title="Harappa">
Harappa

Harappa (; Urdu/) is an archaeological site in Punjab, Pakistan, about west of Sahiwal. The site takes its name from a modern village located near the former course of the Ravi River which now runs in north. The current village of Harappa is less than from the ancient site. Although modern Harappa has a legacy railway station from the period of the British Raj, it is today just a small crossroads town of population 15,000.

The site of the ancient city contains the ruins of a Bronze Age fortified city, which was part of the Indus Valley Civilization centered in Sindh and the Punjab, and then the Cemetery H culture. The city is believed to have had as many as 23,500 residents and occupied about with clay brick houses at its greatest extent during the Mature Harappan phase (2600–1900 BC), which is considered large for its time. Per archaeological convention of naming a previously unknown civilization by its first excavated site, the Indus Valley Civilization is also called the Harappan Civilization.

The ancient city of Harappa was heavily damaged under British rule, when bricks from the ruins were used as track ballast in the construction of the Lahore–Multan Railway. In 2005, a controversial amusement park scheme at the site was abandoned when builders unearthed many archaeological artifacts during the early stages of building work. A plea from the Pakistani archaeologist Mohit Prem Kumar to the Ministry of Culture resulted in a restoration of the site.

The Harappan Civilization has its earliest roots in cultures such as that of Mehrgarh, approximately 6000 BC. The two greatest cities, Mohenjo-daro and Harappa, emerged circa 2600 BC along the Indus River valley in Punjab and Sindh. The civilization, with a possible writing system, urban centers, and diversified social and economic system, was rediscovered in the 1920s also after excavations at Mohenjo-daro in Sindh near Larkana, and Harappa, in west Punjab south of Lahore. A number of other sites stretching from the Himalayan foothills in east Punjab, India in the north, to Gujarat in the south and east, and to Pakistani Balochistan in the west have also been discovered and studied. Although the archaeological site at Harappa was damaged in 1857 when engineers constructing the Lahore-Multan railroad (as part of the Sind and Punjab Railway), used brick from the Harappa ruins for track ballast, an abundance of artifacts have nevertheless been found. The bricks discovered were made of red sand, clay, stones and were baked at very high temperature. As early as 1826 Harappa, located in west Punjab, attracted the attention of a British officer in India, who gets credit for preliminary excavations in Harappa.

The Indus Valley civilization was mainly an urban culture sustained by surplus agricultural production and commerce, the latter including trade with Sumer in southern Mesopotamia. Both Mohenjo-Daro and Harappa are generally characterized as having "differentiated living quarters, flat-roofed brick houses, and fortified administrative or religious centers." Although such similarities have given rise to arguments for the existence of a standardized system of urban layout and planning, the similarities are largely due to the presence of a semi-orthogonal type of civic layout, and a comparison of the layouts of Mohenjo-Daro and Harappa shows that they are in fact, arranged in a quite dissimilar fashion.

The weights and measures of the Indus Valley Civilization, on the other hand, were highly standardized, and conform to a set scale of gradations. Distinctive seals were used, among other applications, perhaps for identification of property and shipment of goods. Although copper and bronze were in use, iron was not yet employed. "Cotton was woven and dyed for clothing; wheat, rice, and a variety of vegetables and fruits were cultivated; and a number of animals, including the humped bull, were domesticated," as well as "fowl for fighting". Wheel-made pottery—some of it adorned with animal and geometric motifs—has been found in profusion at all the major Indus sites. A centralized administration for each city, though not the whole civilization, has been inferred from the revealed cultural uniformity; however, it remains uncertain whether authority lay with a commercial oligarchy. Harappans had many trade routes along the Indus River that went as far as the Persian Gulf, Mesopotamia, and Egypt. Some of the most valuable things traded were carnelian and lapis lazuli.

What is clear is that Harappan society was not entirely peaceful, with the human skeletal remains demonstrating some of the highest rates of injury (15.5%) found in South Asian prehistory. Paleopathological analysis demonstrated that leprosy and tuberculosis were present at Harappa, with the highest prevalence of both disease and trauma present in the skeletons from Area G (an ossuary located south-east of the city walls). Furthermore, rates of cranio-facial trauma and infection increased through time demonstrating that the civilization collapsed amid illness and injury. The bioarchaeologists who examined the remains have suggested that the combined evidence for differences in mortuary treatment and epidemiology indicate that some individuals and communities at Harappa were excluded from access to basic resources like health and safety, a basic feature of hierarchical societies worldwide.

The excavators of the site have proposed the following chronology of Harappa's occupation:

By far the most exquisite and obscure artifacts unearthed to date are the small, square steatite (soapstone) seals engraved with human or animal motifs. A large number of seals have been found at such sites as Mohenjo-Daro and Harappa. Many bear pictographic inscriptions generally thought to be a form of writing or script. Despite the efforts of philologists from all parts of the world, and despite the use of modern cryptographic analysis, the signs remain undeciphered. It is also unknown if they reflect proto-Dravidian or other non-Vedic language(s). The ascription of Indus Valley Civilization iconography and epigraphy to historically known cultures is extremely problematic, in part due to the rather tenuous archaeological evidence of such claims, as well as the projection of modern South Asian political concerns onto the archaeological record of the area. This is especially evident in the radically varying interpretations of Harappan material culture as seen from both Pakistan- and India-based scholars.
In February 2006 a school teacher in the village of Sembian-Kandiyur in Tamil Nadu discovered a stone celt (tool) with an inscription estimated to be up to 3,500 years old.

Clay and stone tablets unearthed at Harappa, which were carbon dated 3300–3200 BC., contain trident-shaped and plant-like markings. "It is a big question as to if we can call what we have found true writing, but we have found symbols that have similarities to what became Indus script" said Dr. Richard Meadow of Harvard University, Director of the Harappa Archeological Research Project. This primitive writing is placed slightly earlier than primitive writings of the Sumerians of Mesopotamia, dated c.3100 BC. These markings have similarities to what later became Indus Script.





</doc>
<doc id="14153" url="https://en.wikipedia.org/wiki?curid=14153" title="Hendecasyllable">
Hendecasyllable

In poetry, a hendecasyllable is a line of eleven syllables. The term "hendecasyllabic" is used to refer to two different poetic meters, the older of which is quantatative and used chiefly in classical (Ancient Greek and Latin) poetry and the newer of which is accentual and used in medieval and modern poetry.

The classical hendecasyllable is a quantitative meter used in Ancient Greece in Aeolic verse and in scolia, and later by the Roman poets Catullus and Martial. Each line has eleven syllables; hence the name, which comes from the Greek word for eleven. The heart of the line is the choriamb (- u u -). There are three different versions.
The pattern of the Phalaecian (Latin: "hendecasyllabus phalaecius") is as follows (using "-" for a long syllable, "u" for a short and "x" for an "anceps" or variable syllable):

Another form of hendecasyllabic verse is the "Alcaic" (Latin: "hendecasyllabus alcaicus"; used in the Alcaic stanza), which has the pattern:

The third form of hendecasyllabic verse is the "Sapphic" (Latin: "hendecasyllabus sapphicus"; so named for its use in the Sapphic stanza), with the pattern:

Forty-three of Catullus's poems are hendecasyllabic; for an example, see Catullus 1.

The metre has been imitated in English, notably by Alfred Tennyson, Swinburne, and Robert Frost, cf. "For Once Then Something." Contemporary American poets Annie Finch ("Lucid Waking") and Patricia Smith ("The Reemergence of the Noose") have published recent examples. Poets wanting to capture the hendecasyllabic rhythm in English have simply transposed the pattern into its accentual-syllabic equivalent: /u|/u|/uu/u|/u|, or trochee/trochee/dactyl/trochee/trochee, so that the long/short pattern becomes a stress/unstress pattern. Tennyson, however, maintained the quantitative features of the metre:

The hendecasyllable () is the principal metre in Italian poetry. Its defining feature is a constant stress on the tenth syllable, so that the number of syllables in the verse may vary, equaling eleven in the usual case where the final word is stressed on the penultimate syllable. The verse also has a stress preceding the caesura, on either the fourth or sixth syllable. The first case is called "endecasillabo a minore", or lesser hendecasyllable, and has the first hemistich equivalent to a "quinario"; the second is called "endecasillabo a maiore", or greater hendecasyllable, and has a "settenario" as the first hemistich.
The most usual stress schemes for the Italian hendecasyllable are stresses on sixth and tenth syllables (for example, ""Nel mezzo del cammin di nostra vita"," Dante Alighieri, first line of "The Divine Comedy)," and on the fourth, seventh and tenth syllables (""Un incalzar di cavalli accorrenti"," Ugo Foscolo, "Dei sepolcri").

Most classical Italian poems are composed in hendecasyllables, including the major works of Dante, Francesco Petrarca, Ludovico Ariosto, and Torquato Tasso. The rhyme system varies from terza rima to ottava, from sonnet to canzone. From the early 16th century, hendecasyllables are often used without a strict system, with few or no rhymes, both in poetry and in drama. An early example is "Le Api" ("the bees") by Giovanni di Bernardo Rucellai, written around 1517 and published in 1525, which begins:

Like other early Italian-language tragedies, the "Sophonisba" of Gian Giorgio Trissino (1515) is in blank hendecasyllables. Later examples can be found in the "Canti" of Giacomo Leopardi, where hendecasyllables are alternated with "settenari". The effect of "endecasillabi sciolti" ("untied" hendecasyllables) may be considered similar to that of English blank verse.

It has a role in Italian poetry, and a formal structure, comparable to the alexandrine in French.

The 11-syllable metre was very popular in Polish poetry, especially in the seventeenth and eighteenth centuries, owing to strong Italian literary influence. It was used by Jan Kochanowski, Piotr Kochanowski (who translated "Jerusalem Delivered" by Torquato Tasso), Sebastian Grabowiecki, Wespazjan Kochowski and Stanisław Herakliusz Lubomirski. The greatest Polish Romantic poet, Adam Mickiewicz, set his poem Grażyna in this measure. The Polish hendecasyllable is widely used when translating English 
blank verse.

Almost always, the 11-syllable line is divided by a caesura into 5 + 6 syllables. Only rarely it is fully iambic. 

A popular form of Polish literature that employs the hendacasyllable is the Sapphic stanza: 11/11/11/5. 

The Polish hendecasyllable is often combined with an 8-syllable line: 11a/8b/11a/8b. Such a stanza was used by Mickiewicz in his ballads, as in the following example.

In Polish, the word for hendecasyllable is "jedenastozgłoskowiec".

The hendecasyllable (Portuguese: "hendecassílabo") is a common meter in Portuguese poetry. The best-known Portuguese poem composed in hendecasyllables is Luís de Camões' epic "Os Lusíadas", which begins as follows:

In Portuguese, hendecasyllables are often referred to as decasyllables ("decassílabo"), even when the work in question consists exclusively of eleven-syllable lines (as does "Os Lusíadas").

The term "hendecasyllable" is sometimes used in English poetry to describe a line of iambic pentameter with an extra short syllable at the end, as in the first line of John Keats's "Endymion:" "A thing of beauty is a joy for ever."





</doc>
<doc id="14155" url="https://en.wikipedia.org/wiki?curid=14155" title="Hebrides">
Hebrides

The Hebrides (; , ; ) compose a widespread and diverse archipelago off the west coast of mainland Scotland. There are two main groups: the Inner and Outer Hebrides. These islands have a long history of occupation dating back to the Mesolithic, and the culture of the residents has been affected by the successive influences of Celtic, Norse, and English-speaking peoples. This diversity is reflected in the names given to the islands, which are derived from the languages that have been spoken there in historic and perhaps prehistoric times.

The Hebrides are the source of much of Scottish Gaelic literature and Gaelic music. Today the economy of the islands is dependent on crofting, fishing, tourism, the oil industry, and renewable energy. The Hebrides have lower biodiversity than mainland Scotland, but there is a significant presence of seals and seabirds.

The Hebrides have a diverse geology ranging in age from Precambrian strata that are amongst the oldest rocks in Europe to Paleogene igneous intrusions. Raised shore platforms in the Hebrides are identified as strandflats formed possibly in Pliocene times and later modified by the Quaternary glaciations. 

The Hebrides can be divided into two main groups, separated from one another by the Minch to the north and the Sea of the Hebrides to the south. The Inner Hebrides lie closer to mainland Scotland and include Islay, Jura, Skye, Mull, Raasay, Staffa and the Small Isles. There are 36 inhabited islands in this group. The Outer Hebrides are a chain of more than 100 islands and small skerries located about west of mainland Scotland. There are 15 inhabited islands in this archipelago. The main islands include Barra, Benbecula, Berneray, Harris, Lewis, North Uist, South Uist, and St Kilda. In total, the islands have an area of approximately and a population of 44,759.

A complication is that there are various descriptions of the scope of the Hebrides. The "Collins Encyclopedia of Scotland" describes the Inner Hebrides as lying "east of the Minch", which would include any and all offshore islands. There are various islands that lie in the sea lochs such as and that might not ordinarily be described as "Hebridean", but no formal definitions exist.

In the past, the Outer Hebrides were often referred to as the "Long Isle" (). Today, they are also known as the "Western Isles", although this phrase can also be used to refer to the Hebrides in general.

The Hebrides have a cool temperate climate that is remarkably mild and steady for such a northerly latitude, due to the influence of the Gulf Stream. In the Outer Hebrides the average temperature for the year is 6 °C (44 °F) in January and 14 °C (57 °F) in summer. The average annual rainfall in Lewis is and sunshine hours range from 1,100 – 1,200 "per annum" (13%). The summer days are relatively long, and May to August is the driest period.

The Hebrides were settled during the Mesolithic era around 6500 BC or earlier, after the climatic conditions improved enough to sustain human settlement. Occupation at a site on is dated to 8590 ±95 uncorrected radiocarbon years BP, which is amongst the oldest evidence of occupation in Scotland. There are many examples of structures from the Neolithic period, the finest example being the standing stones at Callanish, dating to the 3rd millennium BC. Cladh Hallan, a Bronze Age settlement on South Uist is the only site in the UK where prehistoric mummies have been found.

In 55 BC, the Greek historian Diodorus Siculus wrote that there was an island called "Hyperborea" (which means "beyond the North Wind"), where a round temple stood from which the moon appeared only a little distance above the earth every 19 years. This may have been a reference to the stone circle at Callanish.

A traveller called Demetrius of Tarsus related to Plutarch the tale of an expedition to the west coast of Scotland in or shortly before AD 83. He stated it was a gloomy journey amongst uninhabited islands, but he had visited one which was the retreat of holy men. He mentioned neither the druids nor the name of the island.

The first written records of native life begin in the 6th century AD, when the founding of the kingdom of Dál Riata took place. This encompassed roughly what is now Argyll and Bute and Lochaber in Scotland and County Antrim in Ireland. The figure of Columba looms large in any history of Dál Riata, and his founding of a monastery on Iona ensured that the kingdom would be of great importance in the spread of Christianity in northern Britain. However, Iona was far from unique. Lismore in the territory of the Cenél Loairn, was sufficiently important for the death of its abbots to be recorded with some frequency and many smaller sites, such as on Eigg, Hinba, and Tiree, are known from the annals.

North of Dál Riata, the Inner and Outer Hebrides were nominally under Pictish control, although the historical record is sparse. Hunter (2000) states that in relation to King Bridei I of the Picts in the sixth century: "As for Shetland, Orkney, Skye and the Western Isles, their inhabitants, most of whom appear to have been Pictish in culture and speech at this time, are likely to have regarded Bridei as a fairly distant presence.”

Viking raids began on Scottish shores towards the end of the 8th century and the Hebrides came under Norse control and settlement during the ensuing decades, especially following the success of Harald Fairhair at the Battle of in 872. In the Western Isles Ketill Flatnose may have been the dominant figure of the mid 9th century, by which time he had amassed a substantial island realm and made a variety of alliances with other Norse leaders. These princelings nominally owed allegiance to the Norwegian crown, although in practice the latter's control was fairly limited. Norse control of the Hebrides was formalised in 1098 when Edgar of Scotland formally signed the islands over to Magnus III of Norway. The Scottish acceptance of Magnus III as King of the Isles came after the Norwegian king had conquered Orkney, the Hebrides and the Isle of Man in a swift campaign earlier the same year, directed against the local Norwegian leaders of the various island petty kingdoms. By capturing the islands Magnus imposed a more direct royal control, although at a price. His skald Bjorn Cripplehand recorded that in Lewis "fire played high in the heaven" as "flame spouted from the houses" and that in the Uists "the king dyed his sword red in blood".

The Hebrides were now part of the Kingdom of the Isles, whose rulers were themselves vassals of the Kings of Norway. This situation lasted until the partitioning of the Western Isles in 1156, at which time the Outer Hebrides remained under Norwegian control while the Inner Hebrides broke out under Somerled, the Norse-Gael kinsman of the Manx royal house.

Following the ill-fated 1263 expedition of Haakon IV of Norway, the Outer Hebrides and the Isle of Man were yielded to the Kingdom of Scotland as a result of the 1266 Treaty of Perth. Although their contribution to the islands can still be found in personal and place names, the archaeological record of the Norse period is very limited. The best known find is the Lewis chessmen, which date from the mid 12th century.

As the Norse era drew to a close, the Norse-speaking princes were gradually replaced by Gaelic-speaking clan chiefs including the MacLeods of Lewis and Harris, Clan Donald and MacNeil of Barra. This transition did little to relieve the islands of internecine strife although by the early 14th century the MacDonald Lords of the Isles, based on Islay, were in theory these chiefs' feudal superiors and managed to exert some control.

The Lords of the Isles ruled the Inner Hebrides as well as part of the Western Highlands as subjects of the King of Scots until John MacDonald, fourth Lord of the Isles, squandered the family's powerful position. A rebellion by his nephew, Alexander of Lochalsh provoked an exasperated James IV to forfeit the family's lands in 1493.

In 1598, King James VI authorised some "Gentleman Adventurers" from Fife to civilise the "most barbarous Isle of Lewis". Initially successful, the colonists were driven out by local forces commanded by Murdoch and Neil MacLeod, who based their forces on in . The colonists tried again in 1605 with the same result, but a third attempt in 1607 was more successful and in due course Stornoway became a Burgh of Barony. By this time, Lewis was held by the Mackenzies of Kintail (later the Earls of Seaforth), who pursued a more enlightened approach, investing in fishing in particular. The Seaforths' royalist inclinations led to Lewis becoming garrisoned during the Wars of the Three Kingdoms by Cromwell's troops, who destroyed the old castle in Stornoway.

With the implementation of the Treaty of Union in 1707, the Hebrides became part of the new Kingdom of Great Britain, but the clans' loyalties to a distant monarch were not strong. A considerable number of islesmen "came out" in support of the Jacobite Earl of Mar in the "15" and again in the 1745 rising including Macleod of Dunvegan and MacLea of Lismore. The aftermath of the decisive Battle of Culloden, which effectively ended Jacobite hopes of a Stuart restoration, was widely felt. The British government's strategy was to estrange the clan chiefs from their kinsmen and turn their descendants into English-speaking landlords whose main concern was the revenues their estates brought rather than the welfare of those who lived on them. This may have brought peace to the islands, but in the following century it came at a terrible price. In the wake of the rebellion, the clan system was broken up and islands of the Hebrides became a series of landed estates.

The early 19th century was a time of improvement and population growth. Roads and quays were built; the slate industry became a significant employer on Easdale and surrounding islands; and the construction of the Crinan and Caledonian canals and other engineering works such as Telford's "Bridge across the Atlantic" improved transport and access. However, in the mid-19th century, the inhabitants of many parts of the Hebrides were devastated by the Clearances, which destroyed communities throughout the Highlands and Islands as the human populations were evicted and replaced with sheep farms. The position was exacerbated by the failure of the islands' kelp industry that thrived from the 18th century until the end of the Napoleonic Wars in 1815 and large scale emigration became endemic.

As , a Gaelic poet from South Uist, wrote for his countrymen who were obliged to leave the Hebrides in the late 18th century, emigration was the only alternative to "sinking into slavery" as the Gaels had been unfairly dispossessed by rapacious landlords. In the 1880s, the "Battle of the Braes" involved a demonstration against unfair land regulation and eviction, stimulating the calling of the Napier Commission. Disturbances continued until the passing of the 1886 Crofters' Act.

For those who remained, new economic opportunities emerged through the export of cattle, commercial fishing and tourism. Nonetheless emigration and military service became the choice of many and the archipelago's populations continued to dwindle throughout the late 19th century and for much of the 20th century. Lengthy periods of continuous occupation notwithstanding, many of the smaller islands were abandoned.

There were, however, continuing gradual economic improvements, among the most visible of which was the replacement of the traditional thatched blackhouse with accommodation of a more modern design and with the assistance of Highlands and Islands Enterprise many of the islands' populations have begun to increase after decades of decline. The discovery of substantial deposits of North Sea oil in 1965 and the renewables sector have contributed to a degree of economic stability in recent decades. For example, the Arnish yard has had a chequered history but has been a significant employer in both the oil and renewables industries.

The widespread immigration of mainlanders, particularly non-Gaelic speakers, has been a subject of controversy.

Many contemporary Gaelic musicians have roots in the Hebrides, including Julie Fowlis (North Uist), Catherine-Ann MacPhee (Barra), Kathleen MacInnes (South Uist), and Ishbel MacAskill (Lewis). All of these singers have repertoire based on the Hebridean tradition, such as ' and ' (waulking songs). This tradition includes many songs composed by little-known or anonymous poets before 1800, such as "", "" and "". Several of Runrig's songs are inspired by the archipelago; Calum and were raised on North Uist and Donnie Munro on Skye.

The Gaelic poet spent much of his life in the Hebrides and often referred to them in his poetry, including in ' and '. The best known Gaelic poet of her era, (Mary MacPherson, 1821–98), embodied the spirit of the land agitation of the 1870s and 1880s. This, and her powerful evocation of the Hebrides—she was from Skye—has made her among the most enduring Gaelic poets. Allan MacDonald (1859–1905), who spent his adult life on Eriskay and South Uist, composed hymns and verse in honour of the Blessed Virgin, the Christ Child, and the Eucharist. In his secular poetry, MacDonald praised the beauty of Eriskay and its people. In his verse drama, "" ("The Old Wives' Parliament"), he lampooned the gossiping of his female parishioners and local marriage customs.

In the 20th century, Murdo Macfarlane of Lewis wrote ', a well-known poem about the Gaelic revival in the Outer Hebrides. Sorley MacLean, the most respected 20th-century Gaelic writer, was born and raised on Raasay, where he set his best known poem, ', about the devastating effect of the Highland Clearances. , described by MacLean as "one of the few really significant living poets in Scotland, writing in any language" (West Highland Free Press, October 1992), and whose Scottish Gaelic-language novel, "", was voted in the Top Ten of the 100 Best-Ever Books from Scotland, was raised on South Uist.



The residents of the Hebrides have spoken a variety of different languages during the long period of human occupation.

It is assumed that Pictish must once have predominated in the northern Inner Hebrides and Outer Hebrides. The Scottish Gaelic language arrived via Ireland due to the growing influence of the kingdom of Dál Riata from the 6th century AD onwards, and became the dominant language of the southern Hebrides at that time. For a few centuries, the military might of the ' meant that Old Norse was prevalent in the Hebrides. North of , the place names that existed prior to the 9th century have been all but obliterated. The Old Norse name for the Hebrides during the Viking occupation was ', which means "Southern Isles"; in contrast to the ", or "Northern Isles" of Orkney and Shetland.

South of , Gaelic place names are more common, and after the 13th century, Gaelic became the main language of the entire Hebridean archipelago. Due to Scots and English being favoured in government and the educational system, the Hebrides have been in a state of diglossia since at least the 17th century. The Highland Clearances of the 19th century accelerated the language shift away from Scottish Gaelic, as did increased migration and the continuing lower status of Gaelic speakers. Nevertheless, as late as the end of the 19th century, there were significant populations of monolingual Gaelic speakers, and the Hebrides still contain the highest percentages of Gaelic speakers in Scotland. This is especially true of the Outer Hebrides, where a slim majority speak the language. The Scottish Gaelic college, , is based on Skye and Islay.

Ironically, given the status of the Western Isles as the last Gaelic-speaking stronghold in Scotland, the Gaelic language name for the islands – " – means "isles of the foreigners"; from the time when they were under Norse colonisation.

The earliest written references that have survived relating to the islands were made circa 77 AD by Pliny the Elder in his "Natural History", where he states that there are 30 ', and makes a separate reference to ', which Watson (1926) concludes is unequivocally the Outer Hebrides. Writing about 80 years later, in 140-150 AD, Ptolemy, drawing on the earlier naval expeditions of , writes that there are five ' (possibly meaning the Inner Hebrides) and '. Later texts in classical Latin, by writers such as , use the forms ' and '.

The name ' recorded by Ptolemy may be pre-Celtic. Islay is Ptolemy's , the use of the "p" hinting at a Brythonic or Pictish tribal name, , although the root is not Gaelic. Woolf (2012) has suggested that ' may be "an Irish attempt to reproduce the word ' phonetically rather than by translating it" and that the tribe's name may come from the root ' meaning "horse". Watson (1926) also notes the possible relationship between ' and the ancient Irish Ulaid tribal name ' and the personal name of a king recorded in the "".

The names of other individual islands reflect their complex linguistic history. The majority are Norse or Gaelic but the roots of several other Hebrides may have a pre-Celtic origin. Adomnán, the 7th century abbot of Iona, records Colonsay as "Colosus" and Tiree as "Ethica", both of which may be pre-Celtic names. The etymology of Skye is complex and may also include a pre-Celtic root. Lewis is "" in Old Norse and although various suggestions have been made as to a Norse meaning (such as "song house") the name is not of Gaelic origin and the Norse credentials are questionable.

The earliest comprehensive written list of Hebridean island names was undertaken by Donald Monro in 1549, which in some cases also provides the earliest written form of the island name. The derivations of all of the inhabited islands of the Hebrides and some of the larger uninhabited ones are listed below.

Lewis and Harris is the largest island in Scotland and the third largest in the British Isles, after Great Britain and Ireland. It incorporates Lewis in the north and Harris in the south, both of which are frequently referred to as individual islands, although they are joined by a land border. Remarkably, the island does not have a common name in either English or Gaelic and is referred to as "Lewis and Harris", "Lewis with Harris", "Harris with Lewis" etc. For this reason it is treated as two separate islands below. The derivation of Lewis may be pre-Celtic (see above) and the origin of Harris is no less problematic. In the Ravenna Cosmography, "Erimon" may refer to Harris (or possibly the Outer Hebrides as a whole). This word may derive from the ( "desert". The origin of Uist () is similarly unclear.

There are various examples of Inner Hebridean island names that were originally Gaelic but have become completely replaced. For example, Adomnán records "Sainea", "Elena", "Ommon" and "Oideacha" in the Inner Hebrides, which names must have passed out of usage in the Norse era and whose locations are not clear. One of the complexities is that an island may have had a Celtic name, which was replaced by a similar-sounding Norse name, but then reverted to an essentially Gaelic name with a Norse "øy" or "ey" ending. See for example Rona below.

The names of uninhabited islands follow the same general patterns as the inhabited islands. The following are the ten largest in the Hebrides and their outliers.

The etymology of St Kilda, a small archipelago west of the Outer Hebrides, and its main island Hirta, is very complex. No saint is known by the name of Kilda, and various theories have been proposed for the word's origin, which dates from the late 16th century. Haswell-Smith (2004) notes that the full name "St Kilda" first appears on a Dutch map dated 1666, and that it may have been derived from Norse ' ("sweet wellwater") or from a mistaken Dutch assumption that the spring ' was dedicated to a saint. (' is a tautological placename, consisting of the Gaelic and Norse words for "well", i.e. "well well"). The origin of the Gaelic for "Hirta"—', ', or '—which long pre-dates the use of "St Kilda", is similarly open to interpretation. Watson (1926) offers the Old Irish ', a word meaning "death", possibly relating to the dangerous seas. Maclean (1977), drawing on an Icelandic saga describing an early 13th-century voyage to Ireland that mentions a visit to the islands of ', speculates that the shape of Hirta resembles a stag, "" being "stags" in Norse.

The etymology of small islands may be no less complex. In relation to , R. L. Stevenson believed that "black and dismal" was a translation of the name, noting that "as usual, in Gaelic, it is not the only one."

In some respects the Hebrides lack biodiversity in comparison to mainland Britain; for example, there are only half as many mammalian species. However, these islands provide breeding grounds for many important seabird species including the world's largest colony of northern gannets. Avian life includes the corncrake, red-throated diver, rock dove, kittiwake, tystie, Atlantic puffin, goldeneye, golden eagle and white-tailed sea eagle. The last named was re-introduced to Rùm in 1975 and has successfully spread to various neighbouring islands, including Mull. There is a small population of red-billed chough concentrated on the islands of Islay and Colonsay.

Red deer are common on the hills and the grey seal and common seal are present around the coasts of Scotland. Colonies of seals are found on Oronsay and the Treshnish Isles. The rich freshwater streams contain brown trout, Atlantic salmon and water shrew. Offshore, minke whales, Killer whales, basking sharks, porpoises and dolphins are among the sealife that can be seen.

Heather moor containing ling, bell heather, cross-leaved heath, bog myrtle and fescues is abundant and there is a diversity of Arctic and alpine plants including Alpine pearlwort and mossy cyphal.

Loch Druidibeg on South Uist is a national nature reserve owned and managed by Scottish Natural Heritage. The reserve covers 1,677 hectares across the whole range of local habitats. Over 200 species of flowering plants have been recorded on the reserve, some of which are nationally scarce. South Uist is considered the best place in the UK for the aquatic plant slender naiad, which is a European Protected Species.

Hedgehogs are not native to the Outer Hebrides—they were introduced in the 1970s to reduce garden pests—and their spread poses a threat to the eggs of ground nesting wading birds. In 2003, Scottish Natural Heritage undertook culls of hedgehogs in the area although these were halted in 2007 due to protests; trapped animals were instead relocated to the mainland.





</doc>
<doc id="14158" url="https://en.wikipedia.org/wiki?curid=14158" title="HMS Dreadnought">
HMS Dreadnought

Several ships and one submarine of the Royal Navy have borne the name HMS "Dreadnought" in the expectation that they would "dread nought", i.e. "fear nothing". The 1906 ship was one of the Royal Navy's most famous vessels; battleships built after her were referred to as 'dreadnoughts', and earlier battleships became known as pre-dreadnoughts.


Also

Citations

References



</doc>
<doc id="14159" url="https://en.wikipedia.org/wiki?curid=14159" title="Hartmann Schedel">
Hartmann Schedel

Hartmann Schedel (13 February 1440 – 28 November 1514) was a German physician, humanist, historian, and one of the first cartographers to use the printing press. He was born and died in Nuremberg. Matheolus Perusinus served as his tutor. 

Schedel is best known for his writing the text for the "Nuremberg Chronicle", known as "Schedelsche Weltchronik" (English: "Schedel's World Chronicle"), published in 1493 in Nuremberg. It was commissioned by Sebald Schreyer (1446 – 1520) and Sebastian Kammermeister (1446 – 1503). Maps in the "Chronicle" were the first ever illustrations of many cities and countries.

With the invention of the printing press by Johannes Gutenberg in 1447, it became feasible to print books and maps for a larger customer basis. Because they had to be handwritten, books were previously rare and very expensive.

Schedel was also a notable collector of books, art and old master prints. An album he had bound in 1504, which once contained five engravings by Jacopo de' Barbari, provides important evidence for dating de' Barbari's work.





</doc>
<doc id="14160" url="https://en.wikipedia.org/wiki?curid=14160" title="Hexameter">
Hexameter

Hexameter is a metrical line of verses consisting of six feet. It was the standard epic metre in classical Greek and Latin literature, such as in the "Iliad", "Odyssey" and "Aeneid". Its use in other genres of composition include Horace's satires, Ovid's "Metamorphoses," and the Hymns of Orpheus. According to Greek mythology, hexameter was invented by Phemonoe, daughter of Apollo and the first Pythia of Delphi.
In classical hexameter, the six feet follow these rules:

A short syllable (υ) is a syllable with a short vowel and no consonant at the end. A long syllable (–) is a syllable that either has a long vowel, one or more consonants at the end (or a long consonant), or both. Spaces between words are not counted in syllabification, so for instance "cat" is a long syllable in isolation, but "cat attack" would be syllabified as short-short-long: "ca", "ta", "tack" (υ υ –).

Variations of the sequence from line to line, as well as the use of caesura (logical full stops within the line) are essential in avoiding what may otherwise be a monotonous sing-song effect.

Although the rules seem simple, it is hard to use classical hexameter in English, because English is a stress-timed language that condenses vowels and consonants between stressed syllables, while hexameter relies on the regular timing of the phonetic sounds. Languages having the latter properties (i.e., languages that are not stress-timed) include Ancient Greek, Latin, Lithuanian and Hungarian.

While the above classical hexameter has never enjoyed much popularity in English, where the standard metre is iambic pentameter, English poems have frequently been written in iambic hexameter. There are numerous examples from the 16th century and a few from the 17th; the most prominent of these is Michael Drayton's "Poly-Olbion" (1612) in couplets of iambic hexameter. An example from Drayton (marking the feet):

In the 17th century the iambic hexameter, also called alexandrine, was used as a substitution in the heroic couplet, and as one of the types of permissible lines in lyrical stanzas and the Pindaric odes of Cowley and Dryden.

Several attempts were made in the 19th century to naturalise the dactylic hexameter to English, by Henry Wadsworth Longfellow, Arthur Hugh Clough and others, none of them particularly successful. Gerard Manley Hopkins wrote many of his poems in six-foot iambic and sprung rhythm lines. In the 20th century a loose ballad-like six-foot line with a strong medial pause was used by William Butler Yeats. The iambic six-foot line has also been used occasionally, and an accentual six-foot line has been used by translators from the Latin and many poets.

In the late 18th century the hexameter was adapted to the Lithuanian language by Kristijonas Donelaitis. His poem ""Metai" (The Seasons)" is considered the most successful hexameter text in Lithuanian as yet.

Hungarian is extremely suitable to hexameter (and other forms of poetry based on quantitative metre), insomuch that if a student comes to a halt when reciting a poem s/he was supposed to learn, s/he can say "I'm stuck here, unfortunately the rest won't come into my mind," and it will be an impeccable hexameter in Hungarian: "Itt elakadtam, sajnos nem jut eszembe a többi."

Due to this feature, hexameter has been widely used both in translated (Greek and Roman) and in original poetry up to the twentieth century (e.g. by Miklós Radnóti).





</doc>
<doc id="14162" url="https://en.wikipedia.org/wiki?curid=14162" title="Timeline of Polish history">
Timeline of Polish history

This is a timeline of Polish history, comprising important legal and territorial changes and political events in Poland and its predecessor states. To read about the background to these events, see History of Poland. See also the list of Polish monarchs and list of Prime Ministers of Poland.





</doc>
<doc id="14168" url="https://en.wikipedia.org/wiki?curid=14168" title="Himalia">
Himalia

Himalia may refer to:



</doc>
<doc id="14169" url="https://en.wikipedia.org/wiki?curid=14169" title="Heracleidae">
Heracleidae

In Greek mythology, the Heracleidae (; ) or Heraclids were the numerous descendants of Heracles (Hercules), especially applied in a narrower sense to the descendants of Hyllus, the eldest of his four sons by Deianira (Hyllus was also sometimes thought of as Heracles' son by Melite). Other Heracleidae included Macaria, Lamos, Manto, Bianor, Tlepolemus, and Telephus. These Heraclids were a group of Dorian kings who conquered the Peloponnesian kingdoms of Mycenae, Sparta and Argos; according to the literary tradition in Greek mythology, they claimed a right to rule through their ancestor. Since Karl Otfried Müller's "Die Dorier" (1830, English translation 1839), I. ch. 3, their rise to dominance has been associated with a "Dorian invasion".

Though details of genealogy differ from one ancient author to another, the cultural significance of the mythic theme, that the descendants of Heracles, exiled after his death, returned some generations later to reclaim land that their ancestors had held in Mycenaean Greece, was to assert the primal legitimacy of a traditional ruling clan that traced its origin, thus its legitimacy, to Heracles.

Heracles, whom Zeus had originally intended to be ruler of Argos, Lacedaemon and Messenian Pylos, had been supplanted by the cunning of Hera, and his intended possessions had fallen into the hands of Eurystheus, king of Mycenae. After the death of Heracles, his children, after many wanderings, found refuge from Eurystheus at Athens. Eurystheus, on his demand for their surrender being refused, attacked Athens, but was defeated and slain. Hyllus and his brothers then invaded Peloponnesus, but after a year's stay were forced by a pestilence to quit. They withdrew to Thessaly, where Aegimius, the mythical ancestor of the Dorians, whom Heracles had assisted in war against the Lapithae, adopted Hyllus and made over to him a third part of his territory.

After the death of Aegimius, his two sons, Pamphylus and Dymas, voluntarily submitted to Hyllus (who was, according to the Dorian tradition in Herodotus V. 72, really an Achaean), who thus became ruler of the Dorians, the three branches of that race being named after these three heroes. Desiring to reconquer his paternal inheritance, Hyllus consulted the Delphic oracle, which told him to wait for "the third fruit", (or "the third crop") and then enter Peloponnesus by "a narrow passage by sea". Accordingly, after three years, Hyllus marched across the isthmus of Corinth to attack Atreus, the successor of Eurystheus, but was slain in single combat by Echemus, king of Tegea. This second attempt was followed by a third under Cleodaeus and a fourth under Aristomachus, both unsuccessful.

At last, Temenus, Cresphontes and Aristodemus, the sons of Aristomachus, complained to the oracle that its instructions had proved fatal to those who had followed them. They received the answer that by the "third fruit" the "third generation" was meant, and that the "narrow passage" was not the isthmus of Corinth, but the straits of Rhium. They accordingly built a fleet at Naupactus, but before they set sail, Aristodemus was struck by lightning (or shot by Apollo) and the fleet destroyed, because one of the Heracleidae had slain an Acarnanian soothsayer.

The oracle, being again consulted by Temenus, bade him offer an expiatory sacrifice and banish the murderer for ten years, and look out for a man with three eyes to act as guide. On his way back to Naupactus, Temenus fell in with Oxylus, an Aetolian, who had lost one eye, riding on a horse (thus making up the three eyes) and immediately pressed him into his service. According to another account, a mule on which Oxylus rode had lost an eye. The Heracleidae repaired their ships, sailed from Naupactus to Antirrhium, and thence to Rhium in Peloponnesus. A decisive battle was fought with Tisamenus, son of Orestes, the chief ruler in the peninsula, who was defeated and slain. This conquest was traditionally dated eighty years after the Trojan War.

The Heracleidae, who thus became practically masters of Peloponnesus, proceeded to distribute its territory among themselves by lot. Argos fell to Temenus, Lacedaemon to Procles and Eurysthenes, the twin sons of Aristodemus; and Messenia to Cresphontes (tradition maintains that Cresphontes cheated in order to obtain Messenia, which had the best land of all.) The fertile district of Elis had been reserved by agreement for Oxylus. The Heracleidae ruled in Lacedaemon until 221 BCE, but disappeared much earlier in the other countries.

This conquest of Peloponnesus by the Dorians, commonly called the "Dorian invasion" or the "Return of the Heraclidae", is represented as the recovery by the descendants of Heracles of the rightful inheritance of their hero ancestor and his sons. The Dorians followed the custom of other Greek tribes in claiming as ancestor for their ruling families one of the legendary heroes, but the traditions must not on that account be regarded as entirely mythical. They represent a joint invasion of Peloponnesus by Aetolians and Dorians, the latter having been driven southward from their original northern home under pressure from the Thessalians. It is noticeable that there is no mention of these Heraclidae or their invasion in Homer or Hesiod. Herodotus (vi. 52) speaks of poets who had celebrated their deeds, but these were limited to events immediately succeeding the death of Heracles.

At Sparta, the Heraclids formed two dynasties ruling jointly: the Agiads and the Eurypontids.

At Corinth the Heraclids ruled as the Bacchiadae dynasty before the aristocratic revolution, which brought a Bacchiad aristocracy into power. The kings were as follows:

The story was first amplified by the Greek tragedians, who probably drew their inspiration from local legends, which glorified the services rendered by Athens to the rulers of Peloponnesus.

The Heracleidae are the main subject of Euripides' play, "Heracleidae". J. A. Spranger found the political subtext of "Heracleidae", never far to seek, so particularly apt in Athens towards the end of the peace of Nicias, in 419 BCE, that he suggested the date as its first performance.

In the tragedy, Iolaus, Heracles' old comrade, and his children, Macaria and her brothers and sisters have hidden from Eurystheus in Athens, which was ruled by King Demophon; as the first scene makes clear, their expectation is that the blood relationship of the kings with Heracles and their father's past indebtedness to Theseus, will finally provide them sanctuary. As Eurysttheus prepared to attack, an oracle told Demophon that he would win if and only if a noble woman was sacrificed to Persephone. Macaria volunteered for the sacrifice and a spring was named the Macarian spring in her honor.




</doc>
<doc id="14170" url="https://en.wikipedia.org/wiki?curid=14170" title="HIV">
HIV

The human immunodeficiency virus (HIV) is a lentivirus (a subgroup of retrovirus) that causes HIV infection and over time acquired immunodeficiency syndrome (AIDS). AIDS is a condition in humans in which progressive failure of the immune system allows life-threatening opportunistic infections and cancers to thrive. Without treatment, average survival time after infection with HIV is estimated to be 9 to 11 years, depending on the HIV subtype. In most cases, HIV is a sexually transmitted infection and occurs by contact with or transfer of blood, pre-ejaculate, semen, and vaginal fluids. Non-sexual transmission can occur from an infected mother to her infant through breast milk. An HIV-positive mother can transmit HIV to her baby both during pregnancy and childbirth due to exposure to her blood or vaginal fluid. Within these bodily fluids, HIV is present as both free virus particles and virus within infected immune cells.

HIV infects vital cells in the human immune system such as helper T cells (specifically CD4 T cells), macrophages, and dendritic cells. HIV infection leads to low levels of CD4 T cells through a number of mechanisms, including pyroptosis of abortively infected T cells, apoptosis of uninfected bystander cells, direct viral killing of infected cells, and killing of infected CD4 T cells by CD8 cytotoxic lymphocytes that recognize infected cells. When CD4 T cell numbers decline below a critical level, cell-mediated immunity is lost, and the body becomes progressively more susceptible to opportunistic infections, leading to the development of AIDS.

HIV is a member of the genus "Lentivirus", part of the family "Retroviridae". Lentiviruses have many morphologies and biological properties in common. Many species are infected by lentiviruses, which are characteristically responsible for long-duration illnesses with a long incubation period. Lentiviruses are transmitted as single-stranded, positive-sense, enveloped RNA viruses. Upon entry into the target cell, the viral RNA genome is converted (reverse transcribed) into double-stranded DNA by a virally encoded enzyme, reverse transcriptase, that is transported along with the viral genome in the virus particle. The resulting viral DNA is then imported into the cell nucleus and integrated into the cellular DNA by a virally encoded enzyme, integrase, and host co-factors. Once integrated, the virus may become latent, allowing the virus and its host cell to avoid detection by the immune system, for an indiscriminate amount of time. The HIV virus can remain dormant in the human body for up to ten years after primary infection; during this period the virus does not cause symptoms. Alternatively, the integrated viral DNA may be transcribed, producing new RNA genomes and viral proteins, using host cell resources, that are packaged and released from the cell as new virus particles that will begin the replication cycle anew.

Two types of HIV have been characterized: HIV-1 and HIV-2. HIV-1 is the virus that was initially discovered and termed both LAV (Lymphadenopathy Associated Virus) and HTLV-III (Human T cell Lymphotropic Virus III). HIV-1 is more virulent and more infective than HIV-2, and is the cause of the majority of HIV infections globally. The lower infectivity of HIV-2 compared to HIV-1 implies that fewer of those exposed to HIV-2 will be infected per exposure. Due to its relatively poor capacity for transmission, HIV-2 is largely confined to West Africa.

HIV is different in structure from other retroviruses. It is roughly spherical with a diameter of about 120 nm, around 60 times smaller than a red blood cell. It is composed of two copies of positive-sense single-stranded RNA that codes for the virus's nine genes enclosed by a conical capsid composed of 2,000 copies of the viral protein p24. The single-stranded RNA is tightly bound to nucleocapsid proteins, p7, and enzymes needed for the development of the virion such as reverse transcriptase, proteases, ribonuclease and integrase. A matrix composed of the viral protein p17 surrounds the capsid ensuring the integrity of the virion particle.

This is, in turn, surrounded by the viral envelope, that is composed of the lipid bilayer taken from the membrane of a human host cell when the newly formed virus particle buds from the cell. The viral envelope contains proteins from the host cell and relatively few copies of the HIV Envelope protein, which consists of a cap made of three molecules known as glycoprotein (gp) 120, and a stem consisting of three gp41 molecules that anchor the structure into the viral envelope. The Envelope protein, encoded by the HIV "env" gene, allows the virus to attach to target cells and fuse the viral envelope with the target cell's membrane releasing the viral contents into the cell and initiating the infectious cycle.

As the sole viral protein on the surface of the virus, the Envelope protein is a major target for HIV vaccine efforts. Over half of the mass of the trimeric envelope spike is N-linked glycans. The density is high as the glycans shield the underlying viral protein from neutralisation by antibodies. This is one of the most densely glycosylated molecules known and the density is sufficiently high to prevent the normal maturation process of glycans during biogenesis in the endoplasmic and Golgi apparatus. The majority of the glycans are therefore stalled as immature 'high-mannose' glycans not normally present on human glycoproteins that are secreted or present on a cell surface. The unusual processing and high density means that almost all broadly neutralising antibodies that have so far been identified (from a subset of patients that have been infected for many months to years) bind to or, are adapted to cope with, these envelope glycans.

The molecular structure of the viral spike has now been determined by X-ray crystallography and cryo-electron microscopy. These advances in structural biology were made possible due to the development of stable recombinant forms of the viral spike by the introduction of an intersubunit disulphide bond and an isoleucine to proline mutation in gp41. The so-called SOSIP trimers not only reproduce the antigenic properties of the native viral spike but also display the same degree of immature glycans as presented on the native virus. Recombinant trimeric viral spikes are promising vaccine candidates as they display less non-neutralising epitopes than recombinant monomeric gp120, which act to suppress the immune response to target epitopes. 

The RNA genome consists of at least seven structural landmarks (LTR, TAR, RRE, PE, SLIP, CRS, and INS), and nine genes ("gag", "pol", and "env", "tat", "rev", "nef", "vif", "vpr", "vpu", and sometimes a tenth "tev", which is a fusion of "tat", "env" and "rev"), encoding 19 proteins. Three of these genes, "gag", "pol", and "env", contain information needed to make the structural proteins for new virus particles. For example, "env" codes for a protein called gp160 that is cut in two by a cellular protease to form gp120 and gp41. The six remaining genes, "tat", "rev", "nef", "vif", "vpr", and "vpu" (or "vpx" in the case of HIV-2), are regulatory genes for proteins that control the ability of HIV to infect cells, produce new copies of virus (replicate), or cause disease.

The two Tat proteins (p16 and p14) are transcriptional transactivators for the LTR promoter acting by binding the TAR RNA element. The TAR may also be processed into microRNAs that regulate the apoptosis genes ERCC1 and IER3. The Rev protein (p19) is involved in shuttling RNAs from the nucleus and the cytoplasm by binding to the RRE RNA element. The Vif protein (p23) prevents the action of APOBEC3G (a cellular protein that deaminates Cytidine to Uridine in the single stranded viral DNA and/or interferes with reverse transcription). The Vpr protein (p14) arrests cell division at G2/M. The Nef protein (p27) down-regulates CD4 (the major viral receptor), as well as the MHC class I and class II molecules.

Nef also interacts with SH3 domains. The Vpu protein (p16) influences the release of new virus particles from infected cells. The ends of each strand of HIV RNA contain an RNA sequence called the long terminal repeat (LTR). Regions in the LTR act as switches to control production of new viruses and can be triggered by proteins from either HIV or the host cell. The Psi element is involved in viral genome packaging and recognized by Gag and Rev proteins. The SLIP element (TTTTTT) is involved in the frameshift in the Gag-Pol reading frame required to make functional Pol.

The term viral tropism refers to the cell types a virus infects. HIV can infect a variety of immune cells such as CD4 T cells, macrophages, and microglial cells. HIV-1 entry to macrophages and CD4 T cells is mediated through interaction of the virion envelope glycoproteins (gp120) with the CD4 molecule on the target cells' membrane and also with chemokine co-receptors.

Macrophage-tropic (M-tropic) strains of HIV-1, or non-syncytia-inducing strains (NSI; now called R5 viruses) use the "β"-chemokine receptor CCR5 for entry and are, thus, able to replicate in both macrophages and CD4 T cells. This CCR5 co-receptor is used by almost all primary HIV-1 isolates regardless of viral genetic subtype. Indeed, macrophages play a key role in several critical aspects of HIV infection. They appear to be the first cells infected by HIV and perhaps the source of HIV production when CD4 cells become depleted in the patient. Macrophages and microglial cells are the cells infected by HIV in the central nervous system. In tonsils and adenoids of HIV-infected patients, macrophages fuse into multinucleated giant cells that produce huge amounts of virus.

T-tropic strains of HIV-1, or syncytia-inducing (SI; now called X4 viruses) strains replicate in primary CD4 T cells as well as in macrophages and use the "α"-chemokine receptor, CXCR4, for entry.

Dual-tropic HIV-1 strains are thought to be transitional strains of HIV-1 and thus are able to use both CCR5 and CXCR4 as co-receptors for viral entry.

The "α"-chemokine SDF-1, a ligand for CXCR4, suppresses replication of T-tropic HIV-1 isolates. It does this by down-regulating the expression of CXCR4 on the surface of HIV target cells. M-tropic HIV-1 isolates that use only the CCR5 receptor are termed R5; those that use only CXCR4 are termed X4, and those that use both, X4R5. However, the use of co-receptor alone does not explain viral tropism, as not all R5 viruses are able to use CCR5 on macrophages for a productive infection and HIV can also infect a subtype of myeloid dendritic cells, which probably constitute a reservoir that maintains infection when CD4 T cell numbers have declined to extremely low levels.

Some people are resistant to certain strains of HIV. For example, people with the CCR5-Δ32 mutation are resistant to infection by the R5 virus, as the mutation leaves HIV unable to bind to this co-receptor, reducing its ability to infect target cells.

Sexual intercourse is the major mode of HIV transmission. Both X4 and R5 HIV are present in the seminal fluid, which enables the virus to be transmitted from a male to his sexual partner. The virions can then infect numerous cellular targets and disseminate into the whole organism. However, a selection process leads to a predominant transmission of the R5 virus through this pathway. In patients infected with subtype B HIV-1, there is often a co-receptor switch in late-stage disease and T-tropic variants that can infect a variety of T cells through CXCR4. These variants then replicate more aggressively with heightened virulence that causes rapid T cell depletion, immune system collapse, and opportunistic infections that mark the advent of AIDS. Thus, during the course of infection, viral adaptation to the use of CXCR4 instead of CCR5 may be a key step in the progression to AIDS. A number of studies with subtype B-infected individuals have determined that between 40 and 50 percent of AIDS patients can harbour viruses of the SI and, it is presumed, the X4 phenotypes.

HIV-2 is much less pathogenic than HIV-1 and is restricted in its worldwide distribution to West Africa. The adoption of "accessory genes" by HIV-2 and its more promiscuous pattern of co-receptor usage (including CD4-independence) may assist the virus in its adaptation to avoid innate restriction factors present in host cells. Adaptation to use normal cellular machinery to enable transmission and productive infection has also aided the establishment of HIV-2 replication in humans. A survival strategy for any infectious agent is not to kill its host but ultimately become a commensal organism. Having achieved a low pathogenicity, over time, variants that are more successful at transmission will be selected.

The HIV virion enters macrophages and CD4 T cells by the adsorption of glycoproteins on its surface to receptors on the target cell followed by fusion of the viral envelope with the target cell membrane and the release of the HIV capsid into the cell.

Entry to the cell begins through interaction of the trimeric envelope complex (gp160 spike) on the HIV viral envelope and both CD4 and a chemokine co-receptor (generally either CCR5 or CXCR4, but others are known to interact) on the target cell surface. Gp120 binds to integrin αβ activating LFA-1, the central integrin involved in the establishment of virological synapses, which facilitate efficient cell-to-cell spreading of HIV-1. The gp160 spike contains binding domains for both CD4 and chemokine receptors.

The first step in fusion involves the high-affinity attachment of the CD4 binding domains of gp120 to CD4. Once gp120 is bound with the CD4 protein, the envelope complex undergoes a structural change, exposing the chemokine receptor binding domains of gp120 and allowing them to interact with the target chemokine receptor. This allows for a more stable two-pronged attachment, which allows the N-terminal fusion peptide gp41 to penetrate the cell membrane. Repeat sequences in gp41, HR1, and HR2 then interact, causing the collapse of the extracellular portion of gp41 into a hairpin. This loop structure brings the virus and cell membranes close together, allowing fusion of the membranes and subsequent entry of the viral capsid.

After HIV has bound to the target cell, the HIV RNA and various enzymes, including reverse transcriptase, integrase, ribonuclease, and protease, are injected into the cell. During the microtubule-based transport to the nucleus, the viral single-strand RNA genome is transcribed into double-strand DNA, which is then integrated into a host chromosome.

HIV can infect dendritic cells (DCs) by this CD4-CCR5 route, but another route using mannose-specific C-type lectin receptors such as DC-SIGN can also be used. DCs are one of the first cells encountered by the virus during sexual transmission. They are currently thought to play an important role by transmitting HIV to T-cells when the virus is captured in the mucosa by DCs. The presence of FEZ-1, which occurs naturally in neurons, is believed to prevent the infection of cells by HIV.
HIV-1 entry, as well as entry of many other retroviruses, has long been believed to occur exclusively at the plasma membrane. More recently, however, productive infection by pH-independent, clathrin-dependent endocytosis of HIV-1 has also been reported and was recently suggested to constitute the only route of productive entry.

Shortly after the viral capsid enters the cell, an enzyme called reverse transcriptase liberates the positive-sense single-stranded RNA genome from the attached viral proteins and copies it into a complementary DNA (cDNA) molecule. The process of reverse transcription is extremely error-prone, and the resulting mutations may cause drug resistance or allow the virus to evade the body's immune system. The reverse transcriptase also has ribonuclease activity that degrades the viral RNA during the synthesis of cDNA, as well as DNA-dependent DNA polymerase activity that creates a sense DNA from the "antisense" cDNA. Together, the cDNA and its complement form a double-stranded viral DNA that is then transported into the cell nucleus. The integration of the viral DNA into the host cell's genome is carried out by another viral enzyme called integrase.

The integrated viral DNA may then lie dormant, in the latent stage of HIV infection. To actively produce the virus, certain cellular transcription factors need to be present, the most important of which is NF-"κ"B (nuclear factor kappa B), which is upregulated when T cells become activated. This means that those cells most likely to be targeted, entered and subsequently killed by HIV are those currently fighting infection.

During viral replication, the integrated DNA provirus is transcribed into RNA, some of which then undergo RNA splicing to produce mature mRNAs. These mRNAs are exported from the nucleus into the cytoplasm, where they are translated into the regulatory proteins Tat (which encourages new virus production) and Rev. As the newly produced Rev protein is produced it moves to the nucleus, where it binds to full-length, unspliced copies of virus RNAs and allows them to leave the nucleus. Some of these full-length RNAs function as new copies of the virus genome, while others function as mRNAs that are translated to produce the structural proteins Gag and Env. Gag proteins bind to copies of the virus RNA genome to package them into new virus particles.

HIV-1 and HIV-2 appear to package their RNA differently. HIV-1 will bind to any appropriate RNA. HIV-2 will preferentially bind to the mRNA that was used to create the Gag protein itself.

Two RNA genomes are encapsidated in each HIV-1 particle (see Structure and genome of HIV). Upon infection and replication catalyzed by reverse transcriptase, recombination between the two genomes can occur. Recombination occurs as the single-strand (+)RNA genomes are reverse transcribed to form DNA. During reverse transcription, the nascent DNA can switch multiple times between the two copies of the viral RNA. This form of recombination is known as copy-choice. Recombination events may occur throughout the genome. Anywhere from two to 20 recombination events per genome may occur at each replication cycle, and these events can rapidly shuffle the genetic information that is transmitted from parental to progeny genomes.

Viral recombination produces genetic variation that likely contributes to the evolution of resistance to anti-retroviral therapy. Recombination may also contribute, in principle, to overcoming the immune defenses of the host. Yet, for the adaptive advantages of genetic variation to be realized, the two viral genomes packaged in individual infecting virus particles need to have arisen from separate progenitor parental viruses of differing genetic constitution. It is unknown how often such mixed packaging occurs under natural conditions.

Bonhoeffer "et al." suggested that template switching by reverse transcriptase acts as a repair process to deal with breaks in the single-stranded RNA genome. In addition, Hu and Temin suggested that recombination is an adaptation for repair of damage in the RNA genomes. Strand switching (copy-choice recombination) by reverse transcriptase could generate an undamaged copy of genomic DNA from two damaged single-stranded RNA genome copies. This view of the adaptive benefit of recombination in HIV could explain why each HIV particle contains two complete genomes, rather than one. Furthermore, the view that recombination is a repair process implies that the benefit of repair can occur at each replication cycle, and that this benefit can be realized whether or not the two genomes differ genetically. On the view that recombination in HIV is a repair process, the generation of recombinational variation would be a consequence, but not the cause of, the evolution of template switching.

HIV-1 infection causes chronic ongoing inflammation and production of reactive oxygen species. Thus, the HIV genome may be vulnerable to oxidative damages, including breaks in the single-stranded RNA. For HIV, as well as for viruses generally, successful infection depends on overcoming host defensive strategies that often include production of genome-damaging reactive oxygen. Thus, Michod "et al." suggested that recombination by viruses is an adaptation for repair of genome damages, and that recombinational variation is a byproduct that may provide a separate benefit.

The final step of the viral cycle, assembly of new HIV-1 virions, begins at the plasma membrane of the host cell. The Env polyprotein (gp160) goes through the endoplasmic reticulum and is transported to the Golgi complex where it is cleaved by furin resulting in the two HIV envelope glycoproteins, gp41 and gp120. These are transported to the plasma membrane of the host cell where gp41 anchors gp120 to the membrane of the infected cell. The Gag (p55) and Gag-Pol (p160) polyproteins also associate with the inner surface of the plasma membrane along with the HIV genomic RNA as the forming virion begins to bud from the host cell. The budded virion is still immature as the gag polyproteins still need to be cleaved into the actual matrix, capsid and nucleocapsid proteins. This cleavage is mediated by the packaged viral protease and can be inhibited by antiretroviral drugs of the protease inhibitor class. The various structural components then assemble to produce a mature HIV virion. Only mature virions are then able to infect another cell.

The classical process of infection of a cell by a virion can be called "cell-free spread" to distinguish it from a more recently recognized process called "cell-to-cell spread". In cell-free spread (see figure), virus particles bud from an infected T cell, enter the blood or extracellular fluid and then infect another T cell following a chance encounter. HIV can also disseminate by direct transmission from one cell to another by a process of cell-to-cell spread, for which two pathways have been described. Firstly, an infected T cell can transmit virus directly to a target T cell via a virological synapse. Secondly, an antigen-presenting cell (APC), such as a macrophage or dendritic cell, can transmit HIV to T cells by a process that either involves productive infection (in the case of macrophages) or capture and transfer of virions "in trans" (in the case of dendritic cells). Whichever pathway is used, infection by cell-to-cell transfer is reported to be much more efficient than cell-free virus spread. A number of factors contribute to this increased efficiency, including polarised virus budding towards the site of cell-to-cell contact, close apposition of cells, which minimizes fluid-phase diffusion of virions, and clustering of HIV entry receptors on the target cell to the contact zone. Cell-to-cell spread is thought to be particularly important in lymphoid tissues where CD4 T cells are densely packed and likely to interact frequently. Intravital imaging studies have supported the concept of the HIV virological synapse "in vivo". The hybrid spreading mechanisms of HIV contribute to the virus' ongoing replication in spite of anti-retroviral therapies.

HIV differs from many viruses in that it has very high genetic variability. This diversity is a result of its fast replication cycle, with the generation of about 10 virions every day, coupled with a high mutation rate of approximately 3 x 10 per nucleotide base per cycle of replication and recombinogenic properties of reverse transcriptase.

This complex scenario leads to the generation of many variants of HIV in a single infected patient in the course of one day. This variability is compounded when a single cell is simultaneously infected by two or more different strains of HIV. When simultaneous infection occurs, the genome of progeny virions may be composed of RNA strands from two different strains. This hybrid virion then infects a new cell where it undergoes replication. As this happens, the reverse transcriptase, by jumping back and forth between the two different RNA templates, will generate a newly synthesized retroviral DNA sequence that is a recombinant between the two parental genomes. This recombination is most obvious when it occurs between subtypes.

The closely related simian immunodeficiency virus (SIV) has evolved into many strains, classified by the natural host species. SIV strains of the African green monkey (SIVagm) and sooty mangabey (SIVsmm) are thought to have a long evolutionary history with their hosts. These hosts have adapted to the presence of the virus, which is present at high levels in the host's blood, but evokes only a mild immune response, does not cause the development of simian AIDS, and does not undergo the extensive mutation and recombination typical of HIV infection in humans.

In contrast, when these strains infect species that have not adapted to SIV ("heterologous" or similar hosts such as rhesus or cynomologus macaques), the animals develop AIDS and the virus generates genetic diversity similar to what is seen in human HIV infection. Chimpanzee SIV (SIVcpz), the closest genetic relative of HIV-1, is associated with increased mortality and AIDS-like symptoms in its natural host. SIVcpz appears to have been transmitted relatively recently to chimpanzee and human populations, so their hosts have not yet adapted to the virus. This virus has also lost a function of the Nef gene that is present in most SIVs. For non-pathogenic SIV variants, Nef suppresses T cell activation through the CD3 marker. Nef's function in non-pathogenic forms of SIV is to downregulate expression of inflammatory cytokines, MHC-1, and signals that affect T cell trafficking. In HIV-1 and SIVcpz, Nef does not inhibit T-cell activation and it has lost this function. Without this function, T cell depletion is more likely, leading to immunodeficiency.

Three groups of HIV-1 have been identified on the basis of differences in the envelope ("env") region: M, N, and O. Group M is the most prevalent and is subdivided into eight subtypes (or clades), based on the whole genome, which are geographically distinct. The most prevalent are subtypes B (found mainly in North America and Europe), A and D (found mainly in Africa), and C (found mainly in Africa and Asia); these subtypes form branches in the phylogenetic tree representing the lineage of the M group of HIV-1. Co-infection with distinct subtypes gives rise to circulating recombinant forms (CRFs). In 2000, the last year in which an analysis of global subtype prevalence was made, 47.2% of infections worldwide were of subtype C, 26.7% were of subtype A/CRF02_AG, 12.3% were of subtype B, 5.3% were of subtype D, 3.2% were of CRF_AE, and the remaining 5.3% were composed of other subtypes and CRFs. Most HIV-1 research is focused on subtype B; few laboratories focus on the other subtypes. The existence of a fourth group, "P", has been hypothesised based on a virus isolated in 2009. The strain is apparently derived from gorilla SIV (SIVgor), first isolated from western lowland gorillas in 2006.

HIV-2's closest relative is SIVsm, a strain of SIV found in sooty mangabees. Since HIV-1 is derived from SIVcpz, and HIV-2 from SIVsm, the genetic sequence of HIV-2 is only partially homologous to HIV-1 and more closely resembles that of SIVsm.

Many HIV-positive people are unaware that they are infected with the virus. For example, in 2001 less than 1% of the sexually active urban population in Africa had been tested, and this proportion is even lower in rural populations. Furthermore, in 2001 only 0.5% of pregnant women attending urban health facilities were counselled, tested or receive their test results. Again, this proportion is even lower in rural health facilities. Since donors may therefore be unaware of their infection, donor blood and blood products used in medicine and medical research are routinely screened for HIV.

HIV-1 testing is initially done using an enzyme-linked immunosorbent assay (ELISA) to detect antibodies to HIV-1. Specimens with a non-reactive result from the initial ELISA are considered HIV-negative unless new exposure to an infected partner or partner of unknown HIV status has occurred. Specimens with a reactive ELISA result are retested in duplicate. If the result of either duplicate test is reactive, the specimen is reported as repeatedly reactive and undergoes confirmatory testing with a more specific supplemental test (e.g., a polymerase chain reaction (PCR), western blot or, less commonly, an immunofluorescence assay (IFA)). Only specimens that are repeatedly reactive by ELISA and positive by IFA or PCR or reactive by western blot are considered HIV-positive and indicative of HIV infection. Specimens that are repeatedly ELISA-reactive occasionally provide an indeterminate western blot result, which may be either an incomplete antibody response to HIV in an infected person or nonspecific reactions in an uninfected person.

Although IFA can be used to confirm infection in these ambiguous cases, this assay is not widely used. In general, a second specimen should be collected more than a month later and retested for persons with indeterminate western blot results. Although much less commonly available, nucleic acid testing (e.g., viral RNA or proviral DNA amplification method) can also help diagnosis in certain situations. In addition, a few tested specimens might provide inconclusive results because of a low quantity specimen. In these situations, a second specimen is collected and tested for HIV infection.

Modern HIV testing is extremely accurate. A single screening test is correct more than 99% of the time. The chance of a false-positive result in standard two-step testing protocol is estimated to be about 1 in 250,000 in a low risk population. Testing post-exposure is recommended immediately and then at six weeks, three months, and six months.

The latest recommendations of the CDC show that HIV testing must start with an immunoassay combination test for HIV-1 and HIV-2 antibodies and p24 antigen. A negative result rules out HIV exposure, while a positive one must be followed by an HIV-1/2 antibody differentiation immunoassay to detect which antibodies are present. This gives rise to four possible scenarios:

An updated algorithm published by the CDC in June 2014 recommends that diagnosis starts with the p24 antigen test. A negative result rules out infection, while a positive one must be followed by an HIV-1/2 antibody differentiation immunoassay. A positive differentiation test confirms diagnosis, while a negative or indeterminate result must be followed by nucleic acid test (NAT). A positive NAT result confirms HIV-1 infection whereas a negative result rules out infection (false positive p24).

HIV/AIDS research includes all medical research that attempts to prevent, treat, or cure HIV/AIDS, as well as fundamental research about the nature of HIV as an infectious agent and AIDS as the disease caused by HIV.

Many governments and research institutions participate in HIV/AIDS research. This research includes behavioral health interventions, such as research into sex education, and drug development, such as research into microbicides for sexually transmitted diseases, HIV vaccines, and anti-retroviral drugs. Other medical research areas include the topics of pre-exposure prophylaxis, post-exposure prophylaxis, circumcision and HIV, and accelerated aging effects.

After many years of research, an untested HIV vaccine has been created. Bi-specific antibodies, that target both the surface of T-cells and viral epitopes, can prevent entry of the virus into human cells. Another group has utilised the same technology to develop a bi-specific antibody that neutralises viral particles by cross-linking of envelope glycoproteins.

HIV latency, and the consequent viral reservoir in CD4 T cells, dendritic cells, as well as macrophages, is the main barrier to eradication of the virus.

It is important to note that although HIV is highly virulent, transmission is greatly reduced when an HIV-infected person has a suppressed or undetectable viral load (<50 copies/ml) due to prolonged and successful anti-retroviral treatment. Hence, it can be said to be almost impossible (but still non-zero) for an HIV-infected person who has an undetectable viral load to transmit the virus, even during unprotected sexual intercourse, as there would be a negligible amount of HIV present in the seminal fluid, vaginal secretions or blood, for transmission to occur. This does not mean however, that prolonged anti-retroviral treatment will result in a suppressed viral load. An undetectable viral load, generally agreed as less than 50 copies per milliliter of blood, can only be proven by a polymerase chain reaction (PCR) test.

At the same time, it is important to recognise that reaching an undetectable viral load is determined by many factors, including treatment adherence, HIV resistance to certain anti-retroviral drugs, stigma, and inadequate health systems.

AIDS was first clinically observed in 1981 in the United States. The initial cases were a cluster of injection drug users and gay men with no known cause of impaired immunity who showed symptoms of "Pneumocystis carinii" pneumonia (PCP), a rare opportunistic infection that was known to occur in people with very compromised immune systems. Soon thereafter, additional gay men developed a previously rare skin cancer called Kaposi's sarcoma (KS). Many more cases of PCP and KS emerged, alerting U.S. Centers for Disease Control and Prevention (CDC) and a CDC task force was formed to monitor the outbreak. The earliest retrospectively described case of AIDS is believed to have been in Norway beginning in 1966.

In the beginning, the CDC did not have an official name for the disease, often referring to it by way of the diseases that were associated with it, for example, lymphadenopathy, the disease after which the discoverers of HIV originally named the virus. They also used "Kaposi's Sarcoma and Opportunistic Infections", the name by which a task force had been set up in 1981. In the general press, the term "GRID", which stood for gay-related immune deficiency, had been coined. The CDC, in search of a name, and looking at the infected communities coined "the 4H disease", as it seemed to single out homosexuals, heroin users, hemophiliacs, and Haitians. However, after determining that AIDS was not isolated to the gay community, it was realized that the term GRID was misleading and "AIDS" was introduced at a meeting in July 1982. By September 1982 the CDC started using the name AIDS.
In 1983, two separate research groups led by American Robert Gallo and French investigators Françoise Barré-Sinoussi and Luc Montagnier independently declared that a novel retrovirus may have been infecting AIDS patients, and published their findings in the same issue of the journal "Science". Gallo claimed that a virus his group had isolated from a person with AIDS was strikingly similar in shape to other human T-lymphotropic viruses (HTLVs) his group had been the first to isolate. Gallo's group called their newly isolated virus HTLV-III. At the same time, Montagnier's group isolated a virus from a patient presenting with swelling of the lymph nodes of the neck and physical weakness, two classic symptoms of primary HIV infection. Contradicting the report from Gallo's group, Montagnier and his colleagues showed that core proteins of this virus were immunologically different from those of HTLV-I. Montagnier's group named their isolated virus lymphadenopathy-associated virus (LAV). As these two viruses turned out to be the same, in 1986 LAV and HTLV-III were renamed HIV.

Another group working contemporaneously with the Montagnier and Gallo groups was that of Dr. Jay Levy at the University of California, San Francisco. He independently discovered the AIDS virus in 1983 and named it the AIDS associated retrovirus (ARV). This virus was very different from the virus reported by the Montagnier and Gallo groups. The ARV strains indicated, for the first time, the heterogeneity of HIV isolates and several of these remain classic examples of the AIDS virus found in the United States.

Both HIV-1 and HIV-2 are believed to have originated in non-human primates in West-central Africa, and are believed to have transferred to humans (a process known as zoonosis) in the early 20th century.

HIV-1 appears to have originated in southern Cameroon through the evolution of SIV(cpz), a simian immunodeficiency virus (SIV) that infects wild chimpanzees (HIV-1 descends from the SIV(cpz) endemic in the chimpanzee subspecies "Pan troglodytes troglodytes"). The closest relative of HIV-2 is SIV (smm), a virus of the sooty mangabey ("Cercocebus atys atys"), an Old World monkey living in littoral West Africa (from southern Senegal to western Côte d'Ivoire). New World monkeys such as the owl monkey are resistant to HIV-1 infection, possibly because of a genomic fusion of two viral resistance genes.
HIV-1 is thought to have jumped the species barrier on at least three separate occasions, giving rise to the three groups of the virus, M, N, and O.
There is evidence that humans who participate in bushmeat activities, either as hunters or as bushmeat vendors, commonly acquire SIV. However, SIV is a weak virus, and it is typically suppressed by the human immune system within weeks of infection. It is thought that several transmissions of the virus from individual to individual in quick succession are necessary to allow it enough time to mutate into HIV. Furthermore, due to its relatively low person-to-person transmission rate, it can only spread throughout the population in the presence of one or more high-risk transmission channels, which are thought to have been absent in Africa prior to the 20th century.

Specific proposed high-risk transmission channels, allowing the virus to adapt to humans and spread throughout the society, depend on the proposed timing of the animal-to-human crossing. Genetic studies of the virus suggest that the most recent common ancestor of the HIV-1 M group dates back to circa 1910. Proponents of this dating link the HIV epidemic with the emergence of colonialism and growth of large colonial African cities, leading to social changes, including different patterns of sexual contact (especially multiple, concurrent partnerships), the spread of prostitution, and the concomitant high frequency of genital ulcer diseases (such as syphilis) in nascent colonial cities. While transmission rates of HIV during vaginal intercourse are typically low, they are increased manyfold if one of the partners suffers from a sexually transmitted infection resulting in genital ulcers. Early 1900s colonial cities were notable due to their high prevalence of prostitution and genital ulcers to the degree that as of 1928 as many as 45% of female residents of eastern Leopoldville were thought to have been prostitutes and as of 1933 around 15% of all residents of the same city were infected by one of the forms of syphilis.

An alternative view — unsupported by evidence — holds that unsafe medical practices in Africa during years following World War II, such as unsterile reuse of single-use syringes during mass vaccination, antibiotic, and anti-malaria treatment campaigns, were the initial vector that allowed the virus to adapt to humans and spread.

The earliest, well-documented case of HIV in a human dates back to 1959 in the Belgian Congo. The virus may have been present in the United States as early as the mid-to-late 1950s, as a sixteen-year-old male presented with symptoms in 1966 died in 1969.





</doc>
<doc id="14173" url="https://en.wikipedia.org/wiki?curid=14173" title="HOL">
HOL

Hol or HOL may refer to:








</doc>
<doc id="14174" url="https://en.wikipedia.org/wiki?curid=14174" title="Hostile witness">
Hostile witness

A hostile witness, otherwise known as an adverse witness or an unfavorable witness, is a witness at trial whose testimony on direct examination is either openly antagonistic or appears to be contrary to the legal position of the party who called the witness.

During direct examination, if the examining attorney who called the witness finds that their testimony is antagonistic or contrary to the legal position of their client, the attorney may request that the judge declare the witness hostile. If the request is granted, the attorney may proceed to ask the witness leading questions. Leading questions either suggest the answer ("You saw my client sign the contract, correct?") or challenge (impeach) the witness's testimony. As a rule, leading questions are generally only allowed during cross-examination, but a hostile witness is an exception to this rule.

In cross-examination conducted by the opposing party's attorney, a witness is presumed to be hostile and the examining attorney is not required to seek the judge's permission before asking leading questions. Attorneys can influence a hostile witness's responses by using Gestalt psychology to influence the way the witness perceives the situation, and utility theory to understand their likely responses. The attorney will integrate a hostile witness's expected responses into the larger case strategy through pretrial planning and through adapting as necessary during the course of the trial.

In the state of New South Wales, the term 'unfavourable witness' is defined by section 38 of the Evidence Act which permits the prosecution to cross-examine their own witness. For example, if the prosecution calls all material witnesses relevant to a case before the court, and any evidence given is not favourable to, or supports the prosecution case, or a witness has given a prior inconsistent statement, then the prosecution may seek leave of the court, via section 192, to test the witness in relation to their evidence.

In New Zealand, section 94 of the Evidence Act 2006 permits a party to cross-examine their own witness if the presiding judge determines the witness to be hostile and gives permission.




</doc>
<doc id="14179" url="https://en.wikipedia.org/wiki?curid=14179" title="Henry I of England">
Henry I of England

Henry I (c. 1068 – 1 December 1135), also known as Henry Beauclerc, was King of England from 1100 to his death. Henry was the fourth son of William the Conqueror and was educated in Latin and the liberal arts. On William's death in 1087, Henry's elder brothers Robert Curthose and William Rufus inherited Normandy and England, respectively, but Henry was left landless. Henry purchased the County of Cotentin in western Normandy from Robert, but William and Robert deposed him in 1091. Henry gradually rebuilt his power base in the Cotentin and allied himself with William against Robert. Henry was present when William died in a hunting accident in 1100, and he seized the English throne, promising at his coronation to correct many of William's less popular policies. Henry married Matilda of Scotland but continued to have a large number of mistresses by whom he had many illegitimate children.

Robert, who invaded in 1101, disputed Henry's control of England; this military campaign ended in a negotiated settlement that confirmed Henry as king. The peace was short-lived, and Henry invaded the Duchy of Normandy in 1105 and 1106, finally defeating Robert at the Battle of Tinchebray. Henry kept Robert imprisoned for the rest of his life. Henry's control of Normandy was challenged by Louis VI of France, Baldwin VII of Flanders and Fulk V of Anjou, who promoted the rival claims of Robert's son, William Clito, and supported a major rebellion in the Duchy between 1116 and 1119. Following Henry's victory at the Battle of Brémule, a favourable peace settlement was agreed with Louis in 1120.

Considered by contemporaries to be a harsh but effective ruler, Henry skilfully manipulated the barons in England and Normandy. In England, he drew on the existing Anglo-Saxon system of justice, local government and taxation, but also strengthened it with additional institutions, including the royal exchequer and itinerant justices. Normandy was also governed through a growing system of justices and an exchequer. Many of the officials who ran Henry's system were "new men" of obscure backgrounds rather than from families of high status, who rose through the ranks as administrators. Henry encouraged ecclesiastical reform, but became embroiled in a serious dispute in 1101 with Archbishop Anselm of Canterbury, which was resolved through a compromise solution in 1105. He supported the Cluniac order and played a major role in the selection of the senior clergy in England and Normandy.

Henry's only legitimate son and heir, William Adelin, drowned in the "White Ship" disaster of 1120, throwing the royal succession into doubt. Henry took a second wife, Adeliza of Louvain, in the hope of having another son, but their marriage was childless. In response to this, Henry declared his daughter, Empress Matilda, his heir and married her to Geoffrey of Anjou. The relationship between Henry and the couple became strained, and fighting broke out along the border with Anjou. Henry died on 1 December 1135 after a week of illness. Despite his plans for Matilda, the King was succeeded by his nephew, Stephen of Blois, resulting in a period of civil war known as the Anarchy.
Henry was probably born in England in 1068, in either the summer or the last weeks of the year, possibly in the town of Selby in Yorkshire. His father was William the Conqueror, the Duke of Normandy, who had invaded England in 1066 to become the King of England, establishing lands stretching into Wales. The invasion had created an Anglo-Norman elite, many with estates spread across both sides of the English Channel. These Anglo-Norman barons typically had close links to the kingdom of France, which was then a loose collection of counties and smaller polities, under only the minimal control of the king. Henry's mother, Matilda of Flanders, was the granddaughter of Robert II of France, and she probably named Henry after her uncle, King Henry I of France.

Henry was the youngest of William and Matilda's four sons. Physically he resembled his older brothers Robert Curthose, Richard and William Rufus, being, as historian David Carpenter describes, "short, stocky and barrel-chested," with black hair. As a result of their age differences and Richard's early death, Henry would have probably seen relatively little of his older brothers. He probably knew his sister, Adela, well, as the two were close in age. There is little documentary evidence for his early years; historians Warren Hollister and Kathleen Thompson suggest he was brought up predominantly in England, while Judith Green argues he was initially brought up in the Duchy. He was probably educated by the Church, possibly by Bishop Osmund, the King's chancellor, at Salisbury Cathedral; it is uncertain if this indicated an intent by his parents for Henry to become a member of the clergy. It is also uncertain how far Henry's education extended, but he was probably able to read Latin and had some background in the liberal arts. He was given military training by an instructor called Robert Achard, and Henry was knighted by his father on 24 May 1086.

In 1087, William was fatally injured during a campaign in the Vexin. Henry joined his dying father near Rouen in September, where the King partitioned his possessions among his sons. The rules of succession in western Europe at the time were uncertain; in some parts of France, primogeniture, in which the eldest son would inherit a title, was growing in popularity. In other parts of Europe, including Normandy and England, the tradition was for lands to be divided up, with the eldest son taking patrimonial lands – usually considered to be the most valuable – and younger sons given smaller, or more recently acquired, partitions or estates.

In dividing his lands, William appears to have followed the Norman tradition, distinguishing between Normandy, which he had inherited, and England, which he had acquired through war. William's second son, Richard, had died in a hunting accident, leaving Henry and his two brothers to inherit William's estate. Robert, the eldest, despite being in armed rebellion against his father at the time of his death, received Normandy. England was given to William Rufus, who was in favour with the dying king. Henry was given a large sum of money, usually reported as £5,000, with the expectation that he would also be given his mother's modest set of lands in Buckinghamshire and Gloucestershire. William's funeral at Caen was marred by angry complaints from a local man, and Henry may have been responsible for resolving the dispute by buying off the protester with silver.

Robert returned to Normandy, expecting to have been given both the Duchy and England, to find that William Rufus had crossed the Channel and been crowned king, as William II. The two brothers disagreed fundamentally over the inheritance, and Robert soon began to plan an invasion of England to seize the kingdom, helped by a rebellion by some of the leading nobles against William Rufus. Henry remained in Normandy and took up a role within Robert's court, possibly either because he was unwilling to side openly with William Rufus, or because Robert might have taken the opportunity to confiscate Henry's inherited money if he had tried to leave. William Rufus sequestered Henry's new estates in England, leaving Henry landless.

In 1088, Robert's plans for the invasion of England began to falter, and he turned to Henry, proposing that his brother lend him some of his inheritance, which Henry refused. Henry and Robert then came to an alternative arrangement, in which Robert would make Henry the count of western Normandy, in exchange for £3,000. Henry's lands were a new countship based around a delegation of the ducal authority in the Cotentin, but it extended across the Avranchin, with control over the bishoprics of both. This also gave Henry influence over two major Norman leaders, Hugh d'Avranches and Richard de Redvers, and the abbey of Mont Saint-Michel, whose lands spread out further across the Duchy. Robert's invasion force failed to leave Normandy, leaving William Rufus secure in England.

Henry quickly established himself as count, building up a network of followers from western Normandy and eastern Brittany, whom historian John Le Patourel has characterised as "Henry's gang". His early supporters included Roger of Mandeville, Richard of Redvers, Richard d'Avranches and Robert Fitzhamon, along with the churchman Roger of Salisbury. Robert attempted to go back on his deal with Henry and re-appropriate the county, but Henry's grip was already sufficiently firm to prevent this. Robert's rule of the Duchy was chaotic, and parts of Henry's lands became almost independent of central control from Rouen.

During this period, neither William nor Robert seems to have trusted Henry. Waiting until the rebellion against William Rufus was safely over, Henry returned to England in July 1088. He met with the King but was unable to persuade him to grant him their mother's estates, and travelled back to Normandy in the autumn. While he had been away, however, Odo, the Bishop of Bayeux, who regarded Henry as a potential competitor, had convinced Robert that Henry was conspiring against the duke with William Rufus. On landing, Odo seized Henry and imprisoned him in Neuilly-la-Forêt, and Robert took back the county of the Cotentin. Henry was held there over the winter, but in the spring of 1089 the senior elements of the Normandy nobility prevailed upon Robert to release him.

Although no longer formally the Count of Cotentin, Henry continued to control the west of Normandy. The struggle between Henry's brothers continued. William Rufus continued to put down resistance to his rule in England, but began to build a number of alliances against Robert with barons in Normandy and neighbouring Ponthieu. Robert allied himself with Philip I of France. In late 1090 William Rufus encouraged Conan Pilatus, a powerful burgher in Rouen, to rebel against Robert; Conan was supported by most of Rouen and made appeals to the neighbouring ducal garrisons to switch allegiance as well.

Robert issued an appeal for help to his barons, and Henry was the first to arrive in Rouen in November. Violence broke out, leading to savage, confused street fighting as both sides attempted to take control of the city. Robert and Henry left the castle to join the battle, but Robert then retreated, leaving Henry to continue the fighting. The battle turned in favour of the ducal forces and Henry took Conan prisoner. Henry was angry that Conan had turned against his feudal lord. He had him taken to the top of Rouen Castle and then, despite Conan's offers to pay a huge ransom, threw him off the top of the castle to his death. Contemporaries considered Henry to have acted appropriately in making an example of Conan, and Henry became famous for his exploits in the battle.

In the aftermath, Robert forced Henry to leave Rouen, probably because Henry's role in the fighting had been more prominent than his own, and possibly because Henry had asked to be formally reinstated as the count of the Cotentin. In early 1091, William Rufus invaded Normandy with a sufficiently large army to bring Robert to the negotiating table. The two brothers signed a treaty at Rouen, granting William Rufus a range of lands and castles in Normandy. In return, William Rufus promised to support Robert's attempts to regain control of the neighbouring county of Maine, once under Norman control, and help in regaining control over the Duchy, including Henry's lands. They nominated each other as heirs to England and Normandy, excluding Henry from any succession while either one of them lived.

War now broke out between Henry and his brothers. Henry mobilised a mercenary army in the west of Normandy, but as William Rufus and Robert's forces advanced, his network of baronial support melted away. Henry focused his remaining forces at Mont Saint-Michel, where he was besieged, probably in March 1091. The site was easy to defend, but lacked fresh water. The chronicler William of Malmesbury suggested that when Henry's water ran short, Robert allowed his brother fresh supplies, leading to remonstrations between Robert and William Rufus. The events of the final days of the siege are unclear: the besiegers had begun to argue about the future strategy for the campaign, but Henry then abandoned Mont Saint-Michel, probably as part of a negotiated surrender. He left for Brittany and crossed over into France.

Henry's next steps are not well documented; one chronicler, Orderic Vitalis, suggests that he travelled in the French Vexin, along the Normandy border, for over a year with a small band of followers. By the end of the year, Robert and William Rufus had fallen out once again, and the Treaty of Rouen had been abandoned. In 1092, Henry and his followers seized the Normandy town of Domfront. Domfront had previously been controlled by Robert of Bellême, but the inhabitants disliked his rule and invited Henry to take over the town, which he did in a bloodless coup. Over the next two years, Henry re-established his network of supporters across western Normandy, forming what Judith Green terms a "court in waiting". By 1094, he was allocating lands and castles to his followers as if he were the Duke of Normandy. William Rufus began to support Henry with money, encouraging his campaign against Robert, and Henry used some of this to construct a substantial castle at Domfront.

William Rufus crossed into Normandy to take the war to Robert in 1094, and when progress stalled, called upon Henry for assistance. Henry responded, but travelled to London instead of joining the main campaign further east in Normandy, possibly at the request of the King, who in any event abandoned the campaign and returned to England. Over the next few years, Henry appears to have strengthened his power base in western Normandy, visiting England occasionally to attend at William Rufus's court. In 1095 Pope Urban II called the First Crusade, encouraging knights from across Europe to join. Robert joined the Crusade, borrowing money from William Rufus to do so, and granting the King temporary custody of his part of the Duchy in exchange. The King appeared confident of regaining the remainder of Normandy from Robert, and Henry appeared ever closer to William Rufus, the pair campaigning together in the Norman Vexin between 1097 and 1098.

Henry became King of England following the death of William Rufus, who had been shot while hunting. On the afternoon of 2 August 1100, the King had gone hunting in the New Forest, accompanied by a team of huntsmen and a number of the Norman nobility, including Henry. An arrow was fired, possibly by the baron Walter Tirel, which hit and killed William Rufus. Numerous conspiracy theories have been put forward suggesting that the King was killed deliberately; most modern historians reject these, as hunting was a risky activity, and such accidents were common. Chaos broke out, and Tirel fled the scene for France, either because he had fired the fatal shot, or because he had been incorrectly accused and feared that he would be made a scapegoat for the King's death.

Henry rode to Winchester, where an argument ensued as to who now had the best claim to the throne. William of Breteuil championed the rights of Robert, who was still abroad, returning from the Crusade, and to whom Henry and the barons had given homage in previous years. Henry argued that, unlike Robert, he had been born to a reigning king and queen, thereby giving him a claim under the right of porphyrogeniture. Tempers flared, but Henry, supported by Henry de Beaumont and Robert of Meulan, held sway and persuaded the barons to follow him. He occupied Winchester Castle and seized the royal treasury.

Henry was hastily crowned king in Westminster Abbey on 5 August by Maurice, the Bishop of London, as Anselm, the Archbishop of Canterbury, had been exiled by William Rufus, and Thomas, the Archbishop of York, was in the north of England at Ripon. In accordance with English tradition and in a bid to legitimise his rule, Henry issued a coronation charter laying out various commitments. The new king presented himself as having restored order to a trouble-torn country. He announced that he would abandon William Rufus's policies towards the Church, which had been seen as oppressive by the clergy; he promised to prevent royal abuses of the barons' property rights, and assured a return to the gentler customs of Edward the Confessor; he asserted that he would "establish a firm peace" across England and ordered "that this peace shall henceforth be kept".

In addition to his existing circle of supporters, many of whom were richly rewarded with new lands, Henry quickly co-opted many of the existing administration into his new royal household. William Giffard, William Rufus's chancellor, was made the Bishop of Winchester, and the prominent sheriffs Urse d'Abetot, Haimo Dapifer and Robert Fitzhamon continued to play a senior role in government. By contrast, the unpopular Ranulf Flambard, the Bishop of Durham and a key member of the previous regime, was imprisoned in the Tower of London and charged with corruption. The late king had left many church positions unfilled, and Henry set about nominating candidates to these, in an effort to build further support for his new government. The appointments needed to be consecrated, and Henry wrote to Anselm, apologising for having been crowned while the Archbishop was still in France and asking him to return at once.

On 11 November 1100 Henry married Matilda, the daughter of Malcolm III of Scotland. Henry was now around 31 years old, but late marriages for noblemen were not unusual in the 11th century. The pair had probably first met earlier the previous decade, possibly being introduced through Bishop Osmund of Salisbury. Historian Warren Hollister argues that Henry and Matilda were emotionally close, but their union was also certainly politically motivated. Matilda had originally been named Edith, an Anglo-Saxon name, and was a member of the West Saxon royal family, being the niece of Edgar the Ætheling, the great-granddaughter of Edmund Ironside and a descendant of Alfred the Great. For Henry, marrying Matilda gave his reign increased legitimacy, and for Matilda, an ambitious woman, it was an opportunity for high status and power in England.

Matilda had been educated in a sequence of convents, however, and may well have taken the vows to formally become a nun, which formed an obstacle to the marriage progressing. She did not wish to be a nun and appealed to Anselm for permission to marry Henry, and the Archbishop established a council at Lambeth Palace to judge the issue. Despite some dissenting voices, the council concluded that although Matilda had lived in a convent, she had not actually become a nun and was therefore free to marry, a judgement that Anselm then affirmed, allowing the marriage to proceed. Matilda proved an effective queen for Henry, acting as a regent in England on occasion, addressing and presiding over councils, and extensively supporting the arts. The couple soon had two children, Matilda, born in 1102, and William Adelin, born in 1103; it is possible that they also had a second son, Richard, who died young. Following the birth of these children, Matilda preferred to remain based in Westminster while Henry travelled across England and Normandy, either for religious reasons or because she enjoyed being involved in the machinery of royal governance.

Henry had a considerable sexual appetite and enjoyed a substantial number of sexual partners, resulting in a large number of illegitimate children, at least nine sons and 13 daughters, many of whom he appears to have recognised and supported. It was normal for unmarried Anglo-Norman noblemen to have sexual relations with prostitutes and local women, and kings were also expected to have mistresses. Some of these relationships occurred before Henry was married, but many others took place after his marriage to Matilda. Henry had a wide range of mistresses from a range of backgrounds, and the relationships appear to have been conducted relatively openly. He may have chosen some of his noble mistresses for political purposes, but the evidence to support this theory is limited.

By early 1101, Henry's new regime was established and functioning, but many of the Anglo-Norman elite still supported Robert, or would be prepared to switch sides if Henry's elder brother appeared likely to gain power in England. In February, Flambard escaped from the Tower of London and crossed the Channel to Normandy, where he injected fresh direction and energy to Robert's attempts to mobilise an invasion force. By July, Robert had formed an army and a fleet, ready to move against Henry in England. Raising the stakes in the conflict, Henry seized Flambard's lands and, with the support of Anselm, Flambard was removed from his position as bishop. Henry held court in April and June, where the nobility renewed their oaths of allegiance to him, but their support still appeared partial and shaky.

With the invasion imminent, Henry mobilised his forces and fleet outside Pevensey, close to Robert's anticipated landing site, training some of them personally in how to counter cavalry charges. Despite English levies and knights owing military service to the Church arriving in considerable numbers, many of his barons did not appear. Anselm intervened with some of the doubters, emphasising the religious importance of their loyalty to Henry. Robert unexpectedly landed further up the coast at Portsmouth on 20 July with a modest force of a few hundred men, but these were quickly joined by many of the barons in England. However, instead of marching into nearby Winchester and seizing Henry's treasury, Robert paused, giving Henry time to march west and intercept the invasion force.

The two armies met at Alton where peace negotiations began, possibly initiated by either Henry or Robert, and probably supported by Flambard. The brothers then agreed to the Treaty of Alton, under which Robert released Henry from his oath of homage and recognised him as king; Henry renounced his claims on western Normandy, except for Domfront, and agreed to pay Robert £2,000 a year for life; if either brother died without a male heir, the other would inherit his lands; the barons whose lands had been seized by either the King or the Duke for supporting his rival would have them returned, and Flambard would be reinstated as bishop; the two brothers would campaign together to defend their territories in Normandy. Robert remained in England for a few months more with Henry before returning to Normandy.

Despite the treaty, Henry set about inflicting severe penalties on the barons who had stood against him during the invasion. William de Warenne, the Earl of Surrey, was accused of fresh crimes, which were not covered by the Alton amnesty, and was banished from England. In 1102 Henry then turned against Robert of Bellême and his brothers, the most powerful of the barons, accusing him of 45 different offences. Robert escaped and took up arms against Henry. Henry besieged Robert's castles at Arundel, Tickhill and Shrewsbury, pushing down into the south-west to attack Bridgnorth. His power base in England broken, Robert accepted Henry's offer of banishment and left the country for Normandy.

Henry's network of allies in Normandy became stronger during 1103. Henry married Juliana, one of his illegitimate daughters, to Eustace of Breteuil, and another illegitimate daughter, Matilda, to Rotrou, the Count of Perche, on the Normandy border. Henry attempted to win over other members of the Normandy nobility and gave other English estates and lucrative offers to key Norman lords. Duke Robert continued to fight Robert of Bellême, but the Duke's position worsened, until by 1104, he had to ally himself formally with Bellême to survive. Arguing that Duke Robert had broken the terms of their treaty, Henry crossed over the Channel to Domfront, where he met with senior barons from across Normandy, eager to ally themselves with the King. Henry confronted his brother and accused him of siding with his enemies, before returning to England.

Normandy continued to disintegrate into chaos. In 1105, Henry sent his friend Robert Fitzhamon and a force of knights into the Duchy, apparently to provoke a confrontation with Duke Robert. Fitzhamon was captured, and Henry used this as an excuse to invade, promising to restore peace and order. Henry had the support of most of the neighbouring counts around Normandy's borders, and King Philip of France was persuaded to remain neutral. Henry occupied western Normandy, and advanced east on Bayeux, where Fitzhamon was held. The city refused to surrender, and Henry besieged it, burning it to the ground. Terrified of meeting the same fate, the town of Caen switched sides and surrendered, allowing Henry to advance on Falaise, which he took with some casualties. Henry's campaign stalled, and the King instead began peace discussions with Robert. The negotiations were inconclusive and the fighting dragged on until Christmas, when Henry returned to England.

Henry invaded again in July 1106, hoping to provoke a decisive battle. After some initial tactical successes, he turned south-west towards the castle of Tinchebray. He besieged the castle and Duke Robert, supported by Robert of Bellême, advanced from Falaise to relieve it. After attempts at negotiation failed, the Battle of Tinchebray took place, probably on 28 September. The battle lasted around an hour, and began with a charge by Duke Robert's cavalry; the infantry and dismounted knights of both sides then joined the battle. Henry's reserves, led by Elias, the Count of Maine and Alan, the Duke of Brittany, attacked the enemy's flanks, routing first Bellême's troops and then the bulk of the ducal forces. Duke Robert was taken prisoner, but Bellême escaped.

Henry mopped up the remaining resistance in Normandy, and Robert ordered his last garrisons to surrender. Reaching Rouen, Henry reaffirmed the laws and customs of Normandy and took homage from the leading barons and citizens. The lesser prisoners taken at Tinchebray were released, but Robert and several other leading nobles were imprisoned indefinitely. Henry's nephew, Robert's son William Clito, was only three years old and was released to the care of Helias of Saint-Saens, a Norman baron. Henry reconciled himself with Robert of Bellême, who gave up the ducal lands he had seized and rejoined the royal court. Henry had no way of legally removing the Duchy from his brother Robert, and initially Henry avoided using the title "duke" at all, emphasising that, as the King of England, he was only acting as the guardian of the troubled Duchy.

Henry inherited the kingdom of England from William Rufus, giving him a claim of suzerainty over Wales and Scotland, and acquired the Duchy of Normandy, a complex entity with troubled borders. The borders between England and Scotland were still uncertain during Henry's reign, with Anglo-Norman influence pushing northwards through Cumbria, but Henry's relationship with King David I of Scotland was generally good, partially due to Henry's marriage to his sister. In Wales, Henry used his power to coerce and charm the indigenous Welsh princes, while Norman Marcher Lords pushed across the valleys of South Wales. Normandy was controlled via various interlocking networks of ducal, ecclesiastical and family contacts, backed by a growing string of important ducal castles along the borders. Alliances and relationships with neighbouring counties along the Norman border were particularly important to maintaining the stability of the Duchy.

Henry ruled through the various barons and lords in England and Normandy, whom he manipulated skilfully for political effect. Political friendships, termed "amicitia" in Latin, were important during the 12th century, and Henry maintained a wide range of these, mediating between his friends in various factions across his realm when necessary, and rewarding those who were loyal to him. Henry also had a reputation for punishing those barons who stood against him, and he maintained an effective network of informers and spies who reported to him on events. Henry was a harsh, firm ruler, but not excessively so by the standards of the day. Over time, he increased the degree of his control over the barons, removing his enemies and bolstering his friends until the "reconstructed baronage", as historian Warren Hollister describes it, was predominantly loyal and dependent on the King.

Henry's itinerant royal court comprised various parts. At the heart was Henry's domestic household, called the "domus"; a wider grouping was termed the "familia regis", and formal gatherings of the court were termed "curia". The "domus" was divided into several parts. The chapel, headed by the chancellor, looked after the royal documents, the chamber dealt with financial affairs and the master-marshal was responsible for travel and accommodation. The "familia regis" included Henry's mounted household troops, up to several hundred strong, who came from a wider range of social backgrounds, and could be deployed across England and Normandy as required. Initially Henry continued his father's practice of regular crown-wearing ceremonies at his "curia", but they became less frequent as the years passed. Henry's court was grand and ostentatious, financing the construction of large new buildings and castles with a range of precious gifts on display, including the King's private menagerie of exotic animals, which he kept at Woodstock Palace. Despite being a lively community, Henry's court was more tightly controlled than those of previous kings. Strict rules controlled personal behaviour and prohibited members of the court from pillaging neighbouring villages, as had been the norm under William Rufus.

Henry was responsible for a substantial expansion of the royal justice system. In England, Henry drew on the existing Anglo-Saxon system of justice, local government and taxes, but strengthened it with additional central governmental institutions. Roger of Salisbury began to develop the royal exchequer after 1110, using it to collect and audit revenues from the King's sheriffs in the shires. Itinerant justices began to emerge under Henry, travelling around the country managing eyre courts, and many more laws were formally recorded. Henry gathered increasing revenue from the expansion of royal justice, both from fines and from fees. The first Pipe Roll that is known to have survived dates from 1130, recording royal expenditures. Henry reformed the coinage in 1107, 1108 and in 1125, inflicting harsh corporal punishments to English coiners who had been found guilty of debasing the currency. In Normandy, Henry restored law and order after 1106, operating through a body of Norman justices and an exchequer system similar to that in England. Norman institutions grew in scale and scope under Henry, although less quickly than in England. Many of the officials that ran Henry's system were termed "new men", relatively low-born individuals who rose through the ranks as administrators, managing justice or the royal revenues.

Henry's ability to govern was intimately bound up with the Church, which formed the key to the administration of both England and Normandy, and this relationship changed considerably over the course of his reign. William the Conqueror had reformed the English Church with the support of his Archbishop of Canterbury, Lanfranc, who became a close colleague and advisor to the King. Under William Rufus this arrangement had collapsed, the King and Archbishop Anselm had become estranged and Anselm had gone into exile. Henry also believed in Church reform, but on taking power in England he became embroiled in the investiture controversy.

The argument concerned who should invest a new bishop with his staff and ring: traditionally, this had been carried out by the king in a symbolic demonstration of royal power, but Pope Urban II had condemned this practice in 1099, arguing that only the papacy could carry out this task, and declaring that the clergy should not give homage to their local temporal rulers. Anselm returned to England from exile in 1100 having heard Urban's pronouncement, and informed Henry that he would be complying with the Pope's wishes. Henry was in a difficult position. On one hand, the symbolism and homage was important to him; on the other hand, he needed Anselm's support in his struggle with his brother Duke Robert.

Anselm stuck firmly to the letter of the papal decree, despite Henry's attempts to persuade him to give way in return for a vague assurance of a future royal compromise. Matters escalated, with Anselm going back into exile and Henry confiscating the revenues of his estates. Anselm threatened excommunication, and in July 1105 the two men finally negotiated a solution. A distinction was drawn between the secular and ecclesiastical powers of the prelates, under which Henry gave up his right to invest his clergy, but retained the custom of requiring them to come and do homage for the temporalities, the landed properties they held in England. Despite this argument, the pair worked closely together, combining to deal with Duke Robert's invasion of 1101, for example, and holding major reforming councils in 1102 and 1108.

A long-running dispute between the Archbishops of Canterbury and York flared up under Anselm's successor, Ralph d'Escures. Canterbury, traditionally the senior of the two establishments, had long argued that the Archbishop of York should formally promise to obey their Archbishop, but York argued that the two episcopates were independent within the English Church and that no such promise was necessary. Henry supported the primacy of Canterbury, to ensure that England remained under a single ecclesiastical administration, but the Pope preferred the case of York. The matter was complicated by Henry's personal friendship with Thurstan, the Archbishop of York, and the King's desire that the case should not end up in a papal court, beyond royal control. Henry badly needed the support of the Papacy in his struggle with Louis of France, however, and therefore allowed Thurstan to attend the Council of Rheims in 1119, where Thurstan was then consecrated by the Pope with no mention of any duty towards Canterbury. Henry believed that this went against assurances Thurstan had previously made and exiled him from England until the King and Archbishop came to a negotiated solution the following year.

Even after the investiture dispute, the King continued to play a major role in the selection of new English and Norman bishops and archbishops. Henry appointed many of his officials to bishoprics and, as historian Martin Brett suggests, "some of his officers could look forward to a mitre with all but absolute confidence". Henry's chancellors, and those of his queens, became bishops of Durham, Hereford, London, Lincoln, Winchester and Salisbury. Henry increasingly drew on a wider range of these bishops as advisors – particularly Roger of Salisbury – breaking with the earlier tradition of relying primarily on the Archbishop of Canterbury. The result was a cohesive body of administrators through which Henry could exercise careful influence, holding general councils to discuss key matters of policy. This stability shifted slightly after 1125, when Henry began to inject a wider range of candidates into the senior positions of the Church, often with more reformist views, and the impact of this generation would be felt in the years after Henry's death.

Like other rulers of the period, Henry donated to the Church and patronised various religious communities, but contemporary chroniclers did not consider him an unusually pious king. His personal beliefs and piety may, however, have developed during the course of his life. Henry had always taken an interest in religion, but in his later years he may have become much more concerned about spiritual affairs. If so, the major shifts in his thinking would appear to have occurred after 1120, when his son William Adelin died, and 1129, when his daughter's marriage teetered on the verge of collapse.

As a proponent of religious reform, Henry gave extensively to reformist groups within the Church. He was a keen supporter of the Cluniac order, probably for intellectual reasons. He donated money to the abbey at Cluny itself, and after 1120 gave generously to Reading Abbey, a Cluniac establishment. Construction on Reading began in 1121, and Henry endowed it with rich lands and extensive privileges, making it a symbol of his dynastic lines. He also focused effort on promoting the conversion of communities of clerks into Augustinian canons, the foundation of leper hospitals, expanding the provision of nunneries, and the charismatic orders of the Savigniacs and Tironensians. He was an avid collector of relics, sending an embassy to Constantinople in 1118 to collect Byzantine items, some of which were donated to Reading Abbey.

Normandy faced an increased threat from France, Anjou and Flanders after 1108. Louis VI succeeded to the French throne in 1108 and began to reassert central royal power. Louis demanded Henry give homage to him and that two disputed castles along the Normandy border be placed into the control of neutral castellans. Henry refused, and Louis responded by mobilising an army. After some arguments, the two kings negotiated a truce and retreated without fighting, leaving the underlying issues unresolved. Fulk V assumed power in Anjou in 1109 and began to rebuild Angevin authority. Fulk also inherited the county of Maine, but refused to recognise Henry as his feudal lord and instead allied himself with Louis. Robert II of Flanders also briefly joined the alliance, before his death in 1111.

In 1108, Henry betrothed his six-year-old daughter, Matilda, to Henry V, the future Holy Roman Emperor. For King Henry, this was a prestigious match; for Henry V, it was an opportunity to restore his financial situation and fund an expedition to Italy, as he received a dowry of £6,666 from England and Normandy. Raising this money proved challenging, and required the implementation of a special "aid", or tax, in England. Matilda was crowned Henry V's queen in 1110.

Henry responded to the French and Angevin threat by expanding his own network of supporters beyond the Norman borders. Some Norman barons deemed unreliable were arrested or dispossessed, and Henry used their forfeited estates to bribe his potential allies in the neighbouring territories, in particular Maine. Around 1110, Henry attempted to arrest the young William Clito, but William's mentors moved him to the safety of Flanders before he could be taken. At about this time, Henry probably began to style himself as the Duke of Normandy. Robert of Bellême turned against Henry once again, and when he appeared at Henry's court in 1112 in a new role as a French ambassador, he was arrested and imprisoned.

Rebellions broke out in France and Anjou between 1111 and 1113, and Henry crossed into Normandy to support his nephew, Count Theobald of Blois, who had sided against Louis in the uprising. In a bid to diplomatically isolate the French King, Henry betrothed his young son, William Adelin, to Fulk's daughter Matilda, and married his illegitimate daughter Matilda to Conan III, the Duke of Brittany, creating alliances with Anjou and Brittany respectively. Louis backed down and in March 1113 met with Henry near Gisors to agree a peace settlement, giving Henry the disputed fortresses and confirming Henry's overlordship of Maine, Bellême and Brittany.

Meanwhile, the situation in Wales was deteriorating. Henry had conducted a campaign in South Wales in 1108, pushing out royal power in the region and colonising the area around Pembroke with Flemings. By 1114, some of the resident Norman lords were under attack, while in Mid-Wales, Owain ap Cadwgan blinded one of the political hostages he was holding, and in North Wales Gruffudd ap Cynan threatened the power of the Earl of Chester. Henry sent three armies into Wales that year, with Gilbert Fitz Richard leading a force from the south, Alexander, King of Scotland, pressing from the north and Henry himself advancing into Mid-Wales. Owain and Gruffudd sued for peace, and Henry accepted a political compromise. Henry reinforced the Welsh Marches with his own appointees, strengthening the border territories.

Concerned about the succession, Henry sought to persuade Louis VI to accept his son, William Adelin, as the legitimate future Duke of Normandy, in exchange for his son's homage. Henry crossed into Normandy in 1115 and assembled the Norman barons to swear loyalty; he also almost successfully negotiated a settlement with King Louis, affirming William's right to the Duchy in exchange for a large sum of money, but the deal fell through and Louis, backed by his ally Baldwin of Flanders, instead declared that he considered William Clito the legitimate heir to the Duchy.

War broke out after Henry returned to Normandy with an army to support Theobald of Blois, who was under attack from Louis. Henry and Louis raided each other's towns along the border, and a wider conflict then broke out, probably in 1116. Henry was pushed onto the defensive as French, Flemish and Angevin forces began to pillage the Normandy countryside. Amaury III of Montfort and many other barons rose up against Henry, and there was an assassination plot from within his own household. Henry's wife, Matilda, died in early 1118, but the situation in Normandy was sufficiently pressing that Henry was unable to return to England for her funeral.

Henry responded by mounting campaigns against the rebel barons and deepening his alliance with Theobald. Baldwin of Flanders was wounded in battle and died in September 1118, easing the pressure on Normandy from the north-east. Henry attempted to crush a revolt in the city of Alençon, but was defeated by Fulk and the Angevin army. Forced to retreat from Alençon, Henry's position deteriorated alarmingly, as his resources became overstretched and more barons abandoned his cause. Early in 1119, Eustace of Breteuil and Henry's daughter, Juliana, threatened to join the baronial revolt. Hostages were exchanged in a bid to avoid conflict, but relations broke down and both sides mutilated their captives. Henry attacked and took the town of Breteuil, despite Juliana's attempt to kill her father with a crossbow. In the aftermath, Henry dispossessed the couple of almost all of their lands in Normandy.

Henry's situation improved in May 1119 when he enticed Fulk to switch sides by finally agreeing to marry William Adelin to Fulk's daughter, Matilda, and paying Fulk a large sum of money. Fulk left for the Levant, leaving the County of Maine in Henry's care, and the King was free to focus on crushing his remaining enemies. During the summer Henry advanced into the Norman Vexin, where he encountered Louis's army, resulting in the Battle of Brémule. Henry appears to have deployed scouts and then organised his troops into several carefully formed lines of dismounted knights. Unlike Henry's forces, the French knights remained mounted; they hastily charged the Anglo-Norman positions, breaking through the first rank of the defences but then becoming entangled in Henry's second line of knights. Surrounded, the French army began to collapse. In the melee, Henry was hit by a sword blow, but his armour protected him. Louis and William Clito escaped from the battle, leaving Henry to return to Rouen in triumph.

The war slowly petered out after this battle, and Louis took the dispute over Normandy to Pope Callixtus II's council in Reims that October. Henry faced a number of French complaints concerning his acquisition and subsequent management of Normandy, and despite being defended by Geoffrey, the Archbishop of Rouen, Henry's case was shouted down by the pro-French elements of the council. Callixtus declined to support Louis, however, and merely advised the two rulers to seek peace. Amaury de Montfort came to terms with Henry, but Henry and William Clito failed to find a mutually satisfactory compromise. In June 1120, Henry and Louis formally made peace on terms advantageous to the King of England: William Adelin gave homage to Louis, and in return Louis confirmed William's rights to the Duchy.

Henry's succession plans were thrown into chaos by the sinking of the "White Ship" on 25 November 1120. Henry had left the port of Barfleur for England in the early evening, leaving William Adelin and many of the younger members of the court to follow on that night in a separate vessel, the "White Ship". Both the crew and passengers were drunk and, just outside the harbour, the ship hit a submerged rock. The ship sank, killing as many as 300 people, with only one survivor, a butcher from Rouen. Henry's court was initially too scared to report William's death to the King. When he was finally told, he collapsed with grief.

The disaster left Henry with no legitimate son, his various nephews now the closest male heirs. Henry announced he would take a new wife, Adeliza of Louvain, opening up the prospect of a new royal son, and the two were married at Windsor Castle in January 1121. Henry appears to have chosen her because she was attractive and came from a prestigious noble line. Adela seems to have been fond of Henry and joined him in his travels, probably to maximise the chances of her conceiving a child. The "White Ship" disaster initiated fresh conflict in Wales, where the drowning of Richard, Earl of Chester, encouraged a rebellion led by Maredudd ap Bleddyn. Henry intervened in North Wales that summer with an army and, although the King was hit by a Welsh arrow, the campaign reaffirmed royal power across the region.

With William dead, Henry's alliance with Anjou – which had been based on his son marrying Fulk's daughter – began to disintegrate. Fulk returned from the Levant and demanded that Henry return Matilda and her dowry, a range of estates and fortifications in Maine. Matilda left for Anjou, but Henry argued that the dowry had in fact originally belonged to him before it came into the possession of Fulk, and so declined to hand the estates back to Anjou. Fulk married his daughter Sibylla to William Clito, and granted them Maine. Once again, conflict broke out, as Amaury de Montfort allied himself with Fulk and led a revolt along the Norman-Anjou border in 1123. Amaury was joined by several other Norman barons, headed by Waleran de Beaumont, one of the sons of Henry's old ally, Robert of Meulan.

Henry dispatched Robert of Gloucester and Ranulf le Meschin to Normandy and then intervened himself in late 1123. Henry began the process of besieging the rebel castles, before wintering in the Duchy. In the spring, campaigning began again. Ranulf received intelligence that the rebels were returning to one of their bases at Vatteville, allowing him to ambush them en route at Rougemontiers; Waleran charged the royal forces, but his knights were cut down by Ranulf's archers and the rebels were quickly overwhelmed. Waleran was captured, but Amaury escaped. Henry mopped up the remainder of the rebellion, blinding some of the rebel leaders – considered, at the time, a more merciful punishment than execution – and recovering the last rebel castles. Henry paid Pope Callixtus a large amount of money, in exchange for the Papacy annulling the marriage of William Clito and Sibylla on the grounds of consanguinity.

Henry and his new wife did not conceive any children, generating prurient speculation as to the possible explanation, and the future of the dynasty appeared at risk. Henry may have begun to look among his nephews for a possible heir. He may have considered Stephen of Blois as a possible option and, perhaps in preparation for this, he arranged a beneficial marriage for Stephen to a wealthy heiress, Matilda. Theobald of Blois, his close ally, may have also felt that he was in favour with Henry. William Clito, who was King Louis's preferred choice, remained opposed to Henry and was therefore unsuitable. Henry may have also considered his own illegitimate son, Robert of Gloucester, as a possible candidate, but English tradition and custom would have looked unfavourably on this.

Henry's plans shifted when the Empress Matilda's husband, the Emperor Henry, died in 1125. King Henry recalled his daughter to England the next year and declared that, should he die without a male heir, she was to be his rightful successor. The Anglo-Norman barons were gathered together at Westminster on Christmas 1126, where they swore to recognise Matilda and any future legitimate heir she might have. Putting forward a woman as a potential heir in this way was unusual: opposition to Matilda continued to exist within the English court, and Louis was vehemently opposed to her candidacy.

Fresh conflict broke out in 1127, when Charles, the childless Count of Flanders, was murdered, creating a local succession crisis. Backed by King Louis, William Clito was chosen by the Flemings to become their new ruler. This development potentially threatened Normandy, and Henry began to finance a proxy war in Flanders, promoting the claims of William's Flemish rivals. In an effort to disrupt the French alliance with William, Henry mounted an attack into France in 1128, forcing Louis to cut his aid to William. William died unexpectedly in July, removing the last major challenger to Henry's rule and bringing the war in Flanders to a halt. Without William, the baronial opposition in Normandy lacked a leader. A fresh peace was made with France, and the King was finally able to release the remaining prisoners from the revolt of 1123, including Waleran of Meulan, who was rehabilitated into the royal court.

Meanwhile, Henry rebuilt his alliance with Fulk of Anjou, this time by marrying Matilda to Fulk's eldest son, Geoffrey. The pair were betrothed in 1127 and married the following year. It is unknown whether Henry intended Geoffrey to have any future claim on England or Normandy, and he was probably keeping his son-in-law's status deliberately uncertain. Similarly, although Matilda was granted a number of Normandy castles as part of her dowry, it was not specified when the couple would actually take possession of them. Fulk left Anjou for Jerusalem in 1129, declaring Geoffrey the Count of Anjou and Maine. The marriage proved difficult, as the couple did not particularly like each other and the disputed castles proved a point of contention, resulting in Matilda returning to Normandy later that year. Henry appears to have blamed Geoffrey for the separation, but in 1131 the couple were reconciled. Much to the pleasure and relief of Henry, Matilda then gave birth to a sequence of two sons, Henry and Geoffrey, in 1133 and 1134.

Relations between Henry, Matilda, and Geoffrey became increasingly strained during the King's final years. Matilda and Geoffrey suspected that they lacked genuine support in England. In 1135 they urged Henry to hand over the royal castles in Normandy to Matilda whilst he was still alive, and insisted that the Norman nobility swear immediate allegiance to her, thereby giving the couple a more powerful position after Henry's death. Henry angrily declined to do so, probably out of concern that Geoffrey would try to seize power in Normandy. A fresh rebellion broke out amongst the barons in southern Normandy, led by William, the Count of Ponthieu, whereupon Geoffrey and Matilda intervened in support of the rebels.

Henry campaigned throughout the autumn, strengthening the southern frontier, and then travelled to Lyons-la-Forêt in November to enjoy some hunting, still apparently healthy. There Henry fell ill – according to the chronicler Henry of Huntingdon, he ate too many ("a surfeit of") lampreys against his physician's advice – and his condition worsened over the course of a week. Once the condition appeared terminal, Henry gave confession and summoned Archbishop Hugh of Amiens, who was joined by Robert of Gloucester and other members of the court. In accordance with custom, preparations were made to settle Henry's outstanding debts and to revoke outstanding sentences of forfeiture. The King died on 1 December 1135, and his corpse was taken to Rouen accompanied by the barons, where it was embalmed; his entrails were buried locally at the priory of Notre-Dame du Pré, and the preserved body was taken on to England, where it was interred at Reading Abbey.

Despite Henry's efforts, the succession was disputed. When news began to spread of the King's death, Geoffrey and Matilda were in Anjou supporting the rebels in their campaign against the royal army, which included a number of Matilda's supporters such as Robert of Gloucester. Many of these barons had taken an oath to stay in Normandy until the late king was properly buried, which prevented them from returning to England. The Norman nobility discussed declaring Theobald of Blois king. Theobald's younger brother, Stephen of Blois, quickly crossed from Boulogne to England, however, accompanied by his military household. With the help of his brother, Henry of Blois, he seized power in England and was crowned king on 22 December. The Empress Matilda did not give up her claim to England and Normandy, leading to the prolonged civil war known as the Anarchy between 1135 and 1153.

Historians have drawn on a range of sources on Henry, including the accounts of chroniclers; other documentary evidence, including early pipe rolls; and surviving buildings and architecture. The three main chroniclers to describe the events of Henry's life were William of Malmesbury, Orderic Vitalis, and Henry of Huntingdon, but each incorporated extensive social and moral commentary into their accounts and borrowed a range of literary devices and stereotypical events from other popular works. Other chroniclers include Eadmer, Hugh the Chanter, Abbot Suger, and the authors of the Welsh "Brut". Not all royal documents from the period have survived, but there are a number of royal acts, charters, writs, and letters, along with some early financial records. Some of these have since been discovered to be forgeries, and others had been subsequently amended or tampered with.

Late medieval historians seized on the accounts of selected chroniclers regarding Henry's education and gave him the title of Henry "Beauclerc", a theme echoed in the analysis of Victorian and Edwardian historians such as Francis Palgrave and Henry Davis. The historian Charles David dismissed this argument in 1929, showing the more extreme claims for Henry's education to be without foundation. Modern histories of Henry commenced with Richard Southern's work in the early 1960s, followed by extensive research during the rest of the 20th century into a wide number of themes from his reign in England, and a much more limited number of studies of his rule in Normandy. Only two major, modern biographies of Henry have been produced, Warren Hollister's posthumous volume in 2001, and Judith Green's 2006 work.

Interpretation of Henry's personality by historians has altered over time. Earlier historians such as Austin Poole and Richard Southern considered Henry as a cruel, draconian ruler. More recent historians, such as Hollister and Green, view his implementation of justice much more sympathetically, particularly when set against the standards of the day, but even Green has noted that Henry was "in many respects highly unpleasant", and Alan Cooper has observed that many contemporary chroniclers were probably too scared of the King to voice much criticism. Historians have also debated the extent to which Henry's administrative reforms genuinely constituted an introduction of what Hollister and John Baldwin have termed systematic, "administrative kingship", or whether his outlook remained fundamentally traditional.

Henry's burial at Reading Abbey is marked by a local cross, but Reading Abbey was slowly demolished during the Dissolution of the Monasteries in the 16th century. The exact location is uncertain, but the most likely location of the tomb itself is now in a built-up area of central Reading, on the site of the former abbey choir. A plan to locate his remains was announced in March 2015, with support from English Heritage and Philippa Langley, who aided with the successful exhumation of Richard III.

In addition to Matilda and William, Henry possibly had a short-lived son, Richard, from his first marriage. Henry and his second wife, Adeliza, had no children.

Henry had a number of illegitimate children by various mistresses.





</doc>
<doc id="14183" url="https://en.wikipedia.org/wiki?curid=14183" title="Hentai">
Hentai

Outside of Japan, ("" ; lit. "pervert") is anime and manga pornography. In the Japanese language, however, "hentai" is not a genre of media but any type of perverse or bizarre sexual desire or act. For example, outside of Japan a work depicting lesbian sex might be described as "yuri hentai", but in Japan it would just be described as "yuri". 

The word is short for , a perverse sexual desire. The original meaning of "hentai" in the Japanese language is a transformation or metamorphosis. The implication of perversion or paraphilia was derived from there. Both meanings can be distinguished in context easily.

 "" is a kanji compound of 変 ("hen"; "change", "weird", or "strange") and 態 ("tai"; "appearance" or "condition"). It also means "perversion" or "abnormality", especially when used as an adjective. It is the shortened form of the phrase which means "sexual perversion". The character "hen" is catch-all for queerness as a peculiarity—it does not carry an explicit sexual reference. While the term has expanded in use to cover a range of publications including homosexual publications, it remains primarily a heterosexual term, as terms indicating homosexuality entered Japan as foreign words. Japanese pornographic works are often simply tagged as , meaning "prohibited to those not yet 18 years old", and . Less official terms also in use include , , and the English initialism AV (for "adult video"). Usage of the term hentai does not define a genre in Japan.

Hentai is defined differently in English. The "Oxford Dictionary Online" defines hentai as "a subgenre of the Japanese genres of manga and anime, characterized by overtly sexualized characters and sexually explicit images and plots." The origin of the word in English is unknown, but AnimeNation's John Oppliger points to the early 1990s, when a "Dirty Pair" erotic "doujinshi" (self-published work) titled "H-Bomb" was released, and when many websites sold access to images culled from Japanese erotic visual novels and games. The earliest English use of the term traces back to the rec.arts.anime boards; with a 1990 post concerning Happosai of "Ranma ½" and the first discussion of the meaning in 1991. A 1995 Glossary on the rec.arts.anime boards contained reference to the Japanese usage and the evolving definition of hentai as "pervert" or "perverted sex". "The Anime Movie Guide", published in 1997, defines as the initial sound of hentai (i.e., the name of the letter "H", as pronounced in Japanese); it included that ecchi was "milder than hentai". A year later it was defined as a genre in "Good Vibrations Guide to Sex". At the beginning of 2000, "hentai" was listed as the 41st most popular search term of the internet, while "anime" ranked 99th. The attribution has been applied retroactively to works such as "Urotsukidōji", "La Blue Girl", and "Cool Devices". "Urotsukidōji" had previously been described with terms such as "Japornimation", and "erotic grotesque", prior to being identified as hentai.

The history of word "hentai" has its origins in science and psychology. By the middle of the Meiji era, the term appeared in publications to describe unusual or abnormal traits, including paranormal abilities and psychological disorders. A translation of German sexologist Richard von Krafft-Ebing's text "Psychopathia Sexualis" originated the concept of "hentai seiyoku", as a "perverse or abnormal sexual desire". Though it was popularized outside psychology, as in the case of Mori Ōgai's 1909 novel "Vita Sexualis". Continued interest in "hentai seiyoku", resulted in numerous journals and publications on sexual advice which circulated in the public, served to establish the sexual connotation of 'hentai' as perverse. Any perverse or abnormal act could be hentai, such as committing "shinjū" (love suicide). It was Nakamura Kokyo's journal "Abnormal Psychology" which started the popular sexology boom in Japan which would see the rise of other popular journals like "Sexuality and Human Nature", "Sex Research" and "Sex". Originally, Tanaka Kogai wrote articles for "Abnormal Psychology", but it would be Tanaka's own journal "Modern Sexuality" which would become one of the most popular sources of scholarly information about erotic and neurotic expression. "Modern Sexuality" was created to promote fetishism, S&M, and necrophilia as a facet of modern life. The ero-guro movement and depiction of perverse, abnormal and often erotic undertones were a response to interest in "hentai seiyoku".

Following the end of World War II, Japan took a new interest in sexualization and public sexuality. Mark McLelland puts forth the observation that the term "hentai" found itself shortened to "H" and that the English pronunciation was "etchi", referring to lewdness and which did not carry the stronger connotation of abnormality or perversion. By the 1950s, the "hentai seiyoku" publications became their own genre and included fetish and homosexual topics. By the 1960s, the homosexual content was dropped in favor of subjects like sadomasochism and stories of lesbianism targeted to male readers. The late 1960s brought a sexual revolution which expanded and solidified the normalizing the terms identity in Japan that continues to exist today through publications such as Bessatsu Takarajima's "Hentai-san ga iku" series.

With the usage of hentai as any erotic depiction, the history of these depictions is split into their media. Japanese artwork and comics serve as the first example of hentai material, coming to represent the iconic style after the publication of Azuma Hideo's "Cybele" in 1979. Japanese animation (anime) had its first hentai, in both definitions, with the 1984 release of Wonderkid's "Lolita Anime", overlooking the erotic and sexual depictions in 1969's "One Thousand and One Arabian Nights" and the bare-breasted Cleopatra in 1970's "Cleopatra" film. Erotic games, another area of contention, has the iconic art style first depicted in sexual acts in 1985's "Tenshitachi no Gogo". The history of each medium itself, complicated based on the broad definition and usage.

Depictions of sex and abnormal sex can be traced back through the ages, predating the term "hentai". , a Japanese term for erotic art, is thought to have and existed in some form since Heian period. From the 16th to the 19th century, shunga works were suppressed by "shōguns". A well-known example is "The Dream of the Fisherman's Wife" which depicts a woman being pleasured by two octopuses. Shunga production fell with the rise of pornographic photographs in the late 19th century.

To define erotic manga, a definition for manga is needed. While the "Hokusai Manga" uses the term "manga" in its title, it does not depict the story-telling aspect common to modern manga, as the images are unrelated. Due to the influence of pornographic photographs in the 19th and 20th centuries, the manga artwork was depicted by realistic characters. However, Osamu Tezuka has helped define the modern look and form of manga, and was later proclaimed as the "God of Manga". His debut work "New Treasure Island" was released in 1947 as a comic book through Ikuei Publishing and sold over 400,000 copies, though it was the popularity of Tezuka's "Astro Boy", "Metropolis", and "Jungle Emperor" manga that would come to define the media. This story-driven manga style is distinctly unique from comic strips like "Sazae-san", and story-driven works are now dominating shōjo and shōnen magazines.

Mature themes in manga have existed since the 1940s, but these depictions were more realistic than the cartoon-cute characters popularized by Tezuka. Early well-known "ero-gekiga" releases were "Ero Mangatropa" (1973), "Erogenica" (1975), and "Alice" (1977). The distinct shift in the style of Japanese pornographic comics from realistic to cartoon-cute characters is accredited to Azuma Hideo, "The Father of Lolicon". In 1979, he penned "Cybele", which offered the first commentary on unrealistic depictions of sexual acts between Tezuka-style characters. This would start a pornographic manga movement. The lolicon boom of the 1980s saw the rise of magazines such as the anthologies "Lemon People" and "Petit Apple Pie".

The publication of erotic materials in America can be traced back to at least 1990, when IANVS Publications printed its first "Anime Shower Special". In March 1994, Antarctic Press released "Bondage Fairies", an English translation of "Insect Hunter".

Because there are fewer animation productions, most erotic works are retroactively tagged as hentai since the coining of the term in English. Hentai is typically defined as consisting of excessive nudity, and graphic sexual intercourse whether or not it is perverse. The term "ecchi" is typically related to fanservice, with no sexual intercourse being depicted.

Two early works escape being defined as hentai, but contain erotic themes. This is likely due to the obscurity and unfamiliarity of the works, arriving in America and fading from public focus a full twenty years before importation and surging interests coined the Americanized term of hentai. The first is the 1969 film "One Thousand and One Arabian Nights" which faithfully includes erotic elements of the original story. In 1970, "", was the first animated film to carry an X rating, but it was mislabeled as erotica in America.

The term typically identifies the "Lolita Anime" series as the first erotic anime and original video animation (OVA); it was released in 1984 by Wonder Kids. Containing eight episodes, the series focused on underage sex and rape and included one episode containing BDSM bondage. Several sub-series were released in response, including a second "Lolita Anime" series released by Nikkatsu. It has not been officially licensed or distributed outside of its original release.

The "Cream Lemon" franchise of works ran from 1984 to 2005, with a number of them entering the American market in various forms. "The Brothers Grime" series released by Excalibur Films contained "Cream Lemon" works as early as 1986. However, they were not billed as anime and were introduced during the same time that the first underground distribution of erotic works began.

The American release of licensed erotic anime was first attempted in 1991 by Central Park Media, with "I Give My All", but it never occurred. In December 1992, "Devil Hunter Yohko" was the first risque (ecchi) title that was released by A.D. Vision. While it contains no sexual intercourse it pushes the limits of the ecchi category with sexual dialogue, nudity and one scene in which the heroine is about to be raped.

It was Central Park Media's 1993 release of "Urotsukidoji" which brought the first hentai film to American viewers. Often cited for creating the hentai and tentacle rape genres, it contains extreme depictions of violence and monster sex. It is notable for being the first depiction of tentacle sex on screen. When the movie premiered in America it was described as being "drenched in graphic scenes of perverse sex and ultra-violence".

Following this release, a wealth of pornographic content began to arrive in America, with companies such as A.D. Vision, Central Park Media and Media Blasters releasing licensed titles under various labels. A.D. Vision's label SoftCel Pictures released 19 titles in 1995 alone. Another label, Critical Mass, was created in 1996 to release an unedited edition of "Violence Jack". When A.D. Vision's hentai label SoftCel Pictures shut down in 2005, most of its titles were acquired by Critical Mass. Following the bankruptcy of Central Park Media in 2009, the licenses for all Anime 18-related products and movies were transferred to Critical Mass.

The term eroge (erotic game) literally defines any erotic game, but has become synonymous with video games depicting the artistic styles of anime and manga. The origins of eroge began in the early 1980s, while the computer industry in Japan was struggling to define a computer standard with makers like NEC, Sharp, and Fujitsu competing against one another. The PC98 series, despite lacking in processing power, CD drives and limited graphics, came to dominate the market, with the popularity of eroge games contributing to their success.

Due to the vague definitions of any erotic game, depending on its classification, citing the first erotic game is a subjective one. If the definition applies to adult themes, the first game was "Softporn Adventure". Released in America in 1981 for the Apple II, this was a text-based comedic game from On-Line Systems. If eroge is defined as the first graphical depictions and/or Japanese adult themes, it would be Koei's 1982 release of "Night Life". Sexual intercourse is depicted through simple graphic outlines. Notably, "Night Life" was not intended to be erotic so much as an instructional guide "to support married life". A series of "undressing" games appeared as early as 1983, such as "Strip Mahjong". The first anime-styled erotic game was Tenshitachi no Gogo, released in 1985 by JAST. In 1988, ASCII released the first erotic role-playing game, "Chaos Angel". In 1989, AliceSoft released the turn-based RPG "Rance" and ELF released "Dragon Knight".

In the late 1980s, eroge began to stagnate under high prices and the majority of games containing uninteresting plots and mindless sex. ELF's 1992 release of "Dokyusei" came as customer frustration with eroge was mounting and spawned a new genre of games called dating sims. "Dokyusei" was unique because it had no defined plot and required the player to build a relationship with different girls in order to advance the story. Each girl had her own story, but the prospect of consummating a relationship required the girl growing to love the player; there was no easy sex.

The term "visual novel" is vague, with Japanese and English definitions classifying the genre as a type of interactive fiction game driven by narration and limited player interaction. While the term is often retroactively applied to many games, it was Leaf that coined the term with their "Leaf Visual Novel Series" (LVNS) with the 1996 release of "Shizuku" and "Kizuato". The success of these two dark eroge games would be followed by the third and final installment of the LVNS, the 1997 romantic eroge "To Heart". Eroge visual novels took a new emotional turn with Tactics' 1998 release "". Key's 1999 release of "Kanon" proved to be a major success and would go on to have numerous console ports, two manga series and two anime series.

Japanese laws have impacted depictions of works since the Meiji Restoration, but these predate the common definition of hentai material. Since becoming law in 1907, Article 175 of the Criminal Code of Japan forbids the publication of obscene materials. Specifically, depictions of male-female sexual intercourse and pubic hair are considered obscene, but bare genitalia is not. As censorship is required for published works, the most common representations are the blurring dots on pornographic videos and "bars" or "lights" on still images. In 1986, Toshio Maeda sought to get past censorship on depictions of sexual intercourse, by creating tentacle sex. This led to the large number of works containing sexual intercourse with monsters, demons, robots, and aliens, whose genitals look different from men's. While western views attribute hentai to any explicit work, it was the products of this censorship which became not only the first titles legally imported to America and Europe, but the first successful ones. While uncut for American release, the United Kingdom's release of "Urotsukidoji" removed many scenes of the violence and tentacle rape scenes.

It was also because of this law that the artists began to depict the characters with a minimum of anatomical details and without pubic hair, by law, prior to 1991. Part of the ban was lifted when Nagisa Oshima prevailed over the obscenity charges at his trial for his film "In the Realm of the Senses". Though not enforced, the lifting of this ban did not apply to anime and manga as they were not deemed artistic exceptions.

However, alterations of material or censorship and even banning of works are common. The U.S. release of the "La Blue Girl" altered the age of the heroine from 16 to 18 and removed sex scenes with a dwarf ninja named Nin-nin, and removed the Japanese censoring blurring dots. "La Blue Girl" was outright rejected by UK censors who refused to classify it and prohibited its distribution. In 2011 the Liberal Democratic Party of Japan sought a ban on the subgenre lolicon.

The most prolific consumers of hentai are men. Eroge games in particular combine three favored media, cartoons, pornography and gaming, into an experience. The hentai genre engages a wide audience that expands yearly, and desires better quality and storylines, or works which push the creative envelope. The unusual and extreme depictions in hentai are not about perversion so much as they are an example of the profit-oriented industry. Anime depicting normal sexual situations enjoy less market success than those that break social norms, such as sex at schools or bondage.

According to Dr. Megha Hazuria Gorem, a clinical psychologist, "Because toons are a kind of final fantasy, you can make the person look the way you want him or her to look. Every fetish can be fulfilled." Dr. Narayan Reddy, a sexologist, commented on the eroge games, "Animators make new games because there is a demand for them, and because they depict things that the gamers do not have the courage to do in real life, or that might just be illegal, these games are an outlet for suppressed desire."

The hentai genre can be divided into numerous subgenres, the broadest of which encompasses heterosexual and homosexual acts. Hentai that features mainly heterosexual interactions occur in both male-targeted ("ero") and female-targeted ("ladies' comics") form. Those that feature mainly homosexual interactions are known as yaoi (male-male) and yuri (female-female). Both yaoi and, to a lesser extent, yuri, are generally aimed at members of the opposite sex from the persons depicted. While yaoi and yuri are not always explicit, their pornographic history and association remain. Yaoi's pornographic usage has remained strong in textual form through fanfiction. The definition of yuri has begun to be replaced by the broader definitions of "lesbian-themed animation or comics".

Hentai is perceived as "dwelling" on sexual fetishes. These include dozens of fetish and paraphilia related subgenres, which can be further classified with additional terms, such as heterosexual or homosexual types.

Many works are focused on depicting the mundane and the impossible across every conceivable act and situation no matter how fantastical. The largest subgenre of hentai is "futanari" (hermaphroditism), which most often features a female with a penis or penis-like appendage in place of, or in addition to normal female genitals. Futanari characters are primarily depicted as having sex with other women and will almost always be submissive with a male; exceptions include Yonekura Kengo's work, which features female empowerment and domination over males.




</doc>
<doc id="14186" url="https://en.wikipedia.org/wiki?curid=14186" title="Henry VII of England">
Henry VII of England

Henry VII (; 28 January 1457 – 21 April 1509) was the King of England and Lord of Ireland from his seizure of the crown on 22 August 1485 to his death on 21 April 1509. He was the first monarch of the House of Tudor.

Henry attained the throne when his forces defeated King Richard III at the Battle of Bosworth Field, the culmination of the Wars of the Roses. He was the last king of England to win his throne on the field of battle. He cemented his claim by marrying Elizabeth of York, daughter of Edward IV and niece of Richard III. Henry was successful in restoring the power and stability of the English monarchy after the civil war.

His supportive stance of the British Isles' wool industry and his standoff with the Low Countries had long lasting benefits to all of the British economy. However, the capriciousness and lack of due process that indebted many would tarnish his legacy and were soon ended upon Henry VII's death, after a commission revealed widespread abuses. According to the contemporary historian Polydore Vergil, simple "greed" underscored the means by which royal control was over-asserted in Henry's final years.
Henry can be credited with a number of administrative, economic and diplomatic initiatives. He paid very close attention to detail, and instead of spending lavishly he concentrated on raising new revenues and after a reign of nearly 24 years, he was peacefully succeeded by his son, Henry VIII. The new taxes were unpopular and two days after his coronation, Henry VIII arrested his father's two most unpopular ministers, Sir Richard Empson and Edmund Dudley. They were charged with high treason and were executed in 1510.
Henry VII was born at Pembroke Castle on 28 January 1457 to Margaret Beaufort, Countess of Richmond. His father, Edmund Tudor, 1st Earl of Richmond, died three months before his birth.

Henry's paternal grandfather, Owen Tudor, originally from the Tudors of Penmynydd, Isle of Anglesey in Wales, had been a page in the court of Henry V. He rose to become one of the "Squires to the Body to the King" after military service at the Battle of Agincourt. Owen is said to have secretly married the widow of Henry V, Catherine of Valois. One of their sons was Edmund Tudor, father of Henry VII. Edmund was created Earl of Richmond in 1452, and "formally declared legitimate by Parliament".

Henry's main claim to the English throne derived from his mother through the House of Beaufort. Henry's mother, Lady Margaret Beaufort, was a great-granddaughter of John of Gaunt, Duke of Lancaster, fourth son of Edward III, and his third wife Katherine Swynford. Katherine was Gaunt's mistress for about 25 years; when they married in 1396, they already had four children, including Henry's great-grandfather John Beaufort. Thus Henry's claim was somewhat tenuous: it was from a woman, and by illegitimate descent. In theory, the Portuguese and Castilian royal families had a better claim (as far as "legitimacy" is concerned) as descendants of Catherine of Lancaster, the daughter of John of Gaunt and his second wife Constance of Castile.

Gaunt's nephew Richard II legitimised Gaunt's children by Katherine Swynford by Letters Patent in 1397. In 1407, Henry IV, who was Gaunt's son by his first wife, issued new Letters Patent confirming the legitimacy of his half-siblings, but also declaring them ineligible for the throne. Henry IV's action was of doubtful legality, as the Beauforts were previously legitimised by an Act of Parliament, but it further weakened Henry's claim.

Nonetheless, by 1483 Henry was the senior male Lancastrian claimant remaining, after the deaths in battle or by murder or execution of Henry VI, his son Edward of Westminster, Prince of Wales, and the other Beaufort line of descent through Lady Margaret's uncle, the 2nd Duke of Somerset.

Henry also made some political capital out of his Welsh ancestry, for example in attracting military support and safeguarding his army's passage through Wales on its way to the Battle of Bosworth. He came from an old, established Anglesey family that claimed descent from Cadwaladr (in legend, the last ancient British king), and on occasion Henry displayed the red dragon of Cadwaladr. He took it, as well as the standard of St George, on his procession through London after the victory at Bosworth. A contemporary writer and Henry's biographer, Bernard André, also made much of Henry's Welsh descent.

In reality, however, his hereditary connections to Welsh aristocracy were not strong. He was descended by the paternal line, through several generations, from Ednyfed Fychan, the seneschal (steward) of Gwynedd and through this seneschal's wife from Rhys ap Tewdwr, the King of Deheubarth in South Wales. His more immediate ancestor, Tudur ap Goronwy, had aristocratic land rights, but his sons, who were first cousins to Owain Glyndŵr, sided with Owain in his revolt. One son was executed and the family land was forfeited. Another son, Henry's great-grandfather, became a butler to the Bishop of Bangor. Owen Tudor, the son of the butler, like the children of other rebels, was provided for by Henry V, a circumstance that precipitated his access to Queen Catherine of Valois. Notwithstanding this lineage, to the bards of Wales, Henry was a candidate for Y Mab Darogan – "The Son of Prophecy" who would free the Welsh from oppression.
In 1456, Henry's father Edmund Tudor was captured while fighting for Henry VI in South Wales against the Yorkists. He died in Carmarthen Castle, three months before Henry was born. Henry's uncle Jasper Tudor, the Earl of Pembroke and Edmund's younger brother, undertook to protect the young widow, who was 13 years old when she gave birth to Henry. When Edward IV became King in 1461, Jasper Tudor went into exile abroad. Pembroke Castle, and later the Earldom of Pembroke, were granted to the Yorkist William Herbert, who also assumed the guardianship of Margaret Beaufort and the young Henry.

Henry lived in the Herbert household until 1469, when Richard Neville, Earl of Warwick (the "Kingmaker"), went over to the Lancastrians. Herbert was captured fighting for the Yorkists and executed by Warwick. When Warwick restored Henry VI in 1470, Jasper Tudor returned from exile and brought Henry to court. When the Yorkist Edward IV regained the throne in 1471, Henry fled with other Lancastrians to Brittany, where he spent most of the next 14 years under the protection of Francis II, Duke of Brittany. In November 1476, Henry's protector fell ill and his principal advisers were more amenable to negotiating with the English king. Henry was handed over and escorted to the Breton port of Saint-Malo. While there, he feigned stomach cramps and in the confusion fled into a monastery. As at Tewkesbury Abbey after 1471 battle, Edward IV prepared to order his extraction and probable execution. The townspeople took exception to his behaviour, however, and Francis recovered from his illness. Thus a small band of scouts rescued Henry.

By 1483, Henry's mother was actively promoting him as an alternative to Richard III, despite her being married to a Yorkist, Lord Stanley. At Rennes Cathedral on Christmas Day 1483, Henry pledged to marry the eldest daughter of Edward IV, Elizabeth of York, who was also Edward's heir since the presumed death of her brothers, the Princes in the Tower (King Edward V and his brother Richard of Shrewsbury, Duke of York). Henry then received the homage of his supporters. With money and supplies borrowed from his host, Francis II, Duke of Brittany, Henry tried to land in England, but his conspiracy unravelled, resulting in the execution of his primary co-conspirator, the Duke of Buckingham. Now supported by Francis II's prime-minister, Pierre Landais, Richard III attempted to extradite Henry from Brittany, but Henry escaped to France. He was welcomed by the French, who readily supplied him with troops and equipment for a second invasion.

Henry gained the support of the Woodvilles, in-laws of the late Edward IV, and sailed with a small French and Scottish force, landing in Mill Bay, Pembrokeshire, close to his birthplace. He marched towards England accompanied by his uncle Jasper and the Earl of Oxford. Wales was traditionally a Lancastrian stronghold, and Henry owed the support he gathered to his Welsh birth and ancestry, being directly descended, through his father, from Rhys ap Gruffydd. He amassed an army of around 5,000 soldiers.

Henry was aware that his best chance to seize the throne was to engage Richard quickly and defeat him immediately, as Richard had reinforcements in Nottingham and Leicester. Richard only needed to avoid being killed to keep his throne. Though outnumbered, Henry's Lancastrian forces decisively defeated Richard's Yorkist army at the Battle of Bosworth Field on 22 August 1485. Several of Richard's key allies, such as the Earl of Northumberland and William and Thomas Stanley, crucially switched sides or left the battlefield. Richard III's death at Bosworth Field effectively ended the Wars of the Roses, although it was not the last battle Henry had to fight.

As king, Henry was styled as His Grace. His full style as king was: "Henry, by the Grace of God, King of England and France and Lord of Ireland".

Upon his succession as king, Henry became entitled to bear the arms of his kingdom. After his marriage, he used the red-and-white rose as his emblem – this continued to be his dynasty's emblem, known as the Tudor rose.

Henry's first concern was to secure his hold on the throne. He declared himself king "by right of conquest" retroactively from 21 August 1485, the day before Bosworth Field. Thus anyone who had fought for Richard against him would be guilty of treason, and Henry could legally confiscate his lands and property of Richard III while restoring his own. However, he spared Richard's nephew and designated heir, the Earl of Lincoln, and he made Margaret Plantagenet, a Yorkist heiress, Countess of Salisbury sui juris. He took great care not to address the baronage, or summon Parliament, until after his coronation, which took place in Westminster Abbey on 30 October 1485. Almost immediately afterwards, he issued an edict that any gentleman who swore fealty to him would, notwithstanding any previous attainder, be secure in his property and person.

Henry then honoured his pledge of December 1483 to marry Elizabeth of York. They were third cousins, as both were great-great-grandchildren of John of Gaunt. The marriage took place on 18 January 1486 at Westminster. The marriage unified the warring houses and gave his children a strong claim to the throne. The unification of the houses of York and Lancaster by this marriage is symbolised by the heraldic emblem of the Tudor rose, a combination of the white rose of York and the red rose of Lancaster. It also ended future discussion as to whether the descendants of the fourth son of Edward III, Edmund, Duke of York, through marriage to Philippa, heiress of the second son, Lionel, Duke of Clarence, had a superior or inferior claim to those of the third son John of Gaunt, who had held the throne for three generations.
In addition, Henry had Parliament repeal "Titulus Regius", the statute that declared Edward IV's marriage invalid and his children illegitimate, thus legitimising his wife. Amateur historians Bertram Fields and Sir Clements Markham have claimed that he may have been involved in the murder of the Princes in the Tower, as the repeal of "Titulus Regius" gave the Princes a stronger claim to the throne than his own. Alison Weir, however, points out that the Rennes ceremony, two years earlier, was possible only if Henry and his supporters were certain that the Princes were already dead.

Henry secured his crown principally by dividing and undermining the power of the nobility, especially through the aggressive use of bonds and recognisances to secure loyalty. He also enacted laws against livery and maintenance, the great lords' practice of having large numbers of "retainers" who wore their lord's badge or uniform and formed a potential private army.

While he was still in Leicester, after the battle of Bosworth Field, Henry was already taking precautions to prevent any rebellions against his reign. Before leaving Leicester to go to London, Henry dispatched Robert Willoughby to Sheriff Hutton in Yorkshire, to have the ten-year-old Edward, Earl of Warwick, arrested and taken to the Tower of London. Edward was the son of George, Duke of Clarence, and as such he presented a threat as a potential rival to the new King Henry VII for the throne of England. However, Henry was threatened by several active rebellions over the next few years. The first was the rebellion of the Stafford brothers and Viscount Lovell of 1486, which collapsed without fighting.

In 1487, Yorkists led by Lincoln rebelled in support of Lambert Simnel, a boy who was claimed to be the Earl of Warwick, son of Edward IV's brother Clarence (who had last been seen as a prisoner in the Tower). The rebellion began in Ireland, where the traditionally Yorkist nobility, headed by the powerful Gerald FitzGerald, 8th Earl of Kildare, proclaimed Simnel King and provided troops for his invasion of England. The rebellion was defeated and Lincoln killed at the Battle of Stoke. Henry showed remarkable clemency to the surviving rebels: he pardoned Kildare and the other Irish nobles, and he made the boy, Simnel, a servant in the royal kitchen.

In 1490, a young Fleming, Perkin Warbeck, appeared and claimed to be Richard, the younger of the "Princes in the Tower". Warbeck won the support of Edward IV's sister Margaret of Burgundy. He led attempted invasions of Ireland in 1491 and England in 1495, and persuaded James IV of Scotland to invade England in 1496. In 1497 Warbeck landed in Cornwall with a few thousand troops, but was soon captured and executed.

In 1499, Henry had the Earl of Warwick executed. However, he spared Warwick's elder sister Margaret. She survived until 1541, when she was executed by Henry VIII.

Henry married Elizabeth of York with the hope of uniting the Yorkist and Lancastrian sides of the Plantagenet dynastic disputes, and he was largely successful. However, such a level of paranoia persisted that anyone (John de la Pole, Earl of Richmond, for example) with blood ties to the Plantagenets was suspected of coveting the throne.

For most of Henry VII's reign Edward Story was Bishop of Chichester. Story's register still exists and, according to the 19th-century historian W.R.W. Stephens, "affords some illustrations of the avaricious and parsimonious character of the king". It seems that the king was skillful at extracting money from his subjects on many pretexts, including that of war with France or war with Scotland. The money so extracted added to the king's personal fortune rather than the stated purpose.

Unlike his predecessors, Henry VII came to the throne without personal experience in estate management or financial administration. Yet during his reign he became a fiscally prudent monarch who restored the fortunes of an effectively bankrupt exchequer. Henry VII introduced stability to the financial administration of England by keeping the same financial advisors throughout his reign. For instance, other than the first few months of the reign, Lord Dynham and Thomas Howard, earl of Surrey were the only two office holders in the position of Lord High Treasurer of England throughout his reign.

Henry VII improved tax collection within the realm by introducing ruthlessly efficient mechanisms of taxation. He was supported in this effort by his chancellor, Archbishop John Morton, whose "Morton's Fork" was a catch-22 method of ensuring that nobles paid increased taxes. Morton's Fork may actually have been invented by another of Henry's supporters, Richard Foxe. However, whether it is called "Morton's Fork" or "Fox's Fork", the result was the same: Those nobles who spent little must have saved much and, thus, they could afford the increased taxes; on the other hand, those nobles who spent much obviously had the means to pay the increased taxes. Royal government was also reformed with the introduction of the King's Council that kept the nobility in check. Henry VIII executed Richard Empson and Edmund Dudley, his two most hated tax collectors, on trumped-up charges of treason.

He established the Pound Avoirdupois as a standard of weight; it became part of the Imperial System and today's International pound units.

Henry VII's policy was both to maintain peace and to create economic prosperity. Up to a point, he succeeded. He was not a military man and had no interest in trying to regain French territories lost during the reigns of his predecessors; he was therefore ready to conclude a treaty with France at Etaples that brought money into the coffers of England, and ensured the French would not support pretenders to the English throne, such as Perkin Warbeck. However, this treaty came at a slight price, as Henry mounted a minor invasion of Brittany in November 1492. Henry decided to keep Brittany out of French hands, signed an alliance with Spain to that end, and sent 6,000 troops to France. The confused, fractious nature of Breton politics undermined his efforts, which finally failed after three sizeable expeditions, at a cost of £24,000. However, as France was becoming more concerned with the Italian Wars, the French were happy to agree to the Treaty of Etaples.
Henry had been under the financial and physical protection of the French throne or its vassals for most of his life, prior to his ascending the throne of England. To strengthen his position, however, he subsidised shipbuilding, so strengthening the navy (he commissioned Europe's first ever – and the world's oldest surviving – dry dock at Portsmouth in 1495) and improving trading opportunities.

Henry VII was one of the first European monarchs to recognise the importance of the newly united Spanish kingdom and concluded the Treaty of Medina del Campo, by which his son, Arthur Tudor, was married to Catherine of Aragon. He also concluded the Treaty of Perpetual Peace with Scotland (the first treaty between England and Scotland for almost two centuries), which betrothed his daughter Margaret to King James IV of Scotland. By means of this marriage, Henry VII hoped to break the Auld Alliance between Scotland and France. Though this was not achieved during his reign, the marriage eventually led to the union of the English and Scottish crowns under Margaret's great-grandson, James VI and I following the death of Henry's granddaughter Elizabeth I.

He also formed an alliance with Holy Roman Emperor Maximilian I (1493–1519) and persuaded Pope Innocent VIII to issue a papal bull of excommunication against all pretenders to Henry's throne.

Henry VII was much enriched by trading alum, which was used in the wool and cloth trades for use as a chemical dye fixative when dyeing fabrics. Since alum was mined in only one area in Europe (Tolfa, Italy), it was a scarce commodity and therefore especially valuable to its land holder, the pope. With the English economy heavily invested in wool production, Henry VII became involved in the alum trade in 1486. With the assistance of the Italian merchant-banker, Lodovico della Fava and the Italian banker, Girolamo Frescobaldi, Henry VII became deeply involved in the trade by licensing ships, obtaining alum from the Ottoman Empire, and selling it to the Low Countries and in England. This trade made an expensive commodity cheaper, which raised opposition from Pope Julius II since the Tolfa mine was a part of papal territory and had given the Pope monopoly control over alum.
Henry's most successful diplomatic achievement as regards the economy was the "Magnus Intercursus" ("great agreement") of 1496. In 1494, Henry embargoed trade (mainly in wool) with the Netherlands as retaliation for Margaret of Burgundy's support of Perkin Warbeck. The Merchant Adventurers, the company which enjoyed the monopoly of the Flemish wool trade, relocated from Antwerp to Calais. At the same time, Flemish merchants were ejected from England. The stand-off eventually paid off for Henry. Both parties realised they were mutually disadvantaged by the reduction in commerce. Its restoration by the "Magnus Intercursus" was very much to England's benefit in removing taxation for English merchants and significantly increasing England's wealth. In turn, Antwerp became an extremely important trade entrepôt (transshipment port), through which, for example, goods from the Baltic, spices from the east and Italian silks were exchanged for English cloth.

In 1506, Henry extorted the Treaty of Windsor from Philip the Handsome of Burgundy. Philip had been shipwrecked on the English coast, and while Henry's guest, was bullied into an agreement so favourable to England at the expense of the Netherlands that it was dubbed the "Malus Intercursus" ("evil agreement"). France, Burgundy, the Holy Roman Empire, Spain and the Hanseatic League all rejected the treaty, which was never in force. Philip died shortly after the negotiations.

Henry's principal problem was to restore royal authority in a realm recovering from the Wars of the Roses. There were too many powerful noblemen and, as a consequence of the system of so-called bastard feudalism, each had what amounted to private armies of indentured retainers (mercenaries masquerading as servants).
He was content to allow the nobles their regional influence if they were loyal to him. For instance, the Stanley family had control of Lancashire and Cheshire, upholding the peace on the condition that they stayed within the law. In other cases, he brought his over-powerful subjects to heel by decree. He passed laws against "livery" (the upper classes' flaunting of their adherents by giving them badges and emblems) and "maintenance" (the keeping of too many male "servants"). These laws were used shrewdly in levying fines upon those that he perceived as threats.

However, his principal weapon was the Court of Star Chamber. This revived an earlier practice of using a small (and trusted) group of the Privy Council as a personal or Prerogative Court, able to cut through the cumbersome legal system and act swiftly. Serious disputes involving the use of personal power, or threats to royal authority, were thus dealt with.

Henry VII used Justices of the Peace on a large, nationwide scale. They were appointed for every shire and served for a year at a time. Their chief task was to see that the laws of the country were obeyed in their area. Their powers and numbers steadily increased during the time of the Tudors, never more so than under Henry's reign. Despite this, Henry was keen to constrain their power and influence, applying the same principles to the Justices of the Peace as he did to the nobility: a similar system of bonds and recognisances to that which applied to both the gentry and the nobles who tried to exert their elevated influence over these local officials.

All Acts of Parliament were overseen by the Justices of the Peace. For example, Justices of the Peace could replace suspect jurors in accordance with the 1495 act preventing the corruption of juries. They were also in charge of various administrative duties, such as the checking of weights and measures.

By 1509, Justices of the Peace were key enforcers of law and order for Henry VII. They were unpaid, which, in comparison with modern standards, meant a lesser tax bill to pay for a police force. Local gentry saw the office as one of local influence and prestige and were therefore willing to serve. Overall, this was a successful area of policy for Henry, both in terms of efficiency and as a method of reducing the corruption endemic within the nobility of the Middle Ages.

In 1502, Henry VII's first son and heir apparent, Arthur, Prince of Wales, died suddenly at Ludlow Castle, very likely from a viral respiratory illness known at the time as the "English sweating sickness". This made Henry, Duke of York (Henry VIII) heir apparent to the throne. The King, normally a reserved man who rarely showed much emotion in public unless angry, surprised his courtiers by his intense grief and sobbing at his son's death, while his concern for the Queen is evidence that the marriage was a happy one, as is his reaction to the Queen's death the following year, when he shut himself away for several days, refusing to speak to anyone.

Henry VII wanted to maintain the Spanish alliance. He therefore arranged a papal dispensation from Pope Julius II for Prince Henry to marry his brother's widow Catherine, a relationship that would have otherwise precluded marriage in the Roman Catholic Church. In 1503, Queen Elizabeth died in childbirth, so King Henry had the dispensation also permit him to marry Catherine himself. After obtaining the dispensation, Henry had second thoughts about the marriage of his son and Catherine. Catherine's mother Isabella I of Castile had died and Catherine's sister Joanna had succeeded her; Catherine was therefore daughter of only one reigning monarch and so less desirable as a spouse for Henry VII's heir-apparent. The marriage did not take place during his lifetime. Otherwise, at the time of his father's arranging of the marriage to Catherine of Aragon, the future Henry VIII was too young to contract the marriage according to Canon Law, and would be ineligible until age fourteen.

Henry made half-hearted plans to remarry and beget more heirs, but these never came to anything. In 1505 he was sufficiently interested in a potential marriage to Joan, the recently widowed Queen of Naples, that he sent ambassadors to Naples to report on the 27-year-old's physical suitability. The wedding never took place, and the physical description Henry sent with his ambassadors of what he desired in a new wife matched the description of Elizabeth. After 1503, records show the Tower of London was never again used as a royal residence by Henry Tudor, and all royal births under Henry VIII took place in palaces. Henry VII was shattered by the loss of Elizabeth, and her death broke his heart. During his lifetime the nobility often jeered him for re-centralizing power in London, and later the 16th-century historian Francis Bacon was ruthlessly critical of the methods by which he enforced tax law, but it is equally true that Henry Tudor was hellbent on keeping detailed records of his personal finances, down to the last halfpenny; these and one account book detailing the expenses of his queen survive in the British National Archives. Until the death of his wife, the evidence is clear from these accounting books that Henry Tudor was a more doting father and husband than was widely known. Many of the entries show a man who loosened his purse strings generously for his wife and children, and not just on necessities: in spring 1491 he spent a great amount of gold on his daughter Mary for a lute; the following year he spent money on a lion for Elizabeth's menagerie.

With Elizabeth's death, the possibility for such family indulgences greatly diminished. Immediately afterward, Henry became very sick and nearly died himself, allowing only Margaret Beaufort, his mother, near him: "privily departed to a solitary place, and would that no man should resort unto him."
Henry VII died at Richmond Palace on 21 April 1509 of tuberculosis and was buried at Westminster Abbey, next to his wife, Elizabeth, in the chapel he commissioned. He was succeeded by his second son, Henry VIII (reign 1509–47). His mother survived him, dying two months later on 29 June 1509.

Henry is the first English king of whose appearance good contemporary visual records in realistic portraits exist that are relatively free of idealization. At 27, he was tall, slender, with small blue eyes, which were said to have a noticeable animation of expression, and noticeably bad teeth in a long, sallow face beneath very fair hair. Amiable and high-spirited, Henry was friendly if dignified in manner, and it was clear to everyone that he was extremely intelligent. His biographer, Professor Chrimes, credits him – even before he had become king – with "a high degree of personal magnetism, ability to inspire confidence, and a growing reputation for shrewd decisiveness". On the debit side, he may have looked a little delicate as he suffered from poor health.

Historians have always compared Henry VII with his continental contemporaries, especially Louis XI of France and Ferdinand II of Aragon. By 1600 historians emphasised Henry's wisdom in drawing lessons in statecraft from other monarchs. By 1900 the "New Monarchy" interpretation stressed the common factors that in each country led to the revival of monarchical power. This approach raised puzzling questions about similarities and differences in the development of national states. In the late 20th century a model of European state formation was prominent in which Henry less resembles Louis and Ferdinand.






</doc>
<doc id="14187" url="https://en.wikipedia.org/wiki?curid=14187" title="Henry VIII of England">
Henry VIII of England

Henry VIII (28 June 1491 – 28 January 1547) was King of England from 1509 until his death. Henry was the second Tudor monarch, succeeding his father, Henry VII.

Henry is best known for his six marriages, in particular his efforts to have his first marriage, to Catherine of Aragon, annulled. His disagreement with the Pope on the question of such an annulment led Henry to initiate the English Reformation, separating the Church of England from papal authority. He appointed himself the Supreme Head of the Church of England and dissolved convents and monasteries, for which he was excommunicated. Henry is also known as "the father of the Royal Navy"; he invested heavily on the navy, increasing its size greatly from a few to more than 50 ships.

Domestically, Henry is known for his radical changes to the English Constitution, ushering into England the theory of the divine right of kings. Besides asserting the sovereign's supremacy over the Church of England, he greatly expanded royal power during his reign. Charges of treason and heresy were commonly used to quell dissent, and those accused were often executed without a formal trial, by means of bills of attainder. He achieved many of his political aims through the work of his chief ministers, some of whom were banished or executed when they fell out of his favour. Thomas Wolsey, Thomas More, Thomas Cromwell, Richard Rich, and Thomas Cranmer all figured prominently in Henry's administration. He was an extravagant spender and used the proceeds from the Dissolution of the Monasteries and acts of the Reformation Parliament to convert into royal revenue the money that was formerly paid to Rome. Despite the influx of money from these sources, Henry was continually on the verge of financial ruin due to his personal extravagance as well as his numerous costly and largely unsuccessful continental wars, particularly with Francis I of France and the Holy Roman Emperor Charles V, as he sought to enforce his claim to the Kingdom of France. At home, he oversaw the legal union of England and Wales with the Laws in Wales Acts 1535 and 1542 and following the Crown of Ireland Act 1542 he was the first English monarch to rule as King of Ireland.

His contemporaries considered Henry in his prime to be an attractive, educated and accomplished king. He has been described as "one of the most charismatic rulers to sit on the English throne". He was an author and composer. As he aged, Henry became severely obese and his health suffered, contributing to his death in 1547. He is frequently characterised in his later life as a lustful, egotistical, harsh, and insecure king. He was succeeded by his son Edward VI.

Born 28 June 1491 at the Palace of Placentia in Greenwich, Kent, Henry Tudor was the third child and second son of Henry VII and Elizabeth of York. Of the young Henry's six siblings, only three – Arthur, Prince of Wales; Margaret; and Mary – survived infancy. He was baptised by Richard Fox, the Bishop of Exeter, at a church of the Observant Franciscans close to the palace. In 1493, at the age of two, Henry was appointed Constable of Dover Castle and Lord Warden of the Cinque Ports. He was subsequently appointed Earl Marshal of England and Lord Lieutenant of Ireland at age three, and was inducted into the Order of the Bath soon after. The day after the ceremony he was created Duke of York and a month or so later made Warden of the Scottish Marches. In May 1495, he was appointed to the Order of the Garter. The reason for all the appointments to a small child was so his father could keep personal control of lucrative positions and not share them with established families. Henry was given a first-rate education from leading tutors, becoming fluent in Latin and French, and learning at least some Italian. Not much is known about his early life – save for his appointments – because he was not expected to become king. In November 1501, Henry also played a considerable part in the ceremonies surrounding his brother's marriage to Catherine of Aragon, the youngest surviving child of King Ferdinand II of Aragon and Queen Isabella I of Castile. As Duke of York, Henry used the arms of his father as king, differenced by a "label of three points ermine". He was further honoured, on 9 February 1506, by Holy Roman Emperor Maximilian I who made him a Knight of the Golden Fleece.

In 1502, Arthur died at the age of 15, possibly of sweating sickness, just 20 weeks after his marriage to Catherine. Arthur's death thrust all his duties upon his younger brother, the 10-year-old Henry. After a little debate, Henry became the new Duke of Cornwall in October 1502, and the new Prince of Wales and Earl of Chester in February 1503. Henry VII gave the boy few tasks. Young Henry was strictly supervised and did not appear in public. As a result, he ascended the throne "untrained in the exacting art of kingship".

Henry VII renewed his efforts to seal a marital alliance between England and Spain, by offering his second son in marriage to Arthur's widow Catherine. Both Isabella and Henry VII were keen on the idea, which had arisen very shortly after Arthur's death. On 23 June 1503, a treaty was signed for their marriage, and they were betrothed two days later. A papal dispensation was only needed for the "impediment of public honesty" if the marriage had not been consummated as Catherine and her duenna claimed, but Henry VII and the Spanish ambassador set out instead to obtain a dispensation for "affinity", which took account of the possibility of consummation. Cohabitation was not possible because Henry was too young. Isabella's death in 1504, and the ensuing problems of succession in Castile, complicated matters. Her father preferred her to stay in England, but Henry VII's relations with Ferdinand had deteriorated. Catherine was therefore left in limbo for some time, culminating in Prince Henry's rejection of the marriage as soon he was able, at the age of 14. Ferdinand's solution was to make his daughter ambassador, allowing her to stay in England indefinitely. Devout, she began to believe that it was God's will that she marry the prince despite his opposition.

Henry VII died on 21 April 1509, and the 17-year-old Henry succeeded him as king. Soon after his father's burial on 10 May, Henry suddenly declared that he would indeed marry Catherine, leaving unresolved several issues concerning the papal dispensation and a missing part of the marriage portion. The new king maintained that it had been his father's dying wish that he marry Catherine. Whether or not this was true, it was certainly convenient. Emperor Maximilian I had been attempting to marry his granddaughter (and Catherine's niece) Eleanor to Henry; she had now been jilted. Henry's wedding to Catherine was kept low-key and was held at the friar's church in Greenwich on 11 June 1509. On 23 June 1509, Henry led the now 23-year-old Catherine from the Tower of London to Westminster Abbey for their coronation, which took place the following day. It was a grand affair: the king's passage was lined with tapestries and laid with fine cloth. Following the ceremony, there was a grand banquet in Westminster Hall. As Catherine wrote to her father, "our time is spent in continuous festival".

Two days after his coronation, Henry arrested his father's two most unpopular ministers, Sir Richard Empson and Edmund Dudley. They were charged with high treason and were executed in 1510. Historian Ian Crofton has maintained that such executions would become Henry's primary tactic for dealing with those who stood in his way; the two executions were certainly not the last. Henry also returned to the public some of the money supposedly extorted by the two ministers. By contrast, Henry's view of the House of York – potential rival claimants for the throne – was more moderate than his father's had been. Several who had been imprisoned by his father, including the Marquess of Dorset, were pardoned. Others (most notably Edmund de la Pole) went unreconciled; de la Pole was eventually beheaded in 1513, an execution prompted by his brother Richard siding against the king.

Soon after, Catherine conceived, but the child, a girl, was stillborn on 31 January 1510. About four months later, Catherine again became pregnant. On New Year's Day 1511, the child – Henry – was born. After the grief of losing their first child, the couple were pleased to have a boy and festivities were held, including a two-day joust known as the Westminster Tournament. However, the child died seven weeks later. Catherine had two stillborn sons in 1514 and 1515, but gave birth in February 1516 to a girl, Mary. Relations between Henry and Catherine had been strained, but they eased slightly after Mary's birth.

Although Henry's marriage to Catherine has since been described as "unusually good", it is known that Henry took mistresses. It was revealed in 1510 that Henry had been conducting an affair with one of the sisters of Edward Stafford, 3rd Duke of Buckingham, either Elizabeth or Anne Hastings, Countess of Huntingdon. The most significant mistress for about three years, starting in 1516, was Elizabeth Blount. Blount is one of only two completely undisputed mistresses, few for a virile young king. Exactly how many Henry had is disputed: David Loades believes Henry had mistresses "only to a very limited extent", whilst Alison Weir believes there were numerous other affairs. Catherine did not protest, and in 1518 fell pregnant again with another girl, who was also stillborn. Blount gave birth in June 1519 to Henry's illegitimate son, Henry FitzRoy. The young boy was made Duke of Richmond in June 1525 in what some thought was one step on the path to his eventual legitimisation. In 1533, FitzRoy married Mary Howard, but died childless three years later. At the time of Richmond's death in June 1536, Parliament was enacting the Second Succession Act, which could have allowed him to become king.

In 1510, France, with a fragile alliance with the Holy Roman Empire in the League of Cambrai, was winning a war against Venice. Henry renewed his father's friendship with Louis XII of France, an issue that divided his council. Certainly war with the combined might of the two powers would have been exceedingly difficult. Shortly thereafter, however, Henry also signed a pact with Ferdinand. After Pope Julius II created the anti-French Holy League in October 1511, Henry followed Ferdinand's lead and brought England into the new League. An initial joint Anglo-Spanish attack was planned for the spring to recover Aquitaine for England, the start of making Henry's dreams of ruling France a reality. The attack, however, following a formal declaration of war in April 1512, was not led by Henry personally and was a considerable failure; Ferdinand used it simply to further his own ends, and it strained the Anglo-Spanish alliance. Nevertheless, the French were pushed out of Italy soon after, and the alliance survived, with both parties keen to win further victories over the French. Henry then pulled off a diplomatic coup by convincing the Emperor to join the Holy League. Remarkably, Henry had also secured the promised title of "Most Christian King of France" from Julius and possibly coronation by the Pope himself in Paris, if only Louis could be defeated.

On 30 June 1513, Henry invaded France, and his troops defeated a French army at the Battle of the Spurs – a relatively minor result, but one which was seized on by the English for propaganda purposes. Soon after, the English took Thérouanne and handed it over to Maximillian; Tournai, a more significant settlement, followed. Henry had led the army personally, complete with large entourage. His absence from the country, however, had prompted his brother-in-law, James IV of Scotland, to invade England at the behest of Louis. Nevertheless, the English army, overseen by Queen Catherine, decisively defeated the Scots at the Battle of Flodden on 9 September 1513. Among the dead was the Scottish king, thus ending Scotland's brief involvement in the war. These campaigns had given Henry a taste of the military success he so desired. However, despite initial indications, he decided not to pursue a 1514 campaign. He had been supporting Ferdinand and Maximilian financially during the campaign but had received little in return; England's coffers were now empty. With the replacement of Julius by Pope Leo X, who was inclined to negotiate for peace with France, Henry signed his own treaty with Louis: his sister Mary would become Louis' wife, having previously been pledged to the younger Charles, and peace was secured for eight years, a remarkably long time.

Charles V ascended the thrones of both Spain and the Holy Roman Empire following the deaths of his grandfathers, Ferdinand in 1516 and Maximilian in 1519. Francis I likewise became king of France upon the death of Louis in 1515, leaving three relatively young rulers and an opportunity for a clean slate. The careful diplomacy of Cardinal Thomas Wolsey had resulted in the Treaty of London in 1518, aimed at uniting the kingdoms of western Europe in the wake of a new Ottoman threat, and it seemed that peace might be secured. Henry met Francis I on 7 June 1520 at the Field of the Cloth of Gold near Calais for a fortnight of lavish entertainment. Both hoped for friendly relations in place of the wars of the previous decade. The strong air of competition laid to rest any hopes of a renewal of the Treaty of London, however, and conflict was inevitable. Henry had more in common with Charles, whom he met once before and once after Francis. Charles brought the Empire into war with France in 1521; Henry offered to mediate, but little was achieved and by the end of the year Henry had aligned England with Charles. He still clung to his previous aim of restoring English lands in France, but also sought to secure an alliance with Burgundy, then part of Charles' realm, and the continued support of Charles. A small English attack in the north of France made up little ground. Charles defeated and captured Francis at Pavia and could dictate peace; but he believed he owed Henry nothing. Sensing this, Henry decided to take England out of the war before his ally, signing the Treaty of the More on 30 August 1525.

During his first marriage to Catherine of Aragon, Henry conducted an affair with Mary Boleyn, Catherine's lady-in-waiting. There has been speculation that Mary's two children, Henry and Catherine Carey, were fathered by Henry, but this has never been proved, and the King never acknowledged them as he did Henry FitzRoy. In 1525, as Henry grew more impatient with Catherine's inability to produce the male heir he desired, he became enamoured of Mary Boleyn's sister, Anne, then a charismatic young woman of 25 in the Queen's entourage. Anne, however, resisted his attempts to seduce her, and refused to become his mistress as her sister Mary Boleyn had. It was in this context that Henry considered his three options for finding a dynastic successor and hence resolving what came to be described at court as the King's "great matter". These options were legitimising Henry FitzRoy, which would take the intervention of the pope and would be open to challenge; marrying off Mary as soon as possible and hoping for a grandson to inherit directly, but Mary was considered unlikely to conceive before Henry's death; or somehow rejecting Catherine and marrying someone else of child-bearing age. Probably seeing the possibility of marrying Anne, the third was ultimately the most attractive possibility to the 34-year-old Henry, and it soon became the King's absorbing desire to annul his marriage to the now 40-year-old Catherine. It was a decision that would lead Henry to reject papal authority and initiate the English Reformation.

Henry's precise motivations and intentions over the coming years are not widely agreed on. Henry himself, at least in the early part of his reign, was a devout and well-informed Catholic to the extent that his 1521 publication "Assertio Septem Sacramentorum" ("Defence of the Seven Sacraments") earned him the title of "Fidei Defensor" (Defender of the Faith) from Pope Leo X. The work represented a staunch defence of papal supremacy, albeit one couched in somewhat contingent terms. It is not clear exactly when Henry changed his mind on the issue as he grew more intent on a second marriage. Certainly, by 1527 he had convinced himself that in marrying Catherine, his brother's wife, he had acted contrary to Leviticus 20:21, an impediment Henry now believed that the Pope never had the authority to dispense with. It was this argument Henry took to Pope Clement VII in 1527 in the hope of having his marriage to Catherine annulled, forgoing at least one less openly defiant line of attack. In going public, all hope of tempting Catherine to retire to a nunnery or otherwise stay quiet were lost. Henry sent his secretary, William Knight, to appeal directly to the Holy See by way of a deceptively worded draft papal bull. Knight was unsuccessful; the Pope could not be misled so easily.

Other missions concentrated on arranging an ecclesiastical court to meet in England, with a representative from Clement VII. Though Clement agreed to the creation of such a court, he never had any intention of empowering his legate, Lorenzo Campeggio, to decide in Henry's favour. This bias was perhaps the result of pressure from Charles V, Catherine's nephew, though it is not clear how far this influenced either Campeggio or the Pope. After less than two months of hearing evidence, Clement called the case back to Rome in July 1529, from which it was clear that it would never re-emerge. With the chance for an annulment lost and England's place in Europe forfeit, Cardinal Wolsey bore the blame. He was charged with "praemunire" in October 1529 and his fall from grace was "sudden and total". Briefly reconciled with Henry (and officially pardoned) in the first half of 1530, he was charged once more in November 1530, this time for treason, but died while awaiting trial. After a short period in which Henry took government upon his own shoulders, Sir Thomas More took on the role of Lord Chancellor and chief minister. Intelligent and able, but also a devout Catholic and opponent of the annulment, More initially cooperated with the king's new policy, denouncing Wolsey in Parliament.

A year later, Catherine was banished from court, and her rooms were given to Anne. Anne was an unusually educated and intellectual woman for her time, and was keenly absorbed and engaged with the ideas of the Protestant Reformers, though the extent to which she herself was a committed Protestant is much debated. When Archbishop of Canterbury William Warham died, Anne's influence and the need to find a trustworthy supporter of the annulment had Thomas Cranmer appointed to the vacant position. This was approved by the Pope, unaware of the King's nascent plans for the Church.

In the winter of 1532, Henry met with Francis I at Calais and enlisted the support of the French king for his new marriage. Immediately upon returning to Dover in England, Henry, now 41, and Anne, now 32, went through a secret wedding service. She soon became pregnant, and there was a second wedding service in London on 25 January 1533. On 23 May 1533, Cranmer, sitting in judgment at a special court convened at Dunstable Priory to rule on the validity of the king's marriage to Catherine of Aragon, declared the marriage of Henry and Catherine null and void. Five days later, on 28 May 1533, Cranmer declared the marriage of Henry and Anne to be valid. Catherine was formally stripped of her title as queen, becoming instead "princess dowager" as the widow of Arthur. In her place, Anne was crowned queen consort on 1 June 1533. The queen gave birth to a daughter slightly prematurely on 7 September 1533. The child was christened Elizabeth, in honour of Henry's mother, Elizabeth of York.

Following the marriage, there was a period of consolidation taking the form of a series of statutes of the Reformation Parliament aimed at finding solutions to any remaining issues, whilst protecting the new reforms from challenge, convincing the public of their legitimacy, and exposing and dealing with opponents. Although the canon law was dealt with at length by Cranmer and others, these acts were advanced by Thomas Cromwell, Thomas Audley and the Duke of Norfolk and indeed by Henry himself. With this process complete, in May 1532 More resigned as Lord Chancellor, leaving Cromwell as Henry's chief minister. With the Act of Succession 1533, Catherine's daughter, Mary, was declared illegitimate; Henry's marriage to Anne was declared legitimate; and Anne's issue was decided to be next in the line of succession. With the Acts of Supremacy in 1534, Parliament also recognised the King's status as head of the church in England and, with the Act in Restraint of Appeals in 1532, abolished the right of appeal to Rome. It was only then that Pope Clement took the step of excommunicating Henry and Thomas Cranmer, although the excommunication was not made official until some time later.

The king and queen were not pleased with married life. The royal couple enjoyed periods of calm and affection, but Anne refused to play the submissive role expected of her. The vivacity and opinionated intellect that had made her so attractive as an illicit lover made her too independent for the largely ceremonial role of a royal wife and it made her many enemies. For his part, Henry disliked Anne's constant irritability and violent temper. After a false pregnancy or miscarriage in 1534, he saw her failure to give him a son as a betrayal. As early as Christmas 1534, Henry was discussing with Cranmer and Cromwell the chances of leaving Anne without having to return to Catherine. Henry is traditionally believed to have had an affair with Margaret ("Madge") Shelton in 1535, although historian Antonia Fraser argues that Henry in fact had an affair with her sister Mary Shelton.

Opposition to Henry's religious policies was quickly suppressed in England. A number of dissenting monks, including the first Carthusian Martyrs, were executed and many more pilloried. The most prominent resisters included John Fisher, Bishop of Rochester, and Sir Thomas More, both of whom refused to take the oath to the King. Neither Henry nor Cromwell sought to have the men executed; rather, they hoped that the two might change their minds and save themselves. Fisher openly rejected Henry as the Supreme Head of the Church, but More was careful to avoid openly breaking the Treason Act, which (unlike later acts) did not forbid mere silence. Both men were subsequently convicted of high treason, however – More on the evidence of a single conversation with Richard Rich, the Solicitor General. Both were duly executed in the summer of 1535.

These suppressions, as well as the Dissolution of the Lesser Monasteries Act of 1536, in turn contributed to more general resistance to Henry's reforms, most notably in the Pilgrimage of Grace, a large uprising in northern England in October 1536. Some 20,000 to 40,000 rebels were led by Robert Aske, together with parts of the northern nobility. Henry VIII promised the rebels he would pardon them and thanked them for raising the issues. Aske told the rebels they had been successful and they could disperse and go home. Henry saw the rebels as traitors and did not feel obliged to keep his promises with them, so when further violence occurred after Henry's offer of a pardon he was quick to break his promise of clemency. The leaders, including Aske, were arrested and executed for treason. In total, about 200 rebels were executed, and the disturbances ended.

On 8 January 1536 news reached the king and the queen that Catherine of Aragon had died. Henry called for public displays of joy regarding Catherine's death. The queen was pregnant again, and she was aware of the consequences if she failed to give birth to a son. Later that month, the King was unhorsed in a tournament and was badly injured and it seemed for a time that his life was in danger. When news of this accident reached the queen, she was sent into shock and miscarried a male child that was about 15 weeks old, on the day of Catherine's funeral, 29 January 1536. For most observers, this personal loss was the beginning of the end of the royal marriage.

Although the Boleyn family still held important positions on the Privy Council, Anne had many enemies, including the Duke of Suffolk. Even her own uncle, the Duke of Norfolk, had come to resent her attitude to her power. The Boleyns preferred France over the Emperor as a potential ally, but the King's favour had swung towards the latter (partly because of Cromwell), damaging the family's influence. Also opposed to Anne were supporters of reconciliation with Princess Mary (among them the former supporters of Catherine), who had reached maturity. A second annulment was now a real possibility, although it is commonly believed that it was Cromwell's anti-Boleyn influence that led opponents to look for a way of having her executed.

Anne's downfall came shortly after she had recovered from her final miscarriage. Whether it was primarily the result of allegations of conspiracy, adultery, or witchcraft remains a matter of debate among historians. Early signs of a fall from grace included the King's new mistress, the 28-year-old Jane Seymour, being moved into new quarters, and Anne's brother, George Boleyn, being refused the Order of the Garter, which was instead given to Nicholas Carew. Between 30 April and 2 May, five men, including Anne's brother, were arrested on charges of treasonable adultery and accused of having sexual relationships with the queen. Anne was also arrested, accused of treasonous adultery and incest. Although the evidence against them was unconvincing, the accused were found guilty and condemned to death. George Boleyn and the other accused men were executed on 17 May 1536. At 8 am on 19 May 1536, Anne was executed on Tower Green.

The day after Anne's execution in 1536 the 45-year-old Henry became engaged to Seymour, who had been one of the Queen's ladies-in-waiting. They were married ten days later. On 12 October 1537, Jane gave birth to a son, Prince Edward, the future Edward VI. The birth was difficult, and the queen died on 24 October 1537 from an infection and was buried in Windsor. The euphoria that had accompanied Edward's birth became sorrow, but it was only over time that Henry came to long for his wife. At the time, Henry recovered quickly from the shock. Measures were immediately put in place to find another wife for Henry, which, at the insistence of Cromwell and the court, were focused on the European continent.

With Charles V distracted by the internal politics of his many kingdoms and external threats, and Henry and Francis on relatively good terms, domestic and not foreign policy issues had been Henry's priority in the first half of the 1530s. In 1536, for example, Henry granted his assent to the Laws in Wales Act 1535, which legally annexed Wales, uniting England and Wales into a single nation. This was followed by the Second Succession Act (the Act of Succession 1536), which declared Henry's children by Jane to be next in the line of succession and declared both Mary and Elizabeth illegitimate, thus excluding them from the throne. The king was also granted the power to further determine the line of succession in his will, should he have no further issue. However, when Charles and Francis made peace in January 1539, Henry became increasingly paranoid, perhaps as a result of receiving a constant list of threats to the kingdom (real or imaginary, minor or serious) supplied by Cromwell in his role as spymaster. Enriched by the dissolution of the monasteries, Henry used some of his financial reserves to build a series of coastal defences and set some aside for use in the event of a Franco-German invasion.

Having considered the matter, Cromwell, now Earl of Essex, suggested Anne, the 25-year-old sister of the Duke of Cleves, who was seen as an important ally in case of a Roman Catholic attack on England, for the duke fell between Lutheranism and Catholicism. Hans Holbein the Younger was dispatched to Cleves to paint a portrait of Anne for the king. Despite speculation that Holbein painted her in an overly flattering light, it is more likely that the portrait was accurate; Holbein remained in favour at court. After seeing Holbein's portrait, and urged on by the complimentary description of Anne given by his courtiers, the 49-year-old king agreed to wed Anne. However, it was not long before Henry wished to annul the marriage so he could marry another. Anne did not argue, and confirmed that the marriage had never been consummated. Anne's previous betrothal to the Duke of Lorraine's son Francis provided further grounds for the annulment. The marriage was subsequently dissolved, and Anne received the title of "The King's Sister", two houses and a generous allowance. It was soon clear that Henry had fallen for the 17-year-old Catherine Howard, the Duke of Norfolk's niece, the politics of which worried Cromwell, for Norfolk was a political opponent.

Shortly after, the religious reformers (and protégés of Cromwell) Robert Barnes, William Jerome and Thomas Garret were burned as heretics. Cromwell, meanwhile, fell out of favour although it is unclear exactly why, for there is little evidence of differences of domestic or foreign policy. Despite his role, he was never formally accused of being responsible for Henry's failed marriage. Cromwell was now surrounded by enemies at court, with Norfolk also able to draw on his niece's position. Cromwell was charged with treason, selling export licences, granting passports, and drawing up commissions without permission, and may also have been blamed for the failure of the foreign policy that accompanied the attempted marriage to Anne. He was subsequently attainted and beheaded.

On 28 July 1540 (the same day Cromwell was executed), Henry married the young Catherine Howard, a first cousin and lady-in-waiting of Anne Boleyn. He was absolutely delighted with his new queen, and awarded her the lands of Cromwell and a vast array of jewellery. Soon after the marriage, however, Queen Catherine had an affair with the courtier Thomas Culpeper. She also employed Francis Dereham, who had previously been informally engaged to her and had an affair with her prior to her marriage, as her secretary. The court was informed of her affair with Dereham whilst Henry was away; they dispatched Thomas Cranmer to investigate, who brought evidence of Queen Catherine's previous affair with Dereham to the king's notice. Though Henry originally refused to believe the allegations, Dereham confessed. It took another meeting of the council, however, before Henry believed the accusations against Dereham and went into a rage, blaming the council before consoling himself in hunting. When questioned, the queen could have admitted a prior contract to marry Dereham, which would have made her subsequent marriage to Henry invalid, but she instead claimed that Dereham had forced her to enter into an adulterous relationship. Dereham, meanwhile, exposed Queen Catherine's relationship with Culpeper. Culpeper and Dereham were both executed, and Catherine too was beheaded on 13 February 1542.

In 1538, the chief minister Thomas Cromwell pursued an extensive campaign against what was termed "idolatry" by the followers of the old religion, culminating in September with the dismantling of the shrine of St. Thomas Becket at Canterbury. As a consequence, the king was excommunicated by Pope Paul III on 17 December of the same year. In 1540, Henry sanctioned the complete destruction of shrines to saints. In 1542, England's remaining monasteries were all dissolved, and their property transferred to the Crown. Abbots and priors lost their seats in the House of Lords; only archbishops and bishops remained. Consequently, the Lords Spiritual—as members of the clergy with seats in the House of Lords were known—were for the first time outnumbered by the Lords Temporal.

The 1539 alliance between Francis and Charles had soured, eventually degenerating into renewed war. With Catherine of Aragon and Anne Boleyn dead, relations between Charles and Henry improved considerably, and Henry concluded a secret alliance with the Emperor and decided to enter the Italian War in favour of his new ally. An invasion of France was planned for 1543. In preparation for it, Henry moved to eliminate the potential threat of Scotland under the youthful James V. The Scots were defeated at Battle of Solway Moss on 24 November 1542, and James died on 15 December. Henry now hoped to unite the crowns of England and Scotland by marrying his son Edward to James' successor, Mary. The Scottish Regent Lord Arran agreed to the marriage in the Treaty of Greenwich on 1 July 1543, but it was rejected by the Parliament of Scotland on 11 December. The result was eight years of war between England and Scotland, a campaign later dubbed "the Rough Wooing". Despite several peace treaties, unrest continued in Scotland until Henry's death.

Despite the early success with Scotland, Henry hesitated to invade France, annoying Charles. Henry finally went to France in June 1544 with a two-pronged attack. One force under Norfolk ineffectively besieged Montreuil. The other, under Suffolk, laid siege to Boulogne. Henry later took personal command, and Boulogne fell on 18 September 1544. However, Henry had refused Charles' request to march against Paris. Charles' own campaign fizzled, and he made peace with France that same day. Henry was left alone against France, unable to make peace. Francis attempted to invade England in the summer of 1545, but reached only the Isle of Wight before being repulsed in the Battle of the Solent. Out of money, France and England signed the Treaty of Camp on 7 June 1546. Henry secured Boulogne for eight years. The city was then to be returned to France for 2 million crowns (£750,000). Henry needed the money; the 1544 campaign had cost £650,000, and England was once again bankrupt.

Henry married his last wife, the wealthy widow Catherine Parr, in July 1543. A reformer at heart, she argued with Henry over religion. Ultimately, Henry remained committed to an idiosyncratic mixture of Catholicism and Protestantism; the reactionary mood which had gained ground following the fall of Cromwell had neither eliminated his Protestant streak nor been overcome by it. Parr helped reconcile Henry with his daughters, Mary and Elizabeth. In 1543, an Act of Parliament put them back in the line of succession after Edward. The same act allowed Henry to determine further succession to the throne in his will.

Late in life, Henry became obese, with a waist measurement of , and had to be moved about with the help of mechanical inventions. He was covered with painful, pus-filled boils and possibly suffered from gout. His obesity and other medical problems can be traced to the jousting accident in 1536 in which he suffered a leg wound. The accident re-opened and aggravated a previous injury he had sustained years earlier, to the extent that his doctors found it difficult to treat. The wound festered for the remainder of his life and became ulcerated, thus preventing him from maintaining the level of physical activity he had previously enjoyed. The jousting accident is also believed to have caused Henry's mood swings, which may have had a dramatic effect on his personality and temperament.

The theory that Henry suffered from syphilis has been dismissed by most historians. Historian Susan Maclean Kybett ascribes his demise to scurvy, which is caused by a lack of fresh fruits and vegetables. Alternatively, his wives' pattern of pregnancies and his mental deterioration have led some to suggest that the king may have been Kell positive and suffered from McLeod syndrome. According to another study, Henry VIII's history and body morphology may have been the result of traumatic brain injury after his 1536 jousting accident, which in turn led to a neuroendocrine cause of his obesity. This analysis identifies growth hormone deficiency (GHD) as the source for his increased adiposity but also significant behavioural changes noted in his later years, including his multiple marriages.
Henry's obesity hastened his death at the age of 55, which occurred on 28 January 1547 in the Palace of Whitehall, on what would have been his father's 90th birthday. He allegedly uttered his last words: "Monks! Monks! Monks!" perhaps in reference to the monks he caused to be evicted during the Dissolution of the Monasteries. Henry VIII was interred in St George's Chapel in Windsor Castle, next to Jane Seymour. Over a hundred years later, King Charles I (1625–1649) was buried in the same vault.

Upon Henry's death, he was succeeded by his son Edward VI. Since Edward was then only nine years old, he could not rule directly. Instead, Henry's will designated 16 executors to serve on a council of regency until Edward reached the age of 18. The executors chose Edward Seymour, 1st Earl of Hertford, Jane Seymour's elder brother, to be Lord Protector of the Realm. If Edward died childless, the throne was to pass to Mary, Henry VIII's daughter by Catherine of Aragon, and her heirs. If Mary's issue failed, the crown was to go to Elizabeth, Henry's daughter by Anne Boleyn, and her heirs. Finally, if Elizabeth's line became extinct, the crown was to be inherited by the descendants of Henry VIII's deceased younger sister, Mary, the Greys. The descendants of Henry's sister Margaret – the Stuarts, rulers of Scotland – were thereby excluded from the succession. This final provision failed when James VI of Scotland became King of England in 1603.

Henry cultivated the image of a Renaissance man, and his court was a centre of scholarly and artistic innovation and glamorous excess, epitomised by the Field of the Cloth of Gold. He scouted the country for choirboys, taking some directly from Wolsey's choir, and introduced Renaissance music into court. Musicians included Benedict de Opitiis, Richard Sampson, Ambrose Lupo, and Venetian organist Dionisio Memo.

Henry himself kept a considerable collection of instruments; he was skilled on the lute, could play the organ, and was a talented player of the virginals. He could also sight read music and sing well. He was an accomplished musician, author, and poet; his best known piece of music is "Pastime with Good Company" ("The Kynges Ballade"). He is often reputed to have written "Greensleeves" but probably did not.

He was an avid gambler and dice player, and excelled at sports, especially jousting, hunting, and real tennis. He was known for his strong defence of conventional Christian piety. The King was involved in the original construction and improvement of several significant buildings, including Nonsuch Palace, King's College Chapel, Cambridge and Westminster Abbey in London. Many of the existing buildings Henry improved were properties confiscated from Wolsey, such as Christ Church, Oxford; Hampton Court Palace; the Palace of Whitehall; and Trinity College, Cambridge.

Henry was an intellectual. The first English king with a modern humanist education, he read and wrote English, French and Latin, and was thoroughly at home in his well-stocked library. He personally annotated many books and wrote and published one of his own. To promote the public support for the reformation of the church, Henry had numerous pamphlets and lectures prepared. For example, Richard Sampson's "Oratio" (1534) was an argument for absolute obedience to the monarchy and claimed that the English church had always been independent from Rome. At the popular level, theatre and minstrel troupes funded by the crown travelled around the land to promote the new religious practices: the pope and Catholic priests and monks were mocked as foreign devils, while the glorious king was hailed as a brave and heroic defender of the true faith. Henry worked hard to present an image of unchallengeable authority and irresistible power.

A large well-built athlete (over tall and strong and broad in proportion), Henry excelled at jousting and hunting. More than pastimes, they were political devices that served multiple goals, from enhancing his athletic royal image to impressing foreign emissaries and rulers, to conveying Henry's ability to suppress any rebellion. Thus he arranged a jousting tournament at Greenwich in 1517, where he wore gilded armour, gilded horse trappings, and outfits of velvet, satin and cloth of gold dripping with pearls and jewels. It suitably impressed foreign ambassadors, one of whom wrote home that, "The wealth and civilisation of the world are here, and those who call the English barbarians appear to me to render themselves such". Henry finally retired from jousting in 1536 after a heavy fall from his horse left him unconscious for two hours, but he continued to sponsor two lavish tournaments a year. He then started adding weight and lost the trim, athletic figure that had made him so handsome; Henry's courtiers began dressing in heavily padded clothes to emulate – and flatter – their increasingly stout monarch. Towards the end of his reign his health rapidly declined.

The power of Tudor monarchs, including Henry, was 'whole' and 'entire', ruling, as they claimed, by the grace of God alone. The crown could also rely on the exclusive use of those functions that constituted the royal prerogative. These included acts of diplomacy (including royal marriages), declarations of war, management of the coinage, the issue of royal pardons and the power to summon and dissolve parliament as and when required. Nevertheless, as evident during Henry's break with Rome, the monarch worked within established limits, whether legal or financial, that forced him to work closely with both the nobility and parliament (representing the gentry).

In practice, Tudor monarchs used patronage to maintain a royal court that included formal institutions such as the Privy Council as well as more informal advisers and confidants. Both the rise and fall of court nobles could be swift: although the often-quoted figure of 72,000 executions during his reign is inflated, Henry did undoubtedly execute at will, burning or beheading two of his wives, twenty peers, four leading public servants, six close attendants and friends, one cardinal (John Fisher) and numerous abbots. Among those who were in favour at any given point in Henry's reign, one could usually be identified as a chief minister, though one of the enduring debates in the historiography of the period has been the extent to which those chief ministers controlled Henry rather than vice versa. In particular, historian G. R. Elton has argued that one such minister, Thomas Cromwell, led a "Tudor revolution in government" quite independent of the king, whom Elton presented as an opportunistic, essentially lazy participant in the nitty-gritty of politics. Where Henry did intervene personally in the running of the country, Elton argued, he mostly did so to its detriment. The prominence and influence of faction in Henry's court is similarly discussed in the context of at least five episodes of Henry's reign, including the downfall of Anne Boleyn.

From 1514 to 1529, Thomas Wolsey (1473–1530), a cardinal of the established Church, oversaw domestic and foreign policy for the young king from his position as Lord Chancellor. Wolsey centralised the national government and extended the jurisdiction of the conciliar courts, particularly the Star Chamber. The Star Chamber's overall structure remained unchanged, but Wolsey used it to provide for much-needed reform of the criminal law. The power of the court itself did not outlive Wolsey, however, since no serious administrative reform was undertaken and its role was eventually devolved to the localities. Wolsey helped fill the gap left by Henry's declining participation in government (particularly in comparison to his father) but did so mostly by imposing himself in the King's place. His use of these courts to pursue personal grievances, and particularly to treat delinquents as if mere examples of a whole class worthy of punishment, angered the rich, who were annoyed as well by his enormous wealth and ostentatious living. Following Wolsey's downfall, Henry took full control of his government, although at court numerous complex factions continued to try to ruin and destroy each other.

Thomas Cromwell (c. 1485–1540) also came to define Henry's government. Returning to England from the continent in 1514 or 1515, Cromwell soon entered Wolsey's service. He turned to law, also picking up a good knowledge of the Bible, and was admitted to Gray's Inn in 1524. He became Wolsey's "man of all work". Cromwell, driven in part by his religious beliefs, attempted to reform the body politic of the English government through discussion and consent, and through the vehicle of continuity and not outward change. He was seen by many people as the man they wanted to bring about their shared aims, including Thomas Audley. By 1531, Cromwell and those associated with him were already responsible for the drafting of much legislation. Cromwell's first office was that of the master of the King's jewels in 1532, from which he began to invigorate the government finances. By this point, Cromwell's power as an efficient administrator in a Council full of politicians exceeded what Wolsey had achieved.

Cromwell did much work through his many offices to remove the tasks of government from the Royal Household (and ideologically from the personal body of the King) and into a public state. He did so, however, in a haphazard fashion that left several remnants, not least because he needed to retain Henry's support, his own power, and the possibility of actually achieving the plan he set out. Cromwell made the various income streams put in place by Henry VII more formal and assigned largely autonomous bodies for their administration. The role of the King's Council was transferred to a reformed Privy Council, much smaller and more efficient than its predecessor. A difference emerged between the financial health of the king, and that of the country, although Cromwell's fall undermined much of his bureaucracy, which required his hand to keep order among the many new bodies and prevent profligate spending that strained relations as well as finances. Cromwell's reforms ground to a halt in 1539, the initiative lost, and he failed to secure the passage of an enabling act, the Proclamation by the Crown Act 1539. He too was executed, on 28 July 1540.

Henry inherited a vast fortune and a prosperous economy from his father Henry VII, who had been frugal and careful with money. This fortune was estimated to £1,250,000 (£375 million by today's standards). By comparison, however, the reign of Henry was a near-disaster in financial terms. Although he further augmented his royal treasury through the seizure of church lands, Henry's heavy spending and long periods of mismanagement damaged the economy.

Much of this wealth was spent by Henry on maintaining his court and household, including many of the building works he undertook on royal palaces. Henry hung 2,000 tapestries in his palaces; by comparison, James V of Scotland hung just 200. Henry took pride in showing off his collection of weapons, which included exotic archery equipment, 2,250 pieces of land ordnance and 6,500 handguns. Tudor monarchs had to fund all the expenses of government out of their own income. This income came from the Crown lands that Henry owned as well as from customs duties like tonnage and poundage, granted by parliament to the king for life. During Henry's reign the revenues of the Crown remained constant (around £100,000), but were eroded by inflation and rising prices brought about by war. Indeed, war and Henry's dynastic ambitions in Europe exhausted the surplus he had inherited from his father by the mid-1520s.

Whereas Henry VII had not involved Parliament in his affairs very much, Henry VIII had to turn to Parliament during his reign for money, in particular for grants of subsidies to fund his wars. The Dissolution of the Monasteries provided a means to replenish the treasury, and as a result the Crown took possession of monastic lands worth £120,000 (£36 million) a year. The Crown had profited a small amount in 1526 when Wolsey had put England onto a gold, rather than silver, standard, and had debased the currency slightly. Cromwell debased the currency more significantly, starting in Ireland in 1540. The English pound halved in value against the Flemish pound between 1540 and 1551 as a result. The nominal profit made was significant, helping to bring income and expenditure together, but it had a catastrophic effect on the overall economy of the country. In part, it helped to bring about a period of very high inflation from 1544 onwards.

Henry is generally credited with initiating the English Reformation – the process of transforming England from a Catholic country to a Protestant one – though his progress at the elite and mass levels is disputed, and the precise narrative not widely agreed. Certainly, in 1527, Henry, until then an observant and well-informed Catholic, appealed to the Pope for an annulment of his marriage to Catherine. No annulment was immediately forthcoming, the result in part of Charles V's control of the Papacy. The traditional narrative gives this refusal as the trigger for Henry's rejection of papal supremacy (which he had previously defended), though as historian A. F. Pollard has argued, even if Henry had not needed an annulment, Henry may have come to reject papal control over the governance of England purely for political reasons.

In any case, between 1532 and 1537, Henry instituted a number of statutes that dealt with the relationship between king and pope and hence the structure of the nascent Church of England. These included the Statute in Restraint of Appeals (passed 1533), which extended the charge of "praemunire" against all who introduced papal bulls into England, potentially exposing them to the death penalty if found guilty. Other acts included the Supplication against the Ordinaries and the Submission of the Clergy, which recognised Royal Supremacy over the church. The Ecclesiastical Appointments Act 1534 required the clergy to elect bishops nominated by the Sovereign. The Act of Supremacy in 1534 declared that the King was "the only Supreme Head on Earth of the Church of England" and the Treasons Act 1534 made it high treason, punishable by death, to refuse the Oath of Supremacy acknowledging the King as such. Similarly, following the passage of the Act of Succession 1533, all adults in the Kingdom were required to acknowledge the Act's provisions (declaring Henry's marriage to Anne legitimate and his marriage to Catherine illegitimate) by oath; those who refused were subject to imprisonment for life, and any publisher or printer of any literature alleging that the marriage to Anne was invalid subject to the death penalty. Finally, the Peter's Pence Act was passed, and it reiterated that England had "no superior under God, but only your Grace" and that Henry's "imperial crown" had been diminished by "the unreasonable and uncharitable usurpations and exactions" of the Pope. The King had much support from the Church under Cranmer.

Henry, to Thomas Cromwell's annoyance, insisted on parliamentary time to discuss questions of faith, which he achieved through the Duke of Norfolk. This led to the passing of the Act of Six Articles, whereby six major questions were all answered by asserting the religious orthodoxy, thus restraining the reform movement in England. It was followed by the beginnings of a reformed liturgy and of the Book of Common Prayer, which would take until 1549 to complete. The victory won by religious conservatives did not convert into much change in personnel, however, and Cranmer remained in his position. Overall, the rest of Henry's reign saw a subtle movement away from religious orthodoxy, helped in part by the deaths of prominent figures from before the break with Rome, especially the executions of Thomas More and John Fisher in 1535 for refusing to renounce papal authority. Henry established a new political theology of obedience to the crown that was continued for the next decade. It reflected Martin Luther's new interpretation of the fourth commandment ("Honour thy father and mother"), brought to England by William Tyndale. The founding of royal authority on the Ten Commandments was another important shift: reformers within the Church used the Commandments' emphasis on faith and the word of God, while conservatives emphasised the need for dedication to God and doing good. The reformers' efforts lay behind the publication of the "Great Bible" in 1539 in English. Protestant Reformers still faced persecution, particularly over objections to Henry's annulment. Many fled abroad, including the influential Tyndale, who was eventually executed and his body burned at Henry's behest.

When taxes once payable to Rome were transferred to the Crown, Cromwell saw the need to assess the taxable value of the Church's extensive holdings as they stood in 1535. The result was an extensive compendium, the "Valor Ecclesiasticus". In September of the same year, Cromwell commissioned a more general visitation of religious institutions, to be undertaken by four appointee visitors. The visitation focussed almost exclusively on the country's religious houses, with largely negative conclusions. In addition to reporting back to Cromwell, the visitors made the lives of the monks more difficult by enforcing strict behavioural standards. The result was to encourage self-dissolution. In any case, the evidence gathered by Cromwell led swiftly to the beginning of the state-enforced dissolution of the monasteries with all religious houses worth less than £200 vested by statute in the crown in January 1536. After a short pause, surviving religious houses were transferred one by one to the Crown and onto new owners, and the dissolution confirmed by a further statute in 1539. By January 1540 no such houses remained: some 800 had been dissolved. The process had been efficient, with minimal resistance, and brought the crown some £90,000 a year. The extent to which the dissolution of all houses was planned from the start is debated by historians; there is some evidence that major houses were originally intended only to be reformed. Cromwell's actions transferred a fifth of England's landed wealth to new hands. The programme was designed primarily to create a landed gentry beholden to the crown, which would use the lands much more efficiently. Although little opposition to the supremacy could be found in England's religious houses, they had links to the international church and were an obstacle to further religious reform.

Response to the reforms was mixed. The religious houses had been the only support of the impoverished, and the reforms alienated much of the population outside London, helping to provoke the great northern rising of 1536–1537, known as the Pilgrimage of Grace. Elsewhere the changes were accepted and welcomed, and those who clung to Catholic rites kept quiet or moved in secrecy. They would re-emerge during the reign of Henry's daughter Mary (1553–1558).

Apart from permanent garrisons at Berwick, Calais, and Carlisle, England's standing army numbered only a few hundred men. This was increased only slightly by Henry. Henry's invasion force of 1513, some 30,000 men, was composed of billmen and longbowmen, at a time when the other European nations were moving to hand guns and pikemen. The difference in capability was at this stage not significant, however, and Henry's forces had new armour and weaponry. They were also supported by battlefield artillery and the war wagon, relatively new innovations, and several large and expensive siege guns. The invasion force of 1544 was similarly well-equipped and organised, although command on the battlefield was laid with the dukes of Suffolk and Norfolk, which in the case of the latter produced disastrous results at Montreuil.

Henry's break with Rome incurred the threat of a large-scale French or Spanish invasion. To guard against this, in 1538, he began to build a chain of expensive, state-of-the-art defences, along Britain's southern and eastern coasts from Kent to Cornwall, largely built of material gained from the demolition of the monasteries. These were known as Henry VIII's Device Forts. He also strengthened existing coastal defence fortresses such as Dover Castle and, at Dover, Moat Bulwark and Archcliffe Fort, which he personally visited for a few months to supervise. Wolsey had many years before conducted the censuses required for an overhaul of the system of militia, but no reform resulted. In 1538–39, Cromwell overhauled the shire musters, but his work mainly served to demonstrate how inadequate they were in organisation. The building works, including that at Berwick, along with the reform of the militias and musters, were eventually finished under Queen Mary.

Henry is traditionally cited as one of the founders of the Royal Navy. Technologically, Henry invested in large cannon for his warships, an idea that had taken hold in other countries, to replace the smaller serpentines in use. He also flirted with designing ships personally – although his contribution to larger vessels, if any, is not known, it is believed that he influenced the design of rowbarges and similar galleys. Henry was also responsible for the creation of a permanent navy, with the supporting anchorages and dockyards. Tactically, Henry's reign saw the Navy move away from boarding tactics to employ gunnery instead. The Tudor navy was enlarged up to fifty ships (the "Mary Rose" was one of them), and Henry was responsible for the establishment of the "council for marine causes" to specifically oversee all the maintenance and operation of the Navy, becoming the basis for the later Admiralty.

At the beginning of Henry's reign, Ireland was effectively divided into three zones: the Pale, where English rule was unchallenged; Leinster and Munster, the so-called "obedient land" of Anglo-Irish peers; and the Gaelic Connaught and Ulster, with merely nominal English rule. Until 1513, Henry continued the policy of his father, to allow Irish lords to rule in the king's name and accept steep divisions between the communities. However, upon the death of the 8th Earl of Kildare, governor of Ireland, fractious Irish politics combined with a more ambitious Henry to cause trouble. When Thomas Butler, 7th Earl of Ormond died, Henry recognised one successor for Ormond's English, Welsh and Scottish lands, whilst in Ireland another took control. Kildare's successor, the 9th Earl, was replaced as Lord Lieutenant of Ireland by Thomas Howard, Earl of Surrey in 1520. Surrey's ambitious aims were costly, but ineffective; English rule became trapped between winning the Irish lords over with diplomacy, as favoured by Henry and Wolsey, and a sweeping military occupation as proposed by Surrey. Surrey was recalled in 1521, with Piers Butler – one of claimants to the Earldom of Ormond – appointed in his place. Butler proved unable to control opposition, including that of Kildare. Kildare was appointed chief governor in 1524, resuming his dispute with Butler, which had before been in a lull. Meanwhile, the Earl of Desmond, an Anglo-Irish peer, had turned his support to Richard de la Pole as pretender to the English throne; when in 1528 Kildare failed to take suitable actions against him, Kildare was once again removed from his post.

The Desmond situation was resolved on his death in 1529, which was followed by a period of uncertainty. This was effectively ended with the appointment of Henry FitzRoy, Duke of Richmond and the king's son, as lord lieutenant. Richmond had never before visited Ireland, his appointment a break with past policy. For a time it looked as if peace might be restored with the return of Kildare to Ireland to manage the tribes, but the effect was limited and the Irish parliament soon rendered ineffective. Ireland began to receive the attention of Cromwell, who had supporters of Ormond and Desmond promoted. Kildare, on the other hand, was summoned to London; after some hesitation, he departed for London in 1534, where he would face charges of treason. His son, Thomas, Lord Offaly was more forthright, denouncing the king and leading a "Catholic crusade" against the king, who was by this time mired in marital problems. Offaly had the Archbishop of Dublin murdered, and besieged Dublin. Offaly led a mixture of Pale gentry and Irish tribes, although he failed to secure the support of Lord Darcy, a sympathiser, or Charles V. What was effectively a civil war was ended with the intervention of 2,000 English troops – a large army by Irish standards – and the execution of Offaly (his father was already dead) and his uncles.

Although the Offaly revolt was followed by a determination to rule Ireland more closely, Henry was wary of drawn-out conflict with the tribes, and a royal commission recommended that the only relationship with the tribes was to be promises of peace, their land protected from English expansion. The man to lead this effort was Sir Antony St Leger, as Lord Deputy of Ireland, who would remain into the post past Henry's death. Until the break with Rome, it was widely believed that Ireland was a Papal possession granted as a mere fiefdom to the English king, so in 1541 Henry asserted England's claim to the Kingdom of Ireland free from the Papal overlordship. This change did, however, also allow a policy of peaceful reconciliation and expansion: the Lords of Ireland would grant their lands to the King, before being returned as fiefdoms. The incentive to comply with Henry's request was an accompanying barony, and thus a right to sit in the Irish House of Lords, which was to run in parallel with England's. The Irish law of the tribes did not suit such an arrangement, because the chieftain did not have the required rights; this made progress tortuous, and the plan was abandoned in 1543, not to be replaced.

The complexities and sheer scale of Henry's legacy ensured that, in the words of Betteridge and Freeman, "throughout the centuries, Henry has been praised and reviled, but he has never been ignored". Historian J.D. Mackie sums up Henry's personality and its impact on his achievements and popularity:

A particular focus of modern historiography has been the extent to which the events of Henry's life (including his marriages, foreign policy and religious changes) were the result of his own initiative and, if they were, whether they were the result of opportunism or of a principled undertaking by Henry. The traditional interpretation of those events was provided by historian A.F. Pollard, who in 1902 presented his own, largely positive, view of the king, lauding him, "as the king and statesman who, whatever his personal failings, led England down the road to parliamentary democracy and empire". Pollard's interpretation remained the dominant interpretation of Henry's life until the publication of the doctoral thesis of G. R. Elton in 1953.

Elton's book on "The Tudor Revolution in Government", maintained Pollard's positive interpretation of the Henrician period as a whole, but reinterpreted Henry himself as a follower rather than a leader. For Elton, it was Cromwell and not Henry who undertook the changes in government – Henry was shrewd, but lacked the vision to follow a complex plan through. Henry was little more, in other words, than an "ego-centric monstrosity" whose reign "owed its successes and virtues to better and greater men about him; most of its horrors and failures sprang more directly from [the king]".

Although the central tenets of Elton's thesis have since been questioned, it has consistently provided the starting point for much later work, including that of J. J. Scarisbrick, his student. Scarisbrick largely kept Elton's regard for Cromwell's abilities, but returned agency to Henry, who Scarisbrick considered to have ultimately directed and shaped policy. For Scarisbrick, Henry was a formidable, captivating man who "wore regality with a splendid conviction". The effect of endowing Henry with this ability, however, was largely negative in Scarisbrick's eyes: to Scarisbrick the Henrician period was one of upheaval and destruction and those in charge worthy of blame more than praise. Even among more recent biographers, including David Loades, David Starkey and John Guy, there has ultimately been little consensus on the extent to which Henry was responsible for the changes he oversaw or the correct assessment of those he did bring about.

This lack of clarity about Henry's control over events has contributed to the variation in the qualities ascribed to him: religious conservative or dangerous radical; lover of beauty or brutal destroyer of priceless artefacts; friend and patron or betrayer of those around him; chivalry incarnate or ruthless chauvinist. One traditional approach, favoured by Starkey and others, is to divide Henry's reign into two halves, the first Henry being dominated by positive qualities (politically inclusive, pious, athletic but also intellectual) who presided over a period of stability and calm, and the latter a "hulking tyrant" who presided over a period of dramatic, sometimes whimsical, change. Other writers have tried to merge Henry's disparate personality into a single whole; Lacey Baldwin Smith, for example, considered him an egotistical borderline neurotic given to great fits of temper and deep and dangerous suspicions, with a mechanical and conventional, but deeply held piety, and having at best a mediocre intellect.

Many changes were made to the royal style during his reign. Henry originally used the style "Henry the Eighth, by the Grace of God, King of England, France and Lord of Ireland". In 1521, pursuant to a grant from Pope Leo X rewarding Henry for his "Defence of the Seven Sacraments", the royal style became "Henry the Eighth, by the Grace of God, King of England and France, Defender of the Faith and Lord of Ireland". Following Henry's excommunication, Pope Paul III rescinded the grant of the title "Defender of the Faith", but an Act of Parliament (35 Hen 8 c 3) declared that it remained valid; and it continues in royal usage to the present day. Henry's motto was "Coeur Loyal" ("true heart"), and he had this embroidered on his clothes in the form of a heart symbol and with the word "loyal". His emblem was the Tudor rose and the Beaufort portcullis. As king, Henry's arms were the same as those used by his predecessors since Henry IV: "Quarterly, Azure three fleurs-de-lys Or (for France) and Gules three lions passant guardant in pale Or (for England)".

In 1535, Henry added the "supremacy phrase" to the royal style, which became "Henry the Eighth, by the Grace of God, King of England and France, Defender of the Faith, Lord of Ireland and of the Church of England in Earth Supreme Head". In 1536, the phrase "of the Church of England" changed to "of the Church of England and also of Ireland". In 1541, Henry had the Irish Parliament change the title "Lord of Ireland" to "King of Ireland" with the Crown of Ireland Act 1542, after being advised that many Irish people regarded the Pope as the true head of their country, with the Lord acting as a mere representative. The reason the Irish regarded the Pope as their overlord was that Ireland had originally been given to King Henry II of England by Pope Adrian IV in the 12th century as a feudal territory under papal overlordship. The meeting of Irish Parliament that proclaimed Henry VIII as King of Ireland was the first meeting attended by the Gaelic Irish chieftains as well as the Anglo-Irish aristocrats. The style "Henry the Eighth, by the Grace of God, King of England, France and Ireland, Defender of the Faith and of the Church of England and also of Ireland in Earth Supreme Head" remained in use until the end of Henry's reign.








</doc>
<doc id="14189" url="https://en.wikipedia.org/wiki?curid=14189" title="Haryana">
Haryana

Haryana (), carved out of the former state of East Punjab on 1November 1966 on linguistic basis, is one of the 29 states in India. Situated in North India with less than 1.4% () of India's land area, it is ranked 21st in terms of area. Chandigarh is the capital, Faridabad in National Capital Region is the most populous city of the state and the Gurugram is the financial hub of NCR with major Fortune 500 companies located in it. Haryana has 6 administrative divisions, 22 districts, 72 sub-divisions, 93 revenue tehsils, 50 sub-tehsils, 140 community development blocks, 154 cities and towns, 6,841 villages and 6212 villages panchayats.

As the largest recipient of investment per capita since 2000 in India, and among one of the wealthiest and most economically developed regions in South Asia, Haryana has the sixth highest per capita income among Indian states and union territories at against the national average of for year 2016–17. Haryana's 2017-18 estimated state GSDP of US$95 billion (52% services, 30% industries and 18% agriculture) is growing at 12.96% 2012-17 CAGR and placed on the 14th position behind only much bigger states, is also boosted by 30 SEZs (mainly along DMIC, ADKIC and DWPE in NCR), 7% national agricultural exports, 60% of national Basmati rice export, 67% cars, 60% motorbikes, 50% tractors and 50% refrigerators produced in India. Faridabad has been described as eighth fastest growing city in the world and third most in India by City Mayors Foundation survey. In services, Gurugram ranks number 1 in India in IT growth rate and existing technology infrastructure, and number 2 in startup ecosystem, innovation and livability (Nov 2016).

Among the world's oldest and largest ancient civilizations, the Indus Valley Civilization sites at Rakhigarhi village in Hisar district and Bhirrana in Fatehabad district are 9,000 years old. Rich in history, monuments, heritage, flora and fauna, human resources and tourism with well developed economy, national highways and state roads, it is bordered by Himachal Pradesh to the north-east, by river Yamuna along its eastern border with Uttar Pradesh, by Rajasthan to the west and south, and Ghaggar-Hakra River flows along its northern border with Punjab. Since Haryana surrounds the country's capital Delhi on three sides (north, west and south), consequently a large area of Haryana is included in the economically-important National Capital Region for the purposes of planning and development.

The name Haryana is found in the works of the 12th-century AD Apabhramsha writer Vibudh Shridhar (VS 1189–1230). The name Haryana has been derived from the Sanskrit words "Hari" (the Hindu god Vishnu) and "ayana" (home), meaning "the Abode of God". However, scholars such as Muni Lal, Murli Chand Sharma, HA Phadke and Sukhdev Singh Chib believe that the name comes from a compound of the words "Hari" (Sanskrit "Harit", "green") and "Aranya" (forest).

The Vedic state of Brahmavarta is claimed to be located in south Haryana, where the initial Vedic scriptures were composed after the great floods some 10,000 years ago.

Rakhigarhi village in Hisar district and Bhirrana in Fatehabad district are home to the largest and one of the world's oldest ancient Indus Valley Civilization sites, dated at over 9,000 years old. Evidence of paved roads, a drainage system, a large-scale rainwater collection storage system, terracotta brick and statue production, and skilled metal working (in both bronze and precious metals) have been uncovered. According to archaeologists, Rakhigarhi may be the origin of Harappan civilisation, which arose in the Ghaggar basin in Haryana and gradually and slowly moved to the Indus valley.

Ancient bronze and stone idols of Jain Tirthankara were found in archaeological expeditions in Badli, Bhiwani (Ranila, Charkhi Dadri, Badhara village), Dadri, Gurgaon (Ferozpur Jhirka), Hansi, Hisar (Agroha), Kasan, Nahad, Narnaul, Pehowa, Rewari, Rohad, Rohtak (Asthal-Bohar) and Sonepat in Haryana.

After the sack of Bhatner fort during the Timurid conquests of India in 1398, Timur attacked and sacked the cities of Sirsa, Fatehabad, Sunam, Kaithal and Panipat. When he reached the town of Sarsuti, the residents, who were mostly non-Muslims, fled and were chased by a detachment of Timur's troops, with thousands of them being killed and looted by the troops. From there he travelled to Fatehabad, whose residents fled and a large number of those remaining in the town were massacred. The Ahirs resisted him at Ahruni but were defeated, with thousands being killed and many being taken prisoners while the town was burnt to ashes. From there he travelled to Tohana, whose Jat inhabitants were stated to be robbers according to Sharaf ad-Din Ali Yazdi. They tried to resist but were defeated and fled. Timur's army pursued and killed 200 Jats, while taking many more as prisoners. He then sent a detachment to chase the fleeing Jats and killed 2,000 of them while their wives and children were enslaved and their property plundered. From there he proceeded to Kaithal whose residents were massacred and plundered, destroying all villages along the way. On the next day, he came to Assandh whose residents were "fire-worshippers" according to Yazdi, and had fled to Delhi. Next he travelled to and subdued Tughlaqpur fort and Salwan before reaching Panipat whose residents had already fled. He then marched on to Loni fort.

The area that is now Haryana has been ruled by some of the major empires of India. Panipat is known for three seminal battles in the history of India. In the First Battle of Panipat (1526), Babur defeated the Lodis. In the Second Battle of Panipat (1556), Akbar defeated the local Haryanvi Hindu Emperor of Delhi, who belonged to Rewari. Hem Chandra Vikramaditya had earlier won 22 battles across India from Punjab to Bengal, defeating Mughals and Afghans. Hemu had defeated Akbar's forces twice at Agra and the Battle of Delhi in 1556 to become the last Hindu Emperor of India with a formal coronation at Purana Quila in Delhi on 7 October 1556. In the Third Battle of Panipat (1761), the Afghan king Ahmad Shah Abdali defeated the Marathas.

Haryana as a state came into existence on 1November 1966 the Punjab Reorganisation Act (1966). The Indian government set up the Shah Commission under the chairmanship of Justice JC Shah on 23 April 1966 to divide the existing state of Punjab and determine the boundaries of the new state of Haryana after consideration of the languages spoken by the people. The commission delivered its report on 31May 1966 whereby the then-districts of Hisar, Mahendragarh, Gurgaon, Rohtak and Karnal were to be a part of the new state of Haryana. Further, the tehsils of Jind and Narwana in the Sangrur district — along with Naraingarh, Ambala and Jagadhri — were to be included.

The commission recommended that the tehsil of Kharad, which includes Chandigarh, the state capital of Punjab, should be a part of Haryana. However, only a small portion of Kharad was given to Haryana. The city of Chandigarh was made a union territory, serving as the capital of both Punjab and Haryana.

Bhagwat Dayal Sharma became the first Chief Minister of Haryana.

According to the 2011 census, of total 25,350,000 population of Haryana, Hindus (87.46%) constitute the majority of the state's population with Muslims (7.03%) (mainly Meos) and Sikhs (4.91%) being the largest minorities.

The Jats are the dominant caste in Haryana, and form nearly 17% of the state's electorate. The rest of the electorate includes OBC (24%, including Ahirs/Yadavs); upper-caste (30%, including Brahmins, Baniyas and Punjabis); and Dalits (21%).gurjars are 13 percent in haryana 

Muslims are mainly found in the Mewat and Nuh districts. Haryana has the second largest Sikh population in India after Punjab, and they mostly live in the districts adjoining Punjab, such as Hisar, Sirsa, Jind, Fatehabad, Kaithal, Kurukshetra, Ambala, Narnaul and Panchkula karnal.

Hindi was the sole official language of Haryana till 2010 and it is spoken by the majority of the population (87.31%). Haryana has 70% rural population who primarily speak Haryanvi dialect of Hindi, as well as other related dialects, such as Bagri and Mewati.

Haryana has its own unique traditional folk music, folk dances, saang (folk theater), cinema, belief system such as Jathera (ancestral worship), and arts such as Phulkari and Shisha embroidery.

Folk music and dances of Haryana are based on satisfying cultural needs of primarily agrarian and martial natures of Haryanavi tribes.

Haryanvi musical folk theater main types are Saang, Rasa lila and Ragini. The Saang and Ragini form of theater was popularised by Lakhmi Chand.

Haryanvi folk dances and music have fast energetic movements. Three popular categories of dance are: festive-seasonal, devotional, and ceremonial-recreational. The festive-seasonal dances and songs are Gogaji/Gugga, Holi, Phaag, Sawan, Teej. The devotional dances and songs are Chaupaiya, Holi, Manjira, Ras Leela, Raginis). The ceremonial-recreational dances and songs are of following types: legendary bravery (Kissa and Ragini of male warriors and female Satis), love and romance (Been and its variant Nāginī dance, and Ragini), ceremonial (Dhamal Dance, Ghoomar, Jhoomar (male), Khoria, Loor, and Ragini).

Haryanvi folk music are based on day to day themes and injecting earthy humor enlivens the feel of the songs. Haryanvi music takes two main forms: "Classical folk music" and "Desi Folk music" (Country Music of Haryana), and sung in the form of ballads and love, valor and bravery, harvest, happiness and pangs of parting of lovers.

Classical Haryanvi folk music is based on Indian classical music. Hindustani classical ragas, learnt in gharana parampara of guru–shishya tradition, are used to sing songs of heroic bravery (such as Alha-Khand (1663-1202 CE) about bravery of Alha and Udal, Jaimal Fatta of Maharana Udai Singh II), Brahmas worship and festive seasonal songs (such as Teej, Holi and Phaag songs of Phalgun month near Holi). Kissa legendary folklores of bravery and love such as Nihalde Sultan, Sati Manorama, Jai Singh ki Mrityu, Saran de, etc. are some of the most popular folklores. Bravery songs are sung in high pitch.

Desi Haryanvi folk music (Haryanvi country folk music) The country-side or desi (native) form of Haryanvi music is based on Raag Bhairvi, Raag Bhairav, Raag Kafi, Raag Jaijaivanti, Raag Jhinjhoti and Raag Pahadi and used for celebrating community bonhomie to sing seasonal songs, ballads, ceremonial songs (wedding, etc.) and related religious legendary tales such as Puran Bhagat. Relationship and songs celebrating love and life are sung in medium pitch. Ceremonial and religious songs are sung in low pitch. Young girls and women usually sing entertaining and fast seasonal, love, relationship and friendship related songs such as Phagan (song for eponymous season/month), Katak (songs for the eponymous season/month), Samman (songs for the eponymous season/month), bande-bandi (male-female duet songs), sathne (songs of sharing heartfelt feelings among female friends). Older women usually sing devotional Mangal Geet (auspicious songs) and ceremonial songs such as Bhajan, Bhat (wedding gift to the mother of bride or groom by her brother), Sagai, Ban (Hindu wedding ritual where pre-wedding festivities starts), Kuan-Poojan (a custom that is performed to welcome the birth of male child by worshiping the well or source of drinking water), Sanjhi and Holi festival.

Music and dance for Haryanvi people is a great way of demolishing societal differences as folk singers are highly esteemed and they are sought after and invited for the events, ceremonies and special occasions regardless of their caste or status. These inter-caste songs are fluid in nature, and never personalized for any specific caste, and they are sung collectively by women from different strata, castes, dialects. These songs do transform fluidly in dialect, style, words, etc. This adoptive style can be seen from the adoption of tunes of Bollywood movie songs into Haryanvi songs. Despite this continuous fluid transforming nature, Haryanvi songs have a distinct style of their own as explained above.

81% people of Haryana are vegetarian, and cuisine of Haryana is based on fresh, earthy and wholesome ethos of its agrarian culture, where staples are roti, saag, vegetarian sabzi and abundance of milk products such as homemade nooni or tindi ghee, ghee (clarified butter), milk, lassi, kheer.

Haryana is a landlocked state in northern India. It is between 27°39' to 30°35' N latitude and between 74°28' and 77°36' E longitude. The total geographical area of the state is 4.42 m ha, which is 1.4% of the geographical area of the country. The altitude of Haryana varies between 700 and 3600 ft (200 metres to 1200 metres) above sea level. Haryana has only 4% (compared to national 21.85%) area under forests.

Haryana has four main geographical features.

The Yamuna, tributary of Ganges, flows along the state's eastern boundary.

Northern Haryana has several north-east to south-west flowing rivers originating from the Sivalik Hills of Himalayas, such as Ghaggar-Hakra (palaeochannel of vedic Sarasvati river), Chautang (paleochannel of vedic Drishadvati river, tributary of Ghagghar), Tangri river (tributary of Ghagghar), Kaushalya river (tributary of Ghagghar), Markanda River (tributary of Ghagghar), Sarsuti, Dangri, Somb river. Haryana's main seasonal river, the Ghaggar-Hakra, known as Ghaggar before the Ottu barrage and as the Hakra downstream of the barrage, rises in the outer Himalayas, between the Yamuna and the Satluj and enters the state near Pinjore in the Panchkula district, passes through Ambala and Sirsa, it reaches Bikaner in Rajasthan and runs for before disappearing into the deserts of Rajasthan. The seasonal Markanda River, known as the "Aruna" in ancient times, originates from the lower Shivalik Hills and enters Haryana west of Ambala, and swells into a raging torrent during monsoon is notorious for its devastating power, carries its surplus water on to the "Sanisa Lake" where the "Markanda" joins the "Sarasuti" and later the "Ghaggar".

Southern Haryana has several south-east to north-west flowing seasonal rivulets originating from the Aravalli Range in and around the hills in Mewat region, including Sahibi River (called Najafgarh drain in Delhi), Dohan river (tributary of Sahibi, originates at Mandoli village near Neem Ka Thana in Jhunjhunu district of Rajasthan and then disappears in Mahendragarh district), Krishnavati river (former tributary of Sahibi river, originates near Dariba and disappears in Mahendragarh district much before reaching Sahibi river) and Indori river (longest tributary of Sahibi River, originates in Sikar district of Rajasthan and flows to Rewari district of Haryana), these once were tributaries of the Drishadwati/Saraswati river.

Major canals are Western Yamuna Canal,

Major dams are Kaushalya Dam in Panchkula district, Hathnikund Barrage and Tajewala Barrage on Yamuna in Yamunanagar district, Pathrala barrage on Somb river in Yamunanagar district, ancient Anagpur Dam near Surajkund in Faridabad district, and Ottu barrage on Ghaggar-Hakra River in Sirsa district.

Major lakes are Dighal Wetland, Basai Wetland, Badkhal Lake in Faridabad, holy Brahma Sarovar and Sannihit Sarovar in Kurukshetra, Blue Bird Lake in Hisar, Damdama Lake at Sohna in Gurgram district, Hathni Kund in Yamunanagar district, Karna Lake at Karnal, ancient Surajkund in Faridabad, and Tilyar Lake in Rohtak.

The "Haryana State Waterbody Management Board" is responsible for rejuvenation of 14,000 Johads of Haryana and up to 60 lakes in National Capital Region falling within the Haryana state.

Only hot spring of Haryana is the Sohna Sulphur Hot Spring at Sohna in Gurugram district. Tosham Hill range has several sacred sulphur pond of religious significance that are revered for the healing impact of sulfur, such as "Pandu Teerth Kund", "Surya Kund", "Kukkar Kund", "Gyarasia Kund" or "Vyas Kund".

Seasonal waterfalls include Tikkar Taal twin lakes at Morni hiills, Dhosi Hill in Mahendragarh district and Pali village on outskirts of Faridabad.

Haryana is extremely hot in summer at around and mild in winter. The hottest months are May and June and the coldest December and January. The climate is arid to semi-arid with average rainfall of 354.5 mm. Around 29% of rainfall is received during the months from July to September, and the remaining rainfall is received during the period from December to February.

Forest Cover in the state in 2013 was 3.59% (1586 km) and the Tree Cover in the state was 2.90% (1282 km), giving a total forest and tree Cover of 6.49%. In 2016-17, 18,412 hectares were brought under tree cover by planting 14.1 million seedlings. Thorny, dry, deciduous forest and thorny shrubs can be found all over the state. During the monsoon, a carpet of grass covers the hills. Mulberry, eucalyptus, pine, kikar, shisham and babul are some of the trees found here. The species of fauna found in the state of Haryana include black buck, nilgai, panther, fox, mongoose, jackal and wild dog. More than 450 species of birds are found here.

Haryana has two national parks, eight wildlife sanctuaries, two wildlife conservation areas, four animal and bird breeding centers, one deer park and three zoos, all of which are managed by the Haryana Forest Department of the Government of Haryana.

Haryana Environment Protection Council is the advisory committee and |Department of Environment, Haryana]] is the department responsible for administration of environment. Areas of Haryana surrounding Delhi NCR are most polluted. During smog of November 2017, Air quality index of Gurugram and Faridabad showed that the density of Fine particulates (2.5 PM diameter) was an average of 400 PM and monthly average of Haryana was 60 PM. Other sources of pollution are exhaust gases from old vehicles, stone crushers and brick kiln. Haryana has 75 lakh (7,500,000) old vehicles, of which 40% are old more polluting vehicles, besides 500,000 new vehicles are added every year. Other majorly polluted cities are Bhiwani, Bahadurgarh, Dharuhera, Hisar and Yamunanagar.

The state is divided into divided into 6 revenue divisions, 5 Police Ranges and 3 Police Commissionerates (c. January 2017). Six revenue divisions are: Ambala, Rohtak, Gurgaon, Hisar, Karnal and Faridabad. Haryana has 10 municipal corporations (Gurigram, Faridabad, Ambala, Panchkula, Yamunanagar, Rohtak, Hisar, Panipat, Karnal and Sonepat), 18 municipal councils and 52 municipalities (c. Jan 2018).

Within these there are 22 districts, 72 sub-divisions, 93 tehsils, 50 sub-tehsils, 140 blocks, 154 cities and towns, 6,841 villages, 6212 villages panchayats and numerous smaller dhanis.

Haryana Police force is the law enforcement agency of Haryana. Five Police Ranges are Ambala, Hissar, Karnal, Rewari and Rohtak. Three Police Commissionerates are Faridabad, Gurgaon and Panchkula. Cybercrime investigation cell is based in Gurgaon's Sector 51.

The highest judicial authority in the state is the Punjab and Haryana High Court, with next higher right of appeal to Supreme Court of India. Haryana uses e-filing facility.

The Common Service Centres (CSCs) have been upgraded in all districts to offer hundreds of e-services to citizens, including application of new water connection, sewer connection, electricity bill collection, ration card member registration, result of HBSE, admit cards for board examinations, online admission form for government colleges, long route booking of buses, admission forms for Kurukshetra University and HUDA plots status inquiry. Haryana has become the first state to implement Aadhaar-enabled birth registration in all the districts. Thousands of all traditional offline state and central government services are also available 24/7 online through single unified UMANG app and portal as part of Digital India initiative.

Haryana's 14th placed 12.96% 2012-17 CAGR estimated 2017-18 GSDP of US$95 billion is split in to 52% services, 30% industries and 18% agriculture.

Services sector is split across 45% in real estate and financial & professional services, 26% trade and hospitality, 15% state and central govt employees, and 14% transport and logistics & warehousing. In IT services, Gurugram ranks number 1 in India in growth rate and existing technology infrastructure, and number 2 in startup ecosystem, innovation and livability (Nov 2016).

Industries sector is split across 69% manufacturing, 28% construction, 2% utilities and 1% mining. In industrial manufacturing, Haryana produces India's 67% of passenger cars, 60% of motorcycles, 50% of tractors and 50% of the refrigerators.

Services and industrial sectors are boosted by 7 operational SEZs and additional 23 formally approved SEZs (20 already notified and 3 in-principal approval) that are mostly spread along the Delhi–Mumbai Industrial Corridor, Amritsar Delhi Kolkata Industrial Corridor and Delhi Western Peripheral Expressway in NCR).

Agriculture sector is split across 93% crops and livestock, 4% commercial forestry and logging, and 2% fisheries. Agriculture sector of Haryana, with only less than 1.4% area of India, contributes 15% food grains to the central food security public distribution system, and 7% of total national agricultural exports including 60% of total national Basmati rice export.

Haryana is traditionally an agrarian society of zamindars (owner-cultivator farmers). The Green Revolution in Haryana of 1960s combined with completion of Bhakra Dam in 1963 and Western Yamuna Command Network canal system in 1970s resulted in the significantly increased food grain production.

In 2015-2016, Haryana produced the following principal crops: 13,352,000 tonne wheat, 4,145,000 tonne rice, 7,169,000 tonne sugarcane, 993,000 tonne cotton and 855,000 tonne oilseeds (mustard seed, sunflower, etc.).

Vegetable production was: Potato 853,806 tonnes, Onion 705,795 tonnes, Tomato 675,384 tonnes, Cauliflower 578,953 tonnes, Leafy Vegetables 370,646 tonnes, Brinjal 331,169 tonnes, guard 307,793 tonnes, Peas 111,081 tonnes and others 269,993 tonnes.

Fruits production was: Citrus 301,764 tonnes, Guava 152,184 tonnes, Mango 89,965 tonnes, Chikoo 16,022 tonnes, Aonla 12,056 tonnes and other fruits 25,848 tonnes.

Spices production was: Garlic 40,497 tonnes, Fenugreek 9,348 tonnes, Ginger 4,304 tonnes and others 840 tonnes.

Cut flowers production was: Marigold 61,830 tonnes, Gladiolus 24,486,200 lakh, Rose 18,611,600 lakh and other 6,913,000 lakh.

Medicinal plants production was: Aloe vera 1403 tonnes and Stevia 13 tonnes.

Haryana is well known for its high-yield Murrah buffalo. Other breeds of cattle native to Haryana are Haryanvi, Mewati, Sahiwal and Nili-Ravi.

To support its agrarian economy, both central government (Central Institute for Research on Buffaloes, Central Sheep Breeding Farm, National Research Centre on Equines, Central Institute of Fisheries, National Dairy Research Institute, Indian Institute of Wheat and Barley Research and National Bureau of Animal Genetic Resources) and state government (CCS HAU, LUVAS, Government Livestock Farm, Regional Fodder Station and Northern Region Farm Machinery Training and Testing Institute) have opened several institutes for research and education.


Haryana State has always given high priority to the expansion of electricity infrastructure, as it is one of the most important inputs for the development of the state. Haryana was the first state in the country to achieve 100% rural electrification in 1970 as well as the first in the country to link all villages with all-weather roads and provide safe drinking water facilities throughout the state.

Power in the state are:

Haryana has a total road length of , including 29 national highways, state highways, Major District Roads (MDR) and Other District Roads (ODR) (c. December 2017). A fleet of 3,864 Haryana Roadways buses covers a distance of 1.15 million km per day, and it was the first state in the country to introduce luxury video coaches.

Ancient Delhi Multan Road and Grand Trunk Road, South Asia's oldest and longest major roads, pass through Haryana. GT Road passes through the districts of Sonipat, Panipat, Karnal, Kurukshetra and Ambala in north Haryana where it enters Delhi and subsequently the industrial town of Faridabad on its way. The Kundli-Manesar-Palwal Expressway(KMP) will provide a high-speed link to northern Haryana with its southern districts such as Sonepat, Gurgaon, Jhajjar and Faridabad.

The Delhi-Agra Expressway (NH-2) that passes through Faridabad is being widened to six lanes from current four lanes. It will further boost Faridabad's connectivity with Delhi.

Rail network in Haryana is covered by 5 rail divisions under 3 rail zones. Diamond Quadrilateral High-speed rail network, Eastern Dedicated Freight Corridor (72 km) and Western Dedicated Freight Corridor (177 km) pass through Haryana.

Bikaner railway division of North Western Railway zone manages rail network in western and southern Haryana covering Bhatinda-Dabwali-Hanumangarh line, Rewari-Bhiwani-Hisar-Bathinda line, Hisar-Sadulpur line and Rewari-Loharu-Sadulpur line. Jaipur railway division of North Western Railway zone manages rail network in south-west Haryana covering Rewari-Reengas-Jaipur line, Delhi-Alwar-Jaipur line and Loharu-Sikar line.

Delhi railway division of Northern Railway zone manages rail network in north and east and central Haryana covering Delhi-Ambala line, Delhi-Rohtak-Tohana line, Rewari–Rohtak line, Jind-Sonepat line and Delhi-Rewari line. Agra railway division of North Central Railway zone manages another very small part of network in south-east Haryana covering Palwal-Mathura line only.

Ambala railway division of Northern Railway zone manages small part of rail network in north-east Haryana covering Ambala-Yamunanagar line, Ambala-Kurukshetra line and UNESCO World Heritage Kalka–Shimla Railway.

Delhi Metro connects the national capital Delhi with NCR cities such as Faridabad, Gurugram and Bahadurgarh. Faridabad has the longest metro network in the NCR Region consisting of 9 stations and track length being 14 km.

The Haryana and Delhi governments have constructed the international standard Delhi Faridabad Skyway, the first of its kind in North India, to connect Delhi and Faridabad.

Haryana has a statewide network of telecommunication facilities. Haryana Government has its own statewide area network by which all government offices of 22 districts and 126 blocks across the state are connected with each other thus making it the first SWAN of the country. Bharat Sanchar Nigam Limited and most of the leading private sector players (such as Reliance Infocom, Tata Teleservices, Bharti Telecom, Idea Vodafone Essar, Aircel, Uninor and Videocon) have operations in the state. Important areas around Delhi are an integral part of the local Delhi Mobile Telecommunication System. This network system would easily cover major towns like Faridabad and Gurgaon.

Electronic media channels include, MTV, 9XM, Star Group, SET Max, News Time, NDTV 24x7 and Zee Group. The radio stations include All India Radio and other FM stations.

The major newspapers of Haryana include "Dainik Bhaskar", "Punjab Kesari", "Jag Bani", "Dainik Jagran", "The Tribune", "Amar Ujala", "Hindustan Times", "Dainik Tribune", "The Times of India" and "Hari-Bhumi".

The Total Fertility Rate of Haryana is 2.3. The Infant Mortality Rate is 41 (SRS 2013) and Maternal Mortality Ratio is 146 (SRS 2010–2012).

Literacy rate in Haryana has seen an upward trend and is 76.64 percent as per 2011 population census. Male literacy stands at 85.38 percent, while female literacy is at 66.67 percent. In 2001, the literacy rate in Haryana stood at 67.91 percent of which male and female were 78.49 percent and 55.73 percent literate respectively. , Gurgaon city had the highest literacy rate in Haryana at 86.30% followed by Panchkula at 81.9 per cent and Ambala at 81.7 percent. In terms of districts, Rewari had the highest literacy rate in Haryana at 74%, higher than the national average of 59.5%: male literacy was 79%, and female 67%.

Haryana Board of School Education, established in September 1969 and shifted to Bhiwani in 1981, conducts public examinations at middle, matriculation, and senior secondary levels twice a year. Over seven lakh candidates attend annual examinations in February and March; 150,000 attend supplementary examinations each November. The Board also conducts examinations for Haryana Open School at senior and senior secondary levels twice a year. The Haryana government provides free education to women up to the bachelor's degree level.

In 2015-2016, there were nearly 20,000 schools, including 10,100 state government schools (36 Aarohi Schools, 11 Kasturba Gandhi Balika Vidyalayas, 21 Model Sanskriti Schools, 8744 government primary school, 3386 government middle school, 1284 government high school and 1967 government senior secondary schools), 7,635 private schools (200 aided, 6612 recognized unaided, and 821 unrecognied unaided private schools.)and several hundred other central government and private schools such as Kendriya Vidyalaya, Indian Army Public Schools, Jawahar Navodaya Vidyalaya and DAV schools affiliated to central government's CBSE and ICSE school boards.

Haryana has 29 universities and 299 colleges, including 115 government colleges, 88 govt-aided colleges and 96 self-finance colleges (c. Jan 2018). Hisar has three universities: Chaudhary Charan Singh Haryana Agricultural University - Asia's largest agricultural university, Guru Jambheshwar University of Science and Technology, Lala Lajpat Rai University of Veterinary & Animal Sciences); several national agricultural and veterinary research centres (National Research Centre on Equines), Central Sheep Breeding Farm, National Institute on Pig Breeding and Research, Northern Region Farm Machinery Training and Testing Institute and Central Institute for Research on Buffaloes (CIRB); and more than 20 colleges including Maharaja Agrasen Medical College, Agroha.

Union Minister Ravi Shankar Prasad announced on 27 February 2016 that National Institute of Electronics and Information Technology (NIELIT) would be set up in Kurukshetra to provide computer training to youth and a Software Technology Park of India (STPI) would be set up in Panchkula’s existing HSIIDC IT Park in Sector 23. Hindi and English are compulsory languages in schools whereas Punjabi, Sanskrit and Urdu are chosen as optional languages.

In the 2010 Commonwealth Games at Delhi, 22 out of 38 gold medals that India won came from Haryana. During the 33rd National Games held in Assam in 2007, Haryana stood first in the nation with a medal tally of 80, including 30 gold, 22 silver and 28 bronze medals.

The 1983 World-Cup-winning captain Kapil Dev is from Haryana. Nahar Singh Stadium was built in Faridabad in the year 1981 for international cricket. This ground has the capacity to hold around 25,000 people as spectators. Tejli Sports Complex is an Ultra-Modern sports complex in Yamuna Nagar. Tau Devi Lal Stadium in Gurgaon is a multi-sport complex.

Chief Minister of Haryana Manohar Lal Khattar announced the "Haryana Sports and Physical Fitness Policy", a policy to support 26 Olympic sports, on 12 January 2015 with the words "We will develop Haryana as the sports hub of the country."

Haryana is home to Haryana Gold, one of India's eight professional basketball teams which compete in the country's UBA Pro Basketball League.





</doc>
<doc id="14190" url="https://en.wikipedia.org/wiki?curid=14190" title="Himachal Pradesh">
Himachal Pradesh

Himachal Pradesh (; literally "snow-laden province") is an Indian state located in North India. Situated in the Western Himalayas, it is bordered by states of Jammu and Kashmir on the north, Punjab on the west, Haryana on the southwest, Uttarakhand on the southeast, and the Tibet Autonomous Region on the east. At its southernmost point, it also touches the state of Uttar Pradesh. The state's name was coined from the Sanskrit—"Him" means 'snow' and "achal" means 'land' or 'abode'—by acharya Diwakar Datt Sharma, one of the state's eminent Sanskrit scholars.

The state is spread across valleys. About 90% of the state's population lives in rural areas. Many perennial rivers flow in the state with numerous hydropower plants producing surplus electricity that is sold to other states, such as Delhi, Punjab and West Bengal. Tourism and agriculture are also important constituents of the state's economy. The state has one of the highest per-capita incomes among the Indian states and union territories.

Shimla district has the largest urban population in the state at 25%. The villages have good connectivity with roads, public health centres, and high-speed broadband. Practically all houses have a toilet and 100% hygiene has been achieved in the state. Notable actions by the state government include a ban on polyethylene bags and tobacco products. According to a survey of CMS - India Corruption Study 2017, Himachal Pradesh is India's least corrupt state.

The history of the area that now constitutes Himachal Pradesh dates to the Indus valley civilisation that flourished between 2250 and 1750 BCE. Tribes such as the Koili, Hali, Dagi, Dhaugri, Dasa, Khasa, Kinnar, and Kirat inhabited the region from the prehistoric era.

During the Vedic period, several small republics known as "Janapada" existed which were later conquered by the Gupta Empire. After a brief period of supremacy by King Harshavardhana, the region was divided into several local powers headed by chieftains, including some Rajput principalities. These kingdoms enjoyed a large degree of independence and were invaded by Delhi Sultanate a number of times. Mahmud Ghaznavi conquered Kangra at the beginning of the 10th century. Timur and Sikander Lodi also marched through the lower hills of the state and captured a number of forts and fought many battles. Several hill states acknowledged Mughal suzerainty and paid regular tribute to the Mughals.
The Kingdom of Gorkha conquered many kingdoms and came to power in Nepal in 1768. They consolidated their military power and began to expand their territory. Gradually, the Kingdom of Nepal annexed Sirmour and Shimla. Under the leadership of Amar Singh Thapa, the Nepali army laid siege to Kangra. They managed to defeat Sansar Chand Katoch, the ruler of Kangra, in 1806 with the help of many provincial chiefs. However, the Nepali army could not capture Kangra fort which came under Maharaja Ranjeet Singh in 1809. After the defeat, they began to expand towards the south of the state. However, Raja Ram Singh, Raja of Siba State, captured the fort of Siba from the remnants of Lahore Darbar in Samvat 1846, during the First Anglo-Sikh War.

They came into direct conflict with the British along the "tarai" belt after which the British expelled them from the provinces of the Satluj. The British gradually emerged as the paramount power in the region. In the revolt of 1857, or first Indian war of independence, arising from a number of grievances against the British, the people of the hill states were not as politically active as were those in other parts of the country. They and their rulers, with the exception of Bushahr, remained more or less inactive. Some, including the rulers of Chamba, Bilaspur, Bhagal and Dhami, rendered help to the British government during the revolt.
The British territories came under the British Crown after Queen Victoria's proclamation of 1858. The states of Chamba, Mandi and Bilaspur made good progress in many fields during the British rule. During World War I, virtually all rulers of the hill states remained loyal and contributed to the British war effort, both in the form of men and materials. Among these were the states of Kangra, Jaswan, Datarpur, Guler, Rajgarh, Nurpur, Chamba, Suket, Mandi, and Bilaspur.

After independence, the Chief Commissioner's Province of Himachal Pradesh was organized on 15 April 1948 as a result of integration of 28 petty princely states (including feudal princes and "zaildars") in the promontories of the western Himalayas. These were known as the Simla Hills States and four Punjab southern hill states under the Himachal Pradesh (Administration) Order, 1948 under Sections 3 and 4 of the Extra-Provincial Jurisdiction Act, 1947 (later renamed as the Foreign Jurisdiction Act, 1947 vide A.O. of 1950). The State of Bilaspur was merged into Himachal Pradesh on 1 July 1954 by the Himachal Pradesh and Bilaspur (New State) Act, 1954.

Himachal became a Part 'C' state on 26 January 1950 with the implementation of the Constitution of India and the Lieutenant Governor was appointed. The Legislative Assembly was elected in 1952. Himachal Pradesh became a union territory on 1 November 1956. Some areas of Punjab State— namely Simla, Kangra, Kullu and Lahul and Spiti Districts, Nalagarh tehsil of Ambala District, Lohara, Amb and Una kanungo circles, some area of Santokhgarh kanungo circle and some other specified area of Una tehsil of Hoshiarpur District, besides some parts of Dhar Kalan Kanungo circle of Pathankot tehsil of Gurdaspur District—were merged with Himachal Pradesh on 1 November 1966 on enactment by Parliament of Punjab Reorganisation Act, 1966. On 18 December 1970, the State of Himachal Pradesh Act was passed by Parliament, and the new state came into being on 25 January 1971. Himachal became the 18th state of the Indian Union with Dr. Yashwant Singh Parmar as its first chief minister.

Himachal is in the western Himalayas. Covering an area of , it is a mountainous state. Most of the state lies on the foothills of the Dhauladhar Range. At 6,816 m Reo Purgyil is the highest mountain peak in the state of Himachal Pradesh.

The drainage system of Himachal is composed both of rivers and glaciers. Himalayan rivers criss-cross the entire mountain chain.
Himachal Pradesh provides water to both the Indus and Ganges basins. The drainage systems of the region are the Chandra Bhaga or the Chenab, the Ravi, the Beas, the Sutlej, and the Yamuna. These rivers are perennial and are fed by snow and rainfall. They are protected by an extensive cover of natural vegetation.

Due to extreme variation in elevation, great variation occurs in the climatic conditions of Himachal. The climate varies from hot and subhumid tropical in the southern tracts to, with more elevation, cold, alpine, and glacial in the northern and eastern mountain ranges. The state's winter capital, Dharamsala receives very heavy rainfall, while areas like Lahaul and Spiti are cold and almost rainless. Broadly, Himachal experiences three seasons: summer, winter, and rainy season. Summer lasts from mid-April till the end of June and most parts become very hot (except in the alpine zone which experiences a mild summer) with the average temperature ranging from . Winter lasts from late November till mid March. Snowfall is common in alpine tracts (generally above i.e. in the higher and trans-Himalayan region).

Himachal Pradesh is one of the states that lies in the Indian Himalayan Region (IHR), one of the richest reservoirs of biological diversity in the world. The IHR is currently undergoing large scale irrational extraction of wild, medicinal herbs, thus endangering many of its high-value gene stock. To address this, a workshop on ‘Endangered Medicinal Plant Species in Himachal Pradesh’ was held in 2002 and the conference was attended by forty experts from diverse disciplines.
According to 2003 Forest Survey of India report, legally defined forest areas constitute 66.52% of the area of Himachal Pradesh. Vegetation in the state is dictated by elevation and precipitation. The state endows with a high diversity of medicinal and aromatic plants. Lahaul-Spiti region of the state, being a cold desert, supports unique plants of medicinal value including "Ferula jaeschkeana", "Hyoscyamus niger", "Lancea tibetica", and "Saussurea bracteata".

Himachal is also said to be the fruit bowl of the country, with orchards being widespread. Meadows and pastures are also seen clinging to steep slopes. After the winter season, the hillsides and orchards bloom with wild flowers, while gladiolas, carnations, marigolds, roses, chrysanthemums, tulips and lilies are carefully cultivated. Himachal Pradesh Horticultural Produce Marketing and Processing Corporation Ltd. (HPMC) is a state body that markets fresh and processed fruits.

Himachal Pradesh has around 463 bird 77 mammalian, 44 reptile and 80 fish species. Great Himalayan National Park, a UNESCO World Heritage Site and Pin Valley National Park are the national Parks located in the state. The state also has 30 wildlife sanctuaries and 3 conservation reserves.

The Legislative Assembly of Himachal Pradesh has no pre-Constitution history. The State itself is a post-Independence creation. It came into being as a centrally administered territory on 15 April 1948 from the integration of thirty erstwhile princely states.

Himachal Pradesh is governed through a parliamentary system of representative democracy, a feature the state shares with other Indian states. Universal suffrage is granted to residents. The legislature consists of elected members and special office bearers such as the Speaker and the Deputy Speaker who are elected by the members. Assembly meetings are presided over by the Speaker or the Deputy Speaker in the Speaker's absence. The judiciary is composed of the Himachal Pradesh High Court and a system of lower courts. Executive authority is vested in the Council of Ministers headed by the , although the titular head of government is the Governor. The Governor is the head of state appointed by the President of India. The leader of the party or coalition with a majority in the Legislative Assembly is appointed as the Chief Minister by the Governor, and the Council of Ministers are appointed by the Governor on the advice of the Chief Minister. The Council of Ministers reports to the Legislative Assembly. The Assembly is unicameral with 68 Members of the Legislative Assembly (MLA). Terms of office run for 5 years, unless the Assembly is dissolved prior to the completion of the term. Auxiliary authorities known as "panchayats", for which local body elections are regularly held, govern local affairs.

In the assembly elections held in November 2017, the BJP secured an absolute majority. The BJP won 44 of the 68 seats while the Congress won only 21 of the 68 seats. Jai Ram Thakur was sworn-in as Himachal Pradesh's Chief Minister for the first time in Shimla on 27 December 2017.

The state of Himachal Pradesh is divided into 12 districts which are grouped into three divisions, Shimla, Kangra and Mandi. The districts are further divided into 69 subdivisions, 78 blocks and 145 Tehsils.

The era of planning in Himachal Pradesh started in 1951 along with the rest of India with the implementation of the first five-year plan. The First Plan allocated 52.7 million to Himachal Pradesh. More than 50% of this expenditure was incurred on transport and communication; while the power sector got a share of just 4.6%, though it had steadily increased to 7% by the Third Plan. Expenditure on agriculture and allied activities increased from 14.4% in the First Plan to 32% in the Third Plan, showing a progressive decline afterwards from 24% in the Fourth Plan to less than 10% in the Tenth Plan. Expenditure on energy sector was 24.2% of the total in the Tenth Plan. The total GDP for 2005-06 was estimated at 254 billion as against 230 billion in the year 2004–05, showing an increase of 10.5%. The GDP for fiscal 2015-16 was estimated at 1.110 trillion recording an annual growth of 7.7%. As per the advance estimates for fiscal 2016-17, the state's GDP increased to 1.247 trillion. The per capita incomes for fiscal years 2015-16 and 2016-17 were estimated at 130,067 and 147,277 respectively. The state government's advance estimates for fiscal 2017-18 stated the total GDP and per capita income as 1.359 trillion and 158,462 respectively. Himachal now ranks one of the highest among the states and union territories of India in terms of per capita income.

Himachal Pradesh also ranks as the second best performing state in the country on human development indicators after Kerala. One of the Indian government's key initiatives to tackle unemployment is the National Rural Employment Guarantee Act (NREGA). The participation of women in the NREGA has been observed to vary across different regions of the nation. As of the year 2009-2010, Himachal Pradesh joined the category of high female participation, recording a 46% share of NREGS (National Rural Employment Guarantee Scheme) work days to women. This was a drastic increase from the 13% that was recorded in 2006-2007.

Agriculture contributes about 9.4% to the net state domestic product. It is the main source of income and employment in Himachal. About 90% of the population in Himachal depends directly upon agriculture, which provides direct employment to 62% of total workers of state. The main cereals grown include wheat, maize, rice and barley with major cropping systems being maize-wheat, rice-wheat and maize-potato-wheat. Pulses, fruits, vegetables and oilseeds are among the other crops grown in the state. Land husbandry initiatives such as the Mid-Himalayan Watershed Development Project, which includes the Himachal Pradesh Reforestation Project (HPRP), the world's largest clean development mechanism (CDM) undertaking, have improved agricultural yields and productivity, and raised rural household incomes.

Apple is the principal cash crop of the state grown principally in the districts of Shimla, Kinnaur, Kullu, Mandi, Chamba and some parts of Sirmaur and Lahaul-Spiti with an average annual production of 5 lakh tonnes and per hectare production of 8 to 10 tonnes. The apple cultivation constitute 49 per cent of the total area under fruit crops and 85% of total fruit production in the state with an estimated economy of 3500 crore. Apples from Himachal are exported to other Indian states and even other countries. In 2011-12, the total area under apple cultivation was 1.04 lakh hectares, increased from 90,347 hectares in 2000-01. According to the provisional estimates of Ministry of Agriculture & Farmers Welfare, the annual apple production in Himachal for fiscal 2015-16 stood at 7.53 lakh tonnes, making it India's second largest apple producing state after Jammu and Kashmir.

Hydropower is also one of the major sources of income generation for the state. The state has an abundance of Hydropower resources because of the presence of various perennial rivers. Many high capacity projects are being constructed to capitalize on this. In addition, the rich hydropower resources of Himachal have resulted in the state becoming almost universally electrified with around 94.8% houses receiving electricity as of 2001, as compared to the national average of 55.9%. The income generated from exporting the electricity to other states is being provided as subsidy to the consumers in the state. Himachal's hydro-electric power production is however yet to be fully utilized. The identified Hydroelectric Potential for the state is 27,436 MW in five river basins and the annual hydroelectricity production in 2016 was 10,351 MW.

Tourism in Himachal Pradesh is a major contributor to the state's economy and growth. The mountainous state with its Himalayan landscapes attracts tourists from all over the world. Hill stations like Shimla, Manali, Dharamshala, Dalhousie, Chamba, Khajjiar, Kullu and Kasauli are popular destinations for both domestic and foreign tourists. The state also has many important Hindu pilgrimage sites with prominent temples like Naina Devi Temple, Vajreshwari Devi Temple, Jwala Ji Temple, Chintpurni, Chamunda Devi Temple, Baijnath Temple, Bhimakali Temple, Bijli Mahadev and Jakhoo Temple. Manimahesh Lake situated in the Bharmour region of Chamba district is the venue of an annual Hindu pilgrimage trek held in the month of August which attracts lakhs of devotees. The state is also referred to as ""Dev Bhoomi"" (literally meaning "Abode of Gods") due to its mention as such in ancient Hindu texts and occurrence of a large number of historical temples in the state. It is also called the Land of the Gods on account of the Hindu belief that deities like Lord Shiva considered the Himalayas their abode, and much of the state is located among the Himalayan mountains. Although modern pop-literature writers online have often also referred to Uttarakhand as the land of the gods because it also contains Himalayan mountains, officially it is Himachal Pradesh that has been considered the land of the gods since before the state of Uttarakhand existed (the UK as it is abbreviated on license plates for automobiles in the state, and the state was founded in the year 2000). A tourism department board on the road when entering Himachal Pradesh from the state of Punjab states "Welcome to the Land of the Gods."
The state is also known for its adventure tourism activities like ice skating in Shimla, paragliding in Bir Billing and Solang valley, rafting in Kullu, skiing in Manali, boating in Bilaspur and trekking, horse riding and fishing in different parts in the state. Shimla, the state's capital, is home to Asia's only natural ice skating rink. Spiti Valley in Lahaul & Spiti District situated at an altitude of over 3000 metres with its picturesque landscapes is an important destination for adventure seekers. The region also has some of the oldest Buddhist Monasteries in Asia.

Himachal hosted the first Paragliding World Cup in India from 24 October to 31 October in 2015. Venue for paragliding world cup was Bir Billing, which is 70 km from the tourist town Macleod Ganj, located in the heart of Himachal in Kangra District. Bir Billing is the centre for aero sports in Himachal and considered as best for paragliding. Buddhist monasteries, trekking to tribal villages, mountain biking are other activities to do here.

Himachal has three domestic airports in Kangra, Kullu and Shimla districts. The air routes connect the state with Delhi and Chandigarh.

Himachal is known for its narrow-gauge railways. One is the Kalka-Shimla Railway, a UNESCO World Heritage Site, and another is the Pathankot-Jogindernagar line. The total length of these two tracks is . The Kalka-Shimla Railway passes through many tunnels, while the Pathankot–Jogindernagar meanders through a maze of hills and valleys. The state also has broad-gauge railway track, which connects Amb and Una (district headquarters of Una district) to Delhi. A survey is being conducted to extend this railway line to Hamirpur. The total route length of the operational railway network in the state is . Other proposed railways in the state are Dharamsala-Palampur, Baddi-Chandigarh and Bilaspur-Manali-Leh.

Roads are the major mode of transport in the hilly terrains. The state has road network of , including eight National Highways (NH) that constitute and 19 State Highways with a total length of . Hamirpur district has the highest road density in the country. Some roads get closed during winter and monsoon seasons due to snow and landslides. The state-owned Himachal Road Transport Corporation with a fleet of over 3,100, operates bus services connecting important cities and towns with the villages within the state and also on various interstate routes. In addition, around 3,000 private buses run by various operators also ply in the state.

Himachal Pradesh has a total population of 6,864,602 including 3,481,873 males and 3,382,729 females as per the final results of the Census of India 2011. This is only 0.57 per cent of India's total population, recording a growth of 12.81 per cent. The scheduled castes and scheduled tribes account for 25.19 per cent and 5.71 per cent of the population respectively. The sex ratio stood at 972 females per 1000 males, recording a marginal increase from 968 in 2001. The child sex ratio increased from 896 in 2001 to 909 in 2011. The total fertility rate (TFR) per woman in 2015 stood at 1.7, one of the lowest in India.

In the census, the state is placed 21st on the population chart, followed by Tripura at 22nd place. Kangra district was top ranked with a population strength of 1,507,223 (21.98%), Mandi district 999,518 (14.58%), Shimla district 813,384 (11.86%), Solan district 576,670 (8.41%), Sirmaur district 530,164 (7.73%), Una district 521,057 (7.60%), Chamba district 518,844 (7.57%), Hamirpur district 454,293 (6.63%), Kullu district 437,474 (6.38%), Bilaspur district 382,056 (5.57%), Kinnaur district 84,298 (1.23%) and Lahaul Spiti 31,528 (0.46%).

The life expectancy at birth in Himachal Pradesh increased significantly from 52.6 years in the period from 1970-75 (above the national average of 49.7 years) to 72.0 years for the period 2011-15 (above the national average of 68.3 years). The infant mortality rate stood at 40 in 2010, and the crude birth rate has declined from 37.3 in 1971 to 16.9 in 2010, below the national average of 26.5 in 1998. The crude death rate was 6.9 in 2010. Himachal Pradesh's literacy rate has almost doubled between 1981 and 2011 (see table to right). The state is one of the most literate states of India with a literacy rate of 83.78% as of 2011.

Hindi is the official language of Himachal Pradesh and is spoken by the majority of the population as a lingua franca. English is given the status of an additional official language. Most of the languages spoken natively belong to the group of the Himachali languages. According to the 2001 Census of India, the languages spoken in the state in descending order of native speakers are Hindi, spoken by 89.01% of the population (including Himachali languages as dialects of Hindi); followed by Punjabi (5.99%), Nepali (1.16%) and Kinnauri (1.06%).

Hinduism is the major religion in Himachal Pradesh. More than 95% of the total population adheres to the Hindu faith, the distribution of which is evenly spread throughout the state. Himachal Pradesh has the highest proportion of Hindu population among all the states and union territories in India.

Other religions that form a small percentage are Islam, Sikhism and Buddhism. Muslims are mainly concentrated in Sirmaur, Chamba, Una and Solan districts where they form 2.53-6.27% of the population. Sikhs mostly live in towns and cities and constitute 1.16% of the state population. The Buddhists, who constitute 1.15%, are mainly natives and tribals from Lahaul and Spiti, where they form a majority of 62%, and Kinnaur, where they form 21.5%.

Himachal Pradesh was one of the few states that had remained largely untouched by external customs, largely due to its difficult terrain. With remarkable economic and social advancements, the state has changed very rapidly. Himachal Pradesh is a multireligious, multicultural as well as a multilingual state like other Indian states. Western Pahari languages also known as Himachali languages are widely spoken in the state. Some of the most commonly spoken individual languages are Kangri, Mandeali, Kulvi, Chambeali, Bharmauri and Kinnauri. The Hindu communities residing in Himachal include the "Brahmins", "Rajputs", "Kayasthas", "Sunars", "Kannets", "Rathis" and "Kolis". The tribal population of the state consists mainly of "Gaddis", "Gujjars", "Kanauras", "Pangwalas", "Bhots", "Swanglas" and "Lahaulas".

Himachal is well known for its handicrafts. The carpets, leather works, Kullu shawls, Kangra paintings, Chamba Rumals, stoles, embroidered grass footwear ("Pullan chappal"), silver jewelry, metal-ware, knitted woolen socks, "Pattoo", basketry of cane and bamboo ("Wicker" and "Rattan") and woodwork are among the notable ones. Of late, the demand for these handicrafts has increased within and outside the country. Himachali caps of various colour bands are also well-known art work of the local people, and are often treated as a symbol of the Himachali identity. The colour of the Himachali caps has been an indicator of political loyalties in the hill state for a long period of time with Congress party leaders like Virbhadra Singh always donning caps with green band and the rival BJP leader Prem Kumar Dhumal always wearing a cap with maroon band. The former has served six terms as the Chief Minister of the state while the latter is a two-time Chief Minister. Local music and dance also reflects the cultural identity of the state. Through their dance and music, the Himachali people entreat their gods during local festivals and other special occasions.

Apart from the fairs and festivals that are celebrated all over India, there are number of other fairs and festivals, including the temple fairs in nearly every region that are of great significance to Himachal Pradesh. The Kullu Dussehra festival is very famous all over India. The day to day cuisine of "Himachalis" is very similar to the rest of the north India with a significant influence of Punjabi and Tibetan cuisines. Lentils ("Dāl"), rice ("Chāwal" or "Bhāț"), vegetables ("Sabzī") and chapati (wheat flatbread) form the staple food of the local population. As compared to other states in north India, non-vegetarian food is more preferred and accepted in Himachal Pradesh, partly owing to difficulty in finding a variety of fresh vegetables on the hilly terrain of the state. Some of the local specialities of Himachali cuisine include "Siddu", "Babru", "Khatta", "Mhanee", "Channa Madra", "Patrode", "Mah Ki Dal", "Chamba Style Fried Fish", "Kullu Trout", "Chha Gosht", "Pahadi Chicken", "Sepu Badi", "Auriya Kaddu", "Aloo Palda", "Pateer", "Makki Ki Roti and Sarson Ka Saag", "Chouck", "Bhagjery" and "Chutney" of Til.


At the time of Independence, Himachal Pradesh had a literacy rate of 8% - one of the lowest in the country. By 2011, the literacy rate surged to over 82%, making Himachal one of the most literate states in the country. There are over 10,000 primary schools, 1,000 secondary schools and more than 1,300 high schools in the state. In meeting the constitutional obligation to make primary education compulsory, Himachal became the first state in India to make elementary education accessible to every child."" Although gender bias in education levels is a prominent issue all over India, Himachal Pradesh is one of the exceptions. The state has a female literacy rate of around 76%. In addition, school enrollment and participation rates for girls are almost universal at the primary level. While higher levels of education do reflect a gender based disparity, Himachal is still significantly ahead of other states at bridging the gap. The Hamirpur District in particular stands out for high literacy rates across all metrics of measurement.

The state government has played an instrumental role in the rise of literacy in the state by spending a significant proportion of the state's GDP on education. During the first six five-year plans, most of the development expenditure in education sector was utilized in quantitative expansion, but after the seventh five-year-plan the state government switched emphasis on qualitative improvement and modernisation of education. In an effort to raise the number of teaching staff at primary schools they appointed over 1000 teacher aids through the Vidya Upasak Yojna in 2001. The Sarva Shiksha Abhiyan is another HP government initiative that not only aims for universal elementary education but also encourages communities to engage in the management of schools. The Rashtriya Madhayamic Shiksha Abhiyan launched in 2009, is a similar scheme but focuses on improving access to quality secondary education.

The standard of education in the state has reached a considerably high level as compared to other states in India with several reputed educational institutes for higher studies.The Indian Institute of Technology Mandi, Indian Institute of Management Sirmaur, Himachal Pradesh University in Shimla, National Institute of Technology, Hamirpur, Indian Institute of Information Technology Una, Alakh Prakash Goyal University and Baddi University of Emerging Sciences and Technologies are some of the notable universities in the state. Indira Gandhi Medical College and Hospital in Shimla, Dr. Rajendra Prasad Government Medical College in Kangra, Rajiv Gandhi Government Post Graduate Ayurvedic College in Paprola and Homoeopathic Medical College & Hospital in Kumarhatti are the prominent medical institutes in the state. Besides these, there is a Government Dental College in Shimla which is the state's first recognised dental institute. The state government has also decided to start three major nursing colleges to develop the healthcare system of the state. CSK Himachal Pradesh Krishi Vishwavidyalya Palampur is one of the most renowned hill agriculture institutes in the world. Dr. Yashwant Singh Parmar University of Horticulture and Forestry has earned a unique distinction in India for imparting teaching, research and extension education in horticulture, forestry and allied disciplines. Further, state-run Jawaharlal Nehru Government Engineering College was inaugurated in 2006 at Sundernagar.
Himachal Pradesh also hosts a campus of the prestigious fashion college, National Institute of Fashion Technology (NIFT) in Kangra.

Source: "Department of Information and Public Relations."
Census 2011-

Largest District (km²)
(1) Lahul and Spiti 13841
(2) Chamba 6522
(3) Kinnaur 6401
(4) Kangra 5739
(5) Kullu 5503

Highest Percentage of Child Population
(1) Chamba 13.55%
(2) Sirmaur 13.14%
(3) Solan 11.74%
(4) Kullu 11.52%
(5) Una 11.36%

Highest Density
(1) Hamirpur 407
(2) Una 338
(3) Bilaspur 327
(4) Solan 300
(5) Kangra 263

Top Population Growth
(1) Una 16.26%
(2) Solan 15.93%
(3) Sirmaur 15.54%
(4) Kullu 14.76%
(5) Kangra 12.77%

Highest Literacy
(1) Hamirpur 100%
(2) Una 87.23%
(3) Kangra 86.49%
(4) Blaspur 85.87%
(5) Solan 85.02%

Highest Sex Ratio
(1) Hamirpur 1050
(2) Kangra 1012
(3) Mandi 1007
(4) Chamba 986
(5) Bilaspur 981




</doc>
<doc id="14192" url="https://en.wikipedia.org/wiki?curid=14192" title="Helene">
Helene

Helene may refer to:




</doc>
<doc id="14193" url="https://en.wikipedia.org/wiki?curid=14193" title="Hyperion">
Hyperion

Hyperion may refer to:













</doc>
<doc id="14194" url="https://en.wikipedia.org/wiki?curid=14194" title="History of medicine">
History of medicine

The history of medicine shows how societies have changed in their approach to illness and disease from ancient times to the present.

Early medical traditions include those of Babylon, China, Egypt and India. The Greeks introduced the concepts of medical diagnosis, prognosis, and advanced medical ethics. The Hippocratic Oath was written in ancient Greece in the 5th century BCE, and is a direct inspiration for oaths of office that physicians swear upon entry into the profession today. In the medieval age, surgical practices inherited from the ancient masters were improved and then systematized in Rogerius's "The Practice of Surgery". Universities began systematic training of physicians around the years 1220 in Italy. During the Renaissance, understanding of anatomy improved, and the microscope was invented. The germ theory of disease in the 19th century led to cures for many infectious diseases. Military doctors advanced the methods of trauma treatment and surgery. Public health measures were developed especially in the 19th century as the rapid growth of cities required systematic sanitary measures. Advanced research centers opened in the early 20th century, often connected with major hospitals. The mid-20th century was characterized by new biological treatments, such as antibiotics. These advancements, along with developments in chemistry, genetics, and radiography led to modern medicine. Medicine was heavily professionalized in the 20th century, and new careers opened to women as nurses (from the 1870s) and as physicians (especially after 1970).

Although there is no record to establish when plants were first used for medicinal purposes (herbalism), the use of plants as healing agents, as well as clays and soils is ancient. Over time through emulation of the behavior of fauna a medicinal knowledge base developed and passed between generations. As tribal culture specialized specific castes, shamans and apothecaries fulfilled the role of healer.
The first known dentistry dates to c 7000 BC in Baluchistan where Neolithic dentists used flint-tipped drills and bowstrings. The first known trepanning operation was carried out c 5000 BC in Ensisheim, France. A possible amputation was carried out c 4,900 BC in Buthiers-Bulancourt, France.

The ancient Mesopotamians had no distinction between "rational science" and magic. When a person became ill, doctors would prescribe both magical formulas to be recited as well as medicinal treatments. The earliest medical prescriptions appear in Sumerian during the Third Dynasty of Ur ( 2112 BC – 2004 BC). The oldest Babylonian texts on medicine date back to the Old Babylonian period in the first half of the 2nd millennium BCE. The most extensive Babylonian medical text, however, is the "Diagnostic Handbook" written by the "ummânū", or chief scholar, Esagil-kin-apli of Borsippa, during the reign of the Babylonian king Adad-apla-iddina (1069–1046 BCE). Along with the Egyptians, the Babylonians introduced the practice of diagnosis, prognosis, physical examination, and remedies. In addition, the "Diagnostic Handbook" introduced the methods of therapy and cause. The text contains a list of medical symptoms and often detailed empirical observations along with logical rules used in combining observed symptoms on the body of a patient with its diagnosis and prognosis. The "Diagnostic Handbook" was based on a logical set of axioms and assumptions, including the modern view that through the examination and inspection of the symptoms of a patient, it is possible to determine the patient's disease, its cause and future development, and the chances of the patient's recovery. The symptoms and diseases of a patient were treated through therapeutic means such as bandages, herbs and creams.

In East Semitic cultures, the main medicinal authority was a kind of exorcist-healer known as an "āšipu". The profession was generally passed down from father to son and was held in extremely high regard. Of less frequent recourse was another kind of healer known as an "asu", who corresponds more closely to a modern physician and treated physical symptoms using primarily folk remedies composed of various herbs, animal products, and minerals, as well as potions, enemas, and ointments or poultices. These physicians, who could be either male or female, also dressed wounds, set limbs, and performed simple surgeries. The ancient Mesopotamians also practiced prophylaxis and took measures to prevent the spread of disease.

Mental illnesses were well known in ancient Mesopotamia, where diseases and mental disorders were believed to be caused by specific deities. Because hands symbolized control over a person, mental illnesses were known as "hands" of certain deities. One psychological illness was known as "Qāt Ištar", meaning "Hand of Ishtar". Others were known as "Hand of Shamash", "Hand of the Ghost", and "Hand of the God". Descriptions of these illnesses, however, are so vague that it is usually impossible to determine which illnesses they correspond to in modern terminology. Mesopotamian doctors kept detailed record of their patients' hallucinations and assigned spiritual meanings to them. A patient who hallucinated that he was seeing a dog was predicted to die; whereas, if he saw a gazelle, he would recover. The royal family of Elam was notorious for its members frequently suffering from insanity. Erectile dysfunction was recognized as being rooted in psychological problems.

Ancient Egypt developed a large, varied and fruitful medical tradition. Herodotus described the Egyptians as "the healthiest of all men, next to the Libyans", because of the dry climate and the notable public health system that they possessed. According to him, "the practice of medicine is so specialized among them that each physician is a healer of one disease and no more." Although Egyptian medicine, to a considerable extent, dealt with the supernatural, it eventually developed a practical use in the fields of anatomy, public health, and clinical diagnostics.

Medical information in the Edwin Smith Papyrus may date to a time as early as 3000 BC. Imhotep in the 3rd dynasty is sometimes credited with being the founder of ancient Egyptian medicine and with being the original author of the "Edwin Smith Papyrus", detailing cures, ailments and anatomical observations. The "Edwin Smith Papyrus" is regarded as a copy of several earlier works and was written c. 1600 BC. It is an ancient textbook on surgery almost completely devoid of magical thinking and describes in exquisite detail the "examination, diagnosis, treatment," and "prognosis" of numerous ailments.

The Kahun Gynaecological Papyrus treats women's complaints, including problems with conception. Thirty four cases detailing diagnosis and treatment survive, some of them fragmentarily. Dating to 1800 BCE, it is the oldest surviving medical text of any kind.

Medical institutions, referred to as "Houses of Life" are known to have been established in ancient Egypt as early as 2200 BC.

The earliest known physician is also credited to ancient Egypt: Hesy-Ra, "Chief of Dentists and Physicians" for King Djoser in the 27th century BCE. Also, the earliest known woman physician, Peseshet, practiced in Ancient Egypt at the time of the 4th dynasty. Her title was "Lady Overseer of the Lady Physicians." In addition to her supervisory role, Peseshet trained midwives at an ancient Egyptian medical school in Sais.

The Atharvaveda, a sacred text of Hinduism dating from the Early Iron Age, is one of the first Indian text dealing with medicine. The Atharvaveda also contain prescriptions of herbs for various ailments. The use of herbs to treat ailments would later form a large part of Ayurveda.

Ayurveda, meaning the "complete knowledge for long life" is another medical system of India. Its two most famous texts belong to the schools of Charaka and Sushruta. The earliest foundations of Ayurveda were built on a synthesis of traditional herbal practices together with a massive addition of theoretical conceptualizations, new nosologies and new therapies dating from about 600 BCE onwards, and coming out of the communities of thinkers who included the Buddha and others.

According to the compendium of Charaka, the Charakasamhitā, health and disease are not predetermined and life may be prolonged by human effort. The compendium of Suśruta, the Suśrutasamhitā defines the purpose of medicine to cure the diseases of the sick, protect the healthy, and to prolong life. Both these ancient compendia include details of the examination, diagnosis, treatment, and prognosis of numerous ailments. The Suśrutasamhitā is notable for describing procedures on various forms of surgery, including rhinoplasty, the repair of torn ear lobes, perineal lithotomy, cataract surgery, and several other excisions and other surgical procedures. Most remarkable is Sushruta's penchant for scientific classification:
His medical treatise consists of 184 chapters, 1,120 conditions are listed, including injuries and illnesses relating to aging and mental illness.

The Ayurvedic classics mention eight branches of medicine: kāyācikitsā (internal medicine), śalyacikitsā (surgery including anatomy), śālākyacikitsā (eye, ear, nose, and throat diseases), kaumārabhṛtya (pediatrics with obstetrics and gynaecology), bhūtavidyā (spirit and psychiatric medicine), agada tantra (toxicology with treatments of stings and bites), rasāyana (science of rejuvenation), and vājīkaraṇa (aphrodisiac and fertility). Apart from learning these, the student of Āyurveda was expected to know ten arts that were indispensable in the preparation and application of his medicines: distillation, operative skills, cooking, horticulture, metallurgy, sugar manufacture, pharmacy, analysis and separation of minerals, compounding of metals, and preparation of alkalis. The teaching of various subjects was done during the instruction of relevant clinical subjects. For example, teaching of anatomy was a part of the teaching of surgery, embryology was a part of training in pediatrics and obstetrics, and the knowledge of physiology and pathology was interwoven in the teaching of all the clinical disciplines.
The normal length of the student's training appears to have been seven years. But the physician was to continue to learn.

As an alternative form of medicine in India, Unani medicine got deep roots and royal patronage during medieval times. It progressed during Indian sultanate and mughal periods. Unani medicine is very close to Ayurveda. Both are based on theory of the presence of the elements (in Unani, they are considered to be fire, water, earth and air) in the human body. According to followers of Unani medicine, these elements are present in different fluids and their balance leads to health and their imbalance leads to illness.

By the 18th century CE, Sanskrit medical wisdom still dominated. Muslim rulers built large hospitals in 1595 in Hyderabad, and in Delhi in 1719, and numerous commentaries on ancient texts were written.

China also developed a large body of traditional medicine. Much of the philosophy of traditional Chinese medicine derived from empirical observations of disease and illness by Taoist physicians and reflects the classical Chinese belief that individual human experiences express causative principles effective in the environment at all scales. These causative principles, whether material, essential, or mystical, correlate as the expression of the natural order of the universe.

The foundational text of Chinese medicine is the Huangdi neijing, (or "Yellow Emperor's Inner Canon"), written 5th century to 3rd century BCE. Near the end of the 2nd century CE, during the Han dynasty, Zhang Zhongjing, wrote a "Treatise on Cold Damage", which contains the earliest known reference to the "Neijing Suwen". The Jin Dynasty practitioner and advocate of acupuncture and moxibustion, Huangfu Mi (215–282), also quotes the Yellow Emperor in his "Jiayi jing", c. 265. During the Tang Dynasty, the "Suwen" was expanded and revised, and is now the best extant representation of the foundational roots of traditional Chinese medicine. Traditional Chinese Medicine that is based on the use of herbal medicine, acupuncture, massage and other forms of therapy has been practiced in China for thousands of years.

In the 18th century, during the Qing dynasty, there was a proliferation of popular books as well as more advanced encyclopedias on traditional medicine. Jesuit missionaries introduced Western science and medicine to the royal court, the Chinese physicians ignored them.

Finally in the 19th century, Western medicine was introduced at the local level by Christian medical missionaries from the London Missionary Society (Britain), the Methodist Church (Britain) and the Presbyterian Church (USA). Benjamin Hobson (1816–1873) in 1839, set up a highly successful Wai Ai Clinic in Guangzhou, China. The Hong Kong College of Medicine for Chinese was founded in 1887 by the London Missionary Society, with its first graduate (in 1892) being Sun Yat-sen, who later led the Chinese Revolution (1911). The Hong Kong College of Medicine for Chinese was the forerunner of the School of Medicine of the University of Hong Kong, which started in 1911.

Because of the social custom that men and women should not be near to one another, the women of China were reluctant to be treated by male doctors. The missionaries sent women doctors such as Dr. Mary Hannah Fulton (1854–1927). Supported by the Foreign Missions Board of the Presbyterian Church (USA) she in 1902 founded the first medical college for women in China, the Hackett Medical College for Women, in Guangzhou.

Around 800 BCE Homer in The Iliad gives descriptions of wound treatment by the two sons of Asklepios, the admirable physicians Podaleirius and Machaon and one acting doctor, Patroclus. Because Machaon is wounded and Podaleirius is in combat Eurypylus asks Patroclus to "cut out this arrow from my thigh, wash off the blood with warm water and spread soothing ointment on the wound". Asklepios like Imhotep becomes god of healing over time.

Temples dedicated to the healer-god Asclepius, known as "Asclepieia" (, sing. , "'Asclepieion"), functioned as centers of medical advice, prognosis, and healing. At these shrines, patients would enter a dream-like state of induced sleep known as "enkoimesis" () not unlike anesthesia, in which they either received guidance from the deity in a dream or were cured by surgery. Asclepeia provided carefully controlled spaces conducive to healing and fulfilled several of the requirements of institutions created for healing. In the Asclepeion of Epidaurus, three large marble boards dated to 350 BCE preserve the names, case histories, complaints, and cures of about 70 patients who came to the temple with a problem and shed it there. Some of the surgical cures listed, such as the opening of an abdominal abscess or the removal of traumatic foreign material, are realistic enough to have taken place, but with the patient in a state of enkoimesis induced with the help of soporific substances such as opium. Alcmaeon of Croton wrote on medicine between 500 and 450 BCE. He argued that channels linked the sensory organs to the brain, and it is possible that he discovered one type of channel, the optic nerves, by dissection.

A towering figure in the history of medicine was the physician Hippocrates of Kos (c. 460c. 370 BCE), considered the "father of modern medicine." The Hippocratic Corpus is a collection of around seventy early medical works from ancient Greece strongly associated with Hippocrates and his students. Most famously, Hippocrates invented the Hippocratic Oath for physicians. Until today physicians swear an oath of office, which includes aspects found already in the Hippocratic Oath, (such as not to give a lethal dose of medicines, even if requested by the patient).

Hippocrates and his followers were first to describe many diseases and medical conditions. He is given credit for the first description of clubbing of the fingers, an important diagnostic sign in chronic suppurative lung disease, lung cancer and cyanotic heart disease. For this reason, clubbed fingers are sometimes referred to as "Hippocratic fingers". Hippocrates was also the first physician to describe the Hippocratic face in "Prognosis". Shakespeare famously alludes to this description when writing of Falstaff's death in Act II, Scene iii. of "Henry V".

Hippocrates began to categorize illnesses as acute, chronic, endemic and epidemic, and use terms such as, "exacerbation, relapse, resolution, crisis, paroxysm, peak, and convalescence."

Another of Hippocrates's major contributions may be found in his descriptions of the symptomatology, physical findings, surgical treatment and prognosis of thoracic empyema, i.e. suppuration of the lining of the chest cavity. His teachings remain relevant to present-day students of pulmonary medicine and surgery. Hippocrates was the first documented person to practise cardiothoracic surgery, and his findings are still valid.

Some of the techniques and theories developed by Hippocrates are now put into practice by the fields of Environmental and Integrative Medicine. These include recognizing the importance of taking a complete history which includes environmental exposures as well as foods eaten by the patient which might play a role in his or her illness.

Two great Alexandrians laid the foundations for the scientific study of anatomy and physiology, Herophilus of Chalcedon and Erasistratus of Ceos. Other Alexandrian surgeons gave us ligature (hemostasis), lithotomy, hernia operations, ophthalmic surgery, plastic surgery, methods of reduction of dislocations and fractures, tracheotomy, and mandrake as an anaesthetic. Some of what we know of them comes from Celsus and Galen of Pergamum.

Herophilus of Chalcedon, working at the medical school of Alexandria placed intelligence in the brain, and connected the nervous system to motion and sensation. Herophilus also distinguished between veins and arteries, noting that the latter pulse while the former do not. He and his contemporary, Erasistratus of Chios, researched the role of veins and nerves, mapping their courses across the body. Erasistratus connected the increased complexity of the surface of the human brain compared to other animals to its superior intelligence. He sometimes employed experiments to further his research, at one time repeatedly weighing a caged bird, and noting its weight loss between feeding times. In Erasistratus' physiology, air enters the body, is then drawn by the lungs into the heart, where it is transformed into vital spirit, and is then pumped by the arteries throughout the body. Some of this vital spirit reaches the brain, where it is transformed into animal spirit, which is then distributed by the nerves.

The Greek Galen (c. 129–216 CE) was one of the greatest physicians of the ancient world, studying and traveling widely in ancient Rome. He dissected animals to learn about the body, and performed many audacious operations—including brain and eye surgeries— that were not tried again for almost two millennia. In "Ars medica" ("Arts of Medicine"), he explained mental properties in terms of specific mixtures of the bodily parts.

Galen's medical works were regarded as authoritative until well into the Middle Ages. Galen left a physiological model of the human body that became the mainstay of the medieval physician's university anatomy curriculum, but it suffered greatly from stasis and intellectual stagnation because some of Galen's ideas were incorrect; he did not dissect a human body. Greek and Roman taboos had meant that dissection was usually banned in ancient times, but in the Middle Ages it changed.

In 1523 Galen's "On the Natural Faculties" was published in London. In the 1530s Belgian anatomist and physician Andreas Vesalius launched a project to translate many of Galen's Greek texts into Latin. Vesalius's most famous work, "De humani corporis fabrica" was greatly influenced by Galenic writing and form.

The Romans invented numerous surgical instruments, including the first instruments unique to women, as well as the surgical uses of forceps, scalpels, cautery, cross-bladed scissors, the surgical needle, the sound, and speculas. Romans also performed cataract surgery.

The Roman army physician Dioscorides (c. 40–90 CE), was a Greek botanist and pharmacologist. He wrote the encyclopedia "De Materia Medica" describing over 600 herbal cures, forming an influential pharmacopoeia which was used extensively for the following 1,500 years.

Byzantine medicine encompasses the common medical practices of the Byzantine Empire from about 400 AD to 1453 AD. Byzantine medicine was notable for building upon the knowledge base developed by its Greco-Roman predecessors. In preserving medical practices from antiquity, Byzantine medicine influenced Islamic medicine as well as fostering the Western rebirth of medicine during the Renaissance.

Byzantine physicians often compiled and standardized medical knowledge into textbooks. Their records tended to include both diagnostic explanations and technical drawings. The Medical Compendium in Seven Books, written by the leading physician Paul of Aegina, survived as a particularly thorough source of medical knowledge. This compendium, written in the late seventh century, remained in use as a standard textbook for the following 800 years.

Late antiquity ushered in a revolution in medical science, and historical records often mention civilian hospitals (although battlefield medicine and wartime triage were recorded well before Imperial Rome). Constantinople stood out as a center of medicine during the Middle Ages, which was aided by its crossroads location, wealth, and accumulated knowledge.
copied content from Byzantine medicine; see that page's history for attribution

The first ever known example of separating conjoined twins occurred in the Byzantine Empire in the 10th century. The next example of separating conjoined twins will be first recorded many centuries later in Germany in 1689.

The Byzantine Empire's neighbors, the Persian Sassanid Empire, also made their noteworthy contributions mainly with the establishment of the Academy of Gondeshapur, which was "the most important medical center of the ancient world during the 6th and 7th centuries." In addition, Cyril Elgood, British physician and a historian of medicine in Persia, commented that thanks to medical centers like the Academy of Gondeshapur, "to a very large extent, the credit for the whole hospital system must be given to Persia."

The Islamic civilization rose to primacy in medical science as its physicians contributed significantly to the field of medicine, including anatomy, ophthalmology, pharmacology, pharmacy, physiology, surgery, and the pharmaceutical sciences. The Arabs were influenced by ancient Indian, Persian, Greek, Roman and Byzantine medical practices, and helped them develop further. Galen & Hippocrates were pre-eminent authorities. The translation of 129 of Galen's works into Arabic by the Nestorian Christian Hunayn ibn Ishaq and his assistants, and in particular Galen's insistence on a rational systematic approach to medicine, set the template for Islamic medicine, which rapidly spread throughout the Arab Empire. while Europe was in its Dark Ages, Islam expanded in West 
Asia and enjoyed a golden age. Its most famous 
physicians included Muhammad ibn Zakariya al-Razi and the polymath Ibn Sina, who 
wrote more than 40 works on health, medicine, and well-being. 
Taking leads from Greece and Rome, Islamic scholars kept both 
the art and science of medicine alive and moving forward.

After A.D. 400, the study and practice of medicine in the Western Roman Empire went into deep decline. Medical services were provided, especially for the poor, in the thousands of monastic hospitals that sprang up across Europe, but the care was rudimentary and mainly palliative. Most of the writings of Galen and Hippocrates were lost to the West, with the summaries and compendia of St. Isidore of Seville being the primary channel for transmitting Greek medical ideas. The Carolingian renaissance brought increased contact with Byzantium and a greater awareness of ancient medicine, but only with the twelfth century renaissance and the new translations coming from Muslim and Jewish sources in Spain, and the fifteenth century flood of resources after the fall of Constantinople did the West fully recover its acquaintance with classical antiquity.

Greek and Roman taboos had meant that dissection was usually banned in ancient times, but in the Middle Ages it changed: medical teachers and students at Bologna began to open human bodies, and Mondino de Luzzi (ca. 1275–1326) produced the first known anatomy textbook based on human dissection.

Wallis identifies a prestige hierarchy with university educated physicians on top, followed by learned surgeons; craft-trained surgeons; barber surgeons; itinerant specialists such as dentist and oculists; empirics; and midwives.

The first medical schools were opened in the 9th century, most notably the Schola Medica Salernitana at Salerno in southern Italy. The cosmopolitan influences from Greek, Latin, Arabic, and Hebrew sources gave it an international reputation as the Hippocratic City. Students from wealthy families came for three years of preliminary studies and five of medical studies. The medicine, following the laws of Federico II, that he founded in 1224 the University ad improved the Schola Salernitana, in the period between 1200 and 1400, it had in Sicily (so-called Sicilian Middle Ages) a particular development so much to create a true school of Jewish medicine.

As a result of which, after a legal examination, was conferred to a Jewish Sicilian woman, Virdimura, wife of another physician Pasquale of Catania, the hystorical record of before woman officially trained to exercise of the medical profession.

By the thirteenth century, the medical school at Montpellier began to eclipse the Salernitan school. In the 12th century, universities were founded in Italy, France, and England, which soon developed schools of medicine. The University of Montpellier in France and Italy's University of Padua and University of Bologna were leading schools. Nearly all the learning was from lectures and readings in Hippocrates, Galen, Avicenna, and Aristotle.

The underlying principle of most medieval medicine was Galen's theory of humours. This was derived from the ancient medical works, and dominated all western medicine until the 19th century. The theory stated that within every individual there were four humours, or principal fluids – black bile, yellow bile, phlegm, and blood, these were produced by various organs in the body, and they had to be in balance for a person to remain healthy. Too much phlegm in the body, for example, caused lung problems; and the body tried to cough up the phlegm to restore a balance. The balance of humours in humans could be achieved by diet, medicines, and by blood-letting, using leeches. The four humours were also associated with the four seasons, black bile-autumn, yellow bile-summer, phlegm-winter and blood-spring.

Healing included both physical and spiritual therapeutics, such as the right herbs, a suitable diet, clean bedding, and the sense that care was always at hand. Other procedures used to help patients included the Mass, prayers, relics of saints, and music used to calm a troubled mind or quickened pulse.

In 1376, in Sicily, it was historically given, in relationship to the laws of Federico II that they foresaw an examination with a regal errand of physicists, the first qualification to the exercise of the medicine to a woman, Virdimura a Jewess of Catania, whose document is preserved in Palermo to the Italian national archives.

The Renaissance brought an intense focus on scholarship to Christian Europe. A major effort to translate the Arabic and Greek scientific works into Latin emerged. Europeans gradually became experts not only in the ancient writings of the Romans and Greeks, but in the contemporary writings of Islamic scientists. During the later centuries of the Renaissance came an increase in experimental investigation, particularly in the field of dissection and body examination, thus advancing our knowledge of human anatomy.

The development of modern neurology began in the 16th century in Italy and France with Niccolò Massa, Jean Fernel, Jacques Dubois and Andreas Vesalius. Vesalius described in detail the anatomy of the brain and other organs; he had little knowledge of the brain's function, thinking that it resided mainly in the ventricles. Over his lifetime he corrected over 200 of Galen's mistakes. Understanding of medical sciences and diagnosis improved, but with little direct benefit to health care. Few effective drugs existed, beyond opium and quinine. Folklore cures and potentially poisonous metal-based compounds were popular treatments.
Independently from Ibn al-Nafis, Michael Servetus rediscovered the pulmonary circulation, but this discovery did not reach the public because it was written down for the first time in the "Manuscript of Paris" in 1546, and later published in the theological work which he paid with his life in 1553. Later this was perfected by Renaldus Columbus and Andrea Cesalpino. Later William Harvey correctly described the circulatory system. The most useful tomes in medicine used both by students and expert physicians were "De Materia Medica" and Pharmacopoeia.
Bacteria and protists were first observed with a microscope by Antonie van Leeuwenhoek in 1676, initiating the scientific field of microbiology.

Paracelsus (1493–1541), was an erratic and abusive innovator who rejected Galen and bookish knowledge, calling for experimental research, with heavy doses of mysticism, alchemy and magic mixed in. He rejected sacred magic (miracles) under Church auspisces and looked for cures in nature. He preached but he also pioneered the use of chemicals and minerals in medicine. His hermetical views were that sickness and health in the body relied on the harmony of man (microcosm) and Nature (macrocosm). He took an approach different from those before him, using this analogy not in the manner of soul-purification but in the manner that humans must have certain balances of minerals in their bodies, and that certain illnesses of the body had chemical remedies that could cure them. Most of his influence came after his death. Paracelsus is a highly controversial figure in the history of medicine, with most experts hailing him as a Father of Modern Medicine for shaking off religious orthodoxy and inspiring many researchers; others say he was a mystic more than a scientist and downplay his importance.

University training of physicians began in the 13th century.

The University of Padua was founded about 1220 by walkouts from the University of Bologna, and began teaching medicine in 1222. It played a leading role in the identification and treatment of diseases and ailments, specializing in autopsies and the inner workings of the body. Starting in 1595, Padua's famous anatomical theatre drew artists and scientists studying the human body during public dissections. The intensive study of Galen led to critiques of Galen modeled on his own writing, as in the first book of Vesalius's "De humani corporis fabrica." Andreas Vesalius held the chair of Surgery and Anatomy ("explicator chirurgiae") and in 1543 published his anatomical discoveries in De Humani Corporis Fabrica. He portrayed the human body as an interdependent system of organ groupings. The book triggered great public interest in dissections and caused many other European cities to establish anatomical theatres.

At the University of Bologna the training of physicians began in 1219. The Italian city attracted students from across Europe. Taddeo Alderotti built a tradition of medical education that established the characteristic features of Italian learned medicine and was copied by medical schools elsewhere. Turisanus (d. 1320) was his student. The curriculum was revised and strengthened in 1560–1590. A representative professor was Julius Caesar Aranzi (Arantius) (1530–89). He became Professor of Anatomy and Surgery at the University of Bologna in 1556, where he established anatomy as a major branch of medicine for the first time. Aranzi combined anatomy with a description of pathological processes, based largely on his own research, Galen, and the work of his contemporary Italians. Aranzi discovered the 'Nodules of Aranzio' in the semilunar valves of the heart and wrote the first description of the superior levator palpebral and the coracobrachialis muscles. His books (in Latin) covered surgical techniques for many conditions, including hydrocephalus, nasal polyp, goitre and tumours to phimosis, ascites, haemorrhoids, anal abscess and fistulae.

Catholic women played large roles in health and healing in medieval and early modern Europe. A life as a nun was a prestigious role; wealthy families provided dowries for their daughters, and these funded the convents, while the nuns provided free nursing care for the poor.

The Catholic elites provided hospital services because of their theology of salvation that good works were the route to heaven. The Protestant reformers rejected the notion that rich men could gain God's grace through good works—and thereby escape purgatory—by providing cash endowments to charitable institutions. They also rejected the Catholic idea that the poor patients earned grace and salvation through their suffering. Protestants generally closed all the convents and most of the hospitals, sending women home to become housewives, often against their will. On the other hand, local officials recognized the public value of hospitals, and some were continued in Protestant lands, but without monks or nuns and in the control of local governments.

In London, the crown allowed two hospitals to continue their charitable work, under nonreligious control of city officials. The convents were all shut down but Harkness finds that women—some of them former nuns—were part of a new system that delivered essential medical services to people outside their family. They were employed by parishes and hospitals, as well as by private families, and provided nursing care as well as some medical, pharmaceutical, and surgical services.

Meanwhile, in Catholic lands such as France, rich families continued to fund convents and monasteries, and enrolled their daughters as nuns who provided free health services to the poor. Nursing was a religious role for the nurse, and there was little call for science.

During the Age of Enlightenment, the 18th-century, science was held in high esteem and physicians upgraded their social status by becoming more scientific. The health field was crowded with self-trained barber-surgeons, apothecaries, midwives, drug peddlers, and charlatans.

Across Europe medical schools relied primarily on lectures and readings. The final year student would have limited clinical experience by trailing the professor through the wards. Laboratory work was uncommon, and dissections were rarely done because of legal restrictions on cadavers. Most schools were small, and only Edinburgh, Scotland, with 11,000 alumni, produced large numbers of graduates.

In Britain, there were but three small hospitals after 1550. Pelling and Webster estimate that in London in the 1580 to 1600 period, out of a population of nearly 200,000 people, there were about 500 medical practitioners. Nurses and midwives are not included. There were about 50 physicians, 100 licensed surgeons, 100 apothecaries, and 250 additional unlicensed practitioners. In the last category about 25% were women. All across Britain—and indeed all of the world—the vast majority of the people in city, town or countryside depended for medical care on local amateurs with no professional training but with a reputation as wise healers who could diagnose problems and advise sick people what to do—and perhaps set broken bones, pull a tooth, give some traditional herbs or brews or perform a little magic to cure what ailed them.

The London Dispensary opened in 1696, the first clinic in the British Empire to dispense medicines to poor sick people. The innovation was slow to catch on, but new dispensaries were open in the 1770s. In the colonies, small hospitals opened in Philadelphia in 1752, New York in 1771, and Boston (Massachusetts General Hospital) in 1811.

Guy's Hospital, the first great British hospital opened in 1721 in London, with funding from businessman Thomas Guy. In 1821 a bequest of £200,000 by William Hunt in 1829 funded expansion for an additional hundred beds. Samuel Sharp (1709–78), a surgeon at Guy's Hospital, from 1733 to 1757, was internationally famous; his "A Treatise on the Operations of Surgery" (1st ed., 1739), was the first British study focused exclusively on operative technique.

English physician Thomas Percival (1740–1804) wrote a comprehensive system of medical conduct, "Medical Ethics, or a Code of Institutes and Precepts, Adapted to the Professional Conduct of Physicians and Surgeons" (1803) that set the standard for many textbooks.

In the Spanish Empire, the viceregal capital of Mexico City was a site of medical training for physicians and the creation of hospitals. Epidemic disease had decimated indigenous populations starting with the early sixteenth-century Spanish conquest of the Aztec empire, when a black auxiliary in the armed forces of conqueror Hernán Cortés, with an active case of smallpox, set off a virgin land epidemic among indigenous peoples, Spanish allies and enemies alike. Aztec emperor Cuitlahuac died of smallpox. Disease was a significant factor in the Spanish conquest elsewhere as well.

Medical education instituted at the Royal and Pontifical University of Mexico chiefly served the needs of urban elites. Male and female "curanderos" or lay practitioners, attended to the ills of the popular classes. The Spanish crown began regulating the medical profession just a few years after the conquest, setting up the Royal Tribunal of the Protomedicato, a board for licensing medical personnel in 1527. Licensing became more systematic after 1646 with physicians, druggists, surgeons, and bleeders requiring a license before they could publicly practice. Crown regulation of medical practice became more general in the Spanish empire.

Elites and the popular classes alike called on divine intervention in personal and society-wide health crises, such as the epidemic of 1737. The intervention of the Virgin of Guadalupe was depicted in a scene of dead and dying Indians, with elites on their knees praying for her aid. In the late eighteenth century, the crown began implementing secularizing policies on the Iberian peninsula and its overseas empire to control disease more systematically and scientifically.

The practice of medicine changed in the face of rapid advances in science, as well as new approaches by physicians. Hospital doctors began much more systematic analysis of patients' symptoms in diagnosis. Among the more powerful new techniques were anaesthesia, and the development of both antiseptic and aseptic operating theatres. Effective cures were developed for certain endemic infectious diseases. However the decline in many of the most lethal diseases was due more to improvements in public health and nutrition than to advances in medicine.

Medicine was revolutionized in the 19th century and beyond by advances in chemistry, laboratory techniques, and equipment. Old ideas of infectious disease epidemiology were gradually replaced by advances in bacteriology and virology.

In the 1830s in Italy, Agostino Bassi traced the silkworm disease muscardine to microorganisms. Meanwhile, in Germany, Theodor Schwann led research on alcoholic fermentation by yeast, proposing that living microorganisms were responsible.
Leading chemists, such as Justus von Liebig, seeking solely physicochemical explanations, derided this claim and alleged that Schwann was regressing to vitalism.

In 1847 in Vienna, Ignaz Semmelweis (1818–1865), dramatically reduced the death rate of new mothers (due to childbed fever) by requiring physicians to clean their hands before attending childbirth, yet his principles were marginalized and attacked by professional peers. At that time most people still believed that infections were caused by foul odors called miasmas.

Eminent French scientist Louis Pasteur confirmed Schwann's fermentation experiments in 1857 and afterwards supported the hypothesis that yeast were microorganisms. Moreover, he suggested that such a process might also explain contagious disease. In 1860, Pasteur's report on bacterial fermentation of butyric acid motivated fellow Frenchman Casimir Davaine to identify a similar species (which he called "bacteridia") as the pathogen of the deadly disease anthrax. Others dismissed "bacteridia" as a mere byproduct of the disease. British surgeon Joseph Lister, however, took these findings seriously and subsequently introduced antisepsis to wound treatment in 1865.

German physician Robert Koch, noting fellow German Ferdinand Cohn's report of a spore stage of a certain bacterial species, traced the life cycle of Davaine's "bacteridia", identified spores, inoculated laboratory animals with them, and reproduced anthrax—a breakthrough for experimental pathology and germ theory of disease. Pasteur's group added ecological investigations confirming spores' role in the natural setting, while Koch published a landmark treatise in 1878 on the bacterial pathology of wounds. In 1881, Koch reported discovery of the "tubercle bacillus", cementing germ theory and Koch's acclaim.

Upon the outbreak of a cholera epidemic in Alexandria, Egypt, two medical missions went to investigate and attend the sick, one was sent out by Pasteur and the other led by Koch. Koch's group returned in 1883, having successfully discovered the cholera pathogen. In Germany, however, Koch's bacteriologists had to vie against Max von Pettenkofer, Germany's leading proponent of miasmatic theory. Pettenkofer conceded bacteria's casual involvement, but maintained that other, environmental factors were required to turn it pathogenic, and opposed water treatment as a misdirected effort amid more important ways to improve public health. The massive cholera epidemic in Hamburg in 1892 devastasted Pettenkoffer's position, and yielded German public health to "Koch's bacteriology".

On losing the 1883 rivalry in Alexandria, Pasteur switched research direction, and introduced his third vaccine—rabies vaccine—the first vaccine for humans since Jenner's for smallpox. From across the globe, donations poured in, funding the founding of Pasteur Institute, the globe's first biomedical institute, which opened in 1888. Along with Koch's bacteriologists, Pasteur's group—which preferred the term "microbiology"—led medicine into the new era of "scientific medicine" upon bacteriology and germ theory. Accepted from Jakob Henle, Koch's steps to confirm a species' pathogenicity became famed as "Koch's postulates". Although his proposed tuberculosis treatment, tuberculin, seemingly failed, it soon was used to test for infection with the involved species. In 1905, Koch was awarded the Nobel Prize in Physiology or Medicine, and remains renowned as the founder of medical microbiology.

Women had always served in ancillary roles, and as midwives and healers. The professionalization of medicine forced them increasingly to the sidelines. As hospitals multiplied they relied in Europe on orders of Roman Catholic nun-nurses, and German Protestant and Anglican deaconesses in the early 19th century. They were trained in traditional methods of physical care that involved little knowledge of medicine. The breakthrough to professionalization based on knowledge of advanced medicine was led by Florence Nightingale in England. She resolved to provide more advanced training than she saw on the Continent. At Kaiserswerth, where the first German nursing schools were founded in 1836 by Theodor Fliedner, she said, "The nursing was nil and the hygiene horrible.") Britain's male doctors preferred the old system, but Nightingale won out and her Nightingale Training School opened in 1860 and became a model. The Nightingale solution depended on the patronage of upper class women, and they proved eager to serve. Royalty became involved. In 1902 the wife of the British king took control of the nursing unit of the British army, became its president, and renamed it after herself as the Queen Alexandra's Royal Army Nursing Corps; when she died the next queen became president. Today its Colonel In Chief is Sophie, Countess of Wessex, the daughter-in-law of Queen Elizabeth II. In the United States, upper middle class women who already supported hospitals promoted nursing. The new profession proved highly attractive to women of all backgrounds, and schools of nursing opened in the late 19th century. They soon a function of large hospitals, where they provided a steady stream of low-paid idealistic workers. The International Red Cross began operations in numerous countries in the late 19th century, promoting nursing as an ideal profession for middle class women.

The Nightingale model was widely copied. Linda Richards (1841–1930) studied in London and became the first professionally trained American nurse. She established nursing training programs in the United States and Japan, and created the first system for keeping individual medical records for hospitalized patients. The Russian Orthodox Church sponsored seven orders of nursing sisters in the late 19th century. They ran hospitals, clinics, almshouses, pharmacies, and shelters as well as training schools for nurses. In the Soviet era (1917–1991), with the aristocratic sponsors gone, nursing became a low-prestige occupation based in poorly maintained hospitals.

It was very difficult for women to become doctors in any field before the 1970s. Elizabeth Blackwell (1821–1910) became the first woman to formally study and practice medicine in the United States. She was a leader in women's medical education. While Blackwell viewed medicine as a means for social and moral reform, her student Mary Putnam Jacobi (1842–1906) focused on curing disease. At a deeper level of disagreement, Blackwell felt that women would succeed in medicine because of their humane female values, but Jacobi believed that women should participate as the equals of men in all medical specialties using identical methods, values and insights. In the Soviet Union although the majority of medical doctors were women, they were paid less than the mostly male factory workers.

Paris (France) and Vienna were the two leading medical centers on the Continent in the era 1750–1914.

In the 1770s–1850s Paris became a world center of medical research and teaching. The "Paris School" emphasized that teaching and research should be based in large hospitals and promoted the professionalization of the medical profession and the emphasis on sanitation and public health. A major reformer was Jean-Antoine Chaptal (1756–1832), a physician who was Minister of Internal Affairs. He created the Paris Hospital, health councils, and other bodies.
Louis Pasteur (1822–1895) was one of the most important founders of medical microbiology. He is remembered for his remarkable breakthroughs in the causes and preventions of diseases. His discoveries reduced mortality from puerperal fever, and he created the first vaccines for rabies and anthrax. His experiments supported the germ theory of disease. He was best known to the general public for inventing a method to treat milk and wine in order to prevent it from causing sickness, a process that came to be called pasteurization. He is regarded as one of the three main founders of microbiology, together with Ferdinand Cohn and Robert Koch. He worked chiefly in Paris and in 1887 founded the Pasteur Institute there to perpetuate his commitment to basic research and its practical applications. As soon as his institute was created, Pasteur brought together scientists with various specialties. The first five departments were directed by Emile Duclaux (general microbiology research) and Charles Chamberland (microbe research applied to hygiene), as well as a biologist, Ilya Ilyich Mechnikov (morphological microbe research) and two physicians, Jacques-Joseph Grancher (rabies) and Emile Roux (technical microbe research). One year after the inauguration of the Institut Pasteur, Roux set up the first course of microbiology ever taught in the world, then entitled "Cours de Microbie Technique" (Course of microbe research techniques). It became the model for numerous research centers around the world named "Pasteur Institutes."

The First Viennese School of Medicine, 1750–1800, was led by the Dutchman Gerard van Swieten (1700–1772), who aimed to put medicine on new scientific foundations – promoting unprejudiced clinical observation, botanical and chemical research, and introducing simple but powerful remedies. When the Vienna General Hospital opened in 1784, it at once became the world's largest hospital and physicians acquired a facility that gradually developed into the most important research centre. Progress ended with the Napoleonic wars and the government shutdown in 1819 of all liberal journals and schools; this caused a general return to traditionalism and eclecticism in medicine.

Vienna was the capital of a diverse empire and attracted not just Germans but Czechs, Hungarians, Jews, Poles and others to its world-class medical facilities. After 1820 the Second Viennese School of Medicine emerged with the contributions of physicians such as Carl Freiherr von Rokitansky, Josef Škoda, Ferdinand Ritter von Hebra, and Ignaz Philipp Semmelweis. Basic medical science expanded and specialization advanced. Furthermore, the first dermatology, eye, as well as ear, nose, and throat clinics in the world were founded in Vienna. The textbook of ophthalmologist Georg Joseph Beer (1763–1821) "Lehre von den Augenkrankheiten" combined practical research and philosophical speculations, and became the standard reference work for decades.

After 1871 Berlin, the capital of the new German Empire, became a leading center for medical research. Robert Koch (1843–1910) was a representative leader. He became famous for isolating "Bacillus anthracis" (1877), the "Tuberculosis bacillus" (1882) and "Vibrio cholerae" (1883) and for his development of Koch's postulates. He was awarded the Nobel Prize in Physiology or Medicine in 1905 for his tuberculosis findings. Koch is one of the founders of microbiology, inspiring such major figures as Paul Ehrlich and Gerhard Domagk.

In the American Civil War (1861–65), as was typical of the 19th century, more soldiers died of disease than in battle, and even larger numbers were temporarily incapacitated by wounds, disease and accidents. Conditions were poor in the Confederacy, where doctors and medical supplies were in short supply. The war had a dramatic long-term impact on medicine in the U.S., from surgical technique to hospitals to nursing and to research facilities. Weapon development -particularly the appearance of Springfield Model 1861, mass-produced and much more accurate than muskets led to generals underestimating the risks of long range rifle fire; risks exemplified in the death of John Sedgwick and the disastrous Pickett's Charge. The rifles could shatter bone forcing amputation and longer ranges meant casualties were sometimes not quickly found. Evacuation of the wounded from Second Battle of Bull Run took a week. As in earlier wars, untreated casualties sometimes survived unexpectedly due to maggots debriding the wound -an observation which led to the surgical use of maggots -still a useful method in the absence of effective antibiotics.

The hygiene of the training and field camps was poor, especially at the beginning of the war when men who had seldom been far from home were brought together for training with thousands of strangers. First came epidemics of the childhood diseases of chicken pox, mumps, whooping cough, and, especially, measles. Operations in the South meant a dangerous and new disease environment, bringing diarrhea, dysentery, typhoid fever, and malaria. There were no antibiotics, so the surgeons prescribed coffee, whiskey, and quinine. Harsh weather, bad water, inadequate shelter in winter quarters, poor policing of camps, and dirty camp hospitals took their toll.

This was a common scenario in wars from time immemorial, and conditions faced by the Confederate army were even worse. The Union responded by building army hospitals in every state. What was different in the Union was the emergence of skilled, well-funded medical organizers who took proactive action, especially in the much enlarged United States Army Medical Department, and the United States Sanitary Commission, a new private agency. Numerous other new agencies also targeted the medical and morale needs of soldiers, including the United States Christian Commission as well as smaller private agencies.

The U.S. Army learned many lessons and in August 1886, it established the Hospital Corps.

A major breakthrough in epidemiology came with the introduction of statistical maps and graphs. They allowed careful analysis of seasonality issues in disease incidents, and the maps allowed public health officials to identify critical loci for the dissemination of disease. John Snow in London developed the methods. In 1849, he observed that the symptoms of cholera, which had already claimed around 500 lives within a month, were vomiting and diarrhoea. He concluded that the source of contamination must be through ingestion, rather than inhalation as was previously thought. It was this insight that resulted in the removal of The Pump On Broad Street, after which deaths from cholera plummeted afterwards. English nurse Florence Nightingale pioneered analysis of large amounts of statistical data, using graphs and tables, regarding the condition of thousands of patients in the Crimean War to evaluate the efficacy of hospital services. Her methods proved convincing and led to reforms in military and civilian hospitals, usually with the full support of the government.

By the late 19th and early 20th century English statisticians led by Francis Galton, Karl Pearson and Ronald Fisher developed the mathematical tools such as correlations and hypothesis tests that made possible much more sophisticated analysis of statistical data.

During the U.S. Civil War the Sanitary Commission collected enormous amounts of statistical data, and opened up the problems of storing information for fast access and mechanically searching for data patterns. The pioneer was John Shaw Billings (1838–1913). A senior surgeon in the war, Billings built the Library of the Surgeon General's Office (now the National Library of Medicine), the centerpiece of modern medical information systems. Billings figured out how to mechanically analyze medical and demographic data by turning facts into numbers and punching the numbers onto cardboard cards that could be sorted and counted by machine. The applications were developed by his assistant Herman Hollerith; Hollerith invented the punch card and counter-sorter system that dominated statistical data manipulation until the 1970s. Hollerith's company became International Business Machines (IBM) in 1911.

Johns Hopkins Hospital, founded in 1889, originated several modern medical practices, including residency and rounds.

European ideas of modern medicine were spread widely through the world by medical missionaries, and the dissemination of textbooks. Japanese elites enthusiastically embraced Western medicine after the Meiji Restoration of the 1860s. However they had been prepared by their knowledge of the Dutch and German medicine, for they had some contact with Europe through the Dutch. Highly influential was the 1765 edition of Hendrik van Deventer's pioneer work "Nieuw Ligt" ("A New Light") on Japanese obstetrics, especially on Katakura Kakuryo's publication in 1799 of "Sanka Hatsumo" ("Enlightenment of Obstetrics"). A cadre of Japanese physicians began to interact with Dutch doctors, who introduced smallpox vaccinations. By 1820 Japanese ranpô medical practitioners not only translated Dutch medical texts, they integrated their readings with clinical diagnoses. These men became leaders of the modernization of medicine in their country. They broke from Japanese traditions of closed medical fraternities and adopted the European approach of an open community of collaboration based on expertise in the latest scientific methods.

Kitasato Shibasaburō (1853–1931) studied bacteriology in Germany under Robert Koch. In 1891 he founded the Institute of Infectious Diseases in Tokyo, which introduced the study of bacteriology to Japan. He and French researcher Alexandre Yersin went to Hong Kong in 1894, where; Kitasato confirmed Yersin's discovery that the bacterium "Yersinia pestis" is the agent of the plague. In 1897 he isolates and described the organism that caused dysentery. He became the first dean of medicine at Keio University, and the first president of the Japan Medical Association.

Japanese physicians immediately recognized the values of X-Rays. They were able to purchase the equipment locally from the Shimadzu Company, which developed, manufactured, marketed, and distributed X-Ray machines after 1900. Japan not only adopted German methods of public health in the home islands, but implemented them in its colonies, especially Korea and Taiwan, and after 1931 in Manchuria. A heavy investment in sanitation resulted in a dramatic increase of life expectancy.

Until the nineteenth century, the care of the insane was largely a communal and family responsibility rather than a medical one. The vast majority of the mentally ill were treated in domestic contexts with only the most unmanageable or burdensome likely to be institutionally confined. This situation was transformed radically from the late eighteenth century as, amid changing cultural conceptions of madness, a new-found optimism in the curability of insanity within the asylum setting emerged. Increasingly, lunacy was perceived less as a physiological condition than as a mental and moral one to which the correct response was persuasion, aimed at inculcating internal restraint, rather than external coercion. This new therapeutic sensibility, referred to as moral treatment, was epitomised in French physician Philippe Pinel's quasi-mythological unchaining of the lunatics of the Bicêtre Hospital in Paris and realised in an institutional setting with the foundation in 1796 of the Quaker-run York Retreat in England.

From the early nineteenth century, as lay-led lunacy reform movements gained in influence, ever more state governments in the West extended their authority and responsibility over the mentally ill. Small-scale asylums, conceived as instruments to reshape both the mind and behaviour of the disturbed, proliferated across these regions. By the 1830s, moral treatment, together with the asylum itself, became increasingly medicalised and asylum doctors began to establish a distinct medical identity with the establishment in the 1840s of associations for their members in France, Germany, the United Kingdom and America, together with the founding of medico-psychological journals. Medical optimism in the capacity of the asylum to cure insanity soured by the close of the nineteenth century as the growth of the asylum population far outstripped that of the general population. Processes of long-term institutional segregation, allowing for the psychiatric conceptualisation of the natural course of mental illness, supported the perspective that the insane were a distinct population, subject to mental pathologies stemming from specific medical causes. As degeneration theory grew in influence from the mid-nineteenth century, heredity was seen as the central causal element in chronic mental illness, and, with national asylum systems overcrowded and insanity apparently undergoing an inexorable rise, the focus of psychiatric therapeutics shifted from a concern with treating the individual to maintaining the racial and biological health of national populations.

Emil Kraepelin (1856–1926) introduced new medical categories of mental illness, which eventually came into psychiatric usage despite their basis in behavior rather than pathology or underlying cause. Shell shock among frontline soldiers exposed to heavy artillery bombardment was first diagnosed by British Army doctors in 1915. By 1916, similar symptoms were also noted in soldiers not exposed to explosive shocks, leading to questions as to whether the disorder was physical or psychiatric. In the 1920s surrealist opposition to psychiatry was expressed in a number of surrealist publications. In the 1930s several controversial medical practices were introduced including inducing seizures (by electroshock, insulin or other drugs) or cutting parts of the brain apart (leucotomy or lobotomy). Both came into widespread use by psychiatry, but there were grave concerns and much opposition on grounds of basic morality, harmful effects, or misuse.

In the 1950s new psychiatric drugs, notably the antipsychotic chlorpromazine, were designed in laboratories and slowly came into preferred use. Although often accepted as an advance in some ways, there was some opposition, due to serious adverse effects such as tardive dyskinesia. Patients often opposed psychiatry and refused or stopped taking the drugs when not subject to psychiatric control. There was also increasing opposition to the use of psychiatric hospitals, and attempts to move people back into the community on a collaborative user-led group approach ("therapeutic communities") not controlled by psychiatry. Campaigns against masturbation were done in the Victorian era and elsewhere. Lobotomy was used until the 1970s to treat schizophrenia. This was denounced by the anti-psychiatric movement in the 1960s and later.

The ABO blood group system was discovered in 1901, and the Rhesus blood group system in 1937, facilitating blood transfusion.

During the 20th century, large-scale wars were attended with medics and mobile hospital units which developed advanced techniques for healing massive injuries and controlling infections rampant in battlefield conditions. During the Mexican Revolution (1910–1920), General Pancho Villa organized hospital trains for wounded soldiers. Boxcars marked "Servicio Sanitario" ("sanitary service") were re-purposed as surgical operating theaters and areas for recuperation, and staffed by up to 40 Mexican and U.S. physicians. Severely wounded soldiers were shuttled back to base hospitals. Canadian physician Norman Bethune, M.D. developed a mobile blood-transfusion service for frontline operations in the Spanish Civil War (1936–1939), but ironically, he himself died of blood poisoning.
Thousands of scarred troops provided the need for improved prosthetic limbs and expanded techniques in plastic surgery or reconstructive surgery. Those practices were combined to broaden cosmetic surgery and other forms of elective surgery.

During the First World War, Alexis Carrel and Henry Dakin developed the Carrel-Dakin method of treating wounds with an irrigation, Dakin's solution, a germicide which helped prevent gangrene.

The Great War spurred the usage of Roentgen's X-ray, and the electrocardiograph, for the monitoring of internal bodily functions. This was followed in the inter-war period by the development of the first anti-bacterial agents such as the sulpha antibiotics.

Public health measures became particular important during the 1918 flu pandemic, which killed at least 50 million people around the world. It became an important case study in epidemiology. Bristow shows there was a gendered response of health caregivers to the pandemic in the United States. Male doctors were unable to cure the patients, and they felt like failures. Women nurses also saw their patients die, but they took pride in their success in fulfilling their professional role of caring for, ministering, comforting, and easing the last hours of their patients, and helping the families of the patients cope as well.

From 1917 to 1923, the American Red Cross moved into Europe with a battery of long-term child health projects. It built and operated hospitals and clinics, and organized antituberculosis and antityphus campaigns. A high priority involved child health programs such as clinics, better baby shows, playgrounds, fresh air camps, and courses for women on infant hygiene. Hundreds of U.S. doctors, nurses, and welfare professionals administered these programs, which aimed to reform the health of European youth and to reshape European public health and welfare along American lines.

The advances in medicine made a dramatic difference for Allied troops, while the Germans and especially the Japanese and Chinese suffered from a severe lack of newer medicines, techniques and facilities. Harrison finds that the chances of recovery for a badly wounded British infantryman were as much as 25 times better than in the First World War. The reason was that:

Unethical human subject research, and killing of patients with disabilities, peaked during the Nazi era, with Nazi human experimentation and Aktion T4 during the Holocaust as the most significant examples. Many of the details of these and related events were the focus of the Doctors' Trial. Subsequently, principles of medical ethics, such as the Nuremberg Code, were introduced to prevent a recurrence of such atrocities. After 1937, the Japanese Army established programs of biological warfare in China. In Unit 731, Japanese doctors and research scientists conducted large numbers of vivisections and experiments on human beings, mostly Chinese victims.

Starting in World War II, DDT was used as insecticide to combat insect vectors carrying malaria, which was endemic in most tropical regions of the world. The first goal was to protect soldiers, but it was widely adopted as a public health device. In Liberia, for example, the United States had large military operations during the war and the U.S. Public Health Service began the use of DDT for indoor residual spraying (IRS) and as a larvicide, with the goal of controlling malaria in Monrovia, the Liberian capital. In the early 1950s, the project was expanded to nearby villages. In 1953, the World Health Organization (WHO) launched an antimalaria program in parts of Liberia as a pilot project to determine the feasibility of malaria eradication in tropical Africa. However these projects encountered a spate of difficulties that foreshadowed the general retreat from malaria eradication efforts across tropical Africa by the mid-1960s.

The World Health Organization was founded in 1948 as a United Nations agency to improve global health. In most of the world, life expectancy has improved since then, and was about 67 years as of 2010, and well above 80 years in some countries. Eradication of infectious diseases is an international effort, and several new vaccines have been developed during the post-war years, against infections such as measles, mumps, several strains of influenza and human papilloma virus. The long-known vaccine against Smallpox finally eradicated the disease in the 1970s, and Rinderpest was wiped out in 2011. Eradication of polio is underway. Tissue culture is important for development of vaccines. Though the early success of antiviral vaccines and antibacterial drugs, antiviral drugs were not introduced until the 1970s. Through the WHO, the international community has developed a response protocol against epidemics, displayed during the SARS epidemic in 2003, the Influenza A virus subtype H5N1 from 2004, the Ebola virus epidemic in West Africa and onwards.

As infectious diseases have become less lethal, and the most common causes of death in developed countries are now tumors and cardiovascular diseases, these conditions have received increased attention in medical research. Tobacco smoking as a cause of lung cancer was first researched in the 1920s, but was not widely supported by publications until the 1950s. Cancer treatment has been developed with radiotherapy, chemotherapy and surgical oncology.

Oral rehydration therapy has been extensively used since the 1970s to treat cholera and other diarrhea-inducing infections.

The sexual revolution included taboo-breaking research in human sexuality such as the 1948 and 1953 Kinsey reports, invention of hormonal contraception, and the normalization of abortion and homosexuality in many countries. Family planning has promoted a demographic transition in most of the world. With threatening sexually transmitted infections, not least HIV, use of barrier contraception has become imperative. The struggle against HIV has improved antiretroviral treatments.

X-ray imaging was the first kind of medical imaging, and later ultrasonic imaging, CT scanning, MR scanning and other imaging methods became available.

Genetics have advanced with the discovery of the DNA molecule, genetic mapping and gene therapy. Stem cell research took off in the 2000s (decade), with stem cell therapy as a promising method.

Evidence-based medicine is a modern concept, not introduced to literature until the 1990s.

Prosthetics have improved. In 1958, Arne Larsson in Sweden became the first patient to depend on an artificial cardiac pacemaker. He died in 2001 at age 86, having outlived its inventor, the surgeon, and 26 pacemakers. Lightweight materials as well as neural prosthetics emerged in the end of the 20th century.

Cardiac surgery was revolutionized in the 1948 as open-heart surgery was introduced for the first time since 1925.

In 1954 Joseph Murray, J. Hartwell Harrison and others accomplished the first kidney transplantation. Transplantations of other organs, such as heart, liver and pancreas, were also introduced during the later 20th century. The first partial face transplant was performed in 2005, and the first full one in 2010. By the end of the 20th century, microtechnology had been used to create tiny robotic devices to assist microsurgery using micro-video and fiber-optic cameras to view internal tissues during surgery with minimally invasive practices.

Laparoscopic surgery was broadly introduced in the 1990s. Natural orifice surgery has followed. Remote surgery is another recent development, with the Lindbergh operation in 2001 as a groundbreaking example.








</doc>
<doc id="14196" url="https://en.wikipedia.org/wiki?curid=14196" title="Hamoaze">
Hamoaze

The Hamoaze (; ) is an estuarine stretch of the tidal River Tamar, between its confluence with the River Lynher and Plymouth Sound, England.

The name first appears as "ryver of Hamose" in 1588 and it originally most likely applied just to a creek of the estuary that led up to the manor of Ham, north of the present-day Devonport Dockyard. The name evidently later came to be used for the estuary's main channel. The "ose" element possibly derives from Old English "wāse" meaning 'mud' (as in 'ooze') – the creek consisting of mud-banks at low tide.

The Hamoaze flows past Devonport Dockyard, which is one of three major bases of the Royal Navy today. The presence of large numbers of small watercraft are a challenge and hazard to the warships using the naval base and dockyard. Navigation on the waterway is controlled by the Queen's Harbour Master for Plymouth.

Settlements on the banks of the Hamoaze are Saltash, Wilcove, Torpoint and Cremyll in Cornwall, as well as Devonport and Plymouth in Devon.

Two regular ferry services crossing the Hamoaze exist: the Torpoint Ferry (a chain ferry that takes vehicles) and the Cremyll Ferry (passengers and cyclists only).


</doc>
<doc id="14197" url="https://en.wikipedia.org/wiki?curid=14197" title="Hanover">
Hanover

Hanover or Hannover (; ), on the River Leine, is the capital and largest city of the German state of Lower Saxony ("Niedersachsen"), and was once by personal union the family seat of the Hanoverian Kings of the United Kingdom of Great Britain and Ireland, under their title as the dukes of Brunswick-Lüneburg (later described as the Elector of Hanover). At the end of the Napoleonic Wars, the Electorate was enlarged to become a Kingdom with Hanover as its capital.

From 1868 to 1946 Hanover was the capital of the Prussian Province of Hanover and afterward of the Hanover administrative region until that was abolished in 2005. Since 2001 it has been part of the Hanover district ("Region Hannover"), which is a municipal body made up of the former district ("Landkreis Hannover") and city of Hanover (note: although both "Region" and "Landkreis" are translated as "district" they are not the same).

With a population of 518,000, Hanover is a major center of Northern Germany and the country's thirteenth largest city. Hanover hosts annual commercial trade fairs such as the Hanover Fair and the CeBIT. Every year Hanover hosts the Schützenfest Hannover, the world's largest marksmen's festival, and the Oktoberfest Hannover, the second largest festival of its kind in Germany. In 2000, Hanover hosted the world fair Expo 2000. The Hanover fairground, due to numerous extensions, especially for the Expo 2000, is the largest in the world. Hanover is of national importance because of its universities and medical school, its international airport and its large zoo. The city is also a major crossing point of railway lines and highways (Autobahnen), connecting European main lines in both the east-west (Berlin–Ruhr area) and north-south (Hamburg–Munich, etc.) directions.

"Hanover" is the traditional English spelling. The German spelling (with a double n) is becoming more popular in English; recent editions of encyclopedias prefer the German spelling, and the local government uses the German spelling on English websites. The English pronunciation, with stress on the first syllable, is applied to both the German and English spellings, which is different from German pronunciation, with stress on the second syllable and a long second vowel. The traditional English spelling is still used in historical contexts, especially when referring to the British House of Hanover.

Hanover was founded in medieval times on the east bank of the River Leine. Its original name "Honovere" may mean "high (river)bank", though this is debated (cf. "das Hohe Ufer"). Hanover was a small village of ferrymen and fishermen that became a comparatively large town in the 13th century, receiving town privileges in 1241, due to its position at a natural crossroads. As overland travel was relatively difficult, its position on the upper navigable reaches of the river helped it to grow by increasing trade. It was connected to the Hanseatic League city of Bremen by the Leine, and was situated near the southern edge of the wide North German Plain and north-west of the Harz mountains, so that east-west traffic such as mule trains passed through it. Hanover was thus a gateway to the Rhine, Ruhr and Saar river valleys, their industrial areas which grew up to the southwest and the plains regions to the east and north, for overland traffic skirting the Harz between the Low Countries and Saxony or Thuringia.

In the 14th century the main churches of Hanover were built, as well as a city wall with three city gates. The beginning of industrialization in Germany led to trade in iron and silver from the northern Harz Mountains, which increased the city's importance.

In 1636 George, Duke of Brunswick-Lüneburg, ruler of the Brunswick-Lüneburg principality of Calenberg, moved his residence to Hanover. The Dukes of Brunswick-Lüneburg were elevated by the Holy Roman Emperor to the rank of Prince-Elector in 1692, and this elevation was confirmed by the Imperial Diet in 1708. Thus the principality was upgraded to the Electorate of Brunswick-Lüneburg, colloquially known as the Electorate of Hanover after Calenberg's capital (see also: House of Hanover). Its Electors later become monarchs of Great Britain (and from 1801, of the United Kingdom of Great Britain and Ireland). The first of these was George I Louis, who acceded to the British throne in 1714. The last British monarch who reigned in Hanover was William IV. Semi-Salic law, which required succession by the male line if possible, forbade the accession of Queen Victoria in Hanover. As a male-line descendant of George I, Queen Victoria was herself a member of the House of Hanover. Her descendants, however, bore her husband's titular name of Saxe-Coburg-Gotha. Three kings of Great Britain, or the United Kingdom, were concurrently also Electoral Princes of Hanover.

During the time of the personal union of the crowns of the United Kingdom and Hanover (1714–1837), the monarchs rarely visited the city. In fact, during the reigns of the final three joint rulers (1760–1837), there was only one short visit, by George IV in 1821. From 1816 to 1837 Viceroy Adolphus represented the monarch in Hanover.

During the Seven Years' War, the Battle of Hastenbeck was fought near the city on 26 July 1757. The French army defeated the Hanoverian Army of Observation, leading to the city's occupation as part of the Invasion of Hanover. It was recaptured by Anglo-German forces led by Ferdinand of Brunswick the following year.

After Napoleon imposed the Convention of Artlenburg (Convention of the Elbe) on July 5, 1803, about 35,000 French soldiers occupied Hanover. The Convention also required disbanding the army of Hanover. However, George III did not recognize the Convention of the Elbe. This resulted in a great number of soldiers from Hanover eventually emigrating to Great Britain, where the King's German Legion was formed. It was only troops from Hanover and Brunswick that consistently opposed France throughout the entire Napoleonic wars. The Legion later played an important role in the Battle of Waterloo in 1815. The Congress of Vienna in 1815 elevated the electorate to the Kingdom of Hanover. The capital town Hanover expanded to the western bank of the Leine and since then has grown considerably.

In 1837, the personal union of the United Kingdom and Hanover ended because William IV's heir in the United Kingdom was female (Queen Victoria). Hanover could be inherited only by male heirs. Thus, Hanover passed to William IV's brother, Ernest Augustus, and remained a kingdom until 1866, when it was annexed by Prussia during the Austro-Prussian war. Despite Hanover being expected to defeat Prussia at the Battle of Langensalza, Prussia employed Moltke the Elder's Kesselschlacht order of battle to instead destroy the Hanoverian army. The city of Hanover became the capital of the Prussian Province of Hanover. After the annexation, the people of Hanover generally opposed the Prussian government.

To Hanover's industry, however, the new connection with Prussia meant an improvement in business. The introduction of free trade promoted economic growth and led to the recovery of the Gründerzeit (the founders' era). Between 1879 and 1902 Hanover's population grew from 87,600 to 313,940. 

In 1842 the first horse railway was inaugurated, and from 1893 an electric tram was installed. In 1887 Hanover's Emile Berliner invented the record and the gramophone.

After 1937 the Lord Mayor and the state commissioners of Hanover were members of the NSDAP (Nazi party). A large Jewish population then existed in Hanover. In October 1938, 484 Hanoverian Jews of Polish origin were expelled to Poland, including the Grynszpan family. However, Poland refused to accept them, leaving them stranded at the border with thousands of other Polish-Jewish deportees, fed only intermittently by the Polish Red Cross and Jewish welfare organisations. The Grynszpans' son Herschel Grynszpan was in Paris at the time. When he learned of what was happening, he drove to the German embassy in Paris and shot the German diplomat Eduard Ernst vom Rath, who died shortly afterwards.

The Nazis took this act as a pretext to stage a nationwide pogrom known as Kristallnacht (9 November 1938). On that day, the synagogue of Hanover, designed in 1870 by Edwin Oppler in neo-romantic style, was burnt by the Nazis.

In September 1941, through the "Action Lauterbacher" plan, a ghettoisation of the remaining Hanoverian Jewish families began. Even before the Wannsee Conference, on 15 December 1941, the first Jews from Hanover were deported to Riga. A total of 2,400 people were deported, and very few survived. During the war seven concentration camps were constructed in Hanover, in which many Jews were confined. Of the approximately 4,800 Jews who had lived in Hannover in 1938, fewer than 100 were still in the city when troops of the United States Army arrived on 10 April 1945 to occupy Hanover at the end of the war. Today, a memorial at the Opera Square is a reminder of the persecution of the Jews in Hanover. 
After the war a large group of Orthodox Jewish survivors of the nearby Bergen-Belsen concentration camp settled in Hanover.
As an important railroad and road junction and production center, Hanover was a major target for strategic bombing during World War II, including the Oil Campaign. Targets included the AFA (Stöcken), the Deurag-Nerag refinery (Misburg), the Continental plants (Vahrenwald and Limmer), the United light metal works (VLW) in Ricklingen and Laatzen (today Hanover fairground), the Hanover/Limmer rubber reclamation plant, the Hanomag factory (Linden) and the tank factory "M.N.H. Maschinenfabrik Niedersachsen" (Badenstedt). Residential areas were also targeted, and more than 6,000 civilians were killed by the Allied bombing raids. More than 90% of the city center was destroyed in a total of 88 bombing raids. After the war, the was not rebuilt and its ruins were left as a war memorial.

The Allied ground advance into Germany reached Hanover in April 1945. The US 84th Infantry Division captured the city on 10 April 1945.

Hanover was in the British zone of occupation of Germany, and became part of the new state (Land) of Lower Saxony in 1946.

Today Hanover is a Vice-President City of Mayors for Peace, an international mayoral organisation mobilising cities and citizens worldwide to abolish and eliminate nuclear weapons by the year 2020.

Hanover experiences an oceanic climate (Köppen climate classification "Cfb").



One of the most famous sights is the "Royal Gardens of Herrenhausen":

The "Great Garden" is an important European baroque garden. The palace itself was largely destroyed by Allied bombing but has been reconstructed and reopened in 2013. Among the points of interest is the "Grotto". Its interior was designed by the French artist Niki de Saint Phalle). The Great Garden consists of several parts and contains Europe's highest garden fountain. The historic "Garden Theatre" hosted the musicals of the German rock musician Heinz Rudolf Kunze.

The "Berggarten" is a botanical garden with the most varied collection of orchids in Europe. Some points of interest are the "Tropical House", the "Cactus House", the "Canary House" and the "Orchid House", and free-flying birds and butterflies. Near the entrance to the Berggarten is the historic "Library Pavillon". The "Mausoleum" of the Guelphs is also located in the Berggarten. Like the Great Garden, the Berggarten also consists of several parts, for example the "Paradies" and the "Prairie Garden". There is also the "Sea Life Centre Hanover", which is the first tropical aquarium in Germany.

The "Georgengarten" is an English landscape garden. The "Leibniz Temple" and the "Georgen Palace" are two points of interest there.

The landmark of Hanover is the New Town Hall ("Neues Rathaus"). Inside the building are four scale models of the city. A worldwide unique diagonal/arch elevator goes up the large dome at a 17 degree angle to an observation deck.

The "Hanover Zoo" received the Park Scout Award for the fourth year running in 2009/10, placing it among the best zoos in Germany. The zoo consists of several theme areas: Sambesi, Meyers Farm, Gorilla-Mountain, Jungle-Palace, and Mullewapp. Some smaller areas are Australia, the wooded area for wolves, and the so-called swimming area with many seabirds. There is also a tropical house, a jungle house, and a show arena. The new Canadian-themed area, Yukon Bay, opened in 2010. In 2010 the Hanover Zoo had over 1.6 million visitors.

Another point of interest is the "Old Town". In the centre are the large Marktkirche (Church St. Georgii et Jacobi, preaching venue of the bishop of the Lutheran Landeskirche Hannovers) and the "Old Town Hall". Nearby are the "Leibniz House", the "Nolte House", and the "Beguine Tower". The "Kreuz-Church-Quarter" around the "Kreuz Church" contains many little lanes. Nearby is the old royal sports hall, now called the "Ballhof" theatre. On the edge of the Old Town are the "Market Hall", the "Leine Palace", and the ruin of the "Aegidien Church" which is now a monument to the victims of war and violence. Through the "Marstall Gate" you arrive at the bank of the river "Leine", where the "Nanas" of Niki de Saint Phalle are located. They are part of the "Mile of Sculptures", which starts from Trammplatz, leads along the river bank, crosses Königsworther Square, and ends at the entrance of the Georgengarten. Near the Old Town is the district of Calenberger Neustadt where the Catholic Basilica Minor of "St. Clemens", the "Reformed Church" and the Lutheran Neustädter Hof- und Stadtkirche St. Johannis are located.

Some other popular sights are the "Waterloo Column", the "Laves House", the "Wangenheim Palace", the "Lower Saxony State Archives", the "Hanover Playhouse", the "Kröpcke Clock", the "Anzeiger Tower Block", the "Administration Building of the NORD/LB", the "Cupola Hall" of the Congress Centre, the "Lower Saxony Stock", the "Ministry of Finance", the "Garten Church", the "Luther Church", the "Gehry Tower" (designed by the American architect Frank O. Gehry), the specially designed "Bus Stops", the "Opera House", "the Central Station", the "Maschsee" lake and the city forest "Eilenriede", which is one of the largest of its kind in Europe. With around 40 parks, forests and gardens, a couple of lakes, two rivers and one canal, Hanover offers a large variety of leisure activities.

Since 2007 the historic "Leibniz Letters", which can be viewed in the "Gottfried Wilhelm Leibniz Library", are on UNESCO's Memory of the World Register.

Outside the city centre is the "EXPO-Park", the former site of EXPO 2000. Some points of interest are the "Planet M.", the former "German Pavillon", some nations' vacant pavilions, the "Expowale", the "EXPO-Plaza" and the "EXPO-Gardens" (Parc Agricole, EXPO-Park South and the Gardens of change). The fairground can be reached by the "Exponale", one of the largest pedestrian bridges in Europe.

The "Hanover fairground" is the largest Exhibition Centre in the world.
It provides 496,000 square metres of covered indoor space, 58,000 square metres of open-air space, 27 halls and pavilions. Many of the Exhibition Centre's halls are architectural highlights. Furthermore, it offers the Convention Center with its 35 function rooms, glassed-in areas between halls, grassy park-like recreation zones and its own heliport.

Two important sights on the fairground are the "Hermes Tower" (88.8 metres high) and the "EXPO Roof", the largest wooden roof in the world.

In the district of Anderten is the "European Cheese Centre", the only Cheese Experience Centre in Europe. Another tourist sight in Anderten is the "Hindenburg Lock", which was the biggest lock in Europe at the time of its construction in 1928. The "Tiergarten" (literally the "animals' garden") in the district of Kirchrode is a large forest originally used for deer and other game for the king's table.

In the district of Groß-Buchholz the 282-metre-high "Telemax" is located, which is the tallest building in Lower Saxony and the highest television tower in Northern Germany. Some other notable towers are the "VW-Tower" in the city centre and the old towers of the former middle-age defence belt: "Döhrener Tower", "Lister Tower" and the "Horse Tower".

The 36 most important sights of the city centre are connected with a -long red line, which is painted on the pavement. This so-called "Red Thread" marks out a walk that starts at the Tourist Information Office and ends on the Ernst-August-Square in front of the central station. There is also a guided sightseeing-bus tour through the city.

Hanover is headquarters for several Protestant organizations, including the World Communion of Reformed Churches, the Evangelical Church in Germany, the Reformed Alliance, the United Evangelical Lutheran Church of Germany, and the Independent Evangelical-Lutheran Church.

In 2015, 31.1% of the population were Protestant and 13.4% were Roman Catholic. The majority 55.5% were irreligious or other faith.

The "Historic Museum" describes the history of Hanover, from the medieval settlement "Honovere" to the world-famous Exhibition City of today. The museum focuses on the period from 1714 to 1834 when Hanover had a strong relationship with the British royal house.

With more than 4,000 members, the "Kestnergesellschaft" is the largest art society in Germany. The museum hosts exhibitions from classical modernist art to contemporary art. One big focus is put on film, video, contemporary music and architecture, room installments and big presentations of contemporary paintings, sculptures and video art.

The "Kestner-Museum" is located in the "House of 5.000 windows". The museum is named after August Kestner and exhibits 6,000 years of applied art in four areas: Ancient cultures, ancient Egypt, applied art and a valuable collection of historic coins.

The "KUBUS" is a forum for contemporary art. It features mostly exhibitions and projects of famous and important artists from Hanover.

The "Kunstverein Hannover" (Art Society Hanover) shows contemporary art and was established in 1832 as one of the first art societies in Germany. It is located in the "Künstlerhaus" (House of artists). There are around 7 international monografic and thematic Exhibitions in one year.

The "Lower Saxony State Museum" is the largest museum in Hanover. The "State Gallery" shows the European Art from the 11th to the 20th century, the "Nature Department" shows the zoology, geology, botanic, geology and a "Vivarium" with fishes, insects, reptiles and amphibians. The "Primeval Department" shows the primeval history of Lower Saxony and the "Folklore Department" shows the cultures from all over the world.

The "Sprengel Museum" shows the art of the 20th century. It is one of the most notable art museums in Germany. The focus is put on the classical modernist art with the collection of "Kurt Schwitters", works of German expressionism, and French cubism, the cabinet of abstracts, the graphics and the department of photography and media. Furthermore, the museum shows the famous works of the French artist Niki de Saint-Phalle.

The "Theatre Museum" shows an exhibition of the history of the theatre in Hanover from the 17th century up to now: opera, concert, drama and ballet. The museum also hosts several touring exhibitions during the year.

The "Wilhelm Busch Museum" is the "German Museum of Caricature and Critical Graphic Arts". The collection of the works of Wilhelm Busch and the extensive collection of cartoons and critical graphics is this museum unique in Germany. Furthermore, the museum hosts several exhibitions of national and international artists during the year.

A cabinet of coins is the "Münzkabinett der TUI-AG". The "Polizeigeschichtliche Sammlung Niedersachsen" is the largest police museum in Germany. Textiles from all over the world can be visited in the "Museum for textile art". The "EXPOseeum" is the museum of the world-exhibition "EXPO 2000 Hannover". Carpets and objects from the orient can be visited in the "Oriental Carpet Museum". The "Blind Man Museum" is a rarity in Germany, another one is only in Berlin. The "Museum of veterinary medicine" is unique in Germany. The "Museum for Energy History" describes the 150 years old history of the application of energy. The "Home Museum Ahlem" shows the history of the district of Ahlem. The "Mahn- und Gedenkstätte Ahlem" describes the history of the Jewish people in Hanover and the "Stiftung Ahlers Pro Arte / Kestner Pro Arte" shows modern art. Modern art is also the main topic of the "Kunsthalle Faust", the "Nord/LB Art Gellery" and of the "Foro Artistico / Eisfabrik".

Some leading art events in Hanover are the "Long Night of the museums" and the "Zinnober Kunstvolkslauf" which features all the galleries in Hanover.

People who are interested in astronomy should visit the "Observatory Geschwister Herschel" on the Lindener Mountain or the small planetarium inside of the Bismarck School.

Around 40 theatres are located in Hanover. The "Opera House", the "Schauspielhaus" (Play House), the "Ballhof eins", the "Ballhof zwei" and the "Cumberlandsche Galerie" belong to the "Lower Saxony State Theatre". The "Theater am Aegi" is Hanover's big theatre for musicals, shows and guest performances. The "Neues Theater" (New Theatre) is the boulevard theatre of Hanover. The "Theater für Niedersachsen" is another big theatre in Hanover, which also has an own musical company. Some of the most important musical productions are the rock musicals of the German rock musician Heinz Rudolph Kunze, which take place at the "Garden-Theatre" in the Great Garden.

Some important theatre-events are the "Tanztheater International", the "Long Night of the Theatres", the "Festival Theaterformen" and the "International Competition for Choreographs".

Hanover's leading cabaret-stage is the "GOP Variety theatre" which is located in the "Georgs Palace". Some other famous cabaret-stages are the "Variety Marlene", the "Uhu-Theatre". the theatre "Die Hinterbühne", the "Rampenlich Variety" and the revue-stage "TAK". The most important cabaret event is the "Kleines Fest im Großen Garten" (Little Festival in the Great Garden) which is the most successful cabaret festival in Germany. It features artists from around the world. Some other important events are the "Calenberger Cabaret Weeks", the "Hanover Cabaret Festival" and the "Wintervariety".

Hanover has two symphony orchestras: The Lower Saxon State Orchestra Hanover and the NDR Radiophilharmonie (North German Radio Philharmonic Orchestra). Two notable choirs have their homes in Hanover: the Girls Choir Hanover (Mädchenchor Hannover) and the Knabenchor Hannover (Boys Choir Hanover).

There are/were two big international competitions for classical music in Hanover:

The rock bands Scorpions and Fury in the Slaughterhouse are originally from Hanover. Acclaimed DJ Mousse T also has his main recording studio in the area. Rick J. Jordan, member of the band Scooter was born here in 1968. Eurovision Song Contest winner of 2010, Lena, is also from Hanover.

Hannover 96 (nickname "Die Roten" or 'The Reds') is the top local football team that is again playing in the Bundesliga top division after being relegated to the 2. Bundesliga after the 2015–16 season. Home games are played at the HDI-Arena, which hosted matches in the 1974 and 2006 World Cups and the Euro 1988. Their reserve team Hannover 96 II plays in the fourth league. Their home games were played in the traditional Eilenriedestadium till they moved to the HDI Arena due to DFL directives. Arminia Hannover is another very traditional soccer team in Hanover that has played in the first league for years and plays now in the Niedersachsen-West Liga (Lower Saxony League West). Home matches are played in the Rudolf-Kalweit-Stadium.

The Hannover Indians are the local ice hockey team. They play in the third tier. Their home games are played at the traditional Eisstadion am Pferdeturm. The Hannover Scorpions played in Hanover in Germany's top league until 2013 when they sold their license and moved to Langenhagen.

Hanover was one of the Rugby union capitals in Germany. The first German Rugby team was founded in Hanover in 1878. Hanover-based teams dominated the German Rugby scene for a long time. DRC Hannover plays in the first division, and "SV Odin von 1905" as well as SG 78/08 Hannover play in the second division.

The first German Fencing Club was founded in Hanover in 1862. Today there are three more Fencing Clubs in Hanover.

The Hannover Korbjäger are the city's top basketball team. They play their home games at the IGS Linden.

Hanover is a centre for Water sports. Thanks to the lake Maschsee, the rivers Ihme and Leine and to the channel Mittellandkanal, Hanover hosts sailing schools, yacht schools, waterski clubs, rowing clubs, canoe clubs and paddle clubs. The water polo team WASPO W98 plays in the first division.

The Hannover Regents play in the third Bundesliga (baseball) division.

The Hannover Grizzlies, Armina Spartans and Hannover Stampeders are the local American Football teams.

The Hannover Marathon is the biggest running event in Hanover with more than 11,000 participants and usually around 200.000 spectators. Some other important running events are the Gilde Stadtstaffel (relay), the Sport-Check Nachtlauf (night-running), the Herrenhäuser Team-Challenge, the Hannoversche Firmenlauf (company running) and the Silvesterlauf (sylvester running).

Hanover hosts also an important international cycle race: The "Nacht von Hannover" (night of Hanover). The race takes place around the Market Hall.

The lake Maschsee hosts the International Dragon Boat Races and the Canoe Polo-Tournament. Many regattas take place during the year. "Head of the river Leine" on the river Leine is one of the biggest rowing regattas in Hanover.

One of Germany's most successful dragon boat teams, the All Sports Team Hannover, which has won since its foundation in year 2000 more than 100 medals on national and international competitions, is doing practising on the Maschsee in the heart of Hannover. The All Sports Team has received the award "Team of the Year 2013" in Lower Saxony.

Some other important sport events are the Lower Saxony Beach Volleyball Tournament, the international horse show "German Classics" and the international ice hockey tournament Nations Cup.

Hanover is one of the leading Exhibition Cities in the world. Each year Hanover hosts more than 60 international and national exhibitions. The most popular ones are the "CeBIT", the "Hanover Fair", the "Domotex", the "Ligna", the "IAA Nutzfahrzeuge" and the "Agritechnica". Hanover also hosts a huge number of congresses and symposiums like "International Symposium on Society and Resource Management"

But Hanover is not only one of the most important Exhibition Cities in the world, it is also one of the German capitals for marksmen. The "Schützenfest Hannover" is the largest Marksmen's Fun Fair in the world and takes place once a year (late June to early July) (2014 - July 4th to the 13th). It consists of more than 260 rides and inns, five large beer tents and a big entertainment programme. The highlight of this fun fair is the long "Parade of the Marksmen" with more than 12.000 participants from all over the world, among them around 5.000 marksmen, 128 bands and more than 70 wagons, carriages and big festival vehicles. It is the longest procession in Europe. Around 2 million people visit this fun fair every year. The landmark of this Fun Fair is the biggest transportable Ferris Wheel in the world ( high). The origins of this fun fair is located in the year 1529.

Hanover also hosts one of the two largest Spring Festivals in Europe with around 180 rides and inns, 2 large beer tents and around 1.5 million visitors each year. The Oktoberfest Hannover is the second largest Oktoberfest in the world with around 160 rides and inns, two large beer tents and around 1 million visitors each year.

The "Maschsee Festival" takes place around the Maschsee Lake. Each year around 2 million visitors come to enjoy live music, comedy, cabaret and much more. It is the largest Volksfest of its kind in Northern Germany.

The Great Garden hosts every year the "International Fireworks Competition", and the "International Festival Weeks Herrenhausen" with lots of music and cabaret.

The "Carnival Procession" is around long and consists of 3.000 participants, around 30 festival vehicles and around 20 bands and takes place every year.

Some more festivals are for example the Festival "Feuer und Flamme" (Fire and Flames), the "Gartenfestival" (Garden Festival), the "Herbstfestival" (Autumn Festival), the "Harley Days", the "Steintor Festival" (Steintor is a party area in the city centre) and the "Lister-Meile-Festival" (Lister Meile is a large pedestrian area).

Hanover also hosts Food Festivals, for example the "Wine Festival" and the "Gourmet Festival".

Furthermore, Hanover hosts some special markets. The "Old Town Flea Market" is the oldest flea market in Germany and the "Market for Art and Trade" has a high reputation. Some other big markets are of course the "Christmas Markets of the City of Hanover" in the Old Town and city centre and the Lister Meile.

The city's central station, Hannover Hauptbahnhof, is a hub of the German high-speed ICE network. It is the starting point of the Hanover-Würzburg high-speed rail line and also the central hub for the Hanover S-Bahn. It offers many international and national connections.

Hanover and its area is served by Hanover/Langenhagen International Airport (IATA code: HAJ; ICAO code: EDDV)

Hanover is also an important hub of Germany's Autobahn network; the junction of two major autobahns, the A2 and A7 is at "Kreuz Hannover-Ost", at the northeastern edge of the city.

Local autobahns are A 352 (a short cut between A7 [north] and A2 [west], also known as the "airport autobahn" because it passes "Hanover Airport") and the A 37.

The Schnellweg "(en: expressway)" system, a number of Bundesstraße roads, forms a structure loosely resembling a large ring road together with A2 and A7. The roads are B 3, B 6 and Bundesstraße 65|B 65, called Westschnellweg (B6 on the northern part, B3 on the southern part), Messeschnellweg (B3, becomes A37 near Burgdorf, crosses A2, becomes B3 again, changes to B6 at "Seelhorster Kreuz", then passes the Hanover fairground as B6 and becomes A37 again before merging into A7) and Südschnellweg (starts out as B65, becomes B3/B6/B65 upon crossing "Westschnellweg", then becomes B65 again at "Seelhorster Kreuz").

Hanover has an extensive Stadtbahn and bus system, operated by üstra. The city is famous for its designer buses and tramways, the TW 6000 and TW 2000 trams being the most well-known examples.

Cycle paths are very common in the city centre. At off-peak hours you are allowed to take your bike on a tram or bus.

Various industrial businesses are located in Hannover. The Volkswagen Commercial Vehicles Transporter (VWN) factory at Hannover-Stöcken is the biggest employer in the region and operates a huge plant at the northern edge of town adjoining the Mittellandkanal and Motorway A2. Jointly with a factory of German tire and automobile parts manufacturer Continental AG, they have a coal-burning power plant. Continental AG, founded in Hanover in 1871, is one of the city's major companies, as is Sennheiser. Since 2008 a take-over is in progress: the Schaeffler Group from Herzogenaurach (Bavaria) holds the majority of the stock but were required due to the financial crisis to deposit the options as securities at banks.
TUI AG has its HQ in Hanover.
Hanover is home to many insurance companies e.g. Talanx, VHV Group, Concordia Insurance. One major global reinsurance company is Hannover Re, whose headquarters are east of the city centre.

In 2012, the city generated a GDP of €29.5 billion, which is equivalent to €74,822 per employee. The gross value of production in 2012 was €26.4 billion, which is equivalent to €66,822 per employee.
Around 300,000 employees were counted in 2014. Of these, 189,000 had their primary residence in Hanover, while 164,892 commute into the city every day.

In 2014 the city was home to 34,198 businesses, of which 9,342 were registered in the German Trade Register and 24,856 counted as small businesses. Hence, more than half of the metropolitan area's businesses in the German Trade Register are located in Hanover (17,485 total).
Hannoverimpuls GMBH is a joint business development company from the city and region of Hannover. The company was founded in 2003 and supports the start-up, growth and relocation of businesses in the Hannover Region. The focus is on seven sectors, which stand for sustainable economic growth: Automotive, Energy Solutions, Information and Communications Technology, Life Sciences, Optical Technologies, Creative Industries and Production Engineering.

A range of programmes supports companies from the key industries in their expansion plans in Hannover or abroad. Three regional centres specifically promote international economic relations with Russia, India and Turkey.

The Leibniz University Hannover is the largest funded institution in Hanover for providing higher education to the students from around the world. Below are the names of the universities and some of the important schools, including newly opened Hannover Medical Research School in 2003 for attracting the students from biology background from around the world.

There are several universities in Hanover:
There is one University of Applied Science and Arts in Hanover:

The "Schulbiologiezentrum Hannover" maintains practical biology schools in four locations (Botanischer Schulgarten Burg, Freiluftschule Burg, Zooschule Hannover, and Botanischer Schulgarten Linden). The University of Veterinary Medicine Hanover also maintains its own botanical garden specializing in medicinal and poisonous plants, the Heil- und Giftpflanzengarten der Tierärztlichen Hochschule Hannover.

The following is a selection of famous Hanover-natives, personalities connected with the city and honorary citizens:

Hanover is twinned with:




</doc>
<doc id="14199" url="https://en.wikipedia.org/wiki?curid=14199" title="Handheld game console">
Handheld game console

A handheld game console is a small, portable self-contained video game console with a built-in screen, game controls, and speakers. Handheld game consoles are smaller than home video game consoles and contain the console, screen, speakers, and controls in one unit, allowing people to carry them and play them at any time or place.

In 1976, Mattel introduced the first handheld electronic game with the release of "Auto Race". Later, several companies—including Coleco and Milton Bradley—made their own single-game, lightweight table-top or handheld electronic game devices. The oldest true handheld game console with interchangeable cartridges is the Milton Bradley Microvision in 1979.

Nintendo is credited with popularizing the handheld console concept with the release of the Game Boy in 1989 and as of 2018 continues to dominate the handheld console market with the Nintendo Switch.

The origins of handheld game consoles are found in handheld and tabletop electronic game devices of the 1970s and early 1980s. These electronic devices are capable of playing only a single game, they fit in the palm of the hand or on a tabletop, and they may make use of a variety of video displays such as LED, VFD, or LCD. In 1978, handheld electronic games were described by "Popular Electronics" magazine as "nonvideo electronic games" and "non-TV games" as distinct from devices that required use of a television screen. Handheld electronic games, in turn, find their origins in the synthesis of previous handheld and tabletop electro-mechanical devices such as Waco's "Electronic Tic-Tac-Toe" (1972) Cragstan's "Periscope-Firing Range" (1951), and the emerging optoelectronic-display-driven calculator market of the early 1970s. This synthesis happened in 1976, when "Mattel began work on a line of calculator-sized sports games that became the world's first handheld electronic games. The project began when Michael Katz, Mattel's new product category marketing director, told the engineers in the electronics group to design a game the size of a calculator, using LED (light-emitting diode) technology."

The result was the 1976 release of "Auto Race". Followed by "Football" later in 1977, the two games were so successful that according to Katz, "these simple electronic handheld games turned into a '$400 million category.'" Mattel would later win the honor of being recognized by the industry for innovation in handheld game device displays. Soon, other manufacturers including Coleco, Parker Brothers, Milton Bradley, Entex, and Bandai began following up with their own tabletop and handheld electronic games.

In 1979 the LCD-based Microvision, designed by Smith Engineering and distributed by Milton-Bradley, became the first handheld game console and the first to use interchangeable game cartridges. The Microvision game "Cosmic Hunter" (1981) also introduced the concept of a directional pad on handheld gaming devices, and is operated by using the thumb to manipulate the on-screen character in any of four directions.

In 1979, Gunpei Yokoi, traveling on a bullet train, saw a bored businessman playing with an LCD calculator by pressing the buttons. Yokoi then thought of an idea for a watch that doubled as a miniature game machine for killing time. Starting in 1980, Nintendo began to release a series of electronic games designed by Yokoi called the Game & Watch games. Taking advantage of the technology used in the credit-card-sized calculators that had appeared on the market, Yokoi designed the series of LCD-based games to include a digital time display in the corner of the screen. For later, more complicated Game & Watch games, Yokoi invented a cross shaped directional pad or "D-pad" for control of on-screen characters. Yokoi also included his directional pad on the NES controllers, and the cross-shaped thumb controller soon became standard on game console controllers and ubiquitous across the video game industry since. When Yokoi began designing Nintendo's first handheld game console, he came up with a device that married the elements of his Game & Watch devices and the Famicom console, including both items' D-pad controller. The result was the Nintendo Game Boy.

In 1982, the Bandai LCD Solarpower was the first solar-powered gaming device. Some of its games, such as the horror-themed game "Terror House", features two LCD panels, one stacked on the other, for an early 3D effect. In 1983, Takara Tomy's Tomytronic 3D simulates 3D by having two LCD panels that were lit by external light through a window on top of the device, making it the first dedicated home video 3D hardware.

The late 1980s and early 1990s saw the beginnings of the handheld game console industry as we know it, after the demise of the Microvision. As backlit LCD game consoles with color graphics consume a lot of power, they were not battery-friendly like the non-backlit original Game Boy whose monochrome graphics allowed longer battery life. By this point, rechargeable battery technology had not yet matured and so the more advanced game consoles of the time such as the Sega Game Gear and Atari Lynx did not have nearly as much success as the Game Boy.

Even though third-party rechargeable batteries were available for the battery-hungry alternatives to the Game Boy, these batteries employed a nickel-cadmium process and had to be completely discharged before being recharged to ensure maximum efficiency; lead-acid batteries could be used with automobile circuit limiters (cigarette lighter plug devices); but the batteries had mediocre portability. The later NiMH batteries, which do not share this requirement for maximum efficiency, were not released until the late 1990s, years after the Game Gear, Atari Lynx, and original Game Boy had been discontinued. During the time when technologically superior handhelds had strict technical limitations, batteries had a very low mAh rating since batteries with heavy power density were not yet available.

Modern game systems such as the Nintendo DS and PlayStation Portable have rechargeable Lithium-Ion batteries with proprietary shapes. Other seventh-generation consoles such as the GP2X use standard alkaline batteries. Because the mAh rating of alkaline batteries has increased since the 1990s, the power needed for handhelds like the GP2X may be supplied by relatively few batteries.

Nintendo released the Game Boy on April 21, 1989 (September 1990 for the UK). The design team headed by Gunpei Yokoi had also been responsible for the Game & Watch system, as well as the Nintendo Entertainment System games "Metroid" and "Kid Icarus". The Game Boy came under scrutiny by some industry critics, saying that the monochrome screen was too small, and the processing power was inadequate. The design team had felt that low initial cost and battery economy were more important concerns, and when compared to the Microvision, the Game Boy was a huge leap forward.

Yokoi recognized that the Game Boy needed a killer app—at least one game that would define the console, and persuade customers to buy it. In June 1988, Minoru Arakawa, then-CEO of Nintendo of America saw a demonstration of the game "Tetris" at a trade show. Nintendo purchased the rights for the game, and packaged it with the Game Boy system as a launch title. It was almost an immediate hit. By the end of the year more than a million units were sold in the US. As of March 31, 2005, the Game Boy and Game Boy Color combined to sell over 118 million units worldwide.

In 1987, Epyx created the Handy Game; a device that would turn into the Atari Lynx in 1989. It is the first color handheld console ever made, as well as the first with a backlit screen. It also features networking support with up to 17 other players, and advanced hardware that allows the zooming and scaling of sprites. The Lynx can also be turned upside down to accommodate left-handed players. However, all these features came at a very high price point, which drove consumers to seek cheaper alternatives. The Lynx is also very unwieldy, consumes batteries very quickly, and lacked the third-party support enjoyed by its competitors. Due to its high price, short battery life, production shortages, a dearth of compelling games, and Nintendo's aggressive marketing campaign, and despite a redesign in 1991, the Lynx became a commercial failure. Despite this, companies like Telegames helped to keep the system alive long past its commercial relevance, and when new owner Hasbro released the rights to develop for the public domain, independent developers like Songbird have managed to release new commercial games for the system every year until 2004's "Winter Games".

The TurboExpress is a portable version of the TurboGrafx, released in 1990 for $249.99 (the price was briefly raised to $299.99, soon dropped back to $249.99, and by 1992 it was $199.99). Its Japanese equivalent is the PC Engine GT.

It is the most advanced handheld of its time and can play all the TurboGrafx-16's games (which are on a small, credit-card sized media called HuCards). It has a 66 mm (2.6 in.) screen, the same as the original Game Boy, but in a much higher resolution, and can display 64 sprites at once, 16 per scanline, in 512 colors. Although the hardware can only handle 481 simultaneous colors. It has 8 kilobytes of RAM. The Turbo runs the HuC6820 CPU at 1.79 or 7.16 MHz.

The optional "TurboVision" TV tuner includes RCA audio/video input, allowing users to use TurboExpress as a video monitor. The "TurboLink" allowed two-player play. "Falcon", a flight simulator, included a "head-to-head" dogfight mode that can only be accessed via TurboLink. However, very few TG-16 games offered co-op play modes especially designed with the TurboExpress in mind.

The Bitcorp Gamate is the one of the first handheld game systems created in response to the Nintendo Game Boy. It was released in Asia in 1990 and distributed worldwide by 1991.

Like the Sega Game Gear, it was horizontal in orientation and like the Game Boy, required 4 AA batteries. Unlike many later Game Boy clones, its internal components were professionally assembled (no "glop-top" chips). Unfortunately the system's fatal flaw is its screen. Even by the standards of the day, its screen is rather difficult to use, suffering from similar motion blur problems that were common complaints with the first generation Game Boys. Likely because of this fact sales were quite poor, and Bitcorp closed by 1992. However, new games continued to be published for the Asian market, possibly as late as 1994. The total number of games released for the system remains unknown.

Gamate games were designed for stereo sound, but the console is only equipped with a mono speaker. 

The Game Gear is the third color handheld console, after the Lynx and the TurboExpress; produced by Sega. Released in Japan in 1990 and in North America and Europe in 1991, it is based on the Master System, which gave Sega the ability to quickly create Game Gear games from its large library of games for the Master System. While never reaching the level of success enjoyed by Nintendo, the Game Gear proved to be a fairly durable competitor, lasting longer than any other Game Boy rivals.

While the Game Gear is most frequently seen in black or navy blue, it was also released in a variety of additional colors: red, light blue, yellow, clear, and violet. All of these variations were released in small quantities and frequently only in the Asian market.

Following Sega's success with the Game Gear, they began development on a successor during the early 1990s, which was intended to feature a touchscreen interface, many years before the Nintendo DS. However, such a technology was very expensive at the time, and the handheld itself was estimated to have cost around $289 were it to be released. Sega eventually chose to shelve the idea and instead release the Genesis Nomad, a handheld version of the Genesis, as the successor.

The Watara Supervision was released in 1992 in an attempt to compete with the Nintendo Game Boy. The first model was designed very much like a Game Boy, but it is grey in color and has a slightly larger screen. The second model was made with a hinge across the center and can be bent slightly to provide greater comfort for the user. While the system did enjoy a modest degree of success, it never impacted the sales of Nintendo or Sega. The Supervision was redesigned a final time as "The Magnum". Released in limited quantities it was roughly equivalent to the Game Boy Pocket. It was available in three colors: yellow, green and grey. Watara designed many of the games themselves, but did receive some third party support, most notably from Sachen.

A TV adapter was available in both PAL and NTSC formats that could transfer the Supervision's black-and-white palette to 4 colors, similar in some regards to the Super Game Boy from Nintendo.

The Hartung Game Master is an obscure handheld released at an unknown point in the early 1990s. Its graphics were much lower than most of its contemporaries, similar in complexity to the Atari 2600. It was available in black, white, and purple, and was frequently rebranded by its distributors, such as Delplay, Videojet and Virella.
The exact number of games released is not known, but is likely around 20. The system most frequently turns up in Europe and Australia.

By this time, the lack of significant development in Nintendo's product line began allowing more advanced systems such as the Neo Geo Pocket Color and the WonderSwan Color to be developed.

The Nomad was released in October 1995 in North America only. The release was five years into the market span of the Genesis, with an existing library of more than 500 Genesis games. According to former Sega of America research and development head Joe Miller, the Nomad was not intended to be the Game Gear's replacement and believes that there was little planning from Sega of Japan for the new handheld. Sega was supporting five different consoles: Saturn, Genesis, Game Gear, Pico, and the Master System, as well as the Sega CD and 32X add-ons. In Japan, the Mega Drive had never been successful and the Saturn was more successful than Sony's PlayStation, so Sega Enterprises CEO Hayao Nakayama decided to focus on the Saturn. By 1999, the Nomad was being sold at less than a third of its original price.

The Game Boy Pocket is a redesigned version of the original Game Boy having the same features. It was released in 1996. Notably, this variation is smaller and lighter. It comes in seven different colors; red, yellow, green, black, clear, silver, blue, and pink. It has space for two AAA batteries, which provide approximately 10 hours of game play. The screen was changed to a true black-and-white display, rather than the "pea soup" monochromatic display of the original Game Boy. Although, like its predecessor, the Game Boy Pocket has no backlight to allow play in a darkened area, it did notably improve visibility and pixel response-time (mostly eliminating ghosting).

Another notable improvement over the original Game Boy is a black-and-white display screen, rather than the green-tinted display of the original Game Boy, that also featured improved response time for less blurring during motion. The Game Boy Pocket takes two AAA batteries as opposed to four AA batteries for roughly ten hours of gameplay. The first model of the Game Boy Pocket did not have an LED to show battery levels, but the feature was added due to public demand. The Game Boy Pocket was not a new software platform and played the same software as the original Game Boy model.

The Game.com (pronounced in TV commercials as "game com", not "game dot com", and not capitalized in marketing material) is a handheld game console released by Tiger Electronics in September 1997. It featured many new ideas for handheld consoles and was aimed at an older target audience, sporting PDA-style features and functions such as a touch screen and stylus. However, Tiger hoped it would also challenge Nintendo's Game Boy and gain a following among younger gamers too. Unlike other handheld game consoles, the first game.com consoles included two slots for game cartridges, which would not happen again until the Tapwave Zodiac, the DS and DS Lite, and could be connected to a 14.4 kbit/s modem. Later models had only a single cartridge slot.

The Game Boy Color (also referred to as GBC or CGB) is Nintendo's successor to the Game Boy and was released on October 21, 1998, in Japan and in November of the same year in the United States. It features a color screen, and is slightly bigger than the Game Boy Pocket. The processor is twice as fast as a Game Boy's and has twice as much memory. It also had an infrared communications port for wireless linking which did not appear in later versions of the Game Boy, such as the Game Boy Advance.

The Game Boy Color was a response to pressure from game developers for a new system, as they felt that the Game Boy, even in its latest incarnation, the Game Boy Pocket, was insufficient. The resulting product was backward compatible, a first for a handheld console system, and leveraged the large library of games and great installed base of the predecessor system. This became a major feature of the Game Boy line, since it allowed each new launch to begin with a significantly larger library than any of its competitors. As of March 31, 2005, the Game Boy and Game Boy Color combined to sell 118.69 million units worldwide.

The console is capable of displaying up to 56 different colors simultaneously on screen from its palette of 32,768, and can add basic four-color shading to games that had been developed for the original Game Boy. It can also give the sprites and backgrounds separate colors, for a total of more than four colors.

The Neo Geo Pocket Color (or NGPC) was released in 1999 in Japan, and later that year in the United States and Europe. It is a 16-bit color handheld game console designed by SNK, the maker of the Neo Geo home console and arcade machine. It came after SNK's original Neo Geo Pocket monochrome handheld, which debuted in 1998 in Japan.

In 2000 following SNK's purchase by Japanese Pachinko manufacturer Aruze, the Neo Geo Pocket Color was dropped from both the US and European markets, purportedly due to commercial failure.

The system seemed well on its way to being a success in the U.S. It was more successful than any Game Boy competitor since Sega's Game Gear, but was hurt by several factors, such as SNK's infamous lack of communication with third-party developers, and anticipation of the Game Boy Advance. The decision to ship U.S. games in cardboard boxes in a cost-cutting move rather than hard plastic cases that Japanese and European releases were shipped in may have also hurt US sales.

The WonderSwan Color is a handheld game console designed by Bandai. It was released on December 9, 2000, in Japan, Although the WonderSwan Color was slightly larger and heavier (7 mm and 2 g) compared to the original WonderSwan, the color version featured 512 kB of RAM and a larger color LCD screen. In addition, the WonderSwan Color is compatible with the original WonderSwan library of games.

Prior to WonderSwan's release, Nintendo had virtually a monopoly in the Japanese video game handheld market. After the release of the WonderSwan Color, Bandai took approximately 8% of the market share in Japan partly due to its low price of 6800 yen (approximately US$65). Another reason for the WonderSwan's success in Japan was the fact that Bandai managed to get a deal with Square to port over the original Famicom "Final Fantasy" games with improved graphics and controls. However, with the popularity of the Game Boy Advance and the reconciliation between Square and Nintendo, the WonderSwan Color and its successor, the SwanCrystal quickly lost its competitive advantage.

The 2000s saw a major leap in innovation, particularly in the second half with the release of the DS and PSP.

In 2001, Nintendo released the Game Boy Advance (GBA or AGB), which added two shoulder buttons, a larger screen, and more computing power than the Game Boy Color.

The design was revised two years later when the Game Boy Advance SP (GBA SP), a more compact version, was released. The SP features a "clamshell" design (folding open and closed, like a laptop computer), as well as a frontlit color display and rechargeable battery. Despite the smaller form factor, the screen remained the same size as that of the original. In 2005, the Game Boy Micro was released. This revision sacrifices screen size and backwards compatibility with previous Game Boys for a dramatic reduction in total size and a brighter backlit screen. A new SP model with a backlit screen was released in some regions around the same time.

Along with the Nintendo GameCube, the GBA also introduced the concept of "connectivity": using a handheld system as a console controller. A handful of games use this feature, most notably "Animal Crossing", "Pac-Man Vs.", "Final Fantasy Crystal Chronicles", "", "", "Metroid Prime", and "".

As of December 31, 2007, the GBA, GBA SP, and the Game Boy Micro combined have sold 80.72 million units worldwide.

The original GP32 was released in 2001 by the South Korean company Game Park a few months after the launch of the Game Boy Advance. It featured a 32-bit CPU, 133 MHz processor, MP3 and Divx player, and e-book reader. SmartMedia cards were used for storage, and could hold up to 128mb of anything downloaded through a USB cable from a PC. The GP32 was redesigned in 2003. A front-lit screen was added and the new version was called GP32 FLU (Front Light Unit). In summer 2004, another redesign, the GP32 BLU, was made, and added a backlit screen. This version of the handheld was planned for release outside South Korea; in Europe, and it was released for example in Spain (VirginPlay was the distributor). While not a commercial success on a level with mainstream handhelds (only 30,000 units were sold), it ended up being used mainly as a platform for user-made applications and emulators of other systems, being popular with developers and more technically adept users.

Nokia released the N-Gage in 2003. It was designed as a combination MP3 player, cellphone, PDA, radio, and gaming device. The system received much criticism alleging defects in its physical design and layout, including its vertically oriented screen and requirement of removing the battery to change game cartridges. The most well known of these was "sidetalking", or the act of placing the phone speaker and receiver on an edge of the device instead of one of the flat sides, causing the user to appear as if they are speaking into a taco.

The N-Gage QD was later released to address the design flaws of the original. However, certain features available in the original N-Gage, including MP3 playback, FM radio reception, and USB connectivity were removed.

Second generation of N-Gage launched on April 3, 2008 in the form of a service for selected Nokia Smartphones.

The Cybiko is a Russian hand-held computer introduced in May 2000 by David Yang's company and designed for teenage audiences, featuring its own two-way radio text messaging system. It has over 430 "official" freeware games and applications. Because of the text messaging system, it features a QWERTY keyboard that was used with a stylus. An MP3 player add-on was made for the unit as well as a SmartMedia card reader. The company stopped manufacturing the units after two product versions and only a few years on the market. Cybikos can communicate with each other up to a maximum range of 300 metres (0.19 miles). Several Cybikos can chat with each other in a wireless chatroom.

Cybiko Classic:

There were two models of the Classic Cybiko. Visually, the only difference was that the original version had a power switch on the side, whilst the updated version used the "escape" key for power management. Internally, the differences between the two models were in the internal memory, and the location of the firmware.

Cybiko Xtreme:

The Cybiko Xtreme was the second-generation Cybiko handheld. It featured various improvements over the original Cybiko, such as a faster processor, more RAM, more ROM, a new operating system, a new keyboard layout and case design, greater wireless range, a microphone, improved audio output, and smaller size.

In 2003, Tapwave released the Zodiac. It was designed to be a PDA-handheld game console hybrid. It supported photos, movies, music, Internet, and documents. The Zodiac used a special version Palm OS 5, 5.2T, that supported the special gaming buttons and graphics chip. Two versions were available, Zodiac 1 and 2, differing in memory and looks. The Zodiac line ended in July 2005 when Tapwave declared bankruptcy.

The Nintendo DS was released in November 2004. Among its new features were the incorporation of two screens, a touchscreen, wireless connectivity, and a microphone port. As with the Game Boy Advance SP, the DS features a clamshell design, with the two screens aligned vertically on either side of the hinge.

The DS's lower screen is touch sensitive, designed to be pressed with a stylus, a user's finger or a special "thumb pad" (a small plastic pad attached to the console's wrist strap, which can be affixed to the thumb to simulate an analog stick). More traditional controls include four face buttons, two shoulder buttons, a D-pad, and "Start" and "Select" buttons. The console also features online capabilities via the Nintendo Wi-Fi Connection and ad-hoc wireless networking for multiplayer games with up to sixteen players. It is backwards-compatible with all Game Boy Advance games, but not games designed for the Game Boy or Game Boy Color.

In January 2006, Nintendo revealed an updated version of the DS: the Nintendo DS Lite (released on March 2, 2006, in Japan) with an updated, smaller form factor (42% smaller and 21% lighter than the original Nintendo DS), a cleaner design, longer battery life, and brighter, higher-quality displays, with adjustable brightness. It is also able to connect wirelessly with Nintendo's Wii console.

In October 2008, Nintendo announced the Nintendo DSi, with larger, 3.25-inch screens and two integrated cameras. It has an SD card storage slot in place of the Game Boy Advance slot, plus internal flash memory for storing downloaded games. It was released on November 1, 2008, in Japan, and was released in North America April 5, 2009, and April 3, 2009, in Europe.

As of December 31, 2009, the Nintendo DS, Nintendo DS Lite and Nintendo DSi combined have sold 125.13 million units worldwide. In 2010 Nintendo released a larger version of the DSi, called the DSi XL.

The GameKing is a handheld game console released by the Chinese company TimeTop in 2004. The first model while original in design owes a large debt to Nintendo's Game Boy Advance. The second model, the GameKing 2, is believed to be inspired by Sony's PSP. This model also was upgraded with a backlit screen, with a distracting background transparency (which can be removed by opening up the console). A color model, the GameKing 3 apparently exists, but was only made for a brief time and was difficult to purchase outside of Asia. Whether intentionally or not, the GameKing has the most primitive graphics of any handheld released since the Game Boy of 1989. 

As many of the games have an "old school" simplicity, the device has developed a small cult following. The Gameking's speaker is quite loud and the cartridges' sophisticated looping soundtracks (sampled from other sources) are seemingly at odds with its primitive graphics.

TimeTop made at least one additional device sometimes labeled as "GameKing", but while it seems to possess more advanced graphics, is essentially an emulator that plays a handful of multi-carts (like the GB Station Light II). Outside of Asia (especially China) however the Gameking remains relatively unheard of due to the enduring popularity of Japanese handhelds such as those manufactured by Nintendo and Sony.

The PlayStation Portable (officially abbreviated PSP) is a handheld game console manufactured and marketed by Sony Computer Entertainment. Development of the console was first announced during E3 2003, and it was unveiled on May 11, 2004, at a Sony press conference before E3 2004. The system was released in Japan on December 12, 2004, in North America on March 24, 2005, and in the PAL region on September 1, 2005.

The PlayStation Portable is the first handheld video game console to use an optical disc format, Universal Media Disc (UMD), for distribution of its games. UMD Video discs with movies and television shows were also released. The PSP utilized the Sony/SanDisk Memory Stick Pro Duo format as its primary storage medium. Other distinguishing features of the console include its large viewing screen, multi-media capabilities, and connectivity with the PlayStation 3, other PSPs, and the Internet.

Tiger's Gizmondo came out in the UK during March 2005 and it was released in the U.S. during October 2005. It is designed to play music, movies, and games, have a camera for taking and storing photos, and have GPS functions. It also has Internet capabilities. It has a phone for sending text and multimedia messages. Email was promised at launch, but was never released before Gizmondo, and ultimately Tiger Telematics', downfall in early 2006. Users obtained a second service pack, unreleased, hoping to find such functionality. However, Service Pack B did not activate the e-mail functionality.

The GP2X is an open-source, Linux-based handheld video game console and media player created by GamePark Holdings of South Korea, designed for homebrew developers as well as commercial developers. It is commonly used to run emulators for game consoles such as Neo-Geo, Genesis, Master System, Game Gear, Amstrad CPC, Commodore 64, Nintendo Entertainment System, TurboGrafx-16, MAME and others.

A new version called the "F200" was released October 30, 2007, and features a touchscreen, among other changes. Followed by GP2X Wiz (2009) and GP2X Caanoo (2010).

The Dingoo A-320 is a micro-sized gaming handheld that resembles the Game Boy Micro and is open to game development. It also supports music, radio, emulators (8 bit and 16 bit) and video playing capabilities with its own interface much like the PSP. There is also an onboard radio and recording program. It is currently available in two colors — white and black. Other similar products from the same manufacturer are the Dingoo A-330 (also known as Geimi), Dingoo A-360, Dingoo A-380 (available in pink, white and black) and the recently released Dingoo A-320E.

The PSP Go is a version of the PlayStation Portable handheld game console manufactured by Sony. It was released on October 1, 2009, in American and European territories, and on November 1 in Japan. It was revealed prior to E3 2009 through Sony's Qore VOD service. Although its design is significantly different from other PSPs, it is not intended to replace the PSP 3000, which Sony continued to manufacture, sell, and support. On April 20, 2011, the manufacturer announced that the PSP Go would be discontinued so that they may concentrate on the PlayStation Vita. Sony later said that only the European and Japanese versions were being cut, and that the console would still be available in the US.
Unlike previous PSP models, the PSP Go does not feature a UMD drive, but instead has 16 GB of internal flash memory to store games, video, pictures, and other media. This can be extended by up to 32 GB with the use of a Memory Stick Micro (M2) flash card. Also unlike previous PSP models, the PSP Go's rechargeable battery is not removable or replaceable by the user. The unit is 43% lighter and 56% smaller than the original PSP-1000, and 16% lighter and 35% smaller than the PSP-3000. It has a 3.8" 480 × 272 LCD (compared to the larger 4.3" 480 × 272 pixel LCD on previous PSP models). The screen slides up to reveal the main controls. The overall shape and sliding mechanism are similar to that of Sony's mylo COM-2 internet device.

The Pandora is a handheld game console/UMPC/PDA hybrid designed to take advantage of existing open source software and to be a target for home-brew development. It runs a full distribution of Linux, and in functionality is like a small PC with gaming controls. It is developed by OpenPandora, which is made up of former distributors and community members of the GP32 and GP2X handhelds.

OpenPandora began taking pre-orders for one batch of 4000 devices in November 2008 and after manufacturing delays, began shipping to customers on May 21, 2010.

The FC-16 Go is a portable Super NES hardware clone manufactured by Yobo Gameware in 2009. It features a 3.5-inch display, two wireless controllers, and CRT cables that allow cartridges to be played on a television screen. Unlike other Super NES clone consoles, it has region tabs that only allow NTSC North American cartridges to be played. Later revisions feature stereo sound output, larger shoulder buttons, and a slightly re-arranged button, power, and A/V output layout.

The Nintendo 3DS is the successor to Nintendo's DS handheld. The autostereoscopic device is able to project stereoscopic three-dimensional effects without requirement of active shutter or passive polarized glasses, which are required by most current 3D televisions to display the 3D effect. The 3DS was released in Japan on February 26, 2011; in Europe on March 25, 2011; in North America on March 27, 2011, and in Australia on March 31, 2011. The system features backward compatibility with Nintendo DS series software, including Nintendo DSi software. It also features an online service called the Nintendo eShop, launched on June 6, 2011, in North America and June 7, 2011, in Europe and Japan, which allows owners to download games, demos, applications and information on upcoming film and game releases. On November 24, 2011, a limited edition Legend of Zelda 25th Anniversary 3DS was released that contained a unique Cosmo Black unit decorated with gold Legend of Zelda related imagery, along with a copy of The Legend of Zelda: Ocarina of Time 3D.

There are also other models including the Nintendo 2DS and the New Nintendo 3DS, the latter with a larger (XL/LL) variant, like the original Nintendo 3DS. The 2DS also has a successor, the New Nintendo 2DS XL.
The Sony Ericsson Xperia PLAY is a handheld game console smartphone produced by Sony Ericsson under the Xperia smartphone brand. The device runs Android 2.3 Gingerbread, and is the first to be part of the PlayStation Certified program which means that it can play PlayStation Suite games. The device is a horizontally sliding phone with its original form resembling the Xperia X10 while the slider below resembles the slider of the PSP Go. The slider features a D-pad on the left side, a set of standard PlayStation buttons (, , and ) on the right, a long rectangular touchpad in the middle, start and select buttons on the bottom right corner, a menu button on the bottom left corner, and two shoulder buttons (L and R) on the back of the device. It is powered by a 1 GHz Qualcomm Snapdragon processor, a Qualcomm Adreno 205 GPU, and features a display measuring 4.0 inches (100 mm) (854 × 480), an 8-megapixel camera, 512 MB RAM, 8 GB internal storage, and a micro-USB connector. It supports microSD cards, versus the Memory Stick variants used in PSP consoles. The device was revealed officially for the first time in a Super Bowl ad on Sunday, February 6, 2011. On February 13, 2011, at Mobile World Congress (MWC) 2011, it was announced that the device would be shipping globally in March 2011, with a launch lineup of around 50 software titles.
The PlayStation Vita is the successor to Sony's PlayStation Portable (PSP) Handheld series. It was released in Japan on December 17, 2011 and in Europe, Australia, North and South America on February 22, 2012.

The handheld includes two analog sticks, a 5-inch (130 mm) OLED/LCD multi-touch capacitive touchscreen, and supports Bluetooth, Wi-Fi and optional 3G. Internally, the PS Vita features a 4 core ARM Cortex-A9 MPCore processor and a 4 core SGX543MP4+ graphics processing unit, as well as LiveArea software as its main user interface, which succeeds the XrossMediaBar.

The device is fully backwards-compatible with PlayStation Portable games digitally released on the PlayStation Network via the PlayStation Store. However, PSone Classics and PS2 titles were not compatible at the time of the primary public release in Japan. The Vita's dual analog sticks will be supported on selected PSP games. The graphics for PSP releases will be up-scaled, with a smoothing filter to reduce pixelation.
The Razer Switchblade was a prototype pocket-sized like a Nintendo DSi XL designed to run Windows 7, featured a multi-touch LCD screen and an adaptive keyboard that changed keys depending on the game you play. It also was to feature a full mouse.

It was first unveiled on January 5, 2011, on the Consumer Electronics Show (CES). The Switchblade won The Best of CES 2011 People's Voice award. It has since been in development and the release date is still unknown. The device has likely been suspended indefinitely.
Project Shield is a handheld system developed by Nvidia announced at CES 2013. It runs on Android 4.2 and uses Nvidia Tegra 4 SoC. The hardware includes a 5-inches multitouch screen with support for HD graphics (720p). The console allows for the streaming of games running on a compatible desktop PC, or laptop.
The Nintendo Switch is a hybrid console that can either be used in a handheld form, or inserted into a docking station attached to a television to play on a bigger screen. The Switch features two detachable wireless controllers, called Joy-Con, which can be used individually or attached to a grip to provide a traditional gamepad form.











</doc>
<doc id="14200" url="https://en.wikipedia.org/wiki?curid=14200" title="Heinrich Abeken">
Heinrich Abeken

Heinrich Abeken (August 19, 1809August 8, 1872) was a German theologian and Prussian Privy Legation Councillor in the Ministry of Foreign Affairs in Berlin.

Abeken was born and raised in the city of Osnabrück as a son of a merchant, he was incited to a higher education by the example of his uncle Bernhard Rudolf Abeken. After finishing the college in Osnabrück, he moved in 1827 to visit the University of Berlin to study theology. He soon combined philosophical and philological studies and was interested in art and modern literature.

In 1831, Abeken acquired a licenciate of theology. At the end of the year he visited Rome, and was welcomed in the house of Christian Karl Josias, Freiherr von Bunsen. Abeken participated in Bunsen's works, namely an evangelic prayer and hymn-book. In 1834 became chaplain to the Prussian embassy in Rome. He married his first wife, who died soon thereafter.
Bunsen left Rome in 1838 and Abeken followed soon thereafter to Germany. In 1841, he was sent to England to help founding a German-English evangelic episcopacy in Jerusalem. In the same year, he was sent by Frederick William IV of Prussia to Egypt and Ethiopia, where he joined an expedition led by professor Karl Richard Lepsius. In 1845 and 1846 he returned via Jerusalem and Rome to Germany. He became Legation Councillor in Berlin, later Council Referee at the Ministry of Foreign Affairs.

In 1848 he received an appointment in the Prussian ministry for foreign affairs, and in 1853 was promoted to be privy councillor of legation ("Geheimer Legationsrath"). Abeken remained in charge for more than twenty years of Prussian politics, assisting Otto Theodor Freiherr von Manteuffel and Chancellor Otto von Bismarck. The latter was so much pleased with Abeken's work that officials started to call Abeken "the quill [i.e., the scribe] of Bismarck." Abeken married in 1866 Hedwig von Olfers, daughter of the general director of the royal museums, Privy Council von Olfers.

He was much employed by Bismarck in the writing of official despatches, and stood high in the favour of King William, whom he often accompanied on his journeys as representative of the foreign office. He was present with the king during the campaigns of 1866 and 1870-71. In 1851 he published anonymously "Babylon und Jerusalem," a slashing criticism of the views of the Countess von Hahn-Hahn.

During the war against Austria in 1866 as well as in the wars against France in 1870 and 1871, Abeken stayed in the Prussian headquarters. A major part of the dispatches of the time have been written by him. Unfortunately his health was damaged by the endeavours of these travels, and he died after an illness of several months. Emperor Wilhelm I described Abeken in a condolence letter to his widow: "One of my most reliable advisors, standing on my side in the most decisive moments; His loss is irreplaceable to me; In him his fatherland has lost one of the most noble and most loyal men and officials."

Despite his engagement in politics, Abeken never lost his interest in theology and continued to publish and speak in this sector during all of his life. He was interested in art and archeology, and was sponsor of the Archeological Institute of Rome and member of the Archeological Society of Rome. He founded a Circle of Friends of the Greek Literature in Berlin and was member of the prize commission for the royal Schiller-Prize.

See "Heinrich Abeken, ein schlichtes Leben in bewegter Zeit" (Berlin, 1898), by his widow. This is valuable by reason of the letters written from the Prussian headquarters.




</doc>
<doc id="14201" url="https://en.wikipedia.org/wiki?curid=14201" title="Henry Bruce, 1st Baron Aberdare">
Henry Bruce, 1st Baron Aberdare

Henry Austin Bruce, 1st Baron Aberdare, (16 April 181525 February 1895) was a British Liberal Party politician, who served in government most notably as Home Secretary (1868–1873) and as Lord President of the Council.

Henry Bruce was born at Duffryn, Aberdare, Glamorganshire, the son of John Bruce, a Glamorganshire landowner, and his first wife Sarah, daughter of Reverend Hugh Williams Austin. John Bruce's original family name was Knight, but on coming of age in 1805 he assumed the name of Bruce: his mother, through whom he inherited the Duffryn estate, was the daughter of William Bruce, high sheriff of Glamorganshire.

Henry was educated from the age of twelve at the Bishop Gore School, Swansea (Swansea Grammar School). In 1837 he was called to the bar from Lincoln's Inn. Shortly after he had begun to practice, the discovery of coal beneath the Duffryn and other Aberdare Valley estates brought his family great wealth. From 1847 to 1854 Bruce was stipendiary magistrate for Merthyr Tydfil and Aberdare, resigning the position in the latter year, after entering parliament as Liberal member for Merthyr Tydfil.

Bruce was returned unopposed as MP for Merthyr Tydfil in December 1852, following the death of Sir John Guest. He did so with the enthusiastic support of the late member's political allies, notably the iron masters of Dowlais, and he was thereafter regarded by his political opponents, most notably in the Aberdare Valley, as their nominee. Even so, Bruce's parliamentary record demonstrated support for liberal policies, with the exception of the ballot. The electorate in the constituency at this time remained relatively small, excluding the vast majority of the working classes.

Significantly, however, Bruce's relationship with the miners of the Aberdare Valley, in particular, deteriorated as a result of the Aberdare Strike of 1857-8. In a speech to a large audience of miners at the Aberdare Market Hall, Bruce sought to strike a conciliatory tone in persuading the miners to return to work. In a second speech, however, he delivered a broadside against the trade union movement generally, referring to the violence engendered elsewhere as a result of strikes and to alleged examples of intimidation and violence in the immediate locality. The strike damaged his reputation and may well have contributed to his eventual election defeat ten years later. In 1855, Bruce was appointed a trustee of the Dowlais Iron Company and played a role in the further development of the iron industry.

In November 1862, after nearly ten years in Parliament, he became Under-Secretary of State for the Home Department, and held that office until April 1864. He became a Privy Councillor and a Charity Commissioner for England and Wales in 1864, when he was moved to be Vice-President of the Council of Education.

At the 1868 General Election, Merthyr Tydfil became a two-member constituency with a much-increased electorate as a result of the Second Reform Act of 1867. Since the formation of the constituency, Merthyr Tydfil had dominated representation as the vast majority of the electorate lived in the town and its vicinity, whereas there was a much lower number of electors in the neighbouring Aberdare Valley. During the 1850s and 1860s, however, the population of Aberdare grew rapidly, and the franchise changes in 1867 gave the vote to large numbers of miners in that valley. Amongst these new electors, Bruce, as noted above, remained unpopular as a result of his actions during the 1857 -8 dispute. Initially, it appeared that the Aberdare iron master, Richard Fothergill, would be elected to the second seat alongside Bruce. However, the appearance of a third Liberal candidate, Henry Richard, a nonconformist radical popular in both Merthyr and Aberdare, left Bruce on the defensive and he was ultimately defeated, finishing in third place behind both Richard and Fothergill.

After losing his seat, Bruce was elected for Renfrewshire on 25 January 1869, he was made Home Secretary by William Ewart Gladstone. His tenure of this office was conspicuous for a reform of the licensing laws, and he was responsible for the Licensing Act 1872, which made the magistrates the licensing authority, increased the penalties for misconduct in public-houses and shortened the number of hours for the sale of drink. In 1873 Bruce relinquished the home secretaryship, at Gladstone's request, to become Lord President of the Council, and was elevated to the peerage as Baron Aberdare, of Duffryn in the County of Glamorgan, on 23 August that year. Being a Gladstonian Liberal, Aberdare had hoped for a much more radical proposal to keep existing licensee holders for a further ten years, and to prevent any new applicants. Its unpopularity pricked his nonconformist's conscience, when like Gladstone himself he had a strong leaning towards Temperance. He had already pursued 'moral improvement' on miners in the regulations attempting to further ban boys from the pits. The Trades Union Act 1871 was another more liberal regime giving further rights to unions, and protection from malicious prosecutions.
The defeat of the Liberal government in the following year terminated Lord Aberdare's official political life, and he subsequently devoted himself to social, educational and economic questions. Education became one of Lord Aberdare's main interests in later life. His interest had been shown by the speech on Welsh education which he had made on 5 May 1862. In 1880, he was appointed to chair the Departmental Committee on Intermediate and Higher Education in Wales and Monmouthshire, whose report ultimately led to the Welsh Intermediate Education Act of 1889. The report also stimulated the campaign for the provision of university education in Wales. In 1883, Lord Aberdare was elected the first president of the University College of South Wales and Monmouthshire. In his inaugural address he declared that the framework of Welsh education would not be complete until there was a University of Wales. The University was eventually founded in 1893 and Aberdare became its first chancellor.

In 1876 he was elected a Fellow of the Royal Society; from 1878 to 1891 he was president of the Royal Historical Society. and in 1881 he became president of both the Royal Geographical Society and the Girls' Day School Trust. In 1888 he headed the commission that established the Official Table of Drops, listing how far a person of a particular weight should be dropped when hanged for a capital offence (the only method of 'judicial execution' in the United Kingdom at that time), to ensure an instant and painless death, by cleanly breaking the neck between the 2nd and 3rd vertebrae, an 'exacting science', eventually brought to perfection by Chief Executioner Albert Pierrepoint. Prisoners health, clothing and discipline was a particular concern even at the end of his career. In the Lords he spoke at some length to the Home Affairs Committee chaired by Arthur Balfour about the prison rules system. Aberdare had always maintained a healthy skepticism about intemperate working-classes; in 1878 urging greater vigilance against the vice of excessive drinking, he took evidence on miners and railway colliers habitual imbibing. The committee tried racinate special legislation based on a link between Sunday Opening and absenteeism established in 1868. Aberdare had been interested in the plight of working class drinkers since Gladstone had appointed him Home Secretary. The defeat of the Licensing Bill by the Tory 'beerage' and publicans was drafted to limit hours and protect the public, but it persuaded a convinced Anglican forever more of the iniquities.

In 1882 he began a connection with West Africa which lasted the rest of his life, by accepting the chairmanship of the National African Company, formed by Sir George Goldie, which in 1886 received a charter under the title of the Royal Niger Company and in 1899 was taken over by the British government, its territories being constituted the protectorate of Nigeria. West African affairs, however, by no means exhausted Lord Aberdare's energies, and it was principally through his efforts that a charter was in 1894 obtained for the University College of South Wales and Monmouthshire,a constituent institution of the University of Wales. This is now Cardiff University. Lord Aberdare, who in 1885 was made a Knight Grand Cross of the Order of the Bath, presided over several Royal Commissions at different times.

Henry Bruce married firstly Annabella, daughter of Richard Beadon, of Clifton by Annabella A'Court, sister of 1st Baron Heytesbury, on 6 January 1846. They had one son and three daughters. 

After her death on 28 July 1852 he married secondly on 17 August 1854 Norah Creina Blanche, youngest daughter of Lt-Gen Sir William Napier, KCB the historian of the Peninsular War, whose biography he edited, by Caroline Amelia, second daughter of Gen. Hon Henry Edward Fox, son of the Earl of Ilchester. They had seven daughters and two sons, of whom:

Lord Aberdare died at his London home, 39 Princes Gardens, W, on 25 February 1895, aged 79, and was succeeded in the barony by his only son by his first marriage, Henry. He was survived by his wife, Lady Aberdare, born 1827, who died on 27 April 1897. She was a proponent of women's education and active in the establishment of Aberdare Hall in Cardiff.

Henry Austin Bruce is buried at Aberffrwd Cemetery in Mountain Ash, Wales. His large family plot is surrounded by a chain, and his grave is a simple Celtic cross with double plinth and kerb. In place is written "To God the Judge of all and to the spirits of just men more perfect."




</doc>
<doc id="14203" url="https://en.wikipedia.org/wiki?curid=14203" title="Harpers Ferry (disambiguation)">
Harpers Ferry (disambiguation)

Harpers Ferry is the name of several places in the United States of America:

Harpers Ferry may also refer to:



</doc>
<doc id="14204" url="https://en.wikipedia.org/wiki?curid=14204" title="Halophile">
Halophile

Halophiles are organisms that thrive in high salt concentrations. They are a type of extremophile organisms. The name comes from the Greek word for "salt-loving". While most halophiles are classified into the Archaea domain, there are also bacterial halophiles and some eukaryota, such as the alga "Dunaliella salina" or fungus "Wallemia ichthyophaga". Some well-known species give off a red color from carotenoid compounds, notably bacteriorhodopsin. Halophiles can be found anywhere with a concentration of salt five times greater than the salt concentration of the ocean, such as the Great Salt Lake in Utah, Owens Lake in California, the Dead Sea, and in evaporation ponds.

Halophiles are categorized as slight, moderate, or extreme, by the extent of their halotolerance. Slight halophiles prefer 0.3 to 0.8 M (1.7 to 4.8%—seawater is 0.6 M or 3.5%), moderate halophiles 0.8 to 3.4 M (4.7 to 20%), and extreme halophiles 3.4 to 5.1 M (20 to 30%) salt content. Halophiles require sodium chloride (salt) for growth, in contrast to halotolerant organisms, which do not require salt but can grow under saline conditions.

High salinity represents an extreme environment to which relatively few organisms have been able to adapt and occupy. Most halophilic and all halotolerant organisms expend energy to exclude salt from their cytoplasm to avoid protein aggregation ('salting out'). To survive the high salinities, halophiles employ two differing strategies to prevent desiccation through osmotic movement of water out of their cytoplasm. Both strategies work by increasing the internal osmolarity of the cell. In the first (which is employed by the majority of halophilic bacteria, some archaea, yeasts, algae and fungi), organic compounds are accumulated in the cytoplasm—osmoprotectants which are known as compatible solutes. These can be either synthesised or accumulated from the environment. The most common compatible solutes are neutral or zwitterionic, and include amino acids, sugars, polyols, betaines, and ectoines, as well as derivatives of some of these compounds.

The second, more radical, adaptation involves the selective influx of potassium (K) ions into the cytoplasm. This adaptation is restricted to the moderately halophilic bacterial order Halanaerobiales, the extremely halophilic archaeal family Halobacteriaceae, and the extremely halophilic bacterium "Salinibacter ruber". The presence of this adaptation in three distinct evolutionary lineages suggests convergent evolution of this strategy, it being unlikely to be an ancient characteristic retained in only scattered groups or passed on through massive lateral gene transfer. The primary reason for this is the entire intracellular machinery (enzymes, structural proteins, etc.) must be adapted to high salt levels, whereas in the compatible solute adaptation, little or no adjustment is required to intracellular macromolecules; in fact, the compatible solutes often act as more general stress protectants, as well as just osmoprotectants.

Of particular note are the extreme halophiles or haloarchaea (often known as halobacteria), a group of archaea, which require at least a 2 M salt concentration and are usually found in saturated solutions (about 36% w/v salts). These are the primary inhabitants of salt lakes, inland seas, and evaporating ponds of seawater, such as the deep salterns, where they tint the water column and sediments bright colors. These species most likely perish if they are exposed to anything other than a very high-concentration, salt-conditioned environment. These prokaryotes require salt for growth. The high concentration of sodium chloride in their environment limits the availability of oxygen for respiration. Their cellular machinery is adapted to high salt concentrations by having charged amino acids on their surfaces, allowing the retention of water molecules around these components. They are heterotrophs that normally respire by aerobic means. Most halophiles are unable to survive outside their high-salt native environments. Indeed, many cells are so fragile that when placed in distilled water, they immediately lyse from the change in osmotic conditions.

Halophiles may use a variety of energy sources. They can be aerobic or anaerobic. Anaerobic halophiles include phototrophic, fermentative, sulfate-reducing, homoacetogenic, and methanogenic species.

The Haloarchaea, and particularly the family Halobacteriaceae, are members of the domain Archaea, and comprise the majority of the prokaryotic population in hypersaline environments. Currently, 15 recognised genera are in the family. The domain Bacteria (mainly "Salinibacter ruber") can comprise up to 25% of the prokaryotic community, but is more commonly a much lower percentage of the overall population. At times, the alga "Dunaliella salina" can also proliferate in this environment.

A comparatively wide range of taxa has been isolated from saltern crystalliser ponds, including members of these genera: "Haloferax, Halogeometricum, Halococcus, Haloterrigena, Halorubrum, Haloarcula", and "Halobacterium". However, the viable counts in these cultivation studies have been small when compared to total counts, and the numerical significance of these isolates has been unclear. Only recently has it become possible to determine the identities and relative abundances of organisms in natural populations, typically using PCR-based strategies that target 16S small subunit ribosomal ribonucleic acid (16S rRNA) genes. While comparatively few studies of this type have been performed, results from these suggest that some of the most readily isolated and studied genera may not in fact be significant in the "in situ" community. This is seen in cases such as the genus "Haloarcula", which is estimated to make up less than 0.1% of the" in situ" community, but commonly appears in isolation studies.

The comparative genomic and proteomic analysis showed distinct molecular signatures exist for environmental adaptation of halophiles. At the protein level, the halophilic species are characterized by low hydrophobicity, overrepresentation of acidic residues, underrepresentation of Cys, lower propensities for helix formation, and higher propensities for coil structure. The core of these proteins is less hydrophobic, such as DHFR, that was found to have narrower β-strands.
At the DNA level, the halophiles exhibit distinct dinucleotide and codon usage.

"Halobacterium" is a genus of the Archaea that has a high tolerance for elevated levels of salinity. Some species of halobacteria have acidic proteins that resist the denaturing effects of salts. "Halococcus" is a specific genus of the family Halobacteriaceae.

Some hypersaline lakes are a habitat to numerous families of halophiles. For example, the Makgadikgadi Pans in Botswana form a vast, seasonal, high-salinity water body that manifests halophilic species within the diatom genus "Nitzschia" in the family Bacillariaceae, as well as species within the genus "Lovenula" in the family Diaptomidae. Owens Lake in California also contains a large population of the halophilic bacterium "Halobacterium halobium".

"Wallemia ichthyophaga" is a basidiomycetous fungus, which requires at least 1.5 M sodium chloride for "in vitro" growth, and it thrives even in media saturated with salt. Obligate requirement for salt is an exception in fungi. Even species that can tolerate salt concentrations close to saturation (for example "Hortaea werneckii") in almost all cases grow well in standard microbiological media without the addition of salt.

The fermentation of salty foods (such as soy sauce, Chinese fermented beans, salted cod, salted anchovies, sauerkraut, etc.) often involves halobacteria, as either essential ingredients or accidental contaminants. One example is "Chromohalobacter beijerinckii", found in salted beans preserved in brine and in salted herring. "Tetragenococcus halophilus" is found in salted anchovies and soy sauce.

North Ronaldsay sheep are a breed of sheep originating from Orkney, Scotland. They have limited access to fresh water sources on the island and to their only food source is seaweed. They have adapted to handle salt concentrations that would kill other breeds of sheep.





</doc>
<doc id="14205" url="https://en.wikipedia.org/wiki?curid=14205" title="Herbert A. Simon">
Herbert A. Simon

Herbert Alexander Simon (June 15, 1916 – February 9, 2001) was an American economist and political scientist whose primary interest was decision-making within organizations and is best known for the theories of "bounded rationality" and "satisficing". He received the Nobel Prize in Economics in 1978 and the Turing Award in 1975. His research was noted for its interdisciplinary nature and spanned across the fields of cognitive science, computer science, public administration, management, and political science. He was at Carnegie Mellon University for most of his career, from 1949 to 2001.

Notably, Simon was among the pioneers of several modern-day scientific domains such as artificial intelligence, information processing, decision-making, problem-solving, organization theory, and complex systems. He was among the earliest to analyze the architecture of complexity and to propose a preferential attachment mechanism to explain power law distributions.

Herbert Alexander Simon was born in Milwaukee, Wisconsin, on June 15, 1916. His father, Arthur Simon (1881–1948), was an electrical engineer who had come to the United States from Germany in 1903 after earning his engineering degree from the Technische Hochschule of Darmstadt. An inventor who was granted "several dozen patents", his father also was an independent patent attorney. His mother, Edna Marguerite Merkel, was an accomplished pianist whose ancestors had come from Prague and Cologne. His European ancestors had been piano makers, goldsmiths, and vintners. Simon's father was Jewish and his mother came from a family with Jewish, Lutheran, and Catholic backgrounds. Simon called himself an atheist.

Simon was educated in the Milwaukee public school system, where he developed an interest in science. He found schoolwork to be interesting and easy. Unlike many children, Simon was exposed to the idea that human behavior could be studied scientifically at a relatively young age due to the influence of his mother's younger brother, Harold Merkel, who had studied economics at the University of Wisconsin–Madison under John R. Commons. Through his uncle's books on economics and psychology, Simon discovered the social sciences. Among his earliest influences, Simon has cited Richard Ely's economics textbook, Norman Angell's "The Great Illusion", and Henry George's "Progress and Poverty". At that time, Simon argued "from conviction, rather than cussedness" in favor of George's controversial "single tax" on land rents.

In 1933, Simon entered the University of Chicago, and following those early influences, he studied the social sciences and mathematics. He was interested in biology, but chose not to study it because of his "color-blindness and awkwardness in the laboratory". He chose instead to focus on political science and economics. His most important mentor was Henry Schultz, an econometrician and mathematical economist. Simon received both his B.A. (1936) and his Ph.D. (1943) in political science, from the University of Chicago, where he studied under Harold Lasswell, Nicholas Rashevsky, Rudolf Carnap, Henry Schultz, and Charles Edward Merriam.

After enrolling in a course on "Measuring Municipal Governments", Simon was invited to be a research assistant for Clarence Ridley, with whom he coauthored "Measuring Municipal Activities" in 1938. Eventually his studies led him to the field of organizational decision-making, which would become the subject of his doctoral dissertation.

After graduating with his undergraduate degree, Simon obtained a research assistantship in municipal administration which turned into a directorship at the University of California, Berkeley.

From 1942 to 1949, Simon was a professor of political science and also served as department chairman at Illinois Institute of Technology in Chicago. There, he began participating in the seminars held by the staff of the Cowles Commission who at that time included Trygve Haavelmo, Jacob Marschak, and Tjalling Koopmans. He thus began an in-depth study of economics in the area of institutionalism. Marschak brought Simon in to assist in the study he was currently undertaking with Sam Schurr of the "prospective economic effects of atomic energy".

From 1949 to 2001, Simon was a faculty at Carnegie Mellon. In 1949, Simon became a professor of administration and chairman of the Department of Industrial Management at Carnegie Tech (later to become Carnegie Mellon University). Simon later also taught psychology and computer science in the same university, (occasionally visiting other universities.).

Simon married Dorothea Pye in 1938. Their marriage lasted 63 years until his death from a cancerous tumor. In January 2001, Simon underwent surgery at UPMC Presbyterian to remove a cancerous tumor in his abdomen. Although the surgery was successful, Simon later succumbed to the complications that followed. They had three children, Katherine, Peter, and Barbara. His wife died in 2002.

From 1950 to 1955, Simon studied mathematical economics and during this time, together with David Hawkins, discovered and proved the Hawkins–Simon theorem on the "conditions for the existence of positive solution vectors for input-output matrices". He also developed theorems on near-decomposability and aggregation. Having begun to apply these theorems to organizations, by 1954 Simon determined that the best way to study problem-solving was to simulate it with computer programs, which led to his interest in computer simulation of human cognition. Founded during the 1950s, he was among the first members of the Society for General Systems Research.

Simon had a keen interest in the arts, as he was a pianist. He was a friend of Robert Lepper and Richard Rappaport. Rappaport also painted Simon's commissioned portrait at Carnegie Mellon University. He was also a keen mountain climber. As a testament to his wide interests, he at one point taught an undergraduate course on the French Revolution.

Seeking to replace the highly simplified classical approach to economic modeling, Simon became best known for his theory of corporate decision in his book "Administrative Behavior". In this book he based his concepts with an approach that recognized multiple factors that contribute to decision making. His organization and administration interest allowed him to not only serve three times as a university department chairman, but he also played a big part in the creation of the Economic Cooperation Administration in 1948; administrative team that administered aid to the Marshall Plan for the U.S. government, serving on President Lyndon Johnson's Science Advisory Committee, and also the National Academy of Science. Simon has made a great number of contributions to both economic analysis and applications. Because of this, his work can be found in a number of economic literary works, making contributions to areas such as mathematical economics including theorem, human rationality, behavioral study of firms, theory of casual ordering, and the analysis of the parameter identification problem in econometrics.

"Administrative Behavior", first published in 1947, and updated across the years was based on Simon's doctoral dissertation. It served as the foundation for his life's work. The centerpiece of this book is the behavioral and cognitive processes of humans making rational choices, that is, decisions. By his definition, an operational administrative decision should be correct and efficient, and it must be practical to implement with a set of coordinated means.
Simon recognised that a theory of administration is largely a theory of human decision making, and as such must be based on both economics and on psychology. He states:

Contrary to the "homo economicus" stereotype, Simon argued that alternatives and consequences may be partly known, and means and ends imperfectly differentiated, incompletely related, or poorly detailed.

Simons defined the task of rational decision making is to select the alternative that results in the more preferred set of all the possible consequences. Correctness of administrative decisions was thus measured by:

The task of choice was divided into three required steps:


Any given individual or organization attempting to implement this model in a real situation would be unable to comply with the three requirements. Simon argued that knowledge of all alternatives, or all consequences that follow from each alternative is impossible in many realistic cases.

Simon attempted to determine the techniques and/or behavioral processes that a person or organization could bring to bear to achieve approximately the best result given limits on rational decision making. Simon writes:

Simon therefore, describes work in terms of an economic framework, conditioned on human cognitive limitations: Economic man and Administrative man.

"Administrative Behavior" addresses a wide range of human behaviors, cognitive abilities, management techniques, personnel policies, training goals and procedures, specialized roles, criteria for evaluation of accuracy and efficiency, and all of the ramifications of communication processes. Simon is particularly interested in how these factors influence the making of decisions, both directly and indirectly.

Simons argued that the two outcomes of a choice require monitoring and that many members of the organization would be expected to focus on adequacy, but that administrative management must pay particular attention to the efficiency with which the desired result was obtained.

Simon followed Chester Barnard who pointed out that "the decisions that an individual makes as a member of an organization are quite distinct from his personal decisions". Personal choices may be determined whether an individual joins a particular organization, and continue to be made in his or her extra–organizational private life. As a member of an organization, however, that individual makes decisions not in relationship to personal needs and results, but in an impersonal sense as part of the organizational intent, purpose, and effect. Organizational inducements, rewards, and sanctions are all designed to form, strengthen, and maintain this identification.

Simon saw two universal elements of human social behavior as key to creating the possibility of organizational behavior in human individuals: Authority (addressed in Chapter VII—The Role of Authority) and in Loyalties and Identification (Addressed in Chapter X: Loyalties, and Organizational Identification).

Authority is a well-studied, primary mark of organizational behavior, straightforwardly defined in the organizational context as the ability and right of an individual of higher rank to guide the decisions of an individual of lower rank. The actions, attitudes, and relationships of the dominant and subordinate individuals constitute components of role behavior that may vary widely in form, style, and content, but do not vary in the expectation of obedience by the one of superior status, and willingness to obey from the subordinate.

Loyalty was defined by Simon as the "process whereby the individual substitutes organizational objectives (service objectives or conservation objectives) for his own aims as the value-indices which determine his organizational decisions". This entailed evaluating alternative choices in terms of their consequences for the group rather than only for onself or ones family.

Decisions can be complex admixtures of facts and values. Information about facts, especially empirically-proven facts or facts derived from specialized experience, are more easily transmitted in the exercise of authority than are the expressions of values. Simon is primarily interested in seeking identification of the individual employee with the organizational goals and values. Following Lasswell, he states that "a person identifies himself with a group when, in making a decision, he evaluates the several alternatives of choice in terms of their consequences for the specified group". A person may identify himself with any number of social, geographic, economic, racial, religious, familial, educational, gender, political, and sports groups. Indeed, the number and variety are unlimited. The fundamental problem for organizations is to recognize that personal and group identifications may either facilitate or obstruct correct decision making for the organization. A specific organization has to determine deliberately, and specify in appropriate detail and clear language, its own goals, objectives, means, ends, and values.

Simon has been critical of traditional economics' elementary understanding of decision-making, and argues it "is too quick to build an idealistic, unrealistic picture of the decision-making process and then prescribe on the basis of such unrealistic picture". His contributions to research in the area of administrative decision-making have become increasingly mainstream in the business community.

Simon was a pioneer in the field of artificial intelligence, creating with Allen Newell the Logic Theory Machine (1956) and the General Problem Solver (GPS) (1957) programs. GPS may possibly be the first method developed for separating problem solving strategy from information about particular problems. Both programs were developed using the Information Processing Language (IPL) (1956) developed by Newell, Cliff Shaw, and Simon. Donald Knuth mentions the development of list processing in IPL, with the linked list originally called "NSS memory" for its inventors. In 1957, Simon predicted that computer chess would surpass human chess abilities within "ten years" when, in reality, that transition took about forty years.

In the early 1960s psychologist Ulric Neisser asserted that while machines are capable of replicating "cold cognition" behaviors such as reasoning, planning, perceiving, and deciding, they would never be able to replicate "hot cognition" behaviors such as pain, pleasure, desire, and other emotions. Simon responded to Neisser's views in 1963 by writing a paper on emotional cognition, which he updated in 1967 and published in "Psychological Review". Simon's work on emotional cognition was largely ignored by the artificial intelligence research community for several years, but subsequent work on emotions by Sloman and Picard helped refocus attention on Simon's paper and eventually, made it highly influential on the topic.

Simon also collaborated with James G. March on several works in organization theory.

With Allen Newell, Simon developed a theory for the simulation of human problem solving behavior using production rules. The study of human problem solving required new kinds of human measurements and, with Anders Ericsson, Simon developed the experimental technique of verbal protocol analysis. Simon was interested in the role of knowledge in expertise. He said that to become an expert on a topic required about ten years of experience and he and colleagues estimated that expertise was the result of learning roughly 50,000 chunks of information. A chess expert was said to have learned about 50,000 chunks or chess position patterns.

He was awarded the ACM Turing Award, along with Allen Newell, in 1975. "In joint scientific efforts extending over twenty years, initially in collaboration with J. C. (Cliff) Shaw at the RAND Corporation, and with numerous faculty and student colleagues at Carnegie Mellon University, they have made basic contributions to artificial intelligence, the psychology of human cognition, and list processing."

Simon was interested in how humans learn and, with Edward Feigenbaum, he developed the EPAM (Elementary Perceiver and Memorizer) theory, one of the first theories of learning to be implemented as a computer program. EPAM was able to explain a large number of phenomena in the field of verbal learning. Later versions of the model were applied to concept formation and the acquisition of expertise. With Fernand Gobet, he has expanded the EPAM theory into the CHREST computational model. The theory explains how simple chunks of information form the building blocks of schemata, which are more complex structures. CHREST has been used predominantly, to simulate aspects of chess expertise.

Simon has been credited for revolutionary changes in microeconomics. He is responsible for the concept of organizational decision-making as it is known today. He also was the first to discuss this concept in terms of uncertainty; i.e., it is impossible to have perfect and complete information at any given time to make a decision. While this notion was not entirely new, Simon is best known for its origination. It was in this area that he was awarded the Nobel Prize in 1978.

At the Cowles Commission, Simon's main goal was to link economic theory to mathematics and statistics. His main contributions were to the fields of general equilibrium and econometrics. He was greatly influenced by the marginalist debate that began in the 1930s. The popular work of the time argued that it was not apparent empirically that entrepreneurs needed to follow the marginalist principles of profit-maximization/cost-minimization in running organizations. The argument went on to note that profit maximization was not accomplished, in part, because of the lack of complete information. In decision-making, Simon believed that agents face uncertainty about the future and costs in acquiring information in the present. These factors limit the extent to which agents may make a fully rational decision, thus they possess only "bounded rationality" and must make decisions by "satisficing", or choosing that which might not be optimal, but which will make them happy enough. Bounded rationality is a central theme in behavioral economics. It is concerned with the ways in which the actual decision making process influences decision. Theories of bounded rationality relax one or more assumptions of standard expected utility theory.

Further, Simon emphasized that psychologists invoke a "procedural" definition of rationality, whereas economists employ a "substantive" definition. Gustavos Barros argued that the procedural rationality concept does not have a significant presence in the economics field and has never had nearly as much weight as the concept of bounded rationality. However, in an earlier article, Bhargava (1997) noted the importance of Simon's arguments and emphasized that there are several applications of the "procedural" definition of rationality in econometric analyses of data on health. In particular, economists should employ "auxiliary assumptions" that reflect the knowledge in the relevant biomedical fields, and guide the specification of econometric models for health outcomes.

Simon was also known for his research on industrial organization. He determined that the internal organization of firms and the external business decisions thereof, did not conform to the neoclassical theories of "rational" decision-making. Simon wrote many articles on the topic over the course of his life, mainly focusing on the issue of decision-making within the behavior of what he termed "bounded rationality". "Rational behavior, in economics, means that individuals maximize their utility function under the constraints they face (e.g., their budget constraint, limited choices, ...) in pursuit of their self-interest. This is reflected in the theory of subjective expected utility. The term, bounded rationality, is used to designate rational choice that takes into account the cognitive limitations of both knowledge and cognitive capacity. Bounded rationality is a central theme in behavioral economics. It is concerned with the ways in which the actual decision-making process influences decisions. Theories of bounded rationality relax one or more assumptions of standard expected utility theory".

Simon determined that the best way to study these areas was through computer simulations. As such, he developed an interest in computer science. Simon's main interests in computer science were in artificial intelligence, human–computer interaction, principles of the organization of humans and machines as information processing systems, the use of computers to study (by modeling) philosophical problems of the nature of intelligence and of epistemology, and the social implications of computer technology.

In his youth, Simon took an interest in land economics and Georgism, an idea known at the time as "single tax". The system is meant to redistribute unearned economic rent to the public and improve land use. In 1979, Simon still maintained these ideas and argued that land value tax should replace taxes on wages.

Some of Simon's economic research was directed toward understanding technological change in general and the information processing revolution in particular.

Simon's work has strongly influenced John Mighton, developer of a program that has achieved significant success in improving mathematics performance among elementary and high school students. Mighton cites a 2000 paper by Simon and two coauthors that counters arguments by French mathematics educator, Guy Brousseau, and others suggesting that excessive practice hampers children's understanding:

He received many top-level honors in life, including becoming a fellow of the American Academy of Arts and Sciences in 1959; election to the National Academy of Sciences in 1967; APA Award for Distinguished Scientific Contributions to Psychology (1969); the ACM's Turing Award for making "basic contributions to artificial intelligence, the psychology of human cognition, and list processing" (1975); the Nobel Memorial Prize in Economics "for his pioneering research into the decision-making process within economic organizations" (1978); the National Medal of Science (1986); the APA's Award for Outstanding Lifetime Contributions to Psychology (1993); ACM fellow (1994); and IJCAI Award for Research Excellence (1995).


Simon was a prolific writer and authored 27 books and almost a thousand papers. As of 2016, Simon was the most cited person in artificial intelligence and cognitive psychology on Google Scholar. With almost a thousand highly cited publications, he was one of the most influential social scientists of the twentieth century.







 


</doc>
<doc id="14207" url="https://en.wikipedia.org/wiki?curid=14207" title="Hematite">
Hematite

Hematite, also spelled as haematite, is the mineral form of iron(III) oxide (FeO), one of several iron oxides. It is the oldest known iron oxide mineral that has ever formed on earth, and is widespread in rocks and soils. Hematite crystallizes in the rhombohedral lattice system, and it has the same crystal structure as ilmenite and corundum. Hematite and ilmenite form a complete solid solution at temperatures above .

Hematite is colored black to steel or silver-gray, brown to reddish brown, or red. It is mined as the main ore of iron. Varieties include "kidney ore", "martite" (pseudomorphs after magnetite), "iron rose" and "specularite" (specular hematite). While the forms of hematite vary, they all have a rust-red streak. Hematite is harder than pure iron, but much more brittle. Maghemite is a hematite- and magnetite-related oxide mineral.

Huge deposits of hematite are found in banded iron formations. Gray hematite is typically found in places that can have still standing water or mineral hot springs, such as those in Yellowstone National Park in North America. The mineral can precipitate out of water and collect in layers at the bottom of a lake, spring, or other standing water. Hematite can also occur without water, however, usually as the result of volcanic activity.

Clay-sized hematite crystals can also occur as a secondary mineral formed by weathering processes in soil, and along with other iron oxides or oxyhydroxides such as goethite, is responsible for the red color of many tropical, ancient, or otherwise highly weathered soils.

The name hematite is derived from the Greek word for blood αἷμα "haima," due to the red coloration found in some varieties of hematite. The color of hematite lends itself to use as a pigment. The English name of the stone is derived from Middle French: Hématite Pierre, which was imported from Latin: Lapis Hæmatites around the 15th century, which originated from Ancient Greek: αἱματίτης λίθος ("haimatitēs" lithos, "blood-red stone").

Ochre is a clay that is colored by varying amounts of hematite, varying between 20% and 70%. Red ochre contains unhydrated hematite, whereas yellow ochre contains hydrated hematite (FeO • HO). The principal use of ochre is for tinting with a permanent color.

The red chalk writing of this mineral was one of the earliest in the history of humans. The powdery mineral was first used 164,000 years ago by the Pinnacle-Point man possibly for social purposes. Hematite residues are also found in graves from 80,000 years ago. Near Rydno in Poland and Lovas in Hungary red chalk mines have been found that are from 5000 BC, belonging to the Linear Pottery culture at the Upper Rhine.

Rich deposits of hematite have been found on the island of Elba that have been mined since the time of the Etruscans.

Hematite is an antiferromagnetic material below the Morin transition at , and a canted antiferromagnet or weakly ferromagnetic above the Morin transition and below its Néel temperature at 948 K, above which it is paramagnetic.

The magnetic structure of a-hematite was the subject of considerable discussion and debate in the 1950s because it appeared to be ferromagnetic with a Curie temperature of around 1000 K, but with an extremely tiny magnetic moment (0.002 µ). Adding to the surprise was a transition with a decrease in temperature at around 260 K to a phase with no net magnetic moment. It was shown that the system is essentially antiferromagnetic, but that the low symmetry of the cation sites allows spin–orbit coupling to cause canting of the moments when they are in the plane perpendicular to the c axis. The disappearance of the moment with a decrease in temperature at 260 K is caused by a change in the anisotropy which causes the moments to align along the c axis. In this configuration, spin canting does not reduce the energy. The magnetic properties of bulk hematite differ from their nanoscale counterparts. For example, the Morin transition temperature of hematite decreases with a decrease in the particle size. The suppression of this transition has also been observed in some of the hematite nanoparticles, and the presence of impurities, water molecules and defects in the crystals were attributed to the absence of a Morin transition. Hematite is part of a complex solid solution oxyhydroxide system having various contents of water, hydroxyl groups and vacancy substitutions that affect the mineral's magnetic and crystal chemical properties. Two other end-members are referred to as protohematite and hydrohematite.

Enhanced magnetic coercivities for hematite have been achieved by dry-heating a 2-line ferrihydrite precursor prepared from solution. Hematite exhibited temperature-dependent magnetic coercivity values ranging from 289 to 5,027Oe. The origin of these high coercivity values has been interpreted as a consequence of the subparticle structure induced by the different particle and crystallite size growth rates at increasing annealing temperature. These differences in the growth rates are translated into a progressive development of a subparticle structure at the nanoscale. At lower temperatures (350–600 °C), single particles crystallize however; at higher temperatures (600–1000 °C), the growth of crystalline aggregates with a subparticle structure is favored.

Hematite is present in the waste tailings of iron mines. A recently developed process, magnetation, uses magnets to glean waste hematite from old mine tailings in Minnesota's vast Mesabi Range iron district. Falu red is a pigment used in traditional Swedish house paints. Originally, it was made from tailings of the Falu mine.

The spectral signature of hematite was seen on the planet Mars by the infrared spectrometer on the NASA Mars Global Surveyor ("MGS") and 2001 Mars Odyssey spacecraft in orbit around Mars. The mineral was seen in abundance at two sites on the planet, the Terra Meridiani site, near the Martian equator at 0° longitude, and the Aram Chaos site near the Valles Marineris. Several other sites also showed hematite, e.g., Aureum Chaos. Because terrestrial hematite is typically a mineral formed in aqueous environments or by aqueous alteration, this detection was scientifically interesting enough that the second of the two Mars Exploration Rovers was sent to a site in the Terra Meridiani region designated Meridiani Planum. In-situ investigations by the Opportunity rover showed a significant amount of hematite, much of it in the form of small spherules that were informally named "blueberries" by the science team. Analysis indicates that these spherules are apparently concretions formed from a water solution.
"Knowing just how the hematite on Mars was formed will help us characterize the past environment and determine whether that environment was favorable for life".

Hematite's popularity in jewelry was at its highest in Europe during the Victorian era. Certain types of hematite or iron oxide-rich clay, especially Armenian bole, have been used in gilding. Hematite is also used in art such as in the creation of intaglio engraved gems. Hematine is a synthetic material sold as "magnetic hematite".




</doc>
<doc id="14208" url="https://en.wikipedia.org/wiki?curid=14208" title="Holocene extinction">
Holocene extinction

The Holocene extinction, otherwise referred to as the Sixth extinction or Anthropocene extinction, is the ongoing extinction event of species during the present Holocene epoch, mainly as a result of human activity. The large number of extinctions spans numerous families of plants and animals, including mammals, birds, amphibians, reptiles and arthropods. With widespread degradation of highly biodiverse habitats such as coral reefs and rainforests, as well as other areas, the vast majority of these extinctions are thought to be "undocumented", as no one is even aware of the existence of the species before they go extinct, or no one has yet discovered their extinction. The current rate of extinction of species is estimated at 100 to 1,000 times higher than natural background rates.

The Holocene extinction includes the disappearance of large land animals known as megafauna, starting at the end of the last Ice Age. Megafauna outside of the African continent, which did not evolve alongside humans, proved highly sensitive to the introduction of new predation, and many died out shortly after early humans began spreading and hunting across the Earth (additionally, many African species have also gone extinct in the Holocene). These extinctions, occurring near the Pleistocene–Holocene boundary, are sometimes referred to as the Quaternary extinction event.

The arrival of humans on different continents coincides with megafaunal extinction. The most popular theory is that human overhunting of species added to existing stress conditions. Although there is debate regarding how much human predation affected their decline, certain population declines have been directly correlated with human activity, such as the extinction events of New Zealand and Hawaii. Aside from humans, climate change may have been a driving factor in the megafaunal extinctions, especially at the end of the Pleistocene.

Ecologically, humanity has been noted as an unprecedented "global superpredator" that consistently preys on the adults of other apex predators, and has worldwide effects on food webs. There have been extinctions of species on every land mass and in every ocean: there are many famous examples within Africa, Asia, Europe, Australia, North and South America, and on smaller islands. Overall, the Holocene extinction can be linked to the human impact on the environment. The Holocene extinction continues into the 21st century, with meat consumption, overfishing, ocean acidification and the decline in amphibian populations being a few broader examples of an almost universal, cosmopolitan decline in biodiversity. Human overpopulation (and continued population growth) along with profligate consumption are considered to be the primary drivers of this rapid decline.

The Holocene extinction is also known as the "sixth extinction", as it is possibly the sixth mass extinct event, after the Ordovician–Silurian extinction events, the Late Devonian extinction, the Permian–Triassic extinction event, the Triassic–Jurassic extinction event, and the Cretaceous–Paleogene extinction event. Mass extinctions are characterized by the loss of at least 75% of species within a geologically short period of time. There is no general agreement on where the Holocene, or anthropogenic, extinction begins, and the Quaternary extinction event, which includes climate change resulting in the end of the last ice age, ends, or if they should be considered separate events at all. Some have suggested that anthropogenic extinctions may have begun as early as when the first modern humans spread out of Africa between 200,000 and 100,000 years ago; this is supported by rapid megafaunal extinction following recent human colonisation in Australia, New Zealand and Madagascar, as might be expected when any large, adaptable predator (invasive species) moves into a new ecosystem. In many cases, it is suggested that even minimal hunting pressure was enough to wipe out large fauna, particularly on geographically isolated islands. Only during the most recent parts of the extinction have plants also suffered large losses.

In "The Future of Life" (2002), Edward Osborne Wilson of Harvard calculated that, if the current rate of human disruption of the biosphere continues, one-half of Earth's higher lifeforms will be extinct by 2100. A 1998 poll conducted by the American Museum of Natural History found that 70% of biologists acknowledge an ongoing anthropogenic extinction event. At present, the rate of extinction of species is estimated at 100 to 1,000 times higher than the background extinction rate, the historically typical rate of extinction (in terms of the natural evolution of the planet); also, the current rate of extinction is 10 to 100 times higher than in any of the previous mass extinctions in the history of Earth. One scientist estimates the current extinction rate may be 10,000 times the background extinction rate, although most scientists predict a much lower extinction rate than this outlying estimate. Theoretical ecologist Stuart Pimm stated that the extinction rate for plants is 100 times higher than normal.

In a pair of studies published in 2015, extrapolation from observed extinction of Hawaiian snails led to the conclusion that 7% of all species on Earth may have been lost already.

There is widespread consensus among scientists that human activity is accelerating the extinction of many animal species through the destruction of habitats, the consumption of animals as resources, and the elimination of species that humans view as threats or competitors. But some contend that this biotic destruction has yet to reach the level of the previous five mass extinctions. Stuart Pimm, for example, asserts that the sixth mass extinction "is something that hasn't happened yet – we are on the edge of it." In November 2017, a statement, titled "World Scientists’ Warning to Humanity: A Second Notice", led by eight authors and signed by 15,364 scientists from 184 countries asserted that, among other things, "we have unleashed a mass extinction event, the sixth in roughly 540 million years, wherein many current life forms could be annihilated or at least committed to extinction by the end of this century."

The abundance of species extinctions considered anthropogenic, or due to human activity, have sometimes (especially when referring to hypothesized future events) been collectively called the "Anthropocene extinction". "Anthropocene" is a term introduced in 2000. Some now postulate that a new geological epoch has begun, with the most abrupt and widespread extinction of species since the Cretaceous–Paleogene extinction event 66 million years ago.

The term "anthropocene" is being used more frequently by scientists, and some commentators may refer to the current and projected future extinctions as part of a longer Holocene extinction. The Holocene–Anthropocene boundary is contested, with some commentators asserting significant human influence on climate for much of what is normally regarded as the Holocene Epoch. Other commentators place the Holocene–Anthropocene boundary at the industrial revolution and also say that "[f]ormal adoption of this term in the near future will largely depend on its utility, particularly to earth scientists working on late Holocene successions."

It has been suggested that human activity has made the period starting from the mid-20th century different enough from the rest of the Holocene to consider it a new geological epoch, known as the Anthropocene, a term which was considered for inclusion in the timeline of Earth's history by the International Commission on Stratigraphy in 2016. In order to constitute the Holocene as an extinction event, scientists must determine exactly when anthropogenic greenhouse gas emissions began to measurably alter natural atmospheric levels on a global scale, and when these alterations caused changes to global climate. Using chemical proxies from Antarctic ice cores, researchers have estimated the fluctuations of carbon dioxide (CO) and methane (CH) gases in the Earth's atmosphere during the late Pleistocene and Holocene epochs. Estimates of the fluctuations of these two gases in the atmosphere, using chemical proxies from Antarctic ice cores, generally indicate that the peak of the Anthropocene occurred within the previous two centuries: typically beginning with the Industrial Revolution, when the highest greenhouse gas levels were recorded.

The Holocene extinction is mainly caused by human activity. Extinction of animals, plants, and other organisms caused by human actions may go as far back as the late Pleistocene, over 12,000 years ago. There is a correlation between megafaunal extinction and the arrival of humans, and human overpopulation and human population growth, along with overconsumption and consumption growth, most prominently in the past two centuries, are regarded as one of the underlying causes of extinction.

Megafauna were once found on every continent of the world and large islands such as New Zealand and Madagascar, but are now almost exclusively found on the continent of Africa, with notable comparisons on Australia and the islands previously mentioned experiencing population crashes and trophic cascades shortly after the earliest human settlers. It has been suggested that the African megafauna survived because they evolved alongside humans. The timing of South American megafaunal extinction appears to precede human arrival, although the possibility that human activity at the time impacted the global climate enough to cause such an extinction has been suggested.

It has been noted, in the face of such evidence, that humans are unique in ecology as an unprecedented 'global superpredator', regularly preying on large numbers of fully grown terrestrial and marine apex predators, and with a great deal of influence over food webs and climatic systems worldwide. Although significant debate exists as to how much human predation and indirect effects contributed to prehistoric extinctions, certain population crashes have been directly correlated with human arrival. A 2018 study published in "PNAS" found that since the dawn of human civilization, 83% of wild mammals, 80% of marine mammals, 50% of plants and 15% of fish have vanished. Currently, livestock make up 60% of all mammals on earth, followed by humans (36%) and wild mammals (4%). As for birds, 70% are domesticated, such as poultry, whereas only 30% are wild.

Human civilization flourished in accordance to the efficiency and intensity of prevailing subsistence systems. Local communities that acquire more subsistence strategies increased in number to combat competitive pressures of land utilization. Therefore, the Holocene developed competition on the basis of agriculture. The growth of agriculture has then introduced newer means of climate change, pollution, and ecological development.

Habitat destruction by humans, including oceanic devastation, such as through overfishing and contamination; and the modification and destruction of vast tracts of land and river systems around the world to meet solely human-centered ends (with 13 percent of Earth's ice-free land surface now used as row-crop agricultural sites, 26 percent used as pastures, and 4 percent urban-industrial areas), thus replacing the original local ecosystems. Other, related human causes of the extinction event include deforestation, hunting, pollution, the introduction in various regions of non-native species, and the widespread transmission of infectious diseases spread through livestock and crops.

Recent investigations about hunter-gatherer landscape burning has a major implication for the current debate about the timing of the Anthropocene and the role that humans may have played in the production of greenhouse gases prior to the Industrial Revolution. Studies on early hunter-gatherers raises questions about the current use of population size or density as a proxy for the amount of land clearance and anthropogenic burning that took place in pre-industrial times. Scientists have questioned the correlation between population size and early territorial alterations. Ruddiman and Ellis' research paper in 2009 makes the case that early farmers involved in systems of agriculture used more land per capita than growers later in the Holocene, who intensified their labor to produce more food per unit of area (thus, per laborer); arguing that agricultural involvement in rice production implemented thousands of years ago by relatively small populations have created significant environmental impacts through large-scale means of deforestation.

While a number of human-derived factors are recognized as potentially contributing to rising atmospheric concentrations of CH (methane) and CO (carbon dioxide), deforestation and territorial clearance practices associated with agricultural development may be contributing most to these concentrations globally. Scientists that are employing a variance of archaeological and paleoecological data argue that the processes contributing to substantial human modification of the environment spanned many thousands of years ago on a global scale and thus, not originating as early as the Industrial Revolution. Gaining popularity on his uncommon hypothesis, palaeoclimatologist William Ruddiman in 2003, stipulated that in the early Holocene 11,000 years ago, atmospheric carbon dioxide and methane levels fluctuated in a pattern which was different from the Pleistocene epoch before it. He argued that the patterns of the significant decline of CO levels during the last ice age of the Pleistocene inversely correlates to the Holocene where there have been dramatic increases of CO around 8000 years ago and CH levels 3000 years after that. The correlation between the decrease of CO in the Pleistocene and the increase of it during the Holocene implies that the causation of this spark of greenhouse gases into the atmosphere was the growth of human agriculture during the Holocene such as the anthropogenic expansion of (human) land use and irrigation.

Human arrival in the Caribbean around 6,000 years ago is correlated with the extinction of many species. Examples include many different genera of ground and arboreal sloths across all islands. These sloths were generally smaller than those found on the South American continent. "Megalocnus" were the largest genus at up to , "Acratocnus" were medium-sized relatives of modern two-toed sloths endemic to Cuba, "Imagocnus" also of Cuba, "Neocnus" and many others.

Recent research, based on archaeological and paleontological digs on 70 different Pacific islands has shown that numerous species became extinct as people moved across the Pacific, starting 30,000 years ago in the Bismarck Archipelago and Solomon Islands. It is currently estimated that among the bird species of the Pacific, some 2000 species have gone extinct since the arrival of humans, representing a 20% drop in the biodiversity of birds worldwide.

The first settlers are thought to have arrived in the islands between 300 and 800 CE, with European arrival in the 16th century. Hawaii is notable for its endemism of plants, birds, insects, mollusks and fish; 30% of its organisms are endemic. Many of its species are endangered or have gone extinct, primarily due to accidentally introduced species and livestock grazing. Over 40% of its bird species have gone extinct, and it is the location of 75% of extinctions in the United States. Extinction has increased in Hawaii over the last 200 years and is relatively well documented, with extinctions among native snails used as estimates for global extinction rates.

Australia was once home to a large assemblage of megafauna, with many parallels to those found on the African continent today. Australia's fauna is characterised by primarily marsupial mammals, and many reptiles and birds, all existing as giant forms until recently. Humans arrived on the continent very early, about 50,000 years ago. The extent human arrival contributed is controversial; climatic drying of Australia 40,000–60,000 years ago was an unlikely cause, as it was less severe in speed or magnitude than previous regional climate change which failed to kill off megafauna. Extinctions in Australia continued from original settlement until today in both plants and animals, whilst many more animals and plants have declined or are endangered.

Due to the older timeframe and the soil chemistry on the continent, very little subfossil preservation evidence exists relative to elsewhere. However, continent-wide extinction of all genera weighing over 100 kilograms, and six of seven genera weighing between 45 and 100 kilograms occurred around 46,400 years ago (4,000 years after human arrival) and the fact that megafauna survived until a later date on the island of Tasmania following the establishment of a land bridge suggest direct hunting or anthropogenic ecosystem disruption such as fire-stick farming as likely causes. The first evidence of direct human predation leading to extinction in Australia was published in 2016.
Within 500 years of the arrival of humans between 2,500–2,000 years ago, nearly all of Madagascar's distinct, endemic and geographically isolated megafauna became extinct. The largest animals, of more than , were extinct very shortly after the first human arrival, with large and medium-sized species dying out after prolonged hunting pressure from an expanding human population moving into more remote regions of the island around 1000 years ago. Smaller fauna experienced initial increases due to decreased competition, and then subsequent declines over the last 500 years. All fauna weighing over died out. The primary reasons for this are human hunting and habitat loss from early aridification, both of which persist and threaten Madagascar's remaining taxa today.

The eight or more species of elephant birds, giant flightless ratites in the genera "Aepyornis" and "Mullerornis", are extinct from over-hunting, as well as 17 species of lemur, known as giant, subfossil lemurs. Some of these lemurs typically weighed over , and fossils have provided evidence of human butchery on many species.

New Zealand is characterised by its geographic isolation and island biogeography, and had been isolated from mainland Australia for 80 million years. It was the last large land mass to be colonised by humans. The arrival of Polynesian settlers circa 12th century resulted in the extinction of all of the islands' megafaunal birds within several hundred years. The last moa, large flightless ratites, became extinct within 200 years of the arrival of human settlers. The Polynesians also introduced the Polynesian rat. This may have put some pressure on other birds but at the time of early European contact (18th Century) and colonisation (19th Century) the bird life was prolific. With them, the Europeans brought ship rats, possums, cats and mustelids which decimated native bird life, some of which had adapted flightlessness and ground nesting habits and others had no defensive behavior as a result of having no extant endemic mammalian predators. The kakapo, the world's biggest parrot, which is flightless, now only exists in managed breeding sanctuaries. New Zealand's national emblem, the kiwi, is on the endangered bird list.

There has been a debate as to the extent to which the disappearance of megafauna at the end of the last glacial period can be attributed to human activities by hunting, or even by slaughter of prey populations. Discoveries at Monte Verde in South America and at Meadowcroft Rock Shelter in Pennsylvania have caused a controversy regarding the Clovis culture. There likely would have been human settlements prior to the Clovis Culture, and the history of humans in the Americas may extend back many thousands of years before the Clovis culture. The amount of correlation between human arrival and megafauna extinction is still being debated: for example, in Wrangel Island in Siberia the extinction of dwarf woolly mammoths (approximately 2000 BCE) did not coincide with the arrival of humans, nor did megafaunal mass extinction on the South American continent, although it has been suggested climate changes induced by anthropogenic effects elsewhere in the world may have contributed.

Comparisons are sometimes made between recent extinctions (approximately since the industrial revolution) and the Pleistocene extinction near the end of the last glacial period. The latter is exemplified by the extinction of large herbivores such as the woolly mammoth and the carnivores that preyed on them. Humans of this era actively hunted the mammoth and the mastodon but it is not known if this hunting was the cause of the subsequent massive ecological changes, widespread extinctions and climate changes.

The ecosystems encountered by the first Americans had not been exposed to human interaction, and may have been far less resilient to human made changes than the ecosystems encountered by industrial era humans. Therefore, the actions of the Clovis people, despite seeming insignificant by today's standards could indeed have had a profound effect on the ecosystems and wild life which was entirely unused to human influence.

Africa experienced the smallest decline in megafauna compared to the other continents. This is presumably due to the idea that Afroeurasian megafauna evolved alongside humans, and thus developed a healthy fear of them, unlike the comparatively tame animals of other continents. Unlike other continents, the megafauna of Eurasia went extinct over a relatively long period of time, possibly due to climate fluctuations fragmenting and decreasing populations, leaving them vulnerable to over-exploitation, as with the steppe bison ("Bison priscus"). The warming of the arctic region caused the rapid decline of grasslands, which had a negative effect on the grazing megafauna of Eurasia. Most of what once was mammoth steppe has been converted to mire, rendering the environment incapable of supporting them, notably the woolly mammoth.

One of the main theories to the extinction is climate change. The climate change theory has suggested that a change in climate near the end of the late Pleistocene stressed the megafauna to the point of extinction. Some scientists favor abrupt climate change as the catalyst for the extinction of the mega-fauna at the end of the Pleistocene, but there are many who believe increased hunting from early modern humans also played a part, with others even suggesting that the two interacted. However, the annual mean temperature of the current interglacial period for the last 10,000 years is no higher than that of previous interglacial periods, yet some of the same megafauna survived similar temperature increases. In the Americas, a controversial explanation for the shift in climate is presented under the Younger Dryas impact hypothesis, which states that the impact of comets cooled global temperatures.

Megafauna play a significant role in the lateral transport of mineral nutrients in an ecosystem, tending to translocate them from areas of high to those of lower abundance. They do so by their movement between the time they consume the nutrient and the time they release it through elimination (or, to a much lesser extent, through decomposition after death). In South America's Amazon Basin, it is estimated that such lateral diffusion was reduced over 98% following the megafaunal extinctions that occurred roughly 12,500 years ago. Given that phosphorus availability is thought to limit productivity in much of the region, the decrease in its transport from the western part of the basin and from floodplains (both of which derive their supply from the uplift of the Andes) to other areas is thought to have significantly impacted the region's ecology, and the effects may not yet have reached their limits. The extinction of the mammoths allowed grasslands they had maintained through grazing habits to become birch forests. The new forest and the resulting forest fires may have induced climate change. Such disappearances might be the result of the proliferation of modern humans; some recent studies favor this theory.

Large populations of megaherbivores have the potential to contribute greatly to the atmospheric concentration of methane, which is an important greenhouse gas. Modern ruminant herbivores produce methane as a byproduct of foregut fermentation in digestion, and release it through belching or flatulence. Today, around 20% of annual methane emissions come from livestock methane release. In the Mesozoic, it has been estimated that sauropods could have emitted 520 million tons of methane to the atmosphere annually, contributing to the warmer climate of the time (up to 10 °C warmer than at present). This large emission follows from the enormous estimated biomass of sauropods, and because methane production of individual herbivores is believed to be almost proportional to their mass.

Recent studies have indicated that the extinction of megafaunal herbivores may have caused a reduction in atmospheric methane. This hypothesis is relatively new. One study examined the methane emissions from the bison that occupied the Great Plains of North America before contact with European settlers. The study estimated that the removal of the bison caused a decrease of as much as 2.2 million tons per year. Another study examined the change in the methane concentration in the atmosphere at the end of the Pleistocene epoch after the extinction of megafauna in the Americas. After early humans migrated to the Americas about 13,000 BP, their hunting and other associated ecological impacts led to the extinction of many megafaunal species there. Calculations suggest that this extinction decreased methane production by about 9.6 million tons per year. This suggests that the absence of megafaunal methane emissions may have contributed to the abrupt climatic cooling at the onset of the Younger Dryas. The decrease in atmospheric methane that occurred at that time, as recorded in ice cores, was 2–4 times more rapid than any other decrease in the last half million years, suggesting that an unusual mechanism was at work.

The hyperdisease hypothesis, proposed by Ross MacPhee in 1997, states that the megafaunal die-off was due to an indirect transmission of diseases by newly arriving aboriginal humans. According to MacPhee, aboriginals or animals travelling with them, such as domestic dogs or livestock, introduced one or more highly virulent diseases into new environments whose native population had no immunity to them, eventually leading to their extinction. K-selection animals, such as the now-extinct megafauna, are especially vulnerable to diseases, as opposed to r-selection animals who have a shorter gestation period and a higher population size. Humans are thought to be the sole cause as other earlier migrations of animals into North America from Eurasia did not cause extinctions.

There are many problems with this theory, as this disease would have to meet several criteria: it has to be able to sustain itself in an environment with no hosts; it has to have a high infection rate; and be extremely lethal, with a mortality rate of 50–75%. Disease has to be very virulent to kill off all the individuals in a genus or species, and even such a virulent disease as West Nile Virus is unlikely to have caused extinction.

However, diseases have been the cause for some extinctions. The introduction of avian malaria and avipoxvirus, for example, have had a negative impact on the endemic birds of Hawaii.

The loss of species from ecological communities, defaunation, is primarily driven by human activity. This has resulted in empty forests, ecological communities depleted of large vertebrates. This is not to be confused with extinction, as it includes both the disappearance of species and declines in abundance. Defaunation effects were first implied at the Symposium of Plant-Animal Interactions at the University of Campinas, Brazil in 1988 in the context of neotropical forests. Since then, the term has gained broader usage in conservation biology as a global phenomenon.

Big cat populations have severely declined over the last half-century and could face extinction in the following decades. According to IUCN estimates: lions are down to 25,000, from 450,000; leopards are down to 50,000, from 750,000; cheetahs are down to 12,000, from 45,000; tigers are down to 3,000 in the wild, from 50,000. A December 2016 study by the Zoological Society of London, Panthera Corporation and Wildlife Conservation Society showed that cheetahs are far closer to extinction than previously thought, with only 7,100 remaining in the wild, and crammed within only 9% of their historic range. Human pressures are to blame for the cheetah population crash, including prey loss due to overhunting by people, retaliatory killing from farmers, habitat loss and the illegal wildlife trade.

The term pollinator decline refers to the reduction in abundance of insect and other animal pollinators in many ecosystems worldwide beginning at the end of the twentieth century, and continuing into the present day. Pollinators, which are necessary for 75% of food crops, are declining globally in both abundance and diversity. A 2017 study led by Radboud University's Hans de Kroon indicated that the biomass of insect life in Germany had declined by three-quarters in the previous 25 years. Participating researcher Dave Goulson of Sussex University stated that their study suggested that humans are making large parts of the planet uninhabitable for wildlife. Goulson characterized the situation as an approaching "ecological Armageddon", adding that "if we lose the insects then everything is going to collapse."

Various species are predicted to become extinct in the near future, among them the rhinoceros, nonhuman primates, pangolins, and giraffes. Hunting alone threatens bird and mammalian populations around the world. Some scientists and academics assert that industrial agriculture and the growing demand for meat is contributing to significant global biodiversity loss as this is a significant driver of deforestation and habitat destruction; species-rich habitats, such as significant portions of the Amazon region, are being converted to agriculture for meat production. A 2017 study by the World Wildlife Fund (WWF) found that 60% of biodiversity loss can be attributed to the vast scale of feed crop cultivation required to rear tens of billions of farm animals. Moreover, a 2006 report by the Food and Agriculture Organization (FAO) of the United Nations, "Livestock's Long Shadow", also found that the livestock sector is a "leading player" in biodiversity loss. According to the WWF's 2016 Living Planet Index, global wildlife populations have declined 58% since 1970, primarily due to habitat destruction, over-hunting and pollution. They project that if current trends continue, 67% of wildlife could disappear by 2020. 189 countries, which are signatory to the Convention on Biological Diversity (Rio Accord), have committed to preparing a Biodiversity Action Plan, a first step at identifying specific endangered species and habitats, country by country.

Recent extinctions are more directly attributable to human influences, whereas prehistoric extinctions can be attributed to other factors, such as global climate change. The International Union for Conservation of Nature (IUCN) characterises 'recent' extinction as those that have occurred past the cut-off point of 1500, and at least 875 species have gone extinct since that time and 2012. Some species, such as the Père David's deer and the Hawaiian crow, are extinct in the wild, and survive solely in captive populations. Other species, such as the Florida panther, are ecologically extinct, surviving in such low numbers that they essentially have no impact on the ecosystem. Other populations are only locally extinct (extirpated), still existence elsewhere, but reduced in distribution, as with the extinction of gray whales in the Atlantic, and of the leatherback sea turtle in Malaysia.

Global warming is widely accepted as being a contributor to extinction worldwide, in a similar way that previous extinction events have generally included a rapid change in global climate and meteorology. It is also expected to disrupt sex ratios in many reptiles which have temperature-dependent sex determination. 

The removal of land to clear way for palm oil plantations releases carbon emissions held in the peatlands of Indonesia. Palm oil mainly serves as a cheap cooking oil, and also as a (controversial) biofuel. However, damage to peatland contributes to 4% of global greenhouse gas emissions, and 8% of those caused by burning fossil fuels. Palm oil cultivation has also been criticized for other impacts to the environment, including deforestation, which has threatened critically endangered species such as the orangutan and the tree-kangaroo. The IUCN stated in 2016 that the species could go extinct within a decade if measures are not taken to preserve the rainforests in which they live.

Rising levels of carbon dioxide are resulting in influx of this gas into the ocean, increasing its acidity. Marine organisms which possess calcium carbonate shells or exoskeletons experience physiological pressure as the carbonate reacts with acid. For example, this is already resulting in coral bleaching on various coral reefs worldwide, which provide valuable habitat and maintain a high biodiversity. Marine gastropods, bivalves and other invertebrates are also affected, as are the organisms that feed on them.

Some researchers suggest that by 2050 there could be more plastic than fish in the oceans by weight, with about of plastic being discharged into the oceans annually. Single-use plastics, such as plastic shopping bags, make up the bulk of this, and can often be ingested by marine life, such as with sea turtles. These plastics can degrade into microplastics, smaller particles that can affect a larger array of species. Microplastics make up the bulk of the Great Pacific Garbage Patch, and their smaller size is detrimental to cleanup efforts.

Overhunting can reduce the local population of game animals by more than half, as well as reducing population density, and may lead to extinction for some species. Populations located nearer to villages are significantly more at risk of depletion. Several conservationist organizations, among them IFAW and HSUS, assert that trophy hunters, particularly from the United States, are playing a significant role in the decline of giraffes, which they refer to as a "silent extinction".

The surge in the mass killings by poachers involved in the illegal ivory trade along with habitat loss is threatening African elephant populations. In 1979, their populations stood at 1.7 million; at present there are fewer than 400,000 remaining. Prior to European colonization, scientists believe Africa was home to roughly 20 million elephants. According to the Great Elephant Census, 30% of African elephants (or 144,000 individuals) disappeared over a seven-year period, 2007 to 2014. African elephants could become extinct by 2035 if poaching rates continue.

Fishing has had a devastating effect on marine organism populations for several centuries even before the explosion of destructive and highly effective fishing practices like trawling. Humans are unique among predators in that they regularly predate on other adult apex predators, particularly in marine environments; bluefin tuna, blue whales, North Atlantic right whales and various sharks in particular are particularly vulnerable to predation pressure from human fishing. A 2016 study published in "Science" concludes that humans tend to hunt larger species, and this could disrupt ocean ecosystems for millions of years.

The decline of amphibian populations has also been identified as an indicator of environmental degradation. As well as habitat loss, introduced predators and pollution, Chytridiomycosis, a fungal infection thought to have been accidentally spread by human travel, has caused severe population drops of several species of frogs, including (among many others) the extinction of the golden toad in Costa Rica and the Gastric-brooding frog in Australia. Many other amphibian species now face extinction, including the reduction of Rabb's fringe-limbed treefrog to an endling, and the extinction of the Panamanian golden frog in the wild. Chytrid fungus has spread across Australia, New Zealand, Central America and Africa, including countries with high amphibian diversity such as cloud forests in Honduras and Madagascar. "Batrachochytrium salamandrivorans" is a similar infection currently threatening salamanders. Amphibians are now the most endangered vertebrate group, having existed for more than 300 million years through three other mass extinctions.

Millions of bats in the US have been dying off since 2012 due to a fungal infection spread from European bats, which appear to be immune. Population drops have been as great as 90% within five years, and extinction of at least one bat species is predicted. There is currently no form of treatment, and such declines have been described as "unprecedented" in bat evolutionary history by Alan Hicks of the New York State Department of Environmental Conservation.

Between 2007 and 2013, over ten million beehives were abandoned due to colony collapse disorder, which causes worker bees to abandon the queen. This disorder has been attributed to - amongst other things - neonicotinoids consumed by worker bees from the extensive use of pesticides in human farming methods.





</doc>
<doc id="14209" url="https://en.wikipedia.org/wiki?curid=14209" title="Hollywood-style Lindy Hop">
Hollywood-style Lindy Hop

Hollywood-style Lindy Hop is a variety of Lindy Hop, an American vernacular dance. It is also sometimes referred to as Dean Collins or Smooth-style, but these terms also sometimes refer to different styles of Lindy Hop.

Hollywood is the style reconstructed by Erik Robison and Sylvia Skylar based on movies from 1930s and 1940s featuring dancers like Dean Collins, Jewel McGowan, Jean Veloz and others.. They were the first to call it "Hollywood Style".

The swingout (the basic step of Lindy) is danced in a position often described as someone about to sit on a stool, thereby bringing their center point of balance closer to the ground. This piked position is the classic look of Hollywood with the back straight and a slight forward tilt. The Hollywood style is also a slotted dance, meaning the follower travels in a straight line instead of the more elliptical or circular Savoy-style Lindy Hop.

A popular variation of Hollywood-Style Lindy Hop called LA-style Lindy Hop has a few technical changes in the footwork and fewer steps. The steps are shortened or "cheated" to create this look. The style is geared towards performance and is heavily based on short choreographies. Originating in Los Angeles, California, LA-style is a favorite on the West Coast of the United States.


</doc>
<doc id="14210" url="https://en.wikipedia.org/wiki?curid=14210" title="Harrison Narcotics Tax Act">
Harrison Narcotics Tax Act

The Harrison Narcotics Tax Act (Ch. 1, ) was a United States federal law that regulated and taxed the production, importation, and distribution of opiates and coca products. The act was proposed by Representative Francis Burton Harrison of New York and was approved on December 17, 1914.

"An Act To provide for the registration of, with collectors of internal revenue, and to impose a special tax on all persons who produce, import, manufacture, compound, deal in, dispense, sell, distribute, or give away opium or coca leaves, their salts, derivatives, or preparations, and for other purposes." The courts interpreted this to mean that physicians could prescribe narcotics to patients in the course of normal treatment, but not for the treatment of addiction.

The Harrison Anti-Narcotic legislation consisted of three U.S. House bills imposing restrictions on the availability and consumption of the psychoactive drug opium. U.S. House bills and passed conjointly with House bill or the Opium and Coca Leaves Trade Restrictions Act.

Although technically illegal for purposes of distribution and use, the distribution, sale and use of cocaine was still legal for registered companies and individuals.

Following the Spanish–American War the U.S. acquired the Philippines from Spain. At that time, opium addiction constituted a significant problem in the civilian population of the Philippines.

Charles Henry Brent was an American Episcopal bishop who served as Missionary Bishop of the Philippines beginning in 1901. He convened a Commission of Inquiry, known as the Brent Commission, for the purpose of examining alternatives to a licensing system for opium addicts. The Commission recommended that narcotics should be subject to international control. The recommendations of the Brent Commission were endorsed by the United States Department of State and in 1906 President Theodore Roosevelt called for an international conference, the International Opium Commission, which was held in Shanghai in February 1909. A second conference was held at The Hague in May 1911, and out of it came the first international drug control treaty, the International Opium Convention of 1912.

In the 1800s opiates and cocaine were mostly unregulated drugs. In the 1890s the Sears & Roebuck catalogue, which was distributed to millions of Americans homes, offered a syringe and a small amount of cocaine for $1.50. On the other hand, as early as 1880 some states and localities had already passed laws against smoking opium, at least in public.

At the beginning of the 20th century, cocaine began to be linked to crime. In 1900, the "Journal of the American Medical Association" published an editorial stating, "Negroes in the South are reported as being addicted to a new form of vice – that of 'cocaine sniffing' or the 'coke habit.'" Some newspapers later claimed cocaine use caused blacks to rape white women and was improving their pistol marksmanship. Chinese immigrants were blamed for importing the opium-smoking habit to the U.S. The 1903 blue-ribbon citizens' panel, the Committee on the Acquirement of the Drug Habit, concluded, "If the Chinaman cannot get along without his dope we can get along without him."

Theodore Roosevelt appointed Dr. Hamilton Wright as the first Opium Commissioner of the United States in 1908. In 1909, Wright attended the International Opium Commission in Shanghai as the American delegate. He was accompanied by Charles Henry Brent, the Episcopal Bishop. On March 12, 1911, Dr. Wright was quoted as follows in an article in "The New York Times": "Of all the nations of the world, the United States consumes most habit-forming drugs per capita. Opium, the most pernicious drug known to humanity, is surrounded, in this country, with far fewer safeguards than any other nation in Europe fences it with." Wright further claimed that "it has been authoritatively stated that cocaine is often the direct incentive to the crime of rape by the negroes of the South and other sections of the country," though he failed to mention specifically "which" authorities had stated that, and did not provide any evidence for his claim. Wright also stated that "one of the most unfortunate phases of smoking opium in this country is the large number of women who have become involved and were living as common-law wives or cohabitating with Chinese in the Chinatowns of our various cities".

Opium usage had begun to decline by 1914 after rising dramatically in the post Civil War Era, peaking at around one-half million pounds per year in 1896. Demand gradually declined thereafter in response to mounting public concern, local and state regulations, and the Pure Food and Drugs Act of 1906, which required labeling of patent medicines that contained opiates, cocaine, alcohol, cannabis and other intoxicants. As of 1911, an estimated one U.S. citizen in 400 (0.25%) was addicted to some form of opium. The opium addicts were mostly women who were prescribed and dispensed legal opiates by physicians and pharmacist for "female problems" (probably pain at menstruation) or white men and Chinese at the Opium dens. Between two-thirds and three-quarters of these addicts were women. By 1914, forty-six states had regulations on cocaine and twenty-nine states had laws against opium, morphine, and heroin.

Several authors have argued that the debate was merely to regulate trade and collect a tax. However, the committee report prior to the debate on the house floor and the debate itself, discussed the rise of opiate use in the United States. Harrison stated that "The purpose of this Bill can hardly be said to raise revenue, because it prohibits the importation of something upon which we have hitherto collected revenue." Later Harrison stated, "We are not attempting to collect revenue, but regulate commerce." House representative Thomas Sisson stated, "The purpose of this bill—and we are all in sympathy with it—is to prevent the use of opium in the United States, destructive as it is to human happiness and human life."

The drafters played on fears of "drug-crazed, sex-mad negroes" and made references to Negroes under the influence of drugs murdering whites, degenerate Mexicans smoking marijuana, and "Chinamen" seducing white women with drugs. Dr. Hamilton Wright, testified at a hearing for the Harrison Act. Wright alleged that drugs made blacks uncontrollable, gave them superhuman powers and caused them to rebel against white authority. Dr. Christopher Koch of the State Pharmacy Board of Pennsylvania testified that "Most of the attacks upon the white women of the South are the direct result of a cocaine-crazed Negro brain".

Before the Act was passed, on February 8, 1914, "The New York Times" published an article entitled "Negro Cocaine 'Fiends' Are New Southern Menace: Murder and Insanity Increasing Among Lower-Class Blacks" by Edward Huntington Williams, which reported that Southern sheriffs had increased the caliber of their weapons from .32 to .38 to bring down Negroes under the effect of cocaine.

Despite the extreme racialization of the issue that took place in the buildup to the Act's passage, the contemporary research on the subject indicated that black Americans were in fact using cocaine and opium at much "lower" rates than white Americans.

Enforcement began in 1915.

The act appears to be concerned about the marketing of opiates. However a clause applying to doctors allowed distribution "in the course of his professional practice only." This clause was interpreted after 1917 to mean that a doctor could not prescribe opiates to an addict, since addiction was not considered a disease. A number of doctors were arrested and some were imprisoned. The medical profession quickly learned not to supply opiates to addicts. In "United States v. Doremus", 249 U.S. 86 (1919), the Supreme Court ruled that the Harrison Act was constitutional, and in "Webb v. United States", 249 U.S. 96, 99 (1919) that physicians could not prescribe narcotics solely for maintenance.

The impact of diminished supply was obvious by mid-1915. A 1918 commission called for sterner law enforcement, while newspapers published sensational articles about addiction-related crime waves. Congress responded by tightening up the Harrison Act—the importation of heroin for any purpose was banned in 1924.

After other complementary laws (for example implementing the Uniform State Narcotic Act in 1932), and other actions by the government the number of addicts of opium started to decrease fast from 1925 to a level that in 1945 that was about one tenth of the level in 1914.

The use of the term 'narcotics' in the title of the act to describe not just opiates but also cocaine—which is a central nervous system stimulant, not a narcotic—initiated a precedent of frequent legislative and judicial misclassification of various substances as 'narcotics'. Today, law enforcement agencies, popular media, the United Nations, other nations and even some medical practitioners can be observed applying the term very broadly and often pejoratively in reference to a wide range of illicit substances, regardless of the more precise definition existing in medical contexts. For this reason, however, 'narcotic' has come to mean any illegally used drug, but it is useful as a shorthand for referring to a controlled drug in a context where its legal status is more important than its physiological effects.

The remaining effect of this act, which has largely been superseded by the Controlled Substances Act of 1970, is the warning "*Warning: May be habit forming" on labels, package inserts, and other places where ingredients are listed in the case of many opioids, barbiturates, medicinal formulations of cocaine, and chloral hydrate.

The act also marks the beginning of the creation of the modern, criminal drug addict and the American black market for drugs. Within five years the Rainey Committee, a Special Committee on Investigation appointed by Secretary of the Treasury William Gibbs McAdoo and led by Congressman T. Rainey, reported in June, 1919 that drugs were being smuggled into the country by sea, and across the Mexican and Canadian borders by nationally established organisations and that the United States consumed 470,000 pounds of opium annually, compared to 17,000 pounds in both France and Germany. The Monthly Summary of Foreign Commerce of the United States recorded that in the 7 months to January 1920, 528,635 pounds of opium was imported, compared to 74,650 pounds in the same period in 1919.

The Act's applicability in prosecuting doctors who prescribe narcotics to addicts was successfully challenged in "Linder v. United States" in 1925, as Justice McReynolds ruled that the federal government has no power to regulate medical practice.



</doc>
<doc id="14215" url="https://en.wikipedia.org/wiki?curid=14215" title="Horse tack">
Horse tack

Horse tack is a piece of equipment or accessory equipped on horses in the course of their use as domesticated animals. Saddles, stirrups, bridles, halters, reins, bits, harnesses, martingales, and breastplates are all forms of tack. Equipping an animal is often referred to as tacking up. A room to store such equipment, usually near or in a stable, is a tack room.

Saddles are seats for the rider, fastened to the horse's back by means of a "girth" (English-style riding), known as a "cinch" in the Western US, a wide strap that goes around the horse at a point about four inches behind the forelegs. Some western saddles will also have a second strap known as a "flank" or "back cinch" that fastens at the rear of the saddle and goes around the widest part of the horse's belly.

It is important that the saddle be comfortable for both the rider and the horse, as an improperly fitting saddle may create pressure points on the horse's back muscle (Latissimus dorsi) and cause the horse pain and can lead to the horse, rider, or both getting injured.

There are many types of saddle, each specially designed for its given task.
Saddles are usually divided into two major categories: "English saddles" and "Western saddles" according to the riding discipline they are used in. Other types of saddles, such as racing saddles, Australian saddles, sidesaddles and endurance saddles do not necessarily fit neatly in either category.


Stirrups are supports for the rider's feet that hang down on either side of the saddle. They provide greater stability for the rider but can have safety concerns due to the potential for a rider's feet to get stuck in them. If a rider is thrown from a horse but has a foot caught in the stirrup, they could be dragged if the horse runs away. To minimize this risk, a number of safety precautions are taken. First, most riders wear riding boots with a heel and a smooth sole. Next, some saddles, particularly English saddles, have safety bars that allow a stirrup leather to fall off the saddle if pulled backwards by a falling rider. Other precautions are done with stirrup design itself. Western saddles have wide stirrup treads that make it more difficult for the foot to become trapped. A number of saddle styles incorporate a tapedero, which is covering over the front of the stirrup that keeps the foot from sliding all the way through the stirrup. The English stirrup (or "iron") has several design variations which are either shaped to allow the rider's foot to slip out easily or are closed with a very heavy rubber band. The invention of stirrups was of great historic significance in mounted combat, giving the rider secure foot support while on horseback.

"Bridles", hackamores, "halters" or "headcollars", and similar equipment consist of various arrangements of straps around the horse's head, and are used for control and communication with the animal.

A "halter" (US) or "headcollar" (UK) (occasionally "headstall") consists of a noseband and headstall that buckles around the horse's head and allows the horse to be led or tied. The lead rope is separate, and it may be short (from six to ten feet, two to three meters) for everyday leading and tying, or much longer (up to , eight meters) for tasks such as for leading packhorses or for picketing a horse out to graze.

Some horses, particularly stallions, may have a chain attached to the lead rope and placed over the nose or under the jaw to increase the control provided by a halter while being led. Most of the time, horses are not ridden with a halter, as it offers insufficient precision and control. Halters have no bit.

In Australian and British English, a "halter" is a rope with a spliced running loop around the nose and another over the poll, used mainly for unbroken horses or for cattle. The lead rope cannot be removed from the halter. A show halter is made from rolled leather and the lead attaches to form the chinpiece of the noseband. These halters are not suitable for paddock usage or in loose stalls. An "underhalter" is a lightweight halter or headcollar which is made with only one small buckle, and can be worn under a bridle for tethering a horse without untacking.

Bridles usually have a "bit" attached to "reins" and are used for riding and driving horses.

"English Bridles" have a "cavesson" style noseband and are seen in English riding. Their reins are buckled to one another, and they have little adornment or flashy hardware.

"Western Bridles" used in Western riding usually have no noseband, are made of thin bridle leather. They may have long, separated "Split" reins or shorter closed reins, which sometimes include an attached "Romal". Western bridles are often adorned with silver or other decorative features.

"Double bridles" are a type of English bridle that use two bits in the mouth at once, a snaffle and a curb. The two bits allow the rider to have very precise control of the horse. As a rule, only very advanced horses and riders use double bridles. Double bridles are usually seen in the top levels of dressage, but also are seen in certain types of show hack and Saddle seat competition.

A "hackamore" is a headgear that utilizes a heavy noseband of some sort, rather than a bit, most often used to train young horses or to go easy on an older horse's mouth. Hackamores are more often seen in western riding. Some related styles of headgear that control a horse with a noseband rather than a bit are known as bitless bridles.

The word "hackamore" is derived from the Spanish word "jáquima." Hackamores are seen in western riding disciplines, as well as in endurance riding and English riding disciplines such as show jumping and the stadium phase of eventing. While the classic bosal-style hackamore is usually used to start young horses, other designs, such as various bitless bridles and the mechanical hackamore are often seen on mature horses with dental issues that make bit use painful, horses with certain training problems, and on horses with mouth or tongue injuries. Some riders also like to use them in the winter to avoid putting a frozen metal bit into a horse's mouth.

Like bitted bridles, noseband-based designs can be gentle or harsh, depending on the hands of the rider. It is a myth that a bit is cruel and a hackamore is gentler. The horse's face is very soft and sensitive with many nerve endings. Misuse of a hackamore can cause swelling on the nose, scraping on the nose and jawbone, and extreme misuse may cause damage to the bones and cartilage of the horse's head.

A "longeing cavesson" (UK: "lungeing") is a special type of halter or noseband used for longeing a horse. Longeing is the activity of having a horse walk, trot and/or canter in a large circle around the handler at the end of a rope that is 25 to long. It is used for training and exercise.

Reins consist of leather straps or rope attached to the outer ends of a "bit" and extend to the rider's or driver's hands. Reins are the means by which a horse rider or driver communicates directional commands to the horse's head. Pulling on the reins can be used to steer or stop the horse. The sides of a horse's mouth are sensitive, so pulling on the reins pulls the bit, which then pulls the horse's head from side to side, which is how the horse is controlled.

On some types of harnesses there might be supporting rings to carry the reins over the horse's back. When pairs of horses are used in drawing a wagon or coach it is usual for the outer side of each pair to be connected to reins and the inside of the bits connected by a short bridging strap or rope. The driver carries "four-in-hand" or "six-in-hand" being the number of reins connecting to the pairs of horses.

A rein may be attached to a halter to lead or guide the horse in a circle for training purposes or to lead a packhorse, but a simple lead rope is more often used for these purposes. A longe line is sometimes called a "longe rein," but it is actually a flat line about long, usually made of nylon or cotton web, about one inch wide, thus longer and wider than even a driving rein.

Horses should never be tied by the reins. Not only do they break easily, but, being attached to a bit in the horse's sensitive mouth, a great deal of pain can be inflicted if a bridled horse sets back against being tied.

A bit is a device placed in a horse's mouth, kept on a horse's head by means of a headstall. There are many types, each useful for specific types of riding and training.

The mouthpiece of the bit does not rest on the teeth of the horse, but rather rests on the gums or "bars" of the horse's mouth in an interdental space behind the front incisors and in front of the back molars. It is important that the style of bit is appropriate to the horse's needs and is fitted properly for it to function properly and be as comfortable as possible for the horse.

The basic "classic" styles of bits are:

While there are literally hundreds of types of bit mouthpieces, bit rings and bit shanks, essentially there are really only two broad categories: direct pressure bits, broadly termed snaffle bits; and leverage bits, usually termed curbs.

Bits that act with direct pressure on the tongue and lips of the bit are in the general category of "snaffle" bits. Snaffle bits commonly have a single jointed mouthpiece and act with a nutcracker effect on the bars, tongue and occasionally roof of the mouth. However, regardless of mouthpiece, any bit that operates only on direct pressure is a "snaffle" bit.

Leverage bits have shanks coming off the mouthpiece to create leverage that applies pressure to the poll, chin groove and mouth of the horse are in the category of "curb" bits. Any bit with shanks that works off of leverage is a "curb" bit, regardless of whether the mouthpiece is solid or jointed.

Some combination or hybrid bits combine direct pressure and leverage, such as the Kimblewick or Kimberwicke, which adds slight leverage to a two-rein design that resembles a snaffle; and the four rein designs such as the single mouthpiece Pelham bit and the double bridle, which places a curb and a snaffle bit simultaneously in the horse's mouth.

In the wrong hands even the mildest bit can hurt the horse. Conversely, a very severe bit, in the right hands, can transmit subtle commands that cause no pain to the horse. Bit commands should be given with only the quietest movements of the hands, and much steering and stopping should be done with the legs and seat.

A horse harness is a set of devices and straps that attaches a horse to a cart, carriage, sledge or any other load. There are two main styles of harnesses - breaststrap and collar and hames style. These differ in how the weight of the load is attached. Most Harnesses are made from leather, which is the traditional material for harnesses, though some designs are now made of nylon webbing or synthetic biothane.

A breaststrap harness has a wide leather strap going horizontally across the horses' breast, attached to the traces and then to the load. This is used only for lighter loads. A collar and hames harness has a collar around the horses' neck with wood or metal hames in the collar. The traces attach from the hames to the load. This type of harness is needed for heavy draft work.

Both types will also have a bridle and reins. A harness that is used to support shafts, such as on a cart pulled by a single horse, will also have a "saddle" attached to the harness to help the horse support the shafts and "breeching" to brake the forward motion of the vehicle, especially when stopping or moving downhill. Horses guiding vehicles by means of a pole, such as two-horse teams pulling a wagon, a hay-mower, or a dray, will have "pole-straps" attached to the lower part of the horse collar.

Breastplates, breastcollars or breastgirths attach to the front of the saddle, cross the horse's chest, and usually have a strap that runs between the horse's front legs and attaches to the girth. They keep the saddle from sliding back or sideways. They are usually seen in demanding, fast-paced sports. They are crucial pieces of safety equipment for English riding activities requiring jumping, such as eventing, show jumping, polo, and fox hunting. They are also seen in Western riding events, particularly in rodeo, reining and cutting, where it is particularly important to prevent a saddle from shifting. They may also be worn in other horse show classes for decorative purposes.

A martingale is a piece of equipment that keeps a horse from raising its head too high. Various styles can be used as a control measure, to prevent the horse from avoiding rider commands by raising its head out of position; or as a safety measure to keep the horse from tossing its head high or hard enough to smack its rider in the face.

They are allowed in many types of competition, especially those where speed or jumping may be required, but are not allowed in most "flat" classes at horse shows, though an exception is made in a few classes limited exclusively to young or "green" horses who may not yet be fully trained.

Martingales are usually attached to the horse one of two ways. They are either attached to the center chest ring of a breastplate or, if no breastplate is worn, they are attached by two straps, one that goes around the horse's neck, and the other that attaches to the girth, with the martingale itself beginning at the point in the center of the chest where the neck and girth straps intersect.

Martingale types include:



There are other training devices that fall loosely in the martingale category, in that they use straps attached to the reins or bit which limit the movement of the horse's head or add leverage to the rider's hands in order to control the horse's head. Common devices of this nature include the overcheck, the chambon, de Gogue, grazing reins, draw reins and the "bitting harness" or "bitting rig". However, most of this equipment is used for training purposes and is not legal in any competition. In some disciplines, use of leverage devices, even in training, is controversial.



</doc>
<doc id="14216" url="https://en.wikipedia.org/wiki?curid=14216" title="Hausa language">
Hausa language

Hausa () ("Yaren Hausa" or "Harshen Hausa") is the Chadic language (a branch of the Afroasiatic language family) with the largest number of speakers, spoken as a first language by some 27 million people, and as a second language by another 20 million. The total numer of Hausa speakers as of 2018, totals about 150 million people. The ancestral language of the Hausa people, one of the largest ethnic groups in Central Africa, Hausa is commonly spoken throughout southern Niger and northern Nigeria. It has developed into a lingua franca across much of Western Africa for purposes of trade.

Hausa belongs to the West Chadic languages subgroup of the Chadic languages group, which in turn is part of the Afroasiatic language family.

Native speakers of Hausa, the Hausa people, are mostly found in Niger, in Northern Nigeria, and in Chad. Furthermore, the language is used as a "lingua franca" by non-native speakers in most of Northern Nigeria and Southern Niger, and as a trade language across a much larger swathe of West Africa (Benin, Ghana, Cameroon, Togo, Ivory Coast) and parts of Sudan).

Eastern Hausa dialects include "Dauranchi" in Daura, "Kananci" in Kano, "Bausanchi" in Bauchi, "Gudduranci" in Katagum Misau and part of Borno, and "Hadejanci" in Hadejiya.

Western Hausa dialects include "Sakkwatanci" in Sokoto, "Katsinanci" in Katsina, "Arewanci" in Gobir, Adar, Kebbi, and Zamfara, and "Kurhwayanci" in Kurfey in Niger. Katsina is transitional between Eastern and Western dialects.

Northern Hausa dialects include "Arewa" and "Arawci".

"Zazzaganci" in Zazzau is the major Southern dialect.

The Daura ("Dauranchi") and Kano ("Kananci") dialect are the standard. The BBC, Deutsche Welle, Radio France Internationale and Voice of America offer Hausa services on their international news web sites using Dauranci and Kananci. In recent language development Zazzaganci took over the innovation of writing and speaking the current Hausa language use.

The western to eastern Hausa dialects of "Kurhwayanci", "Daragaram" and "Aderawa", represent the traditional northernmost limit of native Hausa communities. These are spoken in the northernmost sahel and mid-Saharan regions in west and central Niger in the Tillaberi, Tahoua, Dosso, Maradi, Agadez and Zinder regions. While mutually comprehensible with other dialects (especially "Sakkwatanci", and to a lesser extent "Gaananci"), the northernmost dialects have slight grammatical and lexical differences owing to frequent contact with the Zarma and Tuareg groups and cultural changes owing to the geographical differences between the grassland and desert zones. These dialects also have the quality of being non-tonal or pitch accent dialects.

This link between non-tonality and geographic location is not limited to Hausa alone, but is exhibited in other northern dialects of neighbouring languages; such as the difference within Songhay language (between the non-tonal northernmost dialects of Koyra Chiini in Timbuktu and Koyraboro Senni in Gao; and the tonal southern Zarma dialect, spoken from western Niger to northern Ghana), and within the Soninke language (between the non-tonal northernmost dialects of Imraguen and Nemadi spoken in east-central Mauritania; and the tonal southern dialects of Senegal, Mali and the sahel).

The Ghanaian Hausa dialect ("Gaananci"), spoken in Ghana, Togo, and western Ivory Coast, is a distinct western native Hausa dialect-bloc with adequate linguistic and media resources available. Separate smaller Hausa dialects are spoken by an unknown number of Hausa further west in parts of Burkina Faso, and in the Haoussa Foulane, Badji Haoussa, Guezou Haoussa, and Ansongo districts of northeastern Mali (where it is designated as a minority language by the Malian government), but there are very little linguistic resources and research done on these particular dialects at this time.

Gaananci forms a separate group from other Western Hausa dialects, as it now falls outside the contiguous Hausa-dominant area, and is usually identified by the use of "c" for "ky", and "j" for "gy". This is attributed to the fact that Ghana's Hausa population descend from Hausa-Fulani traders settled in the zongo districts of major trade-towns up and down the previous Asante, Gonja and Dagomba kingdoms stretching from the sahel to coastal regions, in particular the cities of Tamale, Salaga, Bawku, Bolgatanga, Achimota, Nima and Kumasi.

Gaananci exhibits noted inflected influences from Zarma, Gur, Dyula and Soninke, as Ghana is the westernmost area in which the Hausa language is a major lingua-franca; as well as it being the westernmost area both the Hausa and Djerma ethnic groups inhabit in large numbers. Immediately west from Ghana (in Ivory Coast, Togo, and Burkina Faso), Hausa is abruptly replaced with Dioula–Bambara as the main lingua-franca of what become predominantly Mandinka areas, and native Hausa populations plummet to a very small urban minority.

Because of this, and the presence of surrounding Akan, Gur and Mande languages, Gaananci was historically isolated from the other Hausa dialects. Despite this difference, grammatical similarities between "Sakkwatanci" and Ghanaian Hausa determine that the dialect, and the origin of the Ghanaian Hausa people themselves, are derived from the northwestern Hausa area surrounding Sokoto.

Hausa is also widely spoken by non-native Gur and Mande Ghanaian Muslims, but differs from Gaananci, and rather has features consistent with non-native Hausa dialects.

Hausa is also spoken in various parts of Cameroon and Chad, which combined the mixed dialects of northern Nigeria and Niger. In addition, Arabic has had a great influence in the way Hausa is spoken by the native Hausa speakers in these areas.

In West Africa, Hausa's use as a lingua franca has given rise to a non-native pronunciation that differs vastly from native pronunciation by way of key omissions of implosive and ejective consonants present in native Hausa dialects, such as "ɗ", "ɓ" and "kʼ/ƙ", which are pronounced by non-native speakers as "d", "b" and "k" respectively. This creates confusion among non-native and native Hausa speakers, as non-native pronunciation does not distinguish words like ' ("correct") and ' ("one-by-one"). Another difference between native and non-native Hausa is the omission of vowel length in words and change in the standard tone of native Hausa dialects (ranging from native Fulani and Tuareg Hausa-speakers omitting tone altogether, to Hausa speakers with Gur or Yoruba mother tongues using additional tonal structures similar to those used in their native languages). Use of masculine and feminine gender nouns and sentence structure are usually omitted or interchanged, and many native Hausa nouns and verbs are substituted with non-native terms from local languages.

Non-native speakers of Hausa numbered more than 25 million and, in some areas, live close to native Hausa. It has replaced many other languages especially in the north-central and north-eastern part of Nigeria and continues to gain popularity in other parts of Africa as a result of Hausa movies and music which spread out throughout the region.

There are several pidgin forms of Hausa. Barikanchi was formerly used in the colonial army of Nigeria. Gibanawa is currently in widespread use in Jega in northwestern Nigeria, south of the native Hausa area.

Hausa has between 23 and 25 consonant phonemes depending on the speaker.

The three-way contrast between palatalized velars , plain velars , and labialized velars is found only before long and short , e.g. ('grass'), ('to increase'), ('shea-nuts'). Before front vowels, only palatalized and labialized velars occur, e.g. ('jealousy') vs. ('side of body'). Before rounded vowels, only labialized velars occur, e.g. ('ringworm').

Hausa has glottalic consonants (implosives and ejectives) at four or five places of articulation (depending on the dialect). They require movement of the glottis during pronunciation and have a staccato sound.

They are written with modified versions of Latin letters. They can also be denoted with an apostrophe, either before or after depending on the letter, as shown below.


Hausa has five phonetic vowel sounds, which can be either short or long, giving a total of 10 monophthongs. In addition, there are four joint vowels (diphthongs), giving a total number of 14 vowel phonemes.


In comparison with the long vowels, the short can be similar in quality to the long vowels, mid-centralized to or centralized to .

Medial can be neutralized to , with the rounding depending on the environment.

Medial are neutralized with .

The short can be either similar in quality to the long , or it can be as high as , with possible intermediate pronunciations ().


Hausa is a tonal language. Each of its five vowels may have low tone, high tone or falling tone. In standard written Hausa, tone is not marked. In recent linguistic and pedagogical materials, tone is marked by means of diacritics.

An acute accent () may be used for high tone, but the usual practice is to leave high tone unmarked.

Hausa's modern official orthography is a Latin-based alphabet called "boko", which was introduced in the 1930s by the British colonial administration.
The letter "ƴ" (y with a right hook) is used only in Niger; in Nigeria it is written "ʼy".

Tone, vowel length, and the distinction between and (which does not exist for all speakers) are not marked in writing. So, for example, "from" and "battle" are both written "daga".

Hausa has also been written in "ajami", an Arabic alphabet, since the early 17th century. The first known work to be written in Hausa is Riwayar Nabi Musa by Abdullahi Suka in the 17th century. There is no standard system of using "ajami", and different writers may use letters with different values. Short vowels are written regularly with the help of vowel marks, which are seldom used in Arabic texts other than the Quran. Many medieval Hausa manuscripts in "ajami", similar to the Timbuktu Manuscripts, have been discovered recently; some of them even describe constellations and calendars.

In the following table, vowels are shown with the Arabic letter for "t" () as an example.

Hausa is one of three indigenous languages of Nigeria which has been rendered in braille.

At least three other writing systems for Hausa have been proposed or "discovered". None of these are in active use beyond perhaps some individuals.






</doc>
<doc id="14220" url="https://en.wikipedia.org/wiki?curid=14220" title="History of mathematics">
History of mathematics

The area of study known as the history of mathematics is primarily an investigation into the origin of discoveries in mathematics and, to a lesser extent, an investigation into the mathematical methods and notation of the past. Before the modern age and the worldwide spread of knowledge, written examples of new mathematical developments have come to light only in a few locales. From 3000 BC the Mesopotamian states of Sumer, Akkad and Assyria, together with Ancient Egypt and Ebla began using arithmetic, algebra and geometry for purposes of taxation, commerce, trade and also in the field of astronomy and to formulate calendars and record time.

The most ancient mathematical texts available are from Mesopotamia and Egypt - "Plimpton 322" (Babylonian c. 1900 BC), the "Rhind Mathematical Papyrus" (Egyptian c. 2000–1800 BC) and the "Moscow Mathematical Papyrus" (Egyptian c. 1890 BC). All of these texts mention the so-called Pythagorean triples and so, by inference, the Pythagorean theorem, seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry.

The study of mathematics as a "demonstrative discipline" begins in the 6th century BC with the Pythagoreans, who coined the term "mathematics" from the ancient Greek "μάθημα" ("mathema"), meaning "subject of instruction". Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics. Although they made virtually no contributions to theoretical mathematics, the ancient Romans used applied mathematics in surveying, structural engineering, mechanical engineering, bookkeeping, creation of lunar and solar calendars, and even arts and crafts. Chinese mathematics made early contributions, including a place value system and the first use of negative numbers. The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics through the work of Muḥammad ibn Mūsā al-Khwārizmī. Islamic mathematics, in turn, developed and expanded the mathematics known to these civilizations. Contemporaneous with but independent of these traditions were the mathematics developed by the Maya civilization of Mexico and Central America, where the concept of zero was given a standard symbol in Maya numerals.

Many Greek and Arabic texts on mathematics were translated into Latin from the 12th century onward, leading to further development of mathematics in Medieval Europe. From ancient times through the Middle Ages, periods of mathematical discovery were often followed by centuries of stagnation. Beginning in Renaissance Italy in the 15th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day. This includes the groundbreaking work of both Isaac Newton and Gottfried Wilhelm Leibniz in the development of infinitesimal calculus during the course of the 17th century. At the end of the 19th century the International Congress of Mathematicians was founded and continues to spearhead advances in the field.

The origins of mathematical thought lie in the concepts of number, magnitude, and form. Modern studies of animal cognition have shown that these concepts are not unique to humans. Such concepts would have been part of everyday life in hunter-gatherer societies. The idea of the "number" concept evolving gradually over time is supported by the existence of languages which preserve the distinction between "one", "two", and "many", but not of numbers larger than two.

Prehistoric artifacts discovered in Africa, dated 20,000 years old or more suggest early attempts to quantify time. The Ishango bone, found near the headwaters of the Nile river (northeastern Congo), may be more than 20,000 years old and consists of a series of marks carved in three columns running the length of the bone. Common interpretations are that the Ishango bone shows either a "tally" of the earliest known demonstration of sequences of prime numbers or a six-month lunar calendar. Peter Rudman argues that the development of the concept of prime numbers could only have come about after the concept of division, which he dates to after 10,000 BC, with prime numbers probably not being understood until about 500 BC. He also writes that "no attempt has been made to explain why a tally of something should exhibit multiples of two, prime numbers between 10 and 20, and some numbers that are almost multiples of 10." The Ishango bone, according to scholar Alexander Marshack, may have influenced the later development of mathematics in Egypt as, like some entries on the Ishango bone, Egyptian arithmetic also made use of multiplication by 2; this however, is disputed.

Predynastic Egyptians of the 5th millennium BC pictorially represented geometric designs. It has been claimed that megalithic monuments in England and Scotland, dating from the 3rd millennium BC, incorporate geometric ideas such as circles, ellipses, and Pythagorean triples in their design. All of the above are disputed however, and the currently oldest undisputed mathematical documents are from Babylonian and dynastic Egyptian sources.

Babylonian mathematics refers to any mathematics of the peoples of Mesopotamia (modern Iraq) from the days of the early Sumerians through the Hellenistic period almost to the dawn of Christianity. The majority of Babylonian mathematical work comes from two widely separated periods: The first few hundred years of the second millennium BC (Old Babylonian period), and the last few centuries of the first millennium BC (Seleucid period). It is named Babylonian mathematics due to the central role of Babylon as a place of study. Later under the Arab Empire, Mesopotamia, especially Baghdad, once again became an important center of study for Islamic mathematics.

In contrast to the sparsity of sources in Egyptian mathematics, our knowledge of Babylonian mathematics is derived from more than 400 clay tablets unearthed since the 1850s. Written in Cuneiform script, tablets were inscribed whilst the clay was moist, and baked hard in an oven or by the heat of the sun. Some of these appear to be graded homework.

The earliest evidence of written mathematics dates back to the ancient Sumerians, who built the earliest civilization in Mesopotamia. They developed a complex system of metrology from 3000 BC. From around 2500 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.
Babylonian mathematics were written using a sexagesimal (base-60) numeral system. From this derives the modern day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 x 6) degrees in a circle, as well as the use of seconds and minutes of arc to denote fractions of a degree. It is likely the sexagesimal system was chosen because 60 can be evenly divided by 2, 3, 4, 5, 6, 10, 12, 15, 20 and 30. Also, unlike the Egyptians, Greeks, and Romans, the Babylonians had a true place-value system, where digits written in the left column represented larger values, much as in the decimal system. The power of the Babylonian notational system lay in that it could be used to represent fractions as easily as whole numbers; thus multiplying two numbers that contained fractions was no different than multiplying integers, similar to our modern notation. The notational system of the Babylonians was the best of any civilization until the Renaissance, and its power allowed it to achieve remarkable computation accuracy and power; for example, the Babylonian tablet YBC 7289 gives an approximation of accurate to five decimal places. The Babylonians lacked, however, an equivalent of the decimal point, and so the place value of a symbol often had to be inferred from the context. By the Seleucid period, the Babylonians had developed a zero symbol as a placeholder for empty positions; however it was only used for intermediate positions. This zero sign does not appear in terminal positions, thus the Babylonians came close but did not develop a true place value system.

Other topics covered by Babylonian mathematics include fractions, algebra, quadratic and cubic equations, and the calculation of regular reciprocal pairs. The tablets also include multiplication tables and methods for solving linear, quadratic equations and cubic equations, a remarkable achievement for the time. Tablets from the Old Babylonian period also contain the earliest known statement of the Pythagorean theorem. However, as with Egyptian mathematics, Babylonian mathematics shows no awareness of the difference between exact and approximate solutions, or the solvability of a problem, and most importantly, no explicit statement of the need for proofs or logical principles.

Egyptian mathematics refers to mathematics written in the Egyptian language. From the Hellenistic period, Greek replaced Egyptian as the written language of Egyptian scholars. Mathematical study in Egypt later continued under the Arab Empire as part of Islamic mathematics, when Arabic became the written language of Egyptian scholars.

The most extensive Egyptian mathematical text is the Rhind papyrus (sometimes also called the Ahmes Papyrus after its author), dated to c. 1650 BC but likely a copy of an older document from the Middle Kingdom of about 2000–1800 BC. It is an instruction manual for students in arithmetic and geometry. In addition to giving area formulas and methods for multiplication, division and working with unit fractions, it also contains evidence of other mathematical knowledge, including composite and prime numbers; arithmetic, geometric and harmonic means; and simplistic understandings of both the Sieve of Eratosthenes and perfect number theory (namely, that of the number 6). It also shows how to solve first order linear equations as well as arithmetic and geometric series.

Another significant Egyptian mathematical text is the Moscow papyrus, also from the Middle Kingdom period, dated to c. 1890 BC. It consists of what are today called "word problems" or "story problems", which were apparently intended as entertainment. One problem is considered to be of particular importance because it gives a method for finding the volume of a frustum (truncated pyramid).

Finally, the Berlin Papyrus 6619 (c. 1800 BC) shows that ancient Egyptians could solve a second-order algebraic equation.

Greek mathematics refers to the mathematics written in the Greek language from the time of Thales of Miletus (~600 BC) to the closure of the Academy of Athens in 529 AD. Greek mathematicians lived in cities spread over the entire Eastern Mediterranean, from Italy to North Africa, but were united by culture and language. Greek mathematics of the period following Alexander the Great is sometimes called Hellenistic mathematics.

Greek mathematics was much more sophisticated than the mathematics that had been developed by earlier cultures. All surviving records of pre-Greek mathematics show the use of inductive reasoning, that is, repeated observations used to establish rules of thumb. Greek mathematicians, by contrast, used deductive reasoning. The Greeks used logic to derive conclusions from definitions and axioms, and used mathematical rigor to prove them.

Greek mathematics is thought to have begun with Thales of Miletus (c. 624–c.546 BC) and Pythagoras of Samos (c. 582–c. 507 BC). Although the extent of the influence is disputed, they were probably inspired by Egyptian and Babylonian mathematics. According to legend, Pythagoras traveled to Egypt to learn mathematics, geometry, and astronomy from Egyptian priests.

Thales used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem. As a result, he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed. Pythagoras established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was "All is number". It was the Pythagoreans who coined the term "mathematics", and with whom the study of mathematics for its own sake begins. The Pythagoreans are credited with the first proof of the Pythagorean theorem, though the statement of the theorem has a long history, and with the proof of the existence of irrational numbers. Although he was preceded by the Babylonians and the Chinese, the Neopythagorean mathematician Nicomachus (60–120 AD) provided one of the earliest Greco-Roman multiplication tables, whereas the oldest extant Greek multiplication table is found on a wax tablet dated to the 1st century AD (now found in the British Museum). The association of the Neopythagoreans with the Western invention of the multiplication table is evident in its later Medieval name: the "mensa Pythagorica".

Plato (428/427 BC – 348/347 BC) is important in the history of mathematics for inspiring and guiding others. His Platonic Academy, in Athens, became the mathematical center of the world in the 4th century BC, and it was from this school that the leading mathematicians of the day, such as Eudoxus of Cnidus, came. Plato also discussed the foundations of mathematics,

Eudoxus (408–c.355 BC) developed the method of exhaustion, a precursor of modern integration and a theory of ratios that avoided the problem of incommensurable magnitudes. The former allowed the calculations of areas and volumes of curvilinear figures, while the latter enabled subsequent geometers to make significant advances in geometry. Though he made no specific technical mathematical discoveries, Aristotle (384–c.322 BC) contributed significantly to the development of mathematics by laying the foundations of logic.

In the 3rd century BC, the premier center of mathematical education and research was the Musaeum of Alexandria. It was there that Euclid (c. 300 BC) taught, and wrote the "Elements", widely considered the most successful and influential textbook of all time. The "Elements" introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the "Elements" were already known, Euclid arranged them into a single, coherent logical framework. The "Elements" was known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today. In addition to the familiar theorems of Euclidean geometry, the "Elements" was meant as an introductory textbook to all mathematical subjects of the time, such as number theory, algebra and solid geometry, including proofs that the square root of two is irrational and that there are infinitely many prime numbers. Euclid also wrote extensively on other subjects, such as conic sections, optics, spherical geometry, and mechanics, but only half of his writings survive.
Archimedes (c. 287–212 BC) of Syracuse, widely considered the greatest mathematician of antiquity, used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. He also showed one could use the method of exhaustion to calculate the value of π with as much precision as desired, and obtained the most accurate value of π then known, 3 < π < 3. He also studied the spiral bearing his name, obtained formulas for the volumes of surfaces of revolution (paraboloid, ellipsoid, hyperboloid), and an ingenious method of exponentiation for expressing very large numbers. While he is also known for his contributions to physics and several advanced mechanical devices, Archimedes himself placed far greater value on the products of his thought and general mathematical principles. He regarded as his greatest achievement his finding of the surface area and volume of a sphere, which he obtained by proving these are 2/3 the surface area and volume of a cylinder circumscribing the sphere.

Apollonius of Perga (c. 262–190 BC) made significant advances to the study of conic sections, showing that one can obtain all three varieties of conic section by varying the angle of the plane that cuts a double-napped cone. He also coined the terminology in use today for conic sections, namely parabola ("place beside" or "comparison"), "ellipse" ("deficiency"), and "hyperbola" ("a throw beyond"). His work "Conics" is one of the best known and preserved mathematical works from antiquity, and in it he derives many theorems concerning conic sections that would prove invaluable to later mathematicians and astronomers studying planetary motion, such as Isaac Newton. While neither Apollonius nor any other Greek mathematicians made the leap to coordinate geometry, Apollonius' treatment of curves is in some ways similar to the modern treatment, and some of his work seems to anticipate the development of analytical geometry by Descartes some 1800 years later.
Around the same time, Eratosthenes of Cyrene (c. 276–194 BC) devised the Sieve of Eratosthenes for finding prime numbers. The 3rd century BC is generally regarded as the "Golden Age" of Greek mathematics, with advances in pure mathematics henceforth in relative decline. Nevertheless, in the centuries that followed significant advances were made in applied mathematics, most notably trigonometry, largely to address the needs of astronomers. Hipparchus of Nicaea (c. 190–120 BC) is considered the founder of trigonometry for compiling the first known trigonometric table, and to him is also due the systematic use of the 360 degree circle. Heron of Alexandria (c. 10–70 AD) is credited with Heron's formula for finding the area of a scalene triangle and with being the first to recognize the possibility of negative numbers possessing square roots. Menelaus of Alexandria (c. 100 AD) pioneered spherical trigonometry through Menelaus' theorem. The most complete and influential trigonometric work of antiquity is the "Almagest" of Ptolemy (c. AD 90–168), a landmark astronomical treatise whose trigonometric tables would be used by astronomers for the next thousand years. Ptolemy is also credited with Ptolemy's theorem for deriving trigonometric quantities, and the most accurate value of π outside of China until the medieval period, 3.1416.

Following a period of stagnation after Ptolemy, the period between 250 and 350 AD is sometimes referred to as the "Silver Age" of Greek mathematics. During this period, Diophantus made significant advances in algebra, particularly indeterminate analysis, which is also known as "Diophantine analysis". The study of Diophantine equations and Diophantine approximations is a significant area of research to this day. His main work was the "Arithmetica", a collection of 150 algebraic problems dealing with exact solutions to determinate and indeterminate equations. The "Arithmetica" had a significant influence on later mathematicians, such as Pierre de Fermat, who arrived at his famous Last Theorem after trying to generalize a problem he had read in the "Arithmetica" (that of dividing a square into two squares). Diophantus also made significant advances in notation, the "Arithmetica" being the first instance of algebraic symbolism and syncopation.

Among the last great Greek mathematicians is Pappus of Alexandria (4th century AD). He is known for his hexagon theorem and centroid theorem, as well as the Pappus configuration and Pappus graph. His "Collection" is a major source of knowledge on Greek mathematics as most of it has survived. Pappus is considered the last major innovator in Greek mathematics, with subsequent work consisting mostly of commentaries on earlier work.
The first woman mathematician recorded by history was Hypatia of Alexandria (AD 350–415). She succeeded her father as Librarian at the Great Library and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria had her stripped publicly and executed. Her death is sometimes taken as the end of the era of the Alexandrian Greek mathematics, although work did continue in Athens for another century with figures such as Proclus, Simplicius and Eutocius. Although Proclus and Simplicius were more philosophers than mathematicians, their commentaries on earlier works are valuable sources on Greek mathematics. The closure of the neo-Platonic Academy of Athens by the emperor Justinian in 529 AD is traditionally held as marking the end of the era of Greek mathematics, although the Greek tradition continued unbroken in the Byzantine empire with mathematicians such as Anthemius of Tralles and Isidore of Miletus, the architects of the Haghia Sophia. Nevertheless, Byzantine mathematics consisted mostly of commentaries, with little in the way of innovation, and the centers of mathematical innovation were to be found elsewhere by this time.

Although ethnic Greek mathematicians continued to live under the rule of the late Roman Republic and subsequent Roman Empire, there were no noteworthy native Latin mathematicians in comparison. Ancient Romans such as Cicero (106-43 BC), an influential Roman statesman who studied mathematics in Greece, believed that Roman surveyors and calculators were far more interested in applied mathematics than the theoretical mathematics and geometry that were prized by the Greeks. It is unclear if the Romans first derived their numerical system directly from the Greek precedent or from Etruscan numerals used by the Etruscan civilization centered in what is now Tuscany, central Italy.

Using calculation, Romans were adept at both instigating and detecting financial fraud, as well as managing taxes for the treasury. Siculus Flaccus, one of the Roman "gromatici" (i.e. land surveyor), wrote the "Categories of Fields", which aided Roman surveyors in measuring the surface areas of allotted lands and territories. Aside from managing trade and taxes, the Romans also regularly applied mathematics to solve problems in engineering, including the erection of architecture such as bridges, road-building, and preparation for military campaigns. Arts and crafts such as Roman mosaics, inspired by previous Greek designs, created illusionist geometric patterns and rich, detailed scenes that required precise measurements for each tessera tile, the opus tessellatum pieces on average measuring eight millimeters square and the finer opus vermiculatum pieces having an average surface of four millimeters square.

The creation of the Roman calendar also necessitated basic mathematics. The first calendar allegedly dates back to 8th century BC during the Roman Kingdom and included 356 days plus a leap year every other year. In contrast, the lunar calendar of the Republican era contained 355 days, roughly ten-and-one-fourth days shorter than the solar year, a discrepancy that was solved by adding an extra month into the calendar after the 23rd of February. This calendar was supplanted by the Julian calendar, a solar calendar organized by Julius Caesar (100-44 BC) and devised by Sosigenes of Alexandria to include a leap day every four years in a 365-day cycle. This calendar, which contained an error of 11 minutes and 14 seconds, was later corrected by the Gregorian calendar organized by Pope Gregory XIII (), virtually the same solar calendar used in modern times as the international standard calendar.

At roughly the same time, the Han Chinese and the Romans both invented the wheeled odometer device for measuring distances traveled, the Roman model first described by the Roman civil engineer and architect Vitruvius (c. 80 BC - c. 15 BC). The device was used at least until the reign of emperor Commodus (), but its design seems to have been lost until experiments were made during the 15th century in Western Europe. Perhaps relying on similar gear-work and technology found in the Antikythera mechanism, the odometer of Vitruvius featured chariot wheels measuring 4 feet (1.2 m) in diameter turning four-hundred times in one Roman mile (roughly 4590 ft/1400 m). With each revolution, a pin-and-axle device engaged a 400-tooth cogwheel that turned a second gear responsible for dropping pebbles into a box, each pebble representing one mile traversed.

An analysis of early Chinese mathematics has demonstrated its unique development compared to other parts of the world, leading scholars to assume an entirely independent development. The oldest extant mathematical text from China is the "Zhoubi Suanjing", variously dated to between 1200 BC and 100 BC, though a date of about 300 BC during the Warring States Period appears reasonable. However, the Tsinghua Bamboo Slips, containing the earliest known decimal multiplication table (although ancient Babylonians had ones with a base of 60), is dated around 305 BC and is perhaps the oldest surviving mathematical text of China.

Of particular note is the use in Chinese mathematics of a decimal positional notation system, the so-called "rod numerals" in which distinct ciphers were used for numbers between 1 and 10, and additional ciphers for powers of ten. Thus, the number 123 would be written using the symbol for "1", followed by the symbol for "100", then the symbol for "2" followed by the symbol for "10", followed by the symbol for "3". This was the most advanced number system in the world at the time, apparently in use several centuries before the common era and well before the development of the Indian numeral system. Rod numerals allowed the representation of numbers as large as desired and allowed calculations to be carried out on the "suan pan", or Chinese abacus. The date of the invention of the "suan pan" is not certain, but the earliest written mention dates from AD 190, in Xu Yue's "Supplementary Notes on the Art of Figures".

The oldest existent work on geometry in China comes from the philosophical Mohist canon c. 330 BC, compiled by the followers of Mozi (470–390 BC). The "Mo Jing" described various aspects of many fields associated with physical science, and provided a small number of geometrical theorems as well. It also defined the concepts of circumference, diameter, radius, and volume.

In 212 BC, the Emperor Qin Shi Huang commanded all books in the Qin Empire other than officially sanctioned ones be burned. This decree was not universally obeyed, but as a consequence of this order little is known about ancient Chinese mathematics before this date. After the book burning of 212 BC, the Han dynasty (202 BC–220 AD) produced works of mathematics which presumably expanded on works that are now lost. The most important of these is "The Nine Chapters on the Mathematical Art", the full title of which appeared by AD 179, but existed in part under other titles beforehand. It consists of 246 word problems involving agriculture, business, employment of geometry to figure height spans and dimension ratios for Chinese pagoda towers, engineering, surveying, and includes material on right triangles. It created mathematical proof for the Pythagorean theorem, and a mathematical formula for Gaussian elimination. The treatise also provides values of π, which Chinese mathematicians originally approximated as 3 until Liu Xin (d. 23 AD) provided a figure of 3.1457 and subsequently Zhang Heng (78-139) approximated pi as 3.1724, as well as 3.162 by taking the square root of 10. Liu Hui commented on the "Nine Chapters" in the 3rd century AD and gave a value of π accurate to 5 decimal places (i.e. 3.14159). Though more of a matter of computational stamina than theoretical insight, in the 5th century AD Zu Chongzhi computed the value of π to seven decimal places (i.e. 3.141592), which remained the most accurate value of π for almost the next 1000 years. He also established a method which would later be called Cavalieri's principle to find the volume of a sphere.

The high-water mark of Chinese mathematics occurred in the 13th century during the latter half of the Song dynasty (960-1279), with the development of Chinese algebra. The most important text from that period is the "Precious Mirror of the Four Elements" by Zhu Shijie (1249–1314), dealing with the solution of simultaneous higher order algebraic equations using a method similar to Horner's method. The "Precious Mirror" also contains a diagram of Pascal's triangle with coefficients of binomial expansions through the eighth power, though both appear in Chinese works as early as 1100. The Chinese also made use of the complex combinatorial diagram known as the magic square and magic circles, described in ancient times and perfected by Yang Hui (AD 1238–1298).

Even after European mathematics began to flourish during the Renaissance, European and Chinese mathematics were separate traditions, with significant Chinese mathematical output in decline from the 13th century onwards. Jesuit missionaries such as Matteo Ricci carried mathematical ideas back and forth between the two cultures from the 16th to 18th centuries, though at this point far more mathematical ideas were entering China than leaving.

Japanese mathematics, Korean mathematics, and Vietnamese mathematics are traditionally viewed as stemming from Chinese mathematics and belonging to the Confucian-based East Asian cultural sphere. Korean and Japanese mathematics were heavily influenced by the algebraic works produced during China's Song dynasty, whereas Vietnamese mathematics was heavily indebted to popular works of China's Ming dynasty (1368-1644). For instance, although Vietnamese mathematical treatises were written in either Chinese or the native Vietnamese Chữ Nôm script, all of them followed the Chinese format of presenting a collection of problems with algorithms for solving them, followed by numerical answers. Mathematics in Vietnam and Korea were mostly associated with the professional court bureaucracy of mathematicians and astronomers, whereas in Japan it was more prevalent in the realm of private schools.

The earliest civilization on the Indian subcontinent is the Indus Valley Civilization (mature phase: 2600 to 1900 BC) that flourished in the Indus river basin. Their cities were laid out with geometric regularity, but no known mathematical documents survive from this civilization.

The oldest extant mathematical records from India are the Sulba Sutras (dated variously between the 8th century BC and the 2nd century AD), appendices to religious texts which give simple rules for constructing altars of various shapes, such as squares, rectangles, parallelograms, and others. As with Egypt, the preoccupation with temple functions points to an origin of mathematics in religious ritual. The Sulba Sutras give methods for constructing a circle with approximately the same area as a given square, which imply several different approximations of the value of π. In addition, they compute the square root of 2 to several decimal places, list Pythagorean triples, and give a statement of the Pythagorean theorem. All of these results are present in Babylonian mathematics, indicating Mesopotamian influence. It is not known to what extent the Sulba Sutras influenced later Indian mathematicians. As in China, there is a lack of continuity in Indian mathematics; significant advances are separated by long periods of inactivity.

Pāṇini (c. 5th century BC) formulated the rules for Sanskrit grammar. His notation was similar to modern mathematical notation, and used metarules, transformations, and recursion. Pingala (roughly 3rd–1st centuries BC) in his treatise of prosody uses a device corresponding to a binary numeral system. His discussion of the combinatorics of meters corresponds to an elementary version of the binomial theorem. Pingala's work also contains the basic ideas of Fibonacci numbers (called "mātrāmeru").

The next significant mathematical documents from India after the "Sulba Sutras" are the "Siddhantas", astronomical treatises from the 4th and 5th centuries AD (Gupta period) showing strong Hellenistic influence. They are significant in that they contain the first instance of trigonometric relations based on the half-chord, as is the case in modern trigonometry, rather than the full chord, as was the case in Ptolemaic trigonometry. Through a series of translation errors, the words "sine" and "cosine" derive from the Sanskrit "jiya" and "kojiya".

In the 5th century AD, Aryabhata wrote the "Aryabhatiya", a slim volume, written in verse, intended to supplement the rules of calculation used in astronomy and mathematical mensuration, though with no feeling for logic or deductive methodology. Though about half of the entries are wrong, it is in the "Aryabhatiya" that the decimal place-value system first appears. Several centuries later, the Muslim mathematician Abu Rayhan Biruni described the "Aryabhatiya" as a "mix of common pebbles and costly crystals".

In the 7th century, Brahmagupta identified the Brahmagupta theorem, Brahmagupta's identity and Brahmagupta's formula, and for the first time, in "Brahma-sphuta-siddhanta", he lucidly explained the use of zero as both a placeholder and decimal digit, and explained the Hindu–Arabic numeral system. It was from a translation of this Indian text on mathematics (c. 770) that Islamic mathematicians were introduced to this numeral system, which they adapted as Arabic numerals. Islamic scholars carried knowledge of this number system to Europe by the 12th century, and it has now displaced all older number systems throughout the world. Various symbol sets are used to represent numbers in the Hindu–Arabic numeral system, all of which evolved from the Brahmi numerals. Each of the roughly dozen major scripts of India has its own numeral glyphs. In the 10th century, Halayudha's commentary on Pingala's work contains a study of the Fibonacci sequence and Pascal's triangle, and describes the formation of a matrix.

In the 12th century, Bhāskara II lived in southern India and wrote extensively on all then known branches of mathematics. His work contains mathematical objects equivalent or approximately equivalent to infinitesimals, derivatives, the mean value theorem and the derivative of the sine function. To what extent he anticipated the invention of calculus is a controversial subject among historians of mathematics.

In the 14th century, Madhava of Sangamagrama, the founder of the so-called Kerala School of Mathematics, found the Madhava–Leibniz series, and, using 21 terms, computed the value of π as 3.14159265359. Madhava also found the Madhava-Gregory series to determine the arctangent, the Madhava-Newton power series to determine sine and cosine and the Taylor approximation for sine and cosine functions. In the 16th century, Jyesthadeva consolidated many of the Kerala School's developments and theorems in the "Yukti-bhāṣā". However, the Kerala School did not formulate a systematic theory of differentiation and integration, nor is there any direct evidence of their results being transmitted outside Kerala.

The Islamic Empire established across Persia, the Middle East, Central Asia, North Africa, Iberia, and in parts of India in the 8th century made significant contributions towards mathematics. Although most Islamic texts on mathematics were written in Arabic, most of them were not written by Arabs, since much like the status of Greek in the Hellenistic world, Arabic was used as the written language of non-Arab scholars throughout the Islamic world at the time. Persians contributed to the world of Mathematics alongside Arabs.

In the 9th century, the Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī wrote several important books on the Hindu–Arabic numerals and on methods for solving equations. His book "On the Calculation with Hindu Numerals", written about 825, along with the work of Al-Kindi, were instrumental in spreading Indian mathematics and Indian numerals to the West. The word "algorithm" is derived from the Latinization of his name, Algoritmi, and the word "algebra" from the title of one of his works, "Al-Kitāb al-mukhtaṣar fī hīsāb al-ğabr wa’l-muqābala" ("The Compendious Book on Calculation by Completion and Balancing"). He gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots, and he was the first to teach algebra in an elementary form and for its own sake. He also discussed the fundamental method of "reduction" and "balancing", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which al-Khwārizmī originally described as "al-jabr". His algebra was also no longer concerned "with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study." He also studied an equation for its own sake and "in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems."

In Egypt, Abu Kamil extended algebra to the set of irrational numbers, accepting square roots and fourth roots as solutions and coefficients to quadratic equations. He also developed techniques used to solve three non-linear simultaneous equations with three unknown variables. One unique feature of his works was trying to find all the possible solutions to some of his problems, including one where he found 2676 solutions. His works formed an important foundation for the development of algebra and influenced later mathematicians, such as al-Karaji and Fibonacci.

Further developments in algebra were made by Al-Karaji in his treatise "al-Fakhri", where he extends the methodology to incorporate integer powers and integer roots of unknown quantities. Something close to a proof by mathematical induction appears in a book written by Al-Karaji around 1000 AD, who used it to prove the binomial theorem, Pascal's triangle, and the sum of integral cubes. The historian of mathematics, F. Woepcke, praised Al-Karaji for being "the first who introduced the theory of algebraic calculus." Also in the 10th century, Abul Wafa translated the works of Diophantus into Arabic. Ibn al-Haytham was the first mathematician to derive the formula for the sum of the fourth powers, using a method that is readily generalizable for determining the general formula for the sum of any integral powers. He performed an integration in order to find the volume of a paraboloid, and was able to generalize his result for the integrals of polynomials up to the fourth degree. He thus came close to finding a general formula for the integrals of polynomials, but he was not concerned with any polynomials higher than the fourth degree.

In the late 11th century, Omar Khayyam wrote "Discussions of the Difficulties in Euclid", a book about what he perceived as flaws in Euclid's "Elements", especially the parallel postulate. He was also the first to find the general geometric solution to cubic equations. He was also very influential in calendar reform.

In the 13th century, Nasir al-Din Tusi (Nasireddin) made advances in spherical trigonometry. He also wrote influential work on Euclid's parallel postulate. In the 15th century, Ghiyath al-Kashi computed the value of π to the 16th decimal place. Kashi also had an algorithm for calculating "n"th roots, which was a special case of the methods given many centuries later by Ruffini and Horner.

Other achievements of Muslim mathematicians during this period include the addition of the decimal point notation to the Arabic numerals, the discovery of all the modern trigonometric functions besides the sine, al-Kindi's introduction of cryptanalysis and frequency analysis, the development of analytic geometry by Ibn al-Haytham, the beginning of algebraic geometry by Omar Khayyam and the development of an algebraic notation by al-Qalasādī.

During the time of the Ottoman Empire and Safavid Empire from the 15th century, the development of Islamic mathematics became stagnant.

In the Pre-Columbian Americas, the Maya civilization that flourished in Mexico and Central America during the 1st millennium AD developed a unique tradition of mathematics that, due to its geographic isolation, was entirely independent of existing European, Egyptian, and Asian mathematics. Maya numerals utilized a base of 20, the vigesimal system, instead of a base of ten that forms the basis of the decimal system used by most modern cultures. The Mayas used mathematics to create the Maya calendar as well as to predict astronomical phenomena in their native Maya astronomy. While the concept of zero had to be inferred in the mathematics of many contemporary cultures, the Mayas developed a standard symbol for it.

Medieval European interest in mathematics was driven by concerns quite different from those of modern mathematicians. One driving element was the belief that mathematics provided the key to understanding the created order of nature, frequently justified by Plato's "Timaeus" and the biblical passage (in the "Book of Wisdom") that God had "ordered all things in measure, and number, and weight".

Boethius provided a place for mathematics in the curriculum in the 6th century when he coined the term "quadrivium" to describe the study of arithmetic, geometry, astronomy, and music. He wrote "De institutione arithmetica", a free translation from the Greek of Nicomachus's "Introduction to Arithmetic"; "De institutione musica", also derived from Greek sources; and a series of excerpts from Euclid's "Elements". His works were theoretical, rather than practical, and were the basis of mathematical study until the recovery of Greek and Arabic mathematical works.

In the 12th century, European scholars traveled to Spain and Sicily seeking scientific Arabic texts, including al-Khwārizmī's "The Compendious Book on Calculation by Completion and Balancing", translated into Latin by Robert of Chester, and the complete text of Euclid's "Elements", translated in various versions by Adelard of Bath, Herman of Carinthia, and Gerard of Cremona. These and other new sources sparked a renewal of mathematics.

Leonardo of Pisa, now known as Fibonacci, serendipitously learned about the Hindu–Arabic numerals on a trip to what is now Béjaïa, Algeria with his merchant father. (Europe was still using Roman numerals.) There, he observed a system of arithmetic (specifically algorism) which due to the positional notation of Hindu–Arabic numerals was much more efficient and greatly facilitated commerce. Leonardo wrote "Liber Abaci" in 1202 (updated in 1254) introducing the technique to Europe and beginning a long period of popularizing it. The book also brought to Europe what is now known as the Fibonacci sequence (known to Indian mathematicians for hundreds of years before that) which was used as an unremarkable example within the text.

The 14th century saw the development of new mathematical concepts to investigate a wide range of problems. One important contribution was development of mathematics of local motion.

Thomas Bradwardine proposed that speed (V) increases in arithmetic proportion as the ratio of force (F) to resistance (R) increases in geometric proportion. Bradwardine expressed this by a series of specific examples, but although the logarithm had not yet been conceived, we can express his conclusion anachronistically by writing:
V = log (F/R). Bradwardine's analysis is an example of transferring a mathematical technique used by al-Kindi and Arnald of Villanova to quantify the nature of compound medicines to a different physical problem.

One of the 14th-century Oxford Calculators, William Heytesbury, lacking differential calculus and the concept of limits, proposed to measure instantaneous speed "by the path that would be described by [a body] if... it were moved uniformly at the same degree of speed with which it is moved in that given instant".

Heytesbury and others mathematically determined the distance covered by a body undergoing uniformly accelerated motion (today solved by integration), stating that "a moving body uniformly acquiring or losing that increment [of speed] will traverse in some given time a [distance] completely equal to that which it would traverse if it were moving continuously through the same time with the mean degree [of speed]".

Nicole Oresme at the University of Paris and the Italian Giovanni di Casali independently provided graphical demonstrations of this relationship, asserting that the area under the line depicting the constant acceleration, represented the total distance traveled. In a later mathematical commentary on Euclid's "Elements", Oresme made a more detailed general analysis in which he demonstrated that a body will acquire in each successive increment of time an increment of any quality that increases as the odd numbers. Since Euclid had demonstrated the sum of the odd numbers are the square numbers, the total quality acquired by the body increases as the square of the time.

During the Renaissance, the development of mathematics and of accounting were intertwined. While there is no direct relationship between algebra and accounting, the teaching of the subjects and the books published often intended for the children of merchants who were sent to reckoning schools (in Flanders and Germany) or abacus schools (known as "abbaco" in Italy), where they learned the skills useful for trade and commerce. There is probably no need for algebra in performing bookkeeping operations, but for complex bartering operations or the calculation of compound interest, a basic knowledge of arithmetic was mandatory and knowledge of algebra was very useful.

Piero della Francesca (c.1415–1492) wrote books on solid geometry and linear perspective, including "De Prospectiva Pingendi (On Perspective for Painting)", "Trattato d’Abaco (Abacus Treatise)", and "De corporibus regularibus (Regular Solids)".

Luca Pacioli's "Summa de Arithmetica, Geometria, Proportioni et Proportionalità" (Italian: "Review of Arithmetic, Geometry, Ratio and Proportion") was first printed and published in Venice in 1494. It included a 27-page treatise on bookkeeping, ""Particularis de Computis et Scripturis"" (Italian: "Details of Calculation and Recording"). It was written primarily for, and sold mainly to, merchants who used the book as a reference text, as a source of pleasure from the mathematical puzzles it contained, and to aid the education of their sons. In "Summa Arithmetica", Pacioli introduced symbols for plus and minus for the first time in a printed book, symbols that became standard notation in Italian Renaissance mathematics. "Summa Arithmetica" was also the first known book printed in Italy to contain algebra. Pacioli obtained many of his ideas from Piero Della Francesca whom he plagiarized.

In Italy, during the first half of the 16th century, Scipione del Ferro and Niccolò Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book "Ars Magna", together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. In 1572 Rafael Bombelli published his "L'Algebra" in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations.

Simon Stevin's book "De Thiende" ('the art of tenths'), first published in Dutch in 1585, contained the first systematic treatment of decimal notation, which influenced all later work on the real number system.

Driven by the demands of navigation and the growing need for accurate maps of large areas, trigonometry grew to be a major branch of mathematics. Bartholomaeus Pitiscus was the first to use the word, publishing his "Trigonometria" in 1595. Regiomontanus's table of sines and cosines was published in 1533.

During the Renaissance the desire of artists to represent the natural world realistically, together with the rediscovered philosophy of the Greeks, led artists to study mathematics. They were also the engineers and architects of that time, and so had need of mathematics in any case. The art of painting in perspective, and the developments in geometry that involved, were studied intensely.

The 17th century saw an unprecedented increase of mathematical and scientific ideas across Europe. Galileo observed the moons of Jupiter in orbit about that planet, using a telescope based on a toy imported from Holland. Tycho Brahe had gathered an enormous quantity of mathematical data describing the positions of the planets in the sky. By his position as Brahe's assistant, Johannes Kepler was first exposed to and seriously interacted with the topic of planetary motion. Kepler's calculations were made simpler by the contemporaneous invention of logarithms by John Napier and Jost Bürgi. Kepler succeeded in formulating mathematical laws of planetary motion.
The analytic geometry developed by René Descartes (1596–1650) allowed those orbits to be plotted on a graph, in Cartesian coordinates.

Building on earlier work by many predecessors, Isaac Newton discovered the laws of physics explaining Kepler's Laws, and brought together the concepts now known as calculus. Independently, Gottfried Wilhelm Leibniz, who is arguably one of the most important mathematicians of the 17th century, developed calculus and much of the calculus notation still in use today. Science and mathematics had become an international endeavor, which would soon spread over the entire world.

In addition to the application of mathematics to the studies of the heavens, applied mathematics began to expand into new areas, with the correspondence of Pierre de Fermat and Blaise Pascal. Pascal and Fermat set the groundwork for the investigations of probability theory and the corresponding rules of combinatorics in their discussions over a game of gambling. Pascal, with his wager, attempted to use the newly developing probability theory to argue for a life devoted to religion, on the grounds that even if the probability of success was small, the rewards were infinite. In some sense, this foreshadowed the development of utility theory in the 18th–19th century.

The most influential mathematician of the 18th century was arguably Leonhard Euler. His contributions range from founding the study of graph theory with the Seven Bridges of Königsberg problem to standardizing many modern mathematical terms and notations. For example, he named the square root of minus 1 with the symbol "i", and he popularized the use of the Greek letter formula_1 to stand for the ratio of a circle's circumference to its diameter. He made numerous contributions to the study of topology, graph theory, calculus, combinatorics, and complex analysis, as evidenced by the multitude of theorems and notations named for him.

Other important European mathematicians of the 18th century included Joseph Louis Lagrange, who did pioneering work in number theory, algebra, differential calculus, and the calculus of variations, and Laplace who, in the age of Napoleon, did important work on the foundations of celestial mechanics and on statistics.

Throughout the 19th century mathematics became increasingly abstract. Carl Friedrich Gauss (1777–1855) epitomizes this trend. He did revolutionary work on functions of complex variables, in geometry, and on the convergence of series, leaving aside his many contributions to science. He also gave the first satisfactory proofs of the fundamental theorem of algebra and of the quadratic reciprocity law.
This century saw the development of the two forms of non-Euclidean geometry, where the parallel postulate of Euclidean geometry no longer holds.
The Russian mathematician Nikolai Ivanovich Lobachevsky and his rival, the Hungarian mathematician János Bolyai, independently defined and studied hyperbolic geometry, where uniqueness of parallels no longer holds. In this geometry the sum of angles in a triangle add up to less than 180°. Elliptic geometry was developed later in the 19th century by the German mathematician Bernhard Riemann; here no parallel can be found and the angles in a triangle add up to more than 180°. Riemann also developed Riemannian geometry, which unifies and vastly generalizes the three types of geometry, and he defined the concept of a manifold, which generalizes the ideas of curves and surfaces.

The 19th century saw the beginning of a great deal of abstract algebra. Hermann Grassmann in Germany gave a first version of vector spaces, William Rowan Hamilton in Ireland developed noncommutative algebra. The British mathematician George Boole devised an algebra that soon evolved into what is now called Boolean algebra, in which the only numbers were 0 and 1. Boolean algebra is the starting point of mathematical logic and has important applications in computer science.

Augustin-Louis Cauchy, Bernhard Riemann, and Karl Weierstrass reformulated the calculus in a more rigorous fashion.

Also, for the first time, the limits of mathematics were explored. Niels Henrik Abel, a Norwegian, and Évariste Galois, a Frenchman, proved that there is no general algebraic method for solving polynomial equations of degree greater than four (Abel–Ruffini theorem). Other 19th-century mathematicians utilized this in their proofs that straightedge and compass alone are not sufficient to trisect an arbitrary angle, to construct the side of a cube twice the volume of a given cube, nor to construct a square equal in area to a given circle. Mathematicians had vainly attempted to solve all of these problems since the time of the ancient Greeks. On the other hand, the limitation of three dimensions in geometry was surpassed in the 19th century through considerations of parameter space and hypercomplex numbers.

Abel and Galois's investigations into the solutions of various polynomial equations laid the groundwork for further developments of group theory, and the associated fields of abstract algebra. In the 20th century physicists and other scientists have seen group theory as the ideal way to study symmetry.

In the later 19th century, Georg Cantor established the first foundations of set theory, which enabled the rigorous treatment of the notion of infinity and has become the common language of nearly all mathematics. Cantor's set theory, and the rise of mathematical logic in the hands of Peano, L. E. J. Brouwer, David Hilbert, Bertrand Russell, and A.N. Whitehead, initiated a long running debate on the foundations of mathematics.

The 19th century saw the founding of a number of national mathematical societies: the London Mathematical Society in 1865, the Société Mathématique de France in 1872, the Circolo Matematico di Palermo in 1884, the Edinburgh Mathematical Society in 1883, and the American Mathematical Society in 1888. The first international, special-interest society, the Quaternion Society, was formed in 1899, in the context of a vector controversy.

In 1897, Hensel introduced p-adic numbers.

The 20th century saw mathematics become a major profession. Every year, thousands of new Ph.D.s in mathematics were awarded, and jobs were available in both teaching and industry. An effort to catalogue the areas and applications of mathematics was undertaken in Klein's encyclopedia.

In a 1900 speech to the International Congress of Mathematicians, David Hilbert set out a list of 23 unsolved problems in mathematics. These problems, spanning many areas of mathematics, formed a central focus for much of 20th-century mathematics. Today, 10 have been solved, 7 are partially solved, and 2 are still open. The remaining 4 are too loosely formulated to be stated as solved or not.

Notable historical conjectures were finally proven. In 1976, Wolfgang Haken and Kenneth Appel used a computer to prove the four color theorem. Andrew Wiles, building on the work of others, proved Fermat's Last Theorem in 1995. Paul Cohen and Kurt Gödel proved that the continuum hypothesis is independent of (could neither be proved nor disproved from) the standard axioms of set theory. In 1998 Thomas Callister Hales proved the Kepler conjecture.

Mathematical collaborations of unprecedented size and scope took place. An example is the classification of finite simple groups (also called the "enormous theorem"), whose proof between 1955 and 1983 required 500-odd journal articles by about 100 authors, and filling tens of thousands of pages. A group of French mathematicians, including Jean Dieudonné and André Weil, publishing under the pseudonym "Nicolas Bourbaki", attempted to exposit all of known mathematics as a coherent rigorous whole. The resulting several dozen volumes has had a controversial influence on mathematical education.
Differential geometry came into its own when Einstein used it in general relativity. Entirely new areas of mathematics such as mathematical logic, topology, and John von Neumann's game theory changed the kinds of questions that could be answered by mathematical methods. All kinds of structures were abstracted using axioms and given names like metric spaces, topological spaces etc. As mathematicians do, the concept of an abstract structure was itself abstracted and led to category theory. Grothendieck and Serre recast algebraic geometry using sheaf theory. Large advances were made in the qualitative study of dynamical systems that Poincaré had begun in the 1890s.
Measure theory was developed in the late 19th and early 20th centuries. Applications of measures include the Lebesgue integral, Kolmogorov's axiomatisation of probability theory, and ergodic theory. Knot theory greatly expanded. Quantum mechanics led to the development of functional analysis. Other new areas include Laurent Schwartz's distribution theory, fixed point theory, singularity theory and René Thom's catastrophe theory, model theory, and Mandelbrot's fractals. Lie theory with its Lie groups and Lie algebras became one of the major areas of study.

Non-standard analysis, introduced by Abraham Robinson, rehabilitated the infinitesimal approach to calculus, which had fallen into disrepute in favour of the theory of limits, by extending the field of real numbers to the Hyperreal numbers which include infinitesimal and infinite quantities. An even larger number system, the surreal numbers were discovered by John Horton Conway in connection with combinatorial games.

The development and continual improvement of computers, at first mechanical analog machines and then digital electronic machines, allowed industry to deal with larger and larger amounts of data to facilitate mass production and distribution and communication, and new areas of mathematics were developed to deal with this: Alan Turing's computability theory; complexity theory; Derrick Henry Lehmer's use of ENIAC to further number theory and the Lucas-Lehmer test; Rózsa Péter's recursive function theory; Claude Shannon's information theory; signal processing; data analysis; optimization and other areas of operations research. In the preceding centuries much mathematical focus was on calculus and continuous functions, but the rise of computing and communication networks led to an increasing importance of discrete concepts and the expansion of combinatorics including graph theory. The speed and data processing abilities of computers also enabled the handling of mathematical problems that were too time-consuming to deal with by pencil and paper calculations, leading to areas such as numerical analysis and symbolic computation. Some of the most important methods and algorithms of the 20th century are: the simplex algorithm, the Fast Fourier Transform, error-correcting codes, the Kalman filter from control theory and the RSA algorithm of public-key cryptography.

At the same time, deep insights were made about the limitations to mathematics. In 1929 and 1930, it was proved the truth or falsity of all statements formulated about the natural numbers plus one of addition and multiplication, was decidable, i.e. could be determined by some algorithm. In 1931, Kurt Gödel found that this was not the case for the natural numbers plus both addition and multiplication; this system, known as Peano arithmetic, was in fact incompletable. (Peano arithmetic is adequate for a good deal of number theory, including the notion of prime number.) A consequence of Gödel's two incompleteness theorems is that in any mathematical system that includes Peano arithmetic (including all of analysis and geometry), truth necessarily outruns proof, i.e. there are true statements that cannot be proved within the system. Hence mathematics cannot be reduced to mathematical logic, and David Hilbert's dream of making all of mathematics complete and consistent needed to be reformulated.
One of the more colorful figures in 20th-century mathematics was Srinivasa Aiyangar Ramanujan (1887–1920), an Indian autodidact who conjectured or proved over 3000 theorems, including properties of highly composite numbers, the partition function and its asymptotics, and mock theta functions. He also made major investigations in the areas of gamma functions, modular forms, divergent series, hypergeometric series and prime number theory.

Paul Erdős published more papers than any other mathematician in history, working with hundreds of collaborators. Mathematicians have a game equivalent to the Kevin Bacon Game, which leads to the Erdős number of a mathematician. This describes the "collaborative distance" between a person and Paul Erdős, as measured by joint authorship of mathematical papers.

Emmy Noether has been described by many as the most important woman in the history of mathematics. She studied the theories of rings, fields, and algebras.

As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: by the end of the century there were hundreds of specialized areas in mathematics and the Mathematics Subject Classification was dozens of pages long. More and more mathematical journals were published and, by the end of the century, the development of the World Wide Web led to online publishing.

In 2000, the Clay Mathematics Institute announced the seven Millennium Prize Problems, and in 2003 the Poincaré conjecture was solved by Grigori Perelman (who declined to accept an award, as he was critical of the mathematics establishment).

Most mathematical journals now have online versions as well as print versions, and many online-only journals are launched. There is an increasing drive towards open access publishing, first popularized by the arXiv.

There are many observable trends in mathematics, the most notable being that the subject is growing ever larger, computers are ever more important and powerful, the application of mathematics to bioinformatics is rapidly expanding, and the volume of data being produced by science and industry, facilitated by computers, is explosively expanding.














</doc>
<doc id="14223" url="https://en.wikipedia.org/wiki?curid=14223" title="HSK">
HSK

HSK may refer to:


</doc>
<doc id="14225" url="https://en.wikipedia.org/wiki?curid=14225" title="Hydrogen atom">
Hydrogen atom

A hydrogen atom is an atom of the chemical element hydrogen. The electrically neutral atom contains a single positively charged proton and a single negatively charged electron bound to the nucleus by the Coulomb force. Atomic hydrogen constitutes about 75% of the baryonic mass of the universe.

In everyday life on Earth, isolated hydrogen atoms (called "atomic hydrogen") are extremely rare. Instead, hydrogen tends to combine with other atoms in compounds, or with itself to form ordinary (diatomic) hydrogen gas, H. "Atomic hydrogen" and "hydrogen atom" in ordinary English use have overlapping, yet distinct, meanings. For example, a water molecule contains two hydrogen atoms, but does not contain atomic hydrogen (which would refer to isolated hydrogen atoms).

Attempts to develop a theoretical understanding of the hydrogen atom have been important to the history of quantum mechanics since all other atoms can be roughly understood by knowing in detail about this simplest atomic structure.

The most abundant isotope, hydrogen-1, protium, or light hydrogen, contains no neutrons and is simply a proton and an electron. Protium is stable and makes up 99.985% of naturally occurring hydrogen atoms.

Deuterium contains one neutron and one proton. Deuterium is stable and makes up 0.0156% of naturally occurring hydrogen and is used in industrial processes like nuclear reactors and Nuclear Magnetic Resonance.

Tritium contains two neutrons and one proton and is not stable, decaying with a half-life of 12.32 years. Because of the short half life, tritium does not exist in nature except in trace amounts.

Higher isotopes of hydrogen are only created in artificial accelerators and reactors and have half lives around the order of 10 seconds.

The formulas below are valid for all three isotopes of hydrogen, but slightly different values of the Rydberg constant (correction formula given below) must be used for each hydrogen isotope.

Hydrogen is not found without its electron in ordinary chemistry (room temperatures and pressures), as ionized hydrogen is highly chemically reactive. When ionized hydrogen is written as "H" as in the solvation of classical acids such as hydrochloric acid, the hydronium ion, HO, is meant, not a literal ionized single hydrogen atom. In that case, the acid transfers the proton to HO to form HO.

Ionized hydrogen without its electron, or free protons, are common in the interstellar medium, and solar wind.

The hydrogen atom has special significance in quantum mechanics and quantum field theory as a simple two-body problem physical system which has yielded many simple analytical solutions in closed-form.

Experiments by Ernest Rutherford in 1909 showed the structure of the atom to be a dense, positive nucleus with a light, negative charge orbiting around it. This immediately caused problems on how such a system could be stable. Classical electromagnetism had shown that any accelerating charge radiates energy described through the Larmor formula. If the electron is assumed to orbit in a perfect circle and radiates energy continuously, the electron would rapidly spiral into the nucleus with a fall time of:

Where formula_2 is the Bohr radius and formula_3 is the classical electron radius. If this were true, all atoms would instantly collapse, however atoms seem to be stable. Furthermore, the spiral inward would release a smear of electromagnetic frequencies as the orbit got smaller. Instead, atoms were observed to only emit discrete frequencies of radiation. The resolution would lie in the development of quantum mechanics.

In 1913, Niels Bohr obtained the energy levels and spectral frequencies of the hydrogen atom after making a number of simple assumptions in order to correct the failed classical model. The assumptions included:
Bohr supposed that the electron's angular momentum is quantized with possible values:

formula_4 where formula_5

and formula_6 is Planck constant over formula_7. He also supposed that the centripetal force which keeps the electron in its orbit is provided by the Coulomb force, and that energy is conserved. Bohr derived the energy of each orbit of the hydrogen atom to be:

where formula_9 is the electron mass, formula_10 is the electron charge, formula_11 is the vacuum permittivity, and formula_12 is the quantum number (now known as the principal quantum number). Bohr's predictions matched experiments measuring the hydrogen spectral series to the first order, giving more confidence to a theory that used quantized values.

For formula_13, the value
is called the Rydberg unit of energy. It is related to the Rydberg constant formula_15 of atomic physics by formula_16

The exact value of the Rydberg constant assumes that the nucleus is infinitely massive with respect to the electron. For hydrogen-1, hydrogen-2 (deuterium), and hydrogen-3 (tritium) the constant must be slightly modified to use the reduced mass of the system, rather than simply the mass of the electron. However, since the nucleus is much heavier than the electron, the values are nearly the same. The Rydberg constant "R" for a hydrogen atom (one electron), "R" is given by

formula_17

where formula_18 is the mass of the atomic nucleus. For hydrogen-1, the quantity formula_19 is about 1/1836 (i.e. the electron-to-proton mass ratio). For deuterium and tritium, the ratios are about 1/3670 and 1/5497 respectively. These figures, when added to 1 in the denominator, represent very small corrections in the value of "R", and thus only small corrections to all energy levels in corresponding hydrogen isotopes.

There were still problems with Bohr's model: 

Most of these shortcomings were repaired by Arnold Sommerfeld's modification of the Bohr model. Sommerfeld introduced two additional degrees of freedom allowing an electron to move on an elliptical orbit, characterized by its eccentricity and declination with respect to a chosen axis. This introduces two additional quantum numbers, which correspond to the orbital angular momentum and its projection on the chosen axis. Thus the correct multiplicity of states (except for the factor 2 accounting for the yet unknown electron spin) was found. Further applying special relativity theory to the elliptic orbits, Sommerfeld succeeded in deriving the correct expression for the fine structure of hydrogen spectra (which happens to be exactly the same as in the most elaborate Dirac theory). However some observed phenomena such as the anomalous Zeeman effect remain unexplained. These issues were resolved with the full development of quantum mechanics and the Dirac equation. It is often alleged, that the Schrödinger equation is superior to the Bohr-Sommerfeld theory in describing hydrogen atom. This is however not the case, as the most results of both approaches coincide or are very close (a remarkable exception is the problem of hydrogen atom in crossed electric and magnetic fields, which cannot be solved in the framework of the Bohr-Sommerfeld theory self-consistently), and their main shortcomings result from the absence of the electron spin in both theories. It was the complete failure of the Bohr-Sommerfeld theory to explain many-electron systems (such as helium atom or hydrogen molecule) which demonstrated its inadequacy in describing quantum phenomena.

The Schrödinger equation allows one to calculate the development of quantum systems with time and can give exact, analytical answers for the non-relativistic hydrogen atom.

The Hamiltonian of the hydrogen atom is the radial kinetic energy operator and coulomb attraction force between the positive proton and negative electron. Using the time-independent Schrödinger equation, ignoring all spin-coupling interactions and using the reduced mass formula_22, the equation is written as:

formula_23

Expanding the Laplacian in spherical coordinates:

formula_24

This is a separable, partial differential equation which can be solved in terms of special functions. The normalized position wavefunctions, given in spherical coordinates are:

where:

The quantum numbers can take the following values: 

Additionally, these wavefunctions are "normalized" (i.e., the integral of their modulus square equals 1) and orthogonal:
where formula_37 is the state represented by the wavefunction formula_38 in Dirac notation, and formula_39 is the Kronecker delta function.

The wavefunctions in momentum space are related to the wavefunctions in position space through a Fourier transform

which, for the bound states, results in 

where formula_42 denotes a Gegenbauer polynomial and formula_43 is in units of formula_44.

The solutions to the Schrödinger equation for hydrogen are analytical, giving a simple expression for the hydrogen energy levels and thus the frequencies of the hydrogen spectral lines and fully reproduced the Bohr model and went beyond it. It also yields two other quantum numbers and the shape of the electron's wave function ("orbital") for the various possible quantum-mechanical states, thus explaining the anisotropic character of atomic bonds.

The Schrödinger equation also applies to more complicated atoms and molecules. When there is more than one electron or nucleus the solution is not analytical and either computer calculations are necessary or simplifying assumptions must be made.

Since the Schrödinger equation is only valid for non-relativistic quantum mechanics, the solutions it yields for the hydrogen atom are not entirely correct. The Dirac equation of relativistic quantum theory improves these solutions (see below).

The solution of the Schrödinger equation (wave equation) for the hydrogen atom uses the fact that the Coulomb potential produced by the nucleus is isotropic (it is radially symmetric in space and only depends on the distance to the nucleus). Although the resulting energy eigenfunctions (the "orbitals") are not necessarily isotropic themselves, their dependence on the angular coordinates follows completely generally from this isotropy of the underlying potential: the eigenstates of the Hamiltonian (that is, the energy eigenstates) can be chosen as simultaneous eigenstates of the angular momentum operator. This corresponds to the fact that angular momentum is conserved in the orbital motion of the electron around the nucleus. Therefore, the energy eigenstates may be classified by two angular momentum quantum numbers, "ℓ" and "m" (both are integers). The angular momentum quantum number determines the magnitude of the angular momentum. The magnetic quantum number determines the projection of the angular momentum on the (arbitrarily chosen) "z"-axis.

In addition to mathematical expressions for total angular momentum and angular momentum projection of wavefunctions, an expression for the radial dependence of the wave functions must be found. It is only here that the details of the 1/"r" Coulomb potential enter (leading to Laguerre polynomials in "r"). This leads to a third quantum number, the principal quantum number . The principal quantum number in hydrogen is related to the atom's total energy.

Note that the maximum value of the angular momentum quantum number is limited by the principal quantum number: it can run only up to "n" − 1, i.e. .

Due to angular momentum conservation, states of the same "ℓ" but different "m" have the same energy (this holds for all problems with rotational symmetry). In addition, for the hydrogen atom, states of the same "n" but different "ℓ" are also degenerate (i.e. they have the same energy). However, this is a specific property of hydrogen and is no longer true for more complicated atoms which have an (effective) potential differing from the form 1/"r" (due to the presence of the inner electrons shielding the nucleus potential).

Taking into account the spin of the electron adds a last quantum number, the projection of the electron's spin angular momentum along the "z"-axis, which can take on two values. Therefore, any eigenstate of the electron in the hydrogen atom is described fully by four quantum numbers. According to the usual rules of quantum mechanics, the actual state of the electron may be any superposition of these states. This explains also why the choice of "z"-axis for the directional quantization of the angular momentum vector is immaterial: an orbital of given "ℓ" and "m"′ obtained for another preferred axis "z"′ can always be represented as a suitable superposition of the various states of different "m" (but same "l") that have been obtained for "z".

In 1928, Paul Dirac found an equation that was fully compatible with Special Relativity, and (as a consequence) made the wave function a 4-component "Dirac spinor" including "up" and "down" spin components, with both positive and "negative" energy (or matter and antimatter). The solution to this equation gave the following results, more accurate than the Schrödinger solution.

The energy levels of hydrogen, including fine structure (excluding Lamb shift and hyperfine structure), are given by the Sommerfeld fine structure expression:

where "α" is the fine-structure constant and "j" is the "total angular momentum" quantum number, which is equal to |"ℓ" ± | depending on the direction of the electron spin. This formula represents a small correction to the energy obtained by Bohr and Schrödinger as given above. The factor in square brackets in the last expression is nearly one; the extra term arises from relativistic effects (for details, see #Features going beyond the Schrödinger solution). It is worth noting that this expression was first obtained by A. Sommerfeld in 1916 based on the relativistic version of the old Bohr theory. Sommerfeld has however used different notation for the quantum numbers.

The image to the right shows the first few hydrogen atom orbitals (energy eigenfunctions). These are cross-sections of the probability density that are color-coded (black represents zero density and white represents the highest density). The angular momentum (orbital) quantum number "ℓ" is denoted in each column, using the usual spectroscopic letter code ("s" means "ℓ" = 0, "p" means "ℓ" = 1, "d" means "ℓ" = 2). The main (principal) quantum number "n" (= 1, 2, 3, ...) is marked to the right of each row. For all pictures the magnetic quantum number "m" has been set to 0, and the cross-sectional plane is the "xz"-plane ("z" is the vertical axis). The probability density in three-dimensional space is obtained by rotating the one shown here around the "z"-axis.

The "ground state", i.e. the state of lowest energy, in which the electron is usually found, is the first one, the 1"s" state (principal quantum level "n" = 1, "ℓ" = 0).

Black lines occur in each but the first orbital: these are the nodes of the wavefunction, i.e. where the probability density is zero. (More precisely, the nodes are spherical harmonics that appear as a result of solving Schrödinger equation in polar coordinates.)

The quantum numbers determine the layout of these nodes. There are:

There are several important effects that are neglected by the Schrödinger equation and which are responsible for certain small but measurable deviations of the real spectral lines from the predicted ones:


Both of these features (and more) are incorporated in the relativistic Dirac equation, with predictions that come still closer to experiment. Again the Dirac equation may be solved analytically in the special case of a two-body system, such as the hydrogen atom. The resulting solution quantum states now must be classified by the total angular momentum number "j" (arising through the coupling between electron spin and orbital angular momentum). States of the same "j" and the same "n" are still degenerate. Thus, direct analytical solution of Dirac equation predicts 2S() and 2P() levels of Hydrogen to have exactly the same energy, which is in a contradiction with observations (Lamb-Retherford experiment).


For these developments, it was essential that the solution of the Dirac equation for the hydrogen atom could be worked out exactly, such that any experimentally observed deviation had to be taken seriously as a signal of failure of the theory.

In the language of Heisenberg's matrix mechanics, the hydrogen atom was first solved by Wolfgang Pauli using a rotational symmetry in four dimensions [O(4)-symmetry] generated by the angular momentum 
and the Laplace–Runge–Lenz vector. By extending the symmetry group O(4) to the dynamical group O(4,2),
the entire spectrum and all transitions were embedded in a single irreducible group representation.

In 1979 the (non relativistic) hydrogen atom was solved for the first time within Feynman's path integral formulation
of quantum mechanics. This work greatly extended the range of applicability of Feynman's method.





</doc>
