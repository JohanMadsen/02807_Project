<doc id="20357" url="https://en.wikipedia.org/wiki?curid=20357" title="Medical psychology">
Medical psychology

Medical psychology is the application of psychological principles to the practice of medicine, and is clearly comprehensive rather than primarily drug-oriented, for both physical and mental disorders. The specialty of Medical Psychology and the National Alliance of Professional Psychology Providers (www.nappp.org) has been instrumental in advocacy and professional publications in increasing the awareness of Governmental Agencies, Scientific Societies, and the World Health Associations about the limited effect of "medication only approaches" to mental disorders and many related chronic physical disorders. A Medical Psychologist is a specialist who holds board certification in Medical Psychology from the American Board of Medical Psychology (www.amphome.org) and approved by the national psychology practitioner association in psychology(www.nappp.org). A specialist in Medical Psychology holds a doctoral degree in one of the clinical specialties in psychology, has done post doctoral graduate or approved didactic training in biomedical and pharmaceutical sciences and physical disease with behavioral and lifestyle components, and has completed a supervised residency providing advanced clinical diagnoses, prescribing or collaborating on medication and psychological treatment interventions in a comprehensive treatment plan, and they have passed one of the acceptable national written examinations, and supplied reviewed work product, and passed an Oral Examination. Medical psychologists are prepared to provide leadership and active roles in primary care and specialty healthcare facilities or consultation services essential for these facilities. A psychopharmacologist is very different than a Medical Psychologist, though one state uses confusing language in its laws.

The American Society for the Advancement of Pharmacotherapy defines medical psychology (an affiliate of the American Psychological Association) as "that branch of psychology integrating somatic and psychotherapeutic modalities into the management of mental illness and emotional, cognitive, behavioral and substance use disorders". 

A medical psychologist who holds prescriptive authority for specific psychiatric medications and other pharmaceutical drugs must first obtain specific qualifications in Psychopharmacology. A trained medical psychologist, or psychopharmacologist who has prescriptive authority is equated with a mid-level provider who has the authority to prescribe psychotropic medication such as antidepressants for neurotic disorders. However, a medical psychologist does not automatically equate with a psychologist who has the authority to prescribe medication. In fact, most medical psychologists do not prescribe medication and do not have the authority to do so.

Medical psychologists apply psychological theories, scientific psychological findings, and techniques of psychotherapy, behavior modification, cognitive, interpersonal, family, and life-style therapy to improve the psychological and physical health of the patient. Psychologists with post doctoral specialty training as medical psychologists are the practitioners with refined skills in clinical psychology, health psychology, behavioral medicine, psychopharmacology, and medical science. Highly qualified and post graduate specialized doctors are trained for service in primary care centers, hospitals, residential care centers, and long-term care facilities and in multidisciplinary collaboration and team treatment.

The field of medical psychology may include pre-doctoral training the disciplines of health psychology, rehabilitation psychology, pediatric psychology, neuropsychology, and clinical psychopharmacology, as well as sub-specialties in pain management, primary care psychology, and hospital-based (or medical school-based) psychology as the foundation psychological training to qualify for proceeding to required post-doctoral specialty training to qualify to become a Diplomate/Specialist in Medical Psychology. To be a Specialist in Medical Psychology a psychologist must hold Board Certification from the American Board of Medical Psychology which requires a doctorate degree in psychology, a license to practice psychology, a post doctorate graduate degree or acceptable post doctoral didactic training, a residency in medical psychology, submission of a work product for examination, a written and oral examination by the American Board of Medical Psychology. The American Board of Medical Psychology maintains a distinction between specialists and psychopharmacological psychologists or those interested in practicing one of the related psychological disciplines in primary care centers. The term Medical Psychologists is not an umbrella term, and many other specialties in psychology such as healthcare psychology, embracing the biopsychosocial paradigm (Engel, 1977) of mental/physical health and extending that paradigm to clinical practice through research and the application of evidenced-based diagnostic and treatment procedures are akin to the specialty and are prepared to practice in Integrated and Primary Care Settings.

Adopting the biopsychosocial paradigm, the field of medical psychology has recognized the Cartesian assumption that the body and mind are separate entities is inadequate, representing as it does an arbitrary dichotomy that works to the detriment of healthcare. The biopsychosocial approach reflects the concept that the psychology of an individual cannot be understood without reference to that individual’s social environment. For the medical psychologist, the medical model of disease cannot in itself explain complex health concerns any more than a strict psychosocial (LeVine & Orabona Foster, 2010) explanation of mental and physical health can in itself be comprehensive.

Medical psychologists and some psychopharmacologists are trained and equipped to modify physical disease states and the actual cytoarchitecture and functioning of the central nervous and related systems using psychological and pharmacological techniques (when allowed by statute), and to provide prevention for the progression of disease having to do with poor personal and life-style choices and conceptualization, behavioral patterns, and chronic exposure to the effects of negative thinking, choosing, attitudes, and negative contexts. The specialty of medical psychology includes training in psychopharmacology and in states providing statutory authority may prescribe psychoactive substances as one technique in a larger treatment plan which includes psychological interventions. The medical psychologists and psychopharmacologists who serve in states that have not yet modernized their psychology prescribing laws may evaluate patients and recommend appropriate psychopharmacological techniques in collaboration with a state authorized prescriber. Medical psychologists and psychopharmacologists who are not Board Certified strive to integrate the major components of an individual’s psychological, biological, and social functioning and are designed to contribute to that person’s well-being in a way that respects the natural interface among these components. The whole is greater than the sum of its parts when it comes to providing comprehensive and sensible behavioral healthcare and the medical psychologist is uniquely qualified to collaborate with physicians that are treating the patients physical illnesses.

The Academy of Medical Psychology defines medical psychology as a specialty trained at the post doctoral level and designed to deliver advanced diagnostic and clinical interventions in Medical and Healthcare Facilities utilizing the knowledge and skills of clinical psychology, health psychology, behavioral medicine, psychopharmacology and basic medical science. The Academy of Medical Psychology makes a distinction between the Psychopharmacologist who is a psychologist with advanced training in psychopharmacology and may prescribe medicine or consult with physician or nurse practitioner prescribers to diagnose mental illness and select and recommend appropriate psychoactive medicines, and the Medical Psychologists who are prepared to do the psychopharmacology consulting or prescribing, but also must have training which prepares them for functioning with Behavioral and Lifestyle components of physical disease and functioning in or in consultation with multidisciplinary healthcare teams in Primary Care Centers or Community Hospitals in addition to traditional roles in the treatment of mental illness and substance abuse disorders. The specialty of Medical Psychology and this distinction from Psychopharmacologist is recognized by the National Alliance of Professional Psychology Providers (the psychology national practitioner association; see www.nappp.org).

A specialty of medical psychology has established a specialty board certification, American Board of Medical Psychology and an Academy of Medical Psychology (www.amphome.org) requiring a doctorate degree in psychology and extensive post doctoral training in the specialty and the passage of an oral or written examination.

Although the Academy of Medical Psychology defines medical psychology as a "specialty" and has established a "specialty board certification," and is recognized by the national psychology practitioner association (www.nappp.org) there is a split in national psychology associations between NAPPP and APA and the American Psychological Association and the National Alliance of Professional Psychology Providers do not currently recognize the same specialties with the APA being a group that represents scientists, academics, and practitioners (as a minority) and NAPPP being an organization that represents only practitioners. However, Louisiana, having a unique to that state definition of medical psychology does recognize the national distinction between Medical Psychology as a Specialty and a psychopharmacology proficiency (See APA proficiency in psychopharmacology) and restricts the term and practice of medical psychology by statute (the Medical Psychology Practice Act) as a "profession of the health sciences" with prescriptive authority. It is equally important to note than the American Psychological Association does not recognize that the term medical psychology has, as a prerequisite, nor should the term be equated with having, prescriptive authority and has established psychology post doctoral prescribing medicines as "a proficiency in psychopharmacology".

In 2006, the American Psychological Association (APA) recommended that the education and training of psychologists, who are specifically pursuing one of several prerequisites for prescribing medication, integrate instruction in the biological sciences, clinical medicine and pharmacology into a formalized program of postdoctoral education. In 2009, the National Alliance of Professional Providers in Psychology recognized the education and training specified by the American Board of Medical Psychology (www.amphome.org; ABMP) and the Academy of Medical Psychology as the approved standards for post graduate training and examination and qualifications in the nationally recognized specialty in Medical Psychology. Since then numerous hospitals, primary care centers, and other health facilities have recognized the ABMP standards and qualifications for privileges in healthcare facilities and verification of specialty status.
The following "Clinical Competencies" are identified as essential in the education and training of psychologists, wishing to pursue prescriptive authority. These recommended prerequisites are not required or specifically recommended by APA for the training and education of medical psychologists not pursuing prerequisites for prescribing medication.:


The 2006 APA recommendations also include supervised clinical experience intended to integrate the above seven knowledge domains and assess competencies in skills and applied knowledge.

The national psychology practitioner association (NAPPP; www.nappp.org) and top national certifying body (Academy of Medical Psychology; www.amphome.org) have established the national training, examination, and specialty practice criterion and guidelines in the specialty of Medical Psychology and have established a national journal in the specialty. Such certifying bodies, view psychopharmacology training (either to prescribe or consult) as one component of the training of a specialist in Medical Psychology, but recognize that training and specialized skills in other aspects of the treatment of behavioral aspects of medical illness, and mental illness affecting physical illness is essential to practice at the specialty level in Medical Psychology. The Louisiana Academy of Medical Psychology (LAMP), currently the largest organization of psychologists with prescriptive authority in the world and the only organization representing practitioners of medical psychology in Louisiana as defined by Louisiana statute within any jurisdiction in the United States, no longer recognizes the Academy of Medical Psychology as an adequate certifying body for its practitioners, and its members have resigned from the Academy of Medical Psychology en masse. Similarly, virtually all members of LAMP have also resigned from the Louisiana Psychological Association (LPA) after many LPA members uncovered that the LAMP's prescriptive authority movement covertly came to an agreement with Louisiana's medical board to transfer the entire practice of psychology for psychologists with prescriptive authority to the medical board. Louisiana is the only state in which the practice of psychology, including psychological testing, psychotherapy, diagnosis, and treatment for some psychologists (i.e., medical psychologists) is regulated by a medical board.




</doc>
<doc id="20358" url="https://en.wikipedia.org/wiki?curid=20358" title="Music lesson">
Music lesson

Music lessons are a type of formal instruction in playing a musical instrument or singing. Typically, a student taking music lessons meets a music teacher for one-on-one training sessions ranging from 30 minutes to one hour in length over a period of weeks or years. For vocal lessons, teachers show students how to sit or stand and breathe, and how to position the head, chest, and mouth for good vocal tone. For instrument lessons, teachers show students how to sit or stand with the instrument, how to hold the instrument, and how to manipulate the fingers and other body parts to produce tones and sounds from the instrument. Music teachers also assign technical exercises, musical pieces, and other activities to help the students improve their musical skills. While most music lessons are one-on-one (private), some teachers also teach groups of two to four students (semi-private lessons), and, for very basic instruction, some instruments are taught in large group lessons, such as piano and acoustic guitar. Since the widespread availability of high speed. low latency Internet, private lessons can also take place through live video chat using webcams, microphones and videotelephony online.

Music lessons are part of both amateur music instruction and professional training. In amateur and recreational music contexts, children and adults take music lessons to improve their singing or instrumental playing skills and learn basic to intermediate techniques. In professional training contexts, such as music conservatories, university music performance programs (e.g., Bachelor of music, Master of music, DMA, etc.), students aiming for a career as professional musicians take a music lesson once a week for an hour or more with a music professor over a period of years to learn advanced playing or singing techniques. Many instrumental performers and singers, including a number of pop music celebrities, have learned music "by ear", especially in folk music styles such as blues and popular styles such as rock music. Nevertheless, even in folk and popular styles, a number of performers have had some type of music lessons, such as meeting with a vocal coach or getting childhood instruction in an instrument such as piano.

For vocal lessons, teachers show students how to sit or stand and breathe, and how to position the head and mouth for good vocal tone. For instrument lessons, teachers show students how to sit or stand with the instrument, how to hold the instrument, and how to manipulate the fingers and other body parts to produce tones and sounds from the instrument. For wind and brass instruments, the teacher shows the student how to use their lips, tongue, and breath to produce tones and sounds. For some instruments, teachers also train students in the use of the feet, as in the case of piano or other keyboard instruments that have damper or sustain pedals on the piano, the pedal keyboard on a pipe organ, and some drums and cymbals in the drum kit such as the bass drum pedal and the hi-hat cymbal pedal. In addition to teaching fingering, teachers also provide other types of instruction. A guitar player learns how to strum and pluck strings; players of wind instruments learn about breath control and embouchure, and singers learn how to make the most of their vocal cords without hurting the throat or vocal cords.

Teachers also show students how to achieve the correct posture for most efficient playing results. For all instruments, the best way to move the fingers and arms to achieve a desired effect is to learn to play with the least tension in your hands and body. This also prevents forming habits that can injure the skeletal frame and muscles. For example, when playing the piano, "fingering"—which fingers to put on which keys—is a skill slowly learned as the student advances, and there are many standard techniques a teacher can pass on.

There are many myths and misconceptions among music teachers, especially in the Western classical tradition, about "good" posture and "bad" posture. Students who find that playing their instruments causes them physical pain should bring this to their teachers' attention. It could be a potentially serious health risk, but it is often overlooked when learning to play an instrument. Learning to use one's body in a manner consistent with the way their anatomy is designed to work can mean the difference between a crippling injury and a lifetime of enjoyment. Many music teachers would caution students about taking "no pain, no gain" as an acceptable response from their music teacher regarding a complaint of physical pain. Concerns about use-related injury and the ergonomics of musicianship have gained more mainstream acceptance in recent years. Musicians have increasingly been turning to medical professionals, physical therapists, and specialized techniques seeking relief from pain and prevention of serious injury. There exists a plurality of special techniques for an even greater plurality of potential difficulties. The Alexander Technique is just one example of these specialized approaches.

To fully understand music being played, the student must learn the basics of the underlying music theory. Along with musical notation, students learn rhythmic techniques—like controlling tempo, recognizing time signatures, and the theory of harmony, including chords and key signatures. In addition to basic theory, a good teacher stresses "musicality", or how to make the music sound good. This includes how to create good, pleasing tone, how to do musical phrasing, and how to use dynamics (loudness and softness) to make the piece or song more expressive.

Most music lessons include some instruction in the history of the type of music that the student is learning. When a student is taking Western classical music lessons, music teachers often spend some time explaining the different eras of western classical music, such as the Baroque Era, the Classical era, the Romantic Era, and the contemporary classical music era, because each era is associated with different styles of music and different performance practice techniques. Instrumental music from the Baroque era is often played in the 2000s as teaching pieces for piano students, string instrument players, and wind instrument players. If students just try to play these Baroque pieces by reading the notes from the score, they might not get the right type of interpretation. However, once a student learns that most Baroque instrumental music was associated with dances, such as the gavotte and the sarabande, and keyboard music from the Baroque era was played on the harpsichord or the pipe organ, a modern-day student is better able to understand how the piece should be played. If, for example, a cello player is assigned a gavotte that was originally written for harpsichord, this gives the student insight in how to play the piece. Since it is a dance, it should have a regular, clear pulse, rather than a Romantic era-style shifting tempo rubato. As well, since it was originally written for the harpsichord, a light-sounding keyboard instrument in which the strings are plucked with quills, this suggests that the notes should be played relatively lightly, and with spaces between each note, rather than in a full-bodied, sustained legato.

Although not universally accepted, many teachers drill students with the repetitive playing of certain patterns, such as scales, arpeggios, and rhythms. Scales are often taught because they are the building blocks of melody in most Western art music. In addition, there are flexibility studies, which make it physically easier to play the instrument. 
Percussion instruments use rudiments that help in the development of sticking patterns, roll techniques and other little nuances such as flams and drags.

There are sets of exercises for piano designed to stretch the connection between fourth and fifth fingers, making them more independent. Brass players practice "lip slurs", which are unarticulated changes in embouchure between partials. Woodwind players (Saxophone, Clarinet, and Flute) have a multitude of exercises to help with tonguing techniques, finger dexterity, and tone development. Entire books of etudes have been written to this purpose.

Teachers typically assign the student pieces (or songs for vocal students) of slowly increasing difficulty. Besides using pieces to teach various musical rudiments (rhythm, harmony, pitch, etc.) and teach the elements of good playing (or singing) style, a good teacher also inspires more intangible qualities—such as expressiveness and musicianship. Pieces (or songs) are more enjoyable for most students than theory or scale exercises, and an emphasis on learning new pieces is usually required to maintain students' motivation. However, the teacher must not over-accommodate a student's desire for "fun" pieces. Often the student's idea of fun music is popular vocal selections, movie soundtracks, and TV show theme songs, etc. While some of these "fun" pieces can be performed, pieces should also be selected for pedagogical reasons, such as challenging the student and honing their skills. Student should learn something from every piece they play. In addition, for students to be well rounded they must play many types of pieces by composers and songwriters from different eras, ranging from Renaissance music to pieces from the 20th and 21st century. A varied repertoire increases the student's musical understanding and skill.

A popular measure of progress, especially for children, is external assessment of the progress of the pupil by a regular examination. A number of exam boards assess pupils on music theory or practice. These are available for almost every musical instrument. A common method to mark progress is graded examinations—for example from grade 1 (beginner) to grade 8 (ready to enter higher study at music school). Some teachers prefer other methods of target-setting for their pupils. The most common is the pupil's concert, which gives experience in playing in public and under a certain degree of pressure, without outright criticism or a more or less arbitrary marking system. Another is the graded system of books followed by teachers of the Suzuki method, in which the completion of each book is celebrated, without a system of marking or ranking of pupils.

Some studies suggests that music lessons provide children with important developmental benefits beyond simply the knowledge or skill of playing a musical instrument. Research suggests that musical lessons may enhance intelligence and academic achievement, build self-esteem and improve discipline. A recent Rockefeller Foundation Study found that music majors have the highest rate of admittance to medical schools, followed by biochemistry and the humanities. On SAT tests, the national average scores were 427 on the verbal and 476 on math. At the same time, music students averaged 465 on the verbal and 497 on the math - 38 and 21 points higher, respectively. However, the observed correlation between musical and mathematical ability may be inherent rather than acquired. Furthermore, it is possible that the correlation between taking music lessons and academic ability exists because both are strongly correlated with parental income and education. Even if music lessons had no impact on academic ability, one would expect to see a correlation between music lessons and academic ability.

Skills learned through the discipline of music may transfer to study skills, communication skills, and cognitive skills useful in every part of a child's studies at school, though. An in-depth Harvard University study found evidence that spatial-temporal reasoning improves when children learn to make music, and this kind of reasoning improves temporarily when adults listen to certain kinds of music, including Mozart. This finding (named "The Mozart effect") suggests that music and spatial reasoning are related psychologically (i.e., they may rely on some of the same underlying skills) and perhaps neurologically as well. However, there has been considerable controversy over this as later researchers have failed to reproduce the original findings of Rauscher (e.g. Steele, Bass & Crook, 1999), questioned both theory and methodology of the original study (Fudis & Lembesis 2004) and suggested that the enhancing effects of music in experiments have been simply due to an increased level of arousal (Thompson, Schellenberg & Husain, 2001).

A relationship between music and the strengthening of math, dance, reading, creative thinking and visual arts skills has also been reported in literature. (Winner, Hetland, Sanni, as reported in "The Arts and Academic Achievement - What the Evidence Shows", 2000) However recent findings by Dr. Levitin of McGill University in Montreal, Canada, undermines the suggested connection between musical ability and higher math skills. In a study conducted on patients with Williams syndrome (a genetic disorder causing low intelligence), he found that even though their intelligence was that of young children, they still possessed an unusually high level of musical ability.



</doc>
<doc id="20359" url="https://en.wikipedia.org/wiki?curid=20359" title="Mutagen">
Mutagen

In genetics, a mutagen is a physical or chemical agent that changes the genetic material, usually DNA, of an organism and thus increases the frequency of mutations above the natural background level. As many mutations can cause cancer, mutagens are therefore also likely to be carcinogens, although not always necessarily so. Some chemicals only become mutagenic through cellular processes. Not all mutations are caused by mutagens: so-called "spontaneous mutations" occur due to spontaneous hydrolysis, errors in DNA replication, repair and recombination.

The first mutagens to be identified were carcinogens, substances that were shown to be linked to cancer. Tumors were described more than 2,000 years before the discovery of chromosomes and DNA; in 500 B.C., the Greek physician Hippocrates named tumors resembling a crab "karkinos" (from which the word "cancer" is derived via Latin), meaning crab. In 1567, Swiss physician Paracelsus suggested that an unidentified substance in mined ore (identified as radon gas in modern times) caused a wasting disease in miners, and in England, in 1761, John Hill made the first direct link of cancer to chemical substances by noting that excessive use of snuff may cause nasal cancer. In 1775, Sir Percivall Pott wrote a paper on the high incidence of scrotal cancer in chimney sweeps, and suggested chimney soot as the cause of scrotal cancer. In 1915, Yamagawa and Ichikawa showed that repeated application of coal tar to rabbit's ears produced malignant cancer. Subsequently, in the 1930s the carcinogen component in coal tar was identified as a polyaromatic hydrocarbon (PAH), benzo[a]pyrene. Polyaromatic hydrocarbons are also present in soot, which was suggested to be a causative agent of cancer over 150 years earlier.

The association of exposure to radiation and cancer had been observed as early as 1902, six years after the discovery of X-ray by Wilhelm Röntgen and radioactivity by Henri Becquerel. Georgii Nadson and German Filippov were the first who created fungi mutants under ionizing radiation in 1925. The mutagenic property of mutagens was first demonstrated in 1927, when Hermann Muller discovered that x-rays can cause genetic mutations in fruit flies, producing phenotypic mutants as well as observable changes to the chromosomes, visible due to presence of enlarged 'polytene' chromosomes in fruit fly salivary glands. His collaborator Edgar Altenburg also demonstrated the mutational effect of UV radiation in 1928. Muller went on to use x-rays to create Drosophila mutants that he used in his studies of genetics. He also found that X-rays not only mutate genes in fruit flies, but also have effects on the genetic makeup of humans. Similar work by Lewis Stadler also showed the mutational effect of X-rays on barley in 1928, and ultraviolet (UV) radiation on maize in 1936. The effect of sunlight had previously been noted in the nineteenth century where rural outdoor workers and sailors were found to be more prone to skin cancer.

Chemical mutagens were not demonstrated to cause mutation until the 1940s, when Charlotte Auerbach and J. M. Robson found that mustard gas can cause mutations in fruit flies. A large number of chemical mutagens have since been identified, especially after the development of the Ames test in the 1970s by Bruce Ames that screens for mutagens and allows for preliminary identification of carcinogens. Early studies by Ames showed around 90% of known carcinogens can be identified in Ames test as mutagenic (later studies however gave lower figures), and ~80% of the mutagens identified through Ames test may also be carcinogens. Mutagens are not necessarily carcinogens, and vice versa. Sodium azide for example may be mutagenic (and highly toxic), but it has not been shown to be carcinogenic.

Mutagens can cause changes to the DNA and are therefore genotoxic. They can affect the transcription and replication of the DNA, which in severe cases can lead to cell death. The mutagen produces mutations in the DNA, and deleterious mutation can result in aberrant, impaired or loss of function for a particular gene, and accumulation of mutations may lead to cancer. Mutagens may therefore be also carcinogens. However, some mutagens exert their mutagenic effect through their metabolites, and therefore whether such mutagens actually become carcinogenic may be dependent on the metabolic processes of an organism, and a compound shown to be mutagenic in one organism may not necessarily be carcinogenic in another.

Different mutagens act on the DNA differently. Powerful mutagens may result in chromosomal instability, causing chromosomal breakages and rearrangement of the chromosomes such as translocation, deletion, and inversion. Such mutagens are called clastogens.

Mutagens may also modify the DNA sequence; the changes in nucleic acid sequences by mutations include substitution of nucleotide base-pairs and insertions and deletions of one or more nucleotides in DNA sequences. Although some of these mutations are lethal or cause serious disease, many have minor effects as they do not result in residue changes that have significant effect on the structure and function of the proteins. Many mutations are silent mutations, causing no visible effects at all, either because they occur in non-coding or non-functional sequences, or they do not change the amino-acid sequence due to the redundancy of codons.

Some mutagens can cause aneuploidy and change the number of chromosomes in the cell. They are known as aneuploidogens.

In Ames test, where the varying concentrations of the chemical are used in the test, the dose response curve obtained is nearly always linear, suggesting that there may be no threshold for mutagenesis. Similar results are also obtained in studies with radiations, indicating that there may be no safe threshold for mutagens. However, the no-threshold model is disputed with some arguing for a dose rate dependent threshold for mutagenesis. Some have proposed that low level of some mutagens may stimulate the DNA repair processes and therefore may not necessarily be harmful. More recent approaches with sensitive analytical methods have shown that there may be non-linear or bilinear dose-responses for genotoxic effects, and that the activation of DNA repair pathways can prevent the occurrence of mutation arising from a low dose of mutagen.

Mutagens may be of physical, chemical or biological origin. They may act directly on the DNA, causing direct damage to the DNA, and most often result in replication error. Some however may act on the replication mechanism and chromosomal partition. Many mutagens are not mutagenic by themselves, but can form mutagenic metabolites through cellular processes, for example through the activity of the cytochrome P450 system and other oxygenases such as cyclooxygenase. Such mutagens are called promutagens.


A large number of chemicals may interact directly with DNA. However, many such as PAHs, aromatic amines, benzene are not necessarily mutagenic by themselves, but through metabolic processes in cells they produce mutagenic compounds.




Many metals, such as arsenic, cadmium, chromium, nickel and their compounds may be mutagenic, but they may act, however, via a number of different mechanisms. Arsenic, chromium, iron, and nickel may be associated with the production of ROS, and some of these may also alter the fidelity of DNA replication. Nickel may also be linked to DNA hypermethylation and histone deacetylation, while some metals such as cobalt, arsenic, nickel and cadmium may also affect DNA repair processes such as DNA mismatch repair, and base and nucleotide excision repair.


Antioxidants are an important group of anticarcinogenic compounds that may help remove ROS or potentially harmful chemicals. These may be found naturally in fruits and vegetables. Examples of antioxidants are vitamin A and its carotenoid precursors, vitamin C, vitamin E, polyphenols, and various other compounds. β-Carotene is the red-orange colored compounds found in vegetables like carrots and tomatoes. Vitamin C may prevent some cancers by inhibiting the formation of mutagenic N-nitroso compounds (nitrosamine). Flavonoids, such as EGCG in green tea, have also been shown to be effective antioxidants and may have anti-cancer properties. Epidemiological studies indicate that a diet rich in fruits and vegetables is associated with lower incidence of some cancers and longer life expectancy, however, the effectiveness of antioxidant supplements in cancer prevention in general is still the subject of some debate.

Other chemicals may reduce mutagenesis or prevent cancer via other mechanisms, although for some the precise mechanism for their protective property may not be certain. Selenium, which is present as a micronutrient in vegetables, is a component of important antioxidant enzymes such as gluthathione peroxidase. Many phytonutrients may counter the effect of mutagens; for example, sulforaphane in vegetables such as broccoli has been shown to be protective against prostate cancer. Others that may be effective against cancer include indole-3-carbinol from cruciferous vegetables and resveratrol from red wine.

An effective precautionary measure an individual can undertake to protect themselves is by limiting exposure to mutagens such as UV radiations and tobacco smoke. In Australia, where people with pale skin are often exposed to strong sunlight, melanoma is the most common cancer diagnosed in people aged 15–44 years.

In 1981, human epidemiological analysis by Richard Doll and Richard Peto indicated that smoking caused 30% of cancers in the US. Diet is also thought to cause a significant number of cancer, and it has been estimated that around 32% of cancer deaths may be avoidable by modification to the diet. Mutagens identified in food include mycotoxins from food contaminated with fungal growths, such as aflatoxins which may be present in contaminated peanuts and corn; heterocyclic amines generated in meat when cooked at high temperature; PAHs in charred meat and smoked fish, as well as in oils, fats, bread, and cereal; and nitrosamines generated from nitrites used as food preservatives in cured meat such as bacon (ascobate, which is added to cured meat, however, reduces nitrosamine formation). Overly-browned starchy food such as bread, biscuits and potatoes can generate acrylamide, a chemical shown to cause cancer in animal studies. Excessive alcohol consumption has also been linked to cancer; the possible mechanisms for its carcinogenicity include formation of the possible mutagen acetaldehyde, and the induction of the cytochrome P450 system which is known to produce mutagenic compounds from promutagens.

For certain mutagens, such as dangerous chemicals and radioactive materials, as well as infectious agents known to cause cancer, government legislations and regulatory bodies are necessary for their control.

Many different systems for detecting mutagen have been developed. Animal systems may more accurately reflect the metabolism of human, however, they are expensive and time-consuming (may take around three years to complete), they are therefore not used as a first screen for mutagenicity or carcinogenicity.


Systems similar to Ames test have been developed in yeast. "Saccharomyces cerevisiae" is generally used. These systems can check for forward and reverse mutations, as well as recombinant events.

Sex-Linked Recessive Lethal Test – Males from a strain with yellow bodies are used in this test. The gene for the yellow body lies on the X-chromosome. The fruit flies are fed on a diet of test chemical, and progenies are separated by sex. The surviving males are crossed with the females of the same generation, and if no males with yellow bodies are detected in the second generation, it would indicate a lethal mutation on the X-chromosome has occurred.

Plants such as "Zea mays", "Arabidopsis thaliana" and "Tradescantia" have been used in various test assays for mutagenecity of chemicals.

Mammalian cell lines such as Chinese hamster V79 cells, Chinese hamster ovary (CHO) cells or mouse lymphoma cells may be used to test for mutagenesis. Such systems include the HPRT assay for resistance to 8-azaguanine or 6-thioguanine, and ouabain-resistance (OUA) assay.

Rat primary hepatocytes may also be used to measure DNA repair following DNA damage. Mutagens may stimulate unscheduled DNA synthesis that results in more stained nuclear material in cells following exposure to mutagens.

These systems check for large scale changes to the chromosomes and may be used with cell culture or in animal test. The chromosomes are stained and observed for any changes. Sister chromatid exchange is a symmetrical exchange of chromosome material between sister chromatids and may be correlated to the mutagenic or carcinogenic potential of a chemical. In micronucleus Test, cells are examined for micronuclei, which are fragments or chromosomes left behind at anaphase, and is therefore a test for clastogenic agents that cause chromosome breakages. Other tests may check for various chromosomal aberrations such as chromatid and chromosomal gaps and deletions, translocations, and ploidy.

Rodents are usually used in animal test. The chemicals under
test are usually administered in the food and in the drinking water, but sometimes by dermal application, by gavage, or by inhalation, and carried out over the major part of the life span for rodents. In tests that check for carcinogens, maximum tolerated dosage is first determined, then a range of doses are given to around 50 animals throughout the notional lifespan of the animal of two years. After death the animals are examined for sign of tumours. Differences in metabolism between rat and human however means that human may not respond in exactly the same way to mutagen, and dosages that produce tumours on the animal test may also be unreasonably high for a human, i.e. the equivalent amount required to produce tumours in human may far exceed what a person might encounter in real life.
Mice with recessive mutations for a visible phenotype may also be used to check for mutagens. Females with recessive mutation crossed with wild-type males would yield the same phenotype as the wild-type, and any observable change to the phenotype would indicate that a mutation induced by the mutagen has occurred.
Mice may also be used for dominant lethal assays where early embryonic deaths are monitored. Male mice are treated with chemicals under test, mated with females, and the females are then sacrificed before parturition and early fetal deaths are counted in the uterine horns.
Transgenic mouse assay using a mouse strain infected with a viral shuttle vector is another method for testing mutagens. Animals are first treated with suspected mutagen, the mouse DNA is then isolated and the phage segment recovered and used to infect "E. coli". Using similar method as the blue-white screen, the plaque formed with DNA containing mutation are white, while those without are blue.

Many mutagens are highly toxic to proliferating cells, and they are often used to destroy cancer cells. Alkylating agents such as cyclophosphamide and cisplatin, as well as intercalating agent such as daunorubicin and doxorubicin may be used in chemotherapy. However, due to their effect on other cells which are also rapidly dividing, they may have side effects such as hair loss and nausea. Research on better targeted therapies may reduce such side-effects. Ionizing radiations are used in radiation therapy.

In science fiction, mutagens are often represented as substances that are capable of completely changing the form of the recipient or gaining them superpower. Powerful radiations are the agents of mutation for the superheroes in Marvel Comics's Fantastic Four, Daredevil, and Hulk, while in the Teenage Mutant Ninja Turtles franchise the mutagen is chemical agent also called "ooze", and for Inhumans the mutagen is the Terrigen Mist. Mutagens are also featured in television series, computer and video games, such as the "Cyberia", "The Witcher", "", "", "Resident Evil", "Infamous", "Command & Conquer", "Gears of War 3", "BioShock", and "Fallout".



</doc>
<doc id="20360" url="https://en.wikipedia.org/wiki?curid=20360" title="Mychal Judge">
Mychal Judge

Mychal Judge, O.F.M. (aka Michael Fallon Judge, May 11, 1933 – September 11, 2001), was a Franciscan friar and Catholic priest who served as a chaplain to the New York City Fire Department. It was while serving in that capacity that he was killed, becoming the first certified fatality of the September 11, 2001 attacks.

Mychal Judge was born Robert Emmett Judge on May 11, 1933 in Brooklyn, New York, the son of immigrants from County Leitrim, Ireland, and the firstborn of a pair of fraternal twins. His twin sister Dympna was born two days later. Judge was baptized in St. Paul's Church in Brooklyn on June 4. They and their older sister Erin, grew up during the Great Depression.

From the ages of three to six, he watched his father suffer and die of mastoiditis, a slow and painful illness of the skull and inner ear. To earn income following his father's death, Judge shined shoes at New York Penn Station and would visit St. Francis of Assisi Church, located across the street. Seeing the Franciscan friars there, he later said, "I realized that I didn't care for material things... I knew then that I wanted to be a friar."

After spending his freshman year at the St. Francis Preparatory School in Brooklyn, where he studied under the Franciscan Brothers of Brooklyn, in 1948, at the age of 15, Judge began the formation process to enter the Order of Friars Minor. He transferred to St. Joseph's Seraphic Seminary in Callicoon, New York, the minor seminary of the Holy Name Province of the Order. After graduation, he enrolled at St. Bonaventure University in Olean, New York. In 1954 he was admitted to the novitiate of the Province in Paterson, New Jersey. After completing that year of formation, he received the religious habit and professed his first vows as a member of the Order. At that time, he was given the religious name of Fallon Michael. He later dropped 'Fallon' and changed 'Michael' to Mychal. According to "Queer There and Everywhere" by Sarah Prager, Mychal changed his name to "differentiate himself from all the other 'Father Michaels.'" He resumed his college studies at St. Bonaventure University, where he earned a bachelor's degree in 1957. He professed his solemn vows as a full member of the Order in 1958. Following this, he did his theological studies at Holy Name College Seminary in Washington, D.C.. Upon completing these studies in 1961, he was ordained a priest.

After his ordination, Judge was assigned to the Shrine of St. Anthony in Boston, Massachusetts. Following his assignment there, he served in various parishes served by the Franciscans: St. Joseph Parish in East Rutherford, New Jersey, Sacred Heart Parish in Rochelle Park, New Jersey, Holy Cross Parish in the Bronx and St. Joseph Parish in West Milford, New Jersey. For three years he served as assistant to the President of Siena College, operated by the Franciscans in Loudonville, New York. In 1986 he was assigned to St. Francis of Assisi Church in Manhattan, where he had first come to know the friars. He lived and worked there until his death.

Around 1971, Judge developed alcoholism, although he never showed obvious signs. In 1978, with the support of Alcoholics Anonymous, he became sober and continued to share his personal story of alcoholism to help others facing addiction.

In 1992, Judge was appointed a chaplain to the New York City Fire Department. As chaplain, he offered encouragement and prayers at fires, rescues, and hospitals, and counseled firemen and their families, often working 16-hour days. "His whole ministry was about love. Mychal loved the fire department and they loved him." He was a member of AFSCME Local 299 (District Council 37).

Judge was also well known in the city for ministering to the homeless, the hungry, recovering alcoholics, people with AIDS, the sick, injured, and grieving, immigrants, gays and lesbians, and those alienated by the Church and society. Judge once gave the winter coat off his back to a homeless woman in the street, later saying, "She needed it more than me." When he anointed a man who was dying of AIDS, the man asked him, "Do you think God hates me?" Judge picked him up, kissed him, and silently rocked him in his arms. Judge worked with St. Clare's hospital, which opened the city's first AIDS ward, in order to start an active AIDS ministry. He visited hospitals and AIDS patients and their families, presided over many funerals, and counseled other prominent gay Catholics like Brendan Fay and Fr. John McNeill. Judge continued to be an advocate for gay rights throughout the rest of his life, marching in pride parades and attending other gay events.

Even before his death, many considered Judge to be a living saint for his extraordinary works of charity and his deep spirituality. While praying, he would sometimes "become so lost in God, as if lost in a trance, that he'd be shocked to find several hours had passed." Judge's former spiritual director, former Jesuit John J. McNeill, observed that Judge achieved an "extraordinary degree of union with the divine. We knew we were dealing with someone directly in line with God."

On September 11, 2001, upon learning that the World Trade Center had been hit by the first of two jetliners, Judge rushed to the site. He was met by Rudolph Giuliani, the Mayor of New York City, who asked him to pray for the city and its victims. Judge prayed over some bodies lying on the streets, then entered the lobby of the World Trade Center North Tower, where an emergency command post had been organized. There he continued offering aid and prayers for the rescuers, the injured, and the dead.

When the South Tower collapsed at 9:59 am, debris went flying through the North Tower lobby, killing many inside, including Judge. At the moment he was struck in the head and killed, Judge was repeatedly praying aloud, "Jesus, please end this right now! God, please end this!", according to Judge's biographer and "New York Daily News" columnist Michael Daly.

Shortly after his death, an NYPD lieutenant found Judge's body. He and two firemen, a FDNY emergency medical technician detailed to the NYC Office of Emergency Management (OEM), and one civilian bystander then carried Judge's body out of the North Tower. This event was captured in the documentary film "9/11", shot by Jules and Gedeon Naudet. Shannon Stapleton, a photographer from Reuters, photographed Judge's body being carried out of the rubble by the five men. It became one of the most famous images related to 9/11. The "Philadelphia Weekly" reported that the photograph is "considered an American "Pietà"." Judge's body was laid before the altar of St. Peter’s Catholic Church before being taken to the medical examiner. 

Mychal Judge was designated as "Victim 0001" and thereby recognized as the first official victim of the attacks. Although others had been killed before him, including the crews, passengers, and hijackers of the first three planes, and occupants of the towers and the Pentagon, Judge was the first certified fatality because he was the first body to be recovered and taken to the medical examiner.

Judge's body was formally identified by NYPD Detective Steven McDonald, a long-time friend. The New York Medical Examiner found that Judge died of "blunt force trauma to the head".

3,000 people attended Judge's funeral Mass on September 15, 2001, at St. Francis of Assisi Church, which was presided over by Cardinal Edward Egan, the then Archbishop of New York. Former President Bill Clinton with wife Senator Hillary Rodham Clinton also attended. The former President Clinton said that Judge's death was a "special loss. We should lift his life up as an example of what has to prevail. We have to be more like Father Mike than the people who killed him."

Judge was buried in the friars' plot at Holy Sepulchre Cemetery in Totowa, New Jersey. On October 11, 2001 Brendan Fay organized a "Month's Mind Memorial" in Good Shepherd Chapel, General Theological Seminary, New York. It was an evening of prayer, stories, traditional Irish music, and personal testimonials about Judge.

Three people in the Roman Catholic Church called to canonize Judge. The Orthodox-Catholic Church of America declared him a saint. Two people claim miraculous healings through prayers to Judge. Evidence of miracles is required for canonization in the Catholic Church.
Judge's fire helmet was presented to Pope John Paul II. France awarded him the Légion d'honneur. Some members of the United States Congress have nominated him for the Congressional Gold Medal. as well as the Presidential Medal of Freedom. In 2002, the City of New York renamed the portion of West 31st Street on which the friary where he lived is located as "Father Mychal F. Judge Street", and christened a commuter ferry, the "Father Mychal Judge" in his honor in 2002.

In 2002, the United States Congress passed "The Mychal Judge Police and Fire Chaplains Public Safety Officers Benefit Act" into law. The law extended federal death benefits to chaplains of police and fire departments, and also marked the first time the federal government extended equal benefits for same-sex couples by allowing the domestic partners of public safety officers killed in the line of duty to collect a federal death benefit. This act was signed into law on June 24, 2002, but would be retroactive only to September 11, 2001.

The New York Press Club instituted The Rev. Mychal Judge Heart of New York Award, which is presented annually for the news story or series that is most complimentary of New York City.

A campaign has been started in East Rutherford, New Jersey to have a statue of Judge erected in its Memorial Park.

Alvernia University, a private independent college in the Franciscan tradition in Reading, Pennsylvania, named a new residence hall in honor of Judge.

The Father Mychal Judge Memorial in the village of Keshcarrigan, County Leitrim, Ireland was dedicated in 2005, on donated land which had belonged to Judge's ancestors. People from the village and surrounding area celebrate his life every year on the 9/11 anniversary.

In 2006 a documentary film, "Saint of 9/11", directed by Glenn Holsten, co-produced by Brendan Fay and narrated by Sir Ian McKellen, was released.

Larry Kirwan, leader of the Irish-American band Black 47, wrote a tribute song entitled "Mychal" in honor of Judge that appeared on the band's 2004 album "New York Town".

The Father Mychal Judge Walk of Remembrance takes place every year in New York on the Sunday before the 9/11 anniversary. It begins with a Mass at St. Francis Church on West 31st Street, then proceeds to the site of Ground Zero, retracing Judge's final journey and praying along the way. Every September 11, there is a Mass in memory of Judge in Boston, attended by many who lost family members on 9/11.

At the National 9/11 Memorial, Judge is memorialized at the South Pool, on Panel S-18, where other first responders are located.

In 2014 Judge was inducted into the Legacy Walk, an outdoor public display which celebrates LGBT history and people. 

In 2015 a statue was dedicated to Judge at St. Joseph's Park in East Rutherford, New Jersey, across the street from St. Joseph's Parish where he served for several years.

In recognition of his heroic actions and his commitment to the dignity of LGBTQ people, Judge was posthumously awarded the Dooley Award by GALA-ND/SMC, an alumni organization of the University of Notre Dame, a prominent American Catholic university.

Following his death, a few of Judge's friends and associates revealed that Judge was gay – as a matter of orientation rather than practice, as he was a celibate priest. According to Fire Department Commissioner Thomas Von Essen: "I actually knew about his homosexuality when I was in the Uniformed Firefighters Association. I kept the secret, but then he told me when I became commissioner five years ago. He and I often laughed about it, because we knew how difficult it would have been for the other firemen to accept it as easily as I had. I just thought he was a phenomenal, warm, sincere man, and the fact that he was gay just had nothing to do with anything."

Judge developed a romantic relationship with a Filipino nurse named Al Alvarado in the last year of his life, which Judge documented in his diaries. The two often did not see each other for months because of Judge's work as a firefighter.

The revelations about sexual orientation were not without controversy, however. Dennis Lynch, a lawyer, wrote an article about Judge that appeared on the website catholic.org. Lynch claimed that Judge was not gay and that any attempt to define him as gay was due to "homosexual activists" who wanted to "attack the Catholic Church" and turn the priest into a "homosexual icon". Others refuted Lynch’s claims with evidence that Judge did in fact identify himself as gay, both to others and in his personal journals.

Judge was a long-term member of Dignity, a Catholic LGBT activist organization that advocates for change in the Catholic Church's teaching on homosexuality. On October 1, 1986, the Vatican's Congregation for the Doctrine of the Faith issued an encyclical, "On the Pastoral Care of Homosexual Persons", which declared homosexuality to be a "strong tendency ordered toward an intrinsic moral evil". In response, many bishops, including John Cardinal O'Connor, banned Dignity from diocesan churches under their control. Judge then welcomed Dignity's AIDS ministry to the Church of Saint Francis of Assisi, which is under the control of the Franciscan friars, thereby partially circumventing the cardinal's ban of Dignity.

Judge disagreed with official Roman Catholic teaching regarding homosexuality, though by all accounts he remained celibate. Judge often asked, "Is there so much love in the world that we can afford to discriminate against any kind of love?"




</doc>
<doc id="20361" url="https://en.wikipedia.org/wiki?curid=20361" title="Moonfleet">
Moonfleet

Moonfleet is a tale of smuggling, royal treasure and shipwreck by the English novelist J. Meade Falkner, first published in 1898. The book was extremely popular among children worldwide up until the 1970s, mostly for its themes of adventure and gripping storyline. It remains a popular story widely read and is still sometimes studied in schools.

In 1757, Moonfleet is a small village near the sea in the south of England. It gets its name from a formerly prominent local family, the Mohunes, whose coat of arms includes a symbol shaped like a capital 'Y'. John Trenchard is an orphan who lives with his aunt, Miss Arnold. Other notable residents are the sexton Mr Ratsey, who is friendly to John; Parson Glennie, the local clergyman who also teaches in the village school; Elzevir Block, the landlord of the local inn, called the "Mohune Arms" but nicknamed the "Why Not?" because of its sign with the Mohune 'Y'; and Mr Maskew, the unpopular local magistrate and his beautiful daughter, Grace.

Village legend tells of the notorious Colonel John "Blackbeard" Mohune who is buried in the family crypt under the church. He is reputed to have stolen a diamond from King Charles I and hidden it. His ghost is said to wander at night looking for it and the mysterious lights in the churchyard are attributed to his activities.

As the main part of the story opens, Block's youthful son, David, has just been killed by Maskew during a raid by the Maskew and other authorities on a smuggling boat. One night a bad storm hits the village and there is a flood. While attending the Sunday service at church, John hears strange sounds from the crypt below. He thinks it is the sound of the coffins of the Mohune family. The next day, he finds Elzevir and Ratsey against the south wall of the church. They claim to be checking for damage from the storm, but John suspects they are searching for Blackbeard's ghost.

Later John finds a large sinkhole has opened in the ground by a grave. He follows the passage and finds himself in the crypt with coffins on shelves and casks on the floor. He realises his friends are smugglers and this is their hiding place. He has to hide behind a coffin when he hears Ratsey and Elzevir coming. When they leave, they fill in the hole, inadvertently trapping him. John finds a locket in the coffin that he hid behind (it turns out to be that of Blackbeard himself),which holds a piece of paper with verses from the Bible. John eventually passes out after drinking too much of the wine while trying to quench his thirst, having not eaten or drunk for over 24 hours. Later he wakes up in the "Why Not?" inn - he has been rescued by Elzevir and Ratsey. When he is better, he returns to his aunt's house, but she, suspecting him of drunken behaviour, throws him out. Fortunately, Elzevir takes him in.

But when Block's lease on the "Why Not?" comes up for renewal, Maskew bids against him in the auction and wins. Block must leave the inn and Moonfleet but plans one last smuggling venture. John feels honour-bound to go with him, and sadly, says goodbye to Grace Maskew, whom he loves and has been seeing in secret, and is given his mother's prayer book by his aunt—her last hope to influence John towards piety. The excisemen and Maskew are aware of the planned smuggling run but do not know exactly where it will occur. During the landing Maskew appears and is caught by the smugglers. Elzevir is bent on vengeance for his son by killing Maskew, and while the rest land the cargo and leave, he and John keep watch over Maskew. Just as Block prepares to shoot Maskew the excisemen attack. They wound John and unintentionally kill Maskew. Block carries John away to safety and they hide in some old quarries. While there, John inadvertently finds out that the verses from Blackbeard's locket contain a code that will reveal the location of his famous diamond.

Once John's wound heals, he and Block decide to recover the diamond from Carisbrooke Castle. After a suspenseful scene in the well where the jewel is hidden, they succeed in escaping to Holland where they try to sell it to a Jewish diamond merchant named Krispijn Aldobrand. The merchant cheats them, claiming the diamond is fake. Elzevir falls for the deceit and angrily throws the diamond out of the window. John, however, knows they have been duped, and suggests they try to recover the diamond through burglary. The attempt fails and, they are arrested and sentenced to prison. John curses the merchant for his lies.

John and Elzevir go to prison for life. Eventually they are separated. Then, unexpectedly, ten years later, their paths cross again. They are being transported, and board a ship. A storm blows up, and by a strong coincidence, the ship is wrecked upon Moonfleet beach. While trying to reach the beach Elzevir helps John to safety, but is himself dragged under by the surf and drowned.

John arrives where he originally started, in the "Why Not?", and is reunited with Ratsey. He is also reunited with Grace. She is now a rich young lady, having inherited her father's money. However, she is still in love with John. John tells her about the diamond and his life in prison. He regrets having lost everything, but she says, rich or not, she loves him.

Then Parson Glennie visits and reveals he had received a letter from Aldobrand. The merchant, suffering a guilty conscience and in an attempt to make amends, had bequeathed the worth of the diamond to John.

John gives the money to the village, and new almshouses are built, and the school and the church renovated. John marries Grace and becomes Lord of the Manor and Justice of the Peace. They have three children, including their first-born son, Elzevir. They grow up and the sons go away to "serve King George on sea and land" and their daughter, too it seems, has married away. But John and Grace themselves do not leave their beloved Moonfleet ever again.

A feature of the narrative is a continuing reference to the boardgame of backgammon which is played by the patrons of the "Why Not?" on an antique board which bears a Latin inscription "Ita in vita ut in lusu alae pessima jactura arte corrigenda est" (translated in the book as "As in life, so in a game of hazard, skill will make something of the worst of throws").

Falkner uses the local geography of Dorset and the Isle of Wight in the book, only changing some of the place names. The village of "Moonfleet" is based on East Fleet in Dorset by Chesil Beach. The headland in the book called "The Snout" is Portland Bill. The castle is Carisbrooke Castle on the Isle of Wight.

The book was filmed by Fritz Lang in 1955 and released under the same name, with a screenplay adapted by from the novel. A handful of scenes from the book survived, including John's ordeal in the church crypt with the remains of Blackbeard (here renamed Redbeard), and his descent into the well to retrieve the diamond, but the movie altered the novel's plot substantially. Among other major changes, its young hero was given the newly invented rogue gentleman Jeremy Fox for a mentor (played by Stewart Granger), while the role of the working class Elzevir Block was reduced to leading a group of the smugglers seeking to kill John. Lang's film has enjoyed some cachet among French film critics.

In 1963 the BBC aired a 5-episode radio series of Moonfleet adapted by Morna Stuart and produced by Brandon Acton-Bond. 

In 1964 the BBC filmed a 6-episode TV adaptation under the title "Smugglers' Bay", starring future Doctor Who stars Frazer Hines and Patrick Troughton as John Trenchard and Ratsey, respectively. It started airing on BBC1 on 12 July 1964.

In 1984 a TV mini-series was filmed, starring Adam Godley and David Daker. There is also a 90-minute BBC radio version, starring Richard Pearce (from BBC Radio's "The Adventures of Tintin", as well) as John Trenchard.

The Colonial Radio Theatre on the Air released a 300 min. production of the book in May 2009, Starring Jerry Robbins, David Ault, and Rob Cattell. It was dramatised by Deniz Cordell, and produced by M. J. Cogburn.

Angel Exit Theatre Company devised a production which toured the UK in 2009.

In 2010 Chris de Burgh released an album of songs called "Moonfleet & Other Stories" featuring a story based on the book.

Sky1 filmed a two-part TV adaptation in Ireland in 2013 starring Ray Winstone, Aneurin Barnard and Karl McCrone. This was aired 28 December 2013 and 29 December 2013 Though more of the plot of the book remains than in the 1955 film, it still bears little relation; John is in his mid-twenties, Maskew has become an aristocrat and tyrannical descendant of Blackbeard, and Elzevir Block the leader of a brothel-frequenting, knife-fighting band of gangsters.

In 2017 the Salisbury Playhouse announced a major new Musical adaptation of Moonfleet with book and lyrics by Gareth Machin and music by Russell Hepplewhite. The show is scheduled to open in the Main House in April 2018. http://www.salisburyplayhouse.com/whats-on/main-house/moonfleet/



</doc>
<doc id="20362" url="https://en.wikipedia.org/wiki?curid=20362" title="Merge algorithm">
Merge algorithm

Merge algorithms are a family of algorithms that take multiple sorted lists as input and produce a single list as output, containing all the elements of the inputs lists in sorted order. These algorithms are used as subroutines in various sorting algorithms, most famously merge sort.

The merge algorithm plays a critical role in the merge sort algorithm, a comparison-based sorting algorithm. Conceptually, merge sort algorithm consists of two steps:


The merge algorithm is used repeatedly in the merge sort algorithm.

An example merge sort is given in the illustration. It starts with an unsorted array of 7 integers. The array is divided into 7 partitions; each partition contains 1 element and is sorted. The sorted partitions are then merged to produce larger, sorted, partitions, until 1 partition, the sorted array, is left.

Merging two sorted lists into one can be done in linear time and linear space. The following pseudocode demonstrates an algorithm that merges input lists (either linked lists or arrays) and into a new list . The function yields the first element of a list; "dropping" an element means removing it from its list, typically by incrementing a pointer or index.

When the inputs are linked lists, this algorithm can be implemented to use only a constant amount of working space; the pointers in the lists' nodes can be reused for bookkeeping and for constructing the final merged list.

In the merge sort algorithm, this subroutine is typically used to merge two sub-arrays , of a single array . This can be done by copying the sub-arrays into a temporary array, then applying the merge algorithm above. The allocation of a temporary array can be avoided, but at the expense of speed and programming ease. Various in-place merge algorithms have been devised, sometimes sacrificing the linear-time bound to produce an algorithm; see for discussion.

-way merging generalizes binary merging to an arbitrary number of sorted input lists. Applications of -way merging arise in various sorting algorithms, including patience sorting and an external sorting algorithm that divides its input into blocks that fit in memory, sorts these one by one, then merges these blocks.

Several solutions to this problem exist. A naive solution is to do a loop over the lists to pick off the minimum element each time, and repeat this loop until all lists are empty:

In the worst case, this algorithm performs element comparisons to perform its work if there are a total of elements in the lists.
It can be improved by storing the lists in a priority queue (min-heap) keyed by their first element:

Searching for the next smallest element to be output (find-min) and restoring heap order can now be done in time (more specifically, comparisons), and the full problem can be solved in time (approximately comparisons).

A third algorithm for the problem is a divide and conquer solution that builds on the binary merge algorithm:

When the input lists to this algorithm are ordered by length, shortest first, it requires fewer than comparisons, i.e., less than half the number used by the heap-based algorithm; in practice, it may be about as fast or slow as the heap-based algorithm.

A parallel version of the binary merge algorithm can serve as a building block of a parallel merge sort. The following pseudocode demonstrates this algorithm in a parallel divide-and-conquer style (adapted from Cormen "et al."). It operates on two sorted arrays and and writes the sorted output to array . The notation denotes the part of from index through , exclusive.

The algorithm operates by splitting either or , whichever is larger, into (nearly) equal halves. It then splits the other array into a part that is smaller than the midpoint of the first, and a part that is larger. (The binary search subroutine returns the index in where would be, if it were in ; that this always a number between and .) Finally, each pair of halves is merged recursively, and since the recursive calls are independent of each other, they can be done in parallel. Hybrid approach, where serial algorithm is used for recursion base case has been shown to perform well in practice 

The work performed by the algorithm for two arrays holding a total of elements, i.e., the running time of a serial version of it, is . This is optimal since elements need to be copied into . Its critical path length, however, is , meaning that it takes that much time on an ideal machine with an unbounded number of processors.

Some computer languages provide built-in or library support for merging sorted collections.

The C++'s Standard Template Library has the function , which merges two sorted ranges of iterators, and , which merges two consecutive sorted ranges "in-place". In addition, the (linked list) class has its own method which merges another list into itself. The type of the elements merged must support the less-than () operator, or it must be provided with a custom comparator.

C++17 allows for differing execution policies, namely sequential, parallel, and parallel-unsequenced.

Python's standard library (since 2.6) also has a function in the module, that takes multiple sorted iterables, and merges them into a single iterator.





</doc>
<doc id="20363" url="https://en.wikipedia.org/wiki?curid=20363" title="ML">
ML

ML or ml may refer to:











</doc>
<doc id="20366" url="https://en.wikipedia.org/wiki?curid=20366" title="Cuisine of the Midwestern United States">
Cuisine of the Midwestern United States

Midwestern cuisine is a regional cuisine of the American Midwest. It draws its culinary roots most significantly from the cuisines of Central, Northern and Eastern Europe, and Native North America, and is influenced by regionally and locally grown foodstuffs and cultural diversity.

Everyday Midwestern home cooking generally showcases simple and hearty dishes that make use of the abundance of locally grown foods. Its culinary profiles may seem synonymous with "American food." Quoted in a 2007 interview with the "Daily Herald", Chef Stephen Langlois, a pioneer in the Midwestern local food movement, described it: "Think of Thanksgiving dinner. Turkey and cranberry sauce and wild rice and apple pie."

The Midwest's restaurants also offer a diverse mix of ethnic cuisines as well as sophisticated, contemporary techniques.

Sometimes called "the breadbasket of America," the Midwest serves as a center for grain production, particularly wheat, corn and soybeans. Midwestern states also produce most of the country's wild rice.

Beef and pork processing always have been important Midwestern industries, with a strong role in regional diets. Chicago and Kansas City were historically stockyard and processing centers of the beef trade and Cincinnati, nicknamed 'Porkopolis', was once the largest pork-producing city in the world. Iowa is the center of pork production in the U.S.

Far from the oceans, Midwesterners traditionally ate little seafood, relying on local freshwater fish, such as perch and trout, supplemented by canned tuna and canned or cured salmon and herring, although modern air shipping of ocean seafood has been increasing Midwesterners' taste for ocean fish.

Dairy products, especially cheese, form an important group of regional ingredients, with Wisconsin known as "America's Dairyland."

The upper Midwest, a prime fruit-growing region, sees the extensive use of apples, blueberries, cranberries, cherries, peaches and other cold-climate fruit in its cuisine.

As with many American regional cuisines, Midwestern cooking has been heavily influenced by immigrant groups. Throughout the northern Midwest, northern European immigrant groups predominated, so Swedish pancakes and Polish pierogi are common. Wisconsin, Missouri, Kansas, Ohio and Illinois were destinations for many ethnic German immigrants, so pork sausages and potatoes are prevalent. In the Rust Belt, many Greeks became restaurateurs, imparting a Mediterranean influence. Native American influences show up in the uses of corn and wild rice.

Traditionally, Midwestern cooks used a light hand with seasonings, preferring sage, dill, caraway, mustard and parsley to hot, bold and spicy flavors. However, with new waves of immigrants from Latin America and Asia moving into the region, these tastes are changing.

This section of the region is also headquarters for several seminal hamburger chains, including McDonald's in Oak Brook, Illinois (founded in California, but turned into the iconic franchise by Ray Kroc beginning with a still-standing store in Des Plaines, Illinois). The Midwest is also home to Hardee's in St. Louis, Missouri, Culver's in Sauk City, Wisconsin; Steak n Shake, founded in Normal, Illinois, and now based in Indianapolis; Wendy's in Dublin, Ohio. Diner chain Big Boy, known for burgers, is headquartered in Warren, Michigan. Both Pizza Hut (now based in Plano, Texas) and White Castle (based in Columbus, Ohio since 1933) were founded in Wichita, Kansas.

Besides Pizza Hut, the Midwest is home to the other largest pizza chains in the U.S.: Little Caesars and Domino's Pizza, both based in Michigan (respectively in Detroit and Ann Arbor), and Papa John's Pizza, founded in Indiana (though its current headquarters are just outside the federally defined Midwest in Louisville, Kentucky, across the Ohio River from the chain's original home of Jeffersonville).
A Wurst mart, sometimes spelled Wurstmart or Wurst Markt, is a variation on a fish fry found predominantly in German-American communities. Wurst marts are usually held by churches as fundraising events, where people will pay for a buffet of sausages and other side dishes. Common side dishes include mashed potatoes, gravy and sauerkraut. Wurst Mart comes from the German word "Wurstmarkt", meaning sausage market. Wurst marts are found mostly in small rural German-American communities in the Midwest, particularly around St. Louis.

Major urban areas in the Midwest feature distinctive cuisines very different from those of the region's rural areas, and some larger cities have world-class restaurants.

Part of the greater Akron area, this small industrial city with a strong Central and Eastern European heritage has a culinary contribution called Barberton Chicken, created by Serbian immigrants, deep fried in lard, and usually accompanied by a hot rice dish, vinegar coleslaw and french fries.

The ethnic mix of the people of Chicago has led to a distinctive cuisine of restaurant foods exclusive to the area, such as Italian beef, the Maxwell Street Polish, the Chicago-style hot dog, Chicago-style pizza, chicken Vesuvio and the jibarito, as well as a large number of steakhouses.

Chicago also boasts many gourmet restaurants, as well as a wide variety of ethnic food stores and eateries, most notably Mexican, Polish, Italian, Greek, Indian/Pakistani and Asian, often clustered in ethnic neighborhoods. Many of these cuisines have evolved significantly in Chicago. For example, the Greek cheese dish saganaki was first flambéed at the table in Greektown.

Chicago is the Midwest's center of molecular gastronomy, likely due to the influence of Grant Achatz.

As a major rail hub, Chicago historically had access to a broad range of the country's foodstuffs, so even in the 19th century, Chicagoans could easily buy items like live oysters and reasonably fresh shrimp. Chicago's oldest signature dish, shrimp de Jonghe, was invented around the turn of the 20th century. Today, flights into O'Hare Airport bring Chicago fresh food from all over the world.

The Queen City is known for its namesake "Cincinnati chili", a Greek-inspired meat sauce (ground beef seasoned with cinnamon, nutmeg, allspice, cloves, bay leaf, cumin, chili powder, and unsweetened dark chocolate), served over spaghetti or hot dogs. Unlike chili con carne, Cincinnati-style chili is almost never eaten by itself and is instead consumed in "ways" or on cheese coneys, which are a regional variation on a chili dog.

Goetta, a meat-and-grain sausage or mush made from pork and oats, is unique to the Greater Cincinnati area and "every bit as much a Queen City icon" as Cincinnati chili. It was inspired by the traditional porridge-like German peasant food stripgrutze but incorporates a higher proportion of meat-to-grain and is thicker, forming a sliceable loaf. Slices are typically fried like sausage patties and served for breakfast. More than a million pounds of goetta are served in the Cincinnati area per year.

The city has a strong German heritage and a variety of German-oriented restaurants and menu items can be found in the area. Cincinnati's Oktoberfest Zinzinnati, an annual food and music celebration held each September, is the second-largest in the world. Taste of Cincinnati, the longest running culinary arts festival in the United States, is held each year on Memorial Day weekend. In 2014, local chefs and food writers organized the first annual Cincinnati Food & Wine Classic, which drew chefs and artisan food producers from the region.

The area was once a national center for pork processing and is often nicknamed Porkopolis, with many references to that heritage in menu-item names and food-event names; pigs are a "well-loved symbol of the city."

Cleveland's many immigrant groups and heavily blue-collar demographic have long played an important role in defining the area's cuisine. Ethnically, Italian foods as well as several Eastern European cuisines, particularly those of Poland and Hungary, have become gastronomical staples in the Greater Cleveland area. Prominent examples of these include cavatelli, rigatoni, pizza, Chicken paprikash, stuffed cabbage, pierogi, and kielbasa all of which are widely popular in and around the city. Local specialties, such as the pork-based dish City Chicken and the Polish Boy (a loaded sausage sandwich native to Cleveland), are dishes definitive of a cuisine that is based on hearty, inexpensive fare. Commercially, Hector Boiardi (aka Chef Boyardee) started his business in Cleveland's Little Italy.

Sweets specific to the Cleveland area include the coconut bar (similar in many respects to the Australian Lamington). Coconut bars, which are found in many Jewish bakeries in the area, are small squares of cake that have been dipped in chocolate and rolled in coconut. In Italian bakeries around the Cleveland area, a variation of the Cassata cake is widely popular. This local version is unlike those typically found elsewhere being that it is made with layers of sponge cake custard and strawberries, then frosted with whipped cream. In a celebrity-chef nod to this version, Mario Batali as 'the best Cassata cake in the USA.'

The Columbus, Ohio area is the home and birthplace of many well-known fast food chains, especially those known for hamburgers. Wendy's opened its first store in Columbus in 1969, and is now headquartered in nearby Dublin. America's oldest hamburger chain, White Castle, is based there. Besides burgers, Columbus is noted for the German Village, a neighborhood south of downtown where German cuisine such as sausages and kuchen are served. In recent years, local restaurants focused on organic, seasonal, and locally or regionally sourced food have become more prevalent, especially in the Short North area, between downtown and the OSU campus. Numerous Somali restaurants are also found in the city, particularly around Cleveland Avenue.

Columbus is also the birthplace of the famed Marzetti Italian Restaurant, opened in 1896. Owner Teresa Marzetti is credited with creation of the beef-and-pasta casserole named after her brother-in-law, Johnny Marzetti. The restaurant's popular salad dressings became the foundation for the T. Marzetti Company, an international specialty foods manufacturer and distributor, headquartered in Columbus.

Detroit specialties include Coney Island hot dogs, found at hundreds of unaffiliated "Coney Island" restaurants. Not to be confused with a chili dog, a coney is served with a ground beef sauce, chopped onions and mustard. The Coney Special has an additional ground beef topping. It is often served with French fries. Food writers Jane and Michael Stern call out Detroit as the only "place to start" in pinpointing "the top Coney Islands in the land."

Detroit also has its own style of pizza, a thick-crusted, Sicilian cuisine-influenced, rectangular type called square pizza. Other Detroit foods include zip sauce, served on steaks; the triple-decker Dinty Moore sandwich, corned beef layered with lettuce, tomato and Russian dressing; and a Chinese-American dish called "warr shu gai" or almond boneless chicken.

The Detroit area has many large groups of immigrants. A large Arabic-speaking population reside in and around the suburb of Dearborn, home to many Lebanese storefronts. Detroit also has a substantial number of Greek restaurateurs. Thus, numerous Mediterranean restaurants dot the region and typical foods such as gyros, hummus and falafel can be found in many run-of-the-mill grocery stores and restaurants.

Polish food is also prominent in the region, including popular dishes such as pierogi, borscht, and pączki. Bakeries concentrated in the Polish enclave of Hamtramck, Michigan, within the city, are celebrated for their "pączki," especially on Fat Tuesday. Hungarian food is featured in nearby eastern Toledo, Ohio with Packo's hotda.

Chinese restaurants in the Detroit area serve Almond boneless chicken, a regional Chinese-American dish consisting of battered fried boneless chicken breasts served sliced on a bed of lettuce with a gravy-like chicken flavored sauce and slivered almonds.

In nearby Ann Arbor the Chipati, a tossed salad, is served inside a freshly baked pita pocket with the "secret" Chipati sauce on the side. The Chipati's origination is claimed by both Pizza Bob's on S. State St. and by Pizza House on Church St.

Indianapolis was settled predominantly by Americans of British descent and Irish and German immigrants, so much of the city's food draws upon these influences. Much of the food is considered to be "Classic American Cuisine". Later immigrants included many Jews, Poles, Eastern Europeans and Italians, all of whom influenced local food. Two of the city's most distinct dishes are the pork tenderloin sandwich and sugar cream pie.

A fast-growing immigrant population from places such as Mexico and India is also beginning to influence the local food. The area offers many diverse, locally owned ethnic restaurants, as well as nationally and internationally renowned restaurants. Indy is also home to many local pubs.

Kansas City is an important barbecue and meat-processing center with a distinctive barbecue style. The Kansas City metropolitan area has more than 100 barbecue restaurants and proclaims itself to be the "world's barbecue capital." The Kansas City Barbeque Society spreads its influence across the nation through its barbecue-contest standards. The oldest continuously operating barbecue restaurant is Rosedale Barbecue near downtown Kansas City. Other popular barbecue restaurants are Gates Bar-B-Q, Joe’s Kansas City Bar-B-Que and Arthur Bryant's. Both Arthur Bryant's and Gates Bar-B-Q sell bottled versions of their barbecue sauces in restaurants and specialty stores in the surrounding areas.

The capital and second-largest city in the state of Wisconsin, Madison has a diverse and cosmopolitan food scene. One representative is Dane County Farmers' Market, the largest producers-only farmer's market in the nation.

Mansfield is the home of two well-known food companies. Isaly Dairy Company (AKA Isaly's) was a chain of family-owned dairies and restaurants started by William Isaly in the early 1900s until the 1970s, famous for creating the Klondike Bar ice cream treat, popularized by the slogan "What would you do for a Klondike Bar?". Stewart's Restaurants is a chain of root beer stands started in Mansfield by Frank Stewart in 1924, famous for their Stewart's Fountain Classics line of premium beverages now sold worldwide.

German immigrants settled Milwaukee. Sauerkraut, bratwurst, beer, and other traditional German favorites continue to be popular, both in homes and at Milwaukee's famous German restaurants. Milwaukee also offers a diverse selection of other ethnic restaurants.

Served under various names, a favorite sandwich for Milwaukeeans and Wisconsinites consists of a brat (often butterflied to lay flat) on top of a hamburger in a kaiser roll.

Frozen custard is a local favorite in the Cream City, with many competing stands throughout the area.

Cheese curds are another local favorite, and Wisconsinites enjoy them "squeaky" (cold), or fried (usually in batter).

Also known as Brew City, Milwaukee is home to many breweries and the traditional and nominal headquarters for national beer brands.

Minneapolis and Saint Paul offer a diverse array of cuisines influenced by their many immigrant groups, as well as those restaurant chefs who follow the trends of larger cities. Within Minnesota, at-home fare varies broadly within various ethnic groups and their culture, historically, the overall majority of Minnesotans were of Northern European ancestry, many with farming backgrounds and many home-cooked meals still reflect this, with comfort food items such as hotdish, hearty soups and stews and meat and potatoes commonly being served. Many Minnesotans claim some Scandinavian heritage, and while iconic dishes such as lefse and lutefisk are quite commonly served at home as well as church potlucks and community get-togethers, few Twin Cities restaurants serve these items. Traditionally wild rice has been popular in Minnesota, which has been gathered in area lakes by Native Americans for centuries; In the fall, the Twin Cities share along with Green Bay, Wisconsin, the tradition of the neighborhood booyah, a cuisine and cultural event featuring a hodge-podge of ingredients in stews. One item of note: Minneapolis and Saint Paul pioneered the Jucy Lucy (or "Juicy Lucy"), a hamburger with a core of melted cheese.

American restaurants in the Twin Cities supply a wide spectrum of choices and styles that range from small diners offering simple short order grill fare and the typical sports bars and decades old supper clubs to high-end steakhouses and eateries that serve new American cuisine using locally grown ingredients. Most types of American regional cuisine can be found at restaurants in the Twin Cities. Barbecue restaurants in the area tend to feature a combination of the various regional styles of this type of cooking. In the inner cities of Minneapolis and Saint Paul, it's not uncommon to find a few Chicago-influenced African American barbecue restaurants. Recently Minneapolis is becoming central to the newly thriving Native American cuisine movement. It has been announced that a Native American restaurant by Sioux Chef author and educator Sean Sherman called Owanmi will be part of the Water Works, a park development project overlooking St. Anthony Falls and the Stone Arch Bridge, set to open in 2019. 

Germans comprise the majority of the state's ethnic heritage and one can find authentic German cuisine at the Gasthaus Bavarian Hunter in nearby Stillwater, and at the Black Forest Inn and the Gasthof zur Gemutlichkeit both found in Minneapolis. The latter restaurant is in Minneapolis' Northeast community which is also home to thriving Czech, Polish, Ukrainian and other Eastern European restaurants such as Jax Café, Kramarczuk's, Mayslack's and the former Nye's Polonaise lending this area an old world character and charm. The Twin Cities can also boast of authentic French, Irish, Italian and Russian restaurants. Spanish tapas restaurants exist, but are more trendy than homage. In the Twin Cities, pizzerias tend to be American rather than rustic Italian. Pies tend to feature thinner crusts, and are usually cut into squares (Also known as "tavern cut" or "party cut"). Rustic Italian pizzerias however, are not absent. A few do exist, and feature inventive styles of pizza. 

Authentic Mexican and Tex-Mex restaurants are quite popular in the Twin Cities, as there are Hispanic neighborhoods in both Saint Paul and Minneapolis. Many entrepreneurs have taken authentic Mexican cuisine into the suburbs as well. Latin American purveyors are also pioneering their home cuisines from Argentina, Brazil, Cuba, Ecuador, Peru and the Spanish Speaking West Indies offering authentic churrasco and ceviche among their dining options.

Asian cuisine was initially dominated by Chinese Cantonese immigrants that served Americanized offerings. In 1883 Woo Yee Sing and his younger brother, Woo Du Sing, opened the Canton Cafe in Minneapolis, the first Chinese restaurant in Minnesota. Authentic offerings began at the influential Nankin Cafe which opened in 1919, and many new Chinese immigrants soon took this cuisine throughout the Twin Cities and to the suburbs. Authentic Chinese cuisine from the provinces of Hunan and Szechaun and from Beijing, Shanghai and Taiwan are relatively new. The cuisine of Japan has been present since the opening of the area's very first Japanese restaurant, Fuji Ya in 1959. Since then, sushi and teppanyaki restaurants have also become increasingly more common. In the 1970s the Twin Cities saw a large influx of Southeast Asian immigrants from Cambodia, Laos, Thailand and Vietnam. The urban areas are now proliferated by Vietnamese phở noodle shops, banh mi and Thai curry restaurants. Since 1976 Supenn Supatanskinkasem (now Harrison) has been cooking and serving Thai food through her Minnesota State Fair Booth, Siam Café, and Sawatdee chain of Thai restaurants. Thanks to her persistence and success, others have opened Thai restaurants and there are now more than 100 establishments throughout Minnesota offering the food of Thailand.
Cambodian cuisine has also flourished given the large Hmong population familiar with it. Korean restaurants are few, as possibly their dining style and flavors have not been as adopted into the American mainstream. In the Twin Cities suburbs, Oriental buffets are popular for offering different Asian cuisines together. Restaurants offering other cuisines of Asia including those from Afghanistan, India, Nepal and the Philippines are also fairly recent additions to the Twin Cities dining scene and have been well received. Local ingredients are often integrated into Asian offerings, for example Chinese steamed walleye and Nepalese curried bison.

The Twin Cities are home to many restaurants that serve the cuisines of the Mediterranean and the Middle East. There are numerous Greek restaurants that range from fine dining to casual fast food shops that specialize in gyros. In both Minneapolis and Saint Paul, there exist long established Jewish cafes and delicatessens. Lebanese restaurants have also had a long time presence in both cities.

Authentic offerings of Arab cuisine, as well as other Middle Eastern cuisines, exist in the Minneapolis/St. Paul Metropolitan area. Egyptian, Iranian (Persian), Kurdish, and Turkish restaurants can be found throughout the Twin Cities.

Related cuisines from Northeast Africa can also be found throughout the Twin Cities metropolitan area. While restaurants that serve Ethiopian dishes have been in the Twin Cities for decades, more recent immigrants from Somalia have also opened a number of restaurants in Minnesota. Somali cuisine consists of an exotic mixture of native Somali, Ethiopian, Yemeni, Indian, Persian, Turkish and Italian culinary influences.

In addition, West African immigrants have introduced their own unique cuisine in recent years. There is also a presence of Afro-Caribbean restaurants, with the famed Nicollet Avenue in Minneapolis being home to two Caribbean restaurants. 

The University of Minnesota has been a center for food research with inventions such as the Haralson, Honeycrisp and SweeTango apple varieties. The Minnesota State Fair offers a sampling of many cuisines each year and Twin Citians claim that the all-American Corn Dog and Pronto Pup made their very first appearances there. Much like other states in the Midwest with a significant dairy industry, deep fried cheese curds are very popular at carnivals, city festivals, baseball games, county fairs, and the aforementioned Minnesota State Fair. Additionally, many important agricultural conglomerates, including Cargill, General Mills/Pillsbury, and International Multifoods make their home in Minneapolis-Saint Paul. The Betty Crocker food brand (named after a non-existent housewife) was born there. Several national restaurant chains, such as Buca di Beppo, Famous Dave's and the now defunct Chi-Chi's started in the Twin Cities. Buffalo Wild Wings, Dairy Queen, KarmelKorn Shoppes, the former Old Country Buffet, Orange Julius and T.G.I. Friday's (a division of Carlson Companies) are also well known chains headquartered in the Twin Cities.

Omaha has some unusual steakhouses such as the famous Gorat's, several of which are Sicilian in origin or adjacent to the Omaha Stockyards. Central European and Southern influences can be seen in the local popularity of carp and South 24th Street contains a multitude of Mexican restaurants. North Omaha also has its own barbecue style.

Omaha is one of the places claiming to have invented the reuben sandwich, supposedly named for Reuben Kulakofsky, a grocer from the Dundee neighborhood.

Bronco's, Godfather's Pizza, and the Garden Cafe are among the chain restaurants that originated in Omaha.

Omaha also has a thriving local pizza scene, with popular restaurants including Zio's, La Casa, Mama's and Valentino's. However, Big Fred's and Johnny Sortino's are the two that routinely vie for the title of the best pizza in town. 

The cheese frenchee is also a local favorite and staple, originating from the original King's Food Host fast food restaurants. Today in Omaha, you can find them at Amigos/Kings Classic and Don & Millies fast food restaurants.

The large number of Irish and German immigrants who came to St. Louis beginning in the early nineteenth century contributed significantly to the shaping of local cuisine as confirmed by a variety of uses of beef, pork and chicken, often roasted or grilled, as well as a variety of desserts including rich cakes, stollens, fruit pies, doughnuts and cookies. Even a local form of fresh stick pretzel, called Gus's Pretzels, has been sold singly or by the bagful by street corner vendors.

Mayfair salad dressing was invented at a St. Louis hotel of the same name, and is richer than Caesar salad dressing. St. Louis is also known for popularizing the ice cream cone and for inventing gooey butter cake (a rich, soft-centered coffee cake) and frozen custard. Iced tea is also rumored to have been invented at the World's Fair, as well as the hot dog bun.

Although St. Louis is typically not included on the list of major styles of barbecue in the United States, it was recognized by Kingsford as "America’s Top Grilling City" in its second annual list of "Top 10 Grilling Cities." A staple of grilling in St. Louis is the pork steak, which is sliced from the shoulder of the pig and often basted with or simmered in barbecue sauce during cooking. Other popular grilled items include crispy snoots, cut from the cheeks and nostrils of the pig; bratwurst; and Italian sausage, often referred to as "sah-zittsa," a localization of its Italian name, salsiccia. Maull's is a popular brand of barbecue sauce in the St. Louis area.

Restaurants on The Hill reflect the lasting influence of the early twentieth century Milanese and Sicilian immigrant community. Two unique Italian-American style dishes include "toasted" ravioli, which is breaded and fried, and St. Louis-style pizza, which has a crisp, thin crust and is usually made with Provel cheese instead of traditional mozzarella cheese.

A Poor boy sandwich is the traditional name in St. Louis for a submarine sandwich. A St. Paul sandwich is a St. Louis sandwich, available in Chinese-American restaurants. A Slinger is a diner and late night specialty consisting of eggs, hash browns and hamburger, topped with chili, cheese and onion.

Illinois is a top producer of corn and soybeans, but corn, particularly sweet corn, figures most substantially in its cuisine. Chicago-style cuisine is dominant in Northeastern Illinois, while other parts of the state mirror adjoining regions.

Springfield, Illinois, and the surrounding area are known for the horseshoe sandwich.

A popular dish seen almost exclusively in Indiana is sugar cream pie, which most likely originated in the state's Amish community. Persimmon pudding is also a favorite Indiana dessert very difficult to find outside of the Hoosier State.

The pork tenderloin sandwich, a sandwich of boneless pork loin that has been pounded flat, breaded, and fried, is a popular state food; Huntington is the likely first appearance in 1913. Beef and noodles is another homespun Hoosier dish.

Frog legs are traditional in old-fashioned Indiana restaurants, and brain sandwiches have a following. Fried biscuits with apple butter are served at many restaurants in southern Indiana, as are fried-brain sandwiches.

Iowa is the leading pork producer in the United States. This is reflected in Iowan cuisine, which includes the pork tenderloin sandwich (or simply 'pork tenderloin'), consisting of a lean section of boneless pork loin that is pounded flat, breaded, and deep fried before being served on a seeded hamburger bun with any or all of ketchup, mustard, mayonnaise, and dill pickle slices. In Iowa, the meat of a pork tenderloin sandwich is often far larger than the area of the bun. 

Iowa is the center for loose-meat sandwiches, also called tavern sandwiches and appearing on many menus by each restaurant's unique name for them. They originated in the region in the Ye Olde Tavern restaurant in 1934 before being popularized by Maid-Rite in 1936, which now has franchises in other midwestern states. The Sioux City take-out restaurant "Tastee Inn and Out" originated in 1955 and is now one of the last extant tavern sandwich specialty single-restaurant operations in the country. In Illinois, this sandwich is also known as a "loose hamburger sandwich".

Dutch letters, pastries filled with almond paste and shaped like an 'S,' are also common in Iowa.

Iowa is the leader in corn production in the United States; Iowans celebrate this with numerous sweet corn feeds during the summer months.

As of November 2006, Kansas still has 29 dry counties and only 17 counties have passed liquor-by-the-drink without a food sales requirement. Today there are more than 2600 liquor and 4000 cereal malt beverage licensees in the state.

Michigan is a large producer of asparagus, a vegetable crop widespread in Spring. Western and northern Michigan are notable in the production of apples, blueberries, and cherries. The Northwestern region of Michigan's Lower Peninsula accounts for approximately 75 percent of the U.S. crop of tart cherries, usually about 250 million pounds (11.3 Gg). A popular dish, Michigan Chicken Salad, includes cherries and often apples. Fruit salsas are also popular with cherry salsa being especially prominent. Michigan's wine and beer industries are substantial in the region. The Traverse City area is a popular destination to visit wineries and the state makes many varieties of wine, such as Rieslings, ice wines, and fruit wines. Micro-breweries continue to blossom creating a wide range of unique beers. Grand Rapids was voted Beer City USA 2013 in the Beer City USA poll, with Founders being the largest of Grand Rapids' breweries. Bell's, another large Michigan craft brewery is located further south in Kalamazoo.

Michigan is the home of both Post and Kellogg, with Battle Creek being called Cereal City. Vernor's ginger ale and Faygo pop also originate in Michigan. Vernor's ginger ale is often used as a home remedy for an upset stomach.

Coney Islands, a type of diner originating with Greek immigrants in Detroit, are fairly common throughout the state. A Coney is many times a Koegel hot dog from Flint-based Koegel Meat Company on a bun, topped with raw onion, mustard, and Coney sauce, a type of chili. Cheese may be added as well and variations are found throughout the state, with each city claiming theirs is the best. These diners usually also have gyros served with cucumber or honey mustard sauce, as well as hamburgers, sandwiches, breakfast and dinner entrees. Most Coney Islands are open 24 hours and also a popular place to get a late or early coffee.

In Polish communities throughout the state Pączki can be found every year on Fat Tuesday (Mardi Gras) in a wide assortment of flavors including lemon, blueberry to custard. Pierogis, goulash, and polish style sausage are common specialties in many restaurants.

Fish fries are common on Fridays and during Lent. Fish fries are usually set up buffet style typically consisting of items including rolls, potatoes (typically in the form of french fries and mashed), salad, coleslaw, apple sauce, deep fried fish and sometimes fried shrimp and baked fish. Fish is generally popular throughout the state due to the states location on four of the Great Lakes. Trout, walleye, perch and catfish are common. Whitefish is a regional specialty usually offered along the coast, with smoked whitefish and whitefish dip being noteworthy.

Cornish immigrant miners introduced the pasty to Michigan's Upper Peninsula (U.P.) as a convenient meal to take to work in the numerous copper, silver, and nickel mines of that region. The pasty is today considered iconic of the U.P.

Fudge is commonly sold in tourist areas, with Mackinac Island being most famous for its fudge, traditionally chocolate, but there is a wide variety of flavors from mint to maple and may include nuts, fruit, or other candy pieces.

Perhaps the most iconic Minnesota dishes are lefse and lutefisk, brought to the state by Scandinavian immigrants. Lefse and lutefisk dinners are held near Christmas and have become associated with that holiday. Lutefisk is a traditional dish of the Nordic countries made from stockfish (air-dried whitefish) and soda lye ("lut"). Walleye is the state fish of Minnesota and it is common to find it on restaurant menus. Its popularity with Minnesotans is such that the residents of the state consume more of the fish than does any other jurisdiction. Battered and deep-fried is a popular preparation for walleye, as is grilling. Many restaurants will feature walleye on their Friday night fish fry, which is popular at locales throughout the state.

Minnesota is known for its church potlucks, where hotdish is often served. Hotdish is any of a variety of casserole dishes, which are popular throughout the United States, although the term "hotdish" is used mainly in Minnesota, Wisconsin, North Dakota, and South Dakota. Hotdishes are filling comfort foods that are convenient and easy to make. "Tater Tot Hotdish" is a popular dish, and as Minnesota is one of the leading producers of wild rice, wild rice hotdishes are quite popular. Minnesota goulash, a combination of tomatoes, macaroni, ground beef and creamed corn is popular as well.

Bars are the second of the two essentials for potlucks in Minnesota. According to "You Know You're in Minnesota When...: 101 Quintessential Places, People, Events, Customs, Lingo, and Eats of the North Star State" by Berit Thorkelson, the bar is a Minnesota staple and a "typical Minnesota dessert". Thorkelson notes that bars are not included in Webster's Dictionary, and the word pronunciation of the "ar" is with "a pirate-like arrr" followed by a soft clipped s.

The immigrants that settled in the state in the 1800s were predominantly from Central and Eastern Europe (particularly Germany) and Scandinavia. They brought with them taste preferences that largely remain to this day. Those Minnesotans with this Northern European ancestry, in general, avoid hot spices in favor of earthy or aromatic spices.

In the northeastern section of the state, in the region collectively known as the Iron Range, the Mesabi Range area is known for Cornish pasties. The pasty, a meat and vegetable combination in a pastry crust, was brought to Minnesota by way of early Finnish iron miners as an easy lunch for miners working deep in the iron mines. It remains a favorite for both "locals" and summer tourists.

A traditional Slovenian nut bread called potica served at Easter and Christmas is still very popular in northern Minnesota. It is a yeast dough rolled and stretched paper thin and spread with a mixture of ground walnuts, butter, eggs, cream, and honey or sugar. It is then rolled jellyroll fashion and baked. Traditionally it was spiraled in a round pan, but now one is more likely to find it baked as a loaf.
The state is a productive area for chicken, dairy and turkey farms and crops such as corn, soybeans, and sugar beets and as such, eggs and meat along with potatoes and vegetables are mainstay foods. Warm baked goods along with stews and hearty soups are a favorite in the winter given the extreme Minnesota climate. Recipes using local wild game such as bison, deer or elk are also common. Other popular dishes statewide include glorified rice, Jell-O salad, and krumkake.

In Missouri, much of the cuisine is influenced by that of the Ozarks. Barbecue, both pork and beef, is popular in both St. Louis and Kansas City, as well as in much of the southern half of the state. In the Bootheel, sweet tea is commonly available at restaurants. Missouri also leans heavily on beer and bratwurst, and St. Louis features the fried brain sandwich, the St. Paul sandwich, toasted ravioli, St. Louis-style pizza, gooey butter cake, the slinger, and many other dishes that are popular throughout the state.

Fishing is a popular sport throughout the state, and many fish fry events feature catfish and large-mouthed bass. The "Missouri Rhineland" along the valley of the Missouri River is known for its wineries.

Square Runza sandwiches are identified with Nebraska.

Cuisine in North Dakota has been heavily influenced by both Norwegians and Germans from Russia, ethnic groups which have historically accounted for a large portion of North Dakota's population. Norwegian influences in the state include lefse, lutefisk, krumkake, and rosettes. Much of the Norwegian-influenced cuisine is also common in Minnesota and other states where Norwegians and their descendants live(d), although Norwegian influence may be the greater in North Dakota than any other state, as Norwegians played a large role in settling the area, and nearly one-third of North Dakotans claim Norwegian ancestry. Norwegian ancestry was historically more widespread throughout the northern half and eastern third of North Dakota, and therefore plays a stronger role in local cuisine in those parts of the state.

German-Russian cuisine is primarily influenced by that of the Schwarzmeerdeutsche, or Black Sea Germans, that heavily populated south-central and southwestern North Dakota (an area known as the German-Russian Triangle), as well as areas of South Dakota. While large numbers of Wolgadeutsche, Germans from Russia who lived near the Volga River in Russia (several hundred miles away from the Black Sea), also settled in the United States, they did not settle in large numbers in the Dakotas. Popular German-Russian cuisine includes kuchen, a thin, cheesecake-like custard pastry often filled with fruit such as cherries, apricot, prunes, and sometimes cottage cheese. Fleischkuekle (or fleischkuechle) is a popular meat-filled thin flatbread that is deep-fried and served hot. Another German-Russian specialty in the area is knoephla, a dumpling soup that almost always includes potatoes, and to a lesser extent, celery.

A confection popular in the state of Ohio is the local variation of a peanut butter cup known as a 'Buckeye'. Coated in chocolate, with a partially exposed center of peanut butter fudge, in appearance the candy resembles the chestnut that grows on the state tree, commonly known as the Buckeye.

Cincinnati-style chili is a Greek-inspired meat sauce, (ground beef seasoned with cinnamon, nutmeg, allspice, cloves, bay leaf, cumin, chili powder, unsweetened dark chocolate, salt and pepper), used as a topping for spaghetti or hot dogs. Additionally, red beans, chopped onions, and shredded cheese are offered as extra toppings referred to as "ways." 

A popular snack food in Ohio are Sauerkraut Balls, a meatball-sized fritter containing sauerkraut and some combination of ham, bacon, and pork. The recipe was invented in the late 1950s by two brothers, Max and Roman Gruber for their five star restaurant, Gruber's, located in Shaker Heights, Ohio. These were a derivative of the various ethnic cultures of Northeast Ohio, which includes Akron and Greater Cleveland. An annual Sauerkraut Festival is held in Waynesville, Ohio. at which sauerkraut balls, along with other sauerkraut specialities, are served.

Clam bakes are very popular in Northeast Ohio. The region, which was originally part of the Connecticut Western Reserve, was initially settled by people from Connecticut and other New England states. A typical Northeast Ohio clam bake typically includes clams, chicken, sweet potatoes, corn, and other side dishes. Unlike in New England, seaweed is not used and the clams, chicken, and sweet potatoes are all steamed together in a large pot.

Wisconsin is "America's Dairyland," and is home to numerous frozen custard stands, particularly around Milwaukee and along the Lake Michigan corridor. The state also has a special relationship with Blue Moon ice cream, being one of the only places the flavor can be found. While the flavor's origins are not well documented, it was most likely developed by flavor chemist Bill "Doc" Sidon of Milwaukee, Wisconsin. The state is also well known as a home to many cheesemakers. Currently, Wisconsin boasts 58 Master Cheesemakers, who are all qualified through an extensive process set by the Wisconsin Milk Marketing Board. The program is the only one of its kind outside of Europe. However, Wisconsin cheesemaking is even more diverse, ranging from artisans who hand-craft their product from the milk of their own dairy herds to large factories. Colby cheese was first created in Wisconsin in 1885 (named after the town it came from), and Brick cheese was first created in the state in 1877. The state has also played origin to Blue Marble Jack cheese, and is the only producer of Limburger cheese in the United States. Cheese curds can be eaten separately "squeaky," or cold, as a snack, or covered in batter and fried as an appetizer, often served with ranch dressing as a dipping sauce. 

Arguably the most universal Wisconsin dessert would be the cream puff, a type of profiterole that is a famous treat at the Wisconsin State Fair. The southeastern Wisconsin city of Racine is known for its Danish kringle, a sweet flaky pastry often served as a dessert. The capital city of Madison, Wisconsin was home to Carson Gulley, a famous local African American chef of the University of Wisconsin-Madison from the 1920s to the 1950s. He is credited with creating the famous dessert Fudge-Bottom pie and spreading its popularity.

The Friday night fish fry, often battered and fried perch or walleye, is traditional throughout Wisconsin, while in northeast Wisconsin along Lake Michigan, the Door County fish boil holds sway. The supper club is another common phenomenon of Wisconsin culinary heritage and often a destination for fish frys, which usually feature a portion of aforementioned fish, along with various sides: a fried food such as french fries and onion rings are common, along with condiments of tartar sauce and cole slaw (especially crimson slaw, a variety of cole slaw that incorporates Wisconsin's cranberries) and garnishes of parsley and lemon wedges.

Besides its "Cheesehead" status, Wisconsin has a reputation for alcohol consumption. Common traits of "drinking culture" are embedded in Wisconsin traditions, from festivals and holidays to everyday life. Many large breweries were founded in Wisconsin, largely in Milwaukee, which gained the epithet "Brew City" before the turn of the century: Miller, Pabst, Schlitz (all from and originally based in Milwaukee) and Leinenkugel all began as local favorites before entering the national and international markets. Wisconsin has experienced a resurgence in this industry, however, with numerous microbreweries and craft beers now being created and exported. Several other favorites include Ale Asylum, Capital, Sprecher, and New Glarus, the latter being well known for the Spotted Cow Farmhouse Ale. Besides beer, Wisconsinites also drink large quantities of brandy, often mixed into the unique Badger libation, the "brandy Old Fashioned," which can be sweet, sour, or press. Another though considerably more recent brandy-based cocktail is the Wisconsin Badger, derived from a mix of brandy, cranberry juice, and cherry schnapps—all very Wisconsin-inspired ingredients. Pewaukee, Wisconsin is also home to the alcoholic beverage Rumchata, described as an "horchata" recipe containing the primary ingredients of rum and Wisconsin cream.

Cooking with alcohol is also quite standard across the state. Wisconsinites commonly boil or braise their sausages (especially bratwursts) in several types of beer (most often a pilsner) with butter and onions, and "Beer batter" fish, typically walleye or perch, as well as cheese curds and onion rings are also common fare. Beer cheese soup is another beloved recipe, usually made from a variety of beer and a sharp cheddar or more mild colby cheese, with sausage, potatoes, and green onions. Another ubiquitous but perhaps less-mentioned recipe involving alcohol is "Beer Butt" or "Beer Can" Chicken (similar to drunken chicken), a vernacular meal involving a whole chicken slow-roasted, typically over a fire, with a can of usually amber beer directly inserted into the poultry's cavity; the result being a tender meat soaked inside with the flavors of the beer chosen. 

"Booyah" is another very common and hearty Wisconsin meal, found especially in the Northeast region of the state. The origins of this dish are disputed, but the Wisconsin origin contends that the word is a vernacular Flemish or Walloon Belgian spelling of the French word "bouillon", in this context meaning "broth." Recipes vary but common ingredients usually involve chicken or other meats--beef, pork, or ox tail are most often used--as well as a mirepoix of vegetables, commonly onion, celery, carrots, cabbage, peas, potatoes, and rutabaga. The ingredients are all cooked together in a special kind of large, cast-iron kettle often known as a "booyah kettle," over low heat for several days.

Wisconsin cuisine also features a large amount of sausage, or "wurst" (German for "sausage"). The state is also a major producer and consumer of summer sausage, as well as the nation's top producer and consumer of brats. Brats are typically boiled in a mix of beer, butter, and onions, served on a bratwurst bun, and topped with sauerkraut and often a spicy, brown style mustard. The city of Madison, Wisconsin, the state's capital, plays host to the annual "World's Largest" Brat Fest, a four-day-long festival incorporating music, recreational activities, and of course bratwursts grilled on a 65-foot-long grill. At Miller Park in the city of Milwaukee, Wisconsin's deep affection for sausage plays out in the Sausage Race, a mascot race involving racing sausage mascots representative of some of the most common sausages found in the state: bratwurst, kielbasa, Italian sausage, the hot dog, and chorizo. Venison sausage, Andouille sausage, and Belgian trippe are a few other common sausages found in the state, though they do not constitute a part of the Sausage Race. Miller Park is also notable for being the only U.S. stadium in which brats outsell hot dogs. 

Seymour, Wisconsin, claims to be the birthplace of the modern hamburger, although several other locations make similar claims. Certainly, however, the "Butter Burger" is a uniquely Wisconsin take on the classic American dish. Traditionally, a pad of butter is worked or "stuffed" into the raw hamburger patty before grilling.

These dishes, while not all exclusive to the Midwest, are typical of Midwestern foods. Although many foods are shared with other U.S. regions, they often feature uniquely Midwestern preparation styles.



</doc>
<doc id="20367" url="https://en.wikipedia.org/wiki?curid=20367" title="Moor">
Moor

Moor may refer to:







</doc>
<doc id="20369" url="https://en.wikipedia.org/wiki?curid=20369" title="Mitosis">
Mitosis

In cell biology, mitosis () is a part of the cell cycle when replicated chromosomes are separated into two new nuclei. In general, mitosis (division of the nucleus) is preceded by the S stage of interphase (during which the DNA is replicated) and is often accompanied or followed by cytokinesis, which divides the cytoplasm, organelles and cell membrane into two new cells containing roughly equal shares of these cellular components. Mitosis and cytokinesis together define the mitotic (M) phase of an animal cell cycle—the division of the mother cell into two daughter cells genetically identical to each other.

The process of mitosis is divided into stages corresponding to the completion of one set of activities and the start of the next. These stages are prophase, prometaphase, metaphase, anaphase, and telophase. During mitosis, the chromosomes, which have already duplicated, condense and attach to spindle fibers that pull one copy of each chromosome to opposite sides of the cell. The result is two genetically identical daughter nuclei. The rest of the cell may then continue to divide by cytokinesis to produce two daughter cells. Producing three or more daughter cells instead of normal two is a mitotic error called tripolar mitosis or multipolar mitosis (direct cell triplication / multiplication). Other errors during mitosis can induce apoptosis (programmed cell death) or cause mutations. Certain types of cancer can arise from such mutations.

Mitosis occurs only in eukaryotic cells. Prokaryotic cells, which lack a nucleus, divide by a different process called binary fission. Mitosis varies between organisms. For example, animal cells undergo an "open" mitosis, where the nuclear envelope breaks down before the chromosomes separate, whereas fungi undergo a "closed" mitosis, where chromosomes divide within an intact cell nucleus. Most animal cells undergo a shape change, known as mitotic cell rounding, to adopt a near spherical morphology at the start of mitosis. Most human cells are produced by mitotic cell division. Important exceptions include the gametes – sperm and egg cells – which are produced by meiosis.

Numerous descriptions of cell division were made during 18th and 19th centuries, with various degrees of accuracy. In 1835, the German botanist Hugo von Mohl, described cell division in the green alga "Cladophora glomerata", stating that multiplication of cells occurs through cell division. In 1838, Schleiden affirmed that the formation of new cells "in their interior" was a general law for cell multiplication in plants, a view later rejected in favour of Mohl model, due to contributions of Robert Remak and others.

In animal cells, cell division with mitosis was discovered in frog, rabbit, and cat cornea cells in 1873 and described for the first time by the Polish histologist Wacław Mayzel in 1875.

Bütschli, Schneider and Fol might have also claimed the discovery of the process presently known as "mitosis". In 1873, the German zoologist Otto Bütschli published data from observations on nematodes. A few years later, he discovered and described mitosis based on those observations.

The term "mitosis", coined by Walther Flemming in 1882, is derived from the Greek word μίτος ("mitos", "warp thread"). There are some alternative names for the process, e.g., "karyokinesis" (nuclear division), a term introduced by Schleicher in 1878, or "equational division", proposed by Weismann in 1887. However, the term "mitosis" is also used in a broad sense by some authors to refer to karyokinesis and cytokinesis together. Presently, "equational division" is more commonly used to refer to meiosis I.

The primary result of mitosis and cytokinesis is the transfer of a parent cell's genome into two daughter cells. The genome is composed of a number of chromosomes—complexes of tightly coiled DNA that contain genetic information vital for proper cell function. Because each resultant daughter cell should be genetically identical to the parent cell, the parent cell must make a copy of each chromosome before mitosis. This occurs during the S phase of interphase. Chromosome duplication results in two identical "sister chromatids" bound together by cohesin proteins at the "centromere".

When mitosis begins, the chromosomes condense and become visible. In some eukaryotes, for example animals, the nuclear envelope, which segregates the DNA from the cytoplasm, disintegrates into small vesicles. The nucleolus, which makes ribosomes in the cell, also disappears. Microtubules project from opposite ends of the cell, attach to the centromeres, and align the chromosomes centrally within the cell. The microtubules then contract to pull the sister chromatids of each chromosome apart. Sister chromatids at this point are called "daughter chromosomes". As the cell elongates, corresponding daughter chromosomes are pulled toward opposite ends of the cell and condense maximally in late anaphase. A new nuclear envelope forms around the separated daughter chromosomes, which decondense to form interphase nuclei.

During mitotic progression, typically after the anaphase onset, the cell may undergo cytokinesis. In animal cells, a cell membrane pinches inward between the two developing nuclei to produce two new cells. In plant cells, a cell plate forms between the two nuclei. Cytokinesis does not always occur; coenocytic (a type of multinucleate condition) cells undergo mitosis without cytokinesis.

The mitotic phase is a relatively short period of the cell cycle. It alternates with the much longer "interphase", where the cell prepares itself for the process of cell division. Interphase is divided into three phases: G (first gap), S (synthesis), and G (second gap). During all three parts of interphase, the cell grows by producing proteins and cytoplasmic organelles. However, chromosomes are replicated only during the S phase. Thus, a cell grows (G), continues to grow as it duplicates its chromosomes (S), grows more and prepares for mitosis (G), and finally divides (M) before restarting the cycle. All these phases in the cell cycle are highly regulated by cyclins, cyclin-dependent kinases, and other cell cycle proteins. The phases follow one another in strict order and there are "checkpoints" that give the cell cues to proceed from one phase to another. Cells may also temporarily or permanently leave the cell cycle and enter G phase to stop dividing. This can occur when cells become overcrowded (density-dependent inhibition) or when they differentiate to carry out specific functions for the organism, as is the case for human heart muscle cells and neurons. Some G cells have the ability to re-enter the cell cycle.

In plant cells only, prophase is preceded by a pre-prophase stage. In highly vacuolated plant cells, the nucleus has to migrate into the center of the cell before mitosis can begin. This is achieved through the formation of a phragmosome, a transverse sheet of cytoplasm that bisects the cell along the future plane of cell division. In addition to phragmosome formation, preprophase is characterized by the formation of a ring of microtubules and actin filaments (called preprophase band) underneath the plasma membrane around the equatorial plane of the future mitotic spindle. This band marks the position where the cell will eventually divide. The cells of higher plants (such as the flowering plants) lack centrioles; instead, microtubules form a spindle on the surface of the nucleus and are then organized into a spindle by the chromosomes themselves, after the nuclear envelope breaks down. The preprophase band disappears during nuclear envelope breakdown and spindle formation in prometaphase.

During prophase, which occurs after G interphase, the cell prepares to divide by tightly condensing its chromosomes and initiating mitotic spindle formation. During interphase, the genetic material in the nucleus consists of loosely packed chromatin. At the onset of prophase, chromatin fibers condense into discrete chromosomes that are typically visible at high magnification through a light microscope. In this stage, chromosomes are long, thin and thread-like. Each chromosome has two chromatids. The two chromatids are joined at the centromere.

Gene transcription ceases during prophase and does not resume until late anaphase to early G1 phase. The nucleolus also disappears during early prophase.

At the beginning of prometaphase in animal cells, phosphorylation of nuclear lamins causes the nuclear envelope to disintegrate into small membrane vesicles. As this happens, microtubules invade the nuclear space. This is called "open mitosis", and it occurs in some multicellular organisms. Fungi and some protists, such as algae or trichomonads, undergo a variation called "closed mitosis" where the spindle forms inside the nucleus, or the microtubules penetrate the intact nuclear envelope.

In late prometaphase, "kinetochore microtubules" begin to search for and attach to chromosomal kinetochores. A "kinetochore" is a proteinaceous microtubule-binding structure that forms on the chromosomal centromere during late prophase. A number of "polar microtubules" find and interact with corresponding polar microtubules from the opposite centrosome to form the mitotic spindle. Although the kinetochore structure and function are not fully understood, it is known that it contains some form of molecular motor. When a microtubule connects with the kinetochore, the motor activates, using energy from ATP to "crawl" up the tube toward the originating centrosome. This motor activity, coupled with polymerisation and depolymerisation of microtubules, provides the pulling force necessary to later separate the chromosome's two chromatids.

After the microtubules have located and attached to the kinetochores in prometaphase, the two centrosomes begin pulling the chromosomes towards opposite ends of the cell. The resulting tension causes the chromosomes to align along the "metaphase plate" or "equatorial plane", an imaginary line that is centrally located between the two centrosomes (at approximately the midline of the cell). To ensure equitable distribution of chromosomes at the end of mitosis, the "metaphase checkpoint" guarantees that kinetochores are properly attached to the mitotic spindle and that the chromosomes are aligned along the metaphase plate. If the cell successfully passes through the metaphase checkpoint, it proceeds to anaphase.

During "anaphase A", the cohesins that bind sister chromatids together are cleaved, forming two identical daughter chromosomes. Shortening of the kinetochore microtubules pulls the newly formed daughter chromosomes to opposite ends of the cell. During "anaphase B", polar microtubules push against each other, causing the cell to elongate. In late anaphase, chromosomes also reach their overall maximal condensation level, to help chromosome segregation and the re-formation of the nucleus. In most animal cells, anaphase A precedes anaphase B, but some vertebrate egg cells demonstrate the opposite order of events.

Telophase (from the Greek word "τελος" meaning "end") is a reversal of prophase and prometaphase events. At telophase, the polar microtubules continue to lengthen, elongating the cell even more. If the nuclear envelope has broken down, a new nuclear envelope forms using the membrane vesicles of the parent cell's old nuclear envelope. The new envelope forms around each set of separated daughter chromosomes (though the membrane does not enclose the centrosomes) and the nucleolus reappears. Both sets of chromosomes, now surrounded by new nuclear membrane, begin to "relax" or decondense. Mitosis is complete. Each daughter nucleus has an identical set of chromosomes. Cell division may or may not occur at this time depending on the organism.

Cytokinesis is not a phase of mitosis but rather a separate process, necessary for completing cell division. In animal cells, a cleavage furrow (pinch) containing a contractile ring develops where the metaphase plate used to be, pinching off the separated nuclei. In both animal and plant cells, cell division is also driven by vesicles derived from the Golgi apparatus, which move along microtubules to the middle of the cell. In plants, this structure coalesces into a cell plate at the center of the phragmoplast and develops into a cell wall, separating the two nuclei. The phragmoplast is a microtubule structure typical for higher plants, whereas some green algae use a phycoplast microtubule array during cytokinesis. Each daughter cell has a complete copy of the genome of its parent cell. The end of cytokinesis marks the end of the M-phase.

There are many cells where mitosis and cytokinesis occur separately, forming single cells with multiple nuclei. The most notable occurrence of this is among the fungi, slime molds, and coenocytic algae, but the phenomenon is found in various other organisms. Even in animals, cytokinesis and mitosis may occur independently, for instance during certain stages of fruit fly embryonic development.

Mitosis "function" or significance relies on the maintenance of the chromosomal set; each cell formed receives chromosomes that are alike in composition and equal in number to the chromosomes of the parent cell.

Mitosis occurs in the following circumstances:

The mitosis process in the cells of eukaryotic organisms follow a similar pattern, but with variations in three main details. "Closed" and "open" mitosis can be distinguished on the basis of nuclear envelope remaining intact or breaking down. An intermediate form with partial degradation of the nuclear envelope is called a "semiopen" mitosis. With respesct to the symmetry of the spindle apparatus during metaphase, an approximately axially symmetric (centered) shape is called as "orthomitosis", distinguisehd from the eccentric spindles of "pleuromitosis", in which mitotic apparatus has a bilateral symmetry. Finally, a third criterion is the location of the central spindle in case of closed pleuromitosis: "extranuclear" (spindle located in the cytoplasm) or "intranuclear" (in the nucleus).

Nuclear division takes place only in cells of organisms of the eukaryotic domain, as bacteria and archaea have no nucleus. Within each of the eukaryotic supergroups, mitosis of the open form can be found, as well as closed mitosis, except for Excavata, which show exclusively closed mitosis. Following, the occurrence of the forms of mitosis in eukaryotes:


Errors can occur during mitosis, especially during early embryonic development in humans. Mitotic errors can create aneuploid cells that have too few or too many of one or more chromosomes, a condition associated with cancer. Early human embryos, cancer cells, infected or intoxicated cells can also suffer from pathological division into three or more daughter cells (tripolar or multipolar mitosis), resulting in severe errors in their chromosomal complements.

In "nondisjunction", sister chromatids fail to separate during anaphase. One daughter cell receives both sister chromatids from the nondisjoining chromosome and the other cell receives none. As a result, the former cell gets three copies of the chromosome, a condition known as "trisomy", and the latter will have only one copy, a condition known as "monosomy". On occasion, when cells experience nondisjunction, they fail to complete cytokinesis and retain both nuclei in one cell, resulting in binucleated cells.

"Anaphase lag" occurs when the movement of one chromatid is impeded during anaphase. This may be caused by a failure of the mitotic spindle to properly attach to the chromosome. The lagging chromatid is excluded from both nuclei and is lost. Therefore, one of the daughter cells will be monosomic for that chromosome.

"Endoreduplication" (or endoreplication) occurs when chromosomes duplicate but the cell does not subsequently divide. This results in polyploid cells or, if the chromosomes duplicates repeatedly, polytene chromosomes. Endoreduplication is found in many species and appears to be a normal part of development. Endomitosis is a variant of endoreduplication in which cells replicate their chromosomes during S phase and enter, but prematurely terminate, mitosis. Instead of being divided into two new daughter nuclei, the replicated chromosomes are retained within the original nucleus. The cells then re-enter G and S phase and replicate their chromosomes again. This may occur multiple times, increasing the chromosome number with each round of replication and endomitosis. Platelet-producing megakaryocytes go through endomitosis during cell differentiation.

"Amitosis" in ciliates and in animal placental tissues results in a random distribution of parental alleles.

"Karyokinesis without cytokinesis" originates multinucleated cells called coenocytes.

In animal tissue, most cells round up to a near-spherical shape during mitosis. In epithelia and epidermis, an efficient rounding process is correlated with proper mitotic spindle alignment and subsequent correct positioning of daughter cells. Moreover, researchers have found that if rounding is heavily suppressed it may result in spindle defects, primarily pole splitting and failure to efficiently capture chromosomes. Therefore, mitotic cell rounding is thought to play a protective role in ensuring accurate mitosis.

Rounding forces are driven by reorganization of F-actin and myosin (actomyosin) into a contractile homogeneous cell cortex that 1) rigidifies the cell periphery and 2) facilitates generation of intracellular hydrostatic pressure (up to 10 fold higher than interphase). The generation of intracellular pressure is particularly critical under confinement, such as would be important in a tissue scenario, where outward forces must be produced to round up against surrounding cells and/or the extracellular matrix. Generation of pressure is dependent on formin-mediated F-actin nucleation and Rho kinase (ROCK)-mediated myosin II contraction, both of which are governed upstream by signaling pathways RhoA and ECT2 through the activity of Cdk1. Due to its importance in mitosis, the molecular components and dynamics of the mitotic actomyosin cortex is an area of active research.

Mitotic cells irradiated with X-rays in the G1 phase of the cell cycle repair recombinogenic DNA damages primarily by recombination between homologous chromosomes. Mitotic cells irradiated in the G2 phase repair such damages preferentially by sister-chromatid recombination. Mutations in genes encoding enzymes employed in recombination cause cells to have increased sensitivity to being killed by a variety of DNA damaging agents. These findings suggest that mitotic recombination is an adaptation for repairing DNA damages including those that are potentially lethal.

There are prokaryotic homologs of all the key molecules of eukaryotic mitosis (e.g., actins, tubulins). Being a universal eukaryotic property, mitosis probably arose at the base of the eukaryotic tree. As mitosis is less complex than meiosis, meiosis presumably arose after mitosis.

While in bacterial cell division, after duplication of DNA, two circular chromosomes are attached to a special region of the cell membrane, eukaryotic mitosis is usually characterized by the presence of many linear chromosomes, whose kinetochores attaches to the microtubules of the spindle. In relation to the forms of mitosis, closed intranuclear pleuromitosis seems to be the most primitive type, as it is the more similar to bacterial division.

Mitotic cells can be visualized microscopically by staining them with fluorescent antibodies and dyes.



</doc>
<doc id="20374" url="https://en.wikipedia.org/wiki?curid=20374" title="Metabolism">
Metabolism

Metabolism (, from "metabolē", "change") is the set of life-sustaining chemical transformations within the cells of organisms. The three main purposes of metabolism are the conversion of food/fuel to energy to run cellular processes, the conversion of food/fuel to building blocks for proteins, lipids, nucleic acids, and some carbohydrates, and the elimination of nitrogenous wastes. These enzyme-catalyzed reactions allow organisms to grow and reproduce, maintain their structures, and respond to their environments. The word metabolism can also refer to the sum of all chemical reactions that occur in living organisms, including digestion and the transport of substances into and between different cells, in which case the set of reactions within the cells is called intermediary metabolism or intermediate metabolism.

Metabolism is usually divided into two categories: catabolism, the "breaking down" of organic matter for example, the breaking down of glucose to pyruvate, by cellular respiration, and anabolism, the "building up" of components of cells such as proteins and nucleic acids. Usually, breaking down releases energy and building up consumes energy.

The chemical reactions of metabolism are organized into metabolic pathways, in which one chemical is transformed through a series of steps into another chemical, by a sequence of enzymes. Enzymes are crucial to metabolism because they allow organisms to drive desirable reactions that require energy that will not occur by themselves, by coupling them to spontaneous reactions that release energy. Enzymes act as catalysts that allow the reactions to proceed more rapidly. Enzymes also allow the regulation of metabolic pathways in response to changes in the cell's environment or to signals from other cells.

The metabolic system of a particular organism determines which substances it will find nutritious and which poisonous. For example, some prokaryotes use hydrogen sulfide as a nutrient, yet this gas is poisonous to animals. The speed of metabolism, the metabolic rate, influences how much food an organism will require, and also affects how it is able to obtain that food.

A striking feature of metabolism is the similarity of the basic metabolic pathways and components between even vastly different species. For example, the set of carboxylic acids that are best known as the intermediates in the citric acid cycle are present in all known organisms, being found in species as diverse as the unicellular bacterium "Escherichia coli" and huge multicellular organisms like elephants. These striking similarities in metabolic pathways are likely due to their early appearance in evolutionary history, and their retention because of their efficacy.

Most of the structures that make up animals, plants and microbes are made from three basic classes of molecule: amino acids, carbohydrates and lipids (often called fats). As these molecules are vital for life, metabolic reactions either focus on making these molecules during the construction of cells and tissues, or by breaking them down and using them as a source of energy, by their digestion. These biochemicals can be joined together to make polymers such as DNA and proteins, essential macromolecules of life.
Proteins are made of amino acids arranged in a linear chain joined together by peptide bonds. Many proteins are enzymes that catalyze the chemical reactions in metabolism. Other proteins have structural or mechanical functions, such as those that form the cytoskeleton, a system of scaffolding that maintains the cell shape. Proteins are also important in cell signaling, immune responses, cell adhesion, active transport across membranes, and the cell cycle. Amino acids also contribute to cellular energy metabolism by providing a carbon source for entry into the citric acid cycle (tricarboxylic acid cycle), especially when a primary source of energy, such as glucose, is scarce, or when cells undergo metabolic stress.

Lipids are the most diverse group of biochemicals. Their main structural uses are as part of biological membranes both internal and external, such as the cell membrane, or as a source of energy. Lipids are usually defined as hydrophobic or amphipathic biological molecules but will dissolve in organic solvents such as benzene or chloroform. The fats are a large group of compounds that contain fatty acids and glycerol; a glycerol molecule attached to three fatty acid esters is called a triacylglyceride. Several variations on this basic structure exist, including alternate backbones such as sphingosine in the sphingolipids, and hydrophilic groups such as phosphate as in phospholipids. Steroids such as cholesterol are another major class of lipids.

Carbohydrates are aldehydes or ketones, with many hydroxyl groups attached, that can exist as straight chains or rings. Carbohydrates are the most abundant biological molecules, and fill numerous roles, such as the storage and transport of energy (starch, glycogen) and structural components (cellulose in plants, chitin in animals). The basic carbohydrate units are called monosaccharides and include galactose, fructose, and most importantly glucose. Monosaccharides can be linked together to form polysaccharides in almost limitless ways.

The two nucleic acids, DNA and RNA, are polymers of nucleotides. Each nucleotide is composed of a phosphate attached to a ribose or deoxyribose sugar group which is attached to a nitrogenous base. Nucleic acids are critical for the storage and use of genetic information, and its interpretation through the processes of transcription and protein biosynthesis. This information is protected by DNA repair mechanisms and propagated through DNA replication. Many viruses have an RNA genome, such as HIV, which uses reverse transcription to create a DNA template from its viral RNA genome. RNA in ribozymes such as spliceosomes and ribosomes is similar to enzymes as it can catalyze chemical reactions. Individual nucleosides are made by attaching a nucleobase to a ribose sugar. These bases are heterocyclic rings containing nitrogen, classified as purines or pyrimidines. Nucleotides also act as coenzymes in metabolic-group-transfer reactions.

Metabolism involves a vast array of chemical reactions, but most fall under a few basic types of reactions that involve the transfer of functional groups of atoms and their bonds within molecules. This common chemistry allows cells to use a small set of metabolic intermediates to carry chemical groups between different reactions. These group-transfer intermediates are called coenzymes. Each class of group-transfer reactions is carried out by a particular coenzyme, which is the substrate for a set of enzymes that produce it, and a set of enzymes that consume it. These coenzymes are therefore continuously made, consumed and then recycled.

One central coenzyme is adenosine triphosphate (ATP), the universal energy currency of cells. This nucleotide is used to transfer chemical energy between different chemical reactions. There is only a small amount of ATP in cells, but as it is continuously regenerated, the human body can use about its own weight in ATP per day. ATP acts as a bridge between catabolism and anabolism. Catabolism breaks down molecules, and anabolism puts them together. Catabolic reactions generate ATP, and anabolic reactions consume it. It also serves as a carrier of phosphate groups in phosphorylation reactions.

A vitamin is an organic compound needed in small quantities that cannot be made in cells. In human nutrition, most vitamins function as coenzymes after modification; for example, all water-soluble vitamins are phosphorylated or are coupled to nucleotides when they are used in cells. Nicotinamide adenine dinucleotide (NAD), a derivative of vitamin B (niacin), is an important coenzyme that acts as a hydrogen acceptor. Hundreds of separate types of dehydrogenases remove electrons from their substrates and reduce NAD into NADH. This reduced form of the coenzyme is then a substrate for any of the reductases in the cell that need to reduce their substrates. Nicotinamide adenine dinucleotide exists in two related forms in the cell, NADH and NADPH. The NAD/NADH form is more important in catabolic reactions, while NADP/NADPH is used in anabolic reactions.

Inorganic elements play critical roles in metabolism; some are abundant (e.g. sodium and potassium) while others function at minute concentrations. About 99% of a mammal's mass is made up of the elements carbon, nitrogen, calcium, sodium, chlorine, potassium, hydrogen, phosphorus, oxygen and sulfur. Organic compounds (proteins, lipids and carbohydrates) contain the majority of the carbon and nitrogen; most of the oxygen and hydrogen is present as water.

The abundant inorganic elements act as ionic electrolytes. The most important ions are sodium, potassium, calcium, magnesium, chloride, phosphate and the organic ion bicarbonate. The maintenance of precise ion gradients across cell membranes maintains osmotic pressure and pH. Ions are also critical for nerve and muscle function, as action potentials in these tissues are produced by the exchange of electrolytes between the extracellular fluid and the cell's fluid, the cytosol. Electrolytes enter and leave cells through proteins in the cell membrane called ion channels. For example, muscle contraction depends upon the movement of calcium, sodium and potassium through ion channels in the cell membrane and T-tubules.

Transition metals are usually present as trace elements in organisms, with zinc and iron being most abundant of those. These metals are used in some proteins as cofactors and are essential for the activity of enzymes such as catalase and oxygen-carrier proteins such as hemoglobin. Metal cofactors are bound tightly to specific sites in proteins; although enzyme cofactors can be modified during catalysis, they always return to their original state by the end of the reaction catalyzed. Metal micronutrients are taken up into organisms by specific transporters and bind to storage proteins such as ferritin or metallothionein when not in use.

Catabolism is the set of metabolic processes that break down large molecules. These include breaking down and oxidizing food molecules. The purpose of the catabolic reactions is to provide the energy and components needed by anabolic reactions which build molecules. The exact nature of these catabolic reactions differ from organism to organism, and organisms can be classified based on their sources of energy and carbon (their primary nutritional groups), as shown in the table below. Organic molecules are used as a source of energy by organotrophs, while lithotrophs use inorganic substrates, and phototrophs capture sunlight as chemical energy. However, all these different forms of metabolism depend on redox reactions that involve the transfer of electrons from reduced donor molecules such as organic molecules, water, ammonia, hydrogen sulfide or ferrous ions to acceptor molecules such as oxygen, nitrate or sulfate. In animals, these reactions involve complex organic molecules that are broken down to simpler molecules, such as carbon dioxide and water. In photosynthetic organisms, such as plants and cyanobacteria, these electron-transfer reactions do not release energy but are used as a way of storing energy absorbed from sunlight.

The most common set of catabolic reactions in animals can be separated into three main stages. In the first stage, large organic molecules, such as proteins, polysaccharides or lipids, are digested into their smaller components outside cells. Next, these smaller molecules are taken up by cells and converted to smaller molecules, usually acetyl coenzyme A (acetyl-CoA), which releases some energy. Finally, the acetyl group on the CoA is oxidised to water and carbon dioxide in the citric acid cycle and electron transport chain, releasing the energy that is stored by reducing the coenzyme nicotinamide adenine dinucleotide (NAD) into NADH.

Macromolecules such as starch, cellulose or proteins cannot be rapidly taken up by cells and must be broken into their smaller units before they can be used in cell metabolism. Several common classes of enzymes digest these polymers. These digestive enzymes include proteases that digest proteins into amino acids, as well as glycoside hydrolases that digest polysaccharides into simple sugars known as monosaccharides.

Microbes simply secrete digestive enzymes into their surroundings, while animals only secrete these enzymes from specialized cells in their guts, including the stomach and pancreas, and salivary glands. The amino acids or sugars released by these extracellular enzymes are then pumped into cells by active transport proteins.
Carbohydrate catabolism is the breakdown of carbohydrates into smaller units. Carbohydrates are usually taken into cells once they have been digested into monosaccharides. Once inside, the major route of breakdown is glycolysis, where sugars such as glucose and fructose are converted into pyruvate and some ATP is generated. Pyruvate is an intermediate in several metabolic pathways, but the majority is converted to acetyl-CoA through aerobic (with oxygen) glycolysis and fed into the citric acid cycle. Although some more ATP is generated in the citric acid cycle, the most important product is NADH, which is made from NAD as the acetyl-CoA is oxidized. This oxidation releases carbon dioxide as a waste product. In anaerobic conditions, glycolysis produces lactate, through the enzyme lactate dehydrogenase re-oxidizing NADH to NAD+ for re-use in glycolysis. An alternative route for glucose breakdown is the pentose phosphate pathway, which reduces the coenzyme NADPH and produces pentose sugars such as ribose, the sugar component of nucleic acids.

Fats are catabolised by hydrolysis to free fatty acids and glycerol. The glycerol enters glycolysis and the fatty acids are broken down by beta oxidation to release acetyl-CoA, which then is fed into the citric acid cycle. Fatty acids release more energy upon oxidation than carbohydrates because carbohydrates contain more oxygen in their structures. Steroids are also broken down by some bacteria in a process similar to beta oxidation, and this breakdown process involves the release of significant amounts of acetyl-CoA, propionyl-CoA, and pyruvate, which can all be used by the cell for energy. "M. tuberculosis" can also grow on the lipid cholesterol as a sole source of carbon, and genes involved in the cholesterol use pathway(s) have been validated as important during various stages of the infection lifecycle of "M. tuberculosis".

Amino acids are either used to synthesize proteins and other biomolecules, or oxidized to urea and carbon dioxide as a source of energy. The oxidation pathway starts with the removal of the amino group by a transaminase. The amino group is fed into the urea cycle, leaving a deaminated carbon skeleton in the form of a keto acid. Several of these keto acids are intermediates in the citric acid cycle, for example the deamination of glutamate forms α-ketoglutarate. The glucogenic amino acids can also be converted into glucose, through gluconeogenesis (discussed below).

In oxidative phosphorylation, the electrons removed from organic molecules in areas such as the protagon acid cycle are transferred to oxygen and the energy released is used to make ATP. This is done in eukaryotes by a series of proteins in the membranes of mitochondria called the electron transport chain. In prokaryotes, these proteins are found in the cell's inner membrane. These proteins use the energy released from passing electrons from reduced molecules like NADH onto oxygen to pump protons across a membrane.

Pumping protons out of the mitochondria creates a proton concentration difference across the membrane and generates an electrochemical gradient. This force drives protons back into the mitochondrion through the base of an enzyme called ATP synthase. The flow of protons makes the stalk subunit rotate, causing the active site of the synthase domain to change shape and phosphorylate adenosine diphosphate – turning it into ATP.

Chemolithotrophy is a type of metabolism found in prokaryotes where energy is obtained from the oxidation of inorganic compounds. These organisms can use hydrogen, reduced sulfur compounds (such as sulfide, hydrogen sulfide and thiosulfate), ferrous iron (FeII) or ammonia as sources of reducing power and they gain energy from the oxidation of these compounds with electron acceptors such as oxygen or nitrite. These microbial processes are important in global biogeochemical cycles such as acetogenesis, nitrification and denitrification and are critical for soil fertility.

The energy in sunlight is captured by plants, cyanobacteria, purple bacteria, green sulfur bacteria and some protists. This process is often coupled to the conversion of carbon dioxide into organic compounds, as part of photosynthesis, which is discussed below. The energy capture and carbon fixation systems can however operate separately in prokaryotes, as purple bacteria and green sulfur bacteria can use sunlight as a source of energy, while switching between carbon fixation and the fermentation of organic compounds.

In many organisms the capture of solar energy is similar in principle to oxidative phosphorylation, as it involves the storage of energy as a proton concentration gradient. This proton motive force then drives ATP synthesis. The electrons needed to drive this electron transport chain come from light-gathering proteins called photosynthetic reaction centres or rhodopsins. Reaction centers are classed into two types depending on the type of photosynthetic pigment present, with most photosynthetic bacteria only having one type, while plants and cyanobacteria have two.

In plants, algae, and cyanobacteria, photosystem II uses light energy to remove electrons from water, releasing oxygen as a waste product. The electrons then flow to the cytochrome b6f complex, which uses their energy to pump protons across the thylakoid membrane in the chloroplast. These protons move back through the membrane as they drive the ATP synthase, as before. The electrons then flow through photosystem I and can then either be used to reduce the coenzyme NADP, for use in the Calvin cycle, which is discussed below, or recycled for further ATP generation.

Anabolism is the set of constructive metabolic processes where the energy released by catabolism is used to synthesize complex molecules. In general, the complex molecules that make up cellular structures are constructed step-by-step from small and simple precursors. Anabolism involves three basic stages. First, the production of precursors such as amino acids, monosaccharides, isoprenoids and nucleotides, secondly, their activation into reactive forms using energy from ATP, and thirdly, the assembly of these precursors into complex molecules such as proteins, polysaccharides, lipids and nucleic acids.

Organisms differ according to the number of constructed molecules in their cells. Autotrophs such as plants can construct the complex organic molecules in cells such as polysaccharides and proteins from simple molecules like carbon dioxide and water. Heterotrophs, on the other hand, require a source of more complex substances, such as monosaccharides and amino acids, to produce these complex molecules. Organisms can be further classified by ultimate source of their energy: photoautotrophs and photoheterotrophs obtain energy from light, whereas chemoautotrophs and chemoheterotrophs obtain energy from inorganic oxidation reactions.

Photosynthesis is the synthesis of carbohydrates from sunlight and carbon dioxide (CO). In plants, cyanobacteria and algae, oxygenic photosynthesis splits water, with oxygen produced as a waste product. This process uses the ATP and NADPH produced by the photosynthetic reaction centres, as described above, to convert CO into glycerate 3-phosphate, which can then be converted into glucose. This carbon-fixation reaction is carried out by the enzyme RuBisCO as part of the Calvin – Benson cycle. Three types of photosynthesis occur in plants, C3 carbon fixation, C4 carbon fixation and CAM photosynthesis. These differ by the route that carbon dioxide takes to the Calvin cycle, with C3 plants fixing CO directly, while C4 and CAM photosynthesis incorporate the CO into other compounds first, as adaptations to deal with intense sunlight and dry conditions.

In photosynthetic prokaryotes the mechanisms of carbon fixation are more diverse. Here, carbon dioxide can be fixed by the Calvin – Benson cycle, a reversed citric acid cycle, or the carboxylation of acetyl-CoA. Prokaryotic chemoautotrophs also fix CO through the Calvin – Benson cycle, but use energy from inorganic compounds to drive the reaction.

In carbohydrate anabolism, simple organic acids can be converted into monosaccharides such as glucose and then used to assemble polysaccharides such as starch. The generation of glucose from compounds like pyruvate, lactate, glycerol, glycerate 3-phosphate and amino acids is called gluconeogenesis. Gluconeogenesis converts pyruvate to glucose-6-phosphate through a series of intermediates, many of which are shared with glycolysis. However, this pathway is not simply glycolysis run in reverse, as several steps are catalyzed by non-glycolytic enzymes. This is important as it allows the formation and breakdown of glucose to be regulated separately, and prevents both pathways from running simultaneously in a futile cycle.

Although fat is a common way of storing energy, in vertebrates such as humans the fatty acids in these stores cannot be converted to glucose through gluconeogenesis as these organisms cannot convert acetyl-CoA into pyruvate; plants do, but animals do not, have the necessary enzymatic machinery. As a result, after long-term starvation, vertebrates need to produce ketone bodies from fatty acids to replace glucose in tissues such as the brain that cannot metabolize fatty acids. In other organisms such as plants and bacteria, this metabolic problem is solved using the glyoxylate cycle, which bypasses the decarboxylation step in the citric acid cycle and allows the transformation of acetyl-CoA to oxaloacetate, where it can be used for the production of glucose.

Polysaccharides and glycans are made by the sequential addition of monosaccharides by glycosyltransferase from a reactive sugar-phosphate donor such as uridine diphosphate glucose (UDP-glucose) to an acceptor hydroxyl group on the growing polysaccharide. As any of the hydroxyl groups on the ring of the substrate can be acceptors, the polysaccharides produced can have straight or branched structures. The polysaccharides produced can have structural or metabolic functions themselves, or be transferred to lipids and proteins by enzymes called oligosaccharyltransferases.

Fatty acids are made by fatty acid synthases that polymerize and then reduce acetyl-CoA units. The acyl chains in the fatty acids are extended by a cycle of reactions that add the acyl group, reduce it to an alcohol, dehydrate it to an alkene group and then reduce it again to an alkane group. The enzymes of fatty acid biosynthesis are divided into two groups: in animals and fungi, all these fatty acid synthase reactions are carried out by a single multifunctional type I protein, while in plant plastids and bacteria separate type II enzymes perform each step in the pathway.

Terpenes and isoprenoids are a large class of lipids that include the carotenoids and form the largest class of plant natural products. These compounds are made by the assembly and modification of isoprene units donated from the reactive precursors isopentenyl pyrophosphate and dimethylallyl pyrophosphate. These precursors can be made in different ways. In animals and archaea, the mevalonate pathway produces these compounds from acetyl-CoA, while in plants and bacteria the non-mevalonate pathway uses pyruvate and glyceraldehyde 3-phosphate as substrates. One important reaction that uses these activated isoprene donors is steroid biosynthesis. Here, the isoprene units are joined together to make squalene and then folded up and formed into a set of rings to make lanosterol. Lanosterol can then be converted into other steroids such as cholesterol and ergosterol.

Organisms vary in their ability to synthesize the 20 common amino acids. Most bacteria and plants can synthesize all twenty, but mammals can only synthesize eleven nonessential amino acids, so nine essential amino acids must be obtained from food. Some simple parasites, such as the bacteria "Mycoplasma pneumoniae", lack all amino acid synthesis and take their amino acids directly from their hosts. All amino acids are synthesized from intermediates in glycolysis, the citric acid cycle, or the pentose phosphate pathway. Nitrogen is provided by glutamate and glutamine. Amino acid synthesis depends on the formation of the appropriate alpha-keto acid, which is then transaminated to form an amino acid.

Amino acids are made into proteins by being joined together in a chain of peptide bonds. Each different protein has a unique sequence of amino acid residues: this is its primary structure. Just as the letters of the alphabet can be combined to form an almost endless variety of words, amino acids can be linked in varying sequences to form a huge variety of proteins. Proteins are made from amino acids that have been activated by attachment to a transfer RNA molecule through an ester bond. This aminoacyl-tRNA precursor is produced in an ATP-dependent reaction carried out by an aminoacyl tRNA synthetase. This aminoacyl-tRNA is then a substrate for the ribosome, which joins the amino acid onto the elongating protein chain, using the sequence information in a messenger RNA.

Nucleotides are made from amino acids, carbon dioxide and formic acid in pathways that require large amounts of metabolic energy. Consequently, most organisms have efficient systems to salvage preformed nucleotides. Purines are synthesized as nucleosides (bases attached to ribose). Both adenine and guanine are made from the precursor nucleoside inosine monophosphate, which is synthesized using atoms from the amino acids glycine, glutamine, and aspartic acid, as well as formate transferred from the coenzyme tetrahydrofolate. Pyrimidines, on the other hand, are synthesized from the base orotate, which is formed from glutamine and aspartate.

All organisms are constantly exposed to compounds that they cannot use as foods and would be harmful if they accumulated in cells, as they have no metabolic function. These potentially damaging compounds are called xenobiotics. Xenobiotics such as synthetic drugs, natural poisons and antibiotics are detoxified by a set of xenobiotic-metabolizing enzymes. In humans, these include cytochrome P450 oxidases, UDP-glucuronosyltransferases, and glutathione "S"-transferases. This system of enzymes acts in three stages to firstly oxidize the xenobiotic (phase I) and then conjugate water-soluble groups onto the molecule (phase II). The modified water-soluble xenobiotic can then be pumped out of cells and in multicellular organisms may be further metabolized before being excreted (phase III). In ecology, these reactions are particularly important in microbial biodegradation of pollutants and the bioremediation of contaminated land and oil spills. Many of these microbial reactions are shared with multicellular organisms, but due to the incredible diversity of types of microbes these organisms are able to deal with a far wider range of xenobiotics than multicellular organisms, and can degrade even persistent organic pollutants such as organochloride compounds.

A related problem for aerobic organisms is oxidative stress. Here, processes including oxidative phosphorylation and the formation of disulfide bonds during protein folding produce reactive oxygen species such as hydrogen peroxide. These damaging oxidants are removed by antioxidant metabolites such as glutathione and enzymes such as catalases and peroxidases.

Living organisms must obey the laws of thermodynamics, which describe the transfer of heat and work. The second law of thermodynamics states that in any closed system, the amount of entropy (disorder) cannot decrease. Although living organisms' amazing complexity appears to contradict this law, life is possible as all organisms are open systems that exchange matter and energy with their surroundings. Thus living systems are not in equilibrium, but instead are dissipative systems that maintain their state of high complexity by causing a larger increase in the entropy of their environments. The metabolism of a cell achieves this by coupling the spontaneous processes of catabolism to the non-spontaneous processes of anabolism. In thermodynamic terms, metabolism maintains order by creating disorder.

As the environments of most organisms are constantly changing, the reactions of metabolism must be finely regulated to maintain a constant set of conditions within cells, a condition called homeostasis. Metabolic regulation also allows organisms to respond to signals and interact actively with their environments. Two closely linked concepts are important for understanding how metabolic pathways are controlled. Firstly, the "regulation" of an enzyme in a pathway is how its activity is increased and decreased in response to signals. Secondly, the "control" exerted by this enzyme is the effect that these changes in its activity have on the overall rate of the pathway (the flux through the pathway). For example, an enzyme may show large changes in activity ("i.e." it is highly regulated) but if these changes have little effect on the flux of a metabolic pathway, then this enzyme is not involved in the control of the pathway.
There are multiple levels of metabolic regulation. In intrinsic regulation, the metabolic pathway self-regulates to respond to changes in the levels of substrates or products; for example, a decrease in the amount of product can increase the flux through the pathway to compensate. This type of regulation often involves allosteric regulation of the activities of multiple enzymes in the pathway. Extrinsic control involves a cell in a multicellular organism changing its metabolism in response to signals from other cells. These signals are usually in the form of soluble messengers such as hormones and growth factors and are detected by specific receptors on the cell surface. These signals are then transmitted inside the cell by second messenger systems that often involved the phosphorylation of proteins.

A very well understood example of extrinsic control is the regulation of glucose metabolism by the hormone insulin. Insulin is produced in response to rises in blood glucose levels. Binding of the hormone to insulin receptors on cells then activates a cascade of protein kinases that cause the cells to take up glucose and convert it into storage molecules such as fatty acids and glycogen. The metabolism of glycogen is controlled by activity of phosphorylase, the enzyme that breaks down glycogen, and glycogen synthase, the enzyme that makes it. These enzymes are regulated in a reciprocal fashion, with phosphorylation inhibiting glycogen synthase, but activating phosphorylase. Insulin causes glycogen synthesis by activating protein phosphatases and producing a decrease in the phosphorylation of these enzymes.

The central pathways of metabolism described above, such as glycolysis and the citric acid cycle, are present in all three domains of living things and were present in the last universal common ancestor. This universal ancestral cell was prokaryotic and probably a methanogen that had extensive amino acid, nucleotide, carbohydrate and lipid metabolism. The retention of these ancient pathways during later evolution may be the result of these reactions having been an optimal solution to their particular metabolic problems, with pathways such as glycolysis and the citric acid cycle producing their end products highly efficiently and in a minimal number of steps. The first pathways of enzyme-based metabolism may have been parts of purine nucleotide metabolism, while previous metabolic pathways were a part of the ancient RNA world.

Many models have been proposed to describe the mechanisms by which novel metabolic pathways evolve. These include the sequential addition of novel enzymes to a short ancestral pathway, the duplication and then divergence of entire pathways as well as the recruitment of pre-existing enzymes and their assembly into a novel reaction pathway. The relative importance of these mechanisms is unclear, but genomic studies have shown that enzymes in a pathway are likely to have a shared ancestry, suggesting that many pathways have evolved in a step-by-step fashion with novel functions created from pre-existing steps in the pathway. An alternative model comes from studies that trace the evolution of proteins' structures in metabolic networks, this has suggested that enzymes are pervasively recruited, borrowing enzymes to perform similar functions in different metabolic pathways (evident in the MANET database) These recruitment processes result in an evolutionary enzymatic mosaic. A third possibility is that some parts of metabolism might exist as "modules" that can be reused in different pathways and perform similar functions on different molecules.

As well as the evolution of new metabolic pathways, evolution can also cause the loss of metabolic functions. For example, in some parasites metabolic processes that are not essential for survival are lost and preformed amino acids, nucleotides and carbohydrates may instead be scavenged from the host. Similar reduced metabolic capabilities are seen in endosymbiotic organisms.

Classically, metabolism is studied by a reductionist approach that focuses on a single metabolic pathway. Particularly valuable is the use of radioactive tracers at the whole-organism, tissue and cellular levels, which define the paths from precursors to final products by identifying radioactively labelled intermediates and products. The enzymes that catalyze these chemical reactions can then be purified and their kinetics and responses to inhibitors investigated. A parallel approach is to identify the small molecules in a cell or tissue; the complete set of these molecules is called the metabolome. Overall, these studies give a good view of the structure and function of simple metabolic pathways, but are inadequate when applied to more complex systems such as the metabolism of a complete cell.

An idea of the complexity of the metabolic networks in cells that contain thousands of different enzymes is given by the figure showing the interactions between just 43 proteins and 40 metabolites to the right: the sequences of genomes provide lists containing anything up to 45,000 genes. However, it is now possible to use this genomic data to reconstruct complete networks of biochemical reactions and produce more holistic mathematical models that may explain and predict their behavior. These models are especially powerful when used to integrate the pathway and metabolite data obtained through classical methods with data on gene expression from proteomic and DNA microarray studies. Using these techniques, a model of human metabolism has now been produced, which will guide future drug discovery and biochemical research. These models are now used in network analysis, to classify human diseases into groups that share common proteins or metabolites.

Bacterial metabolic networks are a striking example of bow-tie organization, an architecture able to input a wide range of nutrients and produce a large variety of products and complex macromolecules using a relatively few intermediate common currencies.

A major technological application of this information is metabolic engineering. Here, organisms such as yeast, plants or bacteria are genetically modified to make them more useful in biotechnology and aid the production of drugs such as antibiotics or industrial chemicals such as 1,3-propanediol and shikimic acid. These genetic modifications usually aim to reduce the amount of energy used to produce the product, increase yields and reduce the production of wastes.

The term "metabolism" is derived from the Greek Μεταβολισμός – "Metabolismos" for "change", or "overthrow".
Aristotle's "The Parts of Animals" sets out enough details of his views on metabolism for an open flow model to be made. He believed that at each stage of the process, materials from food were transformed, with heat being released as the classical element of fire, and residual materials being excreted as urine, bile, or faeces.

Ibn al-Nafis described metabolism in his 1260 AD work titled Al-Risalah al-Kamiliyyah fil Siera al-Nabawiyyah (The Treatise of Kamil on the Prophet's Biography) which included the following phrase "Both the body and its parts are in a continuous state of dissolution and nourishment, so they are inevitably undergoing permanent change." 
The history of the scientific study of metabolism spans several centuries and has moved from examining whole animals in early studies, to examining individual metabolic reactions in modern biochemistry. The first controlled experiments in human metabolism were published by Santorio Santorio in 1614 in his book "Ars de statica medicina". He described how he weighed himself before and after eating, sleep, working, sex, fasting, drinking, and excreting. He found that most of the food he took in was lost through what he called "insensible perspiration".

In these early studies, the mechanisms of these metabolic processes had not been identified and a vital force was thought to animate living tissue. In the 19th century, when studying the fermentation of sugar to alcohol by yeast, Louis Pasteur concluded that fermentation was catalyzed by substances within the yeast cells he called "ferments". He wrote that "alcoholic fermentation is an act correlated with the life and organization of the yeast cells, not with the death or putrefaction of the cells." This discovery, along with the publication by Friedrich Wöhler in 1828 of a paper on the chemical synthesis of urea, and is notable for being the first organic compound prepared from wholly inorganic precursors. This proved that the organic compounds and chemical reactions found in cells were no different in principle than any other part of chemistry.

It was the discovery of enzymes at the beginning of the 20th century by Eduard Buchner that separated the study of the chemical reactions of metabolism from the biological study of cells, and marked the beginnings of biochemistry. The mass of biochemical knowledge grew rapidly throughout the early 20th century. One of the most prolific of these modern biochemists was Hans Krebs who made huge contributions to the study of metabolism. He discovered the urea cycle and later, working with Hans Kornberg, the citric acid cycle and the glyoxylate cycle. Modern biochemical research has been greatly aided by the development of new techniques such as chromatography, X-ray diffraction, NMR spectroscopy, radioisotopic labelling, electron microscopy and molecular dynamics simulations. These techniques have allowed the discovery and detailed analysis of the many molecules and metabolic pathways in cells.


Introductory

Advanced

General information

Human metabolism

Databases

Metabolic pathways


</doc>
<doc id="20375" url="https://en.wikipedia.org/wiki?curid=20375" title="Medieval Inquisition">
Medieval Inquisition

The Medieval Inquisition was a series of Inquisitions (Catholic Church bodies charged with suppressing heresy) from around 1184, including the Episcopal Inquisition (1184–1230s) and later the Papal Inquisition (1230s). The Medieval Inquisition was established in response to movements considered apostate or heretical to Christianity, in particular Catharism and Waldensians in Southern France and Northern Italy. These were the first inquisition movements of many that would follow.

The Cathars were first noted in the 1140s in Southern France, and the Waldensians around 1170 in Northern Italy. Before this point, individual heretics such as Peter of Bruis had often challenged the Church. However, the Cathars were the first mass organization in the second millennium that posed a serious threat to the authority of the Church. This article covers only these early inquisitions, not the Roman Inquisition of the 16th century onwards, or the somewhat different phenomenon of the Spanish Inquisition of the late 15th century, which was under the control of the Spanish monarchy using local clergy. The Portuguese Inquisition of the 16th century and various colonial branches followed the same pattern.

An inquisition was a process that developed to investigate alleged instances of crimes. Its use in ecclesiastical courts was not at first directed to matters of heresy, but a broad assortment of offenses such as clandestine marriage and bigamy.

French historian Jean-Baptiste Guiraud (1866–1953) defined Medieval Inquisition as "... a system of repressive means, some of temporal and some others of spiritual kind, concurrently issued by ecclesiastical and civil authorities in order to protect religious orthodoxy and social order, both threatened by theological and social doctrines of heresy".

Bishop of Lincoln, Robert Grosseteste, defined heresy as "an opinion chosen by human perception, created by human reason, founded on the Scriptures, contrary to the teachings of the Church, publicly avowed, and obstinately defended." The fault was in the obstinate adherence rather than theological error, which could be corrected; and by referencing scripture Grosseteste excludes Jews, Muslims, and other non-Christians from the definition of heretic.

There were many different types of inquisitions depending on the location and methods; historians have generally classified them into the "episcopal inquisition" and the "papal inquisition". All major medieval inquisitions were decentralized, and each tribunal worked independently. Authority rested with local officials based on guidelines from the Holy See, but there was no central top-down authority running the inquisitions, as would be the case in post-medieval inquisitions.

Early Medieval courts generally followed a process called "accusatio", largely based on Germanic practices. In this procedure, an individual would make an accusation against someone to the court. However, if the suspect was judged innocent, the accusers faced legal penalties for bringing false charges. This provided a disincentive to make any accusation unless the accusers were sure it would stand. Later, a threshold requirement was the establishment of the accused's "publica fama", i.e., the fact that the person was widely believed to be guilty of the offense charged.

By the twelfth and early thirteenth centuries, there was a shift away from the accusatorial model toward the legal procedure used in the Roman Empire. Instead of an individual making accusations based on first-hand knowledge, judges now took on the prosecutorial role based on information collected. Under inquisitorial procedures, guilt or innocence was proved by the inquiry ("inquisitio") of the judge into the details of a case.

The mechanism for dealing with heresy developed gradually. Bishops had always the authority to look into alleged heretical activity, but as it wasn't always clear what constituted heresy they conferred with their colleagues and sought advice from Rome. Legates were sent out, at first as advisors, later taking a greater role in the administration. Procedures began to be formalized by time of Pope Gregory IX.

Practices and procedures of episcopal inquisitions could vary from one diocese to another, depending on the resources available to individual bishops and their relative interest or disinterest. Convinced that Church teaching contained revealed truth, the first recourse of bishops was that of "persuasio". Through discourse, debates, and preaching, they sought to present a better explanation of Church teaching. This approach often proved very successful.

In 1076 Pope Gregory VII excommunicated the residents of Cambrai because a mob had seized and burned a Cathar determined by the bishop to have been a heretic. A similar occurrence happened in 1114 during the bishops absence in Strassburg. In 1145 clergy at Leige managed to rescue victims from the crowd.

The first medieval inquisition, the episcopal inquisition, was established in the year 1184 by a papal bull of Pope Lucius III entitled "Ad abolendam", "For the purpose of doing away with." It was a response to the growing Catharist movement in southern France. It was called "episcopal" because it was administered by local bishops, which in Latin is "episcopus", and obliged bishops to visit their diocese twice a year in search of heretics.

The spread of other movements from the 12th century can be seen at least in part as a reaction to the increasing moral corruption of the clergy, which included illegal marriages and the possession of extreme wealth.
In the Middle Ages, the Inquisition's main focus was to eradicate these new sects. Thus its range of action was predominantly in Italy and France, where the Cathars and the Waldensians, the two main heretic movements of the period, were.

During the pontificates of Innocent III, papal legates were sent out to stop the spread of the Cathar and Waldensian heresies to Provence and up the Rhine into Germany.

The Cathars were a group of dissidents mostly in the South of France, in cities like Toulouse. The sect developed in the 12th century, apparently founded by soldiers from the Second Crusade, who, on their way back, were converted by a Bulgarian sect, the Bogomils.

The Cathars' main heresy was their belief in dualism: the evil God created the materialistic world and the good God created the spiritual world. Therefore, Cathars preached poverty, chastity, modesty and all those values which in their view helped people to detach themselves from materialism. The Cathars presented a problem to feudal government by their attitude towards oaths, which they declared under no circumstances allowable. Therefore, considering the religious homogeneity of that age, heresy was an attack against social and political order, besides orthodoxy.

The Albigensian Crusade resulted in the defeat of the Cathars militarily. After this, the Inquisition played an important role in finally destroying Catharism during the 13th and much of the 14th centuries. Punishments for Cathars varied greatly. Most frequently, they were made to wear yellow crosses atop their garments as a sign of outward penance. Others undertook obligatory pilgrimages, many for the purpose of fighting against Muslims. Another common punishment, including for returned pilgrims, was visiting a local church naked once each month to be scourged. Cathars who were slow to repent suffered imprisonment and, often, the loss of property. Others who altogether refused to repent were burned.

The Waldensians were mostly in Germany and North Italy. The Waldensians were a group of orthodox laymen concerned about the increasing wealth of the Church. As time passed, however, they found themselves stepping beyond the bounds of orthodoxy as defined by the hierarchy of the Western Church. In contrast with the Cathars and in line with the Church, they believed in only one God, but they did not recognize a special class of priesthood, believing in the priesthood of all believers. They also objected to the veneration of saints and martyrs, which were part of the Church's orthodoxy. They rejected the sacramental authority of the Church and its clerics and encouraged apostolic poverty. These movements became particularly popular in Southern France as well as Northern Italy and parts of Germany.

One reason for Pope Gregory IX's creation of the Inquisition was to bring order and legality to the process of dealing with heresy, since there had been tendencies by mobs of townspeople to burn alleged heretics without much of a trial. Pope Gregory's original intent for the Inquisition was a court of exception to inquire into and glean the beliefs of those differing from Catholic teaching, and to instruct them in the orthodox doctrine. It was hoped that heretics would see the falsity of their opinion and would return to the Roman Catholic Church. If they persisted in their heresy, however, Pope Gregory, finding it necessary to protect the Catholic community from infection, would have suspects handed over to civil authorities, since public heresy was a crime under civil law as well as Church law. The secular authorities would apply their own brands of punishment for civil disobedience which, at the time, included burning at the stake. Over centuries the tribunals took different forms, investigating and stamping out various forms of heresy, including witchcraft.

The complaints of the two main preaching orders of the period, the Dominicans and the Franciscans, against the moral corruption of the Church, to some extent echoed those of the heretical movements, but they were doctrinally conventional, and were enlisted by Pope Innocent III in the fight against heresy. As a result, many Franciscans and Dominicans became inquisitors. For example, Robert le Bougre, the "Hammer of Heretics" (Malleus Haereticorum), was a Dominican friar who became an inquisitor known for his cruelty and violence. Another example was the case of the province of Venice, which was handed to the Franciscan inquisitors, who quickly became notorious for their frauds against the Church, by enriching themselves with confiscated property from the heretics and the selling of absolutions. Because of their corruption, they were eventually forced by the Pope to suspend their activities in 1302.

In 1231 Pope Gregory IX appointed a number of Papal Inquisitors (Inquisitores haereticae pravitatis), mostly Dominicans and Franciscans, for the various regions of Europe. As mendicants, they were accustomed to travel. Unlike the haphazard episcopal methods, the papal inquisition was thorough and systematic, keeping detailed records. Some of the few documents from the Middle Ages involving first-person speech by medieval peasants come from papal inquisition records. This tribunal or court functioned in France, Italy and parts of Germany and had virtually ceased operation by the early fourteenth century.

Throughout the Inquisition's history, it was rivaled by local ecclesiastical and secular jurisdictions. No matter how determined, no pope succeeded in establishing complete control over the prosecution of heresy. Medieval kings, princes, bishops, and civil authorities all had a role in prosecuting heresy, except where they individually opposed the practice. The practice reached its apex in the second half of the 13th century. During this period, the tribunals were almost entirely free from any authority, including that of the pope. Therefore, it was almost impossible to eradicate abuse.

In southern Europe, Church-run courts existed in the kingdom of Aragon during the medieval period, but not elsewhere in the Iberian peninsula or some other kingdoms, including England. In Scandinavian kingdoms it had hardly any impact.

At the beginning of the fourteenth century, two other movements attracted the attention of the Inquisition, the Knights Templar and the Beguines.

It is not clear if the process against the Templars was initiated by the Inquisition on the basis of suspected heresy or if the Inquisition itself was exploited by the king of France, Philip the Fair, who wanted the knights' wealth. In the search for Templars, two inquisitors were also sent to the British Isles. This is the only instance of inquisitorial action in the British Isles and not a successful one, mainly because the inquisitors could not instigate false confessions through torture, as its use was forbidden by common law.

The Beguines were mainly a women's movement, recognized by the Church since their foundation in the thirteenth century as mystics. However, with the Council of Vienne in the fourteenth century, they were proclaimed heretics and persecuted, with large numbers being burned at the stake in Narbonne, Toulouse and other French cities. They were also attacked in Germany, the first attempt of the Inquisition to operate in the area.

Another aspect of the medieval Inquisition is that little attention was paid to sorcery. In fact several Popes were suspected of having a strong interest or practicing alchemy and it was only with Pope John XXII, who was himself suspected of being a magician, that sorcery became another form of heresy and thus liable to prosecution by the Inquisition.

According to historian Thomas Madden: The Inquisition was not born out of desire to crush diversity or oppress people; it was rather an attempt to stop unjust executions. Yes, you read that correctly. Heresy was a crime against the state. Roman law in the Code of Justinian made heresy a capital offense... [emphasis in original]In the early Middle Ages, people accused of heresy were judged by the local lord, many of whom lacked theological training. Madden claims that "The simple fact is that the medieval Inquisition "saved" uncounted thousands of innocent (and even not-so-innocent) people who would otherwise have been roasted by secular lords or mob rule" [emphasis in original].

Madden argues that while medieval secular leaders were trying to safeguard their kingdoms, the Church was trying to save souls. The Inquisition provided a means for heretics to escape death and return to the community.

In the spring of 1429 during the Hundred Years' War, in obedience to what she said was the command of God, Joan of Arc inspired the Dauphin's armies in a series of stunning military victories which lifted the siege of Orleans and destroyed a large percentage of the remaining English forces at the battle of Patay. A series of military setbacks eventually led to her capture in the Spring of 1430 by the Burgundians, who were allied with the English. They delivered her to them for 10,000 livres. In December of that same year she was transferred to Rouen, the military headquarters and administrative capital in France of King Henry VI of England, and placed on trial for heresy before a Church court headed by Bishop Pierre Cauchon, a supporter of the English.

The trial was politically motivated. Cauchon, although a native of France, had served in the English government since 1418, and he was therefore hostile to a woman who had worked for the opposing side. The same was true of the other tribunal members. Ascribing a diabolic origin to her victories would be an effective way to ruin her reputation and bolster the morale of English troops. Thus the decision to involve the Inquisition, which did not initiate the trial and in fact showed a reluctance throughout its duration. Seventy charges were brought against her, including accusations of heresy and dressing as a male (i.e., wearing soldiers' clothing and armor). Eyewitnesses later said that Joan had told them she was wearing this clothing and keeping it "firmly laced and tied together" because the tunic could be tied to the long boots to keep her guards from pulling her clothing off during their occasional attempts to rape her. Joan was first condemned to life imprisonment and the deputy-inquisitor, Jean Le Maitre (whom the eyewitness said only attended because of threats from the English) obtained from her assurances of relinquishing her male clothes. However, after four days, during which she was said to have been subjected to attempted rape by English soldiers, she put her soldier's clothing back on because (according to the eyewitnesses) she needed protection against rape. Cauchon declared her a relapsed heretic, and she was burned at the stake two days later on 30 May 1431.

In 1455, a petition by Joan of Arc's mother Isabelle led to a re-trial designed to investigate the dubious circumstances which led to Joan's execution. The Inquisitor-General of France was put in charge of the new trial, which opened in Notre Dame de Paris on 7 November 1455. After analyzing all the proceedings, including Joan's answers to the allegations and the testimony of 115 witnesses who were called to testify during the appellate process, the inquisitor overturned her condemnation on 7 July 1456. Joan of Arc was eventually canonized in 1920.

Historian Edward Peters identifies a number of illegalities in Joan's first trial in which she had been convicted.

The papal inquisition developed a number of procedures to discover and prosecute heretics. These codes and procedures detailed how an inquisitorial court was to function. If the accused renounced their heresy and returned to the Church, forgiveness was granted and a penance was imposed. If the accused upheld their heresy, they were excommunicated and turned over to secular authorities. The penalties for heresy, though not as severe as the secular courts of Europe at the time, were codified within the ecclesiastic courts as well (e.g. confiscation of property, turning heretics over to the secular courts for punishment). Additionally, the various "key terms" of the inquisitorial courts were defined at this time, including, for example, "heretics," “believers," “those suspect of heresy," “those simply suspected," “those vehemently suspected," and "those most vehemently suspected".

Legally, there had to be at least two witnesses, although conscientious judges rarely contented themselves with that number.

First, the townspeople would be gathered in a public place. Although attendance was voluntary, those who failed to show would automatically be suspect, so most would come. The inquisitors would provide an opportunity for anyone to step forward and denounce themselves in exchange for easy punishment. As part of this bargain they would need to inform on other heretics.

The inquisitorial trial generally favored the prosecution (the Church). Confessing 'in full' was the best hope of receiving a lighter punishment - but with little hope of escaping at least some punishment. And a 'full' confession was one which implicated others, including other family members. It was acceptable to take testimony from criminals, persons of bad reputation, excommunicated people, and convicted heretics. The inquisitor could keep a defendant in prison for years before the trial to obtain new information, and could return them to prison if he felt that the witness had not fully confessed.

Despite the unfairness of the procedures, the inquisitors did provide some rights to the defendant. At the beginning of the trial, defendants were invited to name those who had "mortal hatred" against them. If the accusers were among those named, the defendant was set free and the charges dismissed; the accusers would face life imprisonment. This option was meant to keep the inquisition from becoming involved in local grudges. Early legal consultations on conducting inquisition stress that it is better that the guilty go free than that the innocent be punished. Gregory IX urged Conrad of Marburg: "ut puniatur sic temeritas perversorum quod innocentiae puritas non laedatur" — i.e., "not to punish the wicked so as to hurt the innocent".

Like the inquisitorial process itself, torture was an ancient Roman legal practice commonly used in secular courts.
On May 15, 1252, Pope Innocent IV issued a papal bull entitled "Ad extirpanda", which authorized the limited use of torture by inquisitors. Much of the brutality commonly associated with the Inquisition was actually previously common in secular courts, but prohibited under the Inquisition, including torture methods that resulted in bloodshed, miscarriages, mutilation or death. Also, torture could be performed only once, and for a limited duration.

In preparation for the Jubilee in 2000, the Vatican opened the archives of the Holy Office (the modern successor to the Inquisition) to a team of 30 scholars from around the world. According to the governor general of the Order of the Holy Sepulchre, recent studies "seem to indicate" that "torture and the death penalty were not applied with the pitiless rigor" often ascribed to the Inquisition. Other methods such as threats and imprisonment seem to have proven more effective.

A council in Tours in 1164, presided over by Pope Alexander III, ordered the confiscation of a heretics goods. Of 5,400 people interrogated in Toulouse between 1245–1246, 184 received penitential yellow crosses, 23 were imprisoned for life, and none were sent to the stake.

The most extreme penalty available in antiheretical proceedings was reserved for relapsed or stubborn heretics. The unrepentant and apostates could be "relaxed" to secular authority, however, opening the convicted to the possibility of various corporal punishments, up to and including being burned at the stake. Execution was neither performed by the Church, nor was it a sentence available to the officials involved in the inquisition, who, as clerics, were forbidden to kill. The accused also faced the possibility that his or her property might be confiscated. In some cases, accusers may have been motivated by a desire to take the property of the accused, though this is a difficult assertion to prove in the majority of areas where the inquisition was active, as the inquisition had several layers of oversight built into its framework in a specific attempt to limit prosecutorial misconduct.

The inquisitors generally preferred not to hand over heretics to the secular arm for execution if they could persuade the heretic to repent: "Ecclesia non novit sanguinem". For example, under Bernard Gui, a famous inquisitor working in the area of Carcassonne (in modern France), out of over 900 guilty verdicts in fifteen years of office, 42 people ended up executed.

By the 14th century the Waldensians had been driven underground. Some residents of the Pays Cathare identify themselves as Cathars even today. They claim to be descended from the Cathars of the Middle Ages. However, the delivering of the consolamentum, on which historical Catharism was based, required a linear succession by a bon homme in good standing. It is believed that one of the last known bons hommes, Guillaume Belibaste, was burned in 1321.






</doc>
<doc id="20377" url="https://en.wikipedia.org/wiki?curid=20377" title="Microorganism">
Microorganism

A microorganism, or microbe, is a microscopic organism, which may exist in its single-celled form or in a colony of cells.

The possible existence of unseen microbial life was suspected from ancient times, such as in Jain scriptures from 6th century BC India and the 1st century BC book "On Agriculture" by Marcus Terentius Varro. Microbiology, the scientific study of microorganisms, began with their observation under the microscope in the 1670s by Antonie van Leeuwenhoek. In the 1850s, Louis Pasteur found that microorganisms caused food spoilage, debunking the theory of spontaneous generation. In the 1880s Robert Koch discovered that microorganisms caused the diseases tuberculosis, cholera and anthrax.

Microorganisms include all unicellular organisms and so are extremely diverse. Of the three domains of life identified by Carl Woese, all of the Archaea and Bacteria are microorganisms. These were previously grouped together in the two domain system as Prokaryotes, the other being the eukaryotes. The third domain Eukaryota includes all multicellular organisms and many unicellular protists and protozoans. Some protists are related to animals and some to green plants. Many of the multicellular organisms are microscopic, namely micro-animals, some fungi and some algae, but these are not discussed here.

They live in almost every habitat from the poles to the equator, deserts, geysers, rocks and the deep sea. Some are adapted to extremes such as very hot or very cold conditions, others to high pressure and a few such as "Deinococcus radiodurans" to high radiation environments. Microorganisms also make up the microbiota found in and on all multicellular organisms. A December 2017 report stated that 3.45 billion year old Australian rocks once contained microorganisms, the earliest direct evidence of life on Earth.

Microbes are important in human culture and health in many ways, serving to ferment foods, treat sewage, produce fuel, enzymes and other bioactive compounds. They are essential tools in biology as model organisms and have been put to use in biological warfare and bioterrorism. They are a vital component of fertile soils. In the human body microorganisms make up the human microbiota including the essential gut flora. They are the pathogens responsible for many infectious diseases and as such are the target of hygiene measures.

The possible existence of microorganisms was discussed for many centuries before their discovery in the 17th century. The existence of unseen microbial life was postulated by Jainism. In the 6th century BC, Mahavira asserted the existence of unseen microbiological creatures living in earth, water, air and fire. The Jain scriptures also describe nigodas, as sub-microscopic creatures living in large clusters and having a very short life, which were said to pervade every part of the universe, even the tissues of plants and animals. The earliest known idea to indicate the possibility of diseases spreading by yet unseen organisms was that of the Roman scholar Marcus Terentius Varro in a 1st-century BC book titled "On Agriculture" in which he called the unseen creatures animalcules, and warns against locating a homestead near a swamp:

In "The Canon of Medicine" (1020), Avicenna suggested that tuberculosis and other diseases might be contagious.

Akshamsaddin (Turkish scientist) mentioned the microbe in his work "Maddat ul-Hayat" (The Material of Life) about two centuries prior to Antonie Van Leeuwenhoek's discovery through experimentation:
In 1546, Girolamo Fracastoro proposed that epidemic diseases were caused by transferable seedlike entities that could transmit infection by direct or indirect contact, or even without contact over long distances.

Antonie Van Leeuwenhoek is considered to be the father of microbiology. He was the first in 1673 to discover, observe, describe, study and conduct scientific experiments with microoorganisms, using simple single-lensed microscopes of his own design. Robert Hooke, a contemporary of Leeuwenhoek, also used microscopy to observe microbial life in the form of the fruiting bodies of moulds. In his 1665 book "Micrographia", he made drawings of studies, and he coined the term "cell".

Louis Pasteur (1822–1895) exposed boiled broths to the air, in vessels that contained a filter to prevent particles from passing through to the growth medium, and also in vessels without a filter, but with air allowed in via a curved tube so dust particles would settle and not come in contact with the broth. By boiling the broth beforehand, Pasteur ensured that no microorganisms survived within the broths at the beginning of his experiment. Nothing grew in the broths in the course of Pasteur's experiment. This meant that the living organisms that grew in such broths came from outside, as spores on dust, rather than spontaneously generated within the broth. Thus, Pasteur dealt the death blow to the theory of spontaneous generation and supported the germ theory of disease.

In 1876, Robert Koch (1843–1910) established that microorganisms can cause disease. He found that the blood of cattle which were infected with anthrax always had large numbers of "Bacillus anthracis". Koch found that he could transmit anthrax from one animal to another by taking a small sample of blood from the infected animal and injecting it into a healthy one, and this caused the healthy animal to become sick. He also found that he could grow the bacteria in a nutrient broth, then inject it into a healthy animal, and cause illness. Based on these experiments, he devised criteria for establishing a causal link between a microorganism and a disease and these are now known as Koch's postulates. Although these postulates cannot be applied in all cases, they do retain historical importance to the development of scientific thought and are still being used today.

The discovery of microorganisms such as "Euglena" that did not fit into either the animal or plant kingdoms, since they were photosynthetic like plants, but motile like animals, led to the naming of a third kingdom in the 1860s. In 1860 John Hogg called this the Protoctista, and in 1866 Ernst Haeckel named it the Protista.

The work of Pasteur and Koch did not accurately reflect the true diversity of the microbial world because of their exclusive focus on microorganisms having direct medical relevance. It was not until the work of Martinus Beijerinck and Sergei Winogradsky late in the 19th century that the true breadth of microbiology was revealed. Beijerinck made two major contributions to microbiology: the discovery of viruses and the development of enrichment culture techniques. While his work on the Tobacco Mosaic Virus established the basic principles of virology, it was his development of enrichment culturing that had the most immediate impact on microbiology by allowing for the cultivation of a wide range of microbes with wildly different physiologies. Winogradsky was the first to develop the concept of chemolithotrophy and to thereby reveal the essential role played by microorganisms in geochemical processes. He was responsible for the first isolation and description of both nitrifying and nitrogen-fixing bacteria. French-Canadian microbiologist Felix d'Herelle co-discovered bacteriophages and was one of the earliest applied microbiologists.

Microorganisms can be found almost anywhere on Earth. Bacteria and archaea are almost always microscopic, while a number of eukaryotes are also microscopic, including most protists, some fungi, as well as some micro-animals and plants. Viruses are generally regarded as not living and therefore not considered as microorganisms, although a subfield of microbiology is virology, the study of viruses.

Single-celled microorganisms were the first forms of life to develop on Earth, approximately 3–4 billion years ago. Further evolution was slow, and for about 3 billion years in the Precambrian eon, (much of the history of life on Earth), all organisms were microorganisms. Bacteria, algae and fungi have been identified in amber that is 220 million years old, which shows that the morphology of microorganisms has changed little since the Triassic period. The newly discovered biological role played by nickel, however — especially that brought about by volcanic eruptions from the Siberian Traps — may have accelerated the evolution of methanogens towards the end of the Permian–Triassic extinction event.

Microorganisms tend to have a relatively fast rate of evolution. Most microorganisms can reproduce rapidly, and bacteria are also able to freely exchange genes through conjugation, transformation and transduction, even between widely divergent species. This horizontal gene transfer, coupled with a high mutation rate and other means of transformation, allows microorganisms to swiftly evolve (via natural selection) to survive in new environments and respond to environmental stresses. This rapid evolution is important in medicine, as it has led to the development of multidrug resistant pathogenic bacteria, "superbugs", that are resistant to antibiotics.

A possible transitional form of microorganism between a prokaryote and a eukaryote was discovered in 2012 by Japanese scientists. "Parakaryon myojinensis" is a unique microorganism larger than a typical prokaryote, but with nuclear material enclosed in a membrane as in a eukaryote, and the presence of endosymbionts. This is seen to be the first plausible evolutionary form of microorganism, showing a stage of development from the prokaryote to the eukaryote.

Archaea are prokaryotic unicellular organisms, and form the first domain of life, in Carl Woese's three-domain system. A prokaryote is defined as having no cell nucleus or other membrane bound-organelle. Archaea share this defining feature with the bacteria with which they were once grouped. In 1990 the microbiologist Woese proposed the three-domain system that divided living things into bacteria, archaea and eukaryotes, and thereby split the prokaryote domain.

Archaea differ from bacteria in both their genetics and biochemistry. For example, while bacterial cell membranes are made from phosphoglycerides with ester bonds, archaean membranes are made of ether lipids. Archaea were originally described as extremophiles living in extreme environments, such as hot springs, but have since been found in all types of habitats. Only now are scientists beginning to realize how common archaea are in the environment, with Crenarchaeota being the most common form of life in the ocean, dominating ecosystems below 150 m in depth. These organisms are also common in soil and play a vital role in ammonia oxidation.

The combined domains of archaea and bacteria make up the most diverse and abundant group of organisms on Earth and inhabit practically all environments where the temperature is below +140 °C. They are found in water, soil, air, as the microbiome of an organism, hot springs and even deep beneath the Earth's crust in rocks. The number of prokaryotes is estimated to be around five million trillion trillion, or 5 × 10, accounting for at least half the biomass on Earth.

The biodiversity of the prokaryotes is unknown, but may be very large. A May 2016 estimate, based on laws of scaling from known numbers of species against the size of organism, gives an estimate of perhaps 1 trillion species on the planet, of which most would be microorganisms. Currently, only one-thousandth of one percent of that total have been described.

Bacteria like archaea are prokaryotic – unicellular, and having no cell nucleus or other membrane-bound organelle. Bacteria are microscopic, with a few extremely rare exceptions, such as "Thiomargarita namibiensis". Bacteria function and reproduce as individual cells, but they can often aggregate in multicellular colonies. Some species such as myxobacteria can aggregate into complex swarming structures, operating as multicellular groups as part of their life cycle, or form clusters in bacterial colonies such as "E.coli".

Their genome is usually a circular bacterial chromosome – a single loop of DNA, although they can also harbor small pieces of DNA called plasmids. These plasmids can be transferred between cells through bacterial conjugation. Bacteria have an enclosing cell wall, which provides strength and rigidity to their cells. They reproduce by binary fission or sometimes by budding, but do not undergo meiotic sexual reproduction. However, many bacterial species can transfer DNA between individual cells by a horizontal gene transfer process referred to as natural transformation. Some species form extraordinarily resilient spores, but for bacteria this is a mechanism for survival, not reproduction. Under optimal conditions bacteria can grow extremely rapidly and their numbers can double as quickly as every 20 minutes.

Most living things that are visible to the naked eye in their adult form are eukaryotes, including humans. However, a large number of eukaryotes are also microorganisms. Unlike bacteria and archaea, eukaryotes contain organelles such as the cell nucleus, the Golgi apparatus and mitochondria in their cells. The nucleus is an organelle that houses the DNA that makes up a cell's genome. DNA (Deoxyribonucleic acid) itself is arranged in complex chromosomes.
Mitochondria are organelles vital in metabolism as they are the site of the citric acid cycle and oxidative phosphorylation. They evolved from symbiotic bacteria and retain a remnant genome. Like bacteria, plant cells have cell walls, and contain organelles such as chloroplasts in addition to the organelles in other eukaryotes. Chloroplasts produce energy from light by photosynthesis, and were also originally symbiotic bacteria.

Unicellular eukaryotes consist of a single cell throughout their life cycle. This qualification is significant since most multicellular eukaryotes consist of a single cell called a zygote only at the beginning of their life cycles. Microbial eukaryotes can be either haploid or diploid, and some organisms have multiple cell nuclei.

Unicellular eukaryotes usually reproduce asexually by mitosis under favorable conditions. However, under stressful conditions such as nutrient limitations and other conditions associated with DNA damage, they tend to reproduce sexually by meiosis and syngamy.

Of eukaryotic groups, the protists are most commonly unicellular and microscopic. This is a highly diverse group of organisms that are not easy to classify. Several algae species are multicellular protists, and slime molds have unique life cycles that involve switching between unicellular, colonial, and multicellular forms. The number of species of protists is unknown since only a small proportion has been identified. Protist diversity is high in oceans, deep sea-vents, river sediment and an acidic river, suggesting that many eukaryotic microbial communities may yet be discovered.

The fungi have several unicellular species, such as baker's yeast ("Saccharomyces cerevisiae") and fission yeast ("Schizosaccharomyces pombe"). Some fungi, such as the pathogenic yeast "Candida albicans", can undergo phenotypic switching and grow as single cells in some environments, and filamentous hyphae in others.

The green algae are a large group of photosynthetic eukaryotes that include many microscopic organisms. Although some green algae are classified as protists, others such as charophyta are classified with embryophyte plants, which are the most familiar group of land plants. Algae can grow as single cells, or in long chains of cells. The green algae include unicellular and colonial flagellates, usually but not always with two flagella per cell, as well as various colonial, coccoid, and filamentous forms. In the Charales, which are the algae most closely related to higher plants, cells differentiate into several distinct tissues within the organism. There are about 6000 species of green algae.

Microorganisms are found in almost every habitat present in nature, including hostile environments such as the North and South poles, deserts, geysers, and rocks. They also include all the marine microorganisms of the oceans and deep sea. Some types of microorganisms have adapted to extreme environments and sustained colonies; these organisms are known as extremophiles. Extremophiles have been isolated from rocks as much as 7 kilometres below the Earth's surface, and it has been suggested that the amount of organisms living below the Earth's surface is comparable with the amount of life on or above the surface. Extremophiles have been known to survive for a prolonged time in a vacuum, and can be highly resistant to radiation, which may even allow them to survive in space. Many types of microorganisms have intimate symbiotic relationships with other larger organisms; some of which are mutually beneficial (mutualism), while others can be damaging to the host organism (parasitism). If microorganisms can cause disease in a host they are known as pathogens and then they are sometimes referred to as "microbes".
Microorganisms play critical roles in Earth's biogeochemical cycles as they are responsible for decomposition and nitrogen fixation.

Bacteria use regulatory networks that allow them to adapt to almost every environmental niche on earth. A network of interactions among diverse types of molecules including DNA, RNA, proteins and metabolites, is utilised by the bacteria to achieve regulation of gene expression. In bacteria, the principal function of regulatory networks is to control the response to environmental changes, for example nutritional status and environmental stress. A complex organization of networks permits the microorganism to coordinate and integrate multiple environmental signals.

Extremophiles are microorganisms that have adapted so that they can survive and even thrive in extreme environments that are normally fatal to most life-forms. Thermophiles and hyperthermophiles thrive in high temperatures. Psychrophiles thrive in extremely low temperatures. – Temperatures as high as , as low as Halophiles such as "Halobacterium salinarum" (an archaean) thrive in high salt conditions, up to saturation. Alkaliphiles thrive in an alkaline pH of about 8.5–11. Acidophiles can thrive in a pH of 2.0 or less. Piezophiles thrive at very high pressures: up to 1,000–2,000 atm, down to 0 atm as in a vacuum of space. A few extremophiles such as "Deinococcus radiodurans" are radioresistant, resisting radiation exposure of up to 5k Gy. Extremophiles are significant in different ways. They extend terrestrial life into much of the Earth's hydrosphere, crust and atmosphere, their specific evolutionary adaptation mechanisms to their extreme environment can be exploited in biotechnology, and their very existence under such extreme conditions increases the potential for extraterrestrial life.

The nitrogen cycle in soils depends on the fixation of atmospheric nitrogen. This is achieved by a number of diazotrophs. One way this can occur is in the nodules in the roots of legumes that contain symbiotic bacteria of the genera "Rhizobium", "Mesorhizobium", "Sinorhizobium", "Bradyrhizobium", and "Azorhizobium".

A lichen is a symbiosis of a macroscopic fungus with photosynthetic microbial algae or cyanobacteria.

Microorganisms are useful in producing foods, treating waste water, creating biofuels and a wide range of chemicals and enzymes. They are invaluable in research as model organisms. They have been weaponised and sometimes used in warfare and bioterrorism. They are vital to agriculture through their roles in maintaining soil fertility and in decomposing organic matter.

Microorganisms are used in a fermentation process to make yoghurt, cheese, curd, kefir, ayran, xynogala, and other types of food. Fermentation cultures provide flavor and aroma, and inhibit undesirable organisms. They are used to leaven bread, and to convert sugars to alcohol in wine and beer. Microorganisms are used in brewing, wine making, baking, pickling and other food-making processes.

Sewage treatment works depend for their ability to clean up water contaminated with organic material on microorganisms that can respire dissolved substances. Respiration may be aerobic, with a well-oxygenated filter bed such as a slow sand filter. Anaerobic digestion by methanogens generate useful methane gas as a by-product.

Microorganisms are used in fermentation to produce ethanol, and in biogas reactors to produce methane. Scientists are researching the use of algae to produce liquid fuels, and bacteria to convert various forms of agricultural and urban waste into usable fuels.

Microorganisms are used to produce many commercial and industrial chemicals, enzymes and other bioactive molecules. Organic acids produced on a large industrial scale by microbial fermentation include acetic acid produced by acetic acid bacteria such as "Acetobacter aceti", butyric acid made by the bacterium "Clostridium butyricum", lactic acid made by "Lactobacillus" and other lactic acid bacteria, and citric acid produced by the mould fungus "Aspergillus niger". 

Microorganisms are used to prepare bioactive molecules such as Streptokinase from the bacterium "Streptococcus", Cyclosporin A from the ascomycete fungus "Tolypocladium inflatum", and statins produced by the yeast "Monascus purpureus".

Microorganisms are essential tools in biotechnology, biochemistry, genetics, and molecular biology. The yeasts "Saccharomyces cerevisiae" and "Schizosaccharomyces pombe" are important model organisms in science, since they are simple eukaryotes that can be grown rapidly in large numbers and are easily manipulated. They are particularly valuable in genetics, genomics and proteomics. Microorganisms can be harnessed for uses such as creating steroids and treating skin diseases. Scientists are also considering using microorganisms for living fuel cells, and as a solution for pollution.

In the Middle Ages, as an early example of biological warfare, diseased corpses were thrown into castles during sieges using catapults or other siege engines. Individuals near the corpses were exposed to the pathogen and were likely to spread that pathogen to others.
In modern times, bioterrorism has included the 1984 Rajneeshee bioterror attack and the 1993 release of anthrax by Aum Shinrikyo in Tokyo.

Microbes can make nutrients and minerals in the soil available to plants, produce hormones that spur growth, stimulate the plant immune system and trigger or dampen stress responses. In general a more diverse set of soil microbes results in fewer plant diseases and higher yield.

Microorganisms can form an endosymbiotic relationship with other, larger organisms. For example, microbial symbiosis plays a crucial role in the immune system. The microorganisms that make up the gut flora in the gastrointestinal tract contribute to gut immunity, synthesize vitamins such as folic acid and biotin, and ferment complex indigestible carbohydrates. Some microorganisms that are seen to be beneficial to health are termed probiotics and are available as dietary supplements, or food additives.

Microorganisms are the causative agents (pathogens) in many infectious diseases. The organisms involved include pathogenic bacteria, causing diseases such as plague, tuberculosis and anthrax; protozoan parasites, causing diseases such as malaria, sleeping sickness, dysentery and toxoplasmosis; and also fungi causing diseases such as ringworm, candidiasis or histoplasmosis. However, other diseases such as influenza, yellow fever or AIDS are caused by pathogenic viruses, which are not usually classified as living organisms and are not, therefore, microorganisms by the strict definition. No clear examples of archaean pathogens are known, although a relationship has been proposed between the presence of some archaean methanogens and human periodontal disease.

Hygiene is a set of practices to avoid infection or food spoilage by eliminating microorganisms from the surroundings. As microorganisms, in particular bacteria, are found virtually everywhere, harmful microorganisms may be reduced to acceptable levels rather than actually eliminated. In food preparation, microorganisms are reduced by preservation methods such as cooking, cleanliness of utensils, short storage periods, or by low temperatures. If complete sterility is needed, as with surgical equipment, an autoclave is used to kill microorganisms with heat and pressure.



</doc>
<doc id="20378" url="https://en.wikipedia.org/wiki?curid=20378" title="Modulus">
Modulus

Modulus is the diminutive from the Latin word "modus" meaning measure or manner. It, or its plural moduli, may refer to the following:






</doc>
<doc id="20379" url="https://en.wikipedia.org/wiki?curid=20379" title="Micronation">
Micronation

A micronation, sometimes referred to as a model country or new country project, is an entity that claims to be an independent nation or state but is not recognized by world governments or major international organizations.

Micronations are distinguished from imaginary countries and from other kinds of social groups (such as eco-villages, campuses, tribes, clans, sects, and residential community associations) by expressing a formal and persistent, even if unrecognized, claim of sovereignty over some physical territory. Micronations are also distinct from true secessionist movements; micronations' activities are almost always trivial enough to be ignored rather than challenged by the established nations whose territory they claim.

Several micronations have issued coins, flags, postage stamps, passports, medals, and other items. These items are rarely accepted outside their own community but may be sold as novelties to help raise money or collected by enthusiasts.

The earliest known micronations date from the beginning of the 19th century. The advent of the Internet provided the means for people to create many new micronations, whose members are scattered all over the world and interact mostly by electronic means, often calling their nations "nomadic countries". The differences between such Internet micronations, other kinds of social networking groups, and role-playing games are often difficult to define.

The term "micronation" to describe those entities dates at least to the 1970s. The term micropatriology is sometimes used to describe the study of both micronations and microstates by micronationalists, some of whom refer to sovereign nation-states as "macronations".

The term 'micronation' literally means "small nation". It is a neologism originating in the mid-1970s to describe the many thousands of small unrecognised state-like entities that have mostly arisen since that time.

The term has since also come to be used retrospectively to refer to earlier unrecognized entities, some of which date to as far back as the 19th century. Amongst supporters of micronations ("micronationalists") the term "macronation" is in common use to refer to any internationally recognised sovereign nation-state.

Micronations generally have a number of common features, although these may vary widely. They may have a structure similar to established sovereign states, including territorial claims, government institutions, official symbols and citizens, albeit on a much smaller scale. Micronations are often quite small, in both their claimed territory and claimed populations — although there are some exceptions to this rule, with different micronations having different methods of citizenship. Micronations may also issue formal instruments such as postage stamps, coins, banknotes and passports, and bestow honours and titles of nobility.

The Montevideo Convention was one attempt to create a legal definition distinguishing between states and non-states. Some micronations meet this definition, while some do not, and others reject the convention. Some micronations like Sealand or Hutt River reject the term micronation and consider themselves as sovereign states; other micronations like Flandrensis or Molossia have no intention to be recognized as real states.

There are many different types of micronations that have been claimed over the years. A list of the notable micronations is located at List of micronations.



A small number of micronations are founded based on historical anomalies or on legal anomalies (deriving from disputed interpretations of law). These types of micronations are usually located on small (usually disputed) territorial enclaves, generate limited economic activity founded on tourism and philatelic and numismatic sales, and are tolerated or ignored by the nations from which they claim to have seceded. This category includes:


Martin Coles Harman, owner of the British island of Lundy in the early decades of the 20th century, declared himself King and issued private coinage and postage stamps for local use. Although the island was ruled as a virtual fiefdom, its owner never claimed to be independent of the United Kingdom, so Lundy can at best be described as a precursor to later territorial micronations. Another example is the Principality of Outer Baldonia, a rocky island off the coast of Nova Scotia, founded by Russell Arundel, chairman of the Pepsi Cola Company (later: PepsiCo), in 1945 and comprising a population of 69 fishermen.

The 1960s and 1970s witnessed the foundation of a number of territorial micronations. The first of these, Sealand, was established in 1967 on an abandoned World War II gun platform in the North Sea just off the East Anglian coast of England, and still survives. Others were founded on libertarian principles and involved schemes to construct artificial islands, but only three are known to have had even limited success in realizing that goal.

The Republic of Rose Island was a platform built in 1968 in Italian national waters in the Adriatic Sea, off the Italian town of Rimini. It is known to have issued stamps, and to have declared Esperanto to be its official language. Shortly after completion, however, it was seized and destroyed by the Italian Navy for failing to pay state taxes.

In the late 1960s, Leicester Hemingway, brother of author Ernest, was involved in another such project—a small timber platform in international waters off the west coast of Jamaica. This territory, consisting of an by barge, he called "New Atlantis". Hemingway was an honorary citizen and President; however, the structure was damaged by storms and finally pillaged by Mexican fishermen. In 1973, Hemingway was reported to have moved on from New Atlantis to promoting a platform near the Bahamas. The new country was called "Tierra del Mar" ("Land of the Sea"). (Ernest Hemingway's adopted hometown of Key West was later itself part of another micronation; see Conch Republic.)

The Republic of Minerva was set up in 1972 as a libertarian new-country project by Nevada businessman Michael Oliver. Oliver's group conducted dredging operations at the Minerva Reefs, a shoal located in the Pacific Ocean south of Fiji. They succeeded in creating a small artificial island, but their efforts at securing international recognition met with little success, and near-neighbour Tonga sent a military force to the area and annexed it.

On April 1, 1977, bibliophile Richard Booth declared the Welsh town of Hay-on-Wye an independent kingdom with himself as its monarch. The town subsequently developed a healthy tourism industry based on literary interests, and "King Richard" (whose sceptre is a recycled toilet plunger) continues to award Hay-on-Wye peerages and honours to anyone prepared to pay for them.

In 1981, drawing on a news report about Leicester Hemingway's "New Atlantis", novelist Hisashi Inoue wrote a 700-page work of magic realism, "Kirikirijin", about a village that secedes from Japan and proclaims its bumpkinish, marginalized dialect its national language, and its subsequent war of independence. This single-handedly inspired a large number of Japanese villages, mostly in the northern regions, to "declare independence", generally as a move to raise awareness of their unique culture and crafts for urban Japanese who saw village life as backwards and uncultured. These micronations even held "international summits" from 1983 to 1985, and some of them formed confederations. Throughout the 1980s there was a "micronation boom" in Japan that brought many urban tourists to these wayward villages. But the harsh economic impact of the Japanese asset price bubble in 1991 ended the boom. Many of the villages were forced to merge with larger cities, and the micronations and confederations were generally dissolved.

Micronational developments that occurred in New Zealand and Australia in the final three decades of the 20th century included:

Micronationalism shed much of its traditionally eccentric anti-establishment mantle and took on a distinctly hobbyist perspective in the mid-1990s, when the emerging popularity of the Internet made it possible to create and promote statelike entities in an entirely electronic medium with relative ease. An early example is the Kingdom of Talossa, a micronation created in 1979 by then-14-year-old Robert Ben Madison, which went online in November 1995, and was reported in "The New York Times" and other print media in 2000. As a result, the number of exclusively online, fantasy or simulation-based micronations expanded dramatically.

The activities of these types of micronations are almost exclusively limited to simulations of diplomatic activity (including the signing of "treaties" and participation in "supra-micronational" forums such as the League of Micronations) and contribution to wikis. With the introduction of the Internet, many articles on how to create micronations were made available on such wikis, which serve as a hub of online activity for micronations. The most notable wiki for the forum, MicroWiki, was created in 2005 and is currently administered by Jonathan Austen, the leader of Austenasia.

A number of traditional territorial micronations, including the Hutt River Province, Seborga, and Sealand, maintain websites that serve largely to promote their claims and sell merchandise.

In international law, the Montevideo Convention on the Right and Duties of States sets down the criteria for statehood in article 1: "The state as a person of international law should possess the following qualifications: (a) a permanent population; (b) a defined territory; (c) government; and (d) capacity to enter into relations with the other states."

The first sentence of article 3 of the Montevideo Convention explicitly states that ""The political existence of the state is independent of recognition by the other states.""

Under these guidelines, any entity which meets all of the criteria set forth in article 1 can be regarded as sovereign under international law, whether or not other states have recognized it.

The Sovereign Military Order of Malta, as an independent subject of international law does not meet all the criteria for recognition as a State (however it does not claim itself a State either), but is and has been recognized as a sovereign nation for centuries.

The doctrine of territorial integrity does not effectively prohibit unilateral secession from established states in international law, per the relevant section from the text of the Final Act of the Conference on Security and Cooperation in Europe, also known as the Helsinki Final Act, Helsinki Accords or Helsinki Declaration:

In effect, this states that "other" states (i.e., third parties), may not encourage secession in a state. This does not make any statement as regards persons within a state electing to secede of their own accord.

There has been a small but growing amount of attention paid to the micronation phenomenon in recent years. Most interest in academic circles has been concerned with studying the apparently anomalous legal situations affecting such entities as Sealand and the Hutt River Province, in exploring how some micronations represent grassroots political ideas, and in the creation of role-playing entities for instructional purposes.

In 2000, Professor Fabrice O'Driscoll, of the Aix-Marseille University, published a book about micronations: "Ils ne siègent pas à l'ONU" ("They are not in the United Nations"), with more than 300 pages dedicated to the subject.

In May 2000, an article in "The New York Times" titled "Utopian Rulers, and Spoofs, Stake Out Territory Online" brought the phenomenon to a wider audience. Similar articles were published by newspapers such as the Italian "La Repubblica", "O Estado de S. Paulo" in Brazil, and Portugal's "Visão" at around the same time.

Several recent publications have dealt with the subject of particular historical micronations, including "Republic of Indian Stream" (University Press), by Dartmouth College geographer Daniel Doan, "The Land that Never Was", about Gregor MacGregor and the "Principality of Poyais", by David Sinclair (Review, 2003, ) and "An Australian Monarch" about the Principality of Hutt River by William Pitt (CopyRight Publishing, ).

In August 2003, a summit of micronations took place in Helsinki at Finlandia Hall, the site of the Conference for Security and Co-operation in Europe (CSCE). The summit was attended by delegations of the Principality of Sealand, the Kingdoms of Elgaland-Vargaland, NSK-State in Time, Ladonia, the Transnational republic|Transnational Republic, the State of Sabotage and by scholars from various academic institutions.

From 7 November through 17 December 2004, the Reg Vardy Gallery at the University of Sunderland (UK) hosted an exhibition on the subject of micronational group identity and symbolism. The exhibition focused on numismatic, philatelic and vexillological artifacts, as well as other symbols and instruments created and used by a number of micronations from the 1950s through to the present day. A summit of micronations conducted as part of this exhibition was attended by representatives of Sealand, Elgaland-Vargaland, New Utopia, Atlantium, Frestonia and Fusa. The exhibition was reprised at the Andrew Kreps Gallery in New York City from 24 June – 29 July of the following year and organized by R. Blackson and Peter Coffin. Peter Coffin organized a more extensive exhibition about micronations at Paris' Palais de Tokyo in early 2007 called ÉTATS (faites-le vous-même)/States (Do it yourself).

The Sunderland summit was later featured in the 5-part BBC light entertainment television series "How to Start Your Own Country" presented by Danny Wallace. The series told the story of Wallace's experience of founding a micronation, Lovely, located in his London flat. It screened in the UK in 2005.

Similar programs have also aired on television networks in other parts of Europe. In France, several Canal+ programs have centered on the satirical Presipality of Groland, while in Belgium a series by Rob Vanoudenhoven and broadcast on the Flemish commercial network VTM in April 2006 was reminiscent of Wallace's series, and centred on the producer's creation of . Among other things Vanoudenhoven minted his own coins denominated in "Robbies".

In 2006 the travel guide company Lonely Planet published a light-hearted guide micronations named "".

The Democratic Empire of Sunda, which claims to be the Government of the Kingdom of Sunda (an ancient kingdom, in present-day Indonesia) in exile in Switzerland, made media headlines when two so-called princesses, Lamia Roro Wiranatadikusumah Siliwangi Al Misri, 21, and Fathia Reza Wiranatadikusumah Siliwangi Al Misiri, 23, were detained by Malaysian authorities at the border with Brunei, on 13 July 2007, and are charged for entering the country without a valid pass. The hearing continues.

In 2010, a conference of micronations was held on Dangar Island in Sydney, Australia. Micronations with representatives in attendance included the Empire of Atlantium, the Principality of Hutt River, the Principality of Wy and the Gay and Lesbian Kingdom of the Coral Sea Islands

In 2010, a documentary film by Jody Shapiro entitled "How to Start Your Own Country" was screened as part of the Toronto International Film Festival. The documentary explored various micronations around the world, and included an analysis of the concept of statehood and citizenship. Erwin Strauss, author of the eponymous book, was interviewed as part of the film.

In 2012, a conference of micronations (PoliNation 2012) was held in London. Micronations with representatives in attendance included the Empire of Atlantium, the Republic of Molossia, the Grand Duchy of Flandrensis, Ladonia, Neue Slowenische Kunst and Austenasia. A second conference was organized in 2015 in the Free Republic of Alcatraz in Perugia

The manga and anime series "", in which the main characters are the stereotyped personifications of the nations of the world, features several micronations as characters. micronations represented include Sealand, Seborga, Wy, Kugelmugel, Molossia, Hutt River, Ladonia, and the former micronation of Nikko Nikko.

The Australian television comedy series "Micro Nation" is set on the fictional island micronation of Pullamawang, which remained independent from Australia because they "forgot to mail in their paperwork" at the Federation of Australia in 1901.





</doc>
<doc id="20381" url="https://en.wikipedia.org/wiki?curid=20381" title="Mining">
Mining

Mining is the extraction of valuable minerals or other geological materials from the earth, usually from an orebody, lode, vein, seam, reef or placer deposit. These deposits form a mineralized package that is of economic interest to the miner.

Ores recovered by mining include metals, coal, oil shale, gemstones, limestone, chalk, dimension stone, rock salt, potash, gravel, and clay. Mining is required to obtain any material that cannot be grown through agricultural processes, or created artificially in a laboratory or factory. Mining in a wider sense includes extraction of any non-renewable resource such as petroleum, natural gas, or even water.

Mining of stones and metal has been a human activity since pre-historic times. Modern mining processes involve prospecting for ore bodies, analysis of the profit potential of a proposed mine, extraction of the desired materials, and final reclamation of the land after the mine is closed. De Re Metallica, Georgius Agricola, 1550, Book I, Para. 1

Mining operations usually create a negative environmental impact, both during the mining activity and after the mine has closed. Hence, most of the world's nations have passed regulations to decrease the impact. Work safety has long been a concern as well, and modern practices have significantly improved safety in mines.

Levels of metals recycling are generally low. Unless future end-of-life recycling rates are stepped up, some rare metals may become unavailable for use in a variety of consumer products. Due to the low recycling rates, some landfills now contain higher concentrations of metal than mines themselves.

Since the beginning of civilization, people have used stone, ceramics and, later, metals found close to the Earth's surface. These were used to make early tools and weapons; for example, high quality flint found in northern France, southern England and Poland was used to create flint tools. Flint mines have been found in chalk areas where seams of the stone were followed underground by shafts and galleries. The mines at Grimes Graves and Krzemionki are especially famous, and like most other flint mines, are Neolithic in origin (ca 4000–3000 BC). Other hard rocks mined or collected for axes included the greenstone of the Langdale axe industry based in the English Lake District.

The oldest-known mine on archaeological record is the Ngwenya Mine in Swaziland, which radiocarbon dating shows to be about 43,000 years old. At this site Paleolithic humans mined hematite to make the red pigment ochre. Mines of a similar age in Hungary are believed to be sites where Neanderthals may have mined flint for weapons and tools.

Ancient Egyptians mined malachite at Maadi. At first, Egyptians used the bright green malachite stones for ornamentations and pottery. Later, between 2613 and 2494 BC, large building projects required expeditions abroad to the area of Wadi Maghareh in order to secure minerals and other resources not available in Egypt itself. Quarries for turquoise and copper were also found at Wadi Hammamat, Tura, Aswan and various other Nubian sites on the Sinai Peninsula and at Timna.

Mining in Egypt occurred in the earliest dynasties. The gold mines of Nubia were among the largest and most extensive of any in Ancient Egypt. These mines are described by the Greek author Diodorus Siculus, who mentions fire-setting as one method used to break down the hard rock holding the gold. One of the complexes is shown in one of the earliest known maps. The miners crushed the ore and ground it to a fine powder before washing the powder for the gold dust.

Mining in Europe has a very long history. Examples include the silver mines of Laurium, which helped support the Greek city state of Athens. Although they had over 20,000 slaves working them, their technology was essentially identical to their Bronze Age predecessors. At other mines, such as on the island of Thassos, marble was quarried by the Parians after they arrived in the 7th Century BC. The marble was shipped away and was later found by archaeologists to have been used in buildings including the tomb of Amphipolis. Philip II of Macedon, the father of Alexander the Great, captured the gold mines of Mount Pangeo in 357 BC to fund his military campaigns. He also captured gold mines in Thrace for minting coinage, eventually producing 26 tons per year.

However, it was the Romans who developed large scale mining methods, especially the use of large volumes of water brought to the minehead by numerous aqueducts. The water was used for a variety of purposes, including removing overburden and rock debris, called hydraulic mining, as well as washing comminuted, or crushed, ores and driving simple machinery.

The Romans used hydraulic mining methods on a large scale to prospect for the veins of ore, especially a now-obsolete form of mining known as hushing. They built numerous aqueducts to supply water to the minehead. There, the water stored in large reservoirs and tanks. When a full tank was opened, the flood of water sluiced away the overburden to expose the bedrock underneath and any gold veins. The rock was then worked upon by fire-setting to heat the rock, which would be quenched with a stream of water. The resulting thermal shock cracked the rock, enabling it to be removed by further streams of water from the overhead tanks. The Roman miners used similar methods to work cassiterite deposits in Cornwall and lead ore in the Pennines.

The methods had been developed by the Romans in Spain in 25 AD to exploit large alluvial gold deposits, the largest site being at Las Medulas, where seven long aqueducts tapped local rivers and sluiced the deposits. Spain was one of the most important mining regions, but all regions of the Roman Empire were exploited. In Great Britain the natives had mined minerals for millennia, but after the Roman conquest, the scale of the operations increased dramatically, as the Romans needed Britannia's resources, especially gold, silver, tin, and lead.

Roman techniques were not limited to surface mining. They followed the ore veins underground once opencast mining was no longer feasible. At Dolaucothi they stoped out the veins and drove adits through bare rock to drain the stopes. The same adits were also used to ventilate the workings, especially important when fire-setting was used. At other parts of the site, they penetrated the water table and dewatered the mines using several kinds of machines, especially reverse overshot water-wheels. These were used extensively in the copper mines at Rio Tinto in Spain, where one sequence comprised 16 such wheels arranged in pairs, and lifting water about . They were worked as treadmills with miners standing on the top slats. Many examples of such devices have been found in old Roman mines and some examples are now preserved in the British Museum and the National Museum of Wales.

Mining as an industry underwent dramatic changes in medieval Europe. The mining industry in the early Middle Ages was mainly focused on the extraction of copper and iron. Other precious metals were also used, mainly for gilding or coinage. Initially, many metals were obtained through open-pit mining, and ore was primarily extracted from shallow depths, rather than through deep mine shafts. Around the 14th century, the growing use of weapons, armour, stirrups, and horseshoes greatly increased the demand for iron. Medieval knights, for example, were often laden with up to of plate or chain link armour in addition to swords, lances and other weapons. The overwhelming dependency on iron for military purposes spurred iron production and extraction processes.

The silver crisis of 1465 occurred when all mines had reached depths at which the shafts could no longer be pumped dry with the available technology. Although an increased use of banknotes, credit and copper coins during this period did decrease the value of, and dependence on, precious metals, gold and silver still remained vital to the story of medieval mining.

Due to differences in the social structure of society, the increasing extraction of mineral deposits spread from central Europe to England in the mid-sixteenth century. On the continent, mineral deposits belonged to the crown, and this regalian right was stoutly maintained. But in England, royal mining rights were restricted to gold and silver (of which England had virtually no deposits) by a judicial decision of 1568 and a law in 1688. England had iron, zinc, copper, lead, and tin ores. Landlords who owned the base metals and coal under their estates then had a strong inducement to extract these metals or to lease the deposits and collect royalties from mine operators. English, German, and Dutch capital combined to finance extraction and refining. Hundreds of German technicians and skilled workers were brought over; in 1642 a colony of 4,000 foreigners was mining and smelting copper at Keswick in the northwestern mountains.

Use of water power in the form of water mills was extensive. The water mills were employed in crushing ore, raising ore from shafts, and ventilating galleries by powering giant bellows. Black powder was first used in mining in Selmecbánya, Kingdom of Hungary (now Banská Štiavnica, Slovakia) in 1627. Black powder allowed blasting of rock and earth to loosen and reveal ore veins. Blasting was much faster than fire-setting and allowed the mining of previously impenetrable metals and ores. In 1762, the world's first mining academy was established in the same town there.

The widespread adoption of agricultural innovations such as the iron plowshare, as well as the growing use of metal as a building material, was also a driving force in the tremendous growth of the iron industry during this period. Inventions like the arrastra were often used by the Spanish to pulverize ore after being mined. This device was powered by animals and used the same principles used for grain threshing.

Much of the knowledge of medieval mining techniques comes from books such as Biringuccio’s "De la pirotechnia" and probably most importantly from Georg Agricola's "De re metallica" (1556). These books detail many different mining methods used in German and Saxon mines. A prime issue in medieval mines, which Agricola explains in detail, was the removal of water from mining shafts. As miners dug deeper to access new veins, flooding became a very real obstacle. The mining industry became dramatically more efficient and prosperous with the invention of mechanical and animal driven pumps.

Mining in the Philippines began around 1000 BC. The early Filipinos worked various mines of gold, silver, copper and iron. Jewels, gold ingots, chains, calombigas and earrings were handed down from antiquity and inherited from their ancestors. Gold dagger handles, gold dishes, tooth plating, and huge gold ornamets were also used. In Laszlo Legeza's "Tantric elements in pre-Hispanic Philippines Gold Art", he mentioned that gold jewelry of Philippine origin was found in Ancient Egypt. According to Antonio Pigafetta, the people of Mindoro possessed great skill in mixing gold with other metals and gave it a natural and perfect appearance that could deceive even the best of silversmiths. The natives were also known for the jewelries made of other precious stones such as carnelian, agate and pearl. Some outstanding examples of Philippine jewelry included necklaces, belts, armlets and rings placed around the waist.

There are ancient, prehistoric copper mines along Lake Superior, and metallic copper was still found there, near the surface, in colonial times.

Indigenous peoples availed themselves of this copper starting at least 5,000 years ago," and copper tools, arrowheads, and other artifacts that were part of an extensive native trade network have been discovered. In addition, obsidian, flint, and other minerals were mined, worked, and traded. Early French explorers who encountered the sites made no use of the metals due to the difficulties of transporting them, but the copper was eventually traded throughout the continent along major river routes. 

In the early colonial history of the Americas, "native gold and silver was quickly expropriated and sent back to Spain in fleets of gold- and silver-laden galleons," the gold and silver originating mostly from mines in Central and South America. Turquoise dated at 700 AD was mined in pre-Columbian America; in the Cerillos Mining District in New Mexico, estimates are that "about 15,000 tons of rock had been removed from Mt. Chalchihuitl using stone tools before 1700."

Mining in the United States became prevalent in the 19th century, and the General Mining Act of 1872 was passed to encourage mining of federal lands. As with the California Gold Rush in the mid-19th century, mining for minerals and precious metals, along with ranching, was a driving factor in the Westward Expansion to the Pacific coast. With the exploration of the West, mining camps were established and "expressed a distinctive spirit, an enduring legacy to the new nation;" Gold Rushers would experience the same problems as the Land Rushers of the transient West that preceded them. Aided by railroads, many traveled West for work opportunities in mining. Western cities such as Denver and Sacramento originated as mining towns.

When new areas were explored, it was usually the gold (placer and then lode) and then silver that were taken into possession and extracted first. Other metals would often wait for railroads or canals, as coarse gold dust and nuggets do not require smelting and are easy to identify and transport.

In the early 20th century, the gold and silver rush to the western United States also stimulated mining for coal as well as base metals such as copper, lead, and iron. Areas in modern Montana, Utah, Arizona, and later Alaska became predominate suppliers of copper to the world, which was increasingly demanding copper for electrical and households goods. Canada's mining industry grew more slowly than did the United States' due to limitations in transportation, capital, and U.S. competition; Ontario was the major producer of the early 20th century with nickel, copper, and gold.

Meanwhile, Australia experienced the Australian gold rushes and by the 1850s was producing 40% of the world's gold, followed by the establishment of large mines such as the Mount Morgan Mine, which ran for nearly a hundred years, Broken Hill ore deposit (one of the largest zinc-lead ore deposits), and the iron ore mines at Iron Knob. After declines in production, another boom in mining occurred in the 1960s. Now, in the early 21st century, Australia remains a major world mineral producer.

As the 21st century begins, a globalized mining industry of large multinational corporations has arisen. Peak minerals and environmental impacts have also become a concern. Different elements, particularly rare earth minerals, have begun to increase in demand as a result of new technologies.

The process of mining from discovery of an ore body through extraction of minerals and finally to returning the land to its natural state consists of several distinct steps. The first is discovery of the ore body, which is carried out through prospecting or exploration to find and then define the extent, location and value of the ore body. This leads to a mathematical resource estimation to estimate the size and grade of the deposit.

This estimation is used to conduct a pre-feasibility study to determine the theoretical economics of the ore deposit. This identifies, early on, whether further investment in estimation and engineering studies is warranted and identifies key risks and areas for further work. The next step is to conduct a feasibility study to evaluate the financial viability, the technical and financial risks, and the robustness of the project.

This is when the mining company makes the decision whether to develop the mine or to walk away from the project. This includes mine planning to evaluate the economically recoverable portion of the deposit, the metallurgy and ore recoverability, marketability and payability of the ore concentrates, engineering concerns, milling and infrastructure costs, finance and equity requirements, and an analysis of the proposed mine from the initial excavation all the way through to reclamation. The proportion of a deposit that is economically recoverable is dependent on the enrichment factor of the ore in the area.

To gain access to the mineral deposit within an area it is often necessary to mine through or remove waste material which is not of immediate interest to the miner. The total movement of ore and waste constitutes the mining process. Often more waste than ore is mined during the life of a mine, depending on the nature and location of the ore body. Waste removal and placement is a major cost to the mining operator, so a detailed characterization of the waste material forms an essential part of the geological exploration program for a mining operation.

Once the analysis determines a given ore body is worth recovering, development begins to create access to the ore body. The mine buildings and processing plants are built, and any necessary equipment is obtained. The operation of the mine to recover the ore begins and continues as long as the company operating the mine finds it economical to do so. Once all the ore that the mine can produce profitably is recovered, reclamation begins to make the land used by the mine suitable for future use.

Mining techniques can be divided into two common excavation types: surface mining and sub-surface (underground) mining. Today, surface mining is much more common, and produces, for example, 85% of minerals (excluding petroleum and natural gas) in the United States, including 98% of metallic ores.

Targets are divided into two general categories of materials: "placer deposits", consisting of valuable minerals contained within river gravels, beach sands, and other unconsolidated materials; and "lode deposits", where valuable minerals are found in veins, in layers, or in mineral grains generally distributed throughout a mass of actual rock. Both types of ore deposit, placer or lode, are mined by both surface and underground methods.

Some mining, including much of the rare earth elements and uranium mining, is done by less-common methods, such as in-situ leaching: this technique involves digging neither at the surface nor underground. The extraction of target minerals by this technique requires that they be soluble, e.g., potash, potassium chloride, sodium chloride, sodium sulfate, which dissolve in water. Some minerals, such as copper minerals and uranium oxide, require acid or carbonate solutions to dissolve.

Surface mining is done by removing (stripping) surface vegetation, dirt, and, if necessary, layers of bedrock in order to reach buried ore deposits. Techniques of surface mining include: open-pit mining, which is the recovery of materials from an open pit in the ground, quarrying, identical to open-pit mining except that it refers to sand, stone and clay; strip mining, which consists of stripping surface layers off to reveal ore/seams underneath; and mountaintop removal, commonly associated with coal mining, which involves taking the top of a mountain off to reach ore deposits at depth. Most (but not all) placer deposits, because of their shallowly buried nature, are mined by surface methods. Finally, landfill mining involves sites where landfills are excavated and processed. Landfill mining has been thought of as a solution to dealing with long-term methane emissions and local pollution

Sub-surface mining consists of digging tunnels or shafts into the earth to reach buried ore deposits. Ore, for processing, and waste rock, for disposal, are brought to the surface through the tunnels and shafts. Sub-surface mining can be classified by the type of access shafts used, the extraction method or the technique used to reach the mineral deposit. Drift mining utilizes horizontal access tunnels, slope mining uses diagonally sloping access shafts, and shaft mining utilizes vertical access shafts. Mining in hard and soft rock formations require different techniques.

Other methods include shrinkage stope mining, which is mining upward, creating a sloping underground room, long wall mining, which is grinding a long ore surface underground, and room and pillar mining, which is removing ore from rooms while leaving pillars in place to support the roof of the room. Room and pillar mining often leads to retreat mining, in which supporting pillars are removed as miners retreat, allowing the room to cave in, thereby loosening more ore. Additional sub-surface mining methods include hard rock mining, which is mining of hard rock (igneous, metamorphic or sedimentary) materials, bore hole mining, drift and fill mining, long hole slope mining, sub level caving, and block caving.

Highwall mining is another form of surface mining that evolved from auger mining. In Highwall mining, the coal seam is penetrated by a continuous miner propelled by a hydraulic Pushbeam Transfer Mechanism (PTM). A typical cycle includes sumping (launch-pushing forward) and shearing (raising and lowering the cutterhead boom to cut the entire height of the coal seam). As the coal recovery cycle continues, the cutterhead is progressively launched into the coal seam for 19.72 feet (6.01 m). Then, the Pushbeam Transfer Mechanism (PTM) automatically inserts a 19.72-foot (6.01 m) long rectangular Pushbeam (Screw-Conveyor Segment) into the center section of the machine between the Powerhead and the cutterhead. The Pushbeam system can penetrate nearly 1,000 feet (300 m) into the coal seam. One patented Highwall mining system uses augers enclosed inside the Pushbeam that prevent the mined coal from being contaminated by rock debris during the conveyance process. Using a video imaging and/or a gamma ray sensor and/or other Geo-Radar systems like a coal-rock interface detection sensor (CID), the operator can see ahead projection of the seam-rock interface and guide the continuous miner's progress. Highwall mining can produce thousands of tons of coal in contour-strip operations with narrow benches, previously mined areas, trench mine applications and steep-dip seams with controlled water-inflow pump system and/or a gas (inert) venting system.

Heavy machinery is used in mining to explore and develop sites, to remove and stockpile overburden, to break and remove rocks of various hardness and toughness, to process the ore, and to carry out reclamation projects after the mine is closed. Bulldozers, drills, explosives and trucks are all necessary for excavating the land. In the case of placer mining, unconsolidated gravel, or alluvium, is fed into machinery consisting of a hopper and a shaking screen or trommel which frees the desired minerals from the waste gravel. The minerals are then concentrated using sluices or jigs.

Large drills are used to sink shafts, excavate stopes, and obtain samples for analysis. Trams are used to transport miners, minerals and waste. Lifts carry miners into and out of mines, and move rock and ore out, and machinery in and out, of underground mines. Huge trucks, shovels and cranes are employed in surface mining to move large quantities of overburden and ore. Processing plants utilize large crushers, mills, reactors, roasters and other equipment to consolidate the mineral-rich material and extract the desired compounds and metals from the ore.

Once the mineral is extracted, it is often then processed. The science of extractive metallurgy is a specialized area in the science of metallurgy that studies the extraction of valuable metals from their ores, especially through chemical or mechanical means.

Mineral processing (or mineral dressing) is a specialized area in the science of metallurgy that studies the mechanical means of crushing, grinding, and washing that enable the separation (extractive metallurgy) of valuable metals or minerals from their gangue (waste material). Processing of placer ore material consists of gravity-dependent methods of separation, such as sluice boxes. Only minor shaking or washing may be necessary to disaggregate (unclump) the sands or gravels before processing. Processing of ore from a lode mine, whether it is a surface or subsurface mine, requires that the rock ore be crushed and pulverized before extraction of the valuable minerals begins. After lode ore is crushed, recovery of the valuable minerals is done by one, or a combination of several, mechanical and chemical techniques.

Since most metals are present in ores as oxides or sulfides, the metal needs to be reduced to its metallic form. This can be accomplished through chemical means such as smelting or through electrolytic reduction, as in the case of aluminium. Geometallurgy combines the geologic sciences with extractive metallurgy and mining.

In 2018, led by Chemistry and Biochemistry professor Bradley D. Smith, University of Notre Dame researchers "invented a new class of molecules whose shape and size enable them to capture and contain precious metal ions," reported in a study published by the Journal of the American Chemical Society. The new method "converts gold-containing ore into chloroauric acid and extracts it using an industrial solvent. The container molecules are able to selectively separate the gold from the solvent without the use of water stripping." The newly developed molecules can eliminate water stripping, whereas mining traditionally "relies on a 125-year-old method that treats gold-containing ore with large quantities of poisonous sodium cyanide... this new process has a milder environmental impact and that, besides gold, it can be used for capturing other metals such as platinum and palladium," and could also be used in urban mining processes that remove precious metals from wastewater streams.

Environmental issues can include erosion, formation of sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals from mining processes. In some cases, additional forest logging is done in the vicinity of mines to create space for the storage of the created debris and soil. Contamination resulting from leakage of chemicals can also affect the health of the local population if not properly controlled. Extreme examples of pollution from mining activities include coal fires, which can last for years or even decades, producing massive amounts of environmental damage.

Mining companies in most countries are required to follow stringent environmental and rehabilitation codes in order to minimize environmental impact and avoid impacting human health. These codes and regulations all require the common steps of environmental impact assessment, development of environmental management plans, mine closure planning (which must be done before the start of mining operations), and environmental monitoring during operation and after closure. However, in some areas, particularly in the developing world, government regulations may not be well enforced.

For major mining companies and any company seeking international financing, there are a number of other mechanisms to enforce good environmental standards. These generally relate to financing standards such as the Equator Principles, IFC environmental standards, and criteria for Socially responsible investing. Mining companies have used this oversight from the financial sector to argue for some level of industry self-regulation. In 1992, a Draft Code of Conduct for Transnational Corporations was proposed at the Rio Earth Summit by the UN Centre for Transnational Corporations (UNCTC), but the Business Council for Sustainable Development (BCSD) together with the International Chamber of Commerce (ICC) argued successfully for self-regulation instead.

This was followed by the Global Mining Initiative which was begun by nine of the largest metals and mining companies and which led to the formation of the International Council on Mining and Metals, whose purpose was to "act as a catalyst" in an effort to improve social and environmental performance in the mining and metals industry internationally. The mining industry has provided funding to various conservation groups, some of which have been working with conservation agendas that are at odds with an emerging acceptance of the rights of indigenous people – particularly the right to make land-use decisions.

Certification of mines with good practices occurs through the International Organization for Standardization (ISO). For example, ISO 9000 and ISO 14001, which certify an "auditable environmental management system", involve short inspections, although they have been accused of lacking rigor. Certification is also available through Ceres' Global Reporting Initiative, but these reports are voluntary and unverified. Miscellaneous other certification programs exist for various projects, typically through nonprofit groups.

The purpose of a 2012 EPS PEAKS paper was to provide evidence on policies managing ecological costs and maximise socio-economic benefits of mining using host country regulatory initiatives. It found existing literature suggesting donors encourage developing countries to:


Ore mills generate large amounts of waste, called tailings. For example, 99 tons of waste are generated per ton of copper, with even higher ratios in gold mining - because only 5.3 g of gold is extracted per ton of ore, a ton of gold produces 200,000 tons of tailings. (As time goes on and richer deposits are exhausted - and technology improves to permit - this number is going down to .5 g and less.) These tailings can be toxic. Tailings, which are usually produced as a slurry, are most commonly dumped into ponds made from naturally existing valleys. These ponds are secured by impoundments (dams or embankment dams). In 2000 it was estimated that 3,500 tailings impoundments existed, and that every year, 2 to 5 major failures and 35 minor failures occurred; for example, in the Marcopper mining disaster at least 2 million tons of tailings were released into a local river. In central Finland, Talvivaara Terrafame polymetal mine waste effluent since 2008 and numerous leaks of saline mine water has resulted in ecological collapse of nearby lake. Subaqueous tailings disposal is another option. The mining industry has argued that submarine tailings disposal (STD), which disposes of tailings in the sea, is ideal because it avoids the risks of tailings ponds; although the practice is illegal in the United States and Canada, it is used in the developing world.

The waste is classified as either sterile or mineralised, with acid generating potential, and the movement and storage of this material forms a major part of the mine planning process. When the mineralised package is determined by an economic cut-off, the near-grade mineralised waste is usually dumped separately with view to later treatment should market conditions change and it becomes economically viable. Civil engineering design parameters are used in the design of the waste dumps, and special conditions apply to high-rainfall areas and to seismically active areas. Waste dump designs must meet all regulatory requirements of the country in whose jurisdiction the mine is located. It is also common practice to rehabilitate dumps to an internationally acceptable standard, which in some cases means that higher standards than the local regulatory standard are applied.

Many mining sites are remote and not connected to the grid. Electricity is typically generated with diesel generators. Due to high transportation cost and theft during transportation the cost for generating electricity is normally high. Renewable energy applications are becoming an alternative or amendment. Both solar and wind power plants can contribute in saving diesel costs at mining sites. Renewable energy applications have been built at mining sites.
Cost savings can reach up to 70%.

Mining exists in many countries. London is known as the capital of global "mining houses" such as Rio Tinto Group, BHP Billiton, and Anglo American PLC. The US mining industry is also large, but it is dominated by the coal and other nonmetal minerals (e.g., rock and sand), and various regulations have worked to reduce the significance of mining in the United States. In 2007 the total market capitalization of mining companies was reported at US$962 billion, which compares to a total global market cap of publicly traded companies of about US$50 trillion in 2007. In 2002, Chile and Peru were reportedly the major mining countries of South America. The mineral industry of Africa includes the mining of various minerals; it produces relatively little of the industrial metals copper, lead, and zinc, but according to one estimate has as a percent of world reserves 40% of gold, 60% of cobalt, and 90% of the world's platinum group metals. Mining in India is a significant part of that country's economy. In the developed world, mining in Australia, with BHP Billiton founded and headquartered in the country, and mining in Canada are particularly significant. For rare earth minerals mining, China reportedly controlled 95% of production in 2013.

While exploration and mining can be conducted by individual entrepreneurs or small businesses, most modern-day mines are large enterprises requiring large amounts of capital to establish. Consequently, the mining sector of the industry is dominated by large, often multinational, companies, most of them publicly listed. It can be argued that what is referred to as the 'mining industry' is actually two sectors, one specializing in exploration for new resources and the other in mining those resources. The exploration sector is typically made up of individuals and small mineral resource companies, called "juniors", which are dependent on venture capital. The mining sector is made up of large multinational companies that are sustained by production from their mining operations. Various other industries such as equipment manufacture, environmental testing, and metallurgy analysis rely on, and support, the mining industry throughout the world. Canadian stock exchanges have a particular focus on mining companies, particularly junior exploration companies through Toronto's TSX Venture Exchange; Canadian companies raise capital on these exchanges and then invest the money in exploration globally. Some have argued that below juniors there exists a substantial sector of illegitimate companies primarily focused on manipulating stock prices.

Mining operations can be grouped into five major categories in terms of their respective resources. These are oil and gas extraction, coal mining, metal ore mining, nonmetallic mineral mining and quarrying, and mining support activities. Of all of these categories, oil and gas extraction remains one of the largest in terms of its global economic importance. Prospecting potential mining sites, a vital area of concern for the mining industry, is now done using sophisticated new technologies such as seismic prospecting and remote-sensing satellites. Mining is heavily affected by the prices of the commodity minerals, which are often volatile. The 2000s commodities boom ("commodities supercycle") increased the prices of commodities, driving aggressive mining. In addition, the price of gold increased dramatically in the 2000s, which increased gold mining; for example, one study found that conversion of forest in the Amazon increased six-fold from the period 2003–2006 (292 ha/yr) to the period 2006–2009 (1,915 ha/yr), largely due to artisanal mining.

Mining companies can be classified based on their size and financial capabilities:


New regulations and a process of legislative reforms aim to improve the harmonization and stability of the mining sector in mineral-rich countries. New legislation for mining industry in African countries still appears to be an issue, but has the potential to be solved, when a consensus is reached on the best approach. By the beginning of the 21st century the booming and increasingly complex mining sector in mineral-rich countries was providing only slight benefits to local communities, especially in given the sustainability issues. Increasing debate and influence by NGOs and local communities called for a new approahes which would also include disadvantaged communities, and work towards sustainable development even after mine closure (including transparency and revenue management). By the early 2000s, community development issues and resettlements became mainstream concerns in World Bank mining projects. Mining-industry expansion after mineral prices increased in 2003 and also potential fiscal revenues in those countries created an omission in the other economic sectors in terms of finances and development. Furthermore, this highlighted regional and local demand for mining revenues and an inability of sub-national governments to effectively use the revenues. The Fraser Institute (a Canadian think tank) has highlighted the environmental protection laws in developing countries, as well as voluntary efforts by mining companies to improve their environmental impact.

In 2007 the Extractive Industries Transparency Initiative (EITI) was mainstreamed in all countries cooperating with the World Bank in mining industry reform. The EITI operates and was implemented with the support of the EITI multi-donor trust fund, managed by the World Bank. The EITI aims to increase transparency in transactions between governments and companies in extractive industries by monitoring the revenues and benefits between industries and recipient governments. The entrance process is voluntary for each country and is monitored by multiple stakeholders including governments, private companies and civil society representatives, responsible for disclosure and dissemination of the reconciliation report; however, the competitive disadvantage of company-by company public report is for some of the businesses in Ghana at least, the main constraint. Therefore, the outcome assessment in terms of failure or success of the new EITI regulation does not only "rest on the government's shoulders" but also on civil society and companies.

On the other hand, implementation has issues; inclusion or exclusion of artisanal mining and small-scale mining (ASM) from the EITI and how to deal with "non-cash" payments made by companies to subnational governments. Furthermore, the disproportionate revenues the mining industry can bring to the comparatively small number of people that it employs, causes other problems, like a lack of investment in other less lucrative sectors, leading to swings in government revenuebecause of volatility in the oil markets. Artisanal mining is clearly an issue in EITI Countries such as the Central African Republic, D.R. Congo, Guinea, Liberia and Sierra Leone – i.e. almost half of the mining countries implementing the EITI. Among other things, limited scope of the EITI involving disparity in terms of knowledge of the industry and negotiation skills, thus far flexibility of the policy (e.g. liberty of the countries to expand beyond the minimum requirements and adapt it to their needs), creates another risk of unsuccessful implementation. Public awareness increase, where government should act as a bridge between public and initiative for a successful outcome of the policy is an important element to be considered.

The World Bank has been involved in mining since 1955, mainly through grants from its International Bank for Reconstruction and Development, with the Bank's Multilateral Investment Guarantee Agency offering political risk insurance. Between 1955 and 1990 it provided about $2 billion to fifty mining projects, broadly categorized as reform and rehabilitation, greenfield mine construction, mineral processing, technical assistance, and engineering. These projects have been criticized, particularly the Ferro Carajas project of Brazil, begun in 1981. The World Bank established mining codes intended to increase foreign investment; in 1988 it solicited feedback from 45 mining companies on how to increase their involvement.

In 1992 the World Bank began to push for privatization of government-owned mining companies with a new set of codes, beginning with its report "The Strategy for African Mining". In 1997, Latin America's largest miner Companhia Vale do Rio Doce (CVRD) was privatized. These and other developments such as the Philippines 1995 Mining Act led the bank to publish a third report ("Assistance for Minerals Sector Development and Reform in Member Countries") which endorsed mandatory environment impact assessments and attention to the concerns of the local population. The codes based on this report are influential in the legislation of developing nations. The new codes are intended to encourage development through tax holidays, zero custom duties, reduced income taxes, and related measures. The results of these codes were analyzed by a group from the University of Quebec, which concluded that the codes promote foreign investment but "fall very short of permitting sustainable development". The observed negative correlation between natural resources and economic development is known as the resource curse.

Safety has long been a concern in the mining business, especially in sub-surface mining. The Courrières mine disaster, Europe's worst mining accident, involved the death of 1,099 miners in Northern France on March 10, 1906. This disaster was surpassed only by the Benxihu Colliery accident in China on April 26, 1942, which killed 1,549 miners. While mining today is substantially safer than it was in previous decades, mining accidents still occur. Government figures indicate that 5,000 Chinese miners die in accidents each year, while other reports have suggested a figure as high as 20,000. Mining accidents continue worldwide, including accidents causing dozens of fatalities at a time such as the 2007 Ulyanovskaya Mine disaster in Russia, the 2009 Heilongjiang mine explosion in China, and the 2010 Upper Big Branch Mine disaster in the United States. Mining has been identified by the National Institute for Occupational Safety and Health (NIOSH) as a priority industry sector in the National Occupational Research Agenda (NORA) to identify and provide intervention strategies regarding occupational health and safety issues. The Mining Safety and Health Administration (MSHA) was established in 1978 to "work to prevent death, illness, and injury from mining and promote safe and healthful workplaces for US miners." Since its implementation in 1978, the number of miner fatalities has decreased from 242 miners in 1978 to 28 miners in 2015.

There are numerous occupational hazards associated with mining, including exposure to rockdust which can lead to diseases such as silicosis, asbestosis, and pneumoconiosis. Gases in the mine can lead to asphyxiation and could also be ignited. Mining equipment can generate considerable noise, putting workers at risk for hearing loss. Cave-ins, rock falls, and exposure to excess heat are also known hazards. The current NIOSH Recommended Exposure Limit (REL) of noise is 85 dBA with a 3 dBA exchange rate and the MSHA Permissible Exposure Limit (PEL) is 90 dBA with a 5 dBA exchange rate as an 8-hour time-weighted average. NIOSH has found that 25% of noise-exposed workers in Mining, Quarrying, and Oil and Gas Extraction have hearing impairment. The prevalence of hearing loss increased by 1% from 1991-2001 within these workers.

Noise studies have been conducted in several mining environments. Stageloaders (84-102 dBA), shearers (85-99 dBA), auxiliary fans (84-120 dBA), continuous mining machines (78-109 dBA), and roof bolters (92-103 dBA) represent some of the noisiest equipment in underground coal mines. Dragline oilers, dozer operators, and welders using air arcing were occupations with the highest noise exposures among surface coal miners. Coal mines had the highest hearing loss injury likelihood.

Proper ventilation, hearing protection, and spraying equipment with water are important safety practices in mines.

As of 2008, the deepest mine in the world is TauTona in Carletonville, South Africa at , replacing the neighboring Savuka Mine in the North West Province of South Africa at . East Rand Mine in Boksburg, South Africa briefly held the record at , and the first mine declared the deepest in the world was also TauTona when it was at .

The Moab Khutsong gold mine in North West Province (South Africa) has the world's longest winding steel wire rope, able to lower workers to in one uninterrupted four-minute journey.

The deepest mine in Europe is the 16th shaft of the uranium mines in Příbram, Czech Republic at , second is Bergwerk Saar in Saarland, Germany at . 

The deepest open-pit mine in the world is Bingham Canyon Mine in Bingham Canyon, Utah, United States at over . The largest and second deepest open-pit copper mine in the world is Chuquicamata in Chuquicamata, Chile at , 443,000 tons of copper and 20,000 tons of molybdenum produced annually. 

The deepest open-pit mine with respect to sea level is Tagebau Hambach in Germany, where the base of the pit is below sea level.

The largest underground mine is Kiirunavaara Mine in Kiruna, Sweden. With of roads, 40 million tonnes of ore produced yearly, and a depth of , it is also one of the most modern underground mines. The deepest borehole in the world is Kola Superdeep Borehole at . This, however, is not a matter of mining but rather related to scientific drilling.

During the 20th century, the variety of metals used in society grew rapidly. Today, the development of major nations such as China and India and advances in technologies are fueling an ever-greater demand. The result is that metal mining activities are expanding and more and more of the world’s metal stocks are above ground in use rather than below ground as unused reserves. An example is the in-use stock of copper. Between 1932 and 1999, copper in use in the US rose from to per person.

95% of the energy used to make aluminium from bauxite ore is saved by using recycled material. However, levels of metals recycling are generally low. In 2010, the International Resource Panel, hosted by the United Nations Environment Programme (UNEP), published reports on metal stocks that exist within society and their recycling rates.

The report's authors observed that the metal stocks in society can serve as huge mines above ground. However, they warned that the recycling rates of some rare metals used in applications such as mobile phones, battery packs for hybrid cars, and fuel cells are so low that unless future end-of-life recycling rates are dramatically stepped up these critical metals will become unavailable for use in modern technology.

As recycling rates are low and so much metal has already been extracted, some landfills now contain a higher concentrations of metal than mines themselves. This is especially true of aluminium, used in cans, and precious metals, found in discarded electronics. Furthermore, waste after 15 years has still not broken down, so less processing would be required when compared to mining ores. A study undertaken by Cranfield University has found £360 million of metals could be mined from just 4 landfill sites. There is also up to 20MJ/kg of energy in waste, potentially making the re-extraction more profitable. However, although the first landfill mine opened in Tel Aviv, Israel in 1953, little work has followed due to the abundance of accessible ores.




</doc>
<doc id="20388" url="https://en.wikipedia.org/wiki?curid=20388" title="Geography of Myanmar">
Geography of Myanmar

Myanmar (also known as Burma) is the northwestern-most country of mainland Southeast Asia, bordering China, India, Bangladesh, Thailand and Laos. It lies along the Indian and Eurasian Plates, to the southeast of the Himalayas. To its west is the Bay of Bengal and to its south is the Andaman Sea. It is strategically located near major Indian Ocean shipping lanes.

Total Land Border Length: 

Total Land Area: 

Border Countries:

Bangladesh: , India:, China: , Laos: , Thailand: 

Total: 

Total water area: 

Tropical monsoon in the lowlands below ; cloudy, rainy, hot, humid summers (southwest monsoon, June to September); less cloudy, scant rainfall, mild temperatures, lower humidity during winter (northeast monsoon, December to April). 

Climate varies in the highlands depending on elevation; subtropical temperate climate at around , temperate at , cool, alpine at and above the alpine zone, cold, harsh tundra and Arctic climate. The higher elevations are subject to heavy snowfall, especially in the North, and bad weather.

Myanmar is characterised by its central lowlands with the Sittaung Valley and Chindwin Valley and the small mountain ranges of Zeebyu Taungdan, Min-wun Taungdan, Hman-kin Taungdan and Gangaw Taungdan as well as the Bago Yoma (Pegu Range), a relatively low mountain chain between the Irrawaddy and the Sittaung River in central Myanmar. The Central Valley Region is limited by steep, rugged highlands in the North, where ranges at the southern end of the Hengduan System form the border between Myanmar and China. Hkakabo Razi, the country's highest point at , is located at the northern end of the country. This mountain is part of a series of parallel ranges that run from the foothills of the Himalaya through the border areas with Assam, Nagaland and Mizoram.

The Arakan Mountains in the west run from Manipur into western Myanmar southwards through Rakhine State almost to Cape Negrais in the shores of the Bay of Bengal. The Arakan Range includes the Naga Hills, the Chin Hills, and the Patkai range which includes the Lushai Hills.

In Eastern Myanmar the highest point of the Shan Hills is 2,563 m high Loi Pangnao, one of the ultra prominent peaks of Southeast Asia. The Shan Hills form, together with the Karen Hills, Dawna Range and Tenasserim Hills, a natural border with Thailand as well as the Kayah-Karen/Tenasserim moist forests ecoregion which is included in the Global 200 list of ecoregions identified by the World Wildlife Fund (WWF) as priorities for conservation.
Southern Myanmar consists largely of the western slopes of the Bilauktaung, the highest part of the Tenasserim Range, which extends southwards forming the central range of the Malay Peninsula.


The Irrawaddy, the main river of Burma, flows from north to south through the Central Burma Basin and ends in a wide delta. The Mekong runs from the Tibetan Plateau through China's Yunnan province and northeastern Burma into Laos.

In the east the Salween and the Sittaung River run along the western side of the Shan Hills and the northern end of the Dawna Range.
In the narrow southeastern part of Burma, the Ye, Heinze, Dawei (Tavoy), Great Tenasserim (Tanintharyi) and the Lenya rivers are relatively short and flow into the Andaman Sea. Further south the Kraburi River forms the southern border between Thailand and Burma.

"contiguous zone:"

"continental shelf:"
"exclusive economic zone:"

Natural resources in Myanmar are petroleum, timber, tin, antimony, zinc, copper, tungsten, lead, coal, marble, limestone, precious stones, natural gas, and hydropower.
Destructive earthquakes and cyclones; flooding and landslides common during rainy season (June to September); periodic droughts

Deforestation; industrial pollution of air, soil, and water; inadequate sanitation and water treatment contribute to disease

"party to:"
Biodiversity, Climate Change, Desertification, Endangered Species, Law of the Sea, Nuclear Test Ban, Ozone Layer Protection, Ship Pollution, Tropical Timber 83, Tropical Timber 94
"signed, but not ratified:" none of the selected agreements




</doc>
<doc id="20389" url="https://en.wikipedia.org/wiki?curid=20389" title="Demographics of Myanmar">
Demographics of Myanmar

The following is an overview of the demographics of Myanmar (also known as Burma), including statistics such as population, ethnicity, language, education level and religious affiliation.

At the time of the 1983 census in Burma, as of 31 March 1983, the population was 35,442,972. , this was estimated by the "CIA World Factbook" to have increased to 60,584,650. Other estimates put place the total population at around 60 million. China's "People's Daily" reported that Burma had a census in 2007, and at the end of 2009 has 59.2 million people, and growing at 2% annually. with exception for Cyclone Nargis in 2008. Most of these estimates have indeed overlooked the demographic changes that were at work since the 1970s in the country.

Britain-based human rights agencies place the population as high as 70 million. Estimates for the country explicitly take into account the effects of excess mortality due to AIDS. This can result in lower life expectancy, higher infant mortality and death rates, lower population and growth rates, and changes in the distribution of population by age and sex than would otherwise be expected.

No trustworthy census has occurred since the 1930s. In the 1940s, the detailed census results were destroyed during the Japanese invasion of 1942. Census results after that time have been flawed by civil wars and a series of military governments. The census in 1983 occurred at a time when parts of the country were controlled by insurgent groups and inaccessible to the government.

The Provisional results of the 2014 census show that the total population of Myanmar is 51,419,420—a population well below the official estimates of more than 60 million. This total population includes 50,213,067 persons counted during the census and an estimated 1,206,353 persons in parts of northern Rakhine State, Kachin State and Kayin State who were not counted. More females (51.8%) were counted than males (48.2%). People who were out of the country at the time of the census are not included in these figures.

The provisional census results indicated that there were 10,889,348 households in Myanmar. On average, 4.4 people lived in each household in the country. The average household size was highest in Kachin State and Chin State at 5.1. The lowest household sizes were observed in Ayeyawady Region, Bago Region, Magway Region and Naypyidaw Union Territory, each at 4.1.

Burma has a low fertility rate (2.23 in 2011), slightly above replacement level, especially as compared to other Southeast Asian countries of similar economic standing, like Cambodia (3.18) and Laos (4.41), representing a significant decline from 4.7 in 1983 to 2.4 in 2001, despite the absence of any national population policy.

The fertility rate is much pronouncedly lower in urban areas. This is attributed to extreme delays in marriage (almost unparalleled in the region, with the exception of developed countries), the prevalence of illegal abortions, and the high proportion of single, unmarried women of reproductive age (with 25.9% of women aged 30–34 and 33.1% of men and women aged 25–34 single).

These patterns stem from several cultural and economic dynamics. The first is economic hardship, which results in the delay of marriage and family-building (the average age of marriage in Burma is 27.5 for men, 26.4 for women). The second is the social acceptability of celibacy among the Burmese, who are predominantly Buddhist and value celibacy as a means of spiritual development.

Births and deaths 

Total Fertility Rate (TFR) (Wanted Fertility Rate) and Crude Birth Rate (CBR):

Crude Birth Rate (CBR), Total Fertility Rate (TFR), and Total Marital Fertility Rate (TMFR) by region (2014 Myanmar Population and Housing Census):

Structure of the population (01.10.2012) (Estimates) :

Structure of the population (2014) (Census) Population - 51 486 253, enumereted - 50 279 900 :

The Burmese government identifies eight major national ethnic races (which comprise 135 "distinct" ethnic groups), which include the Bamar (68%), Shan (9%), Kayin (7%), Rakhine (4%), Mon (2%), Kayah, and Kachin. However, the government classification system is flawed, because it groups ethnic groups by geography, rather than by linguistic or genetic similarity (e.g. the Kokang are under the Shan ethnicity, although they are a Han-Chinese sub-group).

Unrecognised ethnic groups include Burmese Han-Chinese and Burmese Indians, who form 3% and 2% of the population respectively. The remaining 5% of the population belong to small ethnic groups such as the remnants of the Anglo-Burmese and Anglo-Indian communities, as well as the Lisu, Rawang, Naga, Padaung, Moken, and many minorities across Shan State.

The official language and primary medium of instruction of Burma is Burmese (65%). Multiple languages are spoken in Burma, and include Shan (6.4%), Karen (5.2%), Kachin (1.8%), Chin (1.6%), Mon (1.5%), and Rakhine (1.5%). English is also spoken, particularly by the educated urban elite, and is the secondary language learnt in government schools. Recent years, the education of Chinese language has been recovered, after long-term limitation from the government of Myanmar.

The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.



1.07% (2011 est.)





1.2% of total GDP (2001)



</doc>
<doc id="20390" url="https://en.wikipedia.org/wiki?curid=20390" title="Politics of Myanmar">
Politics of Myanmar

Myanmar (also known as Burma) is a unitary parliamentary republic under its constitution of 2008. The Economist Intelligence Unit has rated Myanmar as "hybrid regime" in 2016. The military of Burma holds a large amount of power in the government, despite the end of the last Burmese military dictatorship.

Myanmar, formerly called Burma, was a monarchy ruled by various dynasties prior to the 19th century. The British colonized Burma (Myanmar) in the late 19th century, and it was part of British India until 1937.

Burma (Myanmar) was ruled as a British colony from 1824 until 1948. While the Bamar heartland was directly administered, first as a part of India and then, from 1937, as British Burma, ethnic regions outside the heartland were allowed some measure of self-rule along the lines of the Princely States of India. This led to split loyalties among the various ethnic groups to outside powers in Burma either to the British or Japanese. The dominant ethnic group in Burma are the Bamar, who make up approximately sixty-eight percent of the population. During World War II, many members of the Bamar ethnic group volunteered to fight alongside the Japanese in hopes of overthrowing the occupying British forces.

Meanwhile, many other ethnic groups supported the Allied forces in combating the Japanese and Burman forces. This conflict would come to be very significant in the aftermath of World War Two when Burma was granted its independence from Great Britain in 1948. By granting independence to Burma, the British government gave the new ruler, Aung San, control over areas that were not traditionally controlled by the Bamar. This conglomeration of formerly British-owned land created a state that is home to over twenty distinct minority ethnic groups.

From the time of the signing of the Burmese Constitution in 1948, ethnic minorities have been denied Constitutional rights, access to lands that were traditionally controlled by their peoples and participation in the government. The various minority ethnic groups have been consistently oppressed by the dominant Burman majority, but have also suffered at the hands of warlords and regional ethnic alliances. Religion also plays a role in the ethnic conflicts that have taken place. Muslims, Hindus, Christians and Buddhists all live in Burma. These religious differences have led to several incidents that have affected hundreds of thousands of citizens that live in Burma.

The SPDC had been responsible for the displacement of several hundred thousand citizens, both inside and outside of Burma. The Karen, Karenni, and Mon ethnic groups have been forced to seek asylum in neighbouring Thailand, where they are also abused by an unfriendly and unsympathetic government. These groups are perhaps more fortunate than the Wa and Shan ethnic groups who have become Internally Displaced Peoples in their own state since being removed from lands by the military junta in 2000. There are reportedly 600,000 of these Internally Displaced Peoples living in Burma today. Many are trying to escape forced labour in the military or for one of the many state-sponsored drug cartels. This displacement of peoples has led to both human rights violations as well as the exploitation of minority ethnic groups at the hands of the dominant Burman group. The primary actors in these ethnic struggles include but are not limited to the Government of Burma (junta), the Karen National Union and the Mong Tai Army.

On 4 January 1948, Burma achieved independence from Britain, and became a democracy based on the parliamentary system.

In late 1946, Aung San became Deputy Chairman of the Executive Council of Burma, a transitional government. But on 19 July 1947, political rivals assassinated Aung San and several cabinet members. On 4 January 1948, the nation became an independent republic, named the "Union of Burma", with Sao Shwe Thaik as its first president and U Nu as its first prime minister. Unlike most other former British colonies, it did not become a member of the Commonwealth. A bicameral parliament was formed, consisting of a Chamber of Deputies and a Chamber of Nationalities. The geographical area Burma encompasses today can be traced to the Panglong Agreement, which combined Burma proper, which consisted of Lower Burma and Upper Burma, and the Frontier Areas, which had been administered separately by the British.

In 1961, U Thant, Burma's Permanent Representative to the United Nations and former secretary to the Prime Minister, was elected Secretary-General of the United Nations; he was the first non-Westerner to head any international organisation and would serve as UN Secretary-General for ten years. Among the Burmese to work at the UN when he was Secretary-General was a young Aung San Suu Kyi.

In 1962, General Ne Win led a coup d'état and established a nominally socialist military government that sought to follow the "Burmese Way to Socialism." The military expropriated private businesses and followed an economic policy of autarky, or economic isolation.

There were sporadic protests against military rule during the Ne Win years and these were almost always violently suppressed. On 7 July 1962, the government broke up demonstrations at Rangoon University, killing 15 students. In 1974, the military violently suppressed anti-government protests at the funeral of U Thant. Student protests in 1975, 1976 and 1977 were quickly suppressed by overwhelming force.

The former Head of state was Senior General Than Shwe who held the title of "Chairman of the State Peace and Development Council." His appointed prime minister was Khin Nyunt until 19 October 2004, when he was forcibly deposed in favour of Gen. Soe Win. Almost all cabinet offices are held by military officers.

US and European government sanctions against the military government, combined with consumer boycotts and shareholder pressure organised by Free Burma activists, have succeeded in forcing most western corporations to withdraw from Burma. However, some western oil companies remain due to loopholes in the sanctions. For example, the French oil company Total S.A. and the American oil company Chevron continue to operate the Yadana natural gas pipeline from Burma to Thailand. Total (formerly TotalFinaElf) is the subject of a lawsuit in French and Belgian courts for alleged complicity in human rights abuses along the gas pipeline. Before it was acquired by Chevron, Unocal settled a similar lawsuit for a reported multimillion-dollar amount. Asian businesses, such as Daewoo, continue to invest in Burma, particularly in natural resource extraction.

The United States and European clothing and shoe industry became the target of Free Burma activists for buying from factories in Burma that were wholly or partly owned by the government or the military. Many stopped sourcing from Burma after protests, starting with Levi Strauss in 1992. From 1992 to 2003, Free Burma activists successfully forced dozens of clothing and shoe companies to stop sourcing from Burma. These companies included Eddie Bauer, Liz Claiborne, Macy's, J. Crew, JoS. A. Banks, Children's Place, Burlington Coat Factory, Wal-Mart, and Target. The US government banned all imports from Burma as part of the "Burmese Freedom and Democracy Act" of 2003. Sanctions have been criticised for their adverse effects on the civilian population. However, Burmese democracy movement leader Aung San Suu Kyi has repeatedly credited sanctions for putting pressure on the ruling military regime.

Human Rights Watch and Amnesty International have documented egregious human rights abuses by the military government. Civil liberties are severely restricted. Human Rights Defenders and Promoters, formed in 2002 to raise awareness among the people of Burma about their human rights, claims that on 18 April 2007, several of its members were met by approximately a hundred people led by a local USDA Secretary U Nyunt Oo and beaten up. The HRDP believes that this attack was condoned by the authorities.

There is no independent judiciary in Burma and the military government suppresses political activity. The government uses software-based filtering from US company Fortinet to limit the materials citizens can access on-line, including free email services, free web hosting and most political opposition and pro-democracy pages.

In 2001, the government permitted NLD office branches to re-open throughout Burma. However, they were shut down or heavily restricted beginning 2004, as part of a government campaign to prohibit such activities. In 2006, many members resigned from NLD, citing harassment and pressure from the Tatmadaw (Armed Forces) and the Union Solidarity and Development Association.

The military government placed Aung San Suu Kyi under house arrest again on 31 May 2003, following an attack on her convoy in northern Burma by a mob reported to be in league with the military. The regime extended her house arrest for yet another year in late November 2005. Despite a direct appeal by Kofi Annan to Than Shwe and pressure from ASEAN, the Burmese government extended Aung San Suu Kyi's house arrest another year on 27 May 2006. She was released in 2010.

The United Nations urged the country to move towards inclusive national reconciliation, the restoration of democracy, and full respect for human rights. In December 2008, the United Nations General Assembly passed a resolution condemning the human rights situation in Burma and calling for Aug San Suu Kyi's release—80 countries voting for the resolution, 25 against and 45 abstentions. Other nations, such as China and Russia, have been less critical of the regime and prefer to co-operate on economic matters.

Facing increasing international isolation, Burma's military government agreed to embark upon a programme of reform, including permitting multiple political parties to contest elections in 2010 and 2012 and the release of political prisoners. However, organisations such as Human Rights Watch allege continued human rights abuses in ongoing conflicts in border regions such as Kachin State.

Myanmar's army-drafted constitution was overwhelmingly approved (by 92.4% of the 22 million voters with alleged voter turnout of 99%) on 10 May 2008 in the first phase of a two-stage referendum and Cyclone Nargis. It was the first national vote since the 1990 election. Multi-party elections in 2010 would end 5 decades of military rule, as the new charter gives the military an automatic 25% of seats in parliament. NLD spokesman Nyan Win, inter alia, criticised the referendum: "This referendum was full of cheating and fraud across the country. In some villages, authorities and polling station officials ticked the ballots themselves and did not let the voters do anything".

An election was held in 2010, with 40 parties approved to contest the elections by the Electoral Commission. some of which are linked to ethnic minorities. The National League for Democracy, which overwhelmingly won the previous 1990 elections but were never allowed to take power, decided not to participate.

The military-backed Union Solidarity and Development Party declared victory, winning 259 of the 330 contested seats. The United Nations and many Western countries have condemned the elections as fraudulent, although the decision to hold elections was praised by China and Russia.

In by-elections held in 2012, the main opposition party National League for Democracy, which was only re-registered for the by-elections on 13 December 2011 won in 43 of the 44 seats they contested (out of 46). Significantly, international observers were invited to monitor the elections, although the government was criticised for placing too many restrictions on election monitors, some of whom were denied visas.

The Union Solidarity and Development Party said it would lodge official complaints to the Union Election Commission on poll irregularities, voter intimidation, and purported campaign incidents that involved National League for Democracy members and supporters, while the National League for Democracy also sent an official complaint to the commission, regarding ballots that had been tampered with.

However, President Thein Sein remarked that the by-elections were conducted "in a very successful manner", and many foreign countries have indicated willingness to lift or loosen sanctions on Burma and its military leaders.

Myanmar general elections were held on 8 November 2015. These were the first openly contested elections held in Myanmar since 1990. The results gave the National League for Democracy an absolute majority of seats in both chambers of the national parliament, enough to ensure that its candidate would become president, while NLD leader Aung San Suu Kyi is constitutionally barred from the presidency.

The resounding victory of Aung San Suu Kyi's National League for Democracy in 2015 general elections has raised hope for a successful political transition from a closely held military rule to a free democratic system. This transition is widely believed to be determining the future of Myanmar.

The president is the de jure head of state and head of government, and oversees the Cabinet of Myanmar. Currently the State Counsellor of Myanmar is the de facto head of government.

On the other hand, Commander-in-chief of Defense ServicesTatmadaw has right to appoint 25% of members in all legislative assembly which means that the legislation cannot be Supermajority without support from Tatmadaw thus preventing from democratically elected members to amend 2008 Constitution of Myanmar which is not created by Burmese citizens. He can also directly appoint ministers in Ministry of Defence (Myanmar) which in turn controls Myanmar Armed Forces and Myanmar Economic Corporation which is the largest economic corporation in Myanmar, Ministry of Border Affairs (Myanmar) which control border affairs of the country and Ministry of Home Affairs (Myanmar) which in turn control Myanmar police forces and administration of the whole country.

Under the 2008 Constitution the legislative power of the Union is shared among the "Pyidaungsu Hluttaw", State and Region Hluttaws. The "Pyidaungsu Hluttaw" consists of the People's Assembly ("Pyithu Hluttaw") elected on the basis of township as well as population, and the House of Nationalities ("Amyotha Hluttaw") with on an equal number of representatives elected from Regions and States. The People's Assembly consists of 440 representatives, with 110 being military personnel nominated by the Commander-in-Chief of the Defence Services. The House of Nationalities consists of 224 representatives with 56 being military personnel nominated by the Commander-in-Chief of the Defence Services.

Burma's judicial system is limited. British-era laws and legal systems remain much intact, but there is no guarantee of a fair public trial. The judiciary is not independent of the executive branch. Burma does not accept compulsory International Court of Justice jurisdiction. The highest court in the land is the Supreme Court. The Chief Justice of the Supreme Court is Tun Tun Oo, and Attorney General is Tun Tun Oo.

Wareru dhammathat or the Manu dhammathat () was the earliest law-book in Burma. It consists of laws ascribed to the ancient Indian sage, Manu, and brought to Burma by Hindu colonists. The collection was made at Wareru’s command, by monks from the writings of earlier Mon scholars preserved in the monasteries of his kingdom. (Wareru seized Martaban in 1281 and obtained the recognition of China as the ruler of Lower Burma and founded a kingdom which lasted until 1539. Martaban was its first capital, and remained so until 1369. It stretched southwards as far as Tenasserim.)

Mon King Dhammazedi (1472–92) was the greatest of the Mon rulers of Wareru’s line. He was famous for his wisdom and the collection of his rulings were recorded in the Kalyani stone inscriptions and known as the Dammazedi pyatton.

Burma is divided into seven regions (previously called divisions-"taing") and seven states ("pyi-nè"), classified by ethnic composition. The seven regions are Ayeyarwady Region, Bago Division, Magway Division, Mandalay Division, Sagaing Division, Tanintharyi Division and Yangon Division; the seven states are Chin State, Kachin State, Kayin State, Kayah State, Mon State, Rakhine State and Shan State.
There are also five Self-administrated zones and a Self-administrated Division "for National races with suitable population"

Within the Sagain Region
Within the Shan State

AsDB, ASEAN, CCC, CP, ESCAP, FAO, G-77, IAEA, IBRD, ICAO, ICRM, IDA, IFAD, IFC, IFRCS, IMF, IMO, Intelsat (nonsignatory user), Interpol, IOC, ITU, NAM, OPCW, UN, UNCTAD, UNESCO, UNIDO, UPU, WHO, WMO, WToO, WTrO, GJC.



</doc>
<doc id="20391" url="https://en.wikipedia.org/wiki?curid=20391" title="Economy of Myanmar">
Economy of Myanmar

Myanmar (also known as Burma) is an emerging economy with a nominal GDP of $69.322 billion in 2017 and an estimated purchasing power adjusted GDP of $327.629 billion billion in 2017 according to World Bank.

Historically, Burma was the main trade route between India and China since 100 BC. The Mon Kingdom of lower Burma served as important trading centre in the Bay of Bengal.

According to Michael Adas, Ian Brown, and other economic historians of Burma, Burma's pre-colonial economy in Burma was essentially a subsistence economy, with the majority of the population involved in rice production and other forms of agriculture. Burma also lacked a formal monetary system until the reign of King Mindon Min in the middle 19th century.

All land was technically owned by the Burmese monarch. Exports, along with oil wells, gem mining and teak production were controlled by the monarch. Burma was vitally involved in the Indian Ocean trade. Logged teak was a prized export that was used in European shipbuilding, because of its durability, and became the focal point of the Burmese export trade from the 1700s to the 1800s.

After Burma was conquered by the British, it became the wealthiest country in Southeast Asia, after the Philippines. It was also once the world's largest exporter of rice. During British administration, Burma supplied oil through the Burmah Oil Company. This supplying market received a setback through the great depression in the 1930s. Burma suffered, like other countries in this region, from the decline in the total level of global trade. Burma also had a wealth of natural and labour resources. It produced 75% of the world's teak and had a highly literate population. The country was believed to be on the fast track to development.

After a parliamentary government was formed in 1948, Prime Minister U Nu embarked upon a policy of nationalisation. He attempted to make Burma a welfare state by adopting central planning measures. The government also tried to implement a poorly thought out Eight-Year plan. But by the 1950s, rice exports had increased by two thirds and mineral exports by over 96%. Plans were implemented in setting up light consumer industries by private sector. The 1962 Burmese coup d'état, was followed by an economic scheme called the Burmese Way to Socialism, a plan to nationalise all industries, with the exception of agriculture. The catastrophic program turned Burma into one of the world's most impoverished countries. Burma's admittance to least developed country status by the United Nations in 1987 highlighted its economic bankruptcy.

After 1988, the regime retreated from totalitarian socialism. It permitted modest expansion of the private sector, allowed some foreign investment, and received much needed foreign exchange. The economy is rated in 2009 as the least free in Asia (tied with North Korea). All fundamental market institutions are suppressed. Private enterprises are often co-owned or indirectly owned by state. The corruption watchdog organisation Transparency International in its 2007 Corruption Perceptions Index released on 26 September 2007 ranked Burma the most corrupt country in the world, tied with Somalia.

The national currency is the kyat. Burma currently has a dual exchange rate system similar to Cuba. The market rate was around two hundred times below the government-set rate in 2006. In 2011, the Burmese government enlisted the aid of International Monetary Fund to evaluate options to reform the current exchange rate system, to stabilise the domestic foreign exchange trading market and creates economic distortions. The dual exchange rate system allows for the government and state-owned enterprises to divert funds and revenues, but also gives the government more control over the local economy and temporarily subdue inflation.

Inflation averaged 30.1% between 2005 and 2007. Inflation is a serious problem for the economy. In April 2007, the National League for Democracy organised a two-day workshop on the economy. The workshop concluded that skyrocketing inflation was impeding economic growth. "Basic commodity prices have increased from 30% to 60% since the military regime promoted a salary increase for government workers in April 2006," said Soe Win, the moderator of the workshop. "Inflation is also correlated with corruption." Myint Thein, an NLD spokesperson, added: "Inflation is the critical source of the current economic crisis."

In recent years, both China and India have attempted to strengthen ties with the government for economic benefit. Many nations, including the United States and Canada, and the European Union, have imposed investment and trade sanctions on Burma. The United States banned all imports from Burma, though this restriction was since lifted. Foreign investment comes primarily from People's Republic of China, Singapore, South Korea, India, and Thailand.

In 2011, when new President Thein Sein's government came to power, Burma embarked on a major policy of reforms including anti-corruption, currency exchange rate, foreign investment laws and taxation. Foreign investments increased from US$300 million in 2009-10 to a US$20 billion in 2010-11 by about 6567%. Large inflow of capital results in stronger Burmese currency, kyat by about 25%. In response, the government relaxed import restrictions and abolished export taxes. Despite current currency problems, Burmese economy is expected to grow by about 8.8% in 2011. After the completion of 58-billion dollar Dawei deep seaport, Burma is expected be at the hub of trade connecting Southeast Asia and the South China Sea, via the Andaman Sea, to the Indian Ocean receiving goods from countries in the Middle East, Europe and Africa, and spurring growth in the ASEAN region.

In 2012, the Asian Development Bank formally began re-engaging with the country, to finance infrastructure and development projects in the country. The $512 million loan is the first issued by the ADB to Myanmar in 30 years and will target banking services, ultimately leading to other major investments in road, energy, irrigation and education projects.

In March 2012, a draft foreign investment law emerged, the first in more than 2 decades. This law oversees unprecedented liberalisation of the economy. It for example stipulates that foreigners no longer require a local partner to start a business in the country, and are able to legally lease land. The draft law also stipulates that Burmese citizens must constitute at least 25% of the firm's skilled workforce, and with subsequent training, up to 50-75%.

On 28 January 2013, the government of Myanmar announced deals with international lenders to cancel or refinance nearly $6 billion of its debt, almost 60 per cent of what it owes to foreign lenders. Japan wrote off US$3 Billion, nations in the group of Paris Club wrote off US$2.2 Billion and Norway wrote off US$534 Million.

Myanmar's inward foreign direct investment has steadily increased since its reform. The country approved US$4.4 billion worth of investment projects between January and November 2014.

According to one report released on 30 May 2013, by the McKinsey Global Institute, Burma's future looks bright, with its economy expected to quadruple by 2030 if it invests in more high-tech industries. This however does assume that other factors (such as drug trade, the continuing war of the government with specific ethnic groups, ...) do not interfere.

As of October 2017, less than 10% of Myanmar‘s population has a bank account. As of 2016-17 98 percent of the population has smartphones and mobile money schemes are being implemented without the use of banks similar to African countries.

In a first ever countrywide study the Myanmar government found that 37 per cent of the nation’s population are unemployed and an average of 26 per cent live in poverty.

The current state of the Burmese economy has also had a significant impact on the people of Burma, as economic hardship results in extreme delays of marriage and family building. The average age of marriage in Burma is 27.5 for men, 26.4 for women, almost unparalleled in the region, with the exception of developed countries like Singapore.

Burma also has a low fertility rate, of 2.07 children per woman (2010), especially as compared to other Southeast Asian countries of similar economic standing, like Cambodia (3.18) and Laos (4.41), representing a significant decline from 4.7 in 1983, despite the absence of a national population policy. This is at least partly attributed to the economic strain that additional children place on the family income, and has resulted in the prevalence of illegal abortions in the country, as well as use of other forms of birth control.

The 2012 foreign investment law draft, included a proposal to transform the Myanmar Investment Commission from a government-appointed body into an independent board. This could bring greater transparency to the process of issuing investment licenses, according to the proposed reforms drafted by experts and senior officials. However, even with this draft, it will still remain a question on whether corruption in the government can be addressed (links have been shown between certain key individuals inside the government and the drug trade, as well as many industries that use forced labour -for example the mining industry-).

Many regions (such as the Golden Triangle) remain off-limits for foreigners, and in some of these regions, the government is still at war with certain ethnic groups.

The major agricultural produce is rice which covers about 60% of the country's total cultivated land area. Rice accounts for 97% of total food grain production by weight. Through collaboration with the International Rice Research Institute (IRRI), 52 modern rice varieties were released in the country between 1966 and 1997, helping increase national rice production to 14 million tons in 1987 and to 19 million tons in 1996. By 1988, modern varieties were planted on half of the country's rice fields, including 98% of the irrigated areas. In 2011, Myanmar's total milled rice production accounted for 10.26 million tons, an increase from the 1.8 per cent back in 2010.

In northern Burma opium bans have ended a century old tradition of growing poppy. Between 20,000 and 30,000 ex-poppyfarmers left the Kokang region as a result of the ban in 2002. People from the Wa region, where the ban was implemented in 2005, fled to areas where growing opium is still possible. Other ex-poppyfarmers are being relocated to areas near rubber plantations. These are often mono-plantations from Chinese investors.

Rubber plantations are being promoted in areas of high elevation like Mong Mao. Sugar plantations are grown in the lowlands such as Mong Pawk District.

The lack of an educated workforce skilled in modern technology contributes to the growing problems of the economy.

Lately, the country lacks adequate infrastructure. Goods travel primarily across Thai and China borders and through the main port in Yangon.

Railroads are old and rudimentary, with few repairs since their construction in the late nineteenth century. Presently China and Japan are providing aid to upgrade rail transport. Highways are normally paved, except in the remote border regions. Energy shortages are common throughout the country including in Yangon. About 30 percent of the country's population is without electricity, with 70 per cent of people living in rural areas.The civilian government has indicated that electricity will be imported from Laos to fulfill demand.

The private sector dominates in agriculture, light industry, and transport activities, while the government controls energy, heavy industry, and military industries.

The garment industry is a major job creator in the Yangon area, with around 200,000 workers employed in total in mid-2015. The Myanmar Government has introduced minimum wage of MMR 3,600 (US$2.80) per day for the garment workers from 1 September 2015.

The Myanmar garments sector has seen significant influx of foreign direct investment, if measured by the number of entries
rather than their value. In March 2012, six of Thailand's largest garment manufacturers announced that they would move production to Burma, principally to the Yangon area, citing lower labour costs. In mid-2015, about 55% of officially registered garment firms in Myanmar were known to be fully or partly foreign-owned, with about 25% of the foreign firms from China and 17% from Hong Kong. Foreign-linked firms supply almost all garment exports, and these have risen rapidly in recent years, especially since EU sanctions were lifted in 2012. Myanmar exported $1.6 billion worth of garment and textiles in 2016.

Burma (Myanmar) is the largest producer of methamphetamines in the world, with the majority of "ya ba" found in Thailand produced in Burma, particularly in the Golden Triangle and Northeastern Shan State, which borders Thailand, Laos and China. Burmese-produced "Ya ba" is typically trafficked to Thailand via Laos, before being transported through the northeastern Thai region of Isan.

In 2010, Burma trafficked 1 billion tablets to neighbouring Thailand. In 2009, Chinese authorities seized over 40 million tablets that had been illegally trafficked from Burma. Ethnic militias and rebel groups (in particular the United Wa State Army) are responsible for much of this production; however, the Burmese military units are believed to be heavily involved in the trafficking of the drugs.

Burma is also the 2nd largest supplier of opium (following Afghanistan) in the world, with 95% of opium grown in Shan State. Illegal narcotics have generated $1 to $2 billion USD in exports annually, with estimates of 40% of the country's foreign exchange coming from drugs. Efforts to eradicate opium cultivation have pushed many ethnic rebel groups, including the United Wa State Army and the Kokang to diversify into methamphetamine production.

Prior to the 1980s, heroin was typically transported from Burma to Thailand, before being trafficked by sea to Hong Kong, which was and still remains the major transit point at which heroin enters the international market. Now, drug trafficking has circumvented to southern China (from Yunnan, Guizhou, Guangxi, Guangdong) because of a growing market for drugs in China, before reaching Hong Kong.

The prominence of major drug traffickers have allowed them to penetrate other sectors of the Burmese economy, including the banking, airline, hotel and infrastructure industries. Their investment in infrastructure have allowed them to make more profits, facilitate drug trafficking and money laundering.


The Union of Myanmar's rulers depend on sales of precious stones such as sapphires, pearls and jade to fund their regime. Rubies are the biggest earner; 90% of the world's rubies come from the country, whose red stones are prized for their purity and hue. Thailand buys the majority of the country's gems. Burma's ""Valley of Rubies"", the mountainous Mogok area, north of Mandalay, is noted for its rare pigeon's blood rubies and blue sapphires.

In 2007, following the crackdown on pro-democracy protests in Myanmar, human rights organisations, gem dealers, and US First Lady Laura Bush called for a boycott of a Myanmar gem auction held twice yearly, arguing that the sale of the stones profits the dictatorial regime in that country. Debbie Stothard of the Alternative ASEAN Network on Burma stated that mining operators used drugs on employees to improve productivity, with needles shared, raising the risk of HIV infection: "These rubies are red with the blood of young people." Brian Leber (41-year-old jeweller who founded The Jewellers' Burma Relief Project) stated that: "For the time being, Burmese gems should not be something to be proud of. They should be an object of revulsion. It's the only country where one obtains really top quality rubies, but I stopped dealing in them. I don't want to be part of a nation's misery. If someone asks for a ruby now I show them a nice pink sapphire."

Richard W. Hughes, author of Ruby and Sapphire, a Bangkok-based gemologist who has made many trips to Burma makes the point that for every ruby sold through the junta, another gem that supports subsistence mining is smuggled over the Thai border. Burma's gemstone industry is a cornerstone of the Burmese economy with exports topping $1 billion.

The permits for new gem mines in Mogoke, Mineshu and Nanyar state will be issued by the ministry according to a statement issued by the ministry on 11 February. While many sanctions placed on the former regime were eased or lifted in 2012, the US has left restrictions on importing rubies and jade from Myanmar intact. According to recent amendments to the new Myanmar foreign investment law, there is no longer a minimum capital requirement for investments, except in mining ventures, which require substantial proof of capital and must be documented through a domestic bank. Another important clarification in the investment law is the dropping of foreign ownership restrictions in joint ventures, except in restricted sectors, such as mining, where FDI will be capped at 80 per cent.

Since 1992, the government has encouraged tourism. Until 2008, fewer than 750,000 tourists entered the country annually, but there has been substantial growth over the past years. In 2012, 1.06 million tourists visited the country, and 1.8 million are expected to visit by the end of 2013.

Tourism is thus a growing sector of the economy of Burma. Burma has diverse and varied tourist attractions and is served internationally by numerous airlines via direct flights. Domestic and foreign airlines also operate flights within the country. Cruise ships also dock at Yangon. Overland entry with a border pass is permitted at several border checkpoints. The government requires a valid passport with an entry visa for all tourists and business people. As of May 2010, foreign business visitors from any country can apply for a visa on arrival when passing through Yangon and Mandalay international airports without having to make any prior arrangements with travel agencies. Both the tourist visa and business visa are valid for 28 days, renewable for an additional 14 days for tourism and three months for business. Seeing Burma through a personal tour guide is popular. Travelers can hire guides through travel agencies.

Before 2012, much of the country was completely off-limits to tourists, and the military very tightly controlled interactions between foreigners and the people of Burma. Locals were not allowed to discuss politics with foreigners, under penalty of imprisonment, and in 2001, the Myanmar Tourism Promotion Board issued an order for local officials to protect tourists and limit "unnecessary contact" between foreigners and ordinary Burmese people. Since 2012, Burma has opened up to more tourism and foreign capital, synonymous with the country's transition to democracy.

This is a chart of trend of gross domestic product of Burma at market prices estimated by the International Monetary Fund and EconStats with figures in millions of Myanma kyats.

Though foreign investment has been encouraged, it has so far met with only moderate success. This is because foreign investors have been adversely affected by the junta government policies and because of international pressure to boycott the junta government. The United States has placed trade sanctions on Burma. The European Union has placed embargoes on arms, non-humanitarian aid, visa bans on military regime leaders, and limited investment bans. Both the European Union and the US have placed sanctions on grounds of human rights violations in the country. Many nations in Asia, particularly India, Thailand and China have actively traded with Burma. However, on 22 April the EU suspended economic and political sanctions against Burma.

The public sector enterprises remain highly inefficient and also privatisation efforts have stalled. The estimates of Burmese foreign trade are highly ambiguous because of the great volume of black market trading. A major ongoing problem is the failure to achieve monetary and fiscal stability. Due to this, Burma remains a poor country with no improvement of living standards for the majority of the population over the past decade. The main causes for continued sluggish growth are poor government planning, internal unrest, minimal foreign investment and the large trade deficit. One of the recent government initiatives is to utilise Burma's large natural gas deposits. Currently, Burma has attracted investment from Thai, Malaysian, Filipino, Russian, Australian, Indian, and Singaporean companies. Trade with the US amounted to $243.56 million as of February 2013, accounting for 15 projects and just 0.58 per cent of the total, according to government statistics.

"The Economist"s special report on Burma points to increased economic activity resulting from Burma's political transformation and influx of foreign direct investment from Asian neighbours. Near the Mingaladon Industrial Park, for example, Japanese-owned factories have risen from the "debris" caused by "decades of sanctions and economic mismanagement." Japanese Prime Minister Shinzō Abe has identified Burma as economically attractive market that will help stimulate the Japanese economy. Among its various enterprises, Japan is helping build the Thilawa Port, which is part of the Thilawa Special Economic Zone, and helping fix the electricity supply in Yangon.

Japan is not the largest investor in Myanmar. "Thailand, for instance, the second biggest investor in Myanmar after China, is forging ahead with a bigger version of Thilawa at Dawei, on Myanmar's Tenasserim Coast ... Thai rulers have for centuries been toying with the idea of building a canal across the Kra Isthmus, linking the Gulf of Thailand directly to the Andaman Sea and the Indian Ocean to avoid the journey round peninsular Malaysia through the Strait of Malacca."

Dawei would give Thailand that connection. China, by far the biggest investor in Burma, has focused on constructing oil and gas pipelines that "crisscross the country, starting from a new terminus at Kyaukphyu, just below Sittwe, up to Mandalay and on to the Chinese border town of Ruili and then Kunming, the capital of Yunnan province". This would prevent China from "having to funnel oil from Africa and the Middle East through the bottleneck around Singapore".

According to the CIA World Factbook,
Financing Geothermal projects in Myanmar use an estimated break even power cost of 5.3–8.6 U.S cents/kWh or in Myanmar Kyat 53–86K per kWh. This pegs a non-fluctuating $1=1000K, which is a main concern for power project funding. The main drawback with depreciation pressures, in the current FX market.
Between June 2012 and October 2015, the Myanmar Kyat depreciated by approximately 35%, from 850 down to 1300 against the US Dollar. Local businesses with foreign denominated loans from abroad suddenly found themselves rushing for a strategy to mitigate currency risks. Myanmar's current lack of available currency hedging solutions presents a real challenge for Geothermal project financing.

The level of international aid to Burma ranks amongst the lowest in the world (and the lowest in the Southeast Asian region)—Burma receives the $4 per capita in development assistance, as compared to the average of $42.30 per capita.

In April 2007, the US Government Accountability Office (GAO) identified the financial and other restrictions that the military government places on international humanitarian assistance in the Southeast Asian country. The GAO report, entitled "Assistance Programs Constrained in Burma," outlines the specific efforts of the Burmese government to hinder the humanitarian work of international organisations, including by restricting the free movement of international staff within the country. The report notes that the regime has tightened its control over assistance work since former Prime Minister Khin Nyunt was purged in October 2004.

Furthermore, the reports states that the military government passed guidelines in February 2006, which formalised Burma's restrictive policies. According to the report, the guidelines require that programs run by humanitarian groups "enhance and safeguard the national interest" and that international organisations co-ordinate with state agents and select their Burmese staff from government-prepared lists of individuals. United Nations officials have declared these restrictions unacceptable.

US Representative Ileana Ros-Lehtinen (R-FL) said that the report "underscores the need for democratic change in Burma, whose military regime arbitrarily arrests, tortures, rapes and executes its own people, ruthlessly persecutes ethnic minorities, and bizarrely builds itself a new capital city while failing to address the increasingly urgent challenges of refugee flows, illicit narcotics and human trafficking, and the spread of HIV/AIDS and other communicable diseases." 

Electricity - production:
5.961 billion kWh (2006 est.)

Electricity - consumption:
4.298 billion kWh (2006 est.)

Electricity - exports:
0 kWh (2007)

Electricity - imports:
0 kWh (2007)

Agriculture - products:
rice, pulses, beans, sesame, groundnuts, sugarcane; hardwood; fish and fish products

Currency:
1 kyat (K) = 100 pyas

Exchange rates:
kyats per US dollar - 1,205 (2008 est.), 1,296 (2007), 1,280 (2006), 5.82 (2005), 5.7459 (2004), 6.0764 (2003)
note: unofficial exchange rates ranged in 2004 from 815 kyat/US dollar to nearly 970 kyat/US dollar, and by year end 2005, the unofficial exchange rate was 1,075 kyat/US dollar; data shown for 2003-05 are official exchange rates

Foreign Direct Investment
In the first nine months of 2012-2013, Myanmar has received investment of USD 794 million. China has biggest of investment commitments for this fiscal.

Foreign Trade
Total foreign trade for 2012 was recorded to USD 13.3 billion. It was 27% of Myanmar's GDP.




</doc>
<doc id="20392" url="https://en.wikipedia.org/wiki?curid=20392" title="Telecommunications in Myanmar">
Telecommunications in Myanmar

Myanmar has begun the liberalization of its telecoms market in 2013.

Previously, Myanmar Post and Telecommunication had a monopoly in the country. In 2013, the government started taking steps to open up the telecommunications market, issuing licenses to new service providers. In 2014, Qatar-based Ooredoo and Norwegian Telenor Group entered the market, resulting in the reduction of consumer prices and a rapid growth in the number of subscribers, as well as the expansion of the country's infrastructure. In November 2015, Ericsson named Myanmar the world's fourth fastest-growing mobile market. As of June 2015, Myanmar has a mobile phone penetration rate of 54.6%, up from less than 10% in 2012.

General assessment: meets minimum requirements for local and intercity service for business and government
Domestic: system barely capable of providing basic service; cellular phone system is grossly underdeveloped with a subscribership base of less than 1 per 100 persons
International: country code - 95; landing point for the SEA-ME-WE 3 optical telecommunications submarine cable that provides links to Asia, the Middle East, and Europe; satellite earth stations - 2, Intelsat (Indian Ocean) and ShinSat (2007)

Bids have been offered for two fresh telecom licenses by the Myanmar government. The deadline was set to be 8 February 2013. The licenses were expected to be issued in June and carry a contract duration of up to 20 years. Two more licenses were expected to be offered following this round of bidding.

According to government statistics, 5.4 million of Myanmar’s 60 million population had a mobile phone subscription at the end of 2012, giving the country a mobile penetration of 9 per cent.

According to official figures released in mid-2012, Myanmar had 857 base transceiver stations (BTS) for 1,654,667 local GSM mobile users, 188 BTSs for 225,617 local WCDMA mobile users, 366 BTSs for 633,569 local CDMA-450 mobile users, and 193 BTSs for 341,687 CDMA-800 mobile users. Huawei who has built 40 percent of the towers and ZTE has built 60 percent in Myanmar, which amounts to 1500 across the country, said it has built the towers mostly in Yangon, Mandalay and Naypyidaw.

The Myanmar Telecommunications Operator Tender Evaluation and Selection Committee selected Norwegian Telenor Group and Ooredoo of Qatar as winners of the bidding, for the two telecom licences issued by the government of Myanmar. The licenses allow the operators to build and operate a nationwide wireless network for 15 years. Ooredoo began selling low-price SIM cards at a price of US$1.5 in Yangon, Mandalay and Naypyidaw in August 2014. Prior to 2012, during military rule, SIM cards cost USD 1,500.

Radio broadcast stations
AM 2, FM 9, shortwave 3 (2015)

Television broadcast stations:
6 (2015)

Press

Television

Broadcasts 7 Free Digital Channel available in Naypyidaw, Yangon & Mandalay.
Providing Free to air Channels, Local & International Pay TV Channels, and High Definition Channels. 4TV Has Only Broadcasts 2 Way With DTH and DVB-T2 In Myanmar.

Radio

News agency

The government now allows unrestricted access to the Internet. Many people are using the internet freely, often with widely available smart phones.

Myanmar Teleport (formerly Bagan Cybertech), Information Technology Central Services (ITCS), and the state-owned Myanmar Post and Telecommunication (MPT) are two of the Internet service providers in Myanmar. Internet cafés are common in the larger cities of the country. Satellite (VSAT)internet connection is also available from Skynet, a satellite television provider, and another (VSAT) Operator Com & Com.

According to MPT's official statistics as of July 2010, the country had over 400,000 Internet users (0.8% of the population) with the vast majority of the users located in the two largest cities, Yangon and Mandalay. More recent figures are hard to find, but the widespread use of smart phones and tablets with cellular modems on the 3G and 4G networks means that internet usage is likely to be far higher than the figures from 2010 indicate.




</doc>
<doc id="20393" url="https://en.wikipedia.org/wiki?curid=20393" title="Transport in Myanmar">
Transport in Myanmar

The government of Myanmar (earlier known as Burma) has two ministries controlling transportation:

"total:" 
<br>"paved:" 
<br>"unpaved:" (2006)

The main highways are as follows:

There is one expressway in the country, which features double carriageway and four lanes on its entire length:

The other Highways are as follows:
In 2017, Yangon launched a bus network system that would reduce traffic and commute time of some two million commuters in the city.

, Myanmar had of railways, all gauge. There are currently no rail links to adjacent countries.


Orient-Express Hotels Ltd operates its business in Ayeyarwady River by the name "Road to Mandalay River Cruise". Irrawaddy Flotilla Company was also in service along the Ayeyarwady River in the 20th century, until 1942, when the fleet was destroyed to prevent invading Japanese forces from making use of it. The IFC has since been revived as Pandaw, named for a salvaged original IFC ship, and is now one of the leading river cruise companies in the country.

"total:"
24 ships (with a volume of or over) totalling /
<br>"Ships by type:"
bulk carrier 1, cargo ship 17, passenger ship 2, passenger/cargo 3, specialised tanker 1 (2008)
<br>"note:"
a flag of convenience registry; includes ships of 3 countries: Cyprus 1, Germany 1, Japan 1



In July 2010, the country had 69 airports. Only 11 of them had runways over 3250 meters. Of the 11, only Yangon International and Mandalay International had adequate facilities to handle larger jets.
<br>"total:" 69
<br>"over 3,047 m:" 11
<br>"1524 to 3,047 m:" 27
<br>"Under 1524 m:" 31

4




</doc>
<doc id="20394" url="https://en.wikipedia.org/wiki?curid=20394" title="Tatmadaw">
Tatmadaw

The Tatmadaw (, ) is the official name of the armed forces of Myanmar (Burma). It is administered by the Ministry of Defence and composed of the Army, the Navy and the Air Force. Auxiliary services include the Myanmar Police Force, the People's Militia Units and until 2013 the Frontier Forces, locally known as "Na Sa Kha".

According to the Constitution of Myanmar, the Tatmadaw directly reports to the National Defence and Security Council (NDSC). The NDSC is an eleven-member national security council responsible for security and defence affairs in Myanmar. The NDSC serves as the highest authority in the Government of Myanmar.

Currently, there is no military draft in Myanmar. Thus, all service personnel are theoretically volunteers, but the People's Militia Law allows for conscription if the President considers it necessary for Myanmar's defence that the provisions of the law be activated. The Tatmadaw has been engaged in a bitter battle with ethnic insurgents and the narco-armies since the country gained its independence from the United Kingdom in 1948. However, in a 2014 survey conducted by the International Republican Institute across all Myanmar demographics shows military is the most favourable institution with 84% of respondents saying either "very favorable" or "favorable" ahead of other institutions such as media, government and Burmese opposition.

The military proposed a defence budget of K 2.36 trillion (USD 2.39 billion) for 2014–15 and was approved by the Parliament. The incumbent Minister for Defence Wai Lwin revealed at a Parliament section on 28 October 2014 that 46.2% of the budget is spent on personnel cost, 32.89% on operation and procurement, 14.49% on construction related projects and 2.76% on health and education.

The Royal Armed Forces was the armed forces of the Burmese monarchy from the 9th to 19th centuries. It refers to the military forces of the Pagan Dynasty, the Ava Kingdom, the Toungoo Dynasty and the Konbaung Dynasty in chronological order. The army was one of the major armed forces of Southeast Asia until it was defeated by the British over a six-decade span in the 19th century.

The army was organised into a small standing army of a few thousand, which defended the capital and the palace, and a much larger conscription-based wartime army. Conscription was based on the ahmudan system, which required local chiefs to supply their predetermined quota of men from their jurisdiction on the basis of population in times of war. The wartime army also consisted of elephantry, cavalry, artillery and naval units.

Firearms, first introduced from China in the late 14th century, became integrated into strategy only gradually over many centuries. The first special musket and artillery units, equipped with Portuguese matchlocks and cannon, were formed in the 16th century. Outside the special firearm units, there was no formal training program for the regular conscripts, who were expected to have a basic knowledge of self-defence, and how to operate the musket on their own. As the technological gap between European powers widened in the 18th century, the army was dependent on Europeans' willingness to sell more sophisticated weaponry.

While the army had held its own against the armies of the kingdom's neighbours, its performance against more technologically advanced European armies deteriorated over time. While it defeated the Portuguese and French intrusions in the 17th and 18th centuries respectively, the army could not stop the advance of the British Empire in the 19th century, losing all three Anglo-Burmese wars. On 1 January 1886, the millennium-old Burmese monarchy and its military arm, the Royal Burmese Army, were formally abolished by the British.

The British used mainly Indian and Gurkha troops to conquer and pacify the country. In a divide-and-rule manoeuvre, the British enforced their rule in the province of Burma mainly with Indian troops later joined by indigenous military units of three indigenous ethnic minorities: Karens, Kachins and Chins. The British did not trust the Burmese. Before 1937, with few exceptions, no Burmese were allowed to serve in the military.

At the beginning of World War I, the only indigenous military regiment in the British India army, the 70th Burma Rifles, consisted of three battalions, made up of Karens, Kachins and Chins. During the war, the British relaxed the ban, raising a Burmese battalion in the 70th Burma Rifles, a Burmese company in the 85th Burma Rifles, and seven Burmese Mechanical Transport companies. In addition, three companies of Burma Sappers and Miners, made up of mostly Burmese, and a company of Labour Corps, made up of Chins and Burmese, were also raised. All these units began their overseas assignment in 1917. The 70th Burma Rifles served in Egypt for garrison duties while the Burmese Labour Corps served in France. One company of Burma Sappers and Miners distinguished themselves in Mesopotamia at the crossing the Tigris.

After the war, the British stopped recruiting Burmese, and discharged all but one Burmese companies had been abolished by 1925. The last Burmese company of Burma Sappers and Miners too was disbanded in 1929. The British used Indian and ethnic minority dominated troops to ruthlessly put down ethnic majority dominated rebellions such as Saya San's peasant rebellion in 1930–1931. These policies would lead to long-term negative tensions among the country's ethnic groups. On 1 April 1937, Burma was made a separate colony, and Burmese were now eligible to join the army. But few Burmese bothered to join. Before World War II began, the British Burma Army consisted of Karen (27.8%), Chin (22.6%), Kachin (22.9%), and Burmese 12.3%, without counting their British officer corps.

In December 1941, a group of Burmese independence activists founded the Burma Independence Army (BIA) with Japanese help. The army led by Aung San fought in the Burma Campaign on the side of the Imperial Japanese Army. Thousands of young men joined its ranks—reliable estimates range from 15,000 to 23,000. The great majority of the recruits were Burmese, with little ethnic minority representation. Many of the fresh recruits lacked discipline. At Myaungmya in the Irrawaddy delta, an ethnic war broke out between Burmese BIA men and Karens, with both sides responsible for massacres. The BIA was soon replaced with the Burma Defence Army, founded on 26 August 1942 with three thousand BIA veterans. The army became Burma National Army with Ne Win as its commander on 1 August 1943 when Burma achieved nominal independence. In late 1944, it had a strength of approximately 15,000.

Disillusioned by the Japanese occupation, the BNA switched sides, and joined the allied forces on 27 March 1945.

At the time of Myanmar's independence in 1948, the Tatmadaw was weak, small and disunited. Cracks appeared along the lines of ethnic background, political affiliation, organisational origin and different services. Its unity and operational efficiency was further weakened by the interference of civilians and politicians in military affairs, and the perception gap between the staff officers and field commanders. The most serious problem was the tension between Karen Officers, coming from the British Burma Army and Burmese officers, coming from the Patriotic Burmese Force (PBF).

In accordance with agreement reached at the Kandy Conference in September 1945, the "Tatmadaw" was reorganised by incorporating the British Burma Army and the Patriotic Burmese Force. The officer corps shared by ex-PBF officers and officers from the British Burma Army and Army of Burma Reserve Organisation (ARBO). The British also decided to form what were known as "Class Battalions" based on ethnicity. There were a total of 15 rifle battalions at the time of independence and four of them were made up of former members of PBF. None of the influential positions within the War Office and commands were manned with former PBF Officers. All services including military engineers, supply and transport, ordnance and medical services, Navy and Air Force were commanded by former Officers from ABRO and British Burma Army.
The War Office was officially opened on 8 May 1948 under the Ministry of Defence and managed by a War Office Council chaired by the Minister of Defence. At the head of War Office was Chief of Staff, Vice Chief of Staff, Chief of Naval Staff, Chief of Air Staff, Adjutant General and Quartermaster General. Vice Chief of Staff, who was also Chief of Army Staff and the head of General Staff Office. VCS oversee General Staff matters and there were three branch offices: GS-1 Operation and Training, GS-2 Staff Duty and Planning; GS-3 Intelligence. Signal Corps and Field Engineering Corps are also under the command of General Staff Office.

According to the war establishment adopted on 14 April 1948, Chief of Staff was under the War Office with the rank of major general. It was subsequently upgraded to a lieutenant general. Vice Chief of Staff was a brigadier general. The Chief of Staff was staffed with GSO-I with the rank of lieutenant colonel, three GSO-II with the rank of major, four GSO-III with the rank of captain for operation, training, planning and intelligence, and one Intelligence Officer (IO). The Chief of Staff office also had one GSO-II and one GSO-III for field engineering, and the Chief Signal Officer and a GSO-II for signal. Directorate of Signal and Directorate Field Engineering are also under General Staff Office.

Under Adjutant General Office were Judge Advocate General, Military Secretary, Vice Adjutant General. The Adjutant General (AG) was a brigadier general whereas the Judge Advocate General (JAG), Military Secretary (MS) and Vice Adjutant General (VAG) were colonels. VAG handles adjutant staff matters and there were also three branch offices; AG-1 planning, recruitment and transfer; AG-2 discipline, moral, welfare, and education; AG-3 salary, pension, and other financial matters. The Medical Corps and the Provost Marshall Office were under the Adjutant General Office.

The Quarter Master General office also had three branch offices: QG-1 planning, procurement, and budget; QG-2 maintenance, construction, and cantonment; and QG-3 transportation. Under the QMG office were Garrison Engineering Corps, Electrical and Mechanical Engineering Corps, Military Ordnance Corps, and the Supply and Transport Corps.

Both AG and QMG office similar structure to the General Staff Office, but they only had three ASO-III and three QSO-III respectively.

The Navy and Air Force were separate services under the War office but under the Chief of Staff.

As per War Office order No. (9) 1955 on 28 September 1955, the Chief of Staff become Commander in Chief, the Chief of Army Staff become Vice Chief of Staff (Army), the Chief of Naval Staff become Vice Chief of Staff (Navy) and the Chief of Air Staff become Vice Chief of Staff (Air).

On 1 January 1956, War Office was officially renamed as Ministry of Defence. General Ne Win became the first Chief of Staff of Tatmadaw (Myanmar Armed Forces) to command all three services - Army, Navy and Airforce - under a single unified command for the first time.

Brigadier General Aung Gyi was given the post of Vice Chief of Staff (Army). Brigadier General D. A Blake became commander of South Burma Subdistrict Command (SBSD) and Brigadier General Kyaw Zaw, a member of the Thirty Comrades, became Commander of North Burma Subdistrict Command (NBSD).

Due to deteroriating political situations in 1957, the then Prime minister of Burma, U Nu invited General Ne Win to form a "Caretaker Government" and handed over power on 28 October 1958. Under the stewardship of the Military Caretaker Government, parliamentary elections were held in February 1960. Several high-ranking and senior officers were dismissed due to their involvement and supporting various political parties.

The elections of 1960 had put U Nu back as the Prime Minister and Pyidaungsu Party (Union Party) led civilian government resume control of the country.

On 2 March 1962, the then Chief of Staff of Armed Forces, General Ne Win staged a coup d'état and formed the "Union Revolutionary Council". Around midnight the troops began to move into Yangon to take up strategic position. Prime Minister U Nu and his cabinet ministers were taken into protective custody. At 8:50 am, General Ne Win announced the coup over the radio. He said ""I have to inform you, citizens of the Union that Armed Forces have taken over the responsibility and the task of keeping the country's safety, owing to the greatly deteriorating conditions of the Union."" 

The country would be ruled by the military for the next 12 years. The Burma Socialist Programme Party became the sole political party and it the majority of its full members were military. Government servants underwent military training and the Military Intelligence Service functioned as the secret police of the state.

At the height of the Four Eights Uprising against the socialist government, Former General Ne Win, who at the time was Chairman of the ruling Burma Socialist Programme Party (BSPP), issued a warning against potential protestors during a televised speech. He stated that if the "disturbances" continued the "Army would have to be called and I would like to make it clear that if the Army shoots, it has no tradition of shooting into the Air, it would shoot straight to hit".

Subsequently, the 22 Light Infantry Division, 33 Light Infantry Division and the 44 Light Infantry Division were redeployed to Yangon from front line fighting against ethnic insurgents in the Karen states. Battalions from three Light Infantry Divisions, augmented by infantry battalions under Yangon Regional Military Command and supporting units from Directorate of Artillery and Armour Corps were deployed during the suppression of protests in and around the then capital city of Yangon.

Initially, these troops were deployed in support of the then People's Police Force (now known as Myanmar Police Force) security battalions and to patrol the streets of the capital and to guard government offices and building. However, at midnight of 8 August 1988 troops from 22 Light Infantry Division guarding Yangon City Hall opened fire on unarmed protesters as the crack down against the protests began.

The armed forces under General Saw Maung formed a State Law and Order Restoration Council, repealed the constitution and declared martial law on 18 September 1988. By late September the military had complete control of the country.

Following Myanmar's political openness, Myanmar has made substantial shifts in its relations with major powers comprising China, Russia and the United States. In 2014, Lieutenant-General Anthony Crutchfield, the deputy commander of the United States Pacific Command (USPACOM), was invited to address his counterparts at the Myanmar National Defence College in Naypyidaw, which trains colonels and other high-ranking military officers. In May 2016, Myanmar's Union Parliament approved a military cooperation agreement with Russia following a proposal by Deputy Minister of Defence. In June 2016, Myanmar and Russia signed a defence cooperation agreement. The agreement will envisage exchanging information on international security issues, including fight against terrorism, cooperation in the sphere of culture and vacation of servicemen and their families, along with exchanging experience in peacekeeping activities. 

Moreover, in response to Naypyidaw’s post-2011 political and economic reforms, Australia re-established a ‘normal’ bilateral relationship with Myanmar to support democratisation and reform. In June 2016, the Australian Federal Police signed a new Memorandum of Understanding with its Myanmar counterparts aimed at enhancing transnational crime cooperation and intelligence sharing.

The initial development of Burmese military doctrine post-independence was developed in the early 1950s to cope with external threats from more powerful enemies with a strategy of Strategic Denial under conventional warfare. The perception of threats to state security was more external than internal threats. The internal threat to state security was managed through the use of a mixture of force and political persuasion. Lieutenant Colonel Maung Maung drew up defence doctrine based on conventional warfare concepts, with large infantry divisions, armoured brigades, tanks and motorised war with mass mobilisation for the war effort being the important element of the doctrine.

The objective was to contain the offensive of the invading forces at the border for at least three months, while waiting for the arrival of international forces, similar to the police action by international intervention forces under the directive of United Nations during the war on Korean peninsula. However, the conventional strategy under the concept of total war was undermined by the lack of appropriate command and control system, proper logistical support structure, sound economic bases and efficient civil defence organisations.

At the beginning of the 1950s, while Tatmadaw was able to reassert its control over most part of the country, Kuomintang (KMT) troops under General Li Mai, with support from United States, invaded Burma and used the country's frontier as a springboard for attack against People's Republic of China, which in turn became the external threat to state security and sovereignty of Burma. The first phase of the doctrine was tested for the first time in Operation "Naga Naing" in February 1953 against invading KMT forces. The doctrine did not take into account logistic and political support for KMT from United States and as a result it failed to deliver the objectives and ended in humiliating defeat for the Tatmadaw.

The then Tatmadaw leadership argued that the excessive media coverage was partly to blame for the failure of Operation "Naga Naing". For example, Brigadier General Maung Maung pointed out that newspapers, such as the "Nation", carried reports detailing the training and troops positioning, even went as far to the name and social background of the commanders who are leading the operation thus losing the element of surprise. Colonel Saw Myint, who was second in command for the operation, also complained about the long lines of communications and the excessive pressure imposed upon the units for public relations activities to prove that the support of the people was behind the operation.

Despite failure, Tatmadaw continued to rely on this doctrine until the mid-1960s. The doctrine was under constant review and modifications throughout KMT invasion and gained success in anti-KMT operations in the mid and late 1950s. However, this strategy became increasingly irrelevant and unsuitable in the late 1950s as the insurgents and KMT changed their positional warfare strategy to hit and run guerrilla warfare.

At the 1958 Tatmadaw's annual Commanding Officers (COs) conference, Colonel Kyi Win submitted a report outlining the requirement for new military doctrine and strategy. He stated that 'Tatmadaw did not have a clear strategy to cope with insurgents', even though most of Tatmadaw's commanders were guerrilla fighters during the anti-British and Japanese campaigns during the Second World War, they had very little knowledge of anti-guerrilla or counterinsurgency warfare. Based upon Colonel Kyi Win's report, Tatmadaw begin developing an appropriate military doctrine and strategy to meet the requirements of counterinsurgency warfare.

This second phase of the doctrine was to suppress insurgency with people's war and the perception of threats to state security was more of internal threats. During this phase, external linkage of internal problems and direct external threats were minimised by the foreign policy based on isolation. It was common view of the commanders that unless insurgency was suppressed, foreign interference would be highly probable, therefore counterinsurgency became the core of the new military doctrine and strategy. Beginning in 1961, the Directorate of Military Training took charge the research for national defence planning, military doctrine and strategy for both internal and external threats. This included reviews of international and domestic political situations, studies of the potential sources of conflicts, collection of information for strategic planning and defining the possible routes of foreign invasion.

In 1962, as part of new military doctrine planning, principles of anti-guerrilla warfare were outlined and counterinsurgency-training courses were delivered at the training schools. The new doctrine laid out three potential enemies and they are internal insurgents, historical enemies with roughly an equal strength (i.e. Thailand), and enemies with greater strength. It states that in suppressing insurgencies, Tatmadaw must be trained to conduct long-range penetration with a tactic of continuous search and destroy. Reconnaissance, Ambush and all weather day and night offensive and attack capabilities along with winning the hearts and minds of people are important parts of anti-guerrilla warfare. For countering an historical enemy with equal strength, Tatmadaw should fight a conventional warfare under total war strategy, without giving up an inch of its territory to the enemy. For powerful enemy and foreign invaders, Tatmadaw should engage in total people's war, with a special focus on guerrilla strategy.

To prepare for the transition to the new doctrine, Brigadier General San Yu, the then Vice Chief of Staff (Army), sent a delegation led by Lieutenant Colonel Thura Tun Tin was sent to Switzerland, Yugoslavia, Czechoslovakia and East Germany in July 1964 to study organisation structure, armaments, training, territorial organisation and strategy of people's militias. A research team was also formed at General Staff Office within the War Office to study defence capabilities and militia formations of neighbouring countries.

The new doctrine of total people's war, and the strategy of anti-guerrilla warfare for counterinsurgency and guerrilla warfare for foreign invasion, were designed to be appropriate for Burma. The doctrine flowed from the country's independent and active foreign policy, total people's defence policy, the nature of perceived threats, its geography and the regional environment, the size of its population in comparison with those of its neighbours, the relatively underdeveloped nature of its economy and its historical and political experiences.

The doctrine was based upon 'three totalities': population, time and space (du-thone-du) and 'four strengths': manpower, material, time and morale (Panama-lay-yat). The doctrine did not develop concepts of strategic denial or counter-offensive capabilities. It relied almost totally on irregular low-intensity warfare, such as its guerrilla strategy to counter any form of foreign invasion. The overall counterinsurgency strategy included not only elimination of insurgents and their support bases with the 'four cut' strategy, but also the building and designation of 'white area' and 'black area' as well.

In April 1968, Tatmadaw introduced special warfare training programmes at "Command Training Centres" at various regional commands. Anti-Guerrilla warfare tactics were taught at combat forces schools and other training establishments with special emphasis on ambush and counter-ambush, counterinsurgency weapons and tactics, individual battle initiative for tactical independence, commando tactics, and reconnaissance. Battalion size operations were also practised in the South West Regional Military Command area. The new military doctrine was formally endorsed and adopted at the first party congress of the BSPP in 1971. BSPP laid down directives for "complete annihilation of the insurgents as one of the tasks for national defence and state security" and called for "liquidation of insurgents through the strength of the working people as the immediate objective". This doctrine ensures the role of Tatmadaw at the heart of national policy making.

Throughout BSPP era, the total people's war doctrine was solely applied in counterinsurgency operations, since Burma did not face any direct foreign invasion throughout the period. In 1985, the then Lieutenant General Saw Maung, Vice-Chief of Staff of Tatmadaw reminded his commanders during his speech at the Command and General Staff College:

In Myanmar, out of nearly 35 million people, the combined armed forces (army, navy and air force) are about two hundred thousand. In terms of percentage, that is about 0.01%. It is simply impossible to defend a country the size of ours with only this handful of troops... therefore, what we have to do in the case of foreign invasion is to mobilise people in accordance with the "total people's war" doctrine. To defend our country from aggressors, the entire population must be involved in the war effort as the support of people dictate the outcome of the war.

The third phase of doctrinal development of Myanmar Armed Forces came after the military take over and formation of State Law and Order Restoration Council (SLORC) in September 1988 as part of armed forces modernisation programme. The development was the reflection of sensitivity towards direct foreign invasion or invasion by proxy state during the turbulent years of the late 1980s and early 1990s, for example: the unauthorised presence of a US aircraft carrier Battle Group in Myanmar's territorial waters during the 1988 political uprising as evidence of an infringement of Myanmar's sovereignty. Also, the "Tatmadaw" leadership was concerned that foreign powers might arm the insurgents on the border to exploit the political situation and tensions in the country. This new threat perception, previously insignificant under the nation's isolationist foreign policy, led "Tatmadaw" leaders to review the defence capability and doctrine of the "Tatmadaw".

The third phase was to face the lower level external threats with a strategy of strategic denial under total people's defence concept. Current military leadership has successfully dealt with 17 major insurgent groups, whose 'return to legal fold' in the past decade has remarkably decreased the internal threats to state security, at least for the short and medium terms, even though threat perception of the possibility of external linkage to internal problems, perceived as being motivated by the continuing human rights violations, religious suppression and ethnic cleansing, remains high.

Within the policy, the role of the Tatmadaw was defined as a `modern, strong and highly capable fighting force'. Since the day of independence, the Tatmadaw has been involved in restoring and maintaining internal security and suppressing insurgency. It was with this background that Tatmadaw's "multifaceted" defence policy was formulated and its military doctrine and strategy could be interpreted as defence-in-depth. It was influenced by a number of factors such as history, geography, culture, economy and sense of threats.

The Tatmadaw has developed an 'active defence' strategy based on guerrilla warfare with limited conventional military capabilities, designed to cope with low intensity conflicts from external and internal foes, which threatens the security of the state. This strategy, revealed in joint services exercises, is built on a system of total people's defence, where the armed forces provide the first line of defence and the training and leadership of the nation in the matter of national defence.

It is designed to deter potential aggressors by the knowledge that defeat of the Tatmadaw's regular forces in conventional warfare would be followed by persistent guerrilla warfare in the occupied areas by people militias and dispersed regular troops which would eventually wear down the invading forces, both physically and psychologically, and leave it vulnerable to a counter-offensive. If the conventional strategy of strategic denial fails, then the Tatmadaw and its auxiliary forces will follow Mao's strategic concepts of 'strategic defensive', 'strategic stalemate' and 'strategic offensive'.

Over the past decade, through a series of modernisation programs, the Tatmadaw has developed and invested in better Command, Control, Communication and Intelligence system; real-time intelligence; formidable air defence system; and early warning systems for its 'strategic denial' and 'total people's defence' doctrine.

Overall command of Tatmadaw (armed forces) rested with the country's highest-ranking military officer, a general, who acted concurrently as Defence Minister and Chief of Staff of Defence Services. He thus exercised supreme operational control over all three services, under the direction of the President, State Council and Council of Ministers. There was also a National Security Council which acted in advisory capacity. The Defence Minister cum Chief-of-Staff of Defence Services exercised day-to-day control of the armed forces and assisted by three Vice-Chiefs of Staff, one each for the army, navy and air force. These officers also acted as Deputy Ministers of Defence and commanders of their respective Services. They were all based at Ministry of Defence ("Kakweyay Wungyi Htana") in Rangoon/Yangon. It served as a government ministry as well as joint military operations headquarters.

The Joint Staff within the Ministry of Defence consisted of three major branches, one each for Army, Navy and Air Force, along with a number of independent departments. The Army Office had three major departments; the General (G) Staff to oversee operations, the Adjutant General's (A) Staff administration and the Quartermaster General's (Q) Staff to handle logistics. The General Staff consisted two Bureaus of Special Operations (BSO), which were created in April 1978 and June 1979 respectively.

These BSO are similar to "Army Groups" in Western armies, high level staff units formed to manage different theatres of military operations. They were responsible for the overall direction and co-ordination of the Regional Military Commands (RMC) with BSO-1 covering Northern Command (NC), North Eastern Command (NEC), North Western Command (NWC), Western Command (WC) and Eastern Command (EC). BSO-2 responsible for South Eastern Command (SEC), South Western Command (SWC), Western Command (WC) and Central Command (CC).

The Army's elite mobile Light Infantry Divisions (LID) were managed separately under a staff colonel. Under G Staff, there were also a number of directorates which corresponded to the Army's functional corps, such as Intelligence, Signals, Training, Armour and Artillery. The A Staff was responsible for the Adjutant General, Directorate of Medical Services and the Provost Marshal's Office. The Q Staff included the Directorates of Supply and Transport, Ordnance Services, Electrical and Mechanical Engineering, and Military Engineers.

The Navy and Air Force Offices within the Ministry were headed by the Vice Chiefs of Staff for those Services. Each was supported by a staff officer at full colonel level. All these officers were responsible for the overall management of the various naval and air bases around the country, and the broader administrative functions such as recruitment and training.

Operational Command in the field was exercised through a framework of Regional Military Commands (RMC), the boundaries of which corresponded with the country's Seven States and Seven Divisions. The Regional Military Commanders, all senior army officers, usually of Brigadier General rank, were responsible for the conduct of military operations in their respective RMC areas. Depending on the size of RMC and its operational requirements, Regional Military Commanders have at their disposal 10 or more infantry battalions ("Kha La Ya").

The Tatmadaw's organisational and command structure dramatically changed after the military coup in 1988. In 1990, the country's most senior army officer become a senior general (equivalent to field marshal rank in Western armies) and held the positions of chairman of State Law and Order Restoration Council (SLORC), Prime Minister and Defence Minister, as well as being appointed Commander in Chief of the Defence Services. He thus exercised both political and operational control over the entire country and armed forces.

From 1989, each service has had its own Commander in Chief and Chief of Staff. The Army Commander in Chief is now elevated to full general ("Bo gyoke Kyii") rank and also acted as deputy commander in Chief of the Defence Services. The C-in-C of the Air Force and Navy hold the equivalent of lieutenant general rank, while all three Service Chiefs of Staff were raised to major general level. Chiefs of Bureau of Special Operations (BSO), the heads of Q and A Staffs and the Director of Defence Services Intelligence (DDSI) were also elevated to lieutenant general rank. The reorganisation of the armed forces after 1988 resulted in the upgrading by two ranks of most of the senior positions.

A new command structure was introduced at the Ministry of Defence level in 2002. The most important position created is the Joint Chief of Staff (Army, Navy, Air Force) that commands commanders-in-chief of the Navy and the Air Force.

The Office of Strategic Studies (OSS, or "Sit Maha Byuha Leilaryay Htana") was formed around 1994 and charged with formulating defence policies, and planning and doctrine of the Tatmadaw. The OSS was commanded by Lieutenant General Khin Nyunt, who is also the Director of Defence Service Intelligence (DDSI). Regional Military Commands (RMC) and Light Infantry Divisions (LID) were also reorganised, and LIDs are now directly answerable to Commander in Chief of the Army.

A number of new subordinate command headquarters were formed in response to the growth and reorganisation of the Army. These include Regional Operation Commands (ROC, or Da Ka Sa), which are subordinate to RMCs, and Military Operations Commands (MOC, or Sa Ka Kha), which are equivalent to Western infantry divisions.

The Chief of Staff (Army) retained control of the Directorates of Signals, Directorate of Armour Corps, Directorate of Artillery Corps, Defence Industries, Security Printing, Public Relations and Psychological Warfare, and Military Engineering (field section), People's Militias and Border Troops, Directorate of Defence Services Computers (DDSC), the Defence Services Museum and Historical Research Institute.

Under the Adjutant General Office, there are three directorates: Medical Services, Resettlement, and Provost Martial. Under the Quartermaster General Office are the directorates of Military Engineering (garrison section), Supply and Transport, Ordnance Services, and Electricaland Mechanical Engineering.

Other independent department within the Ministry of Defence are Judge Advocate General, Inspector General, Military Appointment General, Directorate of Procurement, Record Office, Central Military Accounting, and Camp Commandant.

All RMC Commander positions were raised to the level of major general and also serve as appointed chairmen of the state- and division-level Law and Order Restoration Committees. They were formally responsible for both military and civil administrative functions for their command areas. Also, three additional regional military commands were created. In early 1990, a new RMC was formed in Burma's north west, facing India. In 1996, the Eastern Command in Shan State was split into two RMCs, and South Eastern Command was divided to create a new RMC in country's far south coastal regions.

In 1997, the SLORC was abolished and the military government created the State Peace and Development Council (SPDC). The council includes all senior military officers and commanders of the RMCs. A new Ministry of Military Affairs was established and headed by a lieutenant general. This new ministry was abolished after its minister Lt. Gen. Tin Hla was sacked in 2001.

On 18 October 2004, the OSS and DDSI were abolished during the purge of General Khin Nyunt and military intelligence units. OSS ordered 4 regiment to raid in DDSI Headquarter in Yangon. At the same time, all of the MIU in the whole country were raided and arrested by OSS corps. Nearly two thirds of MIU officers were detained for years. A new military intelligence unit called Military Affairs Security (MAS) was formed to take over the functions of the DDSI, but MAS units were much fewer than DDSI's and MAS was under control by local Division commander.

In early 2006, a new Regional Military Command (RMC) was created at the newly formed administrative capital, Naypyidaw.

The Myanmar Army has always been by far the largest service and has always received the lion's share of Burma's defence budget. It has played the most prominent part in Burma's struggle against the 40 or more insurgent groups since 1948 and acquired a reputation as a tough and resourceful military force. In 1981, it was described as "probably the best [army] in Southeast Asia, apart from Vietnam's".

This judgment was echoed in 1983, when another observer noted that "Myanmar's infantry is generally rated as one of the toughest, most combat seasoned in Southeast Asia".

Personnel: 23,000 

The Myanmar Air Force was formed on 16 January 1947, while Myanmar (also known as Burma) was still under British rule. By 1948, the new air force fleet included 40 Airspeed Oxfords, 16 de Havilland Tiger Moths, 4 Austers and 3 Supermarine Spitfires transferred from Royal Air Force with a few hundred personnel. The primary mission of Myanmar Air Force since its inception has been to provide transport, logistical, and close air support to Myanmar Army in counter-insurgency operations.

The Myanmar Navy is the naval branch of the armed forces of Burma with estimated 19,000 men and women. The Myanmar Navy was formed in 1940 and, although very small, played an active part in Allied operations against the Japanese during the Second World War. The Myanmar Navy currently operates more than 122 vessels. Before 1988, the Myanmar Navy was small and its role in the many counterinsurgency operations was much less conspicuous than those of the army and air force. Yet the navy has always been, and remains, an important factor in Burma's security and it was dramatically expanded in recent years to a provide blue water capability and external threat defence role in Burma's territorial waters. Its personnel number 19,000 (including two naval infantry battalions).

The Myanmar Police Force, formally known as The People's Police Force (), was established in 1964 as independent department under the Ministry of Home Affairs. It was reorganised on 1 October 1995 and informally become part of Tatmadaw. Current Director General of Myanmar Police Force is Brigadier General Kyaw Kyaw Tun with its headquarters at Naypyidaw. Its command structure is based on established civil jurisdictions. Each of Burma's seven states and seven divisions has their own Police Forces with headquarters in the respective capital cities. Israel and Australia often provide specialists to enhance the training of Burma's police.
Personnel: 72,000 (including 4,500 Combat/SWAT Police)

Office of Chief of Military Security Affairs commonly referred to by its Burmese acronym Sa Ya Pa, is a branch of the Myanmar armed forces tasked with intelligence gathering. It was created to replace the Directorate of Defence Services Intelligence (DDSI), which was disbanded in 2004.

The Myanmar Defence Industries (DI) consists of 13 major factories throughout the country that produce approximately 70 major products for Army, Navy and Air Force. The main products include automatic rifles, machine guns, sub-machine guns, anti-aircraft guns, complete range of mortar and artillery ammunition, aircraft and anti aircraft ammunition, tank and anti-tank ammunition, bombs, grenades, anti-tank mines, anti-personnel mines such as the M14 pyrotechnics, commercial explosives and commercial products, and rockets and so forth. DI have produced new assault rifles and light machine-guns for the infantry. The MA series of weapons were designed to replace the old German-designed but locally manufactured Heckler & Koch G3s and G4s that equipped Burma's army since the 1960s.

The major factories of the DI are the following:

Heavy Industries were established with Ukrainian assistance mainly to assemble the BTR-3U fleet of the Myanmar Army. Total of 1,000 BTR-3U wheeled APCs are to be assembled in Burma over the next 10 years from parts sent by Ukraine. The BTR-3U is fitted with a number of modern weapon systems including 30 mm gun, 7.62 mm coaxial machine gun, 30 mm automatic grenade launcher and anti-tank guided weapons.

HI has also built APC/IFV such as MAV 1, MAV 2 and BAAC APCs. Little is known about MAV infantry fighting vehicles but it appeared that only 60% of the components are produced locally and some vital components such as fire control systems, turrets, engines and transmissions are imported from China NORINCO industries. Apart from BTR 3Us, MAVs and BAACs, HI is also producing a number of military trucks and jeeps for the Army, Navy and Air Force.

Products of DI are as follow:


It has been claimed by "The Daily Telegraph" that the Tatmadaw conscripts adults and children. According to the International Confederation of Free Trade Unions several hundred thousand men, women, children and elderly people are forced to work against their will by the Burmese army. Individuals refusing to work may be victims of torture, rape or murder.
The International Labour Organization has continuously called on Burma to end the practice of forced labour since the 1960s. In June 2000, the ILO Conference adopted a resolution calling on governments to cease any relations with the country that might aid the junta to continue the use of forced labour.

In a 2003 report, "No Safe Place: Burma's Army and the Rape of Ethnic Women", Refugees International document the widespread use of rape by Burma’s soldiers to brutalise women from five different ethnic nationalities.

According to 2002 Human Rights Watch Report, the forceful recruitment and kidnapping of children into the military is commonplace. In 2002, an estimated 70,000 of the country’s 350,000-400,000 soldiers are children. There are also multiple reports of widespread child labour.

In December 2017, the US imposed sanctions on General Maung Maung Soe, a general of Western Myanmar Command who oversaw the military’s crackdown in Rakhine State. The Tatmadaw had sentenced seven soldiers to 10-year prison terms for killing 10 Rohingya men in Rakhine in September 2017.





</doc>
<doc id="20395" url="https://en.wikipedia.org/wiki?curid=20395" title="Foreign relations of Myanmar">
Foreign relations of Myanmar

Historically strained, Myanmar's foreign relations, particularly with Western nations, have improved since 2012. Myanmar (also known as Burma) has generally maintained warmer relations with neighbouring states and is a member of the Association of Southeast Asian Nations.

The United States has placed broad sanctions on Burma because of the military crackdown in 1988 and the military regime's refusal to honour the election results of the 1990 People's Assembly election. Similarly, the European Union has placed embargoes on Burma, including an arms embargo, cessation of trade preferences, and suspension of all aid with the exception of humanitarian aid.

US and European government sanctions against the military government, alongside boycotts and other types of direct pressure on corporations by western supporters of the Burmese democracy movement, have resulted in the withdrawal from Burma of most US and many European companies. However, several Western companies remain due to loopholes in the sanctions. Asian corporations have generally remained willing to continue investing in Burma and to initiate new investments, particularly in natural resource extraction.

The French oil company Total S.A. is able to operate the Yadana natural gas pipeline from Burma to Thailand despite the European Union's sanctions on Burma. Total is currently the subject of a lawsuit in French and Belgian courts for the condoning and use of Burman civilian slavery to construct the named pipeline. Experts say that the human rights abuses along the gas pipeline are the direct responsibility of Total S.A. and its American partner Chevron Corporation with aid and implementation by the Tatmadaw. Prior to its acquisition by Chevron, Unocal settled a similar human rights lawsuit for a reported multimillion-dollar amount. There remains active debate as to the extent to which the American-led sanctions have had adverse effects on the civilian population or on the military rulers.

Both countries established diplomatic relations on 31 January 2013.

Burma is represented in Denmark through its embassy in the United Kingdom, and Denmark is represented in Burma through its embassy in Thailand. Diplomatic relations were established in 1955. Relations between the two countries are friendly, but economically, Denmark has the "worst" trade with Burma in the European Union. Denmark also supports the Norwegian based radio station, Democratic Voice of Burma.

Development assistance to Burma is a top priority of the Danish International Development Agency's engagement in Southeast Asia. 93 million DKK was given to education and healthcare projects.

Danish development assistance has focused on promoting democracy and human rights. Denmark was one of the first countries to respond to cyclone Nargis by providing humanitarian assistance to Burma. Three Diseases Fund was founded in 2006, and Denmark joined in 2009. Three Diseases Fund helps Burma fight HIV and AIDS, and has assisted with 73 million dollars.

In 1996, the consul in Burma for Denmark, James Leander Nichols, was sentenced to three years in jail. The sentence was for illegal possession of two facsimile machines and a telephone switchboard. Two months later, he died in prison. Despite Danish insistence, Burmese authorities refused to allow an independent autopsy. Soon after, the European Union, with Canada, called for a United Nations gathering on the democratisation process.

On 3 November 2010, students from 140 different gymnasiums in Denmark and DanChurchAid, participated in the annual Day's Work Day. The money earned by the students goes to improve education for young people in Burma.

The Government of Ireland established diplomatic relations with Burma on a non-resident basis on 10 February 2004. The Irish Government was still concerned about the arbitrary detention of the opposition leader Aung San Suu Kyi. Burma Action Ireland is a pro-democracy group that freely operates in the Republic of Ireland.

Ireland supported a UN commission of inquiry and international level monitoring of Burma after 2008, as part of their efforts to support democracy and human rights movements in Burma. This became public knowledge after official papers were leaked in September 2010.

Franco-Burmese relations go back to the early 18th century, as the French East India Company attempted to extend its influence into Southeast Asia. French involvement started in 1729 when it built a shipyard in the city of Syriam. The 1740 revolt of the Mon against Burmese rule, however, forced the French to depart in 1742. They were able to return to Siam in 1751 when the Mon requested French assistance against the Burmese. A French envoy, Sieur de Bruno was sent to evaluate the situation and help in the defence against the Burmese. French warships were sent to support the Mon rebellion, but in vain. In 1756, the Burmese under Alaungpaya vanquished the Mon. Many French were captured and incorporated into the Burmese Army as an elite gunner corps, under Chevalier Milard. In 1769, official contacts resumed when a trade treaty was signed between King Hsinbyushin and the French East India Company.

Soon after, however, France was convulsed by the French Revolution and Napoleonic Wars, thus allowing overwhelming British influence in Burma. French contacts with Burma, effectively a British colony, became almost non-existent. Instead, from the second half of the 19th century, France concentrated on the establishment of French Indochina and the conflicts with China leading to the Sino-French War. Following the end of World War II, ambassador-level diplomatic relationships between France and Burma were established in 1948, soon after the Burmese nation became an independent republic on 4 January 1948, as "Union of Burma", with Sao Shwe Thaik as its first President and U Nu as its first Prime Minister.


The political relations between the United States of America and Burma began to face major problems following the 1988 military coup and the junta's outbursts of repression against pro-democracy activists. Subsequent repression, including that of protestors in September 2007, further strained the relationship. However, following signs of democratisation and economic liberalisation, Hillary Clinton, as Secretary of State, and others called for the mending of America's relations with Burma in 2011. As a result of the refurbishment of ties, the American authorities in 2012 planned for the re-establishment of ambassador-level relations with Burma for the first time since 1990.

Massachusetts, as a US state, attempted to place sanctions against Burma on its own in 1996 but the concept proved to be contradictory to the US Constitution. Later, the United States federal government imposed broad sanctions against Burma under several different legislative and policy vehicles. The Burma Freedom and Democracy Act (BFDA), passed by both the US Senate and their House of Representatives and signed by then President George W. Bush in 2003, imposed a ban on all imports from Burma, a ban on the export of financial services to Burma, a freeze on the assets of certain Burmese financial institutions, alongside further visa restrictions against Burmese officials. American legislators then renewed the BFDA on an almost annual basis, most recently in July 2010.
Since 27 September 2007, the US Department of Treasury froze assets of 25 high-ranking officials Burmese government officials as it was authorised to do so by Executive Order 13310. On 19 October 2007, President George W. Bush imposed a new Executive Order (E.O. 13448) authorising the freezing of assets against individuals who stand accused by the Government of the United States of being party to human rights violations and acts of public corruption, as well as against those who provide material and financial support to the military junta.

In addition, since May 1997, the US Government prohibited new investment by American people and other entities. A number of American companies exited the Burma market prior to the imposition of sanctions due to a worsening business climate and mounting criticism from human rights groups, consumers, and shareholders. The United States has also imposed countermeasures on Burma due to its inadequate measures to eliminate money laundering.

Due to its particularly severe violations of religious freedom, the United States has designated Burma a Country of Particular Concern (CPC) under the International Religious Freedom Act. Burma is also designated a Tier 3 Country in the Trafficking in Persons Report for utilising forced labour, and is subject to additional sanctions as a result. The political relationship between the United States and Burma worsened after the 1988 military coup and violent suppression of pro-democracy demonstrations. Subsequent repression, including the brutal crackdown on peaceful protestors in September 2007, further strained relations.

The United States lowered its level of representation in Burma from Ambassador to Chargé d'Affaires after the government's major outbreaks against opposition groups and protesters in 1988 and its alleged failure to honour the results of the 1990 parliamentary election, although it upgraded back on 13 January 2012, appointing Derek Mitchel as Ambassador and head of mission

US Secretary of State, Hillary Clinton, visited Burma in November–December 2011. In this visit, the first by a Secretary of State since 1955, Hillary met with the President of Burma, Thein Sein, in the official capital Naypyidaw, and later met with democracy activist Aung San Suu Kyi in Yangon. The US announced a reduction of laws against providing aid to Burma and raised the possibility of an exchange of ambassadors.

On 13 January 2012, the US Secretary of State Hillary Clinton announced the US will exchange ambassadors with Burma, after a landmark Burmese political prisoner amnesty.

On Thursday, 17 May 2012, the White House Press Office announced that President Barack Obama of the US Democratic Party had nominated Derek Mitchell to the US Senate for confirmation to serve as US Ambassador to Burma. After being approved by the US Senate in late June, Derek Mitchell, the first U.S ambassador to Myanmar in 22 years formally assumed his job on 11 July 2012 by presenting his credentials to President Thein Sein at the presidential mansion in the capital Naypyitaw.

In July 2012 the United States formally reduced sanctions against Burma. Secretary of State Hillary Rodham Clinton announced plans in the spring of 2012 for a “targeted easing” of sanctions to allow minor US investment in the country, but companies could not move ahead until the sanctions were formally suspended. In July 2012, President Obama ordered the US State Department to issue two special licences, one providing special authorisation to invest in Burma and the other authorising to provide financial services in Burma. Although plans to lift investment restrictions were announced in May 2012, the change awaited what administration officials labelled 'detailed reporting requirements' on US companies doing business in Burma, alongside the creation of mechanisms to prevent US economic ties to the powerful Burmese military and individuals and companies involved in human rights abuses. President Obama also issued an executive order expanding existing sanctions against individuals who violate human rights to include those who threaten Burma’s political restructuring process.

President Obama created a new power for the US government to impose “blocking sanctions” on any individual threatening peace in Myanmar. Also, businesses with more than US$500,000 worth of investment in the country will need to file an annual report with the State Department, in which they will be required to provide details on workers’ rights, land acquisitions and any payments of more than US$10,000 to government entities, including Myanmar’s state-owned enterprises. Although the policy was criticised by human rights groups, American companies and people will be allowed to invest in the state-owned Myanmar Oil and Gas Enterprise—all investors need to notify the State Department within a 60-day period. Human Rights Watch (HRW) expressed its objection in an official statement: “The new United States government policy allowing business activity in Burma’s controversial oil sector with reporting requirements will not adequately prevent new investments from fueling abuses and undermining reform”. HRW’s Business and Human Rights Director Arvind Ganesan stated: “By allowing deals with Burma’s state-owned oil company, the U.S. looks like it caved to industry pressure and undercut Aung San Suu Kyi and others in Burma who are promoting government accountability”.

In May 2013, Sein became the first Myanmar president to visit the US White House in 47 years and President Barack Obama praised the former general for political and economic reforms, and the cessation of tensions between Myanmar and the US Political activists objected to the visit due to concerns over human rights abuses in Myanmar but Obama assured Sein that Myanmar will receive the support from the US. Prior to President Sein, the last Myanmar leader to visit the White House was Ne Win in September 1966. The two leaders discussed Sein's intention to release more political prisoners, the institutionalisation of political reform and rule of law, and ending ethnic conflict in Myanmar—the two governments agreed to sign a bilateral trade and investment framework agreement on 21 May 2013.

On 10 September 2007, the Burmese Government accused the CIA of assassinating a rebel Karen commander from the Karen National Union who wanted to negotiate with the military government. For background on the conflict, see

It is more fully explored on: Namebase (cross-references books on CIA activities in Burma).
In 2011 the "Guardian" newspaper published WikiLeaks cable information regarding Burma. The cables revealed that the US funded some of the civil society groups in Burma that forced the government to suspend the controversial Chinese Myitsone Dam on the Irrawaddy river.

According to media reports citing documents published by Germany's "Der Spiegel" in 2010, the Embassy of the United States in Yangon is the site of an electronic surveillance facility used to monitor telephones and communications networks. The facility is run jointly by the Central Intelligence Agency (CIA) and the National Security Agency (NSA) through a group known as Special Collection Service.

The US Embassy in Burma is located in Rangoon, whilst the Burmese diplomatic representation to America is based in Washington, D.C.


Bilateral relations with the Russian Federation are among the strongest enjoyed by largely isolated Burma. Russia had established diplomatic relations with Burma at independence and these continued after the fall of the Soviet Union. China and Russia once vetoed a U.N. Security Council resolution designed to punish Burma. Today Russia, along with China, still opposes the imposition of sanctions on Burma and supports a policy of dialogue. Russia, along with China, remains part of the UN Security Council which occasionally shields or weakens Burma from global pressure and criticism.

Russia maintains an embassy in Rangoon whilst Burma maintains one in Moscow.

In 2007 Russia and Burma engaged in a deal regarding Burma's nuclear programme. According to the press release, Russia and Burma shall construct a nuclear research centre that 'will comprise a 10MW light-water reactor working on 20%-enriched uranium-235, an activation analysis laboratory, a medical isotope production laboratory, silicon doping system, nuclear waste treatment and burial facilities'.


Burma is a member of the Association of Southeast Asian Nations (ASEAN) and part of ASEAN+3 and the East Asia Summit. Burma agreed to relinquish its turn to hold the rotating ASEAN presidency in 2006 due to others member states' concern of its previous democratic situation.

Asean has announced that it shall not provide defence for Burma at any international forum regarding the authoritarian junta's refusal to restore democracy. In April 2007, the Malaysian Foreign Ministry parliamentary secretary Ahmad Shabery Cheek said that Malaysia and other Asean members had decided not to defend Burma if the country was raised for discussion at any international conference. "Now Burma has to defend itself if it was bombarded at any international forum," he said when winding up a debate at committee stage for the Foreign Ministry. He was replying to queries from Opposition Leader Lim Kit Siang on the next course of action to be taken by Malaysia and Asean with the Burmese military junta. Lim had said Malaysia must play a proactive role in pursuing regional initiatives to bring about a change in Burma and support efforts to bring the situation in Burma to the UN Security Council's attention. Recently, ASEAN did take a stronger tone with Burma, particularly regards to the detention of now-released Aung San Suu Kyi.

Brunei has an embassy in Yangon, and Burma has an embassy in Gadong. The relations have been established since 21 September 1993.

The relations between the two countries were established on 1 March 1957 and the first Burma mission at the legation level was set up in Kuala Lumpur in June 1959 and later raised to the embassy level.

Relations between Burma and Thailand focus mainly on economic issues and trade. There is sporadic conflict with Thailand over the alignment of the border. Recently, Prime Minister Abhisit Vejjajiva made it clear that dialogue encouraging political change is a priority for Thailand, but not through economic sanctions. He also publicised intentions to help reconstruct temples damaged in the aftermath of Cyclone Nargis. However, there were tensions over detained opposition leader Aung San Suu Kyi, with Thailand calling for her release. She was released in 2010. In the Thaksin Shinawatra administration, relations have been characterised by conflicts and confrontations. Border disputes are now coming more prominent and Thailand as disturbed by the imprisonment of Burma’s dissident Aung San Suu Kyi.

Burma has diplomatic offices in Bangkok whilst Thailand maintains an embassy in Rangoon.

Philippines established relations with Burma in 1956 and recognised its political name Myanmar. In 2012, Myanmar ranked 3rd to the lowest among the Philippines' trading partners in ASEAN. It only fared better than Cambodia and Laos. The Philippines and Myanmar traded only $47.07 million in 2012. The Philippines grant Burmese citizens visa-free access for 30 days. Myanmar on the other hand signed the visa exemption for Filipinos on 5 December 2013 effective 4 January 2014. The agreement allows Filipinos to stay in Myanmar up to 14 days visa-free.

The People's Republic of China had poor relations with Burma until the late 1980s. Between 1967 and 1970, Burma broke relations with Beijing because of the latter's support for the Communist Party of Burma (CPB). Deng Xiaoping visited Yangon in 1978 and withdrew support for the long running insurgency of the Communist Party of Burma. However, in the early 1950s Burma enjoyed a hot-and-cold relationship with China. Burma's U သန္႕ and U Nu lobbied for China's entry as a permanent member into the UN Security Council, but denounced the invasion of Tibet.
China and Burma have had many border disputes, dating long before the British annexation of Burma. The last border dispute culminated in 1956, when the People's Liberation Army invaded northern Burma, but were repulsed. A border agreement was reached in 1960.
In the late 1960s, due to Ne Win's propaganda that the PRC was to blame for crop failures, and the increasing number of ethnic Chinese students supporting Chairman Mao Zedong, by carrying the Quotatians from his books, anti-Chinese riots broke out in June 1967. At the same time, many Sino-Burmese were influenced by the Cultural Revolution in China and began to wear Mao badges. Shops and homes were ransacked and burned. The Chinese government heavily berated the Burmese government and started a war of words, but no other actions were taken. The anti-Chinese riots continued till the early 1970s.
However, after 1986, China withdrew support for the CPB and began supplying the military junta with the majority of its arms in exchange for increased access to Burmese markets and a rumoured naval base on Coco Islands in the Andaman Sea. China is supposed to have an intelligence gathering station on the Great Coco Island to monitor Indian naval activity and ISRO & DRDO missile and space launch activities. The influx of Chinese arms turned the tide in Burma against the ethnic insurgencies, many of which had relied indirectly on Chinese complicity. As a result, the military junta of Burma is highly reliant on the Chinese for their currently high level of power.
Burma maintains diplomatic offices in Beijing and consular offices in Kunming and Hong Kong, whilst the PRC has a diplomatic mission in Rangoon and a consulate in Mandalay.

Bilateral relations between Burma and the Republic of India have improved considerably since 1993, overcoming disagreements related to drug trafficking, the suppression of democracy and the rule of the military junta in Burma. Burma is situated to the south of the states of Mizoram, Manipur, Nagaland and Arunachal Pradesh in Northeast India. The proximity of the People's Republic of China give strategic importance to Indo-Burmese relations. The Indo-Burmese border stretches over 1,600 kilometers. India is generally friendly with Burma, but is concerned by the flow of tribal refugees and the arrest of Aung San Suu Kyi.

As a result of increased Chinese influence in Burma as well as the safe haven and arms trafficking occurring along the Indo-Burmese border, India has sought in recent years to refurbish ties with the Union of Burma. Numerous economic arrangements have been established including a roadway connecting the isolated provinces of Northeastern India with Mandalay which opens up trade with China, Burma, and gives access to the Burmese ports. Relations between India and Burma have been strained in the past however due to India's continuing support for the pro-democracy movement in Burma.

In an interview on the BBC, George Fernandes, former Indian Defence Minister and prominent Burma critic, said that Coco Island was part of India until it was donated to Burma by former Prime Minister of India Jawaharlal Nehru. Coco Island is located at 18 km from the Indian Nicobar Islands.

Burma has a fully operating embassy based in New Delhi and India has one in Rangoon, the former capital of Burma. Like the PRC, the Republic of India maintains a Consulate-General in Mandalay.

India is the largest market for Burmese exports, buying about US$220 million worth of goods in 2000; India's exports to Burma stood at US$75.36 million. India is Burma’s 4th largest trading partner after Thailand, the PRC and Singapore, and second largest export market after Thailand, absorbing 25 percent of its total exports. India is also the seventh most important source of Burma’s imports. The governments of India and Burma had set a target of achieving $1 billion and bilateral trade reached US$650 million by 2006. The Indian government has worked to extend air, land and sea routes to strengthen trade links with Myanmar and establish a gas pipeline. While the involvement of India's private sector has been low and growing at a slow pace, both governments are proceeding to enhance co-operation in agriculture, telecommunications, information technology, steel, oil, natural gas, hydrocarbons and food processing. The bilateral border trade agreement of 1994 provides for border trade to be carried out from three designated border points, one each in Manipur, Mizoram and Nagaland.

On 13 February 2001 India and Burma inaugurated a major 160 kilometre highway, called the Indo-Myanmar Friendship Road, built mainly by the Indian Army's Border Roads Organisation and aimed to provide a major strategic and commercial transport route connecting North-East India, and South Asia as a whole, to Southeast Asia.

India and Myanmar have agreed to a four-lane, 3200 km triangular highway connecting India, Myanmar and Thailand. The route, which is expected to be completed by sometime during 2018, will run from India's northeastern states into Myanmar, where over 1,600 km of roads will be built or improved. The first phase connecting Guwahati to Mandalay is set to complete by 2016. This will eventually be extended to Cambodia and Vietnam. This is aimed at creating a new economic zone ranging from Kolkata on the Bay of Bengal to Ho Chi Minh City on the South China Sea.

Operation "Leech" is the name given to an armed operation on the Indo-Burmese border in 1998. As the major player in South Asia, India always sought to promote democracy and install friendly governments in the region. To these ends, India's external intelligence agency, R&AW, cultivated Burmese rebel groups and pro-democracy coalitions, especially the Kachin Independence Army (KIA). India allowed the KIA to carry a limited trade in jade and precious stones using Indian territory and even supplied them with weapons.

However, with increasing bonhomie between the Indian government and the Burmese junta, KIA becomes the main source of training and weapons for all northeastern rebel groups in India. R&AW initiated Operation "Leech", with the help of Indian Army and paramilitary forces, to assassinate the leaders of the Burmese rebels as an example to other groups.

Historical relations between Burma and Bangladesh include centuries of trade, cultural interactions and migration between the kingdoms and empires of Bengal and the kingdoms of Burma, particularly Arakan. Most prominently this is visible in the Indic Buddhist culture of Burma that was transmitted often through Bengal resulting in the imprint of Indian (inclusive of Bengali) culture and civilization currently found in Burma. The two nations also share a heritage of colonial commerce during the British Empire. The Bengali community in Burma is present in Rangoon and the Rakhine. In Bangladesh, a large population of Burmese ancestry resides in Chittagong and southeastern hill districts, including Rakhines and Bohmong, as well as Burmese-Bengalis. After the Bangladesh Liberation War in 1971, Burma became one of the first countries to recognise the independence of Bangladesh.

The presence of 270,000 Burmese Muslim refugees (Rohingya people) in southern Bangladesh have often caused irritants in bilateral relations, which are generally cordial. A 40-year maritime boundary dispute in the Bay of Bengal was resolved by the two countries at a UN tribunal in March 2012.

Bangladesh has sought transit rights through Burma, to establish connectivity with China and ASEAN through projects such as the proposed Chittagong-Mandalay-Kunming highway. The governments of both countries are also in discussions on the possible export of Burmese gas to Bangladesh, as well as setting up a joint hydroelectric power plant in Rakhine State.

The political class and civil society of Bangladesh often voiced support for Burma's pro-democracy struggle. In 2006 a petition by 500 Bangladeshi politicians and intellectuals, including Sheikh Hasina and Kamal Hossain, expressed support for Aung San Suu Kyi and called for the release of all political prisoners in Burma. After winning elections in 2008, Sheikh Hasina reiterated her position on Burma's pro-democracy struggle, calling for an end to the detention of Suu Kyi and Burmese political prisoners. The Democratic Voice of Burma radio station operates bureaus in Dhaka and Chittagong.

Despite border (both territorial and nautical) tensions and the forced migration of 270,000 Rohingya Muslims from Buddhist Burma in 1978, relations with Bangladesh have generally been cordial, albeit somewhat tense at times.

Many Rohingya refugees, not recognised as a sanctioned ethnic group and allegedly suffering abuse from the Burmese authorities, remain in Bangladesh, and have been threatened with forced repatriation to Burma. There are about 28,000 documented refugees remaining in camps in southern Bangladesh.

At the 2008 ASEAN Regional forum summit in Singapore, Bangladesh and Burma have pledged to solve their maritime boundary disputes as quickly as possible especially that a UN deadline in claiming maritime territories will expire in three years time. However, in late 2008, Burma sent in ships into disputed waters in the Bay of Bengal for the exploration of oil and natural gas. Bangladesh responded by sending in three warships to the area and diplomatically pursued efforts to pressure the Burmese junta to withdraw their own ships. During the crisis Burma deployed thousands of troops on its border with Bangladesh. However, following the Bangladeshi deployment, within a week the ships withdrew and the crisis ended.

Burma has an embassy in Dhaka, whilst Bangladesh has an embassy in Rangoon and a consular office in Sittwe. Bangladesh is also one of the first countries to begin constructing a diplomatic mission in Nay Pyi Taw.

Theravada Buddhism was the link between Sri Lanka and Burma from the earliest times. There were frequent exchanges of pilgrims and scriptural knowledge with Ramanna (ancient name of the Burmese Kingdom). The resuscitation of the Sinhalese Sangha after the destructive effects of the Chola conquest owned a great deal to Bhikkus from upper Burma sent over for this purpose by the Burmese King at the request of Vijayabahu I.

By the 11th century these early religious times matured into diplomatic ties. Vijayabahu I (1055–1110 A.D.) who was engaged in a grim struggle against the Cholas received economic aid from King Anawarta of Burma. The alliance with the Burmese appears according to the chronicles to have continued after the expulsion of the Cholas and it was to Burma that Vijayabahu I turned for assistance in re-organizing the Sangha in Sri Lanka, thus underlining the connection between political ties and a common commitment to Buddhism.

The influence of Burmes architecture on Sri Lanka's religious building in Polonnaruwa is also evident. The Satmahalprasada, a setup with an unusual pyramid like form in several levels or storeys in Polonnnaruwa is the best example.

In 1865 the establishment of the Ramanna Nikaya is another major link. The Ramanna Nikaya lays greater stress on poverty and humility. This Nikaya aimed at returning to a purer form of Buddhism.

•Official visit of Hon. Sirimavo Bandaranaike, Prime Minister in January (1976)
•Visit of Hon. A.C.S. Hameed, Foreign Minister (1987)
•Visit of Hon. Lakshman Kadirgamar, Foreign Minister (1999)
•Visit of Hon. W.J.M. Loku Bandara, Minister of Buddha Sasana (2003)
•Visit of Hon. Loku Bandara, Speaker of the Parliament (2005)
•Visit of Hon Mahinda Rajapakse, Prime Minister (2004)
•Visit of Hon. Loku Bandara, Speaker (2005)
•Visit of Hon. Prime Minister (2006)
•Visit of the Hon. Minister of Foreign Affairs for First Joint Commission (2007)

•State Visit of H.E. Gen U Ne Win, President of Myanmar (1966)
•Visit of H.E. U Win Aung, Foreign Minister of Myanmar in (1999)
•Visit of H.E. Professor Kyaw Myint, Minister of Health (2005)
•Visit of Acting Prime Minister, Lt. Gen. Thein Sein (2007)
•Visit of the Foreign Minister of Myanmar (to participate at ECOSOC) (2009)

Burma and North Korea generally enjoy good relations. Burma has an embassy in Pyongyang and North Korea has an embassy in Rangoon.

Since they both achieved independence in 1948, Burma and North Korea have enjoyed a chequered relationship. Burma supported the UN forces during the Korean War, but after the signing of the 1953 armistice it established good working relations with the two Koreas. Consular links with both states were established in 1961 and full diplomatic relations followed in 1975. During the 1960s and 1970s, General Ne Win’s government made efforts to balance the competing demands of North Korea and South Korea for recognition, diplomatic support and trade. However, during the late 1970s the relationship with Pyongyang became slightly stronger than that with Seoul, as Ne Win and the Burma Socialist Programme Party forged fraternal ties with Kim Il-sung and the Workers' Party of Korea.

The bilateral relationship with North Korea dramatically collapsed in 1983, after Pyongyang allegedly sent three agents to Rangoon to assassinate South Korean President Chun Doo Hwan, who was making a state visit to Burma. Due to a last minute, unannounced change to his schedule Chun survived the massive bomb attack at the Martyrs' Mausoleum, but 17 South Korean and four Burmese officials, including four Korean Cabinet ministers, were killed. Forty-six others were injured.

There was probably at least one bilateral agreement as early as 2000, but the relationship seemed to reach a major turning point around 2003. In July that year, it was reported that between 15 and 20 North Korean technicians were working at the Monkey Point naval base in Rangoon. A UN report released on February 1, 2018 cited North Korean ballistic missile transfers to the Myanmar army.

In September 2017, the Ministry of Foreign Affairs of the Maldives announced that it was ceasing all trade ties with Myanmar in response to the government's treatment of the Rohingya people in Rakhine State.

Although Burma officially recognises the PRC and not the Republic of China, there is much other interaction between the two countries. Many Taiwanese nationals own businesses in Burma. There are direct air flights to Taipei, as there are to some major cities in the People's Republic of China, including Kunming, Guangzhou and Hong Kong.

Pakistan and Burma have cordial relations with each other, with embassies in each other's capitals. Pakistan International Airlines has flown to Yangon in the past and still operates Hajj charter flights on behalf of the Burmese government.

Pakistan has a diplomatic mission in Rangoon, whilst Burma maintains a diplomatic office in Islamabad.

The Republic of Korea and Burma generally enjoy good relations. Burma has an embassy in Seoul and South Korea has an embassy in Rangoon.

Below are the years that countries have established ambassador-level diplomatic relationships with Burma.

In 1961, U Thant, then Burma's Permanent Representative to the United Nations and former Secretary to the Prime Minister, was elected Secretary-General of the United Nations; he was the first non-Westerner to head any international organisation and would serve as UN Secretary-General for ten years. Among the Burmese to work at the UN when he was Secretary-General was the young Aung San Suu Kyi.

Until 2005, the United Nations General Assembly annually adopted a detailed resolution about the situation in Burma by consensus. But in 2006 a divided United Nations General Assembly voted through a resolution that strongly called upon the government of Burma to end its systematic violations of human rights.

In January 2007, Russia and China vetoed a draft resolution before the United Nations Security Council calling on the government of Burma to respect human rights and begin a democratic transition. South Africa also voted against the resolution, arguing that since there were no peace and security concerns raised by its neighbours, the question did not belong in the Security Council when there were other more appropriate bodies to represent it, adding, "Ironically, should the Security Council adopt [this resolution] ... the Human Rights Council would not be able to address the situation in Myanmar while the Council remains seized with the matter." The issue had been forced onto the agenda against the votes of Russia and the China by the United States (veto power applies only to resolutions) claiming that the outflow from Burma of refugees, drugs, HIV-AIDS, and other diseases threatened international peace and security.

The following September after the uprisings began and the human rights situation deteriorated, the Secretary-General dispatched his special envoy for the region, Ibrahim Gambari, to meet with the government. After seeing most parties involved, he returned to New York and briefed the Security Council about his visit. During this meeting, the ambassador said that the country "indeed [has experienced] a daunting challenge. However, we have been able to restore stability. The situation has now returned to normalcy. Currently, people all over the country are holding peaceful rallies within the bounds of the law to welcome the successful conclusion of the national convention, which has laid down the fundamental principles for a new constitution, and to demonstrate their aversion to recent provocative demonstrations.

On 11 October the Security Council met and issued a statement and reaffirmed its "strong and unwavering support for the Secretary-General's good offices mission", especially the work by Ibrahim Gambari (During a briefing to the Security Council in November, Gambari admitted that no timeframe had been set by the Government for any of the moves that he had been negotiating for.)

Throughout this period the World Food Program has continued to organise shipments from the Mandalay Division to the famine-struck areas to the north.

In December 2008, the United Nations General Assembly voted for a resolution condemning Burma's human rights record; it was supported by 80 countries, with 25 voting against and 45 abstaining.




</doc>
<doc id="20396" url="https://en.wikipedia.org/wiki?curid=20396" title="Michael Schumacher">
Michael Schumacher

Michael Schumacher (; born 3 January 1969) is a retired German racing driver who raced in Formula One for Jordan Grand Prix, Benetton and Ferrari, where he spent the majority of his career, as well as for Mercedes upon his return to the sport. Widely regarded as one of the greatest Formula One drivers ever, and regarded by some as the greatest of all time, Schumacher is the only driver in history to win seven Formula One World Championships, five of which he won consecutively. The most successful driver in the history of the sport, Schumacher holds the records for the most World Championship titles (7), the most Grand Prix wins (91), the most fastest laps (77) and the most races won in a single season (13), and according to the official Formula One website (Formula1.com), Schumacher was "statistically the greatest driver the sport has ever seen" at the time of his retirement from the sport.

After success in karting as a child, Schumacher won titles in Formula König and Formula Three before joining Mercedes in the World Sportscar Championship. In 1991, his Mercedes-funded race debut for the Jordan Formula One team resulted in Schumacher being signed by Benetton for the rest of that season. He finished third in 1992 and fourth in 1993, before becoming the first German World Drivers' Champion in 1994 by one point over Damon Hill. In 1995 he repeated the success, this time with a greater margin. In 1996, Schumacher moved to Ferrari, who had last won the Drivers' Championship in 1979, and helped them transform into the most successful team in Formula One history, as he came close to winning the 1997 and 1998 titles, before breaking his leg at the 1999 British Grand Prix, ending another title run.

Schumacher won five consecutive drivers' titles from 2000 to 2004, including an unprecedented sixth and seventh title. In 2002, Schumacher won the title with a record six races remaining and finished on the podium in every race. In 2004, Schumacher won twelve out of the first thirteen races and went on to win a record 13 times as he won his final title. Schumacher retired from Formula One in 2006, after finishing runner-up to Renault's Fernando Alonso. Schumacher returned to Formula One in 2010 with Mercedes. He produced the fastest qualifying time at the 2012 Monaco Grand Prix, and achieved his only podium on his return at the 2012 European Grand Prix, where he finished third. In October 2012, Schumacher announced he would retire for a second time at the end of the season.

His career was frequently controversial, as he was twice involved in collisions in the final race of a season that determined the outcome of the World Championship, with Damon Hill in 1994 in Adelaide, and with Jacques Villeneuve in 1997 in Jerez. Schumacher is an ambassador for UNESCO and has been involved in numerous humanitarian efforts throughout his life, donating tens of millions of dollars to charity. Schumacher and his younger brother, Ralf, are the only siblings to win races in Formula One, and they were the first brothers to finish 1st and 2nd in the same race, a feat they repeated in four subsequent races.

On 29 December 2013, Schumacher suffered a traumatic brain injury in a skiing accident. He was placed in a medically induced coma for six months until 16 June 2014. He left the hospital in Grenoble for further rehabilitation at the University Hospital of Lausanne. On 9 September 2014, Schumacher was relocated to his home where he continues to receive medical treatment and rehabilitation privately.

Schumacher was born in Hürth, North Rhine-Westphalia, to Rolf Schumacher, a bricklayer, and his wife Elisabeth. When Schumacher was four, his father modified his pedal kart by adding a small motorcycle engine. When Schumacher crashed it into a lamp post in Kerpen, his parents took him to the karting track at Kerpen-Horrem, where he became the youngest member of the karting club. His father soon built him a kart from discarded parts and at the age of six Schumacher won his first club championship. To support his son's racing, Rolf Schumacher took on a second job renting and repairing karts, while his wife worked at the track's canteen. Nevertheless, when Michael needed a new engine costing 800 DM, his parents were unable to afford it; he was able to continue racing with support from local businessmen.

Regulations in Germany require a driver to be at least fourteen years old to obtain a kart license. To get around this, Schumacher obtained a license in Luxembourg at the age of 12.

In 1983, he obtained his German license, a year after he won the German Junior Kart Championship. From 1984 on, Schumacher won many German and European kart championships. He joined Eurokart dealer Adolf Neubert in 1985 and by 1987 he was the German and European kart champion, then he quit school and began working as a mechanic. In 1988 he made his first step into single-seat car racing by participating in the German Formula Ford and Formula König series, winning the latter.

In 1989, Schumacher signed with Willi Weber's WTS Formula Three team. Funded by Weber, he competed in the German Formula 3 series, winning the title in 1990. He also won the Macau Grand Prix. At the end of 1990, along with his Formula 3 rivals Heinz-Harald Frentzen and Karl Wendlinger, he joined the Mercedes junior racing programme in the World Sports-Prototype Championship. This was unusual for a young driver: most of Schumacher's contemporaries would compete in Formula 3000 on the way to Formula One. However, Weber advised Schumacher that being exposed to professional press conferences and driving powerful cars in long distance races would help his career. In the 1990 World Sportscar Championship season, Schumacher won the season finale at the Autódromo Hermanos Rodríguez in a Sauber–Mercedes C11, and finished fifth in the drivers' championship despite only driving in three of the nine races. He continued with the team in the 1991 World Sportscar Championship season, winning again at the final race of the season at Autopolis in Japan with a Sauber–Mercedes-Benz C291, leading to a ninth-place finish in the drivers' championship. He also competed at Le Mans during that season, finishing 5th in a car shared with Karl Wendlinger and Fritz Kreutzpointner. In 1991, he competed in one race in the Japanese Formula 3000 Championship, finishing second.

Schumacher was noted throughout his career for his ability to produce fast laps at crucial moments in a race and to push his car to the very limit for sustained periods. Motor sport author Christopher Hilton observed in 2003 that a "measure of a driver's capabilities is his performance in wet races, because the most delicate car control and sensitivity are needed", and noted that like other great drivers, Schumacher's record in wet conditions shows very few mistakes: up to the end of the 2003 season, Schumacher won 17 of the 30 races in wet conditions he contested. Some of Schumacher's best performances occurred in such conditions, earning him the nicknames ""Regenkönig"" (rain king) or ""Regenmeister"" (rain master), even in the non-German-language media. He is known as "the Red Baron", because of his red Ferrari and in reference to the German Manfred von Richthofen, the famous flying ace of World War I. Schumacher's nicknames include "Schumi",
"Schuey"
and "Schu".
Schumacher is often credited with popularising Formula One in Germany, where it was formerly considered a fringe sport. When Schumacher retired in 2006, three of the top ten drivers were German, more than any other nationality and more than have ever been present in Formula One history. Younger German drivers, such as Sebastian Vettel, felt Schumacher was key in their becoming Formula One drivers. In the latter part of his Formula One career, and as one of the senior drivers, Schumacher was the president of the Grand Prix Drivers' Association. In a 2006 FIA survey, Michael Schumacher was voted the most popular driver of the season among Formula One fans.

Schumacher made his Formula One debut with the Jordan-Ford team at the 1991 Belgian Grand Prix, driving car number 32 as a replacement for the imprisoned Bertrand Gachot. Schumacher, still a contracted Mercedes driver, was signed by Eddie Jordan after Mercedes paid Jordan $150,000 for his debut.

The week before the race, Schumacher impressed Jordan designer Gary Anderson and team manager Trevor Foster during a test drive at Silverstone. His manager Willi Weber assured Jordan that Schumacher knew the challenging Spa track well, although in fact he had only seen it as a spectator. During the race weekend, teammate Andrea de Cesaris was meant to show Schumacher the circuit, but was held up with contract negotiations. Schumacher then learned the track on his own, by cycling around the track on a fold-up bike he had brought with him. He impressed the paddock by qualifying seventh in this race. This matched the team's season-best grid position, and out-qualified 11-year veteran de Cesaris. Motorsport journalist Joe Saward reported that after qualifying "clumps of German journalists were talking about 'the best talent since Stefan Bellof. Schumacher retired on the first lap of the race with clutch problems.
Following his Belgian Grand Prix debut, and despite an agreement in principle between Jordan and Schumacher's Mercedes management that would see the German race for the Irish team for the remainder of the season, Schumacher was engaged by Benetton-Ford for the following race. Jordan applied for an injunction in the UK courts to prevent Schumacher driving for Benetton, but lost the case as they had not yet signed a final contract.

Schumacher finished the season with four points out of six races. His best finish was fifth in his second race, the , in which he finished ahead of his team-mate and three-time World Champion Nelson Piquet.

At the start of the season the Sauber team, planning their Formula One debut with Mercedes backing for the following year, invoked a clause in Schumacher's contract that stated that if Mercedes entered Formula One, Schumacher would drive for them. It was eventually agreed that Schumacher would stay with Benetton, Peter Sauber said that "[Schumacher] didn't want to drive for us. Why would I have forced him?". The year was dominated by the Williams of Nigel Mansell and Riccardo Patrese, featuring powerful Renault engines, semi-automatic gearboxes and active suspension to control the car's ride height. In the "conventional" Benetton B192 Schumacher took his place on the podium for the first time, finishing third in the . He went on to take his first victory at the , in a wet race at the Spa-Francorchamps circuit, which by 2003 he would call "far and away my favourite track". He finished third in the Drivers' Championship in 1992 with 53 points, three points behind runner-up Patrese.

The Williams of Damon Hill and Alain Prost also dominated the season. Benetton introduced their own active suspension and traction control early in the season, last of the frontrunning teams to do so. Schumacher won one race, the where he beat Prost, and had nine podium finishes, but retired in seven of the other 15 races. He finished the season in fourth, with 52 points.

The season was Schumacher's first Drivers' Championship. The season, however, was marred by the deaths of Ayrton Senna (witnessed by Schumacher, who was directly behind in 2nd position) and Roland Ratzenberger during the , and by allegations that several teams, but most particularly Schumacher's Benetton team, broke the sport's technical regulations.

Schumacher won six of the first seven races and was leading the , before a gearbox failure left him stuck in fifth gear. Schumacher finished the race in second place. Following the San Marino Grand Prix, the Benetton, Ferrari and McLaren teams were investigated on suspicion of breaking the FIA-imposed ban on electronic aids. Benetton and McLaren initially refused to hand over their source code for investigation. When they did so, the FIA discovered hidden functionality in both teams' software, but no evidence that it had been used in a race. Both teams were fined $100,000 for their initial refusal to cooperate. However, the McLaren software, which was a gearbox program that allowed automatic shifts, was deemed legal. By contrast, the Benetton software was deemed to be a form of "launch control" that would have allowed Schumacher to make perfect starts, which was explicitly outlawed by the regulations. However, there was no evidence to suggest that this software was actually used.

At the , Schumacher was penalised for overtaking on the formation lap. He then ignored the penalty and the subsequent black flag, which indicates that the driver must immediately return to the pits, for which he was disqualified and later given a two-race ban. Benetton blamed the incident on a communication error between the stewards and the team. Schumacher was also disqualified after winning the after his car was found to have illegal wear on its skidblock, a measure used after the accidents at Imola to limit downforce and hence cornering speed. Benetton protested that the skidblock had been damaged when Schumacher spun over a kerb, but the FIA rejected their appeal because of the pattern of wear and damage visible on the block.

These incidents helped Damon Hill close the points gap, and Schumacher led by a single point going into the final race in Australia. On lap 36 Schumacher hit the guardrail on the outside of the track while leading. Hill attempted to pass, but as Schumacher's car returned to the track there was a collision on the corner causing them both to retire. As a result, Schumacher won a very controversial championship, the first German to do so (Jochen Rindt raced under the Austrian flag). At the FIA conference after the race, the new World Champion dedicated his title to Ayrton Senna.

In Schumacher successfully defended his title with Benetton. He now had the same Renault engine as Williams. He accumulated 33 more points than second-placed Damon Hill. With teammate Johnny Herbert, he took Benetton to its first Constructors' Championship and became the youngest two-time World Champion in Formula One history.

The season was marred by several collisions with Hill, in particular an overtaking manoeuvre by Hill took them both out of the on lap 45, and again on lap 23 of the Italian Grand Prix. Schumacher won nine of the 17 races, and finished on the podium 11 times. Only once did he qualify worse than fourth; at the , he qualified 16th, but nevertheless went on to win the race.

In , Schumacher joined Ferrari, a team that had last won the Drivers' Championship in and the Constructors' Championship in , for a salary of $60 million over 2 years. He left Benetton a year before his contract with them expired; he later cited the team's damaging actions in 1994 as his reason for opting out of his deal. A year later Benetton employees Rory Byrne (designer) and Ross Brawn (Technical Director) joined Ferrari.

Ferrari had previously come close to the championship in 1982 and 1990. The team had suffered a disastrous downturn in the early 1990s, partially as its famous V12 engine was no longer competitive against the smaller, lighter and more fuel efficient V10s of its competitors. Various drivers, notably Alain Prost, had given the vehicles labels such as "truck", "pig", and "accident waiting to happen". Furthermore, the poor performance of the Ferrari pit crews was considered a running joke. At the end of 1995, though the team had improved into a solid competitor, it was still considered inferior to front-running teams such as Benetton and Williams. Schumacher declared the Ferrari 412T good enough to win the Championship.

Schumacher, Ross Brawn, Rory Byrne, and Jean Todt (hired in 1993), have been credited as turning this once struggling team into the most successful team in Formula One history. Three-time World Champion Jackie Stewart believes the transformation of the Ferrari team was Schumacher's greatest feat. Eddie Irvine also joined the team, moving from Jordan. During winter testing, Schumacher first drove a Ferrari, their 1995 Ferrari 412 T2, and was two seconds faster than former regulars Jean Alesi and Gerhard Berger had been.

Schumacher finished third in the Drivers' Championship in 1996 and helped Ferrari to second place in the Constructors' Championship ahead of his old team Benetton. He won three races, more than the team's total tally for the period from 1991 to 1995. Early in the 1996 season the car had reliability trouble and Schumacher did not finish six of the 16 races. He took his first win for Ferrari at the Spanish Grand Prix, where he lapped the entire field up to third place in the wet. Having taken the lead on lap 19, he consistently lapped five seconds faster than the rest of the field in the difficult conditions. In the Schumacher qualified in pole position, but suffered engine failure on the race's formation lap. However at Spa-Francorchamps, Schumacher used well-timed pit-stops to fend off Williams's Jacques Villeneuve. Following that, at Monza, Schumacher won in front of the tifosi.

Michael Schumacher and Jacques Villeneuve vied for the title in . Villeneuve, driving the superior Williams FW19, led the championship in the early part of the season. However, by mid-season, Schumacher had taken the championship lead, winning five races, and entered the season's final Grand Prix with a one-point advantage. Towards the end of the race, held at Jerez, Schumacher's Ferrari developed a coolant leak and loss of performance indicating he may not finish the race. As Villeneuve approached to pass his rival, Schumacher attempted to provoke an accident, but got the short end of the stick, retiring from the race. Villeneuve went on and scored four points to take the championship. Schumacher was punished for unsportsmanlike conduct for the collision and was disqualified from the Drivers' Championship.

In , Finnish driver Mika Häkkinen became Schumacher's main title competition. Häkkinen won the first two races of the season, gaining a 16-point advantage over Schumacher. Schumacher then won in Argentina and, with the Ferrari improving significantly in the second half of the season, Schumacher took six victories and had five other podium finishes. Ferrari took a 1–2 finish at the , the first Ferrari 1–2 finish since 1990, and the , which tied Schumacher with Häkkinen for the lead of the Drivers' Championship with 80 points, but Häkkinen won the Championship by winning the final two races. There were two controversies; at the Schumacher was leading on the last lap when he turned into the pit lane, crossed the start finish line and stopped for a ten-second stop go penalty. There was some doubt whether this counted as serving the penalty, but, because he had crossed the finish line when he came into the pit lane, the win was valid. At Spa, Schumacher was leading the race by 40 seconds in heavy spray, but collided with David Coulthard's McLaren when the Scot, a lap down, slowed in very poor visibility to let Schumacher past. After both cars returned to the pits, Schumacher leaped out of his car and headed to McLaren's garage in an infuriated manner and accused Coulthard of trying to kill him. Coulthard admitted five years later that the accident had been his mistake.

Schumacher's efforts helped Ferrari win the Constructors title in . He lost his chance to win the Drivers' Championship at the at the high-speed "Stowe Corner", his car's rear brake failed, sending him off the track and resulting in a broken leg. During his 98-day absence, he was replaced by Finnish driver Mika Salo. After missing six races he made his return at the inaugural , qualifying in pole position by almost a second. He then assumed the role of second driver, assisting teammate Eddie Irvine's bid to win the Drivers' Championship for Ferrari. In the last race of the season, the , Häkkinen won his second consecutive title. Schumacher would later say that Häkkinen was the opponent he respected the most.

During this period Schumacher won more races and championships than any other driver in the history of the sport. Schumacher won his third World Championship in after a year-long battle with Häkkinen. Schumacher won the first three races of the season and five of the first eight. Midway through the year, Schumacher's chances suffered with three consecutive non-finishes, allowing Häkkinen to close the gap in the standings. Häkkinen then took another two victories, before Schumacher won at the . At the post race press conference, after equalling the number of wins (41) won by his idol, Ayrton Senna, Schumacher broke into tears. The championship fight would come down to the penultimate race of the season, the . Starting from pole position, Schumacher lost the lead to Häkkinen at the start. After his second pit-stop, however, Schumacher came out ahead of Häkkinen and went on to win the race and the championship.

In , Schumacher took his fourth drivers' title. Four other drivers won races, but none sustained a season-long challenge for the championship. Schumacher scored a record-tying nine wins and clinched the World Championship with four races yet to run. He finished the championship with 123 points, 58 ahead of runner-up Coulthard. Season highlights included the , where Schumacher finished 2nd to his brother Ralf, thus scoring the first ever 1–2 finish by brothers in Formula One; and the Belgian Grand Prix in which Schumacher scored his 52nd career win, breaking Alain Prost's record for most career wins.

In , Schumacher used the Ferrari F2002 to retain his Drivers' Championship.
There was again some controversy, however, at the , where his teammate, Rubens Barrichello was leading, but in the final metres of the race, under team orders, slowed down to allow Schumacher to win the race. The crowd broke into outraged boos at the result and Schumacher tried to make amends by allowing Barrichello to stand on the top step of the podium. At the later that year, Schumacher dominated the race and was set for a close finish with Barrichello. At the end he slowed down to create a formation finish with Barrichello, but slowed too much allowing Barrichello to take the victory. In winning the Drivers' Championship he equalled the record set by Juan Manuel Fangio of five World Championships. Ferrari won 15 out of 17 races, and Schumacher won the title with six races remaining in the season, which is still the earliest point in the season for a driver to be crowned World Champion. Schumacher broke his own record, shared with Nigel Mansell, of nine race wins in a season, by winning eleven times and finishing every race on the podium. He finished with 144 points, a record-breaking 67 points ahead of the runner-up, his teammate Rubens Barrichello. This pair finished nine of the 17 races in the first two places.

Schumacher broke Juan Manuel Fangio's record of five World Drivers' Championships by winning the drivers' title for the sixth time in , a closely contested season. The biggest competition came once again from the McLaren Mercedes and Williams BMW teams. In the first race, Schumacher ran off track, and in the following two, was involved in collisions. He fell 16 points behind Kimi Räikkönen. Schumacher won the and the next two races, and closed within two points of Räikkönen. Aside from Schumacher's victory in Canada, and Barrichello's victory in Britain, the mid-season was dominated by Williams drivers Ralf Schumacher and Juan Pablo Montoya, who each claimed two victories. After the , Michael Schumacher led Montoya and Kimi Räikkönen by only one and two points, respectively. Ahead of the next race, the FIA announced changes to the way tyre widths were to be measured: this forced Michelin, supplier to Williams and McLaren among others, to rapidly redesign their tyres before the . Schumacher, running on Bridgestone tyres, won the next two races. After Montoya was penalised in the , only Schumacher and Räikkönen remained in contention for the title. At the final round, the , Schumacher needed only one point whilst Räikkönen needed to win. By finishing the race in eighth place, Schumacher took one point and assured his sixth World Drivers' title, ending the season two points ahead of Räikkönen.

In , Schumacher won a record twelve of the first thirteen races of the season, only failing to finish in Monaco after an accident with Juan Pablo Montoya during a safety car period when he briefly locked his car's brakes. He clinched a record seventh drivers' title at the . He finished that season with a record 148 points, 34 points ahead of the runner-up, teammate Rubens Barrichello, and set a new record of 13 race wins out of a possible 18, surpassing his previous best of 11 wins from the season.

Rule changes for the 2005 season required tyres to last an entire race, tipping the overall advantage to teams using Michelins over teams such as Ferrari that relied on Bridgestone tyres. The rule changes were partly in an effort to dent Ferrari's dominance and make the series more interesting. The most notable moment of the early season for Schumacher was his battle with Fernando Alonso in San Marino, where he started 13th and finished only 0.2 seconds behind the Spanish driver. Less than halfway through the season, Schumacher said "I don't think I can count myself in this battle any more. It was like trying to fight with a blunted weapon... If your weapons are weak you don't have a chance." Schumacher's sole win in 2005 came at the . Prior to that race, the Michelin tyres were found to have significant safety issues. When no compromise between the teams and the FIA could be reached, all but the six drivers using Bridgestone tyres dropped out of the race after the formation lap. Schumacher retired in six of the 19 races. He finished the season in third with 62 points, fewer than half the points of World Champion Alonso.

Schumacher was stripped of pole position at the and started the race at the back of the grid. This was due to his stopping his car and blocking part of the circuit while Alonso was on his qualifying lap; he still managed to work his way up to 5th place on the notoriously cramped Monaco circuit.
By the , the ninth race of the season, Schumacher was 25 points behind Alonso, but he then won the following three races to reduce his disadvantage to 11. After his victories in Italy (in which Alonso had an engine failure) and China, in which Alonso had tyre problems, Schumacher led in the championship standings for the first time during the season. Although he and Alonso had the same point total, Schumacher was in front because he had won more races.

The Japanese Grand Prix was led by Schumacher with only 16 laps to go, when, for the first time since the 2000 French Grand Prix, Schumacher's car suffered an engine failure. Alonso won the race, giving himself a ten-point championship lead. With only one race left in the season, Schumacher could only win the championship if he won the season finale and Alonso scored no points.

Before the , Schumacher conceded the title to Alonso. In pre-race ceremonies, football legend Pelé presented a trophy to Schumacher for his years of dedication to Formula One. During the race's qualifying session, Schumacher had one of the quickest times during the first session and was fastest in the second session; but a fuel pressure problem prevented him from completing a single lap during the third session, forcing him to start the race in tenth position. Early in the race Schumacher moved up to sixth place. However, in overtaking Alonso's teammate, Giancarlo Fisichella, Schumacher experienced a tyre puncture caused by the front wing of Fisichella's car. Schumacher pitted and consequently fell to 19th place, 70 seconds behind teammate and race leader Felipe Massa. Schumacher recovered and overtook both Fisichella and Räikkönen to secure fourth place. His performance was classified in the press as "heroic", an "utterly breath-taking drive", and a "performance that ... sums up his career".

While Schumacher was on the podium after winning the 2006 Italian Grand Prix, Ferrari issued a press release stating that he would retire from racing at the end of the 2006 season. Schumacher confirmed his retirement. The press release stated that Schumacher would continue working for Ferrari. It was revealed on 29 October 2006 that Ferrari wanted Schumacher to act as assistant to the newly appointed CEO Jean Todt. This would involve selecting the team's future drivers. After Schumacher's announcement, leading Formula One figures such as Niki Lauda and David Coulthard hailed Schumacher as the greatest all-round racing driver in the history of Formula One. The tifosi and the Italian press, who did not always take to Schumacher's relatively cold public persona, displayed an affectionate response after he announced his retirement.

He attended several Grands Prix during the season. Schumacher drove the Ferrari F2007 for the first time on 24 October at Ferrari's home track in Fiorano, Italy. He ran no more than five laps and no lap times were recorded. A Ferrari spokesman said the short drive was done for the Fiat board of directors who were holding their meeting in Maranello.
During the season Schumacher acted as Ferrari's adviser and Jean Todt's 'super assistant'.
On 13 November 2007 Schumacher, who had not driven a Formula One car since he had retired a year earlier, undertook a formal test session for the first time aboard the F2007. He returned in December 2007 to continue helping Ferrari with their development programme at Jerez circuit. He focused on testing electronics and tyres for the 2008 Formula One season.

In 2007, former Ferrari top manager Ross Brawn said that Schumacher was very likely and also happy to continue testing in 2008; Schumacher later explained his role further saying that he would "deal with the development of the car inside Gestione Sportiva" and as part of that "I'd like to drive, but not too often".

During 2008 Schumacher also competed in motorcycle racing in the IDM Superbike-series, but stated that he had no intention of a second competitive career in this sport. He was quoted as saying that riding a Ducati was the most exhilarating thing he had done in his life, the second most being sky diving.

In his capacity as racing advisor to Ferrari, Schumacher was present in Budapest for the when Ferrari driver Felipe Massa was seriously injured after being struck by a suspension spring during qualifying. As it became clear that Massa would be unable to compete in the next race at Valencia, Schumacher was chosen as a replacement for the Brazilian driver and on 29 July 2009, Ferrari announced that they planned to draft in Schumacher for the and subsequent Grands Prix until Massa was able to race again. Schumacher tested in a modified F2007 to prepare himself as he had been unable to test the 2009 car due to testing restrictions. Ferrari appealed for special permission for Schumacher to test in a 2009 spec car, but Williams, Red Bull and Toro Rosso were against this test. In the end, Schumacher was forced to call off his return due to the severity of the neck injury he had received in a motorcycle accident earlier in the year. Massa's place at Ferrari was instead filled by Luca Badoer and Giancarlo Fisichella.

In December 2009 it was announced that Schumacher would be returning to Formula One in the season alongside fellow German driver Nico Rosberg in the new Mercedes GP team. The new Mercedes team was their first majority involvement in an F1 team since 1955. Schumacher stated that his preparations to replace the injured Massa for Ferrari had initiated a renewed interest in F1, which, combined with the opportunity to fulfil a long-held ambition to drive for Mercedes and to be working again with team principal Ross Brawn, led Schumacher to accept the offer once he was passed fit. After a period of intensive training medical tests, it was confirmed that the neck injury that had prevented him driving for Ferrari the year before had fully healed. Schumacher signed a three-year contract, reportedly worth £20m.

Schumacher's surprise return to F1 was compared to Niki Lauda's in 1982 aged 33 and Nigel Mansell's return in 1994 at age 41. Schumacher turned 41 in January 2010 and his prospects with Mercedes were compared with the record set by the oldest F1 champion Juan Manuel Fangio who was 46 when he won his fifth championship.

Schumacher's first drive of the 2010 Mercedes car – the Mercedes MGP W01 – was at an official test in February 2010 in Valencia. He finished sixth in the first race of the season at the . After the Malaysian race, former driver Stirling Moss suggested that Schumacher, who had finished behind his teammate in each of the first four qualifying sessions and races, might be "past it". Many other respected former Formula One drivers thought otherwise, including former rival Damon Hill, who warned "you should never write Schumacher off". GrandPrix.com identified the inherent understeer of the Mercedes car, exacerbated by the narrower front tyres introduced for the 2010 season, as contributing to Schumacher's difficulties. Jenson Button would later claim that Mercedes's 2010 car was designed for him, and that their differing driving styles may have contributed to Schumacher's difficulties.

Mercedes upgraded their car for the where Schumacher finished fourth. At the Schumacher finished sixth after passing Ferrari's Fernando Alonso on the final corner of the race when the safety car returned to the pits. However, he was penalised 20 seconds after the race by the race stewards dropping him to 12th. The stewards judged the pass to be in breach of the FIA's sporting code. Mercedes's differing interpretation of the regulation would later lead to it being clarified by the FIA.
In Turkey, Schumacher qualified fifth, and finished fourth in the race, both his best results since his return. In in Valencia, Schumacher finished 15th, the lowest recorded finish in his career. In Hungary, Schumacher finished outside the points in eleventh, but was found guilty of dangerous driving at while unsuccessfully defending tenth position against Rubens Barrichello. As a result, he was demoted ten places on the grid for the following race, the Belgian Grand Prix, where he finished seventh, despite starting 21st after his grid penalty.

At the season finale in Abu Dhabi, Schumacher was involved in a major accident on the first lap, which occurred after a spin. In recovering from the incident Vitantonio Liuzzi's car collided with Schumacher, barely missing his head. Nobody was hurt in the crash, but Schumacher said the crash had been "frightening".

He finished the season 9th with 72 points. Before, it had happened only in his début in 1991 that he finished without a win, pole position, podium or fastest lap.

Schumacher's first points of 2011 were scored in Malaysia, he later came sixth in Spain and had a strong race at the finishing fourth, after running as high as second in a wet race. Schumacher was passed late in the race by eventual winner Jenson Button.
Schumacher clashed with Vitaly Petrov in Valencia, and with Kamui Kobayashi in Britain, and marked the 20th anniversary of his Formula One début at the . Despite starting last in Belgium, Schumacher raced well and finished fifth. Schumacher again raced well in Italy, duelling with Lewis Hamilton for fourth place. The saw Schumacher lead three laps during the race, marking the first time he had led a race since 2006. In doing so, he became the oldest driver to lead a race since Jack Brabham in .

At the Schumacher started well and finished fifth after overtaking Rosberg at the end of the race. Schumacher diced again with Rosberg in , battling over sixth position on the first lap. Schumacher finished the season in eighth place in the Drivers' Championship, with 76 points.

Schumacher was again partnered by Rosberg at Mercedes for the 2012 season. Schumacher retired from the inaugural race of the season , and scored a point in the second race in Malaysia. In China Schumacher started on the front row alongside Rosberg on pole, but retired due to a loose wheel after a mechanics error during a pit stop.

After causing a collision with Bruno Senna in Spain, Schumacher received a five-place grid penalty for the Monaco Grand Prix. Schumacher was fastest in qualifying in Monaco; but started sixth owing to his penalty. He later retired from seventh place in the race.

At the , Schumacher finished third in the race, his only podium finish since his return to F1 with Mercedes. At the age of 43 years and 173 days, he became the oldest driver to achieve a podium since Jack Brabham's second-place finish at the 1970 British Grand Prix. Further records were set by Schumacher in Germany, where he set the fastest lap in a Grand Prix for the 77th time in his career, and in Belgium where he became the second driver in history to race in 300 Grands Prix.

Schumacher's indecision over his future plans in F1 led to him being replaced by Lewis Hamilton at Mercedes for the 2013 season. In October 2012, Schumacher announced he would retire for a second time at the conclusion of the season. The following week he was quoted as saying: "There were times in the past few months in which I didn't want to deal with Formula One or prepare for the next Grand Prix." The season and his 21-year F1 career concluded with the 2012 Brazilian Grand Prix, in which Schumacher finished seventh. He placed 13th in the 2012 Drivers' Championship.

Schumacher, in conjunction with Schuberth, helped develop the first lightweight carbon helmet. In 2004, a prototype was publicly tested by being driven over by a tank; it survived intact. The helmet keeps the driver cool by funneling directed airflow through fifty holes. Schumacher's original helmet sported the colours of the German flag and his sponsor's decals. On the top was a blue circle with white astroids. From the 2000 Monaco Grand Prix, in order to differentiate his colours from his new teammate Rubens Barrichello, Schumacher changed the upper blue colour and some of the white areas to red. For the Brazilian Grand Prix race of 2006 (at the time intended to be his final Grand Prix), he wore an all-red helmet that included the names of his ninety-one Grand Prix victories. For the 2011 Belgian Grand Prix, Schumacher's 20th anniversary in Formula One, he wore a commemorative gold-leafed helmet. The helmet, very similar to his current helmet, included the year of his début to the present, and the years of his seven World titles. For the 2012 Belgian Grand Prix, Schumacher's 300th Grand Prix appearance, he wore a special platinum-leafed helmet with a message of his achievement.

Schumacher was honoured many times during his career. In April 2002, for his contributions to sport and his contributions in raising awareness of child education, he was named as one of the UNESCO Champions for sport, joining the other eight, which include Pelé, Sergey Bubka and Justine Henin. He won the Laureus World Sportsman of the Year award twice, in 2002 and 2004 for his performances in the and seasons respectively. He also received nominations for the 2001, 2003, 2005 and 2007 awards. He shares the record for having the second-most nominations for the award with Roger Federer with six nominations, and is eclipsed only by Tiger Woods who has been nominated seven times. He holds the distinction of having the most nominations for a motorsport athlete, (Fernando Alonso has been nominated only twice, Sebastian Vettel three times, and Valentino Rossi five times) and being the only motorsport athlete to have won the award more than once.

In honour of Schumacher's racing career and his efforts to improve safety and the sport, he was awarded an FIA Gold Medal for Motor Sport in 2006. In 2007, in recognition of his contribution to Formula One racing, the Nürburgring racing track renamed turns 8 and 9 (the Audi and Shell Kurves) as the "Schumacher S", and a month later he presented A1 Team Germany with the A1 World Cup at the A1GP World Cup of Motorsport 2007 awards ceremony. He was nominated for the Prince of Asturias Award for Sport for 2007, which he won both for sporting prowess and for his humanitarian record.

In 2008, the Swiss Football Association appointed long-time Swiss resident Schumacher as the country's ambassador for the 2008 European football championship.

On 30 April 2010, Schumacher was honored with the Officier of Légion d'honneur title from French prime minister François Fillon.

On 13 November 2014, Schumacher was awarded the Millennium Trophy at the Bambi Awards.

Going into the 1994 Australian Grand Prix, the final race of the 1994 season, Schumacher led Damon Hill by a single point in the Drivers' Championship. Schumacher led the race from the beginning, but on lap 35 he went off track and hit the wall with his right side wheels, returning to the track at reduced speed, and with car damage, but still leading the race. At the next corner Hill attempted to pass on the inside, but Schumacher turned in sharply and they collided. Both cars were eliminated from the race and, as neither driver scored, Schumacher took the title. The race stewards judged it a racing accident and took no action against either driver, but public opinion is divided over the incident, and Schumacher was vilified in the British media.

At the 1997 European Grand Prix at Jerez, the last race of the season, Schumacher led Williams's Jacques Villeneuve by one point in the Drivers' Championship. As Villeneuve attempted to pass Schumacher at the Dry Sac corner on lap 48, Schumacher turned in and the right-front wheel of Schumacher's Ferrari hit the left sidepod of Villeneuve's car. Schumacher retired from the race as a result, but Villeneuve finished in third place, taking four points and so becoming the World Champion. The race stewards did not initially award any penalty, but two weeks after the race Schumacher was disqualified from the entire 1997 Drivers' Championship after an FIA disciplinary hearing found that his "manoeuvre was an instinctive reaction and although deliberate not made with malice or premeditation. It was a serious error." Schumacher accepted the decision and admitted having made a mistake. Schumacher's actions were widely condemned in British, German, and Italian newspapers. This made Schumacher the only driver in the history of the sport, as of 2017, to be disqualified from a Drivers' World Championship.

Historically, team orders have always been an accepted part of Formula One. However, in the final metres of the 2002 Austrian Grand Prix, Schumacher's teammate, Rubens Barrichello, slowed his car under orders from Ferrari to allow Schumacher to pass and win the race. Although the switching of positions did not break any actual sporting or technical regulation, it angered fans and it was claimed that the team's actions showed a lack of sportsmanship and respect to the spectators. Many argued that Schumacher did not need to be "given" wins in only the 6th race of the season, particularly given that he had already won four of the previous five Grands Prix, and that Barrichello had dominated the race weekend up to that point. At the podium ceremony, Schumacher pushed Barrichello onto the top step, and for this disturbance, the Ferrari team incurred a US$1 million fine. Later in the season at the end of the 2002 United States Grand Prix, Schumacher slowed down within sight of the finishing line, allowing Barrichello to win by 0.011 seconds, the 2nd closest margin in F1 history. Schumacher's explanation varied between it being him "returning the favour" for Austria (now that Schumacher's title was secure), or trying to engineer a dead-heat (a feat derided as near-impossible in a sport where timings are taken to within a thousandth of a second). The FIA subsequently banned "team orders which interfere with the race result", but the ban was lifted for the 2011 season because the ruling was difficult to enforce.

During his spell in Sauber, in the 1991 Sportscar World Championship, Schumacher was involved in a serious incident with Derek Warwick in that year's 430 km of Nürburgring. While trying to set his flying lap in qualifying, Schumacher encountered Warwick's Jaguar on a slow lap resulting in lost time for Schumacher. As retaliation for being in his way, Schumacher swerved the Sauber into Warwick's car, hitting the Jaguar's nose and front wheel. Enraged by the German's attitude, Warwick drove to the pits and chased a fleeing Schumacher on foot through the Sauber pits. He eventually caught up with Schumacher, and it took intervention from Jochen Mass to prevent Warwick physically assaulting Schumacher.

Toward the end of the 2010 Hungarian Grand Prix, Rubens Barrichello attempted to pass Schumacher down the inside on the main straight. Schumacher closed the inside line to force Barrichello onto the outside, but Barrichello persisted on the inside at , despite the close proximity of a concrete wall and Schumacher leaving him only inches to spare. Barrichello said "It is the most dangerous thing that I have been through", and "There is not a rule for that, but between ourselves we should take a line, stick to it and that's it." Schumacher said that "Obviously there was space enough to go through. We didn't touch, so I guess I just left enough space for him to come through." Ross Brawn said "at the end of the day he gave him enough space. You can argue that it was marginal, but it was just tough – tough racing." A range of ex-drivers and commentators were highly critical of Schumacher. Although there was no accident, the race steward, the same Derek Warwick of the 1991 Nürburgring incident, wanted to black flag Schumacher since that "would have shown a better example to our young drivers". The Hungaroring incident was ruled to be dangerous and Schumacher received a 10 place grid penalty for the next race. Schumacher accepted the decision, and apologised.

In 1994, suspicion of foul play by the Benetton team (who were eventually found to have been responsible for some technical violations over the course of the season) was said to have troubled Ayrton Senna that season. For example, in the words of his then teammate, Damon Hill, Senna had chosen to spend a bit of time at the first corner of the Aida circuit following his retirement from the Pacific Grand Prix. After listening to Schumacher's Benetton B194 as it went past, Senna "concluded that there was, what he regarded, as unusual noises from the engine". The FIA subsequently issued a press release setting out action that it required teams to take prior to the German Grand Prix, given that various cars were found to have an advanced engine management systems emulating launch and traction control.

In 1995, Schumacher and Williams driver David Coulthard were disqualified for fuel irregularities, after a switch to Renault engines and Elf oils. On appeal, both drivers had their results and points reinstated, but both teams lost the points the results would normally have earned in the Constructors' Championship.

The 1998 Canadian Grand Prix saw Schumacher accused of dangerous driving when his exit from the pit-lane forced Heinz-Harald Frentzen off the track and into retirement. Despite receiving a 10-second penalty, Schumacher recovered and won the race.

Two laps from the finish of the 1998 British Grand Prix, Schumacher was leading the race when he was issued a stop-and-go penalty for overtaking a lapped car (Alexander Wurz) during the early moments of a Safety Car period. This penalty involves going into the pit lane and stopping for 10 seconds, and the rules state that a driver must serve his penalty within three laps of the penalty being issued. On the third lap after receiving the penalty, Schumacher turned into the pit lane to serve his penalty, but as this was the last lap of the race, and as Ferrari's pit box was located after the start/finish line, Schumacher technically finished the race before serving the penalty. The stewards initially resolved that problem by adding 10 seconds to Schumacher's race time, then later rescinded the penalty completely due to the irregularities in how the penalty had been issued.

During qualifying for the 2006 Monaco Grand Prix Schumacher set the fastest time, but his car stopped in the Rascasse corner on the racing line, leaving the corner partially blocked, while his main contender for the season title, Fernando Alonso, was on his final qualifying lap. Schumacher stated that he simply locked up the wheels going into the corner and that the car then stalled while he attempted to reverse out. Alonso believed he would have been on pole if the incident had not happened, and Schumacher was stripped of pole position by the race stewards and started the race at the back of the grid. In the same qualifying session, Giancarlo Fisichella was similarly found to have blocked David Coulthard from improving his time, but Fisichella was only demoted five places on the grid. At the 2010 Monaco Grand Prix, the safety car was deployed after an accident, involving Karun Chandhok and Jarno Trulli, and pulled into the pits on the last lap. Schumacher passed Alonso before the finish line. Mercedes held that "the combination of the race control messages 'Safety Car in this lap' and 'Track Clear' and the green flags and lights shown by the marshals after safety car line one indicated that the race was not finishing under the safety car and all drivers were free to race. ." However an FIA investigation found Schumacher guilty of breaching Safety Car regulations and awarded him a 20-seconds penalty, which cost him six places.

Schumacher's younger brother Ralf was also a Formula One driver until the end of 2007. Their stepbrother Sebastian Stahl has also been a racing driver.

In August 1995, Michael married Corinna Betsch. They have two children, a daughter Gina-Marie, born 20 February 1997 and a son Mick, born 22 March 1999. He has always been very protective of his private life and is known to dislike the celebrity spotlight. The family moved to a newly-built mansion near Gland, Switzerland in 2007, covering an area of with a private beach on Lake Geneva and featuring an underground garage and petrol station.

Schumacher and his wife own horse ranches in Texas and Switzerland.

The family has two dogs – one stray that Corinna fell in love with in Brazil, and an Australian Shepherd named "Ed" whose arrival in the family made headlines. In fact, in 2007, Schumacher personally drove a taxi through the Bavarian town of Coburg to collect the dog and enable the family to make their return flight to Switzerland. Both Schumacher and the taxi driver were reprimanded by local police.

One of his main hobbies was horse riding, and he played football for his local team FC Echichens. He has appeared in several charity football games and organised games between Formula One drivers.

On 23 June 2003, Schumacher was appointed as an Ambassador-at-Large for the Most Serene Republic of San Marino.

Schumacher is a special ambassador to UNESCO and has donated 1.5 million euros to the organization. Additionally, he paid for the construction of a school for poor children and for area improvements in Dakar, Senegal. He supports a hospital for child victims of war in Sarajevo, which specialises in caring for amputees. In Lima, Peru he funded the "Palace for the Poor", a centre for helping homeless street children obtain an education, clothing, food, medical attention, and shelter. He stated his interest in these various efforts was piqued both by his love for children and the fact that these causes had received little attention. While an exact figure for the amount of money he has donated throughout his life is unknown, it is known that in his last four years as a driver, he donated at least $50 million. In 2008, it was revealed that he had donated between $5M and $10M to the William J. Clinton Presidential Center and Park of Bill Clinton.

Since his participation in an FIA European road safety campaign, as part of his punishment after the collision at the 1997 European Grand Prix, Schumacher continued to support other campaigns, such as Make Roads Safe, which is led by the FIA Foundation and calls on G8 countries and the UN to recognise global road deaths as a major global health issue. In 2008, Schumacher was the figurehead of an advertising campaign by Bacardi to raise awareness about responsible drinking, with a focus on communicating an international message 'drinking and driving don't mix'. He featured in an advertising campaign for television, cinema and online media, supported by consumer engagements, public relations and digital media across the World.

On the eve of the 2002 British Grand Prix, on behalf of Fiat, Schumacher presented a Ferrari 360 Modena to the Indian cricketer Sachin Tendulkar at Silverstone.

On 21 June 2009, Schumacher appeared on the BBC's motoring programme "Top Gear" as the Stig. Presenter Jeremy Clarkson hinted later in the programme that Schumacher was not the regular Stig, and the BBC has since confirmed that this is the case. Schumacher was there on that occasion because Ferrari would not allow anyone else to drive the one-of-a-kind black Ferrari FXX that was featured in the show.

During his interview with Clarkson, Schumacher stated that his road cars are a Fiat 500 Abarth, and a Fiat Croma, which is his family car.

In 2004, "Forbes" magazine listed him as the second highest paid athlete in the World. In 2005, "Eurobusiness" magazine identified Schumacher as the World's first billionaire athlete. His 2004 salary was reported to be around US$80 million. "Forbes" magazine ranked him 17th in its "The World's Most Powerful Celebrities" list. A significant share of his income came from advertising. For example, Deutsche Vermögensberatung paid him $8 million over three years from 1999 for wearing a 10 by 8 centimetre advertisement on his post-race cap. The deal was extended until 2010. He donated $10 million for aid after the 2004 Indian Ocean earthquake. His donation surpassed that of any other sports person, most sports leagues, many worldwide corporations and even some countries. Schumacher's bodyguard Burkhard Cramer and Cramer's two sons were killed in the tsunami.

In 2010, his personal fortune was estimated at £515 million. He reportedly received a salary of £21 million each year from the Mercedes team, plus a further £9 million in endorsements.

On 29 December 2013, Schumacher was skiing with his 14-year-old son Mick, descending the Combe de Saulire below the Dent de Burgin above Méribel in the French Alps. While crossing an unsecured off-piste area between Piste Chamois and Piste Mauduit, he fell and hit his head on a rock, sustaining a serious head injury despite wearing a ski helmet. According to his physicians, he would most likely have died if he had not been wearing a helmet. He was airlifted to Grenoble Hospital where he underwent two surgical interventions.
Schumacher was put into a medically induced coma because of traumatic brain injury; his doctors reported on 7 March 2014 that his condition was stable. On 4 April 2014, Schumacher's agent reported that he was showing "moments of consciousness" as he was gradually withdrawn from the medically induced coma, adding to reports by relatives of "small encouraging signs" over the preceding month. In mid-June, he was moved from intensive care into a rehabilitation ward. By 16 June 2014, Schumacher had regained consciousness and left Grenoble Hospital for further rehabilitation at the University Hospital (CHUV) in Lausanne, Switzerland. On 9 September 2014, Schumacher left CHUV and was brought back to his home for further rehabilitation. In November 2014, it was reported that Schumacher was "paralysed and in a wheelchair"; he "cannot speak and has memory problems". In a video interview released in May 2015, Schumacher's manager Sabine Kehm said that his condition is slowly improving "considering the severeness of the injury he had".

In September 2016, Felix Damm, lawyer for Schumacher, told a German court that his client "cannot walk", in response to false reports from December 2015 in German publication "Die Bunte" that he could "walk a couple of steps".


 Schumacher was disqualified from the 1997 World Drivers' Championship due to dangerous driving in the European Grand Prix, where he caused an avoidable accident with Jacques Villeneuve. His points tally would have placed him in second place in that year's standings.

Schumacher holds the following records in Formula One:


Schumacher had a voice role in the Disney/Pixar film "Cars". His character is himself as a Ferrari F430. The French film "Asterix and Obelix at the Olympic Games" features Schumacher in a cameo role as a chariot driver called Schumix.


All race and championship results (1991–2006) are taken from the official Formula 1 website (Formula1.com).



</doc>
<doc id="20398" url="https://en.wikipedia.org/wiki?curid=20398" title="Muonium">
Muonium

Muonium is an exotic atom made up of an antimuon and an electron, which was discovered in 1960 by Vernon W. Hughes

Although muonium is short-lived, physical chemists study it using muon spin spectroscopy (μSR), a magnetic resonance technique analogous to nuclear magnetic resonance (NMR) or electron spin resonance (ESR) spectroscopy. Like ESR, μSR is useful for the analysis of chemical transformations and the structure of compounds with novel or potentially valuable electronic properties. Muonium is usually studied by muon spin rotation, in which the Mu atom's spin precesses in a magnetic field applied transverse to the muon spin direction (since muons are typically produced in a spin-polarized state from the decay of pions), and by avoided level crossing (ALC), which is also called level crossing resonance (LCR). The latter employs a magnetic field applied longitudinally to the polarization direction, and monitors the relaxation of muon spins caused by "flip/flop" transitions with other magnetic nuclei.

Because the muon is a lepton, the atomic energy levels of muonium can be calculated with great precision from quantum electrodynamics (QED), unlike in the case of hydrogen, where the precision is limited by uncertainties related to the internal structure of the proton. For this reason, muonium is an ideal system for studying bound-state QED and also for searching for physics beyond the standard model.

Normally in the nomenclature of particle physics, an atom composed of a positively charged particle bound to an electron is named after the positive particle with "-ium" appended, in this case "muium". The suffix "-onium" is mostly used for bound states of a particle with its own antiparticle. The exotic atom consisting of a muon and an antimuon is known as "true muonium". It is yet to be observed, but it may have been generated in the collision of electron and positron beams.


</doc>
<doc id="20401" url="https://en.wikipedia.org/wiki?curid=20401" title="Medicine man">
Medicine man

A medicine man or medicine woman is a traditional healer and spiritual leader who serves a community of indigenous people of the Americas. Individual cultures have their own names, in their respective Indigenous languages, for the spiritual healers and ceremonial leaders in their particular cultures. 

In the ceremonial context of Indigenous North American communities, "medicine" usually refers to "spiritual" healing. Medicine men/women should not be confused with those who employ Native American ethnobotany, a practice that is very common in a large number of Native American and First Nations households.

The terms "medicine people" or "ceremonial people" are sometimes used in Native American and First Nations communities, for example, when Arwen Nuttall (Cherokee) of the National Museum of the American Indian writes, "The knowledge possessed by medicine people is privileged, and it often remains in particular families."

Native Americans tend to be quite reluctant to discuss issues about medicine or medicine people with non-Indians. In some cultures, the people will not even discuss these matters with Indians from other tribes. In most tribes, medicine elders are prohibited from advertising or introducing themselves as such. As Nuttall writes, "An inquiry to a Native person about religious beliefs or ceremonies is often viewed with suspicion." One example of this is the Apache medicine cord or "Izze-kloth" whose purpose and use by Apache medicine elders was a mystery to nineteenth century ethnologists because "the Apache look upon these cords as so sacred that strangers are not allowed to see them, much less handle them or talk about them."
The 1954 version of "Webster's New World Dictionary of the American Language" reflects the poorly-grounded perceptions of the people whose use of the term effectively defined it for the people of that time: "a man supposed to have supernatural powers of curing disease and controlling spirits." In effect, such definitions were not explanations of what these "medicine people" are to their own communities but instead reported on the consensus of socially and psychologically remote observers when they tried to categorize the individuals. The term "medicine man/woman," like the term "shaman," has been criticized by Native Americans, as well as other specialists in the fields of religion and anthropology.

While non-Native anthropologists sometimes use the term "shaman" for Indigenous healers worldwide, including the Americas, "shaman" is the specific name for a spiritual mediator from the Tungusic peoples of Siberia and is not used in Native American or First Nations communities.

The term "medicine man/woman" has also frequently been used by Europeans to refer to African traditional healers, along with the offensive term "witch doctors".

Cherokee spiritual, ceremonial and healing knowledge has been passed down for thousands of years. The Cherokee people were among the first Native Americans to formalize a written language. Some of the information in the Cherokee ledgers is written in code to prevent other people from trying to misuse or exploit their medicine ways. As in all Native American cultures, Cherokee medicine people had to practice in secret from the post-contact era until 1978, when the American Indian Religious Freedom Act was passed. 

Training a Cherokee medicine person takes many years due to the vast amount of knowledge needed to practice. Modern-day Cherokee medicine people must be born and raised in the Cherokee community and culture, and raised with the language. The skills of gifted and well-trained medicine people are still very important to the Cherokee people, though genocide and oppression have resulted in there being fewer now than pre-contact. There are many fraudulent healers and scam artists who pose as Cherokee "shamans", and the Cherokee Nation has had to speak out against these people, even forming a task force to handle the issue. In order to seek help from a Cherokee medicine person a person needs to know someone in the community who can vouch for them and provide a referral. Usually one makes contact through a relative who knows the healer. 



</doc>
<doc id="20403" url="https://en.wikipedia.org/wiki?curid=20403" title="Malay Peninsula">
Malay Peninsula

The Malay Peninsula (; , , , ) is a peninsula in Southeast Asia. The land mass runs approximately north-south and, at its terminus, is the southernmost point of the Asian mainland. The area contains Peninsular Malaysia, Southern Thailand, and the southernmost tip of Myanmar.

The Titiwangsa Mountains are part of the Tenasserim Hills system, and form the backbone of the Peninsula. They form the southernmost section of the central cordillera which runs from Tibet through the Kra Isthmus (the Peninsula's narrowest point) into the Malay Peninsula. The Strait of Malacca separates the Malay Peninsula from the Indonesian island of Sumatra while the south coast is separated from the island of Singapore by the Straits of Johor.

The Malay term "Tanah Melayu" is derived from the word "Tanah" (land) and "Melayu" (Malays), thus it means "the Malay land". The term can be found in various pre-modern Malay texts, of which the oldest dating back to the early 17th century. It is frequently mentioned in the "Hikayat Hang Tuah", a well known classical work that began as oral tales associated with the legendary heroes of Malacca Sultanate. "Tanah Melayu" in the text is consistently employed to refer to the area under Malaccan dominance. In the early 16th century, Tomé Pires, a Portuguese apothecary who stayed in Malaca from 1512 to 1515, writes an almost identical term, "Terra de Tana Malaio" which he referred to the southeastern part of Sumatra, where the deposed Sultan of Malacca, Mahmud Shah established his exiled government. The 17th century's account of Portuguese historian, Emanuel Godinho de Erédia, noted on the region of "Malaios" surrounded by the Andaman Sea in the north, the entire Malacca Strait in the centre, a part of Sunda Strait in the south, and the western part of South China Sea in the east.

Prior to the foundation of Malacca, reference to Malay peninsula was made in different terms from various foreign sources. According to several Indian scholars, the word "Malayadvipa" ("mountain-insular continent"), mentioned in the ancient Indian text, "Vayu Purana", may possibly refer to the Malay peninsula. Another Indian source, an inscription on the south wall of the Brihadeeswarar Temple, recorded the word "Malaiur", referring to a kingdom in Malay peninsula that had "a strong mountain for its rampart". The Greek source, "Geographia", written by Ptolemy, labelled a geographical part of "Golden Chersonese" as "Maleu-kolon", a term thought to derive from Sanskrit "malayakolam" or "malaikurram". While the Chinese chronicle of Yuan dynasty mentioned the word "Ma-li-yu-er", referring to a nation of Malay peninsula that threatened by the southward expansion of Sukhothai Kingdom under King Ram Khamhaeng. During the same era, Marco Polo made a reference to "Malauir" in his travelogue, as a kingdom located in the Malay peninsula, possibly similar to the one mentioned in Yuan chronicle.

In the early 20th century, the term "Tanah Melayu" was generally used by the Malays of the peninsula during the rise of Malay nationalism to describe uniting all Malay states on the peninsula under one Malay nation, although this ambition was largely realised with the formation of "Persekutuan Tanah Melayu" (Malay for "Federation of Malaya") in 1948.



</doc>
<doc id="20405" url="https://en.wikipedia.org/wiki?curid=20405" title="Miles Davis">
Miles Davis

Miles Dewey Davis III (May 26, 1926September 28, 1991) was an American jazz trumpeter, bandleader, and composer. He is among the most influential and acclaimed figures in the history of jazz and 20th century music. Davis adopted a variety of musical directions in his five-decade career which kept him at the forefront of a number of major stylistic developments in jazz.

Born and raised in Illinois, Davis left his studies at The Juilliard School in New York City and made his professional debut as a member of saxophonist Charlie Parker's bebop quintet from 1944 to 1948. Shortly after, he recorded the "Birth of the Cool" sessions for Capitol Records, which were instrumental to the development of cool jazz. In the early 1950s, Davis recorded some of the earliest hard bop music while on Prestige Records but did so haphazardly due to a heroin addiction. After a widely acclaimed comeback performance at the Newport Jazz Festival in 1955, he signed a long-term contract with Columbia Records and recorded the 1957 album "'Round About Midnight". It was his first work with saxophonist John Coltrane and bassist Paul Chambers, key members of the sextet he led into the early 1960s. During this period, he alternated between orchestral jazz collaborations with arranger Gil Evans, such as the Spanish-influenced "Sketches of Spain" (1960), and band recordings, such as "Milestones" (1958) and "Kind of Blue" (1959). The latter recording remains one of the most popular jazz albums of all time, having sold over four million copies in the U.S.

Davis made several line-up changes while recording "Someday My Prince Will Come" (1961), his 1961 Blackhawk concerts, and "Seven Steps to Heaven" (1963), another mainstream success that introduced bassist Ron Carter, pianist Herbie Hancock, and drummer Tony Williams. After adding saxophonist Wayne Shorter to his new quintet in 1964, Davis led them on a series of more abstract recordings often composed by the band members, helping pioneer the post-bop genre with albums such as "E.S.P" (1965) and "Miles Smiles" (1967), before transitioning into his electric period. During the 1970s, he experimented with rock, funk, African rhythms, emerging electronic music technology, and an ever-changing line-up of musicians, including keyboardist Joe Zawinul, drummer Al Foster, and guitarist John McLaughlin. This period, beginning with Davis' 1969 studio album "In a Silent Way" and concluding with the 1975 concert recording "Agharta", was the most controversial in his career, alienating and challenging many in jazz. His million-selling 1970 record "Bitches Brew" helped spark a resurgence in the genre's commercial popularity with jazz fusion as the decade progressed.

After a five-year retirement due to poor health, Davis resumed his career in the 1980s, employing younger musicians and pop sounds on albums such as "The Man with the Horn" (1981) and "Tutu" (1986). Critics were generally unreceptive but the decade garnered the trumpeter his highest level of commercial recognition. He performed sold-out concerts worldwide while branching out into visual arts, film, and television work, before his death in 1991 from the combined effects of a stroke, pneumonia and respiratory failure. In 2006, Davis was inducted into the Rock and Roll Hall of Fame, which recognized him as "one of the key figures in the history of jazz". "Rolling Stone" described him as "the most revered jazz trumpeter of all time, not to mention one of the most important musicians of the 20th century," while Gerald Early called him inarguably one of the most influential and innovative musicians of that period.

Miles Dewey Davis III was born on May 26, 1926, to an affluent African-American family in Alton, Illinois, fifteen miles north of St. Louis. He had an older sister, Dorothy Mae (born 1925), and a younger brother, Vernon (born 1929). His mother, Cleota Mae Henry of Arkansas, was a music teacher and violinist, and his father, Miles Dewey Davis Jr., also of Arkansas, was a dentist. They owned a 200-acre estate near Pine Bluff, Arkansas with a profitable pig farm. In Pine Bluff, he and his siblings fished, hunted, and rode horses. In 1927, the family moved to East St. Louis, Illinois. They lived on the second floor of a commercial building behind a dental office in a predominantly white neighborhood. From 1932 to 1934, Davis attended John Robinson Elementary School, an all-black school, then Crispus Attucks, where he performed well in mathematics, music, and sports. At an early age he liked music, especially blues, big bands, and gospel.

In 1935, Davis received his first trumpet as a gift from John Eubanks, a friend of his father. He took lessons from Elwood Buchanan, a teacher and musician who was a patient of his father. His mother wanted him to play violin instead. Against the fashion of the time, Buchanan stressed the importance of playing without vibrato and encouraged him to use a clear, mid-range tone. Davis said that whenever he started playing with heavy vibrato, Buchanan slapped his knuckles. In later years Davis said, "I prefer a round sound with no attitude in it, like a round voice with not too much tremolo and not too much bass. Just right in the middle. If I can't get that sound I can't play anything." In 1939, the family moved to 1701 Kansas Avenue in East St. Louis. On his thirteenth birthday his father bought him a new trumpet, and Davis began to play in local bands. He took additional trumpet lessons from Joseph Gustat, principal trumpeter of the St. Louis Symphony Orchestra.

In 1941, the 15-year-old attended East St. Louis Lincoln High School, where he joined the marching band directed by Buchanan and entered music competitions. Years later, Davis said that if he lost a contest, it was because of racism, but he added that these experiences made him a better musician. When a drummer asked him to play a certain passage of music, and he couldn't do it, he began to learn music theory. "I went and got everything, every book I could get to learn about theory". At Lincoln, Davis met his first girlfriend, Irene Birth (later Cawthon). He had a band that performed at the Elks Club. Part of his earnings paid for his sister's education at Fisk University. Davis befriended trumpeter Clark Terry, who suggested he play without vibrato, and performed with him for several years.

With encouragement from his teacher and girlfriend, Davis filled a vacant spot in the Rhumboogie Orchestra, also known as the Blue Devils, led by Eddie Randle. He became the band's musical director, which involved hiring musicians and scheduling rehearsal. Years later, Davis considered this job one of the most important of his career. Sonny Stitt tried to persuade him to join the Tiny Bradshaw band, which was passing through town, but his mother insisted he finish high school before going on tour. He said later, "I didn't talk to her for two weeks. And I didn't go with the band either." In January 1944, Davis finished high school and graduated in absentia in June. During the next month, his girlfriend gave birth to a daughter, Cheryl.

In July 1944, Billy Eckstine visited St. Louis with a band that included Art Blakey, Dizzy Gillespie, and Charlie Parker. Trumpeter Buddy Anderson was too sick to perform, so Davis was invited to join. He played with the band for two weeks at Club Riviera. After playing with these musicians, he was certain he should move to New York City "where the action was". His mother wanted him to go to Fisk University, like his sister, and study piano or violin. Davis had other interests.

In September 1944, Davis accepted his father's idea of studying at the Institute of Musical Arts, later known as the Juilliard School, in New York City. After passing the audition, he attended classes in music theory, piano, and dictation.. But he spent much of his time in clubs looking for his idol, Charlie Parker, despite being advised against doing so by several people, including Coleman Hawkins. After finding Parker, he became one of a cadre of regulars at Minton's and Monroe's in Harlem who held jam sessions every night. The other regulars included J. J. Johnson, Kenny Clarke, Thelonious Monk, Fats Navarro, and Freddie Webster. Davis reunited with Cawthon and their daughter when they moved to New York City. Parker became a roommate.

In mid-1945, Davis failed to register for the year's autumn term at Juilliard and dropped out after three semesters because he wanted to perform full-time. Years later he criticized Juliard for concentrating too much on classical European and "white" repertoire, but he praised the school for teaching him music theory and improving his trumpet technique. 

He began performing at clubs on 52 Street with Coleman Hawkins and Eddie "Lockjaw" Davis. He recorded for the first time on April 24, 1945 when he entered the studio as a sideman for Herbie Fields's band. During the next year, he recorded as a leader for the first time with the Miles Davis Sextet plus Earl Coleman and Ann Hathaway, one of the few times he accompanied a singer.

In 1945, he replaced Dizzy Gillespie in Charlie Parker's quintet. On November 26, Davis participated in several recording sessions as part of Parker's group Reboppers that also involved Gillespie and Max Roach, displaying hints of the style he would become known for. While Parker's song "Now's the Time", Davis played a solo which anticipated cool jazz. He then joined a big band led by Benny Carter, performing in St. Louis and remaining with the band in California. He again played with Parker and Gillespie. In Los Angeles, Parker had a nervous breakdown which left him in the hospital for several months. In March 1946, Davis played in studio sessions with Parker and began a collaboration with bassist Charles Mingus that summer. Cawthon gave birth to Davis's second child, Gregory, in East St. Louis before reuniting with Davis in New York City the following year. Davis noted that by this time, "I was still so much into the music that I was even ignoring Irene". He had also turned to alcohol and cocaine.

He was a member of Billy Eckstine's big band in 1946 and Gillespie's in 1947. He joined a quintet led by Parker that also included Max Roach. Together they performed live with Duke Jordan and Tommy Potter for much of the year, including several studio sessions. In one session that May, Davis wrote the tune "Cheryl", named after his daughter. Davis's first session as a leader followed in August 1947, playing as the Miles Davis All Stars that included Parker, pianist John Lewis, and bassist Nelson Boyd; they recorded "Milestones", "Half Nelson", and "Sippin' at Bells". After touring Chicago and Detroit with Parker's quintet, Davis returned to New York City in March 1948 and joined the Jazz at the Philharmonic tour which included a stop in St. Louis on April 30.

In August 1948, Davis declined an offer to join Duke Ellington's orchestra as he had entered rehearsals with a nine-piece band with pianist and arranger Gil Evans and baritone saxophonist Gerry Mulligan, taking an active role that soon became his own project. Evans' Manhattan apartment had become the meeting place for several young musicians and composers such as Davis, Roach, Lewis, and Mulligan who were unhappy with the increasingly virtuoso instrumental techniques that dominated bebop. These gatherings led to the formation of the Miles Davis Nonet which included the unusual additions of French horn and tuba. The intent was to imitate the human voice through carefully arranged compositions and a relaxed, melodic approach to improvisation. In September, the band completed their sole engagement as the opening band for Count Basie at the Royal Roost for two weeks. Davis had to persuade the venue's manager to write the sign "Miles Davis Nonet. Arrangements by Gil Evans, John Lewis and Gerry Mulligan". He prevailed only with the help of Monte Kay, the club's artistic director. Davis returned to Parker's quintet, but relationships within the quintet were growing tense mainly due to Parker's erratic behavior caused by his drug addiction. Early into his time with Parker, Davis abstained from drugs, ate a vegetarian diet, and spoke of the benefits of water and juice. Matters worsened when Davis and Roach objected to the addition of pianist Duke Jordan and preferred to hire Bud Powell. In December 1948 Davis quit, claiming he was not being paid.

His departure began a period when he worked mainly as a freelancer and sideman. His nonet remained active until the end of 1949. After signing a contract with Capitol Records, they recorded sessions in January and April 1949 which sold little but influenced the "cool" or "west coast" style of jazz. The line-up changed throughout the year and included the additions of tuba player Bill Barber, alto saxophonist Lee Konitz, who had been preferred to Sonny Stitt as his style was considered too bop-oriented, pianist Al Haig, trombone players Mike Zwerin with Kai Winding, French horn players Junior Collins with Sandy Siegelstein and Gunther Schuller, and bassists Al McKibbon and Joe Shulman. One track featured singer Kenny Hagood. The presence of white musicians in the group angered some black players, many of whom were unemployed at the time, yet Davis rebuffed their criticisms. Recording sessions with the nonet for Capitol Records continued until April 1950; much of it remained unreleased until the issue of "Birth of the Cool" (1957), its name given to the cool jazz movement that had developed and the musical direction the group had taken.

In May 1949, Davis performed with the Tadd Dameron Quintet with Kenny Clarke and James Moody at the Paris International Jazz Festival. On his first trip abroad Davis took a strong liking for Paris and its cultural environment, where he felt black jazz musicians and people of color in general were better respected than in America. The trip, he said, "changed the way I looked at things forever". He began an affair with singer and actress Juliette Gréco which lasted for several years.

After returning from Paris in mid-1949, he became depressed and found little work, which included a short engagement with Powell in October and guest spots in New York City, Chicago, and Detroit until January 1950. He was falling behind in hotel rent and attempts were made to repossess his car. His heroin use became an expensive addiction, and Davis, yet to reach 24 years old, "lost my sense of discipline, lost my sense of control over my life, and started to drift". In August 1950, during a family trip to East St. Louis and Chicago in an attempt to improve their fortunes, Cawthon gave birth to Davis's second son, Miles IV. Davis befriended boxer Johnny Bratton and began his interest in the sport. Davis left Cawthon and his three children in New York City in the hands of a friend, jazz singer Betty Carter. He remained grateful to her for the rest of his life. He toured with Eckstine and Billie Holiday and was arrested for heroin possession in Los Angeles. The story was reported in "DownBeat" magazine, which caused a further reduction in work, though he was acquitted weeks later.

In January 1951, Davis's fortunes improved when he signed a one-year contract with Prestige Records after owner Bob Weinstock became a fan of the nonet. Davis chose Lewis, trombonist Bennie Green, bassist Percy Heath, saxophonist Sonny Rollins, and drummer Roy Haynes; they recorded what became part of "Miles Davis and Horns" (1956). Davis was hired for other studio dates in March, June, and September 1951 and started transcribing scores for record labels to fund his heroin addiction. During the next month, he recorded his second session for Prestige as band leader. The material was released on "The New Sounds" (1951), "Dig" (1956), and "Conception" (1956).

Davis supported his heroin habit by playing music and by living the life of a hustler, exploiting prostitutes, and receiving money from friends. By 1953, his addiction began to impair his playing. His drug habit became public in a "Down Beat" interview with Cab Calloway, whom he never forgave as it brought him "all pain and suffering". He returned to St. Louis and stayed with his father for several months. Although he continued to use heroin, he met Roach and Mingus in September 1953 on their way to Los Angeles and joined their band, but the trip caused problems. He returned to his father's home, "determined to kick my habit...that was the only thing on my mind". He locked himself inside the guest house "for about seven or eight days" until he had gone through withdrawal. After the ordeal, he "sat down and started thinking about how I was going to get my life back together".

Davis lived in Detroit for about six months, avoiding New York City where it was easy to get drugs. Although he used heroin, he was able to perform locally with Elvin Jones and Tommy Flanagan as part of Billy Mitchell's house band at the Blue Bird club. He was also "pimping a little". A widely related story, attributed to Richard "Prophet" Jennings, was that Davis stumbled into Baker's Keyboard Lounge out of the rain, carrying his trumpet in a paper bag under his coat. He walked to the bandstand, interrupted Roach and Clifford Brown in the middle of performing "Sweet Georgia Brown", and played "My Funny Valentine" before leaving. Davis was supposedly embarrassed into getting clean by this incident. He disputed this account, stating that Roach had invited him to play and that his decision to quit heroin was unrelated to the incident. He said he was inspired to quit by his idol, boxer Sugar Ray Robinson.

In February 1954 Davis returned to New York City, feeling good "for the first time in a long time", mentally and physically stronger, and joined a gym. He informed Weinstock and management at Blue Note Records that he was ready to record with a quintet, which he was granted. He considered the resulting albums "Miles Davis Quartet" (1954) and "Miles Davis Volume 2" (1956) "very important" because he felt his performances were particularly strong. He was paid roughly $750 (US$ in dollars) for each album and refused to give away his publishing rights.

Davis abandoned the bebop style and turned to the music of pianist Ahmad Jamal, whose approach and use of space influenced him. When he returned to the studio in June 1955 to record "Miles Davis Quartet", he wanted a pianist like Jamal and picked Red Garland. "Blue Haze" (1956), "Bags' Groove" (1957), "Walkin'" (1957), and "Miles Davis and the Modern Jazz Giants" (1959) were recorded after his recovery from heroin addiction. They documented the evolution of his sound with the Harmon mute, also known as a wah-wah mute, placed close to the microphone, and the use of more spacious and relaxed phrasing. He assumed a central role in hard bop, which was slower than bebop, less radical in harmony and melody, and often used popular songs and American standards as starting points for improvisation. Hard bop distanced itself from cool jazz with a harder beat and music inspired by the blues. A few critics consider "Walkin' " (1957) the album that created the hard bop genre.

Davis gained a reputation for being cold, distant—and easily angered. He wrote that in 1954 Sugar Ray Robinson "was the most important thing in my life besides music" and adopted Robinson's "arrogant attitude". He showed contempt for critics and the press. There were well-publicized confrontations with the public and with other musicians. An argument with Thelonious Monk during the recording of "Bags' Groove" was reported. In mid-1954, Davis reunited with Gréco for the first time since 1949 after she arrived in New York City for film prospects; the two had been in occasional contact since he left Paris. Although he was too busy to move to Spain with her, they "remained lovers for many years".

Davis had an operation to remove polyps from his larynx in October 1955. Although he was instructed to remain silent after the operation, he got into an argument that permanently damaged his vocal cords and gave him a raspy voice for the rest of his life. He was called the "prince of darkness", adding a patina of mystery to his public persona.

In July 1955, Davis's fortunes improved considerably when he was invited to the second annual Newport Jazz Festival on July 17, with a line-up of Monk, Heath, drummer Connie Kay, and horn players Zoot Sims and Gerry Mulligan. He convinced organizer George Wein that he should be on the bill, and Wein agreed. The performance was praised by critics and increased his popularity among affluent white audiences. He tied with Dizzy Gillespie for best trumpeter in the 1955 "Down Beat" magazine Readers' Poll. 

George Avakian of Columbia Records saw Davis perform at Newport and wanted to sign him to the label. Davis had one year left on his contract with Prestige which required him to release four more albums. He signed a contract with Columbia that included a $4,000 advance (US$ in dollars) and a condition that his recordings for Columbia would remain unreleased until his agreement with Prestige expired.

At the request of Avakian, he formed the Miles Davis Quintet for a performance at Café Bohemia. The quintet consisted of Davis on trumpet, Sonny Rollins on tenor saxophone, Red Garland on piano, Paul Chambers on double bass, and Philly Joe Jones on drums. This group appeared on his final albums for Prestige: "Cookin' with the Miles Davis Quintet" (1957), "Relaxin' with the Miles Davis Quintet" (1958), "Workin' with the Miles Davis Quintet" (1959), and "Steamin' with the Miles Davis Quintet" (1961). Each album helped establish Davis's quintet as one of the best. Sonny Rollins was replaced by John Coltrane, completing the membership of the first quintet.

The style of the group was an extension of their experience playing with Davis. He played long, legato, melodic lines, while Coltrane contrasted with energetic solos. Their live repertoire was a mix of bebop, standards from the Great American Songbook and pre-bop eras, and traditional tunes. They appeared on "'Round About Midnight", Davis's first album for Columbia.

In 1956, he left his quintet temporarily to tour Europe as part of the Birdland All-Stars, which included the Modern Jazz Quartet and French and German musicians. In Paris he reunited with Greco for the first time since 1949. He then returned home, reunited his quartet, and toured the US for two months. Conflict arose on tour as he grew impatient with the drug habits of Jones and Coltrane. Davis was trying to live a healthier life by exercising and reducing his alcohol. But he continued to use cocaine. At the end of the tour, he fired Jones and Coltrane and replaced them with Sonny Rollins and Art Taylor.

In November 1957, Davis went to Paris and recorded the soundtrack to "Ascenseur pour l'échafaud" directed by Louis Malle (1958). Consisting of French session musicians Barney Wilen, Pierre Michelot, and René Urtreger, and American drummer Kenny Clarke, the group avoided a written score and instead improvised while they watched the film in a recording studio.

After returning to New York City, Davis revived his quintet with Adderly and Coltrane, who was clean from his drug habit. Now a sextet, the group recorded material in early 1958 that was released on "Milestones" (1958), an album that demonstrated Davis's interest in modal jazz. A performance by Les Ballets Africains drew him to slower, deliberate music that allowed the creation of solos from harmony rather than chords. In this form of ballet music, the kalimba was played for a long periods on a single chord, weaving in and out of consonance and dissonance. 

By May 1958, he had replaced Jones with drummer Jimmy Cobb, and Red Garland left the group, leaving Davis to play piano on "Sid's Ahead" for the album "Milestones". He wanted someone who could play modal jazz, so he hired Bill Evans, a young, white pianist with a background in classical music. Evans had an impressionistic approach to piano. His ideas greatly influenced Davis. But after eight months of touring, a tired Evans left. Wynton Kelly, his replacement, brought to the group a swinging style that contrasted with Evans's delicacy. The sextet made their recording debut on "Jazz Track" (1958).

By early 1957, Davis was exhausted from recording and touring with his quintet and wished to pursue new projects. During a two-week residency in Chicago in March, the 30-year-old Davis told journalists of his intention to retire at its conclusion and revealed offers he had received to become a teacher at Harvard University and a musical director at a record label. Avakian agreed that it was time for Davis to explore something different, but Davis rejected his suggestion of returning to his nonet as he considered that a step backward. Avakian then suggested that he work with a bigger ensemble, similar to "Music for Brass" (1957), an album of orchestral and brass-arranged music led by Gunther Schuller featuring Davis as a guest soloist. 

Davis accepted and worked with Gil Evans in what became a five-album collaboration from 1957 to 1962. "Miles Ahead" (1957) showcased Davis playing a flugelhorn and a rendition "The Maids of Cadiz" by Léo Delibes, the first piece of classical music that Davis recorded. Evans devised orchestral passages as transitions, thus turning the album into one long piece of music. "Porgy and Bess" (1959) includes arrangements of pieces from George Gershwin's opera. "Sketches of Spain" (1960) contained music by composers Joaquín Rodrigo and Manuel de Falla and originals by Evans. The classical musicians had trouble improvising, while the jazz musicians couldn't handle the difficult arrangements, but the album was a critical success which sold over 120,000 copies in the US. Davis performed with an orchestra conducted by Evans at Carnegie Hall in May 1961 to raise money for charity. The pair's final album was "Quiet Nights" (1962), a collection of bossa nova songs released against their wishes. Evans stated it was only half an album and blamed the record company; Davis blamed producer Teo Macero and refused to speak to him for more than two years. Davis noted later that "my best friend is Gil Evans"; their work was included in the boxed set "" (1996) which won a Grammy Award for Best Historical Album and Best Album Notes in 1997.

In March and April 1959, Davis recorded what many critics consider his greatest album, "Kind of Blue" (1959). He named the album for its mood. He called back Bill Evans, as the music had been planned around Evans's piano style. Both Davis and Evans were familiar with George Russell's ideas about modal jazz. But Davis neglected to tell pianist Wynton Kelly that Evans was returning, so Kelly appeared on only one song, "Freddie Freeloader". The sextet had played "So What" and "All Blues" at performances, but the remaining three compositions they saw for the first time in the studio.

Released in August 1959, "Kind of Blue" was an instant success, with widespread radio airplay and rave reviews from critics. It remains the best selling jazz album of all time. In October 2008, the album reached 4× platinum certification from the Recording Industry Association of America for selling over four million copies in the US alone. In 2009, the US House of Representatives voted 409–0 to pass a resolution that honored it as a national treasure.

During the success of "Kind of Blue", Davis found himself involved with the law. On August 25, 1959, during a recording session at the Birdland nightclub in New York City for the US armed services, he took a break outside the club. As he was escorting a blonde-haired woman to a taxi, he was told by policeman Gerald Kilduff to "move on". Davis said that he was working at the club, and he refused to move. Kilduff arrested him and grabbed him as he tried to protect himself. Witnesses said the policeman punched Davis in the stomach with a nightstick without provocation. Two detectives held the crowd back, while a third approached Davis from behind and beat him in the head. Davis was taken to jail, charged for assaulting an officer, then taken to the hospital where he received five stitches. He was released on a $525 bail (US$ in dollars). By January 1960, he was acquitted of disorderly conduct and third-degree assault. He later stated the incident "changed my whole life and whole attitude again, made me feel bitter and cynical again when I was starting to feel good about the things that had changed in this country".

Davis and his sextet toured to support "Kind of Blue". He persuaded Coltrane to play with the group on one final European tour in the spring of 1960. Coltrane then departed to form his quartet, although he returned for some of the tracks on Davis's album "Someday My Prince Will Come" (1961). Its front cover shows a photograph of his wife, Frances Taylor, after Davis demanded that Columbia depict black women on his album covers. In early 1958, Davis began a relationship with Frances Taylor, a dancer he had met five years earlier in Los Angeles. They married on December 21, 1960. The relationship involved numerous incidents of Davis' domestic violence towards Taylor. He later wrote, "Every time I hit her, I felt bad because a lot of it really wasn't her fault but had to do with me being temperamental and jealous". One reason for his behavior was that in 1963 he had increased his use of alcohol and cocaine to reduce joint pain caused by sickle cell anemia. He hallucinated, "looking for this imaginary person" in his house while wielding a kitchen knife. About a week after the photograph for the album "E.S.P." (1965) was taken, Taylor left him for the last time. They remained separated until their divorce in February 1968.

In December 1962, Davis, Kelly, Chambers, Cobb, and Rollins played together for the last time as the first three wanted to leave and play as a trio. Rollins left to join them soon after, leaving Davis to pay over $25,000 (US$ in dollars) to cancel upcoming gigs and quickly assemble a new group. Following auditions, he found his new band in tenor saxophonist George Coleman, bassist Ron Carter, pianist Victor Feldman, and drummer Frank Butler. By May 1963, Feldman and Butler were replaced by pianist Herbie Hancock and 17-year-old drummer Tony Williams who made Davis "excited all over again". With this group, Davis completed the rest of what became "Seven Steps to Heaven" (1963) and recorded the live albums "Miles Davis in Europe" (1964), "My Funny Valentine" (1965), and "Four & More" (1966). The quintet played essentially the same bebop tunes and standards that Davis's previous bands had played, but they approached them with structural and rhythmic freedom and occasionally breakneck speed.

In 1964, Coleman was replaced by saxophonist Sam Rivers until Davis persuaded Wayne Shorter to leave Art Blakey. This quintet lasted through 1968. Shorter became the group's principal composer, and the album "E.S.P." (1965) was named after his composition. While touring Europe, the group made its first album, "Miles in Berlin" (1965).

Davis needed medical attention for hip pain, which had worsened since his Japanese tour during the previous year. He underwent hip replacement surgery in April 1965, with bone taken from his shin, but it failed. After his third month in the hospital, he discharged himself due to boredom and went home. He returned to the hospital in August after a fall required the insertion of a plastic hip joint. In November 1965, he had recovered enough to return to performing with his quintet, which included gigs at the Plugged Nickel in Chicago. Teo Macero returned as his engineer and record producer after their rift over "Quiet Nights" had healed.
In January 1966, Davis spent three months in the hospital due to a liver infection. When he resumed touring, he performed more at colleges because he had grown tired of the typical jazz venues. Columbia president Clive Davis noted that in 1966 his sales had declined to around 40,000–50,000 per album, compared to as many as 100,000 per release a few years before. Matters were not helped by the press reporting his apparent financial troubles and imminent demise. After his appearance at the 1966 Newport Jazz Festival, he returned to the studio with his quintet for a series of productive sessions. He started a relationship with actress Cicely Tyson, who helped him reduce his alcohol consumption.

Material from the 1966–1968 sessions was released on "Miles Smiles" (1966), "Sorcerer" (1967), "Nefertiti" (1967), "Miles in the Sky" (1968), and "Filles de Kilimanjaro" (1968). The quintet's approach to the new music became known as "time no changes", which referred to Davis's decision to depart from chordal sequences and adopt a more open approach, with the rhythm section responding to the soloists' melodies. Through "Nefertiti" the studio recordings consisted primarily of originals composed by Shorter, with occasional compositions by the other sidemen. In 1967, the group began to play their concerts in continuous sets, each tune flowing into the next, with only the melody indicating any sort of change. His bands performed this way until his hiatus in 1975.

"Miles in the Sky" and "Filles de Kilimanjaro"—which tentatively introduced electric bass, electric piano, and electric guitar on some tracks—pointed the way to the fusion phase of Davis's career. He also began experimenting with more rock-oriented rhythms on these records. By the time the second half of "Filles de Kilimanjaro" was recorded, bassist Dave Holland and pianist Chick Corea had replaced Carter and Hancock. Davis soon took over the compositional duties of his sidemen.

In September 1968, Davis married 23-year-old model and songwriter Betty Mabry. The marriage ended in divorce the following year, but Mabry, a familiar face in the New York City counterculture, helped introduce Davis to popular rock, soul, and funk musicians. Jazz critic Leonard Feather visited Davis's apartment and was shocked to find him listening to albums by The Byrds, Aretha Franklin, and Dionne Warwick. He also liked James Brown, Jimi Hendrix, and Sly and the Family Stone. 

"In a Silent Way" (1969) was recorded in a single studio session on February 18, 1969, with Shorter, Hancock, Holland, and Williams alongside keyboardists Chick Corea and Josef Zawinul and guitarist John McLaughlin. The album contains two side-long tracks that Macero pieced together from different takes recorded at the session resembling elements of a classical sonata form. When the album was released in July 1969, some critics accused him of "selling out" to the rock and roll audience. Nevertheless, it reached number 134 on the US "Billboard" Top LPs chart, his first album since "My Funny Valentine" to reach the chart. "In a Silent Way" was his entry into jazz fusion. The touring band of 1969–1970 which consisted of Shorter, Corea, Holland, and DeJohnette never completed a studio recording together and became known as the "lost quintet". In October 1969, Davis was shot at five times while in his car with one of his two lovers, Marguerite Eskridge. The incident left him with a graze and Eskridge unharmed.

For the double album "Bitches Brew" (1970), he hired Jack DeJohnette, Airto Moreira, and Bennie Maupin. The album contained long compositions, some over twenty minutes, that were never played in the studio but were constructed from several takes by Macero and Davis. Other studio techniques included splicing, multitrack recording, and tape loops. "Bitches Brew" peaked at No. 35 on the "Billboard" Album chart. In 1976 it was certified gold for selling over 500,000 records. By 2003, it had sold one million copies.
In March 1970, Davis began to perform as the opening act for various rock acts, allowing Columbia to market "Bitches Brew" to a larger audience. Although he was so offended by Clive Davis's suggestion to perform at the Fillmore East that he threatened to switch record labels, he reconsidered and shared a bill with the Steve Miller Band and Neil Young with Crazy Horse on March 6 and 7. Biographer Paul Tingen wrote, "Miles's newcomer status in this environment" led to "mixed audience reactions, often having to play for dramatically reduced fees, and enduring the 'sell-out' accusations from the jazz world", as well as being "attacked by sections of the black press for supposedly genuflecting to white culture". The 1970 tours included the 1970 Isle of Wight Festival on August 29 when he performed to an estimated 600,000 people, the largest of his career. Plans to record with Hendrix ended after the guitarist's death; his funeral was the last that Davis attended. Several live albums with a transitional sextet/septet including Corea, DeJohnette, Holland, Moreira, saxophonist Steve Grossman, and keyboardist Keith Jarrett were recorded during this period, including "Miles Davis at Fillmore" (1970) and "" (1973).
By 1971, Davis had signed a contract with Columbia that paid him $100,000 a year (US$ in dollars) for three years in addition to royalties. He recorded the soundtrack for the 1970 documentary film about heavyweight boxer Jack Johnson, containing two long pieces of 25 and 26 minutes in length with Hancock, McLaughlin, Sonny Sharrock, and Billy Cobham. He was committed to making music for African-Americans who liked more commercial, pop, groove-oriented music. By November 1971, DeJohnette and Moreira had been replaced in the touring ensemble by drummer Leon "Ndugu" Chancler and percussionists James Mtume and Don Alias. "Live-Evil" (1971) was released in the same month. Showcasing former Stevie Wonder touring bassist Michael Henderson, who replaced Holland in the autumn of 1970, the album demonstrated that Davis's ensemble had transformed into a funk-oriented group while retaining the exploratory imperative of "Bitches Brew".

In 1972, composer-arranger Paul Buckmaster introduced Davis to the music of German avant-garde composer Karlheinz Stockhausen, leading to a period of creative exploration. Biographer J. K. Chambers wrote, "The effect of Davis' study of Stockhausen could not be repressed for long ... Davis' own 'space music' shows Stockhausen's influence compositionally." His recordings and performances during this period were described as "space music" by fans, Feather, and Buckmaster, who described it as "a lot of mood changes—heavy, dark, intense—definitely space music". The studio album "On the Corner" (1972) blended the influence of Stockhausen and Buckmaster with funk elements. Davis invited Buckmaster to New York City to oversee the writing and recording of the album with Macero. The album reached No. 1 on the "Billboard" jazz chart but peaked at No. 156 on the more heterogeneous Top 200 Albums chart. Although "On the Corner" elicited favorable reviews from Ralph J. Gleason of "Rolling Stone" and Robert Christgau of "The Village Voice", Davis felt that Columbia marketed it to the wrong audience. "The music was meant to be heard by young black people, but they just treated it like any other jazz album and advertised it that way, pushed it on the jazz radio stations. Young black kids don't listen to those stations; they listen to R&B stations and some rock stations." In October 1972, he broke his ankles in a car crash. He took painkillers and cocaine to cope with the pain. Looking back at his career after the incident, he wrote, "everything started to blur".
After recording "On the Corner", he assembled a group with Henderson, Mtume, Carlos Garnett, guitarist Reggie Lucas, organist Lonnie Liston Smith, tabla player Badal Roy, sitarist Khalil Balakrishna, and drummer Al Foster. Only Smith was a jazz instrumentalist; consequently, the music emphasized rhythmic density and shifting textures instead of solos. This group was recorded live for "In Concert" (1973), but Davis found it unsatisfactory, leading him to drop the tabla and sitar and play keyboards. He also added guitarist Pete Cosey. The compilation studio album "Big Fun" (1974) contains four long improvisations recorded between 1969 and 1972.

Studio activity in the 1970s culminated in sessions throughout 1973 and 1974 for "Get Up with It" (1974), a compilation that included four long pieces (comprising over ninety minutes of new music) alongside four shorter recordings from 1970 and 1972. The album contained "He Loved Him Madly", a thirty-minute tribute to the recently deceased Duke Ellington that presaged later developments in ambient music. In the United States, it performed comparably to "On the Corner", reaching number 8 on the jazz chart and number 141 on the pop chart. He then concentrated on live performance with a series of concerts that Columbia released on the double live albums "Agharta" (1975), "Pangaea" (1976), and "Dark Magus" (1977). The first two are recordings of two sets from February 1, 1975 in Osaka, by which time Davis was troubled by pneumonia, osteoarthritis, sickle-cell anemia, depression, bursitis, and stomach ulcers; he relied on alcohol, codeine, and morphine to get through the engagements. His shows were routinely panned by critics who mentioned his habit of performing with his back to the audience. Although Cosey later asserted that "the band really advanced after the Japanese tour", Davis was again hospitalized for his ulcers and a hernia during a tour of the US while opening for Herbie Hancock. Hancock had eclipsed his former employer from a commercial standpoint with "Head Hunters" (1973) and "Thrust" (1974), two albums that were marketed to pop audiences in the aftermath of the "On the Corner" farrago and peaked at number 13 on the "Billboard" pop chart.

After appearances at the 1975 Newport Jazz Festival in July and the Schaefer Music Festival in New York City on September 5, Davis dropped out of music.

In his autobiography, Davis wrote frankly about his life during his hiatus from music. He called his Upper West Side brownstone a wreck and chronicled his heavy use of alcohol and cocaine, in addition to his sexual encounters with many women. In December 1975, he had regained enough strength to undergo a much needed hip replacement operation. In March 1976, "Rolling Stone" reported rumors of his imminent demise, citing his health problems during the previous tour. In December 1976, Columbia was reluctant to renew his contract and pay his usual large advances. But after his lawyer started negotiating with United Artists, Columbia matched their offer, establishing the Miles Davis Fund to pay him regularly. Pianist Vladimir Horowitz was the only other musician with Columbia that had a similar status. 

In 1978, Julie Coryell interviewed Davis. Concerned about his health, she had him stay with a friend in Norwalk, Connecticut. Davis asked Coryell's husband, fusion guitarist Larry Coryell, to participate in sessions with keyboardists Masabumi Kikuchi and George Pavlis, bassist T. M. Stevens, and drummer Al Foster. Davis played the arranged piece uptempo, abandoned his trumpet for the organ, and had Macero record the session without the band's knowledge. After Coryell declined a spot in a band that Davis was beginning to put together, Davis returned to his reclusive lifestyle in New York City. Soon after, Eskridge had Davis jailed for failing to pay child support to their son, which cost him $10,000 (US$ in dollars) for release on bail. A recording session that involved Buckmaster and Gil Evans was halted, with Evans leaving after failing to receive the payment he was promised. In August 1978, Davis hired a new manager, Mark Rothbaum, who had worked with him since 1972. Despite the dearth of new material, Davis placed in the Top 10 trumpeter poll of "Down Beat" magazine in 1979.
By 1979, Davis had rekindled his relationship with actress Cicely Tyson, with whom he overcame his cocaine addiction and regained his enthusiasm for music. The two married on November 26, 1981, in a ceremony at Bill Cosby's home in Massachusetts that was officiated by politician and civil rights activist Andrew Young. The marriage ended in a divorce that was finalized in 1989.

In October 1979, his contract with Columbia was up for negotiation. Label president Clive Davis was replaced by George Butler, who had visited Davis several times during the previous two years to encourage him to return to the studio. To help his situation, Davis had Buckmaster come over to collaborate on new music. After arriving, Buckmaster organized an intervention for Davis, who was living in squalor among cockroach infestations, in the dark with his curtains always closed. His sister Dorothy cleaned his house with help from Buckmaster, Tyson, and neighbor Chaka Khan. Davis later thanked Buckmaster for helping him.

Davis hadn't played the trumpet much for three years and found it difficult to reclaim his embouchure. His first studio appearance since his hiatus took place on May 1, 1980. A day later, Davis was hospitalized due to a leg infection. He recorded "The Man with the Horn" (1981) from June 1980 to May 1981 with Macero producing. A large band was abandoned in favor of a combo with saxophonist Bill Evans (not to be confused with pianist Bill Evans) and bassist Marcus Miller. Both would collaborate with him during the next decade.

"The Man with the Horn" received a poor critical reception despite selling well. In June 1981, Davis returned to the stage for the first time since 1975 in a ten-minute guest solo as part of Mel Lewis's band at the Village Vanguard. This was followed by appearances with a new band, a four-night run at Kix in Boston, and two shows at Avery Fisher Hall on July 5 as part of the Kool Jazz Festival. Recordings from a mixture of dates from 1981, including the Kix and Avery Fisher Hall gigs, were released on "We Want Miles" (1982), which earned him a Grammy Award for Best Jazz Instrumental Performance by a Soloist.
In January 1982, while Tyson was working in Africa, Davis "went a little wild" with alcohol and had a stroke which temporarily paralyzed his right hand. Tyson returned home and cared for him. After three months of treatment with a Chinese acupuncturist, he was able to play the trumpet. He listened to his doctor's warnings and began a life without alcohol and drugs. He credited Tyson with helping his recovery, which involved exercise, piano playing, and visits to spas. She encouraged him to draw, which he pursued for the rest of his life.

Davis resumed touring in May 1982 with a line-up that included French percussionist Mino Cinelu and guitarist John Scofield, with whom he worked closely on the album "Star People" (1983). In mid-1983, he worked on the tracks for "Decoy", an album mixing soul music and electronica that was released in 1984. He brought in producer, composer, and keyboardist Robert Irving III, who had collaborated with him on "The Man with the Horn". With a seven-piece band that included Scofield, Evans, Irving, Foster, and Darryl Jones, he played a series of European performances that were positively received. In December 1984, while in Denmark, he was awarded the Léonie Sonning Music Prize. Trumpeter Palle Mikkelborg had written a contemporary classical piece titled "Aura" for the event which impressed Davis to the point of returning to Denmark in early 1985 to record his next studio album, "Aura" (1989). Columbia was dissatisfied with the recording and delayed its release.

In May 1985, one month into a tour, Davis signed a contract with Warner Bros. Records which required him to give up his publishing rights. Trumpeter Wynton Marsalis publicly dismissed his more recent fusion recordings as not being "'true' jazz". Davis shrugged off the comment, calling Marsalis "a nice young man, only confused". Marsalis appeared unannounced onstage during Davis's performance at the inaugural Vancouver International Jazz Festival in 1986. Marsalis whispered into Davis' ear that "someone" had told him to do so. Davis responded by ordering him off the stage. Davis had become increasingly irritated at Columbia's delay in releasing "Aura". The breaking point appears to have come when a producer at Columbia asked him to call Marsalis and wish him a happy birthday. The tour in 1985 included a performance in London in July in which Davis performed on stage for five hours. Jazz critic John Fordham concluded, "The leader is clearly enjoying himself." By 1985, Davis was diabetic and required daily injections of insulin.

He released "You're Under Arrest", his final album for Columbia, in September 1985. It included cover versions of two pop songs: "Time After Time" by Cyndi Lauper and "Human Nature" sung by Michael Jackson. He considered releasing an album of pop songs, and he recorded dozens of them, but the idea was rejected. He said that many of today's jazz standards had been pop songs in Broadway theater and that he was simply updating the standards repertoire.

Davis collaborated with a number of figures from the British post-punk and new wave movements during this period, including Scritti Politti. At the invitation of producer Bill Laswell, he recorded some trumpet parts during sessions for Public Image Ltd.'s "Album", according to John Lydon in the liner notes of their "Plastic Box" box set. In Lydon's words, however, "strangely enough, we didn't use [his contributions]". According to Lydon in the "Plastic Box" notes, Davis favorably compared Lydon's singing voice to his trumpet sound during these sessions.

After taking part in the recording of the 1985 protest song "Sun City" as a member of Artists United Against Apartheid, Davis appeared on the instrumental "Don't Stop Me Now" by Toto for their album "Fahrenheit" (1986). For his next studio album, he intended to collaborate with Prince, but the project was dropped. Instead, he worked with Marcus Miller. The result, "Tutu" (1986), was the first time he used modern studio tools such as programmed synthesizers, sampling, and drum loops. Released in September 1986, its front cover is a portrait of Davis by Irving Penn. The album was called the modern counterpart of "Sketches of Spain". In 1987, he won a Grammy Award for Best Jazz Instrumental Performance, Soloist.

In 1988, Davis had a small part as a street musician in the Christmas comedy film "Scrooged" starring Bill Murray. He also collaborated with Zucchero Fornaciari in a version of "Dune Mosse" ("Blue's"), published in 2004 in "Zu & Co." of the Italian bluesman. In November 1988, he was inducted into the Knights of Malta at a ceremony at the Alhambra Palace in Spain. Later that month, Davis cut his European tour short after he collapsed and fainted after a two-hour show in Madrid and flew home. Rumors of his health were made public after the American magazine "Star", in its February 21, 1989 edition, published that Davis had contracted AIDS, prompting his manager Peter Shukat to issue a statement the following day to deny the claim. Shukat revealed Davis had been in the hospital for a mild case of pneumonia and the removal of a benign polyp on his vocal cords and was resting comfortably in preparation for his 1989 tours. Davis later blamed one of his former wives or girlfriends for starting the rumor and decided against taking legal action. He was interviewed on "60 Minutes" by Harry Reasoner. In October 1989, he received a Grande Medaille de Vermeil from Paris mayor Jacques Chirac. In 1990, he received a Grammy Lifetime Achievement Award. In early 1991, he appeared in the Rolf de Heer film "Dingo" as a jazz musician.
Davis followed "Tutu" with "Amandla" (1989) and soundtracks to four films: "Street Smart", "Siesta", "The Hot Spot", and "Dingo." His last albums were released posthumously: the hip hop-influenced "Doo-Bop" (1992) and "Miles & Quincy Live at Montreux" (1993), a collaboration with Quincy Jones from the 1991 Montreux Jazz Festival where, for the first time in three decades, he performed songs from "Miles Ahead", "Porgy and Bess", and "Sketches of Spain".

On July 8, 1991, Davis returned to performing material from his past at the 1991 Montreux Jazz Festival with a band and orchestra conducted by Quincy Jones. The set consisted of arrangements from his albums recorded with Gil Evans. The show was followed by a concert billed as "Miles and Friends" at the Grande halle de la Villette in Paris two days later, with guest performances by musicians from throughout his career, including John McLaughlin, Herbie Hancock, and Joe Zawinul. In Paris, he was awarded the Chevalier of the Legion of Honor. After returning to America, he stopped in New York City to record material for "Doo-Bop", then returned to California to play at the Hollywood Bowl on August 25, his final live performance.

In early September 1991, Davis checked into St. John's Hospital near his home in Santa Monica, California, for routine tests. Doctors suggested he have a tracheal tube implanted to relieve his breathing after repeated bouts of bronchial pneumonia. Their suggestion provoked an outburst from Davis that led to an intracerebral hemorrhage followed by a coma. After several days on life support, his machine was turned off and he died on September 28, 1991. He was 65 years old. His death was attributed to the combined effects of a stroke, pneumonia, and respiratory failure. According to Troupe, Davis was taking azidothymidine (AZT), a type of antiretroviral drug used for the treatment of HIV and AIDS, during his treatments in hospital. A funeral service was held on October 5, 1991, at St. Peter's Church in New York City that was attended by around 500 friends, family members, and musical acquaintances, with many fans standing in the rain. He was buried in Woodlawn Cemetery in The Bronx, New York City, with one of his trumpets, near the site of his idol, Duke Ellington.

Late in his life, from the "electric period" onwards, Davis repeatedly explained his reasons for not wishing to perform his earlier works, such as "Birth of the Cool" or "Kind of Blue". In his view, remaining stylistically static was the wrong option. He commented: So What' or "Kind of Blue", they were done in that era, the right hour, the right day, and it happened. It's over ... What I used to play with Bill Evans, all those different modes, and substitute chords, we had the energy then and we liked it. But I have no feel for it anymore, it's more like warmed-over turkey." When Shirley Horn insisted in 1990 that Miles reconsider playing the ballads and modal tunes of his "Kind of Blue" period, he demurred. "Nah, it hurts my lip", was the reason he gave.

Other musicians regretted Davis' change of style. Bill Evans, who played piano on "Kind of Blue", said, "I would like to hear more of the consummate melodic master, but I feel that big business and his record company have had a corrupting influence on his material. The rock and pop thing certainly draws a wider audience. It happens more and more these days that unqualified people with executive positions try to tell musicians what is good and what is bad music."

Miles Davis is considered one of the most innovative, influential, and respected figures in the history of music. "The Guardian" described him as "a pioneer of 20th-century music, leading many of the key developments in the world of jazz". He has been called "one of the great innovators in jazz", and had the titles the Prince of Darkness and the Picasso of Jazz bestowed upon him. "The Rolling Stone Encyclopedia of Rock & Roll" said, "Miles Davis played a crucial and inevitably controversial role in every major development in jazz since the mid-'40s, and no other jazz musician has had so profound an effect on rock. Miles Davis was the most widely recognized jazz musician of his era, an outspoken social critic and an arbiter of style—in attitude and fashion—as well as music".

William Ruhlmann of AllMusic wrote, "To examine his career is to examine the history of jazz from the mid-1940s to the early 1990s, since he was in the thick of almost every important innovation and stylistic development in the music during that period ... It can even be argued that jazz stopped evolving when Davis wasn't there to push it forward." Music writer Christopher Smith wrote,

Miles Davis's artistic interest was in the creation and manipulation of ritual space, in which gestures could be endowed with symbolic power sufficient to form a functional communicative, and hence musical, vocabulary. ... Miles' performance tradition emphasized orality and the transmission of information and artistic insight from individual to individual. His position in that tradition, and his personality, talents, and artistic interests, impelled him to pursue a uniquely individual solution to the problems and the experiential possibilities of improvised performance.

His approach, owing largely to the African-American performance tradition that focused on individual expression, emphatic interaction, and creative response to shifting contents, had a profound impact on generations of jazz musicians.

"Kind of Blue" remains the best-selling jazz album of all time. On November 5, 2009, U.S. Representative John Conyers of Michigan sponsored a measure in the United States House of Representatives to commemorate the album on its 50th anniversary. The measure also affirms jazz as a national treasure and "encourages the United States government to preserve and advance the art form of jazz music". It passed with a vote of 409–0 on December 15, 2009. The trumpet Davis used on the recording is displayed on the campus of the University of North Carolina at Greensboro. It was donated to the school by Arthur "Buddy" Gist, who met Davis in 1949 and became a close friend. The gift was the reason why the jazz program at UNCG is named the Miles Davis Jazz Studies Program.

In 1986, the New England Conservatory awarded Davis an Honorary Doctorate for his contributions to music. Since 1960 the National Academy of Recording Arts and Sciences (NARAS) honored him with eight Grammy Awards, a Grammy Lifetime Achievement Award, and three Grammy Hall of Fame Awards.

In 2001 a two-hour documentary film by Mike Dibb entitled "The Miles Davis Story" won an International Emmy Award for arts documentary of the year.

"Miles Ahead" was a 2015 American music film directed by Don Cheadle, co-written by Cheadle with Steven Baigelman, Stephen J. Rivele, and Christopher Wilkinson, which interprets the life and compositions of Davis. It premiered at the New York Film Festival in October 2015. The stars Cheadle, Emayatzy Corinealdi as Frances Taylor, Ewan McGregor, Michael Stuhlbarg, and Lakeith Stanfield.

Grammy Awards

Other awards




</doc>
<doc id="20406" url="https://en.wikipedia.org/wiki?curid=20406" title="M-theory">
M-theory

M-theory is a theory in physics that unifies all consistent versions of superstring theory. The existence of such a theory was first conjectured by Edward Witten at a string theory conference at the University of Southern California in the spring of 1995. Witten's announcement initiated a flurry of research activity known as the second superstring revolution.

Prior to Witten's announcement, string theorists had identified five versions of superstring theory. Although these theories appeared, at first, to be very different, work by several physicists showed that the theories were related in intricate and nontrivial ways. In particular, physicists found that apparently distinct theories could be unified by mathematical transformations called S-duality and T-duality. Witten's conjecture was based in part on the existence of these dualities and in part on the relationship of the string theories to a field theory called eleven-dimensional supergravity.

Although a complete formulation of M-theory is not known, the theory should describe two- and five-dimensional objects called branes and should be approximated by eleven-dimensional supergravity at low energies. Modern attempts to formulate M-theory are typically based on matrix theory or the AdS/CFT correspondence.

According to Witten, M should stand for “magic”, “mystery”, or “membrane” according to taste, and the true meaning of the title should be decided when a more fundamental formulation of the theory is known.

Investigations of the mathematical structure of M-theory have spawned important theoretical results in physics and mathematics. More speculatively, M-theory may provide a framework for developing a unified theory of all of the fundamental forces of nature. Attempts to connect M-theory to experiment typically focus on compactifying its extra dimensions to construct candidate models of our four-dimensional world, although so far none has been verified to give rise to physics as observed in high energy physics experiments.

One of the deepest problems in modern physics is the problem of quantum gravity. The current understanding of gravity is based on Albert Einstein's general theory of relativity, which is formulated within the framework of classical physics. However, nongravitational forces are described within the framework of quantum mechanics, a radically different formalism for describing physical phenomena based on probability. A quantum theory of gravity is needed in order to reconcile general relativity with the principles of quantum mechanics, but difficulties arise when one attempts to apply the usual prescriptions of quantum theory to the force of gravity.

String theory is a theoretical framework that attempts to reconcile gravity and quantum mechanics. In string theory, the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory describes how strings propagate through space and interact with each other. In a given version of string theory, there is only one kind of string, which may look like a small loop or segment of ordinary string, and it can vibrate in different ways. On distance scales larger than the string scale, a string will look just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In this way, all of the different elementary particles may be viewed as vibrating strings. One of the vibrational states of a string gives rise to the graviton, a quantum mechanical particle that carries gravitational force.

There are several versions of string theory: type I, type IIA, type IIB, and two flavors of heterotic string theory ( and ). The different theories allow different types of strings, and the particles that arise at low energies exhibit different symmetries. For example, the type I theory includes both open strings (which are segments with endpoints) and closed strings (which form closed loops), while types IIA and IIB include only closed strings. Each of these five string theories arises as a special limiting case of M-theory. This theory, like its string theory predecessors, is an example of a quantum theory of gravity. It describes a force just like the familiar gravitational force subject to the rules of quantum mechanics.

In everyday life, there are three familiar dimensions of space: height, width and depth. Einstein's general theory of relativity treats time as a dimension on par with the three spatial dimensions; in general relativity, space and time are not modeled as separate entities but are instead unified to a four-dimensional spacetime, three spatial dimensions and one time dimension. In this framework, the phenomenon of gravity is viewed as a consequence of the geometry of spacetime.

In spite of the fact that the universe is well described by four-dimensional spacetime, there are several reasons why physicists consider theories in other dimensions. In some cases, by modeling spacetime in a different number of dimensions, a theory becomes more mathematically tractable, and one can perform calculations and gain general insights more easily. There are also situations where theories in two or three spacetime dimensions are useful for describing phenomena in condensed matter physics. Finally, there exist scenarios in which there could actually be more than four dimensions of spacetime which have nonetheless managed to escape detection.

One notable feature of string theory and M-theory is that these theories require extra dimensions of spacetime for their mathematical consistency. In string theory, spacetime is "ten-dimensional" (nine spatial dimensions, and one time dimension), while in M-theory it is "eleven-dimensional" (ten spatial dimensions, and one time dimension). In order to describe real physical phenomena using these theories, one must therefore imagine scenarios in which these extra dimensions would not be observed in experiments.

Compactification is one way of modifying the number of dimensions in a physical theory. In compactification, some of the extra dimensions are assumed to "close up" on themselves to form circles. In the limit where these curled up dimensions become very small, one obtains a theory in which spacetime has effectively a lower number of dimensions. A standard analogy for this is to consider a multidimensional object such as a garden hose. If the hose is viewed from a sufficient distance, it appears to have only one dimension, its length. However, as one approaches the hose, one discovers that it contains a second dimension, its circumference. Thus, an ant crawling on the surface of the hose would move in two dimensions.

Theories that arise as different limits of M-theory turn out to be related in highly nontrivial ways. One of the relationships that can exist between these different physical theories is called S-duality. This is a relationship which says that a collection of strongly interacting particles in one theory can, in some cases, be viewed as a collection of weakly interacting particles in a completely different theory. Roughly speaking, a collection of particles is said to be strongly interacting if they combine and decay often and weakly interacting if they do so infrequently. Type I string theory turns out to be equivalent by S-duality to the heterotic string theory. Similarly, type IIB string theory is related to itself in a nontrivial way by S-duality.

Another relationship between different string theories is T-duality. Here one considers strings propagating around a circular extra dimension. T-duality states that a string propagating around a circle of radius is equivalent to a string propagating around a circle of radius in the sense that all observable quantities in one description are identified with quantities in the dual description. For example, a string has momentum as it propagates around a circle, and it can also wind around the circle one or more times. The number of times the string winds around a circle is called the winding number. If a string has momentum and winding number in one description, it will have momentum and winding number in the dual description. For example, type IIA string theory is equivalent to type IIB string theory via T-duality, and the two versions of heterotic string theory are also related by T-duality.

In general, the term "duality" refers to a situation where two seemingly different physical systems turn out to be equivalent in a nontrivial way. If two theories are related by a duality, it means that one theory can be transformed in some way so that it ends up looking just like the other theory. The two theories are then said to be "dual" to one another under the transformation. Put differently, the two theories are mathematically different descriptions of the same phenomena.

Another important theoretical idea that plays a role in M-theory is supersymmetry. This is a mathematical relation that exists in certain physical theories between a class of particles called bosons and a class of particles called fermions. Roughly speaking, fermions are the constituents of matter, while bosons mediate interactions between particles. In theories with supersymmetry, each boson has a counterpart which is a fermion, and vice versa. When supersymmetry is imposed as a local symmetry, one automatically obtains a quantum mechanical theory that includes gravity. Such a theory is called a supergravity theory.

A theory of strings that incorporates the idea of supersymmetry is called a superstring theory. There are several different versions of superstring theory which are all subsumed within the M-theory framework. At low energies, the superstring theories are approximated by supergravity in ten spacetime dimensions. Similarly, M-theory is approximated at low energies by supergravity in eleven dimensions.

In string theory and related theories such as supergravity theories, a brane is a physical object that generalizes the notion of a point particle to higher dimensions. For example, a point particle can be viewed as a brane of dimension zero, while a string can be viewed as a brane of dimension one. It is also possible to consider higher-dimensional branes. In dimension , these are called -branes. Branes are dynamical objects which can propagate through spacetime according to the rules of quantum mechanics. They can have mass and other attributes such as charge. A -brane sweeps out a -dimensional volume in spacetime called its "worldvolume". Physicists often study fields analogous to the electromagnetic field which live on the worldvolume of a brane. The word brane comes from the word "membrane" which refers to a two-dimensional brane.

In string theory, the fundamental objects that give rise to elementary particles are the one-dimensional strings. Although the physical phenomena described by M-theory are still poorly understood, physicists know that the theory describes two- and five-dimensional branes. Much of the current research in M-theory attempts to better understand the properties of these branes.

In the early 20th century, physicists and mathematicians including Albert Einstein and Hermann Minkowski pioneered the use of four-dimensional geometry for describing the physical world. These efforts culminated in the formulation of Einstein's general theory of relativity, which relates gravity to the geometry of four-dimensional spacetime.

The success of general relativity led to efforts to apply higher dimensional geometry to explain other forces. In 1919, work by Theodor Kaluza showed that by passing to five-dimensional spacetime, one can unify gravity and electromagnetism into a single force. This idea was improved by physicist Oskar Klein, who suggested that the additional dimension proposed by Kaluza could take the form of a circle with radius around cm.

The Kaluza–Klein theory and subsequent attempts by Einstein to develop unified field theory were never completely successful. In part this was because Kaluza–Klein theory predicted a particle that has never been shown to exist, and in part because it was unable to correctly predict the ratio of an electron's mass to its charge. In addition, these theories were being developed just as other physicists were beginning to discover quantum mechanics, which would ultimately prove successful in describing known forces such as electromagnetism, as well as new nuclear forces that were being discovered throughout the middle part of the century. Thus it would take almost fifty years for the idea of new dimensions to be taken seriously again.

New concepts and mathematical tools provided fresh insights into general relativity, giving rise to a period in the 1960s–70s now known as the golden age of general relativity. In the mid-1970s, physicists began studying higher-dimensional theories combining general relativity with supersymmetry, the so-called supergravity theories.

General relativity does not place any limits on the possible dimensions of spacetime. Although the theory is typically formulated in four dimensions, one can write down the same equations for the gravitational field in any number of dimensions. Supergravity is more restrictive because it places an upper limit on the number of dimensions. In 1978, work by Werner Nahm showed that the maximum spacetime dimension in which one can formulate a consistent supersymmetric theory is eleven. In the same year, Eugene Cremmer, Bernard Julia, and Joel Scherk of the École Normale Supérieure showed that supergravity not only permits up to eleven dimensions but is in fact most elegant in this maximal number of dimensions.

Initially, many physicists hoped that by compactifying eleven-dimensional supergravity, it might be possible to construct realistic models of our four-dimensional world. The hope was that such models would provide a unified description of the four fundamental forces of nature: electromagnetism, the strong and weak nuclear forces, and gravity. Interest in eleven-dimensional supergravity soon waned as various flaws in this scheme were discovered. One of the problems was that the laws of physics appear to distinguish between clockwise and counterclockwise, a phenomenon known as chirality. Edward Witten and others observed this chirality property cannot be readily derived by compactifying from eleven dimensions.

In the first superstring revolution in 1984, many physicists turned to string theory as a unified theory of particle physics and quantum gravity. Unlike supergravity theory, string theory was able to accommodate the chirality of the standard model, and it provided a theory of gravity consistent with quantum effects. Another feature of string theory that many physicists were drawn to in the 1980s and 1990s was its high degree of uniqueness. In ordinary particle theories, one can consider any collection of elementary particles whose classical behavior is described by an arbitrary Lagrangian. In string theory, the possibilities are much more constrained: by the 1990s, physicists had argued that there were only five consistent supersymmetric versions of the theory.

Although there were only a handful of consistent superstring theories, it remained a mystery why there was not just one consistent formulation. However, as physicists began to examine string theory more closely, they realized that these theories are related in intricate and nontrivial ways.

In the late 1970s, Claus Montonen and David Olive had conjectured a special property of certain physical theories. A sharpened version of their conjecture concerns a theory called supersymmetric Yang–Mills theory, which describes theoretical particles formally similar to the quarks and gluons that make up atomic nuclei. The strength with which the particles of this theory interact is measured by a number called the coupling constant. The result of Montonen and Olive, now known as Montonen–Olive duality, states that supersymmetric Yang–Mills theory with coupling constant is equivalent to the same theory with coupling constant . In other words, a system of strongly interacting particles (large coupling constant) has an equivalent description as a system of weakly interacting particles (small coupling constant) and vice versa.

In the 1990s, several theorists generalized Montonen–Olive duality to the S-duality relationship, which connects different string theories. Ashoke Sen studied S-duality in the context of heterotic strings in four dimensions. Chris Hull and Paul Townsend showed that type IIB string theory with a large coupling constant is equivalent via S-duality to the same theory with small coupling constant. Theorists also found that different string theories may be related by T-duality. This duality implies that strings propagating on completely different spacetime geometries may be physically equivalent.

String theory extends ordinary particle physics by replacing zero-dimensional point particles by one-dimensional objects called strings. In the late 1980s, it was natural for theorists to attempt to formulate other extensions in which particles are replaced by two-dimensional supermembranes or by higher-dimensional objects called branes. Such objects had been considered as early as 1962 by Paul Dirac, and they were reconsidered by a small but enthusiastic group of physicists in the 1980s.

Supersymmetry severely restricts the possible number of dimensions of a brane. In 1987, Eric Bergshoeff, Ergin Sezgin, and Paul Townsend showed that eleven-dimensional supergravity includes two-dimensional branes. Intuitively, these objects look like sheets or membranes propagating through the eleven-dimensional spacetime. Shortly after this discovery, Michael Duff, Paul Howe, Takeo Inami, and Kellogg Stelle considered a particular compactification of eleven-dimensional supergravity with one of the dimensions curled up into a circle. In this setting, one can imagine the membrane wrapping around the circular dimension. If the radius of the circle is sufficiently small, then this membrane looks just like a string in ten-dimensional spacetime. In fact, Duff and his collaborators showed that this construction reproduces exactly the strings appearing in type IIA superstring theory.

In 1990, Andrew Strominger published a similar result which suggested that strongly interacting strings in ten dimensions might have an equivalent description in terms of weakly interacting five-dimensional branes. Initially, physicists were unable to prove this relationship for two important reasons. On the one hand, the Montonen–Olive duality was still unproven, and so Strominger's conjecture was even more tenuous. On the other hand, there were many technical issues related to the quantum properties of five-dimensional branes. The first of these problems was solved in 1993 when Ashoke Sen established that certain physical theories require the existence of objects with both electric and magnetic charge which were predicted by the work of Montonen and Olive.

In spite of this progress, the relationship between strings and five-dimensional branes remained conjectural because theorists were unable to quantize the branes. Starting in 1991, a team of researchers including Michael Duff, Ramzi Khuri, Jianxin Lu, and Ruben Minasian considered a special compactification of string theory in which four of the ten dimensions curl up. If one considers a five-dimensional brane wrapped around these extra dimensions, then the brane looks just like a one-dimensional string. In this way, the conjectured relationship between strings and branes was reduced to a relationship between strings and strings, and the latter could be tested using already established theoretical techniques.

Speaking at the string theory conference at the University of Southern California in 1995, Edward Witten of the Institute for Advanced Study made the surprising suggestion that all five superstring theories were in fact just different limiting cases of a single theory in eleven spacetime dimensions. Witten's announcement drew together all of the previous results on S- and T-duality and the appearance of two- and five-dimensional branes in string theory. In the months following Witten's announcement, hundreds of new papers appeared on the Internet confirming that the new theory involved membranes in an important way. Today this flurry of work is known as the second superstring revolution.

One of the important developments following Witten's announcement was Witten's work in 1996 with string theorist Petr Hořava. Witten and Hořava studied M-theory on a special spacetime geometry with two ten-dimensional boundary components. Their work shed light on the mathematical structure of M-theory and suggested possible ways of connecting M-theory to real world physics.

Initially, some physicists suggested that the new theory was a fundamental theory of membranes, but Witten was skeptical of the role of membranes in the theory. In a paper from 1996, Hořava and Witten wrote

In the absence of an understanding of the true meaning and structure of M-theory, Witten has suggested that the "M" should stand for "magic", "mystery", or "membrane" according to taste, and the true meaning of the title should be decided when a more fundamental formulation of the theory is known.

In mathematics, a matrix is a rectangular array of numbers or other data. In physics, a matrix model is a particular kind of physical theory whose mathematical formulation involves the notion of a matrix in an important way. A matrix model describes the behavior of a set of matrices within the framework of quantum mechanics.

One important example of a matrix model is the BFSS matrix model proposed by Tom Banks, Willy Fischler, Stephen Shenker, and Leonard Susskind in 1997. This theory describes the behavior of a set of nine large matrices. In their original paper, these authors showed, among other things, that the low energy limit of this matrix model is described by eleven-dimensional supergravity. These calculations led them to propose that the BFSS matrix model is exactly equivalent to M-theory. The BFSS matrix model can therefore be used as a prototype for a correct formulation of M-theory and a tool for investigating the properties of M-theory in a relatively simple setting.

In geometry, it is often useful to introduce coordinates. For example, in order to study the geometry of the Euclidean plane, one defines the coordinates and as the distances between any point in the plane and a pair of axes. In ordinary geometry, the coordinates of a point are numbers, so they can be multiplied, and the product of two coordinates does not depend on the order of multiplication. That is, . This property of multiplication is known as the commutative law, and this relationship between geometry and the commutative algebra of coordinates is the starting point for much of modern geometry.

Noncommutative geometry is a branch of mathematics that attempts to generalize this situation. Rather than working with ordinary numbers, one considers some similar objects, such as matrices, whose multiplication does not satisfy the commutative law (that is, objects for which is not necessarily equal to ). One imagines that these noncommuting objects are coordinates on some more general notion of "space" and proves theorems about these generalized spaces by exploiting the analogy with ordinary geometry.

In a paper from 1998, Alain Connes, Michael R. Douglas, and Albert Schwarz showed that some aspects of matrix models and M-theory are described by a noncommutative quantum field theory, a special kind of physical theory in which the coordinates on spacetime do not satisfy the commutativity property. This established a link between matrix models and M-theory on the one hand, and noncommutative geometry on the other hand. It quickly led to the discovery of other important links between noncommutative geometry and various physical theories.

The application of quantum mechanics to physical objects such as the electromagnetic field, which are extended in space and time, is known as quantum field theory. In particle physics, quantum field theories form the basis for our understanding of elementary particles, which are modeled as excitations in the fundamental fields. Quantum field theories are also used throughout condensed matter physics to model particle-like objects called quasiparticles.

One approach to formulating M-theory and studying its properties is provided by the anti-de Sitter/conformal field theory (AdS/CFT) correspondence. Proposed by Juan Maldacena in late 1997, the AdS/CFT correspondence is a theoretical result which implies that M-theory is in some cases equivalent to a quantum field theory. In addition to providing insights into the mathematical structure of string and M-theory, the AdS/CFT correspondence has shed light on many aspects of quantum field theory in regimes where traditional calculational techniques are ineffective.

In the AdS/CFT correspondence, the geometry of spacetime is described in terms of a certain vacuum solution of Einstein's equation called anti-de Sitter space. In very elementary terms, anti-de Sitter space is a mathematical model of spacetime in which the notion of distance between points (the metric) is different from the notion of distance in ordinary Euclidean geometry. It is closely related to hyperbolic space, which can be viewed as a disk as illustrated on the left. This image shows a tessellation of a disk by triangles and squares. One can define the distance between points of this disk in such a way that all the triangles and squares are the same size and the circular outer boundary is infinitely far from any point in the interior.

Now imagine a stack of hyperbolic disks where each disk represents the state of the universe at a given time. The resulting geometric object is three-dimensional anti-de Sitter space. It looks like a solid cylinder in which any cross section is a copy of the hyperbolic disk. Time runs along the vertical direction in this picture. The surface of this cylinder plays an important role in the AdS/CFT correspondence. As with the hyperbolic plane, anti-de Sitter space is curved in such a way that any point in the interior is actually infinitely far from this boundary surface.

This construction describes a hypothetical universe with only two space dimensions and one time dimension, but it can be generalized to any number of dimensions. Indeed, hyperbolic space can have more than two dimensions and one can "stack up" copies of hyperbolic space to get higher-dimensional models of anti-de Sitter space.

An important feature of anti-de Sitter space is its boundary (which looks like a cylinder in the case of three-dimensional anti-de Sitter space). One property of this boundary is that, within a small region on the surface around any given point, it looks just like Minkowski space, the model of spacetime used in nongravitational physics. One can therefore consider an auxiliary theory in which "spacetime" is given by the boundary of anti-de Sitter space. This observation is the starting point for AdS/CFT correspondence, which states that the boundary of anti-de Sitter space can be regarded as the "spacetime" for a quantum field theory. The claim is that this quantum field theory is equivalent to the gravitational theory on the bulk anti-de Sitter space in the sense that there is a "dictionary" for translating entities and calculations in one theory into their counterparts in the other theory. For example, a single particle in the gravitational theory might correspond to some collection of particles in the boundary theory. In addition, the predictions in the two theories are quantitatively identical so that if two particles have a 40 percent chance of colliding in the gravitational theory, then the corresponding collections in the boundary theory would also have a 40 percent chance of colliding.

One particular realization of the AdS/CFT correspondence states that M-theory on the product space is equivalent to the so-called (2,0)-theory on the six-dimensional boundary. Here "(2,0)" refers to the particular type of supersymmetry that appears in the theory. In this example, the spacetime of the gravitational theory is effectively seven-dimensional (hence the notation ), and there are four additional "compact" dimensions (encoded by the factor). In the real world, spacetime is four-dimensional, at least macroscopically, so this version of the correspondence does not provide a realistic model of gravity. Likewise, the dual theory is not a viable model of any real-world system since it describes a world with six spacetime dimensions.

Nevertheless, the (2,0)-theory has proven to be important for studying the general properties of quantum field theories. Indeed, this theory subsumes many mathematically interesting effective quantum field theories and points to new dualities relating these theories. For example, Luis Alday, Davide Gaiotto, and Yuji Tachikawa showed that by compactifying this theory on a surface, one obtains a four-dimensional quantum field theory, and there is a duality known as the AGT correspondence which relates the physics of this theory to certain physical concepts associated with the surface itself. More recently, theorists have extended these ideas to study the theories obtained by compactifying down to three dimensions.

In addition to its applications in quantum field theory, the (2,0)-theory has spawned important results in pure mathematics. For example, the existence of the (2,0)-theory was used by Witten to give a "physical" explanation for a conjectural relationship in mathematics called the geometric Langlands correspondence. In subsequent work, Witten showed that the (2,0)-theory could be used to understand a concept in mathematics called Khovanov homology. Developed by Mikhail Khovanov around 2000, Khovanov homology provides a tool in knot theory, the branch of mathematics that studies and classifies the different shapes of knots. Another application of the (2,0)-theory in mathematics is the work of Davide Gaiotto, Greg Moore, and Andrew Neitzke, which used physical ideas to derive new results in hyperkähler geometry.

Another realization of the AdS/CFT correspondence states that M-theory on is equivalent to a quantum field theory called the ABJM theory in three dimensions. In this version of the correspondence, seven of the dimensions of M-theory are curled up, leaving four non-compact dimensions. Since the spacetime of our universe is four-dimensional, this version of the correspondence provides a somewhat more realistic description of gravity.

The ABJM theory appearing in this version of the correspondence is also interesting for a variety of reasons. Introduced by Aharony, Bergman, Jafferis, and Maldacena, it is closely related to another quantum field theory called Chern–Simons theory. The latter theory was popularized by Witten in the late 1980s because of its applications to knot theory. In addition, the ABJM theory serves as a semi-realistic simplified model for solving problems that arise in condensed matter physics.

In addition to being an idea of considerable theoretical interest, M-theory provides a framework for constructing models of real world physics that combine general relativity with the standard model of particle physics. Phenomenology is the branch of theoretical physics in which physicists construct realistic models of nature from more abstract theoretical ideas. String phenomenology is the part of string theory that attempts to construct realistic models of particle physics based on string and M-theory.

Typically, such models are based on the idea of compactification. Starting with the ten- or eleven-dimensional spacetime of string or M-theory, physicists postulate a shape for the extra dimensions. By choosing this shape appropriately, they can construct models roughly similar to the standard model of particle physics, together with additional undiscovered particles, usually supersymmetric partners to analogues of known particles. One popular way of deriving realistic physics from string theory is to start with the heterotic theory in ten dimensions and assume that the six extra dimensions of spacetime are shaped like a six-dimensional Calabi–Yau manifold. This is a special kind of geometric object named after mathematicians Eugenio Calabi and Shing-Tung Yau. Calabi–Yau manifolds offer many ways of extracting realistic physics from string theory. Other similar methods can be used to construct models with physics resembling to some extent that of our four-dimensional world based on M-theory.

Partly because of theoretical and mathematical difficulties and partly because of the extremely high energies (beyond what is technologically possible for the foreseeable future) needed to test these theories experimentally, there is so far no experimental evidence that would unambiguously point to any of these models being a correct fundamental description of nature. This has led some in the community to criticize these approaches to unification and question the value of continued research on these problems.

In one approach to M-theory phenomenology, theorists assume that the seven extra dimensions of M-theory are shaped like a manifold. This is a special kind of seven-dimensional shape constructed by mathematician Dominic Joyce of the University of Oxford. These manifolds are still poorly understood mathematically, and this fact has made it difficult for physicists to fully develop this approach to phenomenology.

For example, physicists and mathematicians often assume that space has a mathematical property called smoothness, but this property cannot be assumed in the case of a manifold if one wishes to recover the physics of our four-dimensional world. Another problem is that manifolds are not complex manifolds, so theorists are unable to use tools from the branch of mathematics known as complex analysis. Finally, there are many open questions about the existence, uniqueness, and other mathematical properties of manifolds, and mathematicians lack a systematic way of searching for these manifolds.

Because of the difficulties with manifolds, most attempts to construct realistic theories of physics based on M-theory have taken a more indirect approach to compactifying eleven-dimensional spacetime. One approach, pioneered by Witten, Hořava, Burt Ovrut, and others, is known as heterotic M-theory. In this approach, one imagines that one of the eleven dimensions of M-theory is shaped like a circle. If this circle is very small, then the spacetime becomes effectively ten-dimensional. One then assumes that six of the ten dimensions form a Calabi–Yau manifold. If this Calabi–Yau manifold is also taken to be small, one is left with a theory in four-dimensions.

Heterotic M-theory has been used to construct models of brane cosmology in which the observable universe is thought to exist on a brane in a higher dimensional ambient space. It has also spawned alternative theories of the early universe that do not rely on the theory of cosmic inflation.





</doc>
<doc id="20407" url="https://en.wikipedia.org/wiki?curid=20407" title="Multicast">
Multicast

In computer networking, multicast is group communication where data transmission is addressed to a group of destination computers simultaneously. Multicast can be one-to-many or many-to-many distribution. Multicast should not be confused with physical layer point-to-multipoint communication.

Group communication may either be "application layer multicast" or "network assisted multicast", where the latter makes it possible for the source to efficiently send to the group in a single transmission. Copies are automatically created in other network elements, such as routers, switches and cellular network base stations, but only to network segments that currently contain members of the group. Network assisted multicast may be implemented at the data link layer using one-to-many addressing and switching such as Ethernet multicast addressing, Asynchronous Transfer Mode (ATM), point-to-multipoint virtual circuits (P2MP) or Infiniband multicast. Network assisted multicast may also be implemented at the Internet layer using IP multicast. In IP multicast the implementation of the multicast concept occurs at the IP routing level, where routers create optimal distribution paths for datagrams sent to a multicast destination address.

Multicast is often employed in Internet Protocol (IP) applications of streaming media, such as IPTV and multipoint videoconferencing.

Ethernet frames with a value of 1 in the least-significant bit of the first octet of the destination address are treated as multicast frames and are flooded to all points on the network. This mechanism constitutes multicast at the data link layer. This mechanism is used by IP multicast to achieve one-to-many transmission for IP on Ethernet networks. Modern Ethernet controllers filter received packets to reduce CPU load, by looking up the hash of a multicast destination address in a table, initialized by software, which controls whether a multicast packet is dropped or fully received.

IP multicast is a technique for one-to-many communication over an IP network. The destination nodes send Internet Group Management Protocol "join" and "leave" messages, for example in the case of IPTV when the user changes from one TV channel to another. IP multicast scales to a larger receiver population by not requiring prior knowledge of who or how many receivers there are. Multicast uses network infrastructure efficiently by requiring the source to send a packet only once, even if it needs to be delivered to a large number of receivers. The nodes in the network take care of replicating the packet to reach multiple receivers only when necessary.

The most common transport layer protocol to use multicast addressing is User Datagram Protocol (UDP). By its nature, UDP is not reliable—messages may be lost or delivered out of order. By adding loss detection and re-transmission mechanisms, reliable multicast has been implemented on top of UDP or IP by various middleware products, e.g. those that implement the Real-Time Publish-Subscribe (RTPS) Protocol of the Object Management Group (OMG) Data Distribution Service (DDS) standard, as well as by special transport protocols such as Pragmatic General Multicast (PGM).

Application layer multicast overlay services are not based on IP multicast or data link layer multicast. Instead they use multiple unicast transmissions to simulate a multicast. These services are designed for application-level group communication. Internet Relay Chat (IRC) implements a single spanning tree across its overlay network for all conference groups. The lesser known PSYC technology uses custom multicast strategies per conference. Some peer-to-peer technologies employ the multicast concept known as peercasting when distributing content to multiple recipients.

Explicit multi-unicast (Xcast) is an alternate multicast strategy that includes addresses of all intended destinations within each packet. As such, given maximum transmission unit limitations, Xcast cannot be used for multicast groups with many destinations. The Xcast model generally assumes that stations participating in the communication are known ahead of time, so that distribution trees can be generated and resources allocated by network elements in advance of actual data traffic.

Wireless communications (with exception to point-to-point radio links using directional antennas) are inherently broadcasting media. However, the communication service provided may be unicast, multicast as well as broadcast, depending on if the data is addressed to one, to a group or to all receivers in the covered network, respectively.

In digital television, the concept of multicast service sometimes is used to refer to content protection by broadcast encryption, i.e. encrypted content over a simplex broadcast channel only addressed to paying viewers (pay television). In this case, data is broadcast (or distributed) to all receivers, but only addressed to a specific group.

The concept of "interactive multicast", for example using IP multicast, may be used over TV broadcast networks to improve efficiency, offer more TV programs, or reduce the required spectrum. Interactive multicast implies that TV programs are sent only over transmitters where there are viewers, and that only the most popular programs are transmitted. It relies on an additional interaction channel (a back-channel or return channel), where user equipment may send join and leave messages when the user changes TV channel. Interactive multicast has been suggested as an efficient transmission scheme in DVB-H and DVB-T2 terrestrial digital television systems, A similar concept is "switched broadcast" over cable-TV networks, where only the currently most popular content is delivered in the cable-TV network. Scalable video multicast in an application of interactive multicast, where a subset of the viewers receive additional data for high-resolution video.

TV gateways converts Satellite (DVB-S, DVB-S2), Cable (DVB-C, DVB-C2) and Terrestrial television (DVB-T, DVB-T2) to IP for distribution using unicast and multicast in home, hospitality and enterprise applications
Another similar concept is Cell-TV, and implies TV distribution over 3G cellular networks using the network-assisted multicasting offered by the Multimedia Broadcast Multicast Service (MBMS) service, or over 4G/LTE cellular networks with the eMBMS (enhanced MBMS) service.

In an optical mesh network, protecting multicast lightpaths is one of the key concerns. The most straight forward approach to protect a multicast tree is to establish a link-disjoint backup tree which establishes dedicated protection. It is much easier to find an arc-disjoint path for each leaf node in a light tree. The essence of protecting a multicast session is to find a backup path for each destination node when a link on the working path to that node fails.


</doc>
<doc id="20408" url="https://en.wikipedia.org/wiki?curid=20408" title="Marie Curie">
Marie Curie

Marie Skłodowska Curie (; ; ; born Maria Salomea Skłodowska; 7 November 18674 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, the first person and only woman to win twice, the only person to win a Nobel Prize in two different sciences, and was part of the Curie family legacy of five Nobel Prizes. She was also the first woman to become a professor at the University of Paris, and in 1995 became the first woman to be entombed on her own merits in the Panthéon in Paris.

She was born in Warsaw, in what was then the Kingdom of Poland, part of the Russian Empire. She studied at Warsaw's clandestine Flying University and began her practical scientific training in Warsaw. In 1891, aged 24, she followed her older sister Bronisława to study in Paris, where she earned her higher degrees and conducted her subsequent scientific work. She shared the 1903 Nobel Prize in Physics with her husband Pierre Curie and with physicist Henri Becquerel. She won the 1911 Nobel Prize in Chemistry.

Her achievements included the development of the theory of "radioactivity" (a term that she coined), techniques for isolating radioactive isotopes, and the discovery of two elements, polonium and radium. Under her direction, the world's first studies into the treatment of neoplasms were conducted using radioactive isotopes. She founded the Curie Institutes in Paris and in Warsaw, which remain major centres of medical research today. During World War I, she developed mobile radiography units to provide X-ray services to field hospitals.

While a French citizen, Marie Skłodowska Curie, who used both surnames, never lost her sense of Polish identity. She taught her daughters the Polish language and took them on visits to Poland. She named the first chemical element that she discovered in 1898 "polonium", after her native country.

Marie Curie died in 1934, aged 66, at a sanatorium in Sancellemoz (Haute-Savoie), France, of aplastic anemia from exposure to radiation in the course of her scientific research and in the course of her radiological work at field hospitals during World War I.

Maria Skłodowska was born in Warsaw, in Congress Poland in the Russian Empire, on 7 November 1867, the fifth and youngest child of well-known teachers Bronisława, "née" Boguska, and Władysław Skłodowski. The elder siblings of Maria (nicknamed "Mania") were Zofia (born 1862, nicknamed "Zosia"), (born 1863, nicknamed "Józio"), Bronisława (born 1865, nicknamed "Bronia") and (born 1866, nicknamed "Hela").

On both the paternal and maternal sides, the family had lost their property and fortunes through patriotic involvements in Polish national uprisings aimed at restoring Poland's independence (the most recent had been the January Uprising of 1863–65). This condemned the subsequent generation, including Maria and her elder siblings, to a difficult struggle to get ahead in life. Maria's paternal grandfather, , had been a respected teacher in Lublin, where he taught the young Bolesław Prus, who would become a leading figure in Polish literature.

Władysław Skłodowski taught mathematics and physics, subjects that Maria was to pursue, and was also director of two Warsaw "gymnasia" for boys. After Russian authorities eliminated laboratory instruction from the Polish schools, he brought much of the laboratory equipment home, and instructed his children in its use. He was eventually fired by his Russian supervisors for pro-Polish sentiments, and forced to take lower-paying posts; the family also lost money on a bad investment, and eventually chose to supplement their income by lodging boys in the house. Maria's mother Bronisława operated a prestigious Warsaw boarding school for girls; she resigned from the position after Maria was born. She died of tuberculosis in May 1878, when Maria was ten years old. Less than three years earlier, Maria's oldest sibling, Zofia, had died of typhus contracted from a boarder. Maria's father was an atheist; her mother a devout Catholic. The deaths of Maria's mother and sister caused her to give up Catholicism and become agnostic.

When she was ten years old, Maria began attending the boarding school of J. Sikorska; next she attended a "gymnasium" for girls, from which she graduated on 12 June 1883 with a gold medal. After a collapse, possibly due to depression, she spent the following year in the countryside with relatives of her father, and the next year with her father in Warsaw, where she did some tutoring. Unable to enroll in a regular institution of higher education because she was a woman, she and her sister Bronisława became involved with the clandestine Flying University (sometimes translated as "Floating University"), a Polish patriotic institution of higher learning that admitted women students.

Maria made an agreement with her sister, Bronisława, that she would give her financial assistance during Bronisława's medical studies in Paris, in exchange for similar assistance two years later. In connection with this, Maria took a position as governess: first as a home tutor in Warsaw; then for two years as a governess in Szczuki with a landed family, the Żorawskis, who were relatives of her father. While working for the latter family, she fell in love with their son, Kazimierz Żorawski, a future eminent mathematician. His parents rejected the idea of his marrying the penniless relative, and Kazimierz was unable to oppose them. Maria's loss of the relationship with Żorawski was tragic for both. He soon earned a doctorate and pursued an academic career as a mathematician, becoming a professor and rector of Kraków University. Still, as an old man and a mathematics professor at the Warsaw Polytechnic, he would sit contemplatively before the statue of Maria Skłodowska which had been erected in 1935 before the Radium Institute that she had founded in 1932.

At the beginning of 1890, Bronisława — who a few months earlier had married Kazimierz Dłuski, a Polish physician and social and political activist — invited Maria to join them in Paris. Maria declined because she could not afford the university tuition; it would take her a year and a half longer to gather the necessary funds. She was helped by her father, who was able to secure a more lucrative position again. All that time she continued to educate herself, reading books, exchanging letters, and being tutored herself. In early 1889 she returned home to her father in Warsaw. She continued working as a governess, and remained there till late 1891. She tutored, studied at the Flying University, and began her practical scientific training (1890–91) in a chemical laboratory at the Museum of Industry and Agriculture at "Krakowskie Przedmieście 66", near Warsaw's Old Town. The laboratory was run by her cousin Józef Boguski, who had been an assistant in Saint Petersburg to the Russian chemist Dmitri Mendeleev.

In late 1891, she left Poland for France. In Paris, Maria (or Marie, as she would be known in France) briefly found shelter with her sister and brother-in-law before renting a garret closer to the university, in the Latin Quarter, and proceeding with her studies of physics, chemistry, and mathematics at the University of Paris, where she enrolled in late 1891. She subsisted on her meager resources, suffering from cold winters and occasionally fainting from hunger.

Skłodowska studied during the day and tutored evenings, barely earning her keep. In 1893, she was awarded a degree in physics and began work in an industrial laboratory of Professor Gabriel Lippmann. Meanwhile, she continued studying at the University of Paris, and with the aid of a fellowship she was able to earn a second degree in 1894.

Skłodowska had begun her scientific career in Paris with an investigation of the magnetic properties of various steels, commissioned by the Society for the Encouragement of National Industry ("Société d'encouragement pour l'industrie nationale" ). That same year Pierre Curie entered her life; it was their mutual interest in natural sciences that drew them together. Pierre Curie was an instructor at the School of Physics and Chemistry, the "École supérieure de physique et de chimie industrielles de la ville de Paris" (ESPCI). They were introduced by the Polish physicist, Professor Józef Wierusz-Kowalski, who had learned that she was looking for a larger laboratory space, something that Wierusz-Kowalski thought Pierre Curie had access to. Though Curie did not have a large laboratory, he was able to find some space for Skłodowska where she was able to begin work.

Their mutual passion for science brought them increasingly closer, and they began to develop feelings for one another. Eventually Pierre Curie proposed marriage, but at first Skłodowska did not accept as she was still planning to go back to her native country. Curie, however, declared that he was ready to move with her to Poland, even if it meant being reduced to teaching French. Meanwhile, for the 1894 summer break, Skłodowska returned to Warsaw, where she visited her family. She was still laboring under the illusion that she would be able to work in her chosen field in Poland, but she was denied a place at Kraków University because she was a woman. A letter from Pierre Curie convinced her to return to Paris to pursue a Ph.D. At Skłodowska's insistence, Curie had written up his research on magnetism and received his own doctorate in March 1895; he was also promoted to professor at the School. A contemporary quip would call Skłodowska, "Pierre's biggest discovery." On 26 July 1895 they were married in Sceaux (Seine); neither wanted a religious service. Curie's dark blue outfit, worn instead of a bridal gown, would serve her for many years as a laboratory outfit. They shared two pastimes: long bicycle trips, and journeys abroad, which brought them even closer. In Pierre, Marie had found a new love, a partner, and a scientific collaborator on whom she could depend.

In 1895, Wilhelm Roentgen discovered the existence of X-rays, though the mechanism behind their production was not yet understood. In 1896, Henri Becquerel discovered that uranium salts emitted rays that resembled X-rays in their penetrating power. He demonstrated that this radiation, unlike phosphorescence, did not depend on an external source of energy but seemed to arise spontaneously from uranium itself. Influenced by these two important discoveries, Curie decided to look into uranium rays as a possible field of research for a thesis.

She used an innovative technique to investigate samples. Fifteen years earlier, her husband and his brother had developed a version of the electrometer, a sensitive device for measuring electric charge. Using her husband's electrometer, she discovered that uranium rays caused the air around a sample to conduct electricity. Using this technique, her first result was the finding that the activity of the uranium compounds depended only on the quantity of uranium present. She hypothesized that the radiation was not the outcome of some interaction of molecules but must come from the atom itself. This hypothesis was an important step in disproving the ancient assumption that atoms were indivisible.

In 1897, her daughter Irène was born. To support her family, Curie began teaching at the École Normale Supérieure. The Curies did not have a dedicated laboratory; most of their research was carried out in a converted shed next to the School of Physics and Chemistry. The shed, formerly a medical school dissecting room, was poorly ventilated and not even waterproof. They were unaware of the deleterious effects of radiation exposure attendant on their continued unprotected work with radioactive substances. The School did not sponsor her research, but she would receive subsidies from metallurgical and mining companies and from various organizations and governments.

Curie's systematic studies included two uranium minerals, pitchblende and torbernite (also known as chalcolite). Her electrometer showed that pitchblende was four times as active as uranium itself, and chalcolite twice as active. She concluded that, if her earlier results relating the quantity of uranium to its activity were correct, then these two minerals must contain small quantities of another substance that was far more active than uranium. She began a systematic search for additional substances that emit radiation, and by 1898 she discovered that the element thorium was also radioactive. Pierre Curie was increasingly intrigued by her work. By mid-1898 he was so invested in it that he decided to drop his work on crystals and to join her.

She was acutely aware of the importance of promptly publishing her discoveries and thus establishing her priority. Had not Becquerel, two years earlier, presented his discovery to the "Académie des Sciences" the day after he made it, credit for the discovery of radioactivity, and even a Nobel Prize, would instead have gone to Silvanus Thompson. Curie chose the same rapid means of publication. Her paper, giving a brief and simple account of her work, was presented for her to the "Académie" on 12 April 1898 by her former professor, Gabriel Lippmann. Even so, just as Thompson had been beaten by Becquerel, so Curie was beaten in the race to tell of her discovery that thorium gives off rays in the same way as uranium; two months earlier, Gerhard Carl Schmidt had published his own finding in Berlin.

At that time, no one else in the world of physics had noticed what Curie recorded in a sentence of her paper, describing how much greater were the activities of pitchblende and chalcolite than uranium itself: "The fact is very remarkable, and leads to the belief that these minerals may contain an element which is much more active than uranium." She later would recall how she felt "a passionate desire to verify this hypothesis as rapidly as possible." On 14 April 1898, the Curies optimistically weighed out a 100-gram sample of pitchblende and ground it with a pestle and mortar. They did not realize at the time that what they were searching for was present in such minute quantities that they would eventually have to process tons of the ore.

In July 1898, Curie and her husband published a joint paper announcing the existence of an element which they named "polonium", in honour of her native Poland, which would for another twenty years remain partitioned among three empires (Russian, Austrian, and Prussian). On 26 December 1898, the Curies announced the existence of a second element, which they named "radium", from the Latin word for "ray". In the course of their research, they also coined the word "radioactivity".

To prove their discoveries beyond any doubt, the Curies sought to isolate polonium and radium in pure form. Pitchblende is a complex mineral; the chemical separation of its constituents was an arduous task. The discovery of polonium had been relatively easy; chemically it resembles the element bismuth, and polonium was the only bismuth-like substance in the ore. Radium, however, was more elusive; it is closely related chemically to barium, and pitchblende contains both elements. By 1898 the Curies had obtained traces of radium, but appreciable quantities, uncontaminated with barium, were still beyond reach. The Curies undertook the arduous task of separating out radium salt by differential crystallization. From a ton of pitchblende, one-tenth of a gram of radium chloride was separated in 1902. In 1910, she isolated pure radium metal. She never succeeded in isolating polonium, which has a half-life of only 138 days.

Between 1898 and 1902, the Curies published, jointly or separately, a total of 32 scientific papers, including one that announced that, when exposed to radium, diseased, tumor-forming cells were destroyed faster than healthy cells.

In 1900, Curie became the first woman faculty member at the École Normale Supérieure, and her husband joined the faculty of the University of Paris. In 1902 she visited Poland on the occasion of her father's death.

In June 1903, supervised by Gabriel Lippmann, Curie was awarded her doctorate from the University of Paris. That month the couple were invited to the Royal Institution in London to give a speech on radioactivity; being a woman, she was prevented from speaking, and Pierre Curie alone was allowed to. Meanwhile, a new industry began developing, based on radium. The Curies did not patent their discovery and benefited little from this increasingly profitable business.

In December 1903, the Royal Swedish Academy of Sciences awarded Pierre Curie, Marie Curie, and Henri Becquerel the Nobel Prize in Physics, "in recognition of the extraordinary services they have rendered by their joint researches on the radiation phenomena discovered by Professor Henri Becquerel." At first the committee had intended to honor only Pierre Curie and Henri Becquerel, but a committee member and advocate for women scientists, Swedish mathematician Magnus Goesta Mittag-Leffler, alerted Pierre to the situation, and after his complaint, Marie's name was added to the nomination. Marie Curie was the first woman to be awarded a Nobel Prize.

Curie and her husband declined to go to Stockholm to receive the prize in person; they were too busy with their work, and Pierre Curie, who disliked public ceremonies, was feeling increasingly ill. As Nobel laureates were required to deliver a lecture, the Curies finally undertook the trip in 1905. The award money allowed the Curies to hire their first laboratory assistant. Following the award of the Nobel Prize, and galvanized by an offer from the University of Geneva, which offered Pierre Curie a position, the University of Paris gave him a professorship and the chair of physics, although the Curies still did not have a proper laboratory. Upon Pierre Curie's complaint, the University of Paris relented and agreed to furnish a new laboratory, but it would not be ready until 1906.

In December 1904, Curie gave birth to their second daughter, Ève. She hired Polish governesses to teach her daughters her native language, and sent or took them on visits to Poland.

On 19 April 1906, Pierre Curie was killed in a road accident. Walking across the Rue Dauphine in heavy rain, he was struck by a horse-drawn vehicle and fell under its wheels, causing his skull to fracture. Curie was devastated by her husband's death. On 13 May 1906 the physics department of the University of Paris decided to retain the chair that had been created for her late husband and to offer it to Marie. She accepted it, hoping to create a world-class laboratory as a tribute to her husband Pierre. She was the first woman to become a professor at the University of Paris.

Curie's quest to create a new laboratory did not end with the University of Paris, however. In her later years, she headed the Radium Institute ("Institut du radium", now Curie Institute, "Institut Curie"), a radioactivity laboratory created for her by the Pasteur Institute and the University of Paris. The initiative for creating the Radium Institute had come in 1909 from Pierre Paul Émile Roux, director of the Pasteur Institute, who had been disappointed that the University of Paris was not giving Curie a proper laboratory and had suggested that she move to the Pasteur Institute. Only then, with the threat of Curie leaving, did the University of Paris relent, and eventually the Curie Pavilion became a joint initiative of the University of Paris and the Pasteur Institute.

In 1910, Curie succeeded in isolating radium; she also defined an international standard for radioactive emissions that was eventually named for her and Pierre: the curie. Nevertheless, in 1911 the French Academy of Sciences did not elect her to be a member by one or two votes. Elected instead was Édouard Branly, an inventor who had helped Guglielmo Marconi develop the wireless telegraph.

A doctoral student of Curie, Marguerite Perey, became the first woman elected to membership in the Academy – over half a century later, in 1962. Despite Curie's fame as a scientist working for France, the public's attitude tended toward xenophobia—the same that had led to the Dreyfus affair–which also fueled false speculation that Curie was Jewish. During the French Academy of Sciences elections, she was vilified by the right wing press who criticised her for being a foreigner and an atheist. Her daughter later remarked on the public hypocrisy as the French press often portrayed Curie as an unworthy foreigner when she was nominated for a French honour, but would portray her as a French hero when she received a foreign one such as her Nobel Prizes.

In 1911, it was revealed that in 1910–11 Curie had conducted an affair of about a year's duration with physicist Paul Langevin, a former student of Pierre Curie's—a married man who was estranged from his wife. This resulted in a press scandal that was exploited by her academic opponents. Curie (then in her mid-40s) was five years older than Langevin and was misrepresented in the tabloids as a foreign Jewish home-wrecker. When the scandal broke, she was away at a conference in Belgium; on her return, she found an angry mob in front of her house and had to seek refuge, with her daughters, in the home of her friend, Camille Marbo.

International recognition for her work had been growing to new heights, and the Royal Swedish Academy of Sciences, overcoming opposition prompted by the Langevin scandal, honored her a second time, with the 1911 Nobel Prize in Chemistry. This award was "in recognition of her services to the advancement of chemistry by the discovery of the elements radium and polonium, by the isolation of radium and the study of the nature and compounds of this remarkable element." She was the first person to win or share two Nobel Prizes, and remains alone with Linus Pauling as Nobel laureates in two fields each. A delegation of celebrated Polish men of learning, headed by novelist Henryk Sienkiewicz, encouraged her to return to Poland and continue her research in her native country. Curie's second Nobel Prize enabled her to persuade the French government into supporting the Radium Institute, built in 1914, where research was conducted in chemistry, physics, and medicine. A month after accepting her 1911 Nobel Prize, she was hospitalised with depression and a kidney ailment. For most of 1912 she avoided public life but did spend time in England with her friend and fellow physicist, Hertha Ayrton. She returned to her laboratory only in December, after a break of about 14 months.

In 1912, the Warsaw Scientific Society offered her the directorship of a new laboratory in Warsaw but she declined, focusing on the developing Radium Institute to be completed in August 1914, and on a new street named Rue Pierre-Curie. She was appointed Director of the Curie Laboratory in the Radium Institute of the University of Paris, founded in 1914. She visited Poland in 1913 and was welcomed in Warsaw but the visit was mostly ignored by the Russian authorities. The Institute's development was interrupted by the coming war, as most researchers were drafted into the French Army, and it fully resumed its activities in 1919.

During World War I, Curie recognised that wounded soldiers were best served if operated upon as soon as possible. She saw a need for field radiological centres near the front lines to assist battlefield surgeons. After a quick study of radiology, anatomy, and automotive mechanics she procured X-ray equipment, vehicles, auxiliary generators, and developed mobile radiography units, which came to be popularly known as "petites Curies" ("Little Curies"). She became the director of the Red Cross Radiology Service and set up France's first military radiology centre, operational by late 1914. Assisted at first by a military doctor and by her 17-year-old daughter Irène, Curie directed the installation of 20 mobile radiological vehicles and another 200 radiological units at field hospitals in the first year of the war. Later, she began training other women as aides.

In 1915, Curie produced hollow needles containing "radium emanation", a colorless, radioactive gas given off by radium, later identified as radon, to be used for sterilizing infected tissue. She provided the radium from her own one-gram supply. It is estimated that over a million wounded soldiers were treated with her X-ray units. Busy with this work, she carried out very little scientific research during that period. In spite of all her humanitarian contributions to the French war effort, Curie never received any formal recognition of it from the French government.

Also, promptly after the war started, she attempted to donate her gold Nobel Prize medals to the war effort but the French National Bank refused to accept them. She did buy war bonds, using her Nobel Prize money. She said:I am going to give up the little gold I possess. I shall add to this the scientific medals, which are quite useless to me. There is something else: by sheer laziness I had allowed the money for my second Nobel Prize to remain in Stockholm in Swedish crowns. This is the chief part of what we possess. I should like to bring it back here and invest it in war loans. The state needs it. Only, I have no illusions: this money will probably be lost. She was also an active member in committees of Polonia in France dedicated to the Polish cause. After the war, she summarized her wartime experiences in a book, "Radiology in War" (1919).

In 1920, for the 25th anniversary of the discovery of radium, the French government established a stipend for her; its previous recipient was Louis Pasteur (1822–95). In 1921, she was welcomed triumphantly when she toured the United States to raise funds for research on radium. Mrs. William Brown Meloney, after interviewing Curie, created a "Marie Curie Radium Fund" and raised money to buy radium, publicising her trip.

In 1921, U.S. President Warren G. Harding received her at the White House to present her with the 1 gram of radium collected in the United States. Before the meeting, recognising her growing fame abroad, and embarrassed by the fact that she had no French official distinctions to wear in public, the French government offered her a Legion of Honour award, but she refused. In 1922 she became a fellow of the French Academy of Medicine. She also travelled to other countries, appearing publicly and giving lectures in Belgium, Brazil, Spain, and Czechoslovakia.

Led by Curie, the Institute produced four more Nobel Prize winners, including her daughter Irène Joliot-Curie and her son-in-law, Frédéric Joliot-Curie. Eventually, it became one of four major radioactivity research laboratories, the others being the Cavendish Laboratory, with Ernest Rutherford; the Institute for Radium Research, Vienna, with Stefan Meyer; and the Kaiser Wilhelm Institute for Chemistry, with Otto Hahn and Lise Meitner.

In August 1922, Marie Curie became a member of the newly created International Committee on Intellectual Cooperation of the League of Nations. In 1923, she wrote a biography of her husband Pierre, entitled "Pierre Curie". In 1925, she visited Poland, to participate in the ceremony that laid foundations for the Radium Institute in Warsaw. Her second American tour, in 1929, succeeded in equipping the Warsaw Radium Institute with radium; it was opened in 1932 and her sister Bronisława became its director. These distractions from her scientific labours and the attendant publicity caused her much discomfort but provided resources needed for her work. In 1930, she was elected a member of the International Atomic Weights Committee where she served until her death.

Curie visited Poland for the last time in early 1934. A few months later, on 4 July 1934, she died at the Sancellemoz sanatorium in Passy, Haute-Savoie, from aplastic anemia believed to have been contracted from her long-term exposure to radiation.

The damaging effects of ionising radiation were not known at the time of her work, which had been carried out without the safety measures later developed. She had carried test tubes containing radioactive isotopes in her pocket, and she stored them in her desk drawer, remarking on the faint light that the substances gave off in the dark. Curie was also exposed to X-rays from unshielded equipment while serving as a radiologist in field hospitals during the war. Although her many decades of exposure to radiation caused chronic illnesses (including near-blindness due to cataracts) and ultimately her death, she never really acknowledged the health risks of radiation exposure.

She was interred at the cemetery in Sceaux, alongside her husband Pierre. Sixty years later, in 1995, in honour of their achievements, the remains of both were transferred to the Panthéon, Paris. She became the first woman to be honoured with interment in the Panthéon on her own merits. In 2015, two other women were also interred on their own merits.

Because of their levels of radioactive contamination, her papers from the 1890s are considered too dangerous to handle. Even her cookbook is highly radioactive. Her papers are kept in lead-lined boxes, and those who wish to consult them must wear protective clothing. In her last year, she worked on a book, "Radioactivity", which was published posthumously in 1935.

The physical and societal aspects of the Curies' work contributed substantially to shaping the world of the twentieth and twenty-first centuries. Cornell University professor Williams observes:

If Curie's work helped overturn established ideas in physics and chemistry, it has had an equally profound effect in the societal sphere. To attain her scientific achievements, she had to overcome barriers, in both her native and her adoptive country, that were placed in her way because she was a woman. This aspect of her life and career is highlighted in Françoise Giroud's "Marie Curie: A Life", which emphasizes Curie's role as a feminist precursor.

She was known for her honesty and moderate life style. Having received a small scholarship in 1893, she returned it in 1897 as soon as she began earning her keep. She gave much of her first Nobel Prize money to friends, family, students, and research associates. In an unusual decision, Curie intentionally refrained from patenting the radium-isolation process, so that the scientific community could do research unhindered. She insisted that monetary gifts and awards be given to the scientific institutions she was affiliated with rather than to her. She and her husband often refused awards and medals. Albert Einstein reportedly remarked that she was probably the only person who could not be corrupted by fame.

As one of the most famous women scientists to date, Marie Curie has become an icon in the scientific world and has received tributes from across the globe, even in the realm of pop culture. In a 2009 poll carried out by "New Scientist", she was voted the "most inspirational woman in science". Curie received 25.1 per cent of all votes cast, nearly twice as many as second-place Rosalind Franklin (14.2 per cent).

Poland and France declared 2011 the Year of Marie Curie, and the United Nations declared that this would be the International Year of Chemistry. An artistic installation celebrating "Madame Curie" filled the Jacobs Gallery at San Diego's Museum of Contemporary Art. On 7 November, Google celebrated the anniversary of her birth with a special Google Doodle. On 10 December, the New York Academy of Sciences celebrated the centenary of Marie Curie's second Nobel Prize in the presence of Princess Madeleine of Sweden.

Marie Curie was the first woman to win a Nobel Prize, the first person to win two Nobel Prizes, the only woman to win in two fields, and the only person to win in multiple sciences. Awards that she received include:


Marie Curie's 1898 publication with her husband and their collaborator Gustave Bémont for their discovery of radium and polonium was honored by a Citation for Chemical Breakthrough Award from the Division of History of Chemistry of the American Chemical Society presented to the ESPCI Paris (Ecole supérieure de physique et de chimie industrielles de la Ville de Paris) in 2015.

In 1995, she became the first woman to be entombed on her own merits in the Panthéon, Paris. The curie (symbol Ci), a unit of radioactivity, is named in honour of her and Pierre Curie (although the commission which agreed on the name never clearly stated whether the standard was named after Pierre, Marie or both of them). The element with atomic number 96 was named curium. Three radioactive minerals are also named after the Curies: curite, sklodowskite, and cuprosklodowskite. She received numerous honorary degrees from universities across the world. The Marie Skłodowska-Curie Actions fellowship program of the European Union for young scientists wishing to work in a foreign country is named after her. In Poland, she had received honorary doctorates from the Lwów Polytechnic (1912),Poznań University (1922), Kraków's Jagiellonian University (1924), and the Warsaw Polytechnic (1926). In 1921, in the U.S., she was awarded membership in the Iota Sigma Pi women scientists' society.

Her name is included on the "Monument to the X-ray and Radium Martyrs of All Nations", erected in Hamburg, Germany in 1936.

Numerous locations around the world are named after her. In 2007, a metro station in Paris was renamed to honour both of the Curies. Polish nuclear research reactor Maria is named after her. The 7000 Curie asteroid is also named after her. A KLM McDonnell Douglas MD-11 (registration PH-KCC) is named in her honour.

Several institutions bear her name, starting with the two Curie institutes – the Maria Skłodowska–Curie Institute of Oncology, in Warsaw; and the "Institut Curie" in Paris. She is the patron of Maria Curie-Skłodowska University, in Lublin, founded in 1944; and of Pierre and Marie Curie University (Paris VI), France's pre-eminent science university. In Britain, Marie Curie Cancer Care was organized in 1948 to care for the terminally ill.

Two museums are devoted to Marie Curie. In 1967, the Maria Skłodowska-Curie Museum was established in Warsaw's "New Town", at her birthplace on "ulica Freta" (Freta Street). Her Paris laboratory is preserved as the Musée Curie, open since 1992.

Several works of art bear her likeness. In 1935, Michalina Mościcka, wife of Polish President Ignacy Mościcki, unveiled a statue of Marie Curie before Warsaw's Radium Institute. During the 1944 Second World War Warsaw Uprising against the Nazi German occupation, the monument was damaged by gunfire; after the war it was decided to leave the bullet marks on the statue and its pedestal. In 1955 Jozef Mazur created a stained glass panel of her, the Maria Skłodowska-Curie Medallion, featured in the University at Buffalo Polish Room.

A number of biographies are devoted to her. In 1938 her daughter, Ève Curie, published "Madame Curie". In 1987 Françoise Giroud wrote "Marie Curie: A Life". In 2005 Barbara Goldsmith wrote "Obsessive Genius: The Inner World of Marie Curie". In 2011 Lauren Redniss published "Radioactive: Marie and Pierre Curie, a Tale of Love and Fallout".

Greer Garson and Walter Pidgeon starred in the 1943 U.S. Oscar-nominated film, "Madame Curie", based on her life. More recently, in 1997, a French film about Pierre and Marie Curie was released, "Les Palmes de M. Schutz". It was adapted from a play of the same name. In the film, Marie Curie was played by Isabelle Huppert.

Curie is the subject of the play "False Assumptions" by Lawrence Aronovitch, in which the ghosts of three other women scientists observe events in her life. Curie has also been portrayed by Susan Marie Frontczak in her play "Manya: The Living History of Marie Curie", a one-woman show performed in 30 US states and nine countries, by 2014.

Curie's likeness also has appeared on banknotes, stamps and coins around the world. She was featured on the Polish late-1980s 20,000-"złoty" banknote as well as on the last French 500-franc note, before the franc was replaced by the euro. Curie themed postage stamps from Mali, the Republic of Togo, Zambia, and the Republic of Guinea actually show a picture of Susan Marie Frontczak portraying Curie in a 2001 picture by Paul Schroeder.

On the first centenary of Marie Curie's second Nobel Prize in 2011, an allegorical mural was painted on the façade of her Warsaw birthplace. It depicts an infant Maria Skłodowska holding a test tube from which emanate the elements that she would discover as an adult: polonium and radium. Also in 2011, a new Warsaw bridge over the Vistula was named in her honor.





</doc>
<doc id="20412" url="https://en.wikipedia.org/wiki?curid=20412" title="MATLAB">
MATLAB

MATLAB ("matrix laboratory") is a multi-paradigm numerical computing environment and proprietary programming language developed by MathWorks. MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages, including C, C++, C#, Java, Fortran and Python.

Although MATLAB is intended primarily for numerical computing, an optional toolbox uses the MuPAD symbolic engine, allowing access to symbolic computing abilities. An additional package, Simulink, adds graphical multi-domain simulation and model-based design for dynamic and embedded systems.

As of 2018, MATLAB has more than 3 million users worldwide. MATLAB users come from various backgrounds of engineering, science, and economics.

Cleve Moler, the chairman of the computer science department at the University of New Mexico, started developing MATLAB in the late 1970s. He designed it to give his students access to LINPACK and EISPACK without them having to learn Fortran. It soon spread to other universities and found a strong audience within the applied mathematics community. Jack Little, an engineer, was exposed to it during a visit Moler made to Stanford University in 1983. Recognizing its commercial potential, he joined with Moler and Steve Bangert. They rewrote MATLAB in C and founded MathWorks in 1984 to continue its development. These rewritten libraries were known as JACKPAC. In 2000, MATLAB was rewritten to use a newer set of libraries for matrix manipulation, LAPACK.

MATLAB was first adopted by researchers and practitioners in control engineering, Little's specialty, but quickly spread to many other domains. It is now also used in education, in particular the teaching of linear algebra, numerical analysis, and is popular amongst scientists involved in image processing.

The MATLAB application is built around the MATLAB scripting language. Common usage of the MATLAB application involves using the Command Window as an interactive mathematical shell or executing text files containing MATLAB code.

Variables are defined using the assignment operator, codice_1. MATLAB is a weakly typed programming language because types are implicitly converted. It is an inferred typed language because variables can be assigned without declaring their type, except if they are to be treated as symbolic objects, and that their type can change. Values can come from constants, from computation involving values of other variables, or from the output of a function. For example:
A simple array is defined using the colon syntax: "initial"codice_2"increment"codice_2"terminator". For instance:

defines a variable named codice_4 (or assigns a new value to an existing variable with the name codice_4) which is an array consisting of the values 1, 3, 5, 7, and 9. That is, the array starts at 1 (the "initial" value), increments with each step from the previous value by 2 (the "increment" value), and stops once it reaches (or to avoid exceeding) 9 (the "terminator" value).

the "increment" value can actually be left out of this syntax (along with one of the colons), to use a default value of 1.

assigns to the variable named codice_6 an array with the values 1, 2, 3, 4, and 5, since the default value of 1 is used as the incrementer.

Indexing is one-based, which is the usual convention for matrices in mathematics, although not for some programming languages such as C, C++, and Java.

Matrices can be defined by separating the elements of a row with blank space or comma and using a semicolon to terminate each row. The list of elements should be surrounded by square brackets: []. Parentheses: () are used to access elements and subarrays (they are also used to denote a function argument list).

Sets of indices can be specified by expressions such as "2:4", which evaluates to [2, 3, 4]. For example, a submatrix taken from rows 2 through 4 and columns 3 through 4 can be written as:

A square identity matrix of size "n" can be generated using the function "eye", and matrices of any size with zeros or ones can be generated with the functions "zeros" and "ones", respectively.
Transposing a vector or a matrix is done either by the function "transpose" or by adding prime after a dot to the matrix. Without the dot Matlab will perform conjugate transpose.
Most MATLAB functions can accept matrices and will apply themselves to each element. For example, codice_7 will multiply every element in "J" by 2, and then reduce each element modulo "n". MATLAB does include standard "for" and "while" loops, but (as in other similar applications such as R), using the vectorized notation often produces code that is faster to execute. This code, excerpted from the function "magic.m", creates a magic square "M" for odd values of "n" (MATLAB function codice_8 is used here to generate square matrices I and J containing 1:n).

MATLAB has structure data types. Since all variables in MATLAB are arrays, a more adequate name is "structure array", where each element of the array has the same field names. In addition, MATLAB supports dynamic field names (field look-ups by name, field manipulations, etc.). Unfortunately, MATLAB JIT does not support MATLAB structures, therefore just a simple bundling of various variables into a structure will come at a cost.

When creating a MATLAB function, the name of the file should match the name of the first function in the file. Valid function names begin with an alphabetic character, and can contain letters, numbers, or underscores. Functions are often case sensitive.

MATLAB supports elements of lambda calculus by introducing function handles, or function references, which are implemented either in .m files or anonymous/nested functions.

MATLAB supports object-oriented programming including classes, inheritance, virtual dispatch, packages, pass-by-value semantics, and pass-by-reference semantics. However, the syntax and calling conventions are significantly different from other languages. MATLAB has value classes and reference classes, depending on whether the class has "handle" as a super-class (for reference classes) or not (for value classes).

Method call behavior is different between value and reference classes. For example, a call to a method

can alter any member of "object" only if "object" is an instance of a reference class.

An example of a simple class is provided below.

When put into a file named hello.m, this can be executed with the following commands:
MATLAB supports developing applications with graphical user interface (GUI) features. MATLAB includes GUIDE (GUI development environment) for graphically designing GUIs. It also has tightly integrated graph-plotting features. For example, the function "plot" can be used to produce a graph from two vectors "x" and "y". The code:

produces the following figure of the sine function:

A MATLAB program can produce three-dimensional graphics using the functions "surf", "plot3" or "mesh".
In MATLAB, graphical user interfaces can be programmed with the GUI design environment (GUIDE) tool.

MATLAB can call functions and subroutines written in the programming languages C or Fortran. A wrapper function is created allowing MATLAB data types to be passed and returned. MEX files (MATLAB executables) are the dynamically loadable object files created by compiling such functions. Since 2014 increasing two-way interfacing with Python was being added.

Libraries written in Perl, Java, ActiveX or .NET can be directly called from MATLAB, and many MATLAB libraries (for example XML or SQL support) are implemented as wrappers around Java or ActiveX libraries. Calling MATLAB from Java is more complicated, but can be done with a MATLAB toolbox which is sold separately by MathWorks, or using an undocumented mechanism called JMI (Java-to-MATLAB Interface), (which should not be confused with the unrelated Java Metadata Interface that is also called JMI). Official MATLAB API for Java was added in 2016.

As alternatives to the MuPAD based Symbolic Math Toolbox available from MathWorks, MATLAB can be connected to Maple or Mathematica.

Libraries also exist to import and export MathML.

MATLAB is a proprietary product of MathWorks, so users are subject to vendor lock-in. Although MATLAB Builder products can deploy MATLAB functions as library files which can be used with .NET or Java application building environment, future development will still be tied to the MATLAB language.

Each toolbox is purchased separately. If an evaluation license is requested, the MathWorks sales department requires detailed information about the project for which MATLAB is to be evaluated. If granted (which it often is), the evaluation license is valid for two to four weeks. A student version of MATLAB is available as is a home-use license for MATLAB, Simulink, and a subset of Mathwork's Toolboxes at substantially reduced prices.

It has been reported that European Union (EU) competition regulators are investigating whether MathWorks refused to sell licenses to a competitor. The regulators dropped the investigation after the complainant withdrew its accusation and no evidence of wrongdoing was found.

MATLAB has a number of competitors. Commercial competitors include Mathematica, TK Solver, Maple, and IDL. There are also free open source alternatives to MATLAB, in particular GNU Octave, Scilab, FreeMat, and SageMath, which are intended to be mostly compatible with the MATLAB language; the Julia programming language also initially used MATLAB-like syntax. Among other languages that treat arrays as basic entities (array programming languages) are APL, Fortran 90 and higher, S-Lang, as well as the statistical languages R and S. There are also libraries to add similar functionality to existing languages, such as IT++ for C++, Perl Data Language for Perl, ILNumerics for .NET, NumPy/SciPy/matplotlib for Python, SciLua/Torch for Lua, SciRuby for Ruby, and Numeric.js for JavaScript.

GNU Octave is unique from other alternatives because it treats incompatibility with MATLAB as a bug (see MATLAB Compatibility of GNU Octave), therefore, making GNU Octave a superset of the MATLAB language.

The number (or release number) is the version reported by Concurrent License Manager program FLEXlm.

For a complete list of changes of both MATLAB and official toolboxes, consult the MATLAB release notes.






Several easter eggs exist in MATLAB. These include hidden pictures, and jokes. For example, typing in "spy" used to generate a picture of the spies from Spy vs Spy, but now displays an image of a dog. Typing in "why" randomly outputs a philosophical answer. Other commands include "penny", "toilet", "image", and "life". Not every Easter egg appears in every version of MATLAB.




</doc>
<doc id="20414" url="https://en.wikipedia.org/wiki?curid=20414" title="Meuse">
Meuse

The Meuse (; ; Walloon: "Moûze" ) or Maas ( ; or "Maas") is a major European river, rising in France and flowing through Belgium and the Netherlands before draining into the North Sea from the Rhine–Meuse–Scheldt delta. It has a total length of .

From 1301 the upper Meuse roughly marked the western border of the Holy Roman Empire with the Kingdom of France, after Count Henry III of Bar had to receive the western part of the County of Bar ("Barrois mouvant") as a French fief from the hands of King Philip IV. The border remained stable until the annexation of the Three Bishoprics Metz, Toul and Verdun by King Henry II in 1552 and the occupation of the Duchy of Lorraine by the forces of King Louis XIII in 1633. Its lower Belgian (Walloon) portion, part of the sillon industriel, was the first fully industrialized area in continental Europe. 

The Afgedamde Maas was created in the late Middle Ages, when a major flood made a connection between the Maas and the Merwede at the town of Woudrichem. From that moment on, the current Afgedamde Maas was the main branch of the lower Meuse. The former main branch eventually silted up and is today called the Oude Maasje. In the late 19th century and early 20th century the connection between the Maas and Rhine was closed off and the Maas was given a new, artificial mouth - the Bergse Maas. The resulting separation of the rivers Rhine and Maas reduced the risk of flooding and is considered to be the greatest achievement in Dutch hydraulic engineering before the completion of the Zuiderzee Works and Delta Works. The former main branch was, after the dam at its southern inlet was completed in 1904, renamed "Afgedamde Maas" and no longer receives water from the Maas.

The Meuse and its crossings were a key objective of the last major German WWII counter-offensive on the Western Front, the Battle of the Bulge (Battle of the Ardennes) in the winter of 1944/45.

The Meuse is represented in the documentary "The River People" released in 2012 by Xavier Istasse.

The name "Meuse" is derived from the French name of the river which ultimately derives from the Celtic or Proto-Celtic name *"Mosā".

The Dutch name "Maas" descends from Middle Dutch "Mase", which comes from the presumed but unattested Old Dutch form *"Masa", from Proto-Germanic *"Masō". Modern Dutch and German "Maas" and Limburgish "Maos" preserve this Germanic form. Despite the similarity, the Germanic name is not derived from the Celtic name, judging from the change from earlier "o" into "a", which is characteristic of the Germanic languages.

The Meuse rises in Pouilly-en-Bassigny, commune of Le Châtelet-sur-Meuse on the Langres plateau in France from where it flows northwards past Sedan (the head of navigation) and Charleville-Mézières into Belgium.

At Namur it is joined by the Sambre. Beyond Namur the Meuse winds eastwards, skirting the Ardennes, and passes Liège before turning north. The river then forms part of the Belgian-Dutch border, except that at Maastricht the border lies further to the west. In the Netherlands it continues northwards through Venlo closely along the border to Germany, then turns towards the west, where it runs parallel to the Waal and forms part of the extensive Rhine–Meuse–Scheldt delta, together with the Scheldt in its south and the Rhine in the north. The river has been divided near Heusden into the Afgedamde Maas on the right and the Bergse Maas on the left. The Bergse Maas continues under the name of Amer, which is part of De Biesbosch. The Afgedamde Maas joins the Waal, the main stem of the Rhine at Woudrichem, and then flows under the name of Boven Merwede to Hardinxveld-Giessendam, where it splits into Nieuwe Merwede and Beneden Merwede. Near Lage Zwaluwe, the Nieuwe Merwede joins the Amer, forming the Hollands Diep, which splits into Grevelingen and Haringvliet, before finally flowing into the North Sea.

The Meuse is crossed by railway bridges between the following stations (on the left and right banks respectively):

There are also numerous road bridges and around 32 ferry crossings.

The Meuse is navigable over a substantial part of its total length: In the Netherlands and Belgium, the river is part of the major inland navigation infrastructure, connecting the Rotterdam-Amsterdam-Antwerp port areas to the industrial areas upstream: 's-Hertogenbosch, Venlo, Maastricht, Liège, Namur. Between Maastricht and Maasbracht, an unnavigable section of the Meuse is bypassed by the 36 km Juliana Canal. South of Namur, further upstream, the river can only carry more modest vessels, although a barge as long as 100 m. can still reach the French border town of Givet.

From Givet, the river is canalized over a distance of 272 kilometres. The canalized Meuse used to be called the "Canal de l'Est — Branche Nord" but was recently rebaptized into "Canal de la Meuse". The waterway can be used by the smallest barges that are still in use commercially (almost 40 metres long and just over 5 metres wide). Just upstream of the town of Commercy, the Canal de la Meuse connects with the Marne–Rhine Canal by means of a short diversion canal.

The Cretaceous sea reptile Mosasaur is named after the river Meuse. The first fossils of it were discovered outside Maastricht 1780.

An international agreement was signed in 2002 in Ghent, Belgium about the management of the river amongst France, Germany, Luxembourg, the Netherlands, and Belgium. Also participating in the agreement were the Belgian regional governments of Flanders, Wallonia, and Brussels (which is not in the basin of the Meuse but pumps running water into the Meuse).

Most of the basin area (approximately 36,000 km) is in Wallonia (12,000 km), followed by France (9,000 km), the Netherlands (8,000 km), Germany (2,000 km), Flanders (2,000 km) and Luxembourg (a few km).

An International Commission on the Meuse has the responsibility of the implementation of the treaty.

The costs of this Commission are met by all these countries, in proportion of their own territory into the basin of the Meuse: Netherlands and Wallonia 30%, France 15%, Germany 14.5%, Flanders 5%, Brussels 4.5%, Kingdom of Belgium and Luxemburg 0.5%.

The map of the basin area of Meuse was joined to the text of the treaty.

On the cultural plan, the river Meuse, as a major communication route, is the origin of the Mosan art, principally (Wallonia and France).

The first landscape painted in the Middle-Age was the landscape of Meuse. For instance Joachim Patinir He was likely the uncle of Henri Blès who is sometimes defined as a Mosan landscape painter active during the second third of the 16th century (i.e., second generation of landscape painters) 

The main tributaries of the Meuse are listed below in downstream-upstream order, with the town where the tributary meets the river:

The mean annual discharge rate of the Meuse has been relatively stable over the last few thousand years. One recent study estimates that average flow has increased about 10% since 2000 BC. The hydrological distribution of the Meuse changed during the later Middle Ages, when a major flood forced it to shift its main course northwards towards the river Merwede. From then on, several stretches of the original Merwede were named "Maas" (i.e. Meuse) instead and served as the primary outflow of that river. Those branches are currently known as the Nieuwe Maas and Oude Maas.

However, during another series of severe floods the Meuse found an additional path towards the sea, resulting in the creation of the Biesbosch wetlands and Hollands Diep estuaries. Thereafter, the Meuse split near Heusden into two main distributaries, one flowing north to join the Merwede, and one flowing directly to the sea. The branch of the Meuse leading directly to the sea eventually silted up, (and now forms the Oude Maasje stream), but in 1904 the canalised Bergse Maas was dug to take over the functions of the silted-up branch. At the same time, the branch leading to the Merwede was dammed at Heusden, (and has since been known as the Afgedamde Maas) so that little water from the Meuse entered the old Maas courses, or the Rhine distributaries. The resulting separation of the rivers Rhine and Meuse is considered to be the greatest achievement in Dutch hydraulic engineering before the completion of the Zuiderzee Works and Delta Works. In 1970 the Haringvlietdam has been finished. Since then the reunited Rhine and Meuse waters reach the North Sea either at this site or, during times of lower discharges of the Rhine, at Hoek van Holland.

A 2008 study notes that the difference between summer and winter flow volumes has increased significantly in the last 100–200 years. These workers point out that the frequency of serious floods ("i.e." flows > 1000% of normal) has increased markedly. They predict that winter flooding of the Meuse may become a recurring problem in the coming decades.

The Meuse flows through the following departments of France, provinces of Belgium, provinces of the Netherlands and towns:

The Meuse ("Maas") is mentioned in the first stanza of the Germany's old national anthem, the "Deutschlandlied". However, since German reunification in 1989, only the third stanza of the "Deutschlandlied" has been sung as the German national anthem: the first and second stanzas being omitted. The lyrics written in 1841 describe a then–disunited Germany with the river as its western boundary, where King William I of the Netherlands had joined the German Confederation with his Duchy of Limburg in 1839. Though the duchy's territory officially became an integral part of the Netherlands by the 1867 Treaty of London, the text passage remained unchanged when the "Deutschlandlied" was declared the national anthem of the Weimar Republic in 1922.




</doc>
<doc id="20418" url="https://en.wikipedia.org/wiki?curid=20418" title="Michael Bentine">
Michael Bentine

Michael Bentine, CBE (born Michael James Bentin; 26 January 1922 – 26 November 1996) was an English comedian, comic actor and founding member of the Goons. He was a Peruvian Briton. In 1971, Bentine received the Order of Merit of Peru following his fund-raising work for the 1970 Great Peruvian earthquake.

Bentine was born in Watford, Hertfordshire, to a Peruvian father, Adam Bentin, and a British mother, Florence Dawkins, and grew up in Folkestone, Kent. He was educated at Eton College. With the help of speech trainer, Harry Burgess, he overcame a stammer and subsequently developed an interest in amateur theatricals, along with the Tomlinson family, including the young David Tomlinson. He spoke fluent Spanish and French. 

His father was an early aeronautical engineer for the Sopwith Aviation Company during and after World War I and invented a tensometer for setting the tension on aircraft rigging wires.

In World War II, he volunteered for all services when the war broke out (the RAF was his first choice owing to the influence of his father's experience), but was initially rejected because of his father's nationality.

He started his acting career in 1940, in a touring company in Cardiff playing a juvenile lead in "Sweet Lavender". He went on to join Robert Atkin's Shakespearean company in Regent's Park, London, until he was called up for service in the RAF. He was appearing in a Shakespearean play in doublet and hose in the open-air theatre in London's Hyde Park when two RAF MPs marched on stage and arrested him for desertion. Unknown to him, an RAF conscription notice had been following him for a month as his company toured.

Once in the RAF he went through flying training. He was the penultimate man going through a medical line receiving inoculations for typhoid with the other flight candidates in his class (they were going to Canada to receive new aircraft) when the vaccine ran out. They refilled the bottle to inoculate him and the other man as well. By mistake they loaded a pure culture of typhoid. The other man died immediately, and Bentine was in a coma for six weeks. When he regained consciousness his eyesight was ruined, leaving him myopic for the rest of his life. Since he was no longer physically qualified for flying, he was transferred to RAF Intelligence and seconded to MI9, a unit that was dedicated to supporting resistance movements and helping prisoners escape. His immediate superior was the Colditz escapee Airey Neave.

At the end of the war, he took part in the liberation of Bergen-Belsen concentration camp. He said about this experience:

After the war he decided to become a comedian and worked in the Windmill Theatre where he met Harry Secombe. He specialised in off-the-wall humour, often involving cartoons and other types of animation. His acts included giving lectures in an invented language called Slobodian, "Imaginative Young Man with a Walking Stick" and "The Chairback", with a broken chairback having a number of uses from comb to machine gun and taking on a demonical life of its own. Peter Sellers told him this was the inspiration for the prosthetic arm routine in "Dr Strangelove". This act led to his engagement by Val Parnell to appear in the Starlight Roof revues starring Vic Oliver, where he met and married his second wife Clementina, with whom he had four children. Also on the bill were Fred Emney and a young Julie Andrews.

He co-founded "The Goon Show" radio show with Spike Milligan, Peter Sellers and Harry Secombe, but appeared in only the first 38 shows on the BBC Light Programme from 1951 to 1953. The first of these shows were actually called "Crazy People" and subtitled "The Junior Crazy Gang"; the term "Goon" was used as the headline of a review of Bentine's act by "Picture Post" dated 5 November 1948. Only one of this first series (and very few of the following three in which he did not appear) has survived, the rest of the original disc recordings having apparently been destroyed or discarded as no longer usable, so there is almost no record of his work as a radio "Goon". He also appeared in the "Goon Show" film "Down Among the Z Men".

In 1951 Bentine was invited to the United States to appear on "The Ed Sullivan Show". On his return he parted amicably from his partners and continued touring in variety, remaining close to Secombe and Sellers for the rest of his life. In 1972, Secombe and Sellers told Michael Parkinson that Bentine was "always calling everyone a genius" and, since he was the only one of the four with a "proper education", they always believed him.

His first appearances on television were as presenter on a 13-part children's series featuring remote controlled puppets, "The Bumblies", which he also devised, designed and wrote. These were three small creatures from outer space who slept on "Professor Bentine's" ceiling and who had come to Earth to learn the ways of Earthling children. Angelo de Calferta modelled the puppets from Bentine's designs and Richard Dendy moulded them in latex rubber. He sold the series to the BBC for less than they had cost to make. He then spent two years touring in Australia (1954–55).
On his return to Britain in 1954, he found he faced hostility for having left the Goons to the extent that his picture had been excised from early pictures and wasn't replaced until Bentine complained to BBC Director-General Michael Checkland in the 1990s. He worked as a scriptwriter for Peter Sellers and then on 39 episodes of his own radio show "Round the Bend in 30 Minutes", which has also been wiped from the BBC archive. He then teamed up with Dick Lester to devise a series of six TV programmes "Before Midnight" for Associated British Corporation (ABC) in Birmingham in 1958. This led to a 13-programme series called "After Hours" in which he appeared alongside Dick Emery, Clive Dunn, David Lodge, Joe Gibbons and Benny Lee. The show featured the "olde English sport of drats, later known as nurdling". Some of the sketches were adapted into a stage revue, "Don't Shoot, We're British". He also appeared in the film comedy "Raising a Riot", starring Kenneth More, which featured his five-year-old daughter "Fusty". He joked that she got better billing.

From 1960 to 1964, he had a television series, "It's a Square World", which won a BAFTA award in 1962 and Grand Prix de la Presse at Montreux in 1963. A prominent feature of the series was the imaginary flea circus where plays were enacted on tiny sets using nothing but special effects to show the movement of things too small to see and sounds with Bentine's commentary. The plays were not serious. One, titled "The Beast of the Black Bog Tarn", was set in a (miniature) haunted house.

He was the subject of "This Is Your Life" in April 1963 when he was surprised by Eamonn Andrews at the BBC Television Theatre.

In 1969-70 he was presenter of "The Golden Silents" on BBC TV, which attempted authentic showings of silent films, without the commentaries with which they were usually shown on television before then.

From 1974 to 1980 he wrote, designed, narrated and presented the children's television programme "Michael Bentine's Potty Time" and made one-off comedy specials.

From January to May 1984 Bentine put out 11 half-hour episodes, in two series, of "The Michael Bentine Show" on Radio 4. These have subsequently been repeated, several times, on the BBC's archive radio station BBC7 (now BBC Radio 4 Extra).

He was also the best-selling writer of 16 novels, comedies and non-fiction books. Four of his books, "The Long Banana Skin" (1975), "The Door Marked Summer" (1981), "Doors to the Mind" and "The Reluctant Jester" (1992) are autobiographical.

In 1968, travelling on the British Hovercraft Corporation (BHC) SR.N6, "GH2012", he took part in the first hovercraft expedition up the River Amazon.
In 1995, Bentine received a CBE from Queen Elizabeth II "for services to entertainment". He was also a holder of the Peruvian Order of Merit, as was his grandfather, Don , for his work leading the fundraising for the Peruvian Earthquake Appeal.

Bentine was a crack pistol shot and helped to start the idea of a counter-terrorist wing within 22 SAS Regiment. In doing so, he became the first non-SAS person ever to fire a gun inside the close-quarters battle training house at Hereford.

His interests included parapsychology. This was as a result of his and his family's extensive research into the paranormal, which resulted in his writing "The Door Marked Summer" and "The Doors of the Mind". He was, for the final years of his life, president of the Association for the Scientific Study of Anomalous Phenomena.

Bentine was also interested in science. On 14 December 1977, he appeared with Arthur C. Clarke on Patrick Moore's BBC "The Sky At Night" programme. The broadcast was entitled "Suns, Spaceships and Bug-Eyed Monsters" - a light-hearted look at how science fiction had become science fact, as well as how ideas of space travel had become reality through the 20th century. In the opening of the programme, Patrick Moore introduces Bentine with Bentine confirming that he was the possessor of a "Readers Digest Degree." This remark was typical of Bentine's comic overtones to most things in life whilst ensuring that he hid his undoubted talent and knowledge of science. Bentine appeared in a subsequent broadcast on a similar theme with Patrick Moore in 1980. Following the death of Arthur C. Clarke, "BBC Sky At Night" magazine released a copy of the 1977 archive programme on the cover of their May 2008 edition.

Bentine was married twice, remaining with his second wife, Clementina Stuart, a Royal Ballet dancer, for over fifty years. He had a child from his first marriage, Elaine, from whom he had a granddaughter, Marie Laurence, and three great-grandsons, William, Arthur and Nicholas. His children from his second marriage were better known by their family nicknames than their birth names: Gus (real name Stewart), Fusty (real name Marylla), Suki (real name Serena) and Peski (real name Richard). Two of his five children, his eldest daughters, died from cancer (breast cancer and lymphoma), while his elder son, Gus, was killed when a Piper PA-18 Super Cub, registration G-AYPN, crashed into a hillside at Ditcham Park Woods near Petersfield, Hampshire, on 28 August 1971. His body, together with that of his pilot friend Andy, and the aircraft, was found on 31 October 1971. The AAIB after an 11-month investigation found that the aircraft went into cloud when taking action to avoid power cables while flying low in poor visibility and subsequently went out of control.

Bentine's subsequent investigation into regulations governing private airfields resulted in his writing a report for Special Branch into the use of personal aircraft in smuggling operations. He fictionalised much of the material in his novel "Lords of the Levels".

From 1975 until his death in 1996, he and his wife spent their winters at a second home in Palm Springs, California, US.

Shortly before his death from prostate cancer at the age of 74, he was visited in hospital by Prince Charles, who was a close friend, as well as a devoted fan of the Goons.

Some of the programmes Bentine appeared in were:





</doc>
<doc id="20419" url="https://en.wikipedia.org/wiki?curid=20419" title="Mania">
Mania

Mania, also known as manic syndrome, is a state of abnormally elevated arousal, affect, and energy level, or "a state of heightened overall activation with enhanced affective expression together with lability of affect." Although mania is often conceived as a "mirror image" to depression, the heightened mood can be either euphoric or irritable; indeed, as the mania intensifies, irritability can be more pronounced and result in violence, or anxiety.

The symptoms of mania include heightened mood (either euphoric or irritable); flight of ideas and pressure of speech; and increased energy, decreased need for sleep, and hyperactivity. They are most plainly evident in fully developed hypomanic states; in full-blown mania, however, they undergo progressively severe exacerbations and become more and more obscured by other signs and symptoms, such as delusions and fragmentation of behavior.
Mania is a syndrome with multiple causes. Although the vast majority of cases occur in the context of bipolar disorder, it is a key component of other psychiatric disorders (such as schizoaffective disorder, bipolar type) and may also occur secondary to various general medical conditions, such as multiple sclerosis; certain medications may perpetuate a manic state, for example prednisone; or substances of abuse, such as cocaine or anabolic steroids. In the current DSM-5, hypomanic episodes are separated from the more severe full manic episodes, which, in turn, are characterized as either mild, moderate, or severe, with specifiers in regard to certain symptomatic features (e.g. catatonia, psychosis). Mania is divided into three stages: hypomania, or stage I; acute mania, or stage II; and delirious mania (delirium), or stage III. This "staging" of a manic episode is very useful from a descriptive and differential diagnostic point of view.

Mania varies in intensity, from mild mania (hypomania) to delirious mania, marked by such symptoms as disorientation, florid psychosis, incoherence, and catatonia. Standardized tools such as Altman Self-Rating Mania Scale and Young Mania Rating Scale can be used to measure severity of manic episodes. Because mania and hypomania have also long been associated with creativity and artistic talent, it is not always the case that the clearly manic bipolar person needs or wants medical help; such persons often either retain sufficient self-control to function normally or are unaware that they have "gone manic" severely enough to be committed or to commit themselves. Manic persons often can be mistaken for being under the influence of drugs.

In a mixed affective state, the individual, though meeting the general criteria for a hypomanic (discussed below) or manic episode, experiences three or more concurrent depressive symptoms. This has caused some speculation, among clinicians, that mania and depression, rather than constituting "true" polar opposites, are, rather, two independent axes in a unipolar—bipolar spectrum.

A mixed affective state, especially with prominent manic symptoms, places the patient at a greater risk for completed suicide. Depression on its own is a risk factor but, when coupled with an increase in energy and goal-directed activity, the patient is far more likely to act with violence on suicidal impulses.

Hypomania is a lowered state of mania that does little to impair function or decrease quality of life. It may, in fact, increase productivity and creativity. In hypomania, there is less need for sleep and both goal-motivated behaviour and metabolism increase. Though the elevated mood and energy level typical of hypomania could be seen as a benefit, mania itself generally has many undesirable consequences including suicidal tendencies, and hypomania can, if the prominent mood is irritable rather than euphoric, be a rather unpleasant experience.

A single manic episode, in the absence of secondary causes, (i.e., substance use disorder, pharmacologic, general medical condition) is sufficient to diagnose bipolar I disorder. Hypomania may be indicative of bipolar II disorder. Manic episodes are often complicated by delusions and/or hallucinations; should the psychotic features persist for a duration significantly longer than the episode of mania (two weeks or more), a diagnosis of schizoaffective disorder is more appropriate. Certain of "obsessive-compulsive spectrum" disorders as well as impulse control disorders share the name "mania," namely, kleptomania, pyromania, and trichotillomania. Despite the unfortunate association implied by the name, however, no connection exists between mania or bipolar disorder and these disorders.
B deficiency can also cause characteristics of mania and psychosis.

Hyperthyroidism can produce similar symptoms to those of mania, such as agitation, elevated mood, increased energy, hyperactivity, sleep disturbances and sometimes, especially in severe cases, psychosis.

A "manic episode" is defined in the American Psychiatric Association's diagnostic manual as a "distinct period of abnormally and persistently elevated, expansive, or irritable mood and abnormally and persistently increased activity or energy, lasting at least 1 week and present most of the day, nearly every day (or any duration if hospitalization is necessary)," where the mood is not caused by drugs/medication or a medical illness (e.g., hyperthyroidism), and (a) is causing obvious difficulties at work or in social relationships and activities, or (b) requires admission to hospital to protect the person or others, or (c) the person is suffering psychosis.

To be classed as a manic episode, while the disturbed mood and an increase in goal directed activity or energy is present at least three (or four if only irritability is present) of the following must have been consistently present:

Though the activities one participates in while in a manic state are not "always" negative, those with the potential to have negative outcomes are far more likely.

If the person is concurrently depressed, they are said to be having a mixed episode.

The World Health Organization's classification system defines "a manic episode" as one where mood is higher than the person's situation warrants and may vary from relaxed high spirits to barely controllable exuberance, accompanied by hyperactivity, a compulsion to speak, a reduced sleep requirement, difficulty sustaining attention and often increased distractibility. Frequently, confidence and self-esteem are excessively enlarged, and grand, extravagant ideas are expressed. Behavior that is out of character and risky, foolish or inappropriate may result from a loss of normal social restraint.

Some people also have physical symptoms, such as sweating, pacing, and weight loss. In full-blown mania, often the manic person will feel as though his or her goal(s) trump all else, that there are no consequences or that negative consequences would be minimal, and that they need not exercise restraint in the pursuit of what they are after. Hypomania is different, as it may cause little or no impairment in function. The hypomanic person's connection with the external world, and its standards of interaction, remain intact, although intensity of moods is heightened. But those who suffer from prolonged unresolved hypomania do run the risk of developing full mania, and indeed may cross that "line" without even realizing they have done so.

One of the signature symptoms of mania (and to a lesser extent, hypomania) is what many have described as racing thoughts. These are usually instances in which the manic person is excessively distracted by objectively unimportant stimuli. This experience creates an absent-mindedness where the manic individual's thoughts totally preoccupy him or her, making him or her unable to keep track of time, or be aware of anything besides the flow of thoughts. Racing thoughts also interfere with the ability to fall asleep.

Manic states are always relative to the normal state of intensity of the afflicted individual; thus, already irritable patients may find themselves losing their tempers even more quickly and an academically gifted person may, during the hypomanic stage, adopt seemingly "genius" characteristics and an ability to perform and articulate at a level far beyond that which would be capable during euthymia. A very simple indicator of a manic state would be if a heretofore clinically depressed patient suddenly becomes inordinately energetic, cheerful, aggressive, or "over happy." Other, often less obvious, elements of mania include delusions (generally of either grandeur or persecution, according to whether the predominant mood is euphoric or irritable), hypersensitivity, hyper vigilance, hypersexuality, hyper-religiosity, hyperactivity and impulsivity, a compulsion to over explain, (typically accompanied by pressure of speech) grandiose schemes and ideas, and a decreased need for sleep (for example, feeling rested after only 3 or 4 hours of sleep); in the case of the latter, the eyes of such patients may both look and feel abnormally "wide" or "open," rarely blinking, and this often contributing to some clinicians’ erroneous belief that these patients are under the influence of a stimulant drug, when the patient, in fact, is either not on any mind-altering substances or is actually on a depressant drug, in a misguided attempt to ward off any undesirable manic symptoms. Individuals may also engage in out-of-character behavior during the episode, such as questionable business transactions, wasteful expenditures of money (e.g., spending sprees), risky sexual activity, abuse of recreational substances, excessive gambling, reckless behavior (as "speed driving" or daredevil activity), abnormal social interaction (as manifest via, for example, over familiarity and conversing with strangers), or highly vocal arguments. These behaviours may increase stress in personal relationships, lead to problems at work and increase the risk of altercations with law enforcement. There is a high risk of impulsively taking part in activities potentially harmful to self and others.

Although "severely elevated mood" sounds somewhat desirable and enjoyable, the experience of mania is ultimately often quite unpleasant and sometimes disturbing, if not frightening, for the person involved and for those close to them, and it may lead to impulsive behaviour that may later be regretted. It can also often be complicated by the sufferer's lack of judgment and insight regarding periods of exacerbation of characteristic states. Manic patients are frequently grandiose, obsessive, impulsive, irritable, belligerent, and frequently deny anything is wrong with them. Because mania frequently encourages high energy and decreased perception of need or ability to sleep, within a few days of a manic cycle, sleep-deprived psychosis may appear, further complicating the ability to think clearly. Racing thoughts and misperceptions lead to frustration and decreased ability to communicate with others.

Mania may also, as earlier mentioned, be divided into three “stages.” Stage I corresponds with hypomania and may feature typical hypomanic characteristics, such as gregariousness and euphoria. In stages II and III mania, however, the patient may be extraordinarily irritable, psychotic or even delirious. These latter two stages are referred to as acute and delirious (or Bell’s), respectively.

Various triggers have been associated with switching from euthymic or depressed states into mania. One common trigger of mania is antidepressant therapy. Studies show that the risk of switching while on an antidepressant is between 6-69 percent. Dopaminergic drugs such as reuptake inhibitors and dopamine agonists may also increase risk of switch. Other medication possibly include glutaminergic agents and drugs that alter the HPA axis. Lifestyle triggers include irregular sleep wake schedules and sleep deprivation, as well as extremely emotional or stressful stimuli.

Various genes that have been implicated in genetic studies of bipolar have been manipulated in preclinical animal models to produce syndromes reflecting different aspects of mania. CLOCK and DBP polymorphisms have been linked to bipolar in population studies, and behavioral changes induced by knockout are reversed by lithium treatment. Metabotropic glutamate receptor 6 has been genetically linked to bipolar, and found to be under-expressed in the cortex. Pituitary adenylate cyclase-activating peptide has been associated with bipolar in gene linkage studies, and knockout in mice produces mania like-behavior. Targets of various treatments such as GSK-3, and ERK1 have also demonstrated mania like behavior in preclinical models.

Mania may be associated with strokes, especially cerebral lesions in the right hemisphere.

Deep brain stimulation of the subthalamic nucleus in Parkinson's disease has been associated with mania, especially with electrodes placed in the ventromedial STN. A proposed mechanism involves increased excitatory input from the STN to dopaminergic nuclei.

Mania can also be caused by physical trauma or illness. When the causes are physical, it is called secondary mania.

The mechanism underlying mania is unknown, but the neurocognitive profile of mania is highly consistent with dysfunction in the right prefrontal cortex, a common finding in neuroimaging studies. Various lines of evidence from post-mortem studies and the putative mechanisms of anti-manic agents point to abnormalities in GSK-3, dopamine, Protein kinase C and Inositol monophosphatase.

Meta analysis of neuroimaging studies demonstrate increased thalamic activity, and bilaterally reduced inferior frontal gyrus activation. Activity in the amygdala and other subcortical structures such as the ventral striatum tend to be increased, although results are inconsistent and likely dependent upon task characteristics such as valence. Reduced functional connectivity between the ventral prefrontal cortex and amygdala along with variable findings supports a hypothesis of general dysregulation of subcortical structures by the prefrontal cortex. A bias towards positively valanced stimuli, and increased responsiveness in reward circuitry may predispose towards mania. Mania tends to be associated with right hemisphere lesions, while depression tends to be associated with left hemisphere lesions.

Post-mortem examinations of bipolar disorder demonstrate increased expression of Protein Kinase C (PKC). While limited, some studies demonstrate manipulation of PKC in animals produces behavioral changes mirroring mania, and treatment with PKC inhibitor tamoxifen(also an anti-estrogen drug) demonstrates antimanic effects. Traditional antimanic drugs also demonstrate PKC inhibiting properties, among other effects such as GSK3 inhibition.

Manic episodes may be triggered by dopamine receptor agonists, and this combined with tentative report of increased VMAT2 activity, measured via PET scans of radioligand binding, suggest a role of dopamine in mania. Decreased cerebrospinal fluid levels of the serotonin metabolite 5-HIAA have been found in manic patients too, which may be explained by a failure of serotonergic regulation and dopaminergic hyperactivity.

Limited evidence suggests that mania is associated with behavioral reward hypersensitivty, as well as with neural reward hypersensitivity. Electrophysiological evidence supporting this comes from studies associating left frontal EEG activity with mania. As left frontal EEG activity generally though to be a reflection of behavioral activation system activity, this is thought to support a role for reward hypersensitivity in mania. Tentative evidence also comes from one study that reported an association between manic traits and feedback negativity during receipt of monetary reward or loss. Neuroimaging evidence during acute mania is sparse, but one study reported elevated orbitofrontal cortex activity to monetary reward, and another study reported elevated striatal activity to reward omission. The latter finding was interpreted in the context of either elevated baseline activity(resulting in a null finding of reward hypersensitivity), or reduced ability to discriminate between reward and punishment, still supporting reward hyperactivity in mania. Punishment hyposensitivity, as reflected in a number of neuroimaging studies as reduced lateral orbitofrontal response to punishment, has been proposed as a mechanism of reward hypersensitivty in mania.

In the ICD-10 there are several disorders with the manic syndrome: organic manic disorder (), mania without psychotic symptoms (), mania with psychotic symptoms (), other manic episodes (), unspecified manic episode (), manic type of schizoaffective disorder (), bipolar affective disorder, current episode manic without psychotic symptoms (), bipolar affective disorder, current episode manic with psychotic symptoms ().

Before beginning treatment for mania, careful differential diagnosis must be performed to rule out secondary causes.

The acute treatment of a manic episode of bipolar disorder involves the utilization of either a mood stabilizer (valproate, lithium, or carbamazepine) or an atypical antipsychotic (olanzapine, quetiapine, risperidone, or aripiprazole). Although hypomanic episodes may respond to a mood stabilizer alone, full-blown episodes are treated with an atypical antipsychotic (often in conjunction with a mood stabilizer, as these tend to produce the most rapid improvement).

When the manic behaviours have gone, long-term treatment then focuses on prophylactic treatment to try to stabilize the patient's mood, typically through a combination of pharmacotherapy and psychotherapy. The likelihood of having a relapse is very high for those who have experienced two or more episodes of mania or depression. While medication for bipolar disorder is important to manage symptoms of mania and depression, studies show relying on medications alone is not the most effective method of treatment. Medication is most effective when used in combination with other bipolar disorder treatments, including psychotherapy, self-help coping strategies, and healthy lifestyle choices.

Lithium is the classic mood stabilizer to prevent further manic and depressive episodes. A systematic review found that long term lithium treatment substantially reduces the risk of bipolar manic relapse, by 42%. Anticonvulsants such as valproate, oxcarbazepine and carbamazepine are also used for prophylaxis. More recent drug solutions include lamotrigine, which is another anticonvulsant. Clonazepam (Klonopin) is also used. Sometimes atypical antipsychotics are used in combination with the previous mentioned medications as well, including olanzapine (Zyprexa) which helps treat hallucinations or delusions, Asenapine (Saphris, Sycrest), aripiprazole (Abilify), risperidone, ziprasidone, and clozapine which is often used for people who do not respond to lithium or anticonvulsants.

Verapamil, a calcium-channel blocker, is useful in the treatment of hypomania and in those cases where lithium and mood stabilizers are contraindicated or ineffective. Verapamil is effective for both short-term and long-term treatment.

Antidepressant monotherapy is not recommended for the treatment of depression in patients with bipolar disorders I or II, and no benefit has been demonstrated by combining antidepressants with mood stabilizers in these patients.

In "Electroboy: A Memoir of Mania" by Andy Behrman, he describes his experience of mania as "the most perfect prescription glasses with which to see the world...life appears in front of you like an oversized movie screen". Behrman indicates early in his memoir that he sees himself not as a person suffering from an uncontrollable disabling illness, but as a director of the movie that is his vivid and emotionally alive life. "When I'm manic, I'm so awake and alert, that my eyelashes fluttering on the pillow sound like thunder". Many people who are artistic and do art in various forms have mania.
Winston Churchill had periods of "manic symptoms" that may have been both an asset and a liability.

The nosology of the various stages of a manic episode has changed over the decades. The word derives from the Ancient Greek μανία ("manía"), "madness, frenzy" and the verb μαίνομαι ("maínomai"), "to be mad, to rage, to be furious".




</doc>
<doc id="20420" url="https://en.wikipedia.org/wiki?curid=20420" title="Multimedia">
Multimedia

Multimedia is content that uses a combination of different content forms such as text, audio, images, animations, video and interactive content. Multimedia contrasts with media that use only rudimentary computer displays such as text-only or traditional forms of printed or hand-produced material.

Multimedia can be recorded and played, displayed, interacted with or accessed by information content processing devices, such as computerized and electronic devices, but can also be part of a live performance. Multimedia devices are electronic media devices used to store and experience multimedia content. Multimedia is distinguished from mixed media in fine art; for example, by including audio it has a broader scope. In the early years of multimedia the term "rich media" was synonymous with interactive multimedia, and "hypermedia" was an application of multimedia.

The term "multimedia" was coined by singer and artist Bob Goldstein (later 'Bobb Goldsteinn') to promote the July 1966 opening of his "LightWorks at L'Oursin" show at Southampton, Long Island. Goldstein was perhaps aware of an American artist named Dick Higgins, who had two years previously discussed a new approach to art-making he called "intermedia".

On August 10, 1966, Richard Albarino of "Variety" borrowed the terminology, reporting: "Brainchild of songscribe-comic Bob ('Washington Square') Goldstein, the 'Lightworks' is the latest "multi-media" music-cum-visuals to debut as discothèque fare." Two years later, in 1968, the term "multimedia" was re-appropriated to describe the work of a political consultant, David Sawyer, the husband of Iris Sawyer—one of Goldstein's producers at L'Oursin.

In the 1993 first edition of "Multimedia: Making It Work", Tay Vaughan declared "Multimedia is any combination of text, graphic art, sound, animation, and video that is delivered by computer. When you allow the user – the viewer of the project – to control what and when these elements are delivered, it is "interactive multimedia". When you provide a structure of linked elements through which the user can navigate, interactive multimedia becomes "hypermedia"."

The German language society Gesellschaft für deutsche Sprache recognized the word's significance and ubiquitousness in the 1990s by awarding it the title of German 'Word of the Year' in 1995. The institute summed up its rationale by stating "[Multimedia] has become a central word in the wonderful new media world".

In common usage, "multimedia" refers to an electronically delivered combination of media including video, still images, audio, and text in such a way that can be accessed interactively. Much of the content on the web today falls within this definition as understood by millions. Some computers which were marketed in the 1990s were called "multimedia" computers because they incorporated a CD-ROM drive, which allowed for the delivery of several hundred megabytes of video, picture, and audio data. That era saw also a boost in the production of educational multimedia CD-ROMs.

The term "video", if not used exclusively to describe motion photography, is ambiguous in multimedia terminology. "Video" is often used to describe the file format, delivery format, or presentation format instead of ""footage"" which is used to distinguish motion photography from ""animation"" of rendered motion imagery. Multiple forms of information content are often not considered modern forms of presentation such as audio or video. Likewise, single forms of information content with single methods of information processing (e.g. non-interactive audio) are often called multimedia, perhaps to distinguish static media from active media. In the fine arts, for example, Leda Luss Luyken's ModulArt brings two key elements of musical composition and film into the world of painting: variation of a theme and movement of and within a picture, making "ModulArt" an interactive multimedia form of art. Performing arts may also be considered multimedia considering that performers and props are multiple forms of both content and media.

Multimedia presentations may be viewed by person on stage, projected, transmitted, or played locally with a media player. A broadcast may be a live or recorded multimedia presentation. Broadcasts and recordings can be either analog or digital electronic media technology. Digital online multimedia may be downloaded or streamed. Streaming multimedia may be live or on-demand.

Multimedia games and simulations may be used in a physical environment with special effects, with multiple users in an online network, or locally with an offline computer, game system, or simulator.

The various formats of technological or digital multimedia may be intended to enhance the users' experience, for example to make it easier and faster to convey information. Or in entertainment or art, to transcend everyday experience.
Enhanced levels of interactivity are made possible by combining multiple forms of media content. Online multimedia is increasingly becoming object-oriented and data-driven, enabling applications with collaborative end-user innovation and personalization on multiple forms of content over time. Examples of these range from multiple forms of content on Web sites like photo galleries with both images (pictures) and title (text) user-updated, to simulations whose co-efficients, events, illustrations, animations or videos are modifiable, allowing the multimedia "experience" to be altered without reprogramming. In addition to seeing and hearing, haptic technology enables virtual objects to be felt. Emerging technology involving illusions of taste and smell may also enhance the multimedia experience.

Multimedia may be broadly divided into linear and non-linear categories:

Multimedia presentations can be live or recorded:

Multimedia finds its application in various areas including, but not limited to, advertisements, art, education, entertainment, engineering, medicine, mathematics, business, scientific research and spatial temporal applications. Several examples are as follows:

Creative industries use multimedia for a variety of purposes ranging from fine arts, to entertainment, to commercial art, to journalism, to media and software services provided for any of the industries listed below. An individual multimedia designer may cover the spectrum throughout their career. Request for their skills range from technical, to analytical, to creative.

Much of the electronic old and new media used by commercial artists and graphic designers is multimedia. Exciting presentations are used to grab and keep attention in advertising. Business to business, and interoffice communications are often developed by creative services firms for advanced multimedia presentations beyond simple slide shows to sell ideas or liven up training. Commercial multimedia developers may be hired to design for governmental services and nonprofit services applications as well.

Multimedia is heavily used in the entertainment industry, especially to develop special effects in movies and animations (VFX, 3D animation, etc.). Multimedia games are a popular pastime and are software programs available either as CD-ROMs or online. Some video games also use multimedia features.
Multimedia applications that allow users to actively participate instead of just sitting by as passive recipients of information are called "interactive multimedia".
In the arts there are multimedia artists, whose minds are able to blend techniques using different media that in some way incorporates interaction with the viewer. One of the most relevant could be Peter Greenaway who is melding cinema with opera and all sorts of digital media. Another approach entails the creation of multimedia that can be displayed in a traditional fine arts arena, such as an art gallery. Although multimedia display material may be volatile, the survivability of the content is as strong as any traditional media. Digital recording material may be just as durable and infinitely reproducible with perfect copies every time.

In education, multimedia is used to produce computer-based training courses (popularly called CBTs) and reference books like encyclopedia and almanacs. A CBT lets the user go through a series of presentations, text about a particular topic, and associated illustrations in various information formats. Edutainment is the combination of education with entertainment, especially multimedia entertainment.

Learning theory in the past decade has expanded dramatically because of the introduction of multimedia. Several lines of research have evolved, e.g. cognitive load and multimedia learning.

From multimedia learning (MML) theory, David Roberts has developed a large group lecture practice using PowerPoint and based on the use of full-slide images in conjunction with a reduction of visible text (all text can be placed in the notes view’ section of PowerPoint). The method has been applied and evaluated in 9 disciplines. In each experiment, students’ engagement and active learning has been approximately 66% greater, than with the same material being delivered using bullet points, text and speech, corroborating a range of theories presented by multimedia learning scholars like Sweller and Mayer. The idea of media convergence is also becoming a major factor in education, particularly higher education. Defined as separate technologies such as voice (and telephony features), data (and productivity applications) and video that now share resources and interact with each other, media convergence is rapidly changing the curriculum in universities all over the world.

Multimedia provides students with an alternate means of acquiring knowledge designed to enhance teaching and learning through various mediums and platforms. This technology allows students to learn at their own pace and gives teachers the ability to observe the individual needs of each student. The capacity for multimedia to be used in multi-disciplinary settings is structured around the idea of creating a hands-on learning environment through the use of technology . Lessons can be tailored to the subject matter as well as be personalized to the students' varying levels of knowledge on the topic. Learning content can be managed through activities that utilize and take advantage of multimedia platforms. This kind of learning encourages interactive communication between students and teachers and opens feedback channels, introducing an active learning process especially with the prevalence of new media and social media. Technology has impacted multimedia as it is largely associated with the use of computers or other electronic devices and digital media due to its capabilities concerning research, communication, problem-solving through simulations and feedback opportunities.

Multimedia is a robust education and research methodology within the social work context. The five different multimedia which supports the education process are narrative media, interactive media, communicative media, adaptive media, and productive media. Contrary to long-standing belief, multimedia technology in social work education existed before the prevalence of the internet. It takes the form of images, audio, and video into the curriculum. 

First introduced to social work education by Seabury & Maple in 1993, multimedia technology is utilized to teach social work practice skills including interviewing, crisis intervention, and group work. In comparison with conventional teaching method, including face-to-face courses, multimedia education shortens transportation time, increases knowledge and confidence in a richer and more authentic context for learning, generates interaction between online users, and enhances understanding of conceptual materials for novice students. 

In an attempt to examine the impact of multimedia technology on students’ study, A. Elizabeth Cauble & Linda P. Thurston conducted a research in which Building Family Foundations (BFF), an interactive multimedia training platform, was utilized to assess social work students’ reactions to multimedia technology on variables of knowledge, attitudes, and self-efficacy. The results states that respondents show a substantial increase in academic knowledge, confidence, and attitude. Multimedia also benefits students because it brings expert to students online, fits students’ schedule, allows students to choose courses that suit them. 

Mayer’s Cognitive Theory of Multimedia Learning suggests, “people learn more from words and pictures than from words alone.” According to Mayer and other scholars, multimedia technology stimulates people’s brains by implementing visual and auditory effects, and thereby assists online users to learn efficiently. Researchers suggest that when users establish dual channels while learning, they tend to understand and memorize better. Mixed literature of this theory are still present in the field of multimedia and social work.

With the spread and development of the English language around the world, it has become an important way of communicating between different people and cultures. Multimedia Technology creates a platform where language can be taught. The traditional form of teaching English as a Second Language (ESL) in classrooms have drastically changed with the prevalence of technology, making easier for students to obtain language learning skills. Multimedia motivates students to learn more languages through audio, visual and animation support. It also helps create English contexts since an important aspect of learning a language is developing their grammar, vocabulary and knowledge of pragmatics and genres. In addition, cultural connections in terms of forms, contexts, meanings and ideologies have to be constructed. By improving thought patterns, multimedia develops students’ communicative competence by improving their capacity to understand the language. One of the studies, carried out by Izquierdo, Simard and Pulido, presented the correlation between “Multimedia Instruction (MI) and learners’ second language (L2)” and its effects on learning behavior. Their findings based on Gardner’s theory of the “socio-educational model of learner motivation and attitudes”, the study shows that there is easier access to language learning materials as well as increased motivation with MI along with the use of Computer-Assisted Language Learning.

Newspaper companies all over are trying to embrace the new phenomenon by implementing its practices in their work. While some have been slow to come around, other major newspapers like "The New York Times", "USA Today" and "The Washington Post" are setting the precedent for the positioning of the newspaper industry in a globalized world.

News reporting is not limited to traditional media outlets. Freelance journalists can make use of different new media to produce multimedia pieces for their news stories. It engages global audiences and tells stories with technology, which develops new communication techniques for both media producers and consumers. The Common Language Project, later renamed to The Seattle Globalist, is an example of this type of multimedia journalism production.

Multimedia reporters who are mobile (usually driving around a community with cameras, audio and video recorders, and laptop computers) are often referred to as mojos, from "mo"bile "jo"urnalist.

Software engineers may use multimedia in computer simulations for anything from entertainment to training such as military or industrial training. Multimedia for software interfaces are often done as a collaboration between creative professionals and software engineers.

In mathematical and scientific research, multimedia is mainly used for modeling and simulation. For example, a scientist can look at a molecular model of a particular substance and manipulate it to arrive at a new substance. Representative research can be found in journals such as the "Journal of Multimedia".

In medicine, doctors can get trained by looking at a virtual surgery or they can simulate how the human body is affected by diseases spread by viruses and bacteria and then develop techniques to prevent it. Multimedia applications such as virtual surgeries also help doctors to get practical training.

In Europe, the reference organisation for the multimedia industry is the European Multimedia Associations Convention (EMMAC).

Scholarly conferences about multimedia include:



</doc>
<doc id="20421" url="https://en.wikipedia.org/wiki?curid=20421" title="Max Headroom (TV series)">
Max Headroom (TV series)

Max Headroom is an American satirical science fiction television series by Chrysalis Visual Programming and Lakeside Productions for Lorimar-Telepictures that aired in the United States on ABC from March 1987 to May 1988. The series is set in a futuristic dystopia ruled by an oligarchy of television networks.

In the future, an oligarchy of television networks rules the world. Even the government functions primarily as a puppet of the network executives, serving mainly to pass laws — such as banning "off" switches on televisions — that protect and consolidate the networks' power. Television technology has advanced to the point that viewers' physical movements and thoughts can be monitored through their television sets. Almost all non-television technology has been discontinued or destroyed. The only real check on the power of the networks is Edison Carter, a crusading investigative journalist who regularly exposes the unethical practices of his own employer, and the team of allies both inside and outside the system who assist him in getting his reports to air and protecting him from the forces that wish to silence or kill him.

Edison Carter (Matt Frewer) is a hard-hitting reporter for Network 23, who sometimes uncovered things that his superiors in the network would have preferred kept private. Eventually, one of these instances required him to flee his workspace, upon which he was injured in a motorcycle accident in a parking lot.

The series depicted very little of the past described by Edison. He met a female televangelist (whom he had dated in college) when his reporting put him at odds with the Vu Age Church that she now headed. Edison was sent on a near-rampage to avenge a former colleague, who died as a result of a story on dream-harvesting.

Edison cares about his co-workers, especially Theora Jones and Bryce Lynch, and he has a deep respect for his producer, Murray (although he rarely shows it).

Max Headroom (Frewer) is a computer reconstruction of Carter, created after Bryce Lynch uploaded a copy of his mind. He appears as a computer-rendered bust of Carter superimposed on a wire-frame background. Since Carter's last sight before the motorcycle crash was the sign "Max. headroom" on a parking garage gate, these were the reconstruction's first words and ultimately his name. While Carter is a dedicated professional, Max is a wisecracking observer of human contradictions.
Theora Jones first appeared in the British-made television pilot film for the series. She was Network 23's star controller ("stolen" from the World One Network by Murray) and, working with Edison, the network's star reporter, she often helped save the day for everyone. She was also a potential love interest for Edison, but that subplot was not explored fully on the show before it was cancelled.

Network 23's personnel files list her father as unknown, her mother as deceased, and her brother as Shawn Jones; Shawn is the focus on the second episode broadcast, "Rakers".

Theora Jones was played by Amanda Pays, who along with Matt Frewer and W. Morgan Sheppard, was one of only three cast members to also appear in the American-made series that followed.
Bryce Lynch (Chris Young), a child prodigy and computer hacker, is Network 23's one-man technology research department.

In the stereotypical hacker ethos, Bryce has few principles and fewer loyalties. He seems to accept any task, even morally questionable ones, as long as he is allowed to have the freedom to play with technology however he sees fit. This, in turn, makes him a greater asset to the technological needs and demands of the network, and the whims of its executives and stars. However, he also generally does not hurt or infringe on others, making him an uncannily neutral character in the Max Headroom universe.

In the pilot episode of the series, Bryce is enlisted by evil network CEO Ned Grossberg (Charles Rocket) to investigate the mental patterns of unconscious reporter Edison Carter, to determine whether or not Carter has discovered the secrets of the "Blipverts" scandal. Bryce uploads the contents of Carter's memory into the Network 23 computer system, creating Max Headroom. It had been Bryce, following orders from Grossberg, who fought a hacking battle of sorts (a la the opening scene to "Hackers") with Theora Jones that led to Edison hitting his head on a traffic barrier and falling unconscious.

After the first episode, Bryce is generally recruited by Carter and his controller, Theora Jones, to provide technical aid to their investigative reporting efforts.
Reg (W. Morgan Sheppard) is a "blank", a person not indexed in the government's database. He broadcasts the underground Big Time Television Network from his bus. He is a good friend of Edison Carter, and saves him on more than one occasion. With colleague Dominique, he operates and is the onscreen voice of Big Time television, "All day every day, making tomorrow seem like yesterday."

He dresses in a punk style and has a Mohawk haircut. He has an energetic personality and a strong nostalgic streak, defending antiquated music videos and printed books in equal measure.
Ned Grossberg is a recurring villain on the series, played by former "Saturday Night Live" cast member Charles Rocket.

In the pilot episode, Grossberg is the chairman of Network 23, a major city television station with the highest-rated investigative-news show in town, hosted by Edison Carter. In the Max Headroom world, real-time ratings equal advertising dollars, and advertisements have replaced stocks as the measure of corporate worth.

Grossberg, with his secret prodigy Bryce Lynch, develops a high-speed advertising delivery method known as Blipverts, which condenses full advertisements into a few seconds. When Carter discovers that Blipverts are killing people, Grossberg orders Lynch to prevent Carter from getting out of the building. Knocked unconscious, Carter's memories are extracted into a computer by Lynch in order to determine whether Carter uncovered Grossberg's knowledge of the danger of Blipverts. The resulting computer file of the memory-extraction process becomes Max Headroom, making Grossberg directly responsible for the creation of the character. In the end, Grossberg is publicly exposed as responsible for the Blipverts scandal, and is removed as chairman of Network 23.

A few episodes later, in "Grossberg's Return", Grossberg reappears as a board member of Network 66. Again, he invents a dubious advertising medium and convinces the chairman of the network to adopt it. When the advertising method is shown to be a complete fraud, the resulting public reaction against the network leads to the chairman being removed, and Grossberg manages to assume the chairmanship.

When under stress, Grossberg exhibits a tic of slightly stretching his neck in his suit's collar, first seen in episode 1 when he confronts Lynch in his lab regarding Max retaining Carter's memory about the blipverts.

In the UK telefilm "Max Headroom: 20 Minutes Into the Future" upon which the American series was based, the character was called Grosman and was played by Nickolas Grace. Rocket portrayed Grossberg as an American yuppie with a characteristic facial and neck-stretching twitch.


The series was based on the Channel 4 British TV film produced by Chrysalis, "". Cinemax aired the UK pilot followed by a six-week run of highlights from "The Max Headroom Show", a UK music video show where Headroom appears between music videos. ABC took an interest in the pilot and asked Chrysalis/Lakeside to produce the series for American audiences.

"Max Headroom: 20 Minutes into the Future" was re-shot as a pilot program for a new series broadcast by the U.S. ABC television network. The pilot featured plot changes and some minor visual touches, but retained the same basic storyline. The only original cast retained for the series were Matt Frewer (Max Headroom/Edison Carter) and Amanda Pays (Theora Jones); a third original cast member, W. Morgan Sheppard, joined the series as "Blank Reg" in later episodes. Among the non-original cast, Jeffrey Tambor co-starred as "Murray", Edison Carter's neurotic producer.

The show went into production in late 1986 and ran for six episodes in the first season and eight in season two.


The series began as a mid-season replacement in spring of 1987, and did well enough to be renewed for the fall television season, but the viewer ratings could not be sustained in direct competition with CBS's Top 20 hit "Dallas" (also produced by Lorimar) and NBC's Top 30 hit "Miami Vice." "Max Headroom" was canceled part-way into its second season. The entire series, along with two previously unbroadcast episodes, was rerun in spring 1988 during the Writers Guild of America strike. A cinema spin-off titled "Max Headroom for President" was announced with production intended to start in early 1988 in order to capitalize on that year's U.S. presidential election, but it was never made.

"Max Headroom" was the first cyberpunk series to run in the United States on one of the main broadcast networks in prime time, although it was not tagged with that label until some time after its cancellation. Like other science fiction, the series introduced the general public to new ideas in the form of cyberpunk themes and social issues. The series portrayed the Blanks, a counter-culture group of people who lived without any official numbers or documentation for the sake of privacy. Various episodes delved into issues like literacy and the lack thereof in a TV-dominated culture (for example, in the episode "Body Banks", Blank Reg says: "It's a book. It's a non-volatile storage medium. It's very rare. You should 'ave one." This statement also anticipates the mid-2000s controversy over the replacement of print by online and e-book sources.)

Of Max Headroom himself, actor Matt Frewer told "Rolling Stone" magazine that "The cool thing about playing Max is that you can say virtually anything because theoretically the guy's not real, right? Can't sue a computer!"

The 1987 Max Headroom broadcast signal intrusion incident involved someone dressed as Max Headroom interrupting the signals of Chicago television stations WGN-TV and WTTW. The person or persons responsible were never identified.

In the late 1990s, U.S. cable TV channels Bravo and the Sci-Fi Channel re-ran the series. Reruns also briefly appeared on TechTV in 2001.

Max Headroom has inspired many imitations and spoofs:

Shout! Factory released "Max Headroom: The Complete Series" on DVD in the United States and Canada on August 10, 2010. The bonus features includes a round-table discussion with most of the major cast members (other than Matt Frewer), and interviews with the writers and producers.

The original British version of the movie "Max Headroom: 20 Minutes into the Future" was released to the Japanese DVD rental market on September 2, 2005.




</doc>
<doc id="20423" url="https://en.wikipedia.org/wiki?curid=20423" title="Malaria">
Malaria

Malaria is a mosquito-borne infectious disease affecting humans and other animals caused by parasitic protozoans (a group of single-celled microorganisms) belonging to the "Plasmodium" type. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death. Symptoms usually begin ten to fifteen days after being bitten. If not properly treated, people may have recurrences of the disease months later. In those who have recently survived an infection, reinfection usually causes milder symptoms. This partial resistance disappears over months to years if the person has no continuing exposure to malaria.
The disease is most commonly transmitted by an infected female "Anopheles" mosquito. The mosquito bite introduces the parasites from the mosquito's saliva into a person's blood. The parasites travel to the liver where they mature and reproduce. Five species of "Plasmodium" can infect and be spread by humans. Most deaths are caused by "P. falciparum" because "P. vivax", "P. ovale", and "P. malariae" generally cause a milder form of malaria. The species "P. knowlesi" rarely causes disease in humans. Malaria is typically diagnosed by the microscopic examination of blood using blood films, or with antigen-based rapid diagnostic tests. Methods that use the polymerase chain reaction to detect the parasite's DNA have been developed, but are not widely used in areas where malaria is common due to their cost and complexity.
The risk of disease can be reduced by preventing mosquito bites through the use of mosquito nets and insect repellents, or with mosquito control measures such as spraying insecticides and draining standing water. Several medications are available to prevent malaria in travellers to areas where the disease is common. Occasional doses of the combination medication sulfadoxine/pyrimethamine are recommended in infants and after the first trimester of pregnancy in areas with high rates of malaria. Despite a need, no effective vaccine exists, although efforts to develop one are ongoing. The recommended treatment for malaria is a combination of antimalarial medications that includes an artemisinin. The second medication may be either mefloquine, lumefantrine, or sulfadoxine/pyrimethamine. Quinine along with doxycycline may be used if an artemisinin is not available. It is recommended that in areas where the disease is common, malaria is confirmed if possible before treatment is started due to concerns of increasing drug resistance. Resistance among the parasites has developed to several antimalarial medications; for example, chloroquine-resistant "P. falciparum" has spread to most malarial areas, and resistance to artemisinin has become a problem in some parts of Southeast Asia.
The disease is widespread in the tropical and subtropical regions that exist in a broad band around the equator. This includes much of Sub-Saharan Africa, Asia, and Latin America. In 2016, there were 216 million cases of malaria worldwide resulting in an estimated 445,000 to 731,000 deaths. Approximately 90% of both cases and deaths occurred in Africa. Rates of disease have decreased from 2000 to 2015 by 37%, but increased from 2014 during which there were 198 million cases. Malaria is commonly associated with poverty and has a major negative effect on economic development. In Africa, it is estimated to result in losses of US$12 billion a year due to increased healthcare costs, lost ability to work, and negative effects on tourism.

The signs and symptoms of malaria typically begin 8–25 days following infection; however, symptoms may occur later in those who have taken antimalarial medications as prevention. Initial manifestations of the disease—common to all malaria species—are similar to flu-like symptoms, and can resemble other conditions such as sepsis, gastroenteritis, and viral diseases. The presentation may include headache, fever, shivering, joint pain, vomiting, hemolytic anemia, jaundice, hemoglobin in the urine, retinal damage, and convulsions.

The classic symptom of malaria is paroxysm—a cyclical occurrence of sudden coldness followed by shivering and then fever and sweating, occurring every two days (tertian fever) in "P. vivax" and "P. ovale" infections, and every three days (quartan fever) for "P. malariae". "P. falciparum" infection can cause recurrent fever every 36–48 hours, or a less pronounced and almost continuous fever.

Severe malaria is usually caused by "P. falciparum" (often referred to as falciparum malaria). Symptoms of falciparum malaria arise 9–30 days after infection. Individuals with cerebral malaria frequently exhibit neurological symptoms, including abnormal posturing, nystagmus, conjugate gaze palsy (failure of the eyes to turn together in the same direction), opisthotonus, seizures, or coma.

Malaria has several serious complications. Among these is the development of respiratory distress, which occurs in up to 25% of adults and 40% of children with severe "P. falciparum" malaria. Possible causes include respiratory compensation of metabolic acidosis, noncardiogenic pulmonary oedema, concomitant pneumonia, and severe anaemia. Although rare in young children with severe malaria, acute respiratory distress syndrome occurs in 5–25% of adults and up to 29% of pregnant women. Coinfection of HIV with malaria increases mortality. Renal failure is a feature of blackwater fever, where hemoglobin from lysed red blood cells leaks into the urine.

Infection with "P. falciparum" may result in cerebral malaria, a form of severe malaria that involves encephalopathy. It is associated with retinal whitening, which may be a useful clinical sign in distinguishing malaria from other causes of fever. Enlarged spleen, enlarged liver or both of these, severe headache, low blood sugar, and hemoglobin in the urine with renal failure may occur. Complications may include spontaneous bleeding, coagulopathy, and shock.

Malaria in pregnant women is an important cause of stillbirths, infant mortality, abortion and low birth weight, particularly in "P. falciparum" infection, but also with "P. vivax".

Malaria parasites belong to the genus "Plasmodium" (phylum Apicomplexa). In humans, malaria is caused by "P. falciparum", "P. malariae", "P. ovale", "P. vivax" and "P. knowlesi". Among those infected, "P. falciparum" is the most common species identified (~75%) followed by "P. vivax" (~20%). Although "P. falciparum" traditionally accounts for the majority of deaths, recent evidence suggests that "P. vivax" malaria is associated with potentially life-threatening conditions about as often as with a diagnosis of "P. falciparum" infection. "P. vivax " proportionally is more common outside Africa. There have been documented human infections with several species of "Plasmodium" from higher apes; however, except for "P. knowlesi"—a zoonotic species that causes malaria in macaques—these are mostly of limited public health importance.

Global warming is likely to affect malaria transmission, but the severity and geographic distribution of such effects is uncertain.

In the life cycle of "Plasmodium", a female "Anopheles" mosquito (the definitive host) transmits a motile infective form (called the sporozoite) to a vertebrate host such as a human (the secondary host), thus acting as a transmission vector. A sporozoite travels through the blood vessels to liver cells (hepatocytes), where it reproduces asexually (tissue schizogony), producing thousands of merozoites. These infect new red blood cells and initiate a series of asexual multiplication cycles (blood schizogony) that produce 8 to 24 new infective merozoites, at which point the cells burst and the infective cycle begins anew.

Other merozoites develop into immature gametocytes, which are the precursors of male and female gametes. When a fertilized mosquito bites an infected person, gametocytes are taken up with the blood and mature in the mosquito gut. The male and female gametocytes fuse and form an ookinete—a fertilized, motile zygote. Ookinetes develop into new sporozoites that migrate to the insect's salivary glands, ready to infect a new vertebrate host. The sporozoites are injected into the skin, in the saliva, when the mosquito takes a subsequent blood meal.

Only female mosquitoes feed on blood; male mosquitoes feed on plant nectar and do not transmit the disease. Females of the mosquito genus "Anopheles" prefer to feed at night. They usually start searching for a meal at dusk and will continue throughout the night until taking a meal. Malaria parasites can also be transmitted by blood transfusions, although this is rare.

Symptoms of malaria can recur after varying symptom-free periods. Depending upon the cause, recurrence can be classified as either recrudescence, relapse, or reinfection. Recrudescence is when symptoms return after a symptom-free period. It is caused by parasites surviving in the blood as a result of inadequate or ineffective treatment. Relapse is when symptoms reappear after the parasites have been eliminated from blood but persist as dormant hypnozoites in liver cells. Relapse commonly occurs between 8–24 weeks and is often seen in "P. vivax" and "P. ovale" infections. However, relapse-like "P. vivax" recurrences are probably being over-attributed to hypnozoite activation. Some of them might have an extra-vascular merozoite origin, making these recurrences recrudescences, not relapses. One newly recognized, non-hypnozoite, possible contributing source to recurrent peripheral "P. vivax" parasitemia is erythrocytic forms in bone marrow. "P. vivax" malaria cases in temperate areas often involve overwintering by hypnozoites, with relapses beginning the year after the mosquito bite. Reinfection means the parasite that caused the past infection was eliminated from the body but a new parasite was introduced. Reinfection cannot readily be distinguished from recrudescence, although recurrence of infection within two weeks of treatment for the initial infection is typically attributed to treatment failure. People may develop some immunity when exposed to frequent infections.

Malaria infection develops via two phases: one that involves the liver (exoerythrocytic phase), and one that involves red blood cells, or erythrocytes (erythrocytic phase). When an infected mosquito pierces a person's skin to take a blood meal, sporozoites in the mosquito's saliva enter the bloodstream and migrate to the liver where they infect hepatocytes, multiplying asexually and asymptomatically for a period of 8–30 days.

After a potential dormant period in the liver, these organisms differentiate to yield thousands of merozoites, which, following rupture of their host cells, escape into the blood and infect red blood cells to begin the erythrocytic stage of the life cycle. The parasite escapes from the liver undetected by wrapping itself in the cell membrane of the infected host liver cell.

Within the red blood cells, the parasites multiply further, again asexually, periodically breaking out of their host cells to invade fresh red blood cells. Several such amplification cycles occur. Thus, classical descriptions of waves of fever arise from simultaneous waves of merozoites escaping and infecting red blood cells.

Some "P. vivax" sporozoites do not immediately develop into exoerythrocytic-phase merozoites, but instead, produce hypnozoites that remain dormant for periods ranging from several months (7–10 months is typical) to several years. After a period of dormancy, they reactivate and produce merozoites. Hypnozoites are responsible for long incubation and late relapses in "P. vivax" infections, although their existence in "P. ovale" is uncertain.

The parasite is relatively protected from attack by the body's immune system because for most of its human life cycle it resides within the liver and blood cells and is relatively invisible to immune surveillance. However, circulating infected blood cells are destroyed in the spleen. To avoid this fate, the "P. falciparum" parasite displays adhesive proteins on the surface of the infected blood cells, causing the blood cells to stick to the walls of small blood vessels, thereby sequestering the parasite from passage through the general circulation and the spleen. The blockage of the microvasculature causes symptoms such as in placental malaria. Sequestered red blood cells can breach the blood–brain barrier and cause cerebral malaria.

According to a 2005 review, due to the high levels of mortality and morbidity caused by malaria—especially the "P. falciparum" species—it has placed the greatest selective pressure on the human genome in recent history. Several genetic factors provide some resistance to it including sickle cell trait, thalassaemia traits, glucose-6-phosphate dehydrogenase deficiency, and the absence of Duffy antigens on red blood cells.

The impact of sickle cell trait on malaria immunity illustrates some evolutionary trade-offs that have occurred because of endemic malaria. Sickle cell trait causes a change in the hemoglobin molecule in the blood. Normally, red blood cells have a very flexible, biconcave shape that allows them to move through narrow capillaries; however, when the modified hemoglobin S molecules are exposed to low amounts of oxygen, or crowd together due to dehydration, they can stick together forming strands that cause the cell to sickle or distort into a curved shape. In these strands the molecule is not as effective in taking or releasing oxygen, and the cell is not flexible enough to circulate freely. In the early stages of malaria, the parasite can cause infected red cells to sickle, and so they are removed from circulation sooner. This reduces the frequency with which malaria parasites complete their life cycle in the cell. Individuals who are homozygous (with two copies of the abnormal hemoglobin beta allele) have sickle-cell anaemia, while those who are heterozygous (with one abnormal allele and one normal allele) experience resistance to malaria without severe anemia. Although the shorter life expectancy for those with the homozygous condition would tend to disfavor the trait's survival, the trait is preserved in malaria-prone regions because of the benefits provided by the heterozygous form.

Liver dysfunction as a result of malaria is uncommon and usually only occurs in those with another liver condition such as viral hepatitis or chronic liver disease. The syndrome is sometimes called "malarial hepatitis". While it has been considered a rare occurrence, malarial hepatopathy has seen an increase, particularly in Southeast Asia and India. Liver compromise in people with malaria correlates with a greater likelihood of complications and death.

Owing to the non-specific nature of the presentation of symptoms, diagnosis of malaria in non-endemic areas requires a high degree of suspicion, which might be elicited by any of the following: recent travel history, enlarged spleen, fever, low number of platelets in the blood, and higher-than-normal levels of bilirubin in the blood combined with a normal level of white blood cells. Reports in 2016 and 2017 from countries were malaria is common suggest high levels of over diagnosis due to insufficient or inaccurate laboratory testing.

Malaria is usually confirmed by the microscopic examination of blood films or by antigen-based rapid diagnostic tests (RDT). In some areas, RDTs need to be able to distinguish whether the malaria symptoms are caused by "Plasmodium falciparum" or by other species of parasites since treatment strategies could differ for non-"P. falciparum" infections. Microscopy is the most commonly used method to detect the malarial parasite—about 165 million blood films were examined for malaria in 2010. Despite its widespread usage, diagnosis by microscopy suffers from two main drawbacks: many settings (especially rural) are not equipped to perform the test, and the accuracy of the results depends on both the skill of the person examining the blood film and the levels of the parasite in the blood. The sensitivity of blood films ranges from 75–90% in optimum conditions, to as low as 50%. Commercially available RDTs are often more accurate than blood films at predicting the presence of malaria parasites, but they are widely variable in diagnostic sensitivity and specificity depending on manufacturer, and are unable to tell how many parasites are present.

In regions where laboratory tests are readily available, malaria should be suspected, and tested for, in any unwell person who has been in an area where malaria is endemic. In areas that cannot afford laboratory diagnostic tests, it has become common to use only a history of fever as the indication to treat for malaria—thus the common teaching "fever equals malaria unless proven otherwise". A drawback of this practice is overdiagnosis of malaria and mismanagement of non-malarial fever, which wastes limited resources, erodes confidence in the health care system, and contributes to drug resistance. Although polymerase chain reaction-based tests have been developed, they are not widely used in areas where malaria is common as of 2012, due to their complexity.

Malaria is classified into either "severe" or "uncomplicated" by the World Health Organization (WHO). It is deemed severe when "any" of the following criteria are present, otherwise it is considered uncomplicated.

Cerebral malaria is defined as a severe "P. falciparum"-malaria presenting with neurological symptoms, including coma (with a Glasgow coma scale less than 11, or a Blantyre coma scale greater than 3), or with a coma that lasts longer than 30 minutes after a seizure.

Various types of malaria have been called by the names below:

Methods used to prevent malaria include medications, mosquito elimination and the prevention of bites. There is no vaccine for malaria. The presence of malaria in an area requires a combination of high human population density, high anopheles mosquito population density and high rates of transmission from humans to mosquitoes and from mosquitoes to humans. If any of these is lowered sufficiently, the parasite will eventually disappear from that area, as happened in North America, Europe and parts of the Middle East. However, unless the parasite is eliminated from the whole world, it could become re-established if conditions revert to a combination that favors the parasite's reproduction. Furthermore, the cost per person of eliminating anopheles mosquitoes rises with decreasing population density, making it economically unfeasible in some areas.

Prevention of malaria may be more cost-effective than treatment of the disease in the long run, but the initial costs required are out of reach of many of the world's poorest people. There is a wide difference in the costs of control (i.e. maintenance of low endemicity) and elimination programs between countries. For example, in China—whose government in 2010 announced a strategy to pursue malaria elimination in the Chinese provinces—the required investment is a small proportion of public expenditure on health. In contrast, a similar program in Tanzania would cost an estimated one-fifth of the public health budget.

In areas where malaria is common, children under five years old often have anemia which is sometimes due to malaria. Giving children with anemia in these areas preventive antimalarial medication improves red blood cell levels slightly but did not affect the risk of death or need for hospitalization.

Vector control refers to methods used to decrease malaria by reducing the levels of transmission by mosquitoes. For individual protection, the most effective insect repellents are based on DEET or picaridin. Insecticide-treated mosquito nets (ITNs) and indoor residual spraying (IRS) have been shown to be highly effective in preventing malaria among children in areas where malaria is common. Prompt treatment of confirmed cases with artemisinin-based combination therapies (ACTs) may also reduce transmission.

Mosquito nets help keep mosquitoes away from people and reduce infection rates and transmission of malaria. Nets are not a perfect barrier and are often treated with an insecticide designed to kill the mosquito before it has time to find a way past the net. Insecticide-treated nets are estimated to be twice as effective as untreated nets and offer greater than 70% protection compared with no net. Between 2000 and 2008, the use of ITNs saved the lives of an estimated 250,000 infants in Sub-Saharan Africa. About 13% of households in Sub-Saharan countries owned ITNs in 2007 and 31% of African households were estimated to own at least one ITN in 2008. In 2000, 1.7 million (1.8%) African children living in areas of the world where malaria is common were protected by an ITN. That number increased to 20.3 million (18.5%) African children using ITNs in 2007, leaving 89.6 million children unprotected and to 68% African children using mosquito nets in 2015. Most nets are impregnated with pyrethroids, a class of insecticides with low toxicity. They are most effective when used from dusk to dawn. It is recommended to hang a large "bed net" above the center of a bed and either tuck the edges under the mattress or make sure it is large enough such that it touches the ground.
Indoor residual spraying is the spraying of insecticides on the walls inside a home. After feeding, many mosquitoes rest on a nearby surface while digesting the bloodmeal, so if the walls of houses have been coated with insecticides, the resting mosquitoes can be killed before they can bite another person and transfer the malaria parasite. As of 2006, the World Health Organization recommends 12 insecticides in IRS operations, including DDT and the pyrethroids cyfluthrin and deltamethrin. This public health use of small amounts of DDT is permitted under the Stockholm Convention, which prohibits its agricultural use. One problem with all forms of IRS is insecticide resistance. Mosquitoes affected by IRS tend to rest and live indoors, and due to the irritation caused by spraying, their descendants tend to rest and live outdoors, meaning that they are less affected by the IRS.
There are a number of other methods to reduce mosquito bites and slow the spread of malaria. Efforts to decrease mosquito larva by decreasing the availability of open water in which they develop or by adding substances to decrease their development is effective in some locations. Electronic mosquito repellent devices which make very high-frequency sounds that are supposed to keep female mosquitoes away, do not have supporting evidence.

Community participation and health education strategies promoting awareness of malaria and the importance of control measures have been successfully used to reduce the incidence of malaria in some areas of the developing world. Recognizing the disease in the early stages can prevent the disease from becoming fatal. Education can also inform people to cover over areas of stagnant, still water, such as water tanks that are ideal breeding grounds for the parasite and mosquito, thus cutting down the risk of the transmission between people. This is generally used in urban areas where there are large centers of population in a confined space and transmission would be most likely in these areas. Intermittent preventive therapy is another intervention that has been used successfully to control malaria in pregnant women and infants, and in preschool children where transmission is seasonal.

There are a number of medications that can help prevent or interrupt malaria in travelers to places where infection is common. Many of these medications are also used in treatment. In places where "Plasmodium" is resistant to one or more medications, three medications—mefloquine, doxycycline , or the combination of atovaquone/proguanil ("Malarone")—are frequently used for prevention. Doxycycline and the atovaquone/proguanil are better tolerated while mefloquine is taken once a week. Areas of the world with chloroquine sensitive malaria are uncommon.

The protective effect does not begin immediately, and people visiting areas where malaria exists usually start taking the drugs one to two weeks before arriving and continue taking them for four weeks after leaving (except for atovaquone/proguanil, which only needs to be started two days before and continued for seven days afterward). The use of preventative drugs is often not practical for those who live in areas where malaria exists, and their use is usually only in pregnant women and short-term visitors. This is due to the cost of the drugs, side effects from long-term use, and the difficulty in obtaining anti-malarial drugs outside of wealthy nations. During pregnancy, medication to prevent malaria has been found to improve the weight of the baby at birth and decrease the risk of anemia in the mother. The use of preventative drugs where malaria-bearing mosquitoes are present may encourage the development of partial resistance.

Malaria is treated with antimalarial medications; the ones used depends on the type and severity of the disease. While medications against fever are commonly used, their effects on outcomes are not clear.
Simple or uncomplicated malaria may be treated with oral medications. The most effective treatment for "P. falciparum" infection is the use of artemisinins in combination with other antimalarials (known as artemisinin-combination therapy, or ACT), which decreases resistance to any single drug component. These additional antimalarials include: amodiaquine, lumefantrine, mefloquine or sulfadoxine/pyrimethamine. Another recommended combination is dihydroartemisinin and piperaquine. ACT is about 90% effective when used to treat uncomplicated malaria. To treat malaria during pregnancy, the WHO recommends the use of quinine plus clindamycin early in the pregnancy (1st trimester), and ACT in later stages (2nd and 3rd trimesters). In the 2000s (decade), malaria with partial resistance to artemisins emerged in Southeast Asia. Infection with "P. vivax", "P. ovale" or "P. malariae" usually do not require hospitalization. Treatment of "P. vivax" requires both treatment of blood stages (with chloroquine or ACT) and clearance of liver forms with primaquine. Treatment with tafenoquine prevents relapses after confirmed "P. vivax" malaria.
Severe and complicated malaria are almost always caused by infection with "P. falciparum". The other species usually cause only febrile disease. Severe and complicated malaria are medical emergencies since mortality rates are high (10% to 50%). Cerebral malaria is the form of severe and complicated malaria with the worst neurological symptoms.
Recommended treatment for severe malaria is the intravenous use of antimalarial drugs. For severe malaria, parenteral artesunate was superior to quinine in both children and adults. In another systematic review, artemisinin derivatives (artemether and arteether) were as efficacious as quinine in the treatment of cerebral malaria in children. Treatment of severe malaria involves supportive measures that are best done in a critical care unit. This includes the management of high fevers and the seizures that may result from it. It also includes monitoring for poor breathing effort, low blood sugar, and low blood potassium.

Drug resistance poses a growing problem in 21st-century malaria treatment. Resistance is now common against all classes of antimalarial drugs apart from artemisinins. Treatment of resistant strains became increasingly dependent on this class of drugs. The cost of artemisinins limits their use in the developing world. Malaria strains found on the Cambodia–Thailand border are resistant to combination therapies that include artemisinins, and may, therefore, be untreatable. Exposure of the parasite population to artemisinin monotherapies in subtherapeutic doses for over 30 years and the availability of substandard artemisinins likely drove the selection of the resistant phenotype. Resistance to artemisinin has been detected in Cambodia, Myanmar, Thailand, and Vietnam, and there has been emerging resistance in Laos.

When properly treated, people with malaria can usually expect a complete recovery. However, severe malaria can progress extremely rapidly and cause death within hours or days. In the most severe cases of the disease, fatality rates can reach 20%, even with intensive care and treatment. Over the longer term, developmental impairments have been documented in children who have suffered episodes of severe malaria. Chronic infection without severe disease can occur in an immune-deficiency syndrome associated with a decreased responsiveness to "Salmonella" bacteria and the Epstein–Barr virus.

During childhood, malaria causes anemia during a period of rapid brain development, and also direct brain damage resulting from cerebral malaria. Some survivors of cerebral malaria have an increased risk of neurological and cognitive deficits, behavioural disorders, and epilepsy. Malaria prophylaxis was shown to improve cognitive function and school performance in clinical trials when compared to placebo groups.

The WHO estimates that in 2015 there were 214 million new cases of malaria resulting in 438,000 deaths. Others have estimated the number of cases at between 350 and 550 million for falciparum malaria The majority of cases (65%) occur in children under 15 years old. About 125 million pregnant women are at risk of infection each year; in Sub-Saharan Africa, maternal malaria is associated with up to 200,000 estimated infant deaths yearly. There are about 10,000 malaria cases per year in Western Europe, and 1300–1500 in the United States. About 900 people died from the disease in Europe between 1993 and 2003. Both the global incidence of disease and resulting mortality have declined in recent years. According to the WHO and UNICEF, deaths attributable to malaria in 2015 were reduced by 60% from a 2000 estimate of 985,000, largely due to the widespread use of insecticide-treated nets and artemisinin-based combination therapies. In 2012, there were 207 million cases of malaria. That year, the disease is estimated to have killed between 473,000 and 789,000 people, many of whom were children in Africa. Efforts at decreasing the disease in Africa since the turn of millennium have been partially effective, with rates of the disease dropping by an estimated forty percent on the continent.

Malaria is presently endemic in a broad band around the equator, in areas of the Americas, many parts of Asia, and much of Africa; in Sub-Saharan Africa, 85–90% of malaria fatalities occur. An estimate for 2009 reported that countries with the highest death rate per 100,000 of population were Ivory Coast (86.15), Angola (56.93) and Burkina Faso (50.66). A 2010 estimate indicated the deadliest countries per population were Burkina Faso, Mozambique and Mali. The Malaria Atlas Project aims to map global endemic levels of malaria, providing a means with which to determine the global spatial limits of the disease and to assess disease burden. This effort led to the publication of a map of "P. falciparum" endemicity in 2010. As of 2010, about 100 countries have endemic malaria. Every year, 125 million international travellers visit these countries, and more than 30,000 contract the disease.

The geographic distribution of malaria within large regions is complex, and malaria-afflicted and malaria-free areas are often found close to each other. Malaria is prevalent in tropical and subtropical regions because of rainfall, consistent high temperatures and high humidity, along with stagnant waters in which mosquito larvae readily mature, providing them with the environment they need for continuous breeding. In drier areas, outbreaks of malaria have been predicted with reasonable accuracy by mapping rainfall. Malaria is more common in rural areas than in cities. For example, several cities in the Greater Mekong Subregion of Southeast Asia are essentially malaria-free, but the disease is prevalent in many rural regions, including along international borders and forest fringes. In contrast, malaria in Africa is present in both rural and urban areas, though the risk is lower in the larger cities.

Although the parasite responsible for "P. falciparum" malaria has been in existence for 50,000–100,000 years, the population size of the parasite did not increase until about 10,000 years ago, concurrently with advances in agriculture and the development of human settlements. Close relatives of the human malaria parasites remain common in chimpanzees. Some evidence suggests that the "P. falciparum" malaria may have originated in gorillas.

References to the unique periodic fevers of malaria are found throughout recorded history. Hippocrates described periodic fevers, labelling them tertian, quartan, subtertian and quotidian. The Roman Columella associated the disease with insects from swamps. Malaria may have contributed to the decline of the Roman Empire, and was so pervasive in Rome that it was known as the "Roman fever". Several regions in ancient Rome were considered at-risk for the disease because of the favourable conditions present for malaria vectors. This included areas such as southern Italy, the island of Sardinia, the Pontine Marshes, the lower regions of coastal Etruria and the city of Rome along the Tiber. The presence of stagnant water in these places was preferred by mosquitoes for breeding grounds. Irrigated gardens, swamp-like grounds, runoff from agriculture, and drainage problems from road construction led to the increase of standing water.

The term malaria originates from Medieval —"bad air"; the disease was formerly called "ague" or "marsh fever" due to its association with swamps and marshland. The term first appeared in the English literature about 1829. Malaria was once common in most of Europe and North America, where it is no longer endemic, though imported cases do occur.

Scientific studies on malaria made their first significant advance in 1880, when Charles Louis Alphonse Laveran—a French army doctor working in the military hospital of Constantine in Algeria—observed parasites inside the red blood cells of infected people for the first time. He, therefore, proposed that malaria is caused by this organism, the first time a protist was identified as causing disease. For this and later discoveries, he was awarded the 1907 Nobel Prize for Physiology or Medicine. A year later, Carlos Finlay, a Cuban doctor treating people with yellow fever in Havana, provided strong evidence that mosquitoes were transmitting disease to and from humans. This work followed earlier suggestions by Josiah C. Nott, and work by Sir Patrick Manson, the "father of tropical medicine", on the transmission of filariasis.

In April 1894, a Scottish physician, Sir Ronald Ross, visited Sir Patrick Manson at his house on Queen Anne Street, London. This visit was the start of four years of collaboration and fervent research that culminated in 1897 when Ross, who was working in the Presidency General Hospital in Calcutta, proved the complete life-cycle of the malaria parasite in mosquitoes. He thus proved that the mosquito was the vector for malaria in humans by showing that certain mosquito species transmit malaria to birds. He isolated malaria parasites from the salivary glands of mosquitoes that had fed on infected birds. For this work, Ross received the 1902 Nobel Prize in Medicine. After resigning from the Indian Medical Service, Ross worked at the newly established Liverpool School of Tropical Medicine and directed malaria-control efforts in Egypt, Panama, Greece and Mauritius. The findings of Finlay and Ross were later confirmed by a medical board headed by Walter Reed in 1900. Its recommendations were implemented by William C. Gorgas in the health measures undertaken during construction of the Panama Canal. This public-health work saved the lives of thousands of workers and helped develop the methods used in future public-health campaigns against the disease.

The first effective treatment for malaria came from the bark of cinchona tree, which contains quinine. This tree grows on the slopes of the Andes, mainly in Peru. The indigenous peoples of Peru made a tincture of cinchona to control fever. Its effectiveness against malaria was found and the Jesuits introduced the treatment to Europe around 1640; by 1677, it was included in the London Pharmacopoeia as an antimalarial treatment. It was not until 1820 that the active ingredient, quinine, was extracted from the bark, isolated and named by the French chemists Pierre Joseph Pelletier and Joseph Bienaimé Caventou.

Quinine became the predominant malarial medication until the 1920s when other medications began to be developed. In the 1940s, chloroquine replaced quinine as the treatment of both uncomplicated and severe malaria until resistance supervened, first in Southeast Asia and South America in the 1950s and then globally in the 1980s.

The medicinal value of "Artemisia annua" has been used by Chinese herbalists in traditional Chinese medicines for 2,000 years. In 1596, Li Shizhen recommended tea made from qinghao specifically to treat malaria symptoms in his "Compendium of Materia Medica". Artemisinins, discovered by Chinese scientist Tu Youyou and colleagues in the 1970s from the plant "Artemisia annua", became the recommended treatment for "P. falciparum" malaria, administered in severe cases in combination with other antimalarials. Tu says she was influenced by a traditional Chinese herbal medicine source, "The Handbook of Prescriptions for Emergency Treatments", written in 340 by Ge Hong. For her work on malaria, Tu Youyou received the 2015 Nobel Prize in Physiology or Medicine.

"Plasmodium vivax" was used between 1917 and the 1940s for malariotherapy—deliberate injection of malaria parasites to induce a fever to combat certain diseases such as tertiary syphilis. In 1927, the inventor of this technique, Julius Wagner-Jauregg, received the Nobel Prize in Physiology or Medicine for his discoveries. The technique was dangerous, killing about 15% of patients, so it is no longer in use.

The first pesticide used for indoor residual spraying was DDT. Although it was initially used exclusively to combat malaria, its use quickly spread to agriculture. In time, pest control, rather than disease control, came to dominate DDT use, and this large-scale agricultural use led to the evolution of resistant mosquitoes in many regions. The DDT resistance shown by "Anopheles" mosquitoes can be compared to antibiotic resistance shown by bacteria. During the 1960s, awareness of the negative consequences of its indiscriminate use increased, ultimately leading to bans on agricultural applications of DDT in many countries in the 1970s. Before DDT, malaria was successfully eliminated or controlled in tropical areas like Brazil and Egypt by removing or poisoning the breeding grounds of the mosquitoes or the aquatic habitats of the larva stages, for example by applying the highly toxic arsenic compound Paris Green to places with standing water.

Malaria vaccines have been an elusive goal of research. The first promising studies demonstrating the potential for a malaria vaccine were performed in 1967 by immunizing mice with live, radiation-attenuated sporozoites, which provided significant protection to the mice upon subsequent injection with normal, viable sporozoites. Since the 1970s, there has been a considerable effort to develop similar vaccination strategies for humans. The first vaccine, called RTS,S, was approved by European regulators in 2015.

Malaria is not just a disease commonly associated with poverty: some evidence suggests that it is also a cause of poverty and a major hindrance to economic development. Although tropical regions are most affected, malaria's furthest influence reaches into some temperate zones that have extreme seasonal changes. The disease has been associated with major negative economic effects on regions where it is widespread. During the late 19th and early 20th centuries, it was a major factor in the slow economic development of the American southern states.

A comparison of average per capita GDP in 1995, adjusted for parity of purchasing power, between countries with malaria and countries without malaria gives a fivefold difference ($1,526 USD versus $8,268 USD). In the period 1965 to 1990, countries where malaria was common had an average per capita GDP that increased only 0.4% per year, compared to 2.4% per year in other countries.

Poverty can increase the risk of malaria since those in poverty do not have the financial capacities to prevent or treat the disease. In its entirety, the economic impact of malaria has been estimated to cost Africa US$12 billion every year. The economic impact includes costs of health care, working days lost due to sickness, days lost in education, decreased productivity due to brain damage from cerebral malaria, and loss of investment and tourism. The disease has a heavy burden in some countries, where it may be responsible for 30–50% of hospital admissions, up to 50% of outpatient visits, and up to 40% of public health spending.
Cerebral malaria is one of the leading causes of neurological disabilities in African children. Studies comparing cognitive functions before and after treatment for severe malarial illness continued to show significantly impaired school performance and cognitive abilities even after recovery. Consequently, severe and cerebral malaria have far-reaching socioeconomic consequences that extend beyond the immediate effects of the disease.

Sophisticated counterfeits have been found in several Asian countries such as Cambodia, China, Indonesia, Laos, Thailand, and Vietnam, and are an important cause of avoidable death in those countries. The WHO said that studies indicate that up to 40% of artesunate-based malaria medications are counterfeit, especially in the Greater Mekong region and have established a rapid alert system to enable information about counterfeit drugs to be rapidly reported to the relevant authorities in participating countries. There is no reliable way for doctors or lay people to detect counterfeit drugs without help from a laboratory. Companies are attempting to combat the persistence of counterfeit drugs by using new technology to provide security from source to distribution.

Another clinical and public health concern is the proliferation of substandard antimalarial medicines resulting from inappropriate concentration of ingredients, contamination with other drugs or toxic impurities, poor quality ingredients, poor stability and inadequate packaging. A 2012 study demonstrated that roughly one-third of antimalarial medications in Southeast Asia and Sub-Saharan Africa failed chemical analysis, packaging analysis, or were falsified.

Throughout history, the contraction of malaria has played a prominent role in the fates of government rulers, nation-states, military personnel, and military actions. In 1910, Nobel Prize in Medicine-winner Ronald Ross (himself a malaria survivor), published a book titled "The Prevention of Malaria" that included a chapter titled "The Prevention of Malaria in War." The chapter's author, Colonel C. H. Melville, Professor of Hygiene at Royal Army Medical College in London, addressed the prominent role that malaria has historically played during wars: "The history of malaria in war might almost be taken to be the history of war itself, certainly the history of war in the Christian era. ... It is probably the case that many of the so-called camp fevers, and probably also a considerable proportion of the camp dysentery, of the wars of the sixteenth, seventeenth and eighteenth centuries were malarial in origin."

Malaria was the most significant health hazard encountered by U.S. troops in the South Pacific during World War II, where about 500,000 men were infected. According to Joseph Patrick Byrne, "Sixty thousand American soldiers died of malaria during the African and South Pacific campaigns."

Significant financial investments have been made to procure existing and create new anti-malarial agents. During World War I and World War II, inconsistent supplies of the natural anti-malaria drugs cinchona bark and quinine prompted substantial funding into research and development of other drugs and vaccines. American military organizations conducting such research initiatives include the Navy Medical Research Center, Walter Reed Army Institute of Research, and the U.S. Army Medical Research Institute of Infectious Diseases of the US Armed Forces.

Additionally, initiatives have been founded such as Malaria Control in War Areas (MCWA), established in 1942, and its successor, the Communicable Disease Center (now known as the Centers for Disease Control and Prevention, or CDC) established in 1946. According to the CDC, MCWA "was established to control malaria around military training bases in the southern United States and its territories, where malaria was still problematic".

Several notable attempts are being made to eliminate the parasite from sections of the world, or to eradicate it worldwide. In 2006, the organization Malaria No More set a public goal of eliminating malaria from Africa by 2015, and the organization plans to dissolve if that goal is accomplished. Several malaria vaccines are in clinical trials, which are intended to provide protection for children in endemic areas and reduce the speed of transmission of the disease. , The Global Fund to Fight AIDS, Tuberculosis and Malaria has distributed 230 million insecticide-treated nets intended to stop mosquito-borne transmission of malaria. The U.S.-based Clinton Foundation has worked to manage demand and stabilize prices in the artemisinin market. Other efforts, such as the Malaria Atlas Project, focus on analysing climate and weather information required to accurately predict the spread of malaria based on the availability of habitat of malaria-carrying parasites. The Malaria Policy Advisory Committee (MPAC) of the World Health Organization (WHO) was formed in 2012, "to provide strategic advice and technical input to WHO on all aspects of malaria control and elimination". In November 2013, WHO and the malaria vaccine funders group set a goal to develop vaccines designed to interrupt malaria transmission with the long-term goal of malaria eradication.

Malaria has been successfully eliminated or greatly reduced in certain areas. Malaria was once common in the United States and southern Europe, but vector control programs, in conjunction with the monitoring and treatment of infected humans, eliminated it from those regions. Several factors contributed, such as the draining of wetland breeding grounds for agriculture and other changes in water management practices, and advances in sanitation, including greater use of glass windows and screens in dwellings. Malaria was eliminated from most parts of the USA in the early 20th century by such methods, and the use of the pesticide DDT and other means eliminated it from the remaining pockets in the South in the 1950s as part of the National Malaria Eradication Program.

In 2018, WHO announced that Paraguay was free of malaria, after an eradication effort that began in 1950.

Bill Gates has said that he thinks global eradication is possible by 2040.

The Malaria Eradication Research Agenda (malERA) initiative was a consultative process to identify which areas of research and development (R&D) needed to be addressed for the worldwide eradication of malaria.

A vaccine against malaria called RTS,S, was approved by European regulators in 2015. It is undergoing pilot trials in select countries in 2016.

Immunity (or, more accurately, tolerance) to "P. falciparum" malaria does occur naturally, but only in response to years of repeated infection. An individual can be protected from a "P. falciparum" infection if they receive about a thousand bites from mosquitoes that carry a version of the parasite rendered non-infective by a dose of X-ray irradiation. The highly polymorphic nature of many "P. falciparum" proteins results in significant challenges to vaccine design. Vaccine candidates that target antigens on gametes, zygotes, or ookinetes in the mosquito midgut aim to block the transmission of malaria. These transmission-blocking vaccines induce antibodies in the human blood; when a mosquito takes a blood meal from a protected individual, these antibodies prevent the parasite from completing its development in the mosquito. Other vaccine candidates, targeting the blood-stage of the parasite's life cycle, have been inadequate on their own. For example, SPf66 was tested extensively in areas where the disease is common in the 1990s, but trials showed it to be insufficiently effective.

Malaria parasites contain apicoplasts, organelles usually found in plants, complete with their own genomes. These apicoplasts are thought to have originated through the endosymbiosis of algae and play a crucial role in various aspects of parasite metabolism, such as fatty acid biosynthesis. Over 400 proteins have been found to be produced by apicoplasts and these are now being investigated as possible targets for novel anti-malarial drugs.

With the onset of drug-resistant "Plasmodium" parasites, new strategies are being developed to combat the widespread disease. One such approach lies in the introduction of synthetic pyridoxal-amino acid adducts, which are taken up by the parasite and ultimately interfere with its ability to create several essential B vitamins. Antimalarial drugs using synthetic metal-based complexes are attracting research interest.


A non-chemical vector control strategy involves genetic manipulation of malaria mosquitoes. Advances in genetic engineering technologies make it possible to introduce foreign DNA into the mosquito genome and either decrease the lifespan of the mosquito, or make it more resistant to the malaria parasite. Sterile insect technique is a genetic control method whereby large numbers of sterile male mosquitoes are reared and released. Mating with wild females reduces the wild population in the subsequent generation; repeated releases eventually eliminate the target population.

Genomics is central to malaria research. With the sequencing of "P. falciparum", one of its vectors "Anopheles gambiae", and the human genome, the genetics of all three organisms in the malaria lifecycle can be studied. Another new application of genetic technology is the ability to produce genetically modified mosquitoes that do not transmit malaria, potentially allowing biological control of malaria transmission.

In one study, a genetically-modified strain of "Anopheles stephensi" was created that no longer supported malaria transmission, and this resistance was passed down to mosquito offspring.

Gene drive is a technique for changing wild populations, for instance to combat insects so they cannot transmit diseases (in particular mosquitoes in the cases of malaria and zika).

Nearly 200 parasitic "Plasmodium" species have been identified that infect birds, reptiles, and other mammals, and about 30 species naturally infect non-human primates. Some malaria parasites that affect non-human primates (NHP) serve as model organisms for human malarial parasites, such as "P. coatneyi" (a model for "P. falciparum") and "P. cynomolgi" ("P. vivax"). Diagnostic techniques used to detect parasites in NHP are similar to those employed for humans. Malaria parasites that infect rodents are widely used as models in research, such as "P. berghei". Avian malaria primarily affects species of the order Passeriformes, and poses a substantial threat to birds of Hawaii, the Galapagos, and other archipelagoes. The parasite "P. relictum" is known to play a role in limiting the distribution and abundance of endemic Hawaiian birds. Global warming is expected to increase the prevalence and global distribution of avian malaria, as elevated temperatures provide optimal conditions for parasite reproduction.





</doc>
<doc id="20424" url="https://en.wikipedia.org/wiki?curid=20424" title="Lunar phase">
Lunar phase

The lunar phase or phase of the Moon is the shape of the directly sunlit portion of the Moon as viewed from Earth. The lunar phases gradually and cyclically change over the period of a synodic month (about 29.53 days), as the orbital positions of the Moon around Earth and of Earth around the Sun shift.

The Moon's rotation is tidally locked by Earth's gravity; therefore, most of the same lunar side always faces Earth. This near side is variously sunlit, depending on the position of the Moon in its orbit. Thus, the sunlit portion of this face can vary from 0% (at new moon) to 100% (at full moon). The lunar terminator is the boundary between the illuminated and darkened hemispheres.

Each of the four "intermediate" lunar phases (see below) is around 7.4 days, but this varies slightly due to the elliptical shape of the Moon's orbit. Aside from some craters near the lunar poles, such as Shoemaker, all parts of the Moon see around 14.77 days of daylight, followed by 14.77 days of "night". (The side of the Moon facing away from Earth is sometimes called the "dark side of the Moon", although that is a misnomer.)

In Western culture, the four "principal phases" of the Moon are new moon, first quarter, full moon, and third quarter (also known as last quarter). These are the instances when the Moon's ecliptic longitude and the Sun's ecliptic longitude differ by 0°, 90°, 180°, and 270°, respectively. Each of these phases occur at slightly different times when viewed from different points on Earth. During the intervals between principal phases, the Moon's apparent shape is either crescent or gibbous. These shapes, and the periods when the Moon shows them, are called the "intermediate phases" and last one-quarter of a synodic month, or 7.38 days, on average. However, their durations vary slightly because the Moon's orbit is rather elliptical, so the satellite's orbital speed is not constant. The descriptor "waxing" is used for an intermediate phase when the Moon's apparent shape is thickening, from new to full moon, and "waning" when the shape is thinning.

The eight principal and intermediate phases are given the following names, in sequential order:

Non-Western cultures may use a different number of lunar phases; for example, traditional Hawaiian culture has a total of 30 different phases (one per day).

When the Sun and Moon are aligned on the same side of the Earth, the Moon is "new", and the side of the Moon facing Earth is not illuminated by the Sun. As the Moon "waxes" (the amount of illuminated surface as seen from Earth is increasing), the lunar phases progress through new moon, crescent moon, first-quarter moon, gibbous moon, and full moon. The Moon is then said to "wane" as it passes through the gibbous moon, third-quarter moon, crescent moon, and back to new moon. The terms "old moon" and "new moon" are not interchangeable. The "old moon" is a waning sliver (which eventually becomes undetectable to the naked eye) until the moment it aligns with the Sun and begins to wax, at which point it becomes new again. "Half moon" is often used to mean the first- and third-quarter moons, while the term "quarter" refers to the extent of the Moon's cycle around the Earth, not its shape.

When an illuminated hemisphere is viewed from a certain angle, the portion of the illuminated area that is visible will have a two-dimensional shape as defined by the intersection of an ellipse and circle (in which the ellipse's major axis coincides with the circle's diameter). If the half-ellipse is convex with respect to the half-circle, then the shape will be gibbous (bulging outwards), whereas if the half-ellipse is concave with respect to the half-circle, then the shape will be a crescent. When a crescent moon occurs, the phenomenon of earthshine may be apparent, where the night side of the Moon dimly reflects indirect sunlight reflected from Earth.

In the Northern Hemisphere, if the left (east) side of the Moon is dark, then the bright part is thickening, and the Moon is described as waxing (shifting toward full moon). If the right (west) side of the Moon is dark, then the bright part is thinning, and the Moon is described as waning (past full and shifting toward new moon). Assuming that the viewer is in the Northern Hemisphere, the right side of the Moon is the part that is always waxing. (That is, if the right side is dark, the Moon is becoming darker; if the right side is lit, the Moon is getting brighter.)

In the Southern Hemisphere, the Moon is observed from a perspective inverted, or rotated 180°, to that of the Northern and to all of the images in this article, so that the opposite sides appear to wax or wane.

Closer to the Equator, the lunar terminator will appear horizontal during the morning and evening. Since the above descriptions of the lunar phases only apply at middle or high latitudes, observers moving towards the tropics from northern or southern latitudes will see the Moon rotated anti-clockwise or clockwise with respect to the images in this article.

The lunar crescent can open upward or downward, with the "horns" of the crescent pointing up or down, respectively. When the Sun appears above the Moon in the sky, the crescent opens downward; when the Moon is above the Sun, the crescent opens upward. The crescent Moon is most clearly and brightly visible when the Sun is below the horizon, which implies that the Moon must be above the Sun, and the crescent must open upward. This is therefore the orientation in which the crescent Moon is most often seen from the tropics. The waxing and waning crescents look very similar. The waxing crescent appears in the western sky in the evening, and the waning crescent in the eastern sky in the morning.

When the Moon as seen from Earth is a narrow crescent, Earth as viewed from the Moon is almost fully lit by the Sun. Often, the dark side of the Moon is dimly illuminated by indirect sunlight reflected from Earth, but is bright enough to be easily visible from Earth. This phenomenon is called earthshine and sometimes picturesquely described as "the old moon in the new moon's arms" or, as pictured here, "the new moon in the old moon's arms".

The Gregorian calendar month, which is of a tropical year, is about 30.44 days, while the cycle of lunar phases (the Moon's synodic period) repeats every 29.53 days on average. Therefore, the timing of the lunar phases shifts by an average of almost one day for each successive month. (A lunar year lasts about 354 days.)

Photographing the Moon's phase everyday for a month (starting in the evening after sunset, and repeating roughly 24 hours and 50 minutes later, and ending in the morning before sunrise) and arranging the series of photos on a calendar would create a composite image like the example calendar (May 8 – June 6, 2005) shown on the left. May 20 is blank because a picture would be taken before midnight on May 19 and the next after midnight on May 21.

Similarly, on a calendar listing moonrise or moonset times, some days will appear to be skipped. When moonrise precedes midnight one night, the next moonrise will follow midnight on the next night (so too with moonset). The "skipped day" is just a feature of the Moon's eastward movement in relation to the Sun, which at most latitudes, causes the Moon to rise later each day. The Moon follows a predictable orbit every month.

Each of the 4 lunar phases lasts approximately 7 days (~7.4 days), but varies slightly due to lunar apogee and perigee.

The number of days counted from the time of the New Moon is the Moon's "age". Each complete cycle of phases is called a "lunation".

The approximate age of the moon, and hence the approximate phase, can be calculated for any date by calculating the number of days since a known new moon (such as January 1, 1900 or August 11, 1999) and reducing this modulo 29.530588853 (the length of a synodic month). The difference between two dates can be calculated by subtracting the Julian Day Number of one from that of the other, or there are simpler formulae giving (for instance) the number of days since December 31, 1899. However, this calculation assumes a perfectly circular orbit and makes no allowance for the time of day at which the new moon happened, therefore may be incorrect by several hours (it also becomes less accurate the larger the difference between the required date and the reference date); it is accurate enough to use in a novelty clock application showing moon phase, but specialist usage taking account of lunar apogee and perigee requires a more elaborate calculation.

The Earth subtends an angle of about two degrees, when seen from the Moon. This means that an observer on Earth who sees the Moon when it is close to the eastern horizon sees it from an angle that is about two degrees different from the line of sight of an observer who sees the Moon on the western horizon. The Moon moves about 12 degrees around its orbit per day, so, if these observers were stationary, they would see the phases of the Moon at times that differ by about one-sixth of a day, or four hours. But in reality the observers are on the surface of the rotating Earth, so someone who sees the Moon on the eastern horizon at one moment sees it on the western horizon about 12 hours later. This adds an oscillation to the apparent progression of the lunar phases. They appear to occur more slowly when the Moon is high in the sky than when it is below the horizon. The Moon appears to move jerkily, and the phases do the same. The amplitude of this oscillation is never more than about four hours, which is a small fraction of a month. It does not have any obvious effect on the appearance of the Moon. However, it does affect accurate calculations of the times of lunar phases.

It might be expected that once every month, when the Moon passes between Earth and the Sun during a new moon, its shadow would fall on Earth causing a solar eclipse, but this does not happen every month. Nor is it true that during every full moon, the Earth's shadow falls on the Moon, causing a lunar eclipse. Solar and lunar eclipses are not observed "every" month because the plane of the Moon's orbit around the Earth is tilted by about 5° with respect to the plane of Earth's orbit around the Sun (the plane of the ecliptic). Thus, when new and full moons occur, the Moon usually lies to the north or south of a direct line through the Earth and Sun. Although an eclipse can only occur when the Moon is either new (solar) or full (lunar), it must also be positioned very near the intersection of Earth's orbital plane about the Sun and the Moon's orbital plane about the Earth (that is, at one of its nodes). This happens about twice per year, and so there are between four and seven eclipses in a calendar year. Most of these eclipses are partial; total eclipses of the Moon or Sun are less frequent.





</doc>
<doc id="20426" url="https://en.wikipedia.org/wiki?curid=20426" title="Metonic cycle">
Metonic cycle

For astronomy and calendar studies, the Metonic cycle or Enneadecaeteris (from , "nineteen years") is a period of very close to 19 years that is nearly a common multiple of the solar year and the synodic (lunar) month. The Greek astronomer Meton of Athens (fifth century BC) observed that a period of 19 years is almost exactly equal to 235 synodic months and, rounded to full days, counts 6,940 days. The difference between the two periods (of 19 years and 235 synodic months) is only a few hours, depending on the definition of the year.

Considering a year to be of this 6,940-day cycle gives a year length of 365 +  +  days (the unrounded cycle is much more accurate), which is about 11 days more than 12 synodic months. To keep a 12-month lunar year in pace with the solar year, an intercalary 13th month would have to be added on seven occasions during the nineteen-year period (235 = 19 × 12 + 7). When Meton introduced the cycle around 432 BC, it was already known by Babylonian astronomers.

A mechanical computation of the cycle is built into the Antikythera mechanism.

The cycle was used in the Babylonian calendar, ancient Chinese calendar systems (the 'Rule Cycle' 章) and the medieval computus (i.e. the calculation of the date of Easter). It regulates the 19-year cycle of intercalary months of the modern Hebrew calendar. The start of the Metonic cycle depends on which of these systems is being used; for Easter, the first year of the current Metonic cycle is 2014.

At the time of Meton, axial precession had not yet been discovered, and he could not distinguish between sidereal years (currently: 365.256363 days) and tropical years (currently: 365.242190 days). Most calendars, like the commonly used Gregorian calendar, are based on the tropical year and maintain the seasons at the same calendar times each year. Nineteen tropical years are about two hours shorter than 235 synodic months. The Metonic cycle's error is, therefore, one full day every 219 years, or 12.4 parts per million.

Note that the 19-year cycle is also close (to somewhat more than half a day) to 255 draconic months, so it is also an eclipse cycle, which lasts only for about 4 or 5 recurrences of eclipses. The Octon is of a Metonic cycle (47 synodic months, 3.8 years), and it recurs about 20 to 25 cycles.

This cycle seems to be a coincidence. The periods of the Moon's orbit around the Earth and the Earth's orbit around the Sun are believed to be independent, and not to have any known physical resonance. An example of a non-coincidental cycle is the orbit of Mercury, with its 3:2 spin-orbit resonance.

A lunar year of 12 synodic months is about 354 days, approximately 11 days short of the "365-day" solar year. Therefore, for a lunisolar calendar, every 2 to 3 years there is a difference of more than a full lunar month between the lunar and solar years, and an extra ("embolismic") month needs to be inserted (intercalation). The Athenians initially seem not to have had a regular means of intercalating a 13th month; instead, the question of when to add a month was decided by an official. Meton's discovery made it possible to propose a regular intercalation scheme. The Babylonians seem to have introduced this scheme around 500 BC, thus well before Meton.

Traditionally, for the Babylonian and Hebrew lunisolar calendars, the years 3, 6, 8, 11, 14, 17, and 19 are the long (13-month) years of the Metonic cycle. This cycle, which can be used to predict eclipses, forms the basis of the Greek and Hebrew calendars, and is used for the computation of the date of Easter each year.

The Babylonians applied the 19-year cycle since the late sixth century BC. As they measured the moon's motion against the stars, the 235:19 relationship may originally have referred to sidereal years, instead of tropical years as it has been used for various calendars.

According to Livy, the king of Rome Numa Pompilius (753-673 BC) inserted intercalary months in such a way that "in the twentieth year the days should fall in with the same position of the sun from which they had started." As "the twentieth year" takes place nineteen years after "the first year", this seems to indicate that the Metonic cycle was applied to Numa's calendar.

Apollo was said to have visited the Hyperboreans once every 19 years, presumably at the high point of the cycle.

The Runic calendar is a perpetual calendar based on the 19-year-long Metonic cycle. Also known as a Rune staff or Runic Almanac, it appears to have been a medieval Swedish invention. This calendar does not rely on knowledge of the duration of the tropical year or of the occurrence of leap years. It is set at the beginning of each year by observing the first full moon after the winter solstice. The oldest one known, and the only one from the Middle Ages, is the Nyköping staff, which is believed to date from the 13th century.

The Bahá'í calendar, established during the middle of the 19th century, is also based on cycles of 19 years.

The Metonic cycle is related to two less accurate subcycles:


By combining appropriate numbers of 11-year and 19-year periods, it is possible to generate ever more accurate cycles. For example, simple arithmetic shows that:


This gives an error of only about half an hour in 687 years (2.5 seconds a year), although this is subject to secular variation in the length of the tropical year and the lunation.

Meton of Athens approximated the cycle to a whole number (6,940) of days, obtained by 125 long months of 30 days and 110 short months of 29 days. During the next century, Callippus developed the Callippic cycle of four 19-year periods for a 76-year cycle with a mean year of exactly 365.25 days.





</doc>
<doc id="20427" url="https://en.wikipedia.org/wiki?curid=20427" title="March 26">
March 26







</doc>
<doc id="20428" url="https://en.wikipedia.org/wiki?curid=20428" title="Marcello Malpighi">
Marcello Malpighi

Marcello Malpighi (10 March 1628 – 29 November 1694) was an Italian biologist and physician, who is referred to as the "Father of microscopical anatomy, histology, physiology and embryology". Malpighi's name bears to several physiological features related to the biological excretory system, such as the Malpighian corpuscles and Malpighian pyramids of the kidneys and the Malpighian tubule system of insects. The splenic lymphoid nodules are often called the "Malpighian bodies of the spleen" or Malpighian corpuscles. The botanical family Malpighiaceae is also named after him. He was the first person to see capillaries in animals, and he discovered the link between arteries and veins that had eluded William Harvey. Malpighi was one of the earliest people to observe red blood cells under a microscope, after Jan Swammerdam. His treatise "De polypo cordis" (1666) was important for understanding blood composition, as well as how blood clots. In it, Malpighi described how the form of a blood clot differed in the right against the left sides of the heart.

The use of the microscope enabled Malpighi to discover that invertebrates do not use lungs to breathe, but small holes in their skin called tracheae. Malpighi also studied the anatomy of the brain and concluded this organ is a gland. In terms of modern endocrinology, this deduction is correct because the hypothalamus of the brain has long been recognized for its hormone-secreting capacity.

Because Malpighi had a wide knowledge of both plants and animals, he made contributions to the scientific study of both. The Royal Society of London published two volumes of his botanical and zoological works in 1675 and 1679. Another edition followed in 1687, and a supplementary volume in 1697. In his autobiography, Malpighi speaks of his "Anatome Plantarum", decorated with the engravings of Robert White, as "the most elegant format in the whole literate world."

His study of plants led him to conclude that plants had tubules similar to those he saw in insects like the silk worm (using his microscope, he probably saw the stomata, through which plants exchange carbon dioxide with oxygen). Malpighi observed that when a ring-like portion of bark was removed on a trunk a swelling occurred in the tissues above the ring, and he correctly interpreted this as growth stimulated by food coming down from the leaves, and being blocked above the ring.
Malpighi was born on 10 March 1628 at Crevalcore near Bologna, Italy. The son of well-to-do parents, Malpighi was educated in his native city, entering the University of Bologna at the age of 17. In a posthumous work delivered and dedicated to the Royal Society in London in 1697, Malpighi says he completed his grammatical studies in 1645, at which point he began to apply himself to the study of peripatetic philosophy. He completed these studies about 1649, where at the persuasion of his mother Frances Natalis, he began to study physics. When his parents and grandmother became ill, he returned to his family home near Bologna to care for them.
Malpighi studied Aristotelian philosophy at the University of Bologna while he was very young. 
Despite opposition from the university authorities because he was non-Bolognese by birth, in 1653 he was granted doctorates in both medicine and philosophy. He later graduated as a medical doctor at the age of 25. Subsequently, he was appointed as a teacher, whereupon he immediately dedicated himself to further study in anatomy and medicine. For most of his career, Malpighi combined an intense interest in scientific research with a fond love of teaching. He was invited to correspond with the Royal Society in 1667 by Henry Oldenburg, and became a fellow of the society the next year.

In 1656, Ferdinand II of Tuscany invited him to the professorship of theoretical medicine at the University of Pisa. There Malpighi began his lifelong friendship with Giovanni Borelli, mathematician and naturalist, who was a prominent supporter of the Accademia del Cimento, one of the first scientific societies. Malpighi questioned the prevailing medical teachings at Pisa, tried experiments on colour changes in blood, and attempted to recast anatomical, physiological, and medical problems of the day. Family responsibilities and poor health prompted Malpighi’s return in 1659 to the University of Bologna, where he continued to teach and do research with his microscopes. In 1661 he identified and described the pulmonary and capillary network connecting small arteries with small veins, one of the major discoveries in the history of science. Malpighi’s views evoked increasing controversy and dissent, mainly from envy and lack of understanding on the part of his colleagues.

In 1653, his father, mother, and grandmother being dead, Malpighi left his family villa and returned to the University of Bologna to study anatomy. In 1656, he was made a reader at Bologna, and then a professor of physics at Pisa, where he began to abandon the disputative method of learning and apply himself to a more experimental method of research. Based on this research, he wrote some "Dialogues against the Peripatetics and Galenists" (those who followed the precepts of Galen), which were destroyed when his house burned down. Weary of philosophical disputation, in 1660, Malpighi returned to Bologna and dedicated himself to the study of anatomy. He subsequently discovered a new structure of the lungs which led him to several disputes with the learned medical men of the times. In 1662, he was made a professor of Physics at the Academy of Messina.

Retiring from university life to his villa in the country near Bologna in 1663, he worked as a physician while continuing to conduct experiments on the plants and insects he found on his estate. There he made discoveries of the structure of plants which he published in his "Observations". At the end of 1666, Malpighi was invited to return to the public academy at Messina, which he did in 1667. Although he accepted temporary chairs at the universities of Pisa and Messina, throughout his life he continuously returned to Bologna to practice medicine, a city that repaid him by erecting a monument in his memory after his death.

In 1668, Malpighi received a letter from Mr. Oldenburg of the Royal Society in London, inviting him to correspond. Malpighi wrote his history of the silkworm in 1668, and sent the manuscript to Mr. Oldenburg. As a result, Malpighi was made a member of the Royal Society in 1669. In 1671, Malpighi’s "Anatomy of Plants" was published in London by the Royal Society, and he simultaneously wrote to Mr. Oldenburg, telling him of his recent discoveries regarding the lungs, fibers of the spleen and testicles, and several other discoveries involving the brain and sensory organs. He also shared more information regarding his research on plants. At that time, he related his disputes with some younger physicians who were strenuous supporters of the Galenic principles and opposed to all new discoveries. Following many other discoveries and publications, in 1691, Malpighi was uprooted from his beloved home in Bologna and summoned to Rome by Pope Innocent XII.

Marcello Malpighi is buried in the church of the Santi Gregorio e Siro, in Bologna, where nowadays can be seen a marble monument to the scientist with an inscription in Latin remembering – among other things – his "SUMMUM INGENIUM / INTEGERRIMAM VITAM / FORTEM STRENUAMQUE MENTEM / AUDACEM SALUTARIS ARTIS AMOREM" (great genius, honest life, strong and tough mind, daring love for the medical art).

Around the age of 38, and with a remarkable academic career behind him, Malpighi decided to dedicate his free time to anatomical studies. Although he conducted some of his studies using vivisection and others through the dissection of corpses, his most illustrative efforts appear to have been based on the use of the microscope. Because of this work, many microscopic anatomical structures are named after Malpighi, including a skin layer (Malpighi layer) and two different Malpighian corpuscles in the kidneys and the spleen, as well as the Malpighian tubules in the excretory system of insects.

See Timeline of microscope technology for more information.
Although a Dutch spectacle maker created the compound lens and inserted it in a microscope around the turn of the 17th century, and Galileo had applied the principle of the compound lens to the making of his microscope patented in 1609, its possibilities as a microscope had remained unexploited for half a century, until Robert Hooke improved the instrument. Following this, Marcello Malpighi, Hooke, and two other early investigators associated with the Royal Society, Nehemiah Grew and Antoine van Leeuwenhoek were fortunate to have a virtually untried tool in their hands as they began their investigations.

Working on frogs and extrapolating to humans, Malpighi demonstrated the structure of the lungs, previously thought to be a homogeneous mass of flesh, and he offered an explanation for how air and blood mixed in the lungs. Malpighi also used the microscope for his studies of the skin, kidneys, and liver. For example, after he dissected a black male, Malpighi made some groundbreaking headway into the discovery of the origin of black skin. He found that the black pigment was associated with a layer of mucus just beneath the skin.

A talented sketch artist, Malpighi seems to have been the first author to have made detailed drawings of individual organs of flowers. In his "Anatome plantarum" is a longitudinal section of a flower of "Nigella" (his Melanthi, literally honey-flower) with details of the nectariferous organs. He adds that it is strange that nature has produced on the leaves of the flower shell-like organs in which honey is produced.

Malpighi had success in tracing the ontogeny of plant organs, and the serial development of the shoot owing to his instinct shaped in the sphere of animal embryology. He specialized in seedling development, and in 1679, he published a volume containing a series of exquisitely drawn and engraved images of the stages of development of Leguminosae (beans) and Cucurbitaceae (squash, melons). Later, he published material depicting the development of the date palm. The great Swedish botanist Linnaeus named the genus "Malpighia" in honor of Malpighi’s work with plants; "Malpighia" is the type genus for the Malpighiaceae, a family of tropical and subtropical flowering plants.

Because Malpighi was concerned with teratology (the scientific study of the visible conditions caused by the interruption or alteration of normal development) he expressed grave misgivings about the view of his contemporaries that the galls of trees and herbs gave birth to insects. He conjectured (correctly) that the creatures in question arose from eggs previously laid in the plant tissue.

Malpighi’s investigations of the lifecycle of plants and animals led him into the topic of reproduction. He created detailed drawings of his studies of chick embryo development, seed development in plants (such as the lemon tree), and the transformation of caterpillars into insects. His discoveries helped to illuminate philosophical arguments surrounding the topics of "emboîtment", pre-existence, preformation, epigenesis, and metamorphosis.

In 1691 Pope Innocent XII invited him to Rome as papal physician. He taught medicine in the Papal Medical School and wrote a long treatise about his studies which he donated to the Royal Society of London.

Marcello Malpighi died of apoplexy (an old-fashioned term for a stroke or stroke-like symptoms) in Rome on 29 September 1694, at the age of 66. In accordance with his wishes, an autopsy was performed. The Royal Society published his studies in 1696. Asteroid 11121 Malpighi is named in his honor.

Malpighi is buried in the church of the Santi Gregorio e Siro, in Bologna, where nowadays can be seen a marble monument to the scientist with an inscription in Latin remembering – among other things – his "SUMMUM INGENIUM / INTEGERRIMAM VITAM / FORTEM STRENUAMQUE MENTEM / AUDACEM SALUTARIS ARTIS AMOREM" (great genius, honest life, strong and tough mind, daring love for the medical art).




</doc>
<doc id="20431" url="https://en.wikipedia.org/wiki?curid=20431" title="Momentum">
Momentum

In Newtonian mechanics, linear momentum, translational momentum, or simply momentum (pl. momenta) is the product of the mass and velocity of an object. It is a vector quantity, possessing a magnitude and a direction in three-dimensional space. If is an object's mass and is the velocity (also a vector), then the momentum is
In SI units, it is measured in kilogram meters per second (kg⋅m/s). Newton's second law of motion states that a body's rate of change in momentum is equal to the net force acting on it.

Momentum depends on the frame of reference, but in any inertial frame it is a "conserved" quantity, meaning that if a closed system is not affected by external forces, its total linear momentum does not change. Momentum is also conserved in special relativity, (with a modified formula) and, in a modified form, in electrodynamics, quantum mechanics, quantum field theory, and general relativity. It is an expression of one of the fundamental symmetries of space and time: translational symmetry.

Advanced formulations of classical mechanics, Lagrangian and Hamiltonian mechanics, allow one to choose coordinate systems that incorporate symmetries and constraints. In these systems the conserved quantity is "generalized momentum", and in general this is different from the "kinetic" momentum defined above. The concept of generalized momentum is carried over into quantum mechanics, where it becomes an operator on a wave function. The momentum and position operators are related by the Heisenberg uncertainty principle.

In continuous systems such as electromagnetic fields, fluids and deformable bodies, a momentum density can be defined, and a continuum version of the conservation of momentum leads to equations such as the Navier–Stokes equations for fluids or the Cauchy momentum equation for deformable solids or fluids.

Momentum is a vector quantity: it has both magnitude and direction. Since momentum has a direction, it can be used to predict the resulting direction and speed of motion of objects after they collide. Below, the basic properties of momentum are described in one dimension. The vector equations are almost identical to the scalar equations (see multiple dimensions).

The momentum of a particle is conventionally represented by the letter . It is the product of two quantities, the particle's mass (represented by the letter ) and its velocity ():

The unit of momentum is the product of the units of mass and velocity. In SI units, if the mass is in kilograms and the velocity is in meters per second then the momentum is in kilogram meters per second (kg⋅m/s). In cgs units, if the mass is in grams and the velocity in centimeters per second, then the momentum is in gram centimeters per second (g⋅cm/s).

Being a vector, momentum has magnitude and direction. For example, a 1 kg model airplane, traveling due north at 1 m/s in straight and level flight, has a momentum of 1 kg⋅m/s due north measured with reference to the ground.

The momentum of a system of particles is the sum of their momenta. If two particles have respective masses and , and velocities and , the total momentum is
The momenta of more than two particles can be added more generally with the following:

A system of particles has a center of mass, a point determined by the weighted sum of their positions:

If all the particles are moving, the center of mass will generally be moving as well (unless the system is in pure rotation around it). If the center of mass is moving at velocity , the momentum is:
This is known as Euler's first law.

If the net force applied to a particle is a constant , and is applied for a time interval , the momentum of the particle changes by an amount

In differential form, this is Newton's second law; the rate of change of the momentum of a particle is equal to the instantaneous force acting on it,

If the net force experienced by a particle changes as a function of time, , the change in momentum (or impulse ) between times and is

Impulse is measured in the derived units of the newton second (1 N⋅s = 1 kg⋅m/s) or dyne second (1 dyne⋅s = 1 g⋅m/s)

Under the assumption of constant mass , it is equivalent to write
hence the net force is equal to the mass of the particle, times its acceleration.

"Example": A model airplane of mass 1 kg accelerates from rest to a velocity of 6 m/s due north in 2 s. The net force required to produce this acceleration is 3 newtons due north. The change in momentum is 6 kg⋅m/s. The rate of change of momentum is 3 (kg⋅m/s)/s = 3 N.

In a closed system (one that does not exchange any matter with its surroundings and is not acted on by external forces) the total momentum is constant. This fact, known as the "law of conservation of momentum", is implied by Newton's laws of motion. Suppose, for example, that two particles interact. Because of the third law, the forces between them are equal and opposite. If the particles are numbered 1 and 2, the second law states that and . Therefore,
with the negative sign indicating that the forces oppose. Equivalently,

If the velocities of the particles are and before the interaction, and afterwards they are and , then

This law holds no matter how complicated the force is between particles. Similarly, if there are several particles, the momentum exchanged between each pair of particles adds up to zero, so the total change in momentum is zero. This conservation law applies to all interactions, including collisions and separations caused by explosive forces. It can also be generalized to situations where Newton's laws do not hold, for example in the theory of relativity and in electrodynamics.

Momentum is a measurable quantity, and the measurement depends on the motion of the observer. For example: if an apple is sitting in a glass elevator that is descending, an outside observer, looking into the elevator, sees the apple moving, so, to that observer, the apple has a non-zero momentum. To someone inside the elevator, the apple does not move, so, it has zero momentum. The two observers each have a frame of reference, in which, they observe motions, and, if the elevator is descending steadily, they will see behavior that is consistent with those same physical laws.

Suppose a particle has position in a stationary frame of reference. From the point of view of another frame of reference, moving at a uniform speed , the position (represented by a primed coordinate) changes with time as
This is called a Galilean transformation. If the particle is moving at speed in the first frame of reference, in the second, it is moving at speed
Since does not change, the accelerations are the same:
Thus, momentum is conserved in both reference frames. Moreover, as long as the force has the same form, in both frames, Newton's second law is unchanged. Forces such as Newtonian gravity, which depend only on the scalar distance between objects, satisfy this criterion. This independence of reference frame is called Newtonian relativity or Galilean invariance.

A change of reference frame, can, often, simplify calculations of motion. For example, in a collision of two particles, a reference frame can be chosen, where, one particle begins at rest. Another, commonly used reference frame, is the center of mass frame – one that is moving with the center of mass. In this frame,
the total momentum is zero.

By itself, the law of conservation of momentum is not enough to determine the motion of particles after a collision. Another property of the motion, kinetic energy, must be known. This is not necessarily conserved. If it is conserved, the collision is called an "elastic collision"; if not, it is an "inelastic collision".

An elastic collision is one in which no kinetic energy is absorbed in the collision. Perfectly elastic "collisions" can occur when the objects do not touch each other, as for example in atomic or nuclear scattering where electric repulsion keeps them apart. A slingshot maneuver of a satellite around a planet can also be viewed as a perfectly elastic collision. A collision between two pool balls is a good example of an "almost" totally elastic collision, due to their high rigidity, but when bodies come in contact there is always some dissipation.

A head-on elastic collision between two bodies can be represented by velocities in one dimension, along a line passing through the bodies. If the velocities are and before the collision and and after, the equations expressing conservation of momentum and kinetic energy are:

A change of reference frame can simplify analysis of a collision. For example, suppose there are two bodies of equal mass , one stationary and one approaching the other at a speed (as in the figure). The center of mass is moving at speed and both bodies are moving towards it at speed . Because of the symmetry, after the collision both must be moving away from the center of mass at the same speed. Adding the speed of the center of mass to both, we find that the body that was moving is now stopped and the other is moving away at speed . The bodies have exchanged their velocities. Regardless of the velocities of the bodies, a switch to the center of mass frame leads us to the same conclusion. Therefore, the final velocities are given by

In general, when the initial velocities are known, the final velocities are given by
If one body has much greater mass than the other, its velocity will be little affected by a collision while the other body will experience a large change.

In an inelastic collision, some of the kinetic energy of the colliding bodies is converted into other forms of energy (such as heat or sound). Examples include traffic collisions, in which the effect of lost kinetic energy can be seen in the damage to the vehicles; electrons losing some of their energy to atoms (as in the Franck–Hertz experiment); and particle accelerators in which the kinetic energy is converted into mass in the form of new particles.

In a perfectly inelastic collision (such as a bug hitting a windshield), both bodies have the same motion afterwards. If one body is motionless to begin with, the equation for conservation of momentum is
so
In a frame of reference moving at the speed , the objects are brought to rest by the collision and 100% of the kinetic energy is converted to other forms of energy.

One measure of the inelasticity of the collision is the coefficient of restitution , defined as the ratio of relative velocity of separation to relative velocity of approach. In applying this measure to a ball bouncing from a solid surface, this can be easily measured using the following formula:

The momentum and energy equations also apply to the motions of objects that begin together and then move apart. For example, an explosion is the result of a chain reaction that transforms potential energy stored in chemical, mechanical, or nuclear form into kinetic energy, acoustic energy, and electromagnetic radiation. Rockets also make use of conservation of momentum: propellant is thrust outward, gaining momentum, and an equal and opposite momentum is imparted to the rocket.

Real motion has both direction and velocity and must be represented by a vector. In a coordinate system with axes, velocity has components in the -direction, in the -direction, in the -direction. The vector is represented by a boldface symbol:
Similarly, the momentum is a vector quantity and is represented by a boldface symbol:

The equations in the previous sections, work in vector form if the scalars and are replaced by vectors and . Each vector equation represents three scalar equations. For example,
represents three equations:

The kinetic energy equations are exceptions to the above replacement rule. The equations are still one-dimensional, but each scalar represents the magnitude of the vector, for example,
Each vector equation represents three scalar equations. Often coordinates can be chosen so that only two components are needed, as in the figure. Each component can be obtained separately and the results combined to produce a vector result.

A simple construction involving the center of mass frame can be used to show that if a stationary elastic sphere is struck by a moving sphere, the two will head off at right angles after the collision (as in the figure).

The concept of momentum plays a fundamental role in explaining the behavior of variable-mass objects such as a rocket ejecting fuel or a star accreting gas. In analyzing such an object, one treats the object's mass as a function that varies with time: . The momentum of the object at time is therefore . One might then try to invoke Newton's second law of motion by saying that the external force on the object is related to its momentum by , but this is incorrect, as is the related expression found by applying the product rule to :

This equation does not correctly describe the motion of variable-mass objects. The correct equation is
where is the velocity of the ejected/accreted mass "as seen in the object's rest frame". This is distinct from , which is the velocity of the object itself as seen in an inertial frame.

This equation is derived by keeping track of both the momentum of the object as well as the momentum of the ejected/accreted mass ("dm"). When considered together, the object and the mass ("dm") constitute a closed system in which total momentum is conserved.

Newtonian physics assumes that absolute time and space exist outside of any observer; this gives rise to Galilean invariance. It also results in a prediction that the speed of light can vary from one reference frame to another. This is contrary to observation. In the special theory of relativity, Einstein keeps the postulate that the equations of motion do not depend on the reference frame, but assumes that the speed of light is invariant. As a result, position and time in two reference frames are related by the Lorentz transformation instead of the Galilean transformation.

Consider, for example, a reference frame moving relative to another at velocity in the direction. The Galilean transformation gives the coordinates of the moving frame as
while the Lorentz transformation gives
where is the Lorentz factor:

Newton's second law, with mass fixed, is not invariant under a Lorentz transformation. However, it can be made invariant by making the "inertial mass" of an object a function of velocity:

The modified momentum,
obeys Newton's second law:

Within the domain of classical mechanics, relativistic momentum closely approximates Newtonian momentum: at low velocity, is approximately equal to , the Newtonian expression for momentum.

In the theory of special relativity, physical quantities are expressed in terms of four-vectors that include time as a fourth coordinate along with the three space coordinates. These vectors are generally represented by capital letters, for example for position. The expression for the "four-momentum" depends on how the coordinates are expressed. Time may be given in its normal units or multiplied by the speed of light so that all the components of the four-vector have dimensions of length. If the latter scaling is used, an interval of proper time, , defined by
is invariant under Lorentz transformations (in this expression and in what follows the metric signature has been used, different authors use different conventions). Mathematically this invariance can be ensured in one of two ways: by treating the four-vectors as Euclidean vectors and multiplying time by ; or by keeping time a real quantity and embedding the vectors in a Minkowski space. In a Minkowski space, the scalar product of two four-vectors and is defined as

In all the coordinate systems, the (contravariant) relativistic four-velocity is defined by
and the (contravariant) four-momentum is
where is the invariant mass. If (in Minkowski space), then
Using Einstein's mass-energy equivalence, , this can be rewritten as
Thus, conservation of four-momentum is Lorentz-invariant and implies conservation of both mass and energy.

The magnitude of the momentum four-vector is equal to :
and is invariant across all reference frames.

The relativistic energy–momentum relationship holds even for massless particles such as photons; by setting it follows that

In a game of relativistic "billiards", if a stationary particle is hit by a moving particle in an elastic collision, the paths formed by the two afterwards will form an acute angle. This is unlike the non-relativistic case where they travel at right angles.

The four-momentum of a planar wave can be related to a wave four-vector
For a particle, the relationship between temporal components, , is the Planck–Einstein relation, and the relation between spatial components, , describes a de Broglie matter wave.

Newton's laws can be difficult to apply to many kinds of motion because the motion is limited by "constraints". For example, a bead on an abacus is constrained to move along its wire and a pendulum bob is constrained to swing at a fixed distance from the pivot. Many such constraints can be incorporated by changing the normal Cartesian coordinates to a set of "generalized coordinates" that may be fewer in number. Refined mathematical methods have been developed for solving mechanics problems in generalized coordinates. They introduce a "generalized momentum", also known as the "canonical" or "conjugate momentum", that extends the concepts of both linear momentum and angular momentum. To distinguish it from generalized momentum, the product of mass and velocity is also referred to as "mechanical", "kinetic" or "kinematic momentum". The two main methods are described below.

In Lagrangian mechanics, a Lagrangian is defined as the difference between the kinetic energy and the potential energy :

If the generalized coordinates are represented as a vector and time differentiation is represented by a dot over the variable, then the equations of motion (known as the Lagrange or Euler–Lagrange equations) are a set of equations:
If a coordinate is not a Cartesian coordinate, the associated generalized momentum component does not necessarily have the dimensions of linear momentum. Even if is a Cartesian coordinate, will not be the same as the mechanical momentum if the potential depends on velocity. Some sources represent the kinematic momentum by the symbol .

In this mathematical framework, a generalized momentum is associated with the generalized coordinates. Its components are defined as
Each component is said to be the "conjugate momentum" for the coordinate .

Now if a given coordinate does not appear in the Lagrangian (although its time derivative might appear), then
This is the generalization of the conservation of momentum.

Even if the generalized coordinates are just the ordinary spatial coordinates, the conjugate momenta are not necessarily the ordinary momentum coordinates. An example is found in the section on electromagnetism.

In Hamiltonian mechanics, the Lagrangian (a function of generalized coordinates and their derivatives) is replaced by a Hamiltonian that is a function of generalized coordinates and momentum. The Hamiltonian is defined as
where the momentum is obtained by differentiating the Lagrangian as above. The Hamiltonian equations of motion are
As in Lagrangian mechanics, if a generalized coordinate does not appear in the Hamiltonian, its conjugate momentum component is conserved.

Conservation of momentum is a mathematical consequence of the homogeneity (shift symmetry) of space (position in space is the canonical conjugate quantity to momentum). That is, conservation of momentum is a consequence of the fact that the laws of physics do not depend on position; this is a special case of Noether's theorem.

In Maxwell's equations, the forces between particles are mediated by electric and magnetic fields. The electromagnetic force ("Lorentz force") on a particle with charge due to a combination of electric field and magnetic field is
(in SI units).
It has an electric potential and magnetic vector potential .
In the non-relativistic regime, its generalized momentum is
while in relativistic mechanics this becomes

In Newtonian mechanics, the law of conservation of momentum can be derived from the law of action and reaction, which states that every force has a reciprocating equal and opposite force. Under some circumstances, moving charged particles can exert forces on each other in non-opposite directions. Nevertheless, the combined momentum of the particles and the electromagnetic field is conserved.

The Lorentz force imparts a momentum to the particle, so by Newton's second law the particle must impart a momentum to the electromagnetic fields.

In a vacuum, the momentum per unit volume is
where is the vacuum permeability and is the speed of light. The momentum density is proportional to the Poynting vector which gives the directional rate of energy transfer per unit area:

If momentum is to be conserved over the volume over a region , changes in the momentum of matter through the Lorentz force must be balanced by changes in the momentum of the electromagnetic field and outflow of momentum. If is the momentum of all the particles in , and the particles are treated as a continuum, then Newton's second law gives
The electromagnetic momentum is
and the equation for conservation of each component of the momentum is
The term on the right is an integral over the surface area of the surface representing momentum flow into and out of the volume, and is a component of the surface normal of . The quantity is called the Maxwell stress tensor, defined as

The above results are for the "microscopic" Maxwell equations, applicable to electromagnetic forces in a vacuum (or on a very small scale in media). It is more difficult to define momentum density in media because the division into electromagnetic and mechanical is arbitrary. The definition of electromagnetic momentum density is modified to
where the H-field is related to the B-field and the magnetization by
The electromagnetic stress tensor depends on the properties of the media.

In quantum mechanics, momentum is defined as a self-adjoint operator on the wave function. The Heisenberg uncertainty principle defines limits on how accurately the momentum and position of a single observable system can be known at once. In quantum mechanics, position and momentum are conjugate variables.

For a single particle described in the position basis the momentum operator can be written as

where is the gradient operator, is the reduced Planck constant, and is the imaginary unit. This is a commonly encountered form of the momentum operator, though the momentum operator in other bases can take other forms. For example, in momentum space the momentum operator is represented as

where the operator acting on a wave function yields that wave function multiplied by the value , in an analogous fashion to the way that the position operator acting on a wave function yields that wave function multiplied by the value "x".

For both massive and massless objects, relativistic momentum is related to the phase constant formula_66 by
Electromagnetic radiation (including visible light, ultraviolet light, and radio waves) is carried by photons. Even though photons (the particle aspect of light) have no mass, they still carry momentum. This leads to applications such as the solar sail. The calculation of the momentum of light within dielectric media is somewhat controversial (see Abraham–Minkowski controversy).

In fields such as fluid dynamics and solid mechanics, it is not feasible to follow the motion of individual atoms or molecules. Instead, the materials must be approximated by a continuum in which there is a particle or fluid parcel at each point that is assigned the average of the properties of atoms in a small region nearby. In particular, it has a density and velocity that depend on time and position . The momentum per unit volume is .

Consider a column of water in hydrostatic equilibrium. All the forces on the water are in balance and the water is motionless. On any given drop of water, two forces are balanced. The first is gravity, which acts directly on each atom and molecule inside. The gravitational force per unit volume is , where is the gravitational acceleration. The second force is the sum of all the forces exerted on its surface by the surrounding water. The force from below is greater than the force from above by just the amount needed to balance gravity. The normal force per unit area is the pressure . The average force per unit volume inside the droplet is the gradient of the pressure, so the force balance equation is

If the forces are not balanced, the droplet accelerates. This acceleration is not simply the partial derivative because the fluid in a given volume changes with time. Instead, the material derivative is needed:
Applied to any physical quantity, the material derivative includes the rate of change at a point and the changes due to advection as fluid is carried past the point. Per unit volume, the rate of change in momentum is equal to . This is equal to the net force on the droplet.

Forces that can change the momentum of a droplet include the gradient of the pressure and gravity, as above. In addition, surface forces can deform the droplet. In the simplest case, a shear stress , exerted by a force parallel to the surface of the droplet, is proportional to the rate of deformation or strain rate. Such a shear stress occurs if the fluid has a velocity gradient because the fluid is moving faster on one side than another. If the speed in the direction varies with , the tangential force in direction per unit area normal to the direction is
where is the viscosity. This is also a flux, or flow per unit area, of x-momentum through the surface.

Including the effect of viscosity, the momentum balance equations for the incompressible flow of a Newtonian fluid are
These are known as the Navier–Stokes equations.

The momentum balance equations can be extended to more general materials, including solids. For each surface with normal in direction and force in direction , there is a stress component . The nine components make up the Cauchy stress tensor , which includes both pressure and shear. The local conservation of momentum is expressed by the Cauchy momentum equation:
where is the body force.

The Cauchy momentum equation is broadly applicable to deformations of solids and liquids. The relationship between the stresses and the strain rate depends on the properties of the material (see Types of viscosity).

A disturbance in a medium gives rise to oscillations, or waves, that propagate away from their source. In a fluid, small changes in pressure can often be described by the acoustic wave equation:
where is the speed of sound. In a solid, similar equations can be obtained for propagation of pressure (P-waves) and shear (S-waves).

The flux, or transport per unit area, of a momentum component by a velocity is equal to . In the linear approximation that leads to the above acoustic equation, the time average of this flux is zero. However, nonlinear effects can give rise to a nonzero average. It is possible for momentum flux to occur even though the wave itself does not have a mean momentum.

In about 530 AD, working in Alexandria, Byzantine philosopher John Philoponus developed a concept of momentum in his commentary to Aristotle's "Physics". Aristotle claimed that everything that is moving must be kept moving by something. For example, a thrown ball must be kept moving by motions of the air. Most writers continued to accept Aristotle's theory until the time of Galileo, but a few were skeptical. Philoponus pointed out the absurdity in Aristotle's claim that motion of an object is promoted by the same air that is resisting its passage. He proposed instead that an impetus was imparted to the object in the act of throwing it. Ibn Sīnā (also known by his Latinized name Avicenna) read Philoponus and published his own theory of motion in "The Book of Healing" in 1020. He agreed that an impetus is imparted to a projectile by the thrower; but unlike Philoponus, who believed that it was a temporary virtue that would decline even in a vacuum, he viewed it as a persistent, requiring external forces such as air resistance to dissipate it.
The work of Philoponus, and possibly that of Ibn Sīnā, was read and refined by the European philosophers Peter Olivi and Jean Buridan. Buridan, who in about 1350 was made rector of the University of Paris, referred to impetus being proportional to the weight times the speed. Moreover, Buridan's theory was different from his predecessor's in that he did not consider impetus to be self-dissipating, asserting that a body would be arrested by the forces of air resistance and gravity which might be opposing its impetus.

René Descartes believed that the total "quantity of motion" () in the universe is conserved, where the quantity of motion is understood as the product of size and speed. This should not be read as a statement of the modern law of momentum, since he had no concept of mass as distinct from weight and size, and more importantly he believed that it is speed rather than velocity that is conserved. So for Descartes if a moving object were to bounce off a surface, changing its direction but not its speed, there would be no change in its quantity of motion. Galileo, in his "Two New Sciences", used the Italian word "impeto" to similarly describe Descarte's quantity of motion.

Leibniz, in his "Discourse on Metaphysics", gave an argument against Descartes' construction of the conservation of the "quantity of motion" using an example of dropping blocks of different sizes different distances. He points out that force is conserved but quantity of motion, construed as the product of size and speed of an object, is not conserved.

The first correct statement of the law of conservation of momentum was by English mathematician John Wallis in his 1670 work, "Mechanica sive De Motu, Tractatus Geometricus": "the initial state of the body, either of rest or of motion, will persist" and "If the force is greater than the resistance, motion will result". Wallis uses "momentum" and "vis" for force. Newton's "Philosophiæ Naturalis Principia Mathematica", when it was first published in 1687, showed a similar casting around for words to use for the mathematical momentum. His Definition II defines "quantitas motus", "quantity of motion", as "arising from the velocity and quantity of matter conjointly", which identifies it as momentum. Thus when in Law II he refers to "mutatio motus", "change of motion", being proportional to the force impressed, he is generally taken to mean momentum and not motion. It remained only to assign a standard term to the quantity of motion. The first use of "momentum" in its proper mathematical sense is not clear but by the time of Jenning's "Miscellanea" in 1721, five years before the final edition of Newton's "Principia Mathematica", momentum or "quantity of motion" was being defined for students as "a rectangle", the product of and , where is "quantity of material" and is "velocity", .




</doc>
<doc id="20432" url="https://en.wikipedia.org/wiki?curid=20432" title="Mood stabilizer">
Mood stabilizer

A mood stabilizer is a psychiatric pharmaceutical drug used to treat mood disorders characterized by intense and sustained mood shifts, typically bipolar disorder type I or type II, borderline personality disorder (BPD) and schizoaffective disorder.

Used to treat bipolar disorder, mood stabilizers suppress swings between mania and depression. Mood-stabilizing drugs are also used in borderline personality disorder and schizoaffective disorder.

The term "mood stabilizer" does not describe a mechanism, but rather an effect. More precise terminology is used to classify these agents.

Drugs commonly classed as mood stabilizers include:


Many agents described as "mood stabilizers" are also categorized as anticonvulsants. The term "anticonvulsant mood stabilizers" is sometimes used to describe these as a class. Although this group is also defined by effect rather than mechanism, there is at least a preliminary understanding of the mechanism of most of the anticonvulsants used in the treatment of mood disorders.


There is insufficient evidence to support the use of various other anticonvulsants, such as gabapentin and topiramate, as mood stabilizers.



In routine practice, monotherapy is often not sufficiently effective for acute and/or maintenance therapy and thus most patients are given combination therapies. Combination therapy (atypical antipsychotic with lithium or valproate) shows better efficacy over monotherapy in the manic phase in terms of efficacy and prevention of relapse. However, side effects are more frequent and discontinuation rates due to adverse events are higher with combination therapy than with monotherapy.

Most mood stabilizers are primarily antimanic agents, meaning that they are effective at treating mania and mood cycling and shifting, but are not effective at treating acute depression. The principal exceptions to that rule, because they treat both manic and depressive symptoms, are lamotrigine, lithium carbonate and quetiapine.

Nevertheless, antidepressants are still often prescribed in addition to mood stabilizers during depressive phases. This brings some risks, however, as antidepressants can induce mania, psychosis, and other disturbing problems in people with bipolar disorder—in particular, when taken alone. The risk of antidepressant-induced mania when given to patients concomitantly on antimanic agents is not known for certain but may still exist. The majority of antidepressants appear ineffective in treating bipolar depression.

Antidepressants cause several risks when given to bipolar patients. They are ineffective in treating acute bipolar depression, preventing relapse, and can cause rapid cycling. Studies have been shown that antidepressants have no benefit versus a placebo or other treatment. Antidepressants can also lead to a higher rate of non-lethal suicidal behavior. Relapse can also be related to treatment with antidepressants. This is less likely to occur if a mood stabilizer is combined with an antidepressant, rather than an antidepressant being used alone. Evidence from previous studies shows that rapid cycling is linked to use of antidepressants. Rapid cycling is defined as the presence of four or more mood episodes within a year's time. Evidence suggests that rapid cycling and mixed symptoms have become more common since antidepressant medication has come into widespread use. There is a need for caution when treating bipolar patients with antidepressant medication due to the risks that they pose. 

Use of mood stabilizers and anticonvulsants such as lamotrigine, carbamazapine, valproate and others may lead to chronic folate deficiency, potentiating depression. Also, "Folate deficiency may increase the risk of depression and reduce the action of antidepressants." L-methylfolate (also formally known as 5-MTHF or Levofolinic acid), a centrally acting trimonoamine modulator, boosts the synthesis of three CNS neurotransmitters: dopamine, norepinephrine and serotonin. Mood stabilizers and anticonvulsants may interfere with folic acid absorption and L-methylfolate formation. Augmentation with the medical food L-methylfolate may improve antidepressant effects of these medicines, including lithium and antidepressants themselves, by boosting the synthesis of antidepressant neurotransmitters. However, the U.S. National Institutes of Health issued a warning caution about the use of L-methylfolate for patients with bipolar disease.

The precise mechanism of action of lithium is still unknown, and it is suspected that it acts at various points of the neuron between the nucleus and the synapse. Lithium is known to inhibit the enzyme GSK-3B. This has the effect relieving pressure on the circadian clock - which is thought to be often malfunctioning in people with bipolar disorder - and positively modulates gene transcription of brain-derived neurotrophic factor (BDNF). The resulting increase in neural plasticity may be central to lithium's therapeutic effects. Lithium may also increase the synthesis of serotonin.

All of the anticonvulsants routinely used to treat bipolar disorder are blockers of voltage-gated sodium channels, affecting the brain's glutamate system. For valproic acid, carbamazepine and oxcarbazepine, however, their mood-stabilizing effects may be more related to effects on the GABAergic system. Lamotrigine is known to decrease the patient's cortisol response to stress.

One possible downstream target of several mood stabilizers such as lithium, valproate, and carbamazepine is the arachidonic acid cascade.



</doc>
<doc id="20433" url="https://en.wikipedia.org/wiki?curid=20433" title="Mere Christianity">
Mere Christianity

Mere Christianity is a theological book by C. S. Lewis, adapted from a series of BBC radio talks made between 1941 and 1944, while Lewis was at Oxford during the Second World War. Considered a classic of Christian apologetics, the transcripts of the broadcasts originally appeared in print as three separate pamphlets: "The Case for Christianity" ("Broadcast Talks" in the UK) (1942), "Christian Behaviour" (1943), and "Beyond Personality" (1944). Lewis was invited to give the talks by the Reverend James Welch, the BBC Director of Religious Broadcasting, who had read his 1940 book, "The Problem of Pain".

Lewis, an Anglican, intended to describe the Christian common ground. In "Mere Christianity", he aims at avoiding controversies to explain fundamental teachings of Christianity, for the sake of those basically educated as well as the intellectuals of his generation, for whom the jargon of formal Christian theology did not retain its original meaning.

Lewis spends most of his defense of the Christian faith on an argument from morality, a point which persuaded him from atheism to Christianity. He bases his case on a moral law, a "rule about right and wrong" commonly known to all human beings, citing the example of Nazism; both Christians and atheists believed that Hitler's actions were morally wrong. On a more mundane level, it is generally accepted that stealing is violating this moral law. Lewis argues that the moral law is like the law of nature in that it was not contrived by humans. However, it is unlike natural laws in that it can be broken or ignored, and it is known intuitively, rather than through observation. After introducing the moral law, Lewis argues that thirst reflects the fact that people naturally need water, and there is no other substance which satisfies that need. Lewis points out that earthly experience does not satisfy the human craving for "joy" and that only God could fit the bill; humans cannot know to yearn for something if it does not exist.

After providing reasons for his conversion to theism, Lewis goes over rival conceptions of God to Christianity. Pantheism, he argues, is incoherent, and atheism too simple. Eventually he arrives at Jesus Christ, and invokes a well-known argument now known as the "Lewis trilemma". Lewis, arguing that Jesus was claiming to be God, uses logic to advance three possibilities: either he really was God, was deliberately lying, or was not God but thought himself to be (which would make him delusional and likely insane). The book goes on to say that the latter two possibilities are not consistent with Jesus' character and it was most likely that he was being truthful.

Lewis claims that to understand Christianity, one must understand the moral law, which is the underlying structure of the universe and is "hard as nails." Unless one grasps the dismay which comes from humanity's failure to keep the moral law, one cannot understand the coming of Christ and his work. The eternal God who is the law's source takes primacy over the created Satan whose rebellion undergirds all evil. The death and resurrection of Christ is introduced as the only way in which our inadequate human attempts to redeem humanity's sins could be made adequate in God's eyes.

God "became a man" in Christ, Lewis says, so that mankind could be "amalgamated with God's nature" and make full atonement possible. Lewis offers several analogies to explain this abstract concept: that of Jesus "paying the penalty" for a crime, "paying a debt," or helping humanity out of a hole. His main point, however, is that redemption is so incomprehensible that it cannot be fully appreciated, and he attempts to explain that the method by which God atones for the sins of humanity is not nearly as important as the fact that he does so.

The next third of the book explores the ethics resulting from Christian belief. He cites the four cardinal virtues: prudence, justice, temperance, and fortitude. After touching on these, he goes into the three theological virtues: hope, faith, and charity. Lewis also explains morality as being composed of three "layers": relationships between man and man, the motivations and attitudes of the man himself, and contrasting worldviews.

Lewis also covers such topics as social relations and forgiveness, sexual ethics and the tenets of Christian marriage, and the relationship between morality and psychoanalysis. He also writes about "the great sin": pride, which he argues to be the root cause of all evil and rebellion.

His most important point is that Christianity mandates that one "love your neighbor as yourself." He points out that all persons unconditionally love themselves. Even if one does not "like" oneself, one would still love oneself. Christians, he writes, must also apply this attitude to others, even if they do not like them. Lewis calls this one of the "great secrets": when one acts as if he loves others, he will presently come to love them.

In 2006, "Mere Christianity" was placed third in "Christianity Today"'s list of the most influential books amongst evangelicals since 1945. The title has influenced "Touchstone Magazine: A Journal of Mere Christianity" and William Dembski's book "Mere Creation". Charles Colson's conversion to Christianity resulted from his reading this book, as did the conversions of Francis Collins, Jonathan Aitken, Josh Caterer and the philosopher C. E. M. Joad.

A passage in the book also influenced the name of contemporary Christian Texan Grammy-nominated pop/rock group Sixpence None the Richer. The phrase, "the hammering process" was used by Christian metal band Living Sacrifice for the name of their album "The Hammering Process". Metalcore band, Norma Jean, derived the title of their song "No Passenger: No Parasite" from the section in the book in which Lewis describes a fully Christian society as having "No passengers or parasites".



</doc>
<doc id="20434" url="https://en.wikipedia.org/wiki?curid=20434" title="Mathematical game">
Mathematical game

A mathematical game is a game whose rules, strategies, and outcomes are defined by clear mathematical parameters. Often, such games have simple rules and match procedures, such as Tic-tac-toe and Dots and Boxes. Generally, mathematical games need not be conceptually intricate to involve deeper computational underpinnings. For example, even though the rules of Mancala are relatively basic, the game can be rigorously analyzed through the lens of combinatorial game theory.

Mathematical games differ sharply from mathematical puzzles in that mathematical puzzles require specific mathematical expertise to complete, whereas mathematical games do not require a deep knowledge of mathematics to play. Often, the arithmetic core of mathematical games is not readily apparent to players untrained to note the statistical or mathematical aspects.

Some mathematical games are of deep interest in the field of recreational mathematics. 

When studying a game's core mathematics, arithmetic theory is generally of higher utility than actively playing or observing the game itself. To analyze a game numerically, it is particularly useful to study the rules of the game insofar as they can yield equations or relevant formulas. This is frequently done to determine winning strategies or to distinguish if the game has a solution.

Sometimes it is not immediately obvious that a particular game involves chance. Often a card game is described as "pure strategy" and such, but a game with any sort of random shuffling or face-down dealing of cards should not be considered to be "no chance". Several abstract strategy games are listed below:






</doc>
<doc id="20435" url="https://en.wikipedia.org/wiki?curid=20435" title="Martin Gardner">
Martin Gardner

Martin Gardner (October 21, 1914May 22, 2010) was an American popular mathematics and popular science writer, with interests also encompassing scientific skepticism, micromagic, philosophy, religion, and literature—especially the writings of Lewis Carroll, L. Frank Baum, and G. K. Chesterton. He is recognized as a leading authority on Lewis Carroll. "The Annotated Alice", which incorporated the text of Carroll's two Alice books, was his most successful work and sold over a million copies. He had a lifelong interest in magic and illusion and was regarded as one of the most important magicians of the twentieth century. He was considered the doyen of American puzzlers. He was a prolific and versatile author, publishing more than 100 books.

Gardner was best known for creating and sustaining interest in recreational mathematics—and by extension, mathematics in general—throughout the latter half of the 20th century, principally through his "Mathematical Games" columns. These appeared for twenty-five years in "Scientific American", and his subsequent books collecting them.

Gardner was one of the foremost anti-pseudoscience polemicists of the 20th century. His 1957 book "Fads and Fallacies in the Name of Science" became a classic and seminal work of the skeptical movement. In 1976 he joined with fellow skeptics to found CSICOP, an organization promoting scientific inquiry and the use of reason in examining extraordinary claims.

Gardner, son of a petroleum geologist, grew up in and around Tulsa, Oklahoma. His lifelong interest in puzzles started in his boyhood when his father gave him a copy of Sam Loyd's "Cyclopedia of 5000 Puzzles, Tricks and Conundrums". He attended the University of Chicago, where he earned his bachelor's degree in philosophy in 1936. Early jobs included reporter on the "Tulsa Tribune", writer at the University of Chicago Office of Press Relations, and case worker in Chicago's Black Belt for the city's Relief Administration. During World War II, he served for four years in the U.S. Navy as a yeoman on board the destroyer escort USS "Pope" in the Atlantic. His ship was still in the Atlantic when the war came to an end with the surrender of Japan in August 1945.

After the war, Gardner returned to the University of Chicago. He attended graduate school for a year there, but he did not earn an advanced degree.

In 1950 he wrote an article in the "Antioch Review" entitled "The Hermit Scientist". It was one of Gardner's earliest articles about junk science, and in 1952 a much-expanded version became his first published book: "In the Name of Science: An Entertaining Survey of the High Priests and Cultists of Science, Past and Present".

In the late 1940s, Gardner moved to New York City and became a writer and editor at "Humpty Dumpty" magazine where for eight years he wrote features and stories for it and several other children's magazines. His paper-folding puzzles at that magazine led to his first work at "Scientific American." For many decades, Gardner, his wife Charlotte, and their two sons, Jim and Tom, lived in Hastings-on-Hudson, New York, where he earned his living as a free-lance author, publishing books with several different publishers, and also publishing hundreds of magazine and newspaper articles. Appropriately enough—given his interest in logic and mathematics—they lived on Euclid Avenue. The year 1960 saw the original edition of his best-selling book ever, "The Annotated Alice".

In 1979, Gardner retired from Scientific American and he and his wife Charlotte moved to Hendersonville, North Carolina. Gardner never really retired as an author, but continued to write math articles, sending them to The Mathematical Intelligencer, Math Horizons, The College Mathematics Journal, and Scientific American. He also revised some of his older books such as "Origami, Eleusis, and the Soma Cube". Charlotte died in 2000 and two years later Gardner returned to Norman, Oklahoma, where his son, James Gardner, was a professor of education at the University of Oklahoma. He died there on May 22, 2010. An autobiography — "Undiluted Hocus-Pocus: The Autobiography of Martin Gardner" — was published posthumously.

Martin Gardner had a major impact on mathematics in the second half of the 20th century. His column was called "Mathematical Games" but it was much more than that. His writing introduced many readers to real mathematics for the first time in their lives. The column lasted for 25 years and was read avidly by the generation of mathematicians and physicists who grew up in the years 1956 to 1981. It was the original inspiration for many of them to become mathematicians or scientists themselves.

David Auerbach wrote, "A case can be made, in purely practical terms, for Martin Gardner as one of the most influential writers of the 20th century. His popularizations of science and mathematical games in Scientific American, over the 25 years he wrote for them, might have helped create more young mathematicians and computer scientists than any other single factor prior to the advent of the personal computer." Among the wide array of mathematicians, physicists, computer scientists, philosophers, magicians, artists, writers, and other influential thinkers who inspired and were in turn inspired by Gardner are John Horton Conway, Bill Gosper, Ron Rivest, Richard K. Guy, Piet Hein, Ronald Graham, Donald Knuth, Robert Nozick, Lee Sallows, Scott Kim, M. C. Escher, Douglas Hofstadter, Roger Penrose, Ian Stewart, David A. Klarner, Benoit Mandelbrot, Elwyn R. Berlekamp, Solomon W. Golomb, Raymond Smullyan, James Randi, Persi Diaconis, Penn & Teller, and Ray Hyman.

His admirers included such diverse people as W. H. Auden, Arthur C. Clarke, Carl Sagan, Isaac Asimov, Richard Dawkins, and Stephen Jay Gould. Salvador Dali once sought him out to discuss four-dimensional hypercubes. Gardner wrote to M.C. Escher in 1961 to ask permission to use his Horseman tessellation in an upcoming column about H.S.M. Coxeter. Escher replied, saying that he knew Gardner as author of "The Annotated Alice", which had been sent to Escher by Coxeter. The correspondence led to Gardner introducing the previously unknown Escher's art to the world. His writing was both broad and deep. Noam Chomsky once wrote, "Martin Gardner's contribution to contemporary intellectual culture is unique—in its range, its insight, and understanding of hard questions that matter." Gardner repeatedly alerted the public (and other mathematicians) to recent discoveries in mathematics–recreational and otherwise. In addition to introducing many first-rate puzzles and topics such as Penrose tiles and Conway's Game of Life, he was equally adept at writing captivating columns about traditional mathematical topics such as knot theory, Fibonacci numbers, Pascal's triangle, the Möbius strip, transfinite numbers, four-dimensional space, Zeno's paradoxes, Fermat's last theorem, and the four-color problem.

Martin Gardner set a new high standard for writing about mathematics. In a 2004 interview he said, "I go up to calculus, and beyond that I don’t understand any of the papers that are being written. I consider that that was an advantage for the type of column I was doing because I had to understand what I was writing about, and that enabled me to write in such a way that an average reader could understand what I was saying. If you are writing popularly about math, I think it’s good not to know too much math." And he was fearsomely bright. John Horton Conway called him "the most learned man I have ever met." Colm Mulcahy said, "Gardner was without doubt the best friend mathematics ever had." Many people would agree with him.

For over a quarter century Gardner wrote a monthly column on the subject of recreational mathematics for "Scientific American". It all began with his free-standing article on hexaflexagons which ran in the December 1956 issue. Flexagons became a bit of a fad and soon people all over New York City were making them. Gerry Piel, the "SA" publisher at the time, asked Gardner, "Is there enough similar material to this to make a regular feature?" Gardner said he thought so. The January 1957 issue contained his first column, entitled "Mathematical Games". Almost 300 more columns were to follow.

The "Mathematical Games" column became the most popular feature of the magazine and was the first thing that many readers turned to. In September 1977 Scientific American acknowledged the prestige and popularity of Gardner's column by moving it from the back to the very front of the magazine. It ran from 1956 to 1981 with sporadic columns afterwards and was the first introduction of many subjects to a wider audience, notably:
Ironically, Gardner had problems learning calculus and never took a mathematics course after high school. While editing "Humpty Dumpty's Magazine" he constructed many paper folding puzzles, and this led to his interest in the flexagons invented by British mathematician Arthur H Stone. The subsequent article he wrote on hexaflexagons led directly to the column.

In the 1980s the "Mathematical Games" column began to appear only irregularly. Other authors began to share the column, and the June 1986 issue saw the final installment under that title. In 1981, on Gardner's retirement from "Scientific American", the column was replaced by Douglas Hofstadter's "Metamagical Themas", a name that is an anagram of "Mathematical Games".

Virtually all of the games columns were collected in book form starting in 1959 with "The Scientific American Book of Mathematical Puzzles & Diversions". Over the next four decades fourteen more books followed. Donald Knuth called them the .

Gardner was an uncompromising critic of fringe science. His book "Fads and Fallacies in the Name of Science" (1952, revised 1957) debunked dubious movements and theories including Fletcherism, Lamarckism, food faddism, Dowsing Rods, Charles Fort, Rudolf Steiner, Dianetics, the Bates method for improving eyesight, Einstein deniers, the Flat Earth theory, the lost continents of Atlantis and Lemuria, Immanuel Velikovsky’s worlds in collision, the reincarnation of Bridey Murphy, Wilhelm Reich's orgone theory, the spontaneous generation of life, extra-sensory perception and psychokinesis, homeopathy, phrenology, palmistry, graphology, and numerology. This book and his subsequent efforts ("Science: Good, Bad and Bogus", 1981; "Order and Surprise", 1983, "Gardner's Whys & Wherefores", 1989, etc.) earned him a wealth of antagonists in fringe science and New Age philosophy; he kept up running dialogues (both public and private) with many of them for decades.

In a review of "Science: Good, Bad and Bogus", Stephen Jay Gould called Gardner "The Quack Detector", a writer who "expunge[d] nonsense" and in so doing had "become a priceless national resource."

In 1976 Gardner joined with fellow skeptics philosopher Paul Kurtz, psychologist Ray Hyman, sociologist Marcello Truzzi, and stage magician James Randi to found the Committee for the Scientific Investigation of Claims of the Paranormal (now called the Committee for Skeptical Inquiry). Luminaries such as astronomer Carl Sagan, author and biochemist Isaac Asimov, psychologist B. F. Skinner, and journalist Philip J. Klass became fellows of the program. From 1983 to 2002 he wrote a monthly column called "Notes of a Fringe Watcher" (originally "Notes of a Psi-Watcher") for "Skeptical Inquirer", that organization's monthly magazine. These columns have been collected in five books starting with "The New Age: Notes of a Fringe Watcher" in 1988.

Gardner was a relentless critic of self-proclaimed Israeli psychic Uri Geller and wrote two satirical booklets about him in the 1970s using the pen name "Uriah Fuller" in which he explained how such purported psychics do their seemingly impossible feats such as mentally bending spoons and reading minds.

Martin Gardner continued to criticize pseudoscience throughout his life–and he was fearless. His targets included not just safe subjects like astrology and UFO sightings, but chiropractic, vegetarianism, Madame Blavatsky, creationism, Scientology, the Laffer curve, Christian Science, and even the Hutchins-Adler Great Books Movement. The last thing he wrote in the spring of 2010 (a month before his death) was an article excoriating the "dubious medical opinions and bogus science" of Oprah Winfrey—particularly her support for the thoroughly discredited theory that vaccinations cause autism; it went on to bemoan the "needless deaths of children" that such notions are likely to cause.

"Skeptical Inquirer" named him one of the Ten Outstanding Skeptics of the Twentieth Century. In 2010 he was posthumously honored with an award for his contributions in the skeptical field from the Independent Investigations Group. In 1982 the Committee for Skeptical Inquiry awarded Gardner its "In Praise of Reason Award" for his "heroic efforts in defense of reason and the dignity of the skeptical attitude", and in 2011 it added Gardner to its Pantheon of Skeptics.

Martin Gardner's father once showed him a magic trick when he was a little boy. Young Martin was fascinated to see physical laws seemingly violated and this led to a lifelong passion for magic and illusion. He wrote for a magic magazine in high school and worked in a department store demonstrating magic tricks while he was at the University of Chicago. The very first thing that Martin Gardner ever published (at the age of fifteen) was a magic trick in "The Sphinx", the official magazine of the Society of American Magicians. He focused mainly on micromagic (table or close-up magic) and, from the 1930s on, published a significant number of original contributions to this secretive field. Magician Joe M. Turner said, "The Encyclopedia of Impromptu Magic", which Gardner wrote in 1985, "is guaranteed to show up in any poll of magicians' favorite magic books." His first magic book for the general public, "Mathematics, Magic and Mystery" (Dover, 1956), is still considered a classic in the field. He was well known for his innovative tapping and spelling effects, with and without playing cards, and was most proud of the effect he called the "Wink Change".

Many of Gardner's lifelong friends were magicians. These included William Simon who introduced Gardner to Charlotte Greenwald, whom he married in 1952, fellow CSICOP founder and pseudoscience fighter James Randi, Dai Vernon, Jerry Andrus, statistician Persi Diaconis, and polymath Raymond Smullyan. Diaconis and Smullyan like Gardner straddled the two worlds of mathematics and magic. Mathematics and magic were frequently intertwined in Gardner's work. One of his earliest books, "Mathematics, Magic and Mystery" (1956), was about mathematically based magic tricks. Mathematical magic tricks were often featured in his "Mathematical Games" column–for example, his August 1962 column was titled "A variety of diverting tricks collected at a fictitious convention of magicians." From 1998 to 2002 he wrote a monthly column on magic tricks called "Trick of the Month" in The Physics Teacher, a journal published by the American Association of Physics Teachers.
In 1999 "Magic magazine" named Gardner one of the "100 Most Influential Magicians of the Twentieth Century". In 2005 he received a 'Lifetime Achievement Fellowship' from the Academy of Magical Arts. The last thing to be published during his lifetime (he had a lot of other stuff in the pipeline) was a magic trick in the May 2010 issue of "".

Gardner believed in a personal God, in an afterlife, and in prayer, but rejected established religion. He considered himself a philosophical theist and a fideist. He had an abiding fascination with religious belief but was critical of organized religion. In his autobiography, he stated: "When many of my fans discovered that I believed in God and even hoped for an afterlife, they were shocked and dismayed... I do not mean the God of the Bible, especially the God of the Old Testament, or any other book that claims to be divinely inspired. For me God is a "Wholly Other" transcendent intelligence, impossible for us to understand. He or she is somehow responsible for our universe and capable of providing, how I have no inkling, an afterlife."

Gardner described his own belief as philosophical theism inspired by the works of philosopher Miguel de Unamuno. While eschewing systematic religious doctrine, he retained a belief in God, asserting that this belief cannot be confirmed or disconfirmed by reason or science. At the same time, he was skeptical of claims that any god has communicated with human beings through spoken or telepathic revelation or through miracles in the natural world. Gardner has been quoted as saying that he regarded parapsychology and other research into the paranormal as tantamount to "tempting God" and seeking "signs and wonders". He stated that while he would expect tests on the efficacy of prayers to be negative, he would not rule out "a priori" the possibility that as yet unknown paranormal forces may allow prayers to influence the physical world.

Gardner wrote repeatedly about what public figures such as Robert Maynard Hutchins, Mortimer Adler, and William F. Buckley, Jr. believed and whether their beliefs were logically consistent. In some cases, he attacked prominent religious figures such as Mary Baker Eddy on the grounds that their claims are unsupportable. His semi-autobiographical novel "The Flight of Peter Fromm" depicts a traditionally Protestant Christian man struggling with his faith, examining 20th century scholarship and intellectual movements and ultimately rejecting Christianity while remaining a theist.

Gardner said that he suspected that the fundamental nature of human consciousness may not be knowable or discoverable, unless perhaps a physics more profound than ("underlying") quantum mechanics is some day developed. In this regard, he said, he was an adherent of the "New Mysterianism".

Gardner was considered a leading authority on Lewis Carroll. His annotated version of "Alice's Adventures in Wonderland" and "Through the Looking Glass", explaining the many mathematical riddles, wordplay, and literary references found in the Alice books, was first published as "The Annotated Alice" (Clarkson Potter, 1960). Sequels were published with new annotations as "More Annotated Alice" (Random House, 1990), and finally as "The Annotated Alice: The Definitive Edition" (Norton, 1999), combining notes from the earlier editions and new material. The original book arose when Gardner found the Alice books "sort of frightening" when he was young, but found them fascinating as an adult. He felt that someone ought to annotate them, and suggested to a publisher that Bertrand Russell be asked; when the publisher was unable to get past Russell's secretary, Gardner was asked to take on the project himself.

In addition to the "Alice" books, Gardner produced annotated editions of G. K. Chesterton’s "The Innocence Of Father Brown" and "The Man Who Was Thursday", as well as of celebrated poems including "The Rime of the Ancient Mariner", "Casey at the Bat", "The Night Before Christmas", and "The Hunting of the Snark"; the last was also written by Lewis Carroll.

Gardner wrote two novels. He was a perennial fan of the Oz books written by L. Frank Baum, and in 1988 he published "Visitors from Oz", based on the characters in Baum's various Oz books. Gardner was a founding member of the International Wizard of Oz Club, and winner of its 1971 L. Frank Baum Memorial Award. His other novel was "The Flight of Peter Fromm" (1973), which reflected his lifelong fascination with religious belief and the problem of faith.

His short stories were collected in "The No-Sided Professor and Other Tales of Fantasy, Humor, Mystery, and Philosophy" (1987).

At the age of 95 Gardner wrote "Undiluted Hocus-Pocus: The Autobiography of Martin Gardner". He was living in a one-room apartment in Norman, Oklahoma and, as was his custom, wrote it on a typewriter and edited it using scissors and rubber cement. He took the title from a grook by his good friend Piet Hein, a grook which perfectly expresses Gardner's abiding sense of mystery and wonder about existence.

<poem>
We glibly talk
but do things have

Black earth turned into
is undiluted
</poem>

Gardner's interest in wordplay led him to conceive of a magazine on recreational linguistics. In 1967 he pitched the idea to Greenwood Periodicals and nominated Dmitri Borgmann as editor. The resulting journal, "Word Ways", carried many of his articles; it was still publishing his submissions posthumously. He also wrote a "Puzzle Tale" column for "Asimov's Science Fiction" magazine from 1977 to 1986. Gardner was a member of the all-male literary banqueting club, the Trap Door Spiders, which served as the basis of Isaac Asimov's fictional group of mystery solvers, the Black Widowers.

Gardner often used pen names. In 1952, while working for the children's magazine "Humpty Dumpty", he contributed stories written by "Humpty Dumpty Jnr". For several years starting in 1953 he was a managing editor of "Polly Pigtails", a magazine for young girls, and also wrote under that name. His "Annotated Casey at the Bat" (1967) included a parody of the poem, attributed to "Nitram Rendrag" (his name spelled backwards). Using the pen name "Uriah Fuller", he wrote two books attacking the alleged psychic Uri Geller. In later years, Gardner often wrote parodies of his favorite poems under the name "Armand T. Ringer", an anagram of his name. In 1983 one George Groth panned Gardner's book "The Whys of a Philosophical Scrivener" in the "New York Review of Books". Only in the last line of the review was it revealed that George Groth was Martin Gardner himself.

In his January 1960 Mathematical Games column, Gardner introduced the fictitious "Dr. Matrix" and wrote about him often over the next two decades. Dr. Matrix was not exactly a pen name, although Gardner did pretend that everything in these columns came from the fertile mind of the good doctor. Then in 1979 Dr. Matrix himself published an article in the quite respectable "Two-Year College Mathematics Journal". It was called "Martin Gardner: Defending the Honor of the Human Mind" and contained a biography of Gardner and a history of his Mathematical Games column.

Gardner was known for his sometimes controversial philosophy of mathematics. He wrote negative reviews of "The Mathematical Experience" by Philip J. Davis and Reuben Hersh and "What Is Mathematics, Really?" by Hersh, both of which were critical of aspects of mathematical Platonism, and the first of which was well received by the mathematical community. While Gardner was often perceived as a hard-core Platonist, his reviews demonstrated some formalist tendencies. Gardner maintained that his views are widespread among mathematicians, but Hersh has countered that in his experience as a professional mathematician and speaker, this is not the case.

Over the years Gardner held forth on many contemporary issues, arguing for his points of view in fields from general semantics to fuzzy logic to watching TV (he once wrote a negative review of Jerry Mander's book "Four Arguments for the Elimination of Television"). He was a frequent contributor to "The New York Review of Books". His philosophical views are described and defended in his book "The Whys of a Philosophical Scrivener" (1983, revised 1999).

The numerous awards Gardner received include:

In 1997, Martin Gardner became a Fellow (Class: Humanities and Arts, Section: Literature) of the American Academy of Arts and Sciences.

The main-belt asteroid "2587 Gardner" discovered by Edward L. G. Bowell at Anderson Mesa Station in 1980 is named after Martin Gardner.

There are eight bricks honoring Gardner in the Paul R. Halmos Commemorative Walk, installed by The Mathematical Association of America (MAA) at their Conference Center in Washington, D.C.

Martin Gardner continued to write up until his death in 2010, and his community of fans grew to span several generations. Moreover, his influence was so broad that many of his fans had little or no contact with each other. This led Atlanta entrepreneur and puzzle collector Tom Rodgers to the idea of hosting a weekend gathering celebrating Gardner's contributions to recreational mathematics, rationality, magic, puzzles, literature, and philosophy. Although Gardner was famously shy, and would usually decline an honor if it required him to make a personal appearance, Rogers persuaded him to attend the first such "Gathering 4 Gardner" (G4G), held in Atlanta in January 1993.

A second such get-together was held in 1996, again with Gardner in attendance, and this led Rodgers and his friends to make the gathering a regular, bi-annual event. Participants over the years have ranged from long-time Gardner friends such as Conway, Elwyn Berlekamp, Ronald Graham, Donald Coxeter, and Richard Guy, to newcomers like mathematician and mathematical artist Erik Demaine and mathematical video maker Vi Hart.

The program at the "G4G" meetings presents topics which Gardner had written about. The first gathering in 1993 was G4G1 and the 1996 event was G4G2. Since then it has been in even-numbered years, so far always in Atlanta. The 2018 event was G4G13.




</doc>
<doc id="20436" url="https://en.wikipedia.org/wiki?curid=20436" title="MIDI timecode">
MIDI timecode

MIDI time code (MTC) embeds the same timing information as standard SMPTE timecode as a series of small 'quarter-frame' MIDI messages. There is no provision for the user bits in the standard MIDI time code messages, and messages are used to carry this information instead. The quarter-frame messages are transmitted in a sequence of eight messages, thus a complete timecode value is specified every two frames. If the MIDI data stream is running close to capacity, the MTC data may arrive a little behind schedule which has the effect of introducing a small amount of jitter. In order to avoid this it is ideal to use a completely separate MIDI port for MTC data. Larger full-frame messages, which encapsulate a frame worth of timecode in a single message, are used to locate to a time while timecode is not running.

Unlike standard SMPTE timecode, MIDI timecode's quarter-frame and full-frame messages carry a two-bit flag value that identifies the rate of the timecode, specifying it as either:

MTC distinguishes between film speed and video speed only by the rate at which timecode advances, not by the information contained in the timecode messages; thus, 29.97 frame/s dropframe is represented as 30 frame/s dropframe at 0.1% pulldown.

MTC allows the synchronisation of a sequencer or DAW with other devices that can synchronise to MTC or for these devices to 'slave' to a tape machine that is striped with SMPTE. For this to happen a SMPTE to MTC converter needs to be employed. It is possible for a tape machine to synchronise to an MTC signal (if converted to SMPTE), if the tape machine is able to 'slave' to incoming timecode via motor control, which is a rare feature.

The MIDI time code is 32 bits long, of which 24 are used, while 8 bits are unused and always zero. Because the full-time code messages requires that the most significant bits of each byte are zero (valid MIDI data bytes), there are really only 28 available bits and 4 spare bits.

Like most audiovisual timecodes such as SMPTE time code, it encodes only time of day, repeating each 24 hours. Time is given in units of hours, minutes, seconds, and frames. There may be 24, 25, or 30 frames per second.

Unlike most other timecodes, the components are encoded in straight binary, not binary-coded decimal.

Each component is assigned one byte:

When there is a jump in the time code, a single full-time code is sent to synchronize attached equipment. This takes the form of a special global system exclusive message:
The manufacturer ID of codice_10 indicates a real-time universal message, the channel of codice_10 indicates it is a global broadcast. The following ID of codice_12 identifies this is a time code type message, and the second codice_12 indicates it is a full-time code message. The 4 bytes of time code follow. Although MIDI is generally little-endian, the 4 time code bytes follow in big-endian order, followed by a codice_14 "end of exclusive" byte.

After a jump, the time clock stops until the first following quarter-frame message is received.

When the time is running continuously, the 32-bit time code is broken into 8 4-bit pieces, and one piece is transmitted each quarter frame. I.e. 96—120 times per second, depending on the frame rate. Since it takes eight quarter frames for a complete time code message, the complete SMPTE time is updated every two frames. A quarter-frame messages consists of a status byte of 0xF1, followed by a single 7-bit data value: 3 bits to identify the piece, and 4 bits of partial time code. When time is running forward, the piece numbers increment from 0–7; with the time that piece 0 is transmitted is the coded instant, and the remaining pieces are transmitted later.

If the MIDI data stream is being rewound, the piece numbers count backward. Again, piece 0 is transmitted at the coded moment.

The time code is divided little-endian as follows:



</doc>
<doc id="20437" url="https://en.wikipedia.org/wiki?curid=20437" title="Mass transfer">
Mass transfer

Mass transfer is the net movement of mass from one location, usually meaning stream, phase, fraction or component, to another. Mass transfer occurs in many processes, such as absorption, evaporation, drying, precipitation, membrane filtration, and distillation. Mass transfer is used by different scientific disciplines for different processes and mechanisms. The phrase is commonly used in engineering for physical processes that involve diffusive and convective transport of chemical species within physical systems.

Some common examples of mass transfer processes are the evaporation of water from a pond to the atmosphere, the purification of blood in the kidneys and liver, and the distillation of alcohol. In industrial processes, mass transfer operations include separation of chemical components in distillation columns, absorbers such as scrubbers or stripping, adsorbers such as activated carbon beds, and liquid-liquid extraction. Mass transfer is often coupled to additional transport processes, for instance in industrial cooling towers. These towers couple heat transfer to mass transfer by allowing hot water to flow in contact with air. The water is cooled by expelling some of its content in the form of water vapour.

In astrophysics, mass transfer is the process by which matter gravitationally bound to a body, usually a star, fills its Roche lobe and becomes gravitationally bound to a second body, usually a compact object (white dwarf, neutron star or black hole), and is eventually accreted onto it. It is a common phenomenon in binary systems, and may play an important role in some types of supernovae and pulsars.

Mass transfer finds extensive application in chemical engineering problems. It is used in reaction engineering, separations engineering, heat transfer engineering, and many other sub-disciplines of chemical engineering.

The driving force for mass transfer is typically a difference in chemical potential, when it can be defined, though other thermodynamic gradients may couple to the flow of mass and drive it as well. A chemical species moves from areas of high chemical potential to areas of low chemical potential. Thus, the maximum theoretical extent of a given mass transfer is typically determined by the point at which the chemical potential is uniform. For single phase-systems, this usually translates to uniform concentration throughout the phase, while for multiphase systems chemical species will often prefer one phase over the others and reach a uniform chemical potential only when most of the chemical species has been absorbed into the preferred phase, as in liquid-liquid extraction.

While thermodynamic equilibrium determines the theoretical extent of a given mass transfer operation, the actual rate of mass transfer will depend on additional factors including the flow patterns within the system and the diffusivities of the species in each phase. This rate can be quantified through the calculation and application of mass transfer coefficients for an overall process. These mass transfer coefficients are typically published in terms of dimensionless numbers, often including Péclet numbers, Reynolds numbers, Sherwood numbers and Schmidt numbers, among others.

There are notable similarities in the commonly used approximate differential equations for momentum, heat, and mass transfer. The molecular transfer equations of Newton's law for fluid momentum at low Reynolds number (Stokes flow), Fourier's law for heat, and Fick's law for mass are very similar, since they are all linear approximations to transport of conserved quantities in a flow field. 
At higher Reynolds number, the analogy between mass and heat transfer and momentum transfer becomes less useful due to the nonlinearity of the Navier-Stokes equation (or more fundamentally, the general momentum conservation equation), but the analogy between heat and mass transfer remains good. A great deal of effort has been devoted to developing analogies among these three transport processes so as to allow prediction of one from any of the others.



</doc>
<doc id="20448" url="https://en.wikipedia.org/wiki?curid=20448" title="Museum of Jurassic Technology">
Museum of Jurassic Technology

The Museum of Jurassic Technology is a museum located at 9341 Venice Boulevard in the Palms district of Los Angeles, California. It was founded by David Hildebrand Wilson and Diana Drake Wilson (husband and wife) in 1988.

The museum calls itself "an educational institution dedicated to the advancement of knowledge and the public appreciation of the Lower Jurassic"; the relevance of the term "Lower Jurassic" to the museum's collections is left uncertain and unexplained. The museum's collection includes a mixture of artistic, scientific, ethnographic, and historic, as well as some unclassifiable exhibits, and the diversity of its offerings evokes the cabinets of curiosities that were the 16th-century predecessors of modern natural history museums. The factual claims of many of the museum's exhibits strain credibility, provoking an array of interpretations from commentators. The museum was the subject of a 1995 book by Lawrence Weschler entitled "Mr. Wilson's Cabinet of Wonder: Pronged Ants, Horned Humans, Mice on Toast, and Other Marvels of Jurassic Technology", which describes in detail many of its exhibits. David Hildebrand Wilson received a MacArthur Foundation fellowship in 2001. The museum is mentioned in the novel "The Museum of Innocence", by Turkish Nobel-laureate Orhan Pamuk.

The museum contains an unusual collection of exhibits and objects with varying and uncertain degrees of authenticity. "The New York Times" critic Edward Rothstein described it as a "museum about museums", "where the persistent question is: what kind of place is this?" "Smithsonian" magazine called it "a witty, self-conscious homage to private museums of yore . . . when natural history was only barely charted by science, and museums were closer to Renaissance cabinets of curiosity." In a similar vein, "The Economist" said the museum "captures a time chronicled in Richard Holmes's recent book "The Age of Wonder", when science mingled with poetry in its pursuit of answers to life's mysterious questions."

Lawrence Weschler's book, "Mr. Wilson's Cabinet of Wonder: Pronged Ants, Horned Humans, Mice on Toast, And Other Marvels of Jurassic Technology", attempts to explain the mystery of the Museum of Jurassic Technology. Weschler deeply explores the museum through conversations with its founder, David Wilson, and through outside research on several exhibitions. His investigations into the history of certain exhibits led to varying results of authenticity; some exhibits seem to have been created by Wilson's imagination while other exhibits might be suitable for display in a natural history museum. The Museum of Jurassic Technology at its heart, according to Wilson, is "a museum interested in presenting phenomena that other natural history museums are unwilling to present."

The museum's introductory slideshow recounts that, "In its original sense, the term, 'museum' meant '"a spot dedicated to the Muses, a place where man's mind could attain a mood of aloofness above everyday affairs"'". In this spirit, the dimly lit atmosphere, wood and glass vitrines, and labyrinthine floorplan lead visitors through an eclectic range of exhibits on art, natural history, history of science, philosophy, and anthropology, with a special focus on the history of museums and the variety of paths to knowledge. The museum attracts approximately 25,000 visitors per year.

Over the years, the museum has expanded both its exhibitions and other public offerings. In 2005, the museum opened its Tula Tea Room, a Russian-style tea room where Georgian tea, and cookies are served to patrons. This room is a miniature reconstruction of the study of Tsar Nicolas II from the Winter Palace in St. Petersburg, Russia. The Borzoi Kabinet Theater screens a series of poetic documentaries produced by the Museum of Jurassic Technology in collaboration with the St. Petersburg–based arts and science collective Kabinet. The series of films, entitled "A Chain of Flowers", draws its name from the quote by Charles Willson Peale: "The Learner must be led always from familiar objects toward the unfamiliar, guided along, as it were, a chain of flowers into the mysteries of life". The titles of the films are "Levsha: The Cross-eyed Lefty from Tula and the Steel Flea" (2001), "Obshee Delo: The Common Task" (2005), "Bol'shoe Sovietskaia Zatmenie: The Great Soviet Eclipse" (2008), "The Book of Wisdom and Lies" (2011), and "Language of the Birds" (2012).

The museum produces a series of leaflets and books about museum exhibits, including:

Many of these books are published in conjunction with the Society for the Diffusion of Useful Information.

The museum maintains over 30 permanent exhibits, including:

From 1992 to 2006, the museum's Foundation Collection was on display in its Tochtermuseum at the Karl Ernst Osthaus-Museum in Hagen, Germany. This exhibition was part of the Museum of Museums wing at the KEOM, which came into being under the stewardship of then-director Michael Fehr.



</doc>
<doc id="20451" url="https://en.wikipedia.org/wiki?curid=20451" title="Men at Work">
Men at Work

Men at Work was an Australian rock band formed in 1979 and best known for their 1981 hit "Down Under". Their founding mainstay was Colin Hay on lead vocals; he formed the group with Jerry Speiser on drums and Ron Strykert on lead guitar. They were joined by Greg Ham on flute, saxophone, and keyboards and John Rees on bass guitar. This line-up achieved national and international success in the early 1980s. In January 1983, they were the first Australian artists to have a simultaneous No. 1 album and No. 1 single in the United States "Billboard" charts: "Business as Usual" (released on 9 November 1981) and "Down Under" (1981), respectively. With the same works, they achieved the distinction of a simultaneous No. 1 album and No. 1 single on the Australian, New Zealand, and United Kingdom charts. Their second album "Cargo" (2 May 1983) was also No. 1 in Australia, No. 2 in New Zealand, No. 3 in the US, and No. 8 in the UK. Their third album "Two Hearts" (3 April 1985) reached the top 20 in Australia and top 50 in the US.

They won the Grammy Award for Best New Artist in 1983, they were inducted into the ARIA Hall of Fame in 1994, and they have sold over 30 million albums worldwide. In May 2001, "Down Under" was listed at No. 4 on the APRA Top 30 Australian songs and "Business as Usual" appeared in the book "100 Best Australian Albums" (October 2010). 

In February 2010, Larrikin Records won a case against Hay and Strykert, their record label (Sony BMG Music Entertainment), and their music publishing company (EMI Songs Australia) arising from the uncredited appropriation of "Kookaburra" for the flute line in "Down Under". The group disbanded in 1986 and reformed in 1996 to disband again by 2002. On 19 April 2012, Greg Ham was found dead at his home from an apparent heart attack.

The nucleus of Men at Work formed in Melbourne in 1979 with Colin Hay on lead vocals; Jerry Speiser on drums; and Ron Strykert on lead guitar; they were soon joined by Greg Ham on flute and keyboards; and then John Rees on bass guitar. Hay had emigrated to Australia in 1967 from Scotland with his family. In 1978, he had formed a duo with Strykert, which expanded by mid-1979 with the addition of Speiser and progressive rocker Greg Sneddon on keyboards (ex-Alroy Band). They formed an unnamed four-piece group. The band's first experience in the recording studio was recording the music to "Riff Raff", a low-budget stage musical on which Sneddon had worked.

Sneddon left and was replaced in late 1979 by Ham, and when Rees joined they adopted the name Men at Work from a construction zone sign near an early venue, The Cricketer's Arms Hotel, Richmond. The band built a "grass roots" reputation as a pub rock band. In 1980 the group issued their debut single, "Keypunch Operator" backed by "Down Under", with both tracks co-written by Hay and Strykert. It was "self-financed" and appeared on their own independent, M. A. W. label. Australian musicologist, Ian McFarlane, felt the A-side was "a fast-paced country-styled rocker with a clean sound and quirky rhythm". Despite not appearing in the top 100 on the Australian Kent Music Report Singles Chart, by the end of that year the group had "grown in stature to become the most in-demand and highly paid, unsigned band of the year".

Early in 1981 Men at Work signed with the Australian branch of Columbia Records on the recommendation of Peter Karpin, the label's A&R person. Fran of the "Woroni" caught their performance at the Refectory in Canberra in April, she noted that they provided "some reggae-ish type music and the minimum of audience attention. From what I saw of them they were perhaps a little bit boring but quite competent and probably deserving of more notice". The group's second single, "Who Can It Be Now?", was released in June 1981 which reached No. 2 and remained in the chart for 24 weeks. It had been produced by United States-based Peter McIan, who was also working on their debut album, "Business as Usual".

Their next single was a re-worked version of "Down Under", Ham added an improvised flute solo, and the group had revisited its tempo and arrangement with McIan. It appeared in October that year and reached No. 1 in November, where it remained for six weeks. "Business as Usual" was also released in October and went to No. 1 on the Australian Kent Music Report Albums Chart, spending a total of nine weeks at the top spot. "The Canberra Times" Garry Raffaele opined that it "generally stays at a high level, tight and jerky ... There is a delicacy about this music — and that is not a thing you can say about too many rock groups. The flute and reeds of Greg Ham do much to further that". McFarlane noted that "[a]side from the strength of the music, part of the album's appeal was its economy. The production sound was low-key, but clean and uncluttered. Indeed, the songs stood by themselves with little embellishment save for a bright, melodic, singalong quality".

By February the following year both "Down Under" and "Business as Usual" had reached No. 1 on the respective Official New Zealand Music Charts – the latter was the first Australian album to reach that peak in New Zealand. Despite its strong Australian and New Zealand showing, and having an American producer (McIan), "Business as Usual" was twice rejected by Columbia's US parent company. Thanks to the persistence of the band's management and Karpin, the album was finally released in the US and the United Kingdom in April 1982 – six months after its Australian release. Their next single, "Be Good Johnny", was issued in Australia in April 1982 and reached No. 8 in Australia, and No. 3 in New Zealand.

Men at Work initially broke through to North American audiences in the western provinces of Canada with "Who Can It Be Now?" hitting top 10 on radio stations in Winnipeg by May 1982. It peaked at No. 8 on the Canadian "RPM" Top Singles Chart in July. In August the group toured Canada and the US to promote the album and related singles, supporting Fleetwood Mac. The band became more popular on Canadian radio in the following months and also started receiving top 40 US airplay by August. In October "Who Can It Be Now?" reached No. 1 on the US "Billboard" Hot 100, while Canada was one single ahead with "Down Under" topping the Canadian charts that same month. In the following month "Business as Usual" began a 15-week run at No. 1 on the "Billboard" 200.

While "Who Can It Be Now?" was still in the top ten in the US, "Down Under" was finally released in that market. It entered the US charts at No. 79 and ten weeks later, it was No. 1. By January 1983 Men at Work had the top album and single in both the US and the UK – never previously achieved by an Australian act. "Be Good Johnny" received moderate airplay in the US; it reached the top 20 in Canada.

"Down Under" gained international media exposure in September 1983 through television coverage of the Australian challenge for the America's Cup yacht trophy in September 1983 when it was adopted as the theme song by the crew of the successful "Australia II".

The band released their second album, "Cargo", in April 1983, which also peaked at No. 1 – for two weeks – on the Australian charts. In New Zealand it reached No.2. It had been finished in mid-1982 with McIan producing again, but was held back due to the success of their debut album on the international market, where "Business as Usual" was still riding high. "Cargo" appeared at No. 3 on the "Billboard" 200, and No. 8 in the UK. The lead single, "Overkill", was issued in Australia ahead of the album in October 1982 and reached No. 6, it peaked at No. 3 in the US. "Dr. Heckyll & Mr. Jive" followed in March 1983 made it to No. 5 in Australia, and No. 28 in the US. "It's a Mistake" reached No. 6 in the US. The band toured the world extensively in 1983.

During 1984 the band took a break as members pursued other interests. Upon reconvening later that year, tensions during rehearsals between Hay and Speiser over songwriting and the band's management led to a split in the band. Both Rees and Speiser were told they were "not required", as Hay, Ham and Strykert used session musicians to record their third album, "Two Hearts" (23 April 1985). Studio musicians included Jeremy Alsop on bass guitar (ex-Ram Band, Pyramid, Broderick Smith Band); and Mark Kennedy on drums (Spectrum, Ayers Rock, Marcia Hines Band). "Two Hearts" was produced by Hay and Ham. It was a critical and commercial failure compared to their previous albums and only peaked at No. 16 in Australia, and No. 50 on the US chart. Strykert had left during its production.

Four tracks were released as singles, "Everything I Need" (May 1985), "Man with Two Hearts", "Maria" (August), and "Hard Luck Story" (October); only the lead single charted in Australia (No. 37) and the US (No. 47). The album relied heavily on drum machines and synthesisers, and reduced the presence of Ham's saxophone, giving it a different feel compared to its predecessors. Hay and Ham hired new bandmates, to tour in support of "Two Hearts", with Alsop and Kennedy joined by James Black on guitar and keyboards (Mondo Rock, The Black Sorrows). Soon after a third guitarist, Colin Bayley (Mi-Sex), was added and Kennedy was replaced on drums by Chad Wackerman (Frank Zappa). Australian singers Kate Ceberano and Renée Geyer had also worked on the album and performed live as guest vocalists.

On 13 July 1985 Men at Work performed three tracks for the Oz for Africa concert (part of the global Live Aid program)—"Maria", "Overkill", and an unreleased one, "The Longest Night". They were broadcast in Australia (on both Seven Network and Nine Network) and on MTV in the US. "Maria" and "Overkill" were also broadcast by American Broadcasting Company (ABC) during their Live Aid telecast. Ham left during the band's time touring behind the album. The final Men at Work performances during 1985 had jazz saxophonist Paul Williamson (The Black Sorrows), replacing Ham. By early 1986 the band was defunct and Hay started recording his first solo album, "Looking for Jack" (January 1987), which had Alsop and Wackerman as session musicians.

By mid-1996, after a ten-year absence, Hay and Ham reformed Men at Work to tour South America. They had enjoyed strong fan support there during their earlier career and demands for a reunion had persisted. The 1996 line up had Stephen Hadley on bass guitar and backing vocals (ex-The Black Sorrows, Paul Kelly Band); Simon Hosford on guitar and backing vocals (Colin Hay backing band); and John Watson on drums (The Black Sorrows). The tour culminated in a performance in São Paulo, which was recorded for the Brazilian release of a live album, "Brazil '96", in 1997, which was co-produced by Hay and Ham for Sony Records. It was re-released worldwide in 1998 as "Brazil" with a bonus track, "The Longest Night", the first new studio track since "Two Hearts".

In 1997 drummer Tony Floyd replaced Watson but by 1998 the lineup was Hay, Ham, James Ryan (guitar, backing vocals), Rick Grossman (of the Hoodoo Gurus) on bass and Peter Maslen (ex-Boom Crash Opera) on drums. In 1999 Ryan, Grossman and Maslen were out and Hosford and Floyd were back in, along with bassist Stuart Speed. Rodrigo Aravena was brought in on bass in 2000, along with Heta Moses on drums. Moses was replaced by Warren Trout in 2001 as Stephen Hadley returned on bass.

The band toured Australia, South America, Europe and the US from 1998 to 2000. Men at Work performed "Down Under" at the closing ceremony of the 2000 Summer Olympics in Sydney, alongside Paul Hogan of ""Crocodile" Dundee" (1986).

One of their European tours for mid-2000 was cancelled and the group had disbanded by 2002, although Hay and Ham periodically reunited Men at Work with guest musicians (including an appearance in February 2009, when they performed "Down Under" as a duo at the Australia Unites Victorian Bushfire Appeal Telethon).

In February 2010 Larrikin Music Publishing won a case against Hay and Strykert, their record label (Sony BMG Music Entertainment) and music publishing company (EMI Songs Australia) arising from the uncredited appropriation of "Kookaburra", originally written in 1932 by Marion Sinclair and for which Larrikin owned the publishing rights, as the flute line in the Men at Work song, "Down Under". Back in early 2009 the Australian music-themed TV quiz, "Spicks and Specks", had posed a question which suggested that "Down Under" contained elements of "Kookaburra".

Larrikin, headed by Norman Lurie (now retired), then filed suit after Larrikin was sold to another company and had demanded between 40% and 60% of the previous six years of earnings from the song. In February 2010 the judge ruled that "Down Under" did contain a flute riff based on "Kookaburra" but stipulated that neither was it necessarily the hook nor a substantial part of the hit song (Hay and Strykert had written the track years before the flute riff was added by Ham). In July 2010 a judge ruled that Larrikin should be paid 5% of past (since 2002) and future profits. Ham took the verdict particularly hard, feeling responsible for having performed the flute riff at the center of the lawsuit and worried that he would only be remembered for copying someone else's music, resulting in depression and anxiety. Ham's body was found in his Carlton North home on April 19, 2012 after he suffered a fatal heart attack at age 58.

Hay maintained a solo career and played with Ringo Starr & His All-Starr Band. Strykert relocated to Hobart in 2009 from Los Angeles, and continued to play music and released his first solo album, "Paradise", in September that year. He expressed resentment towards Hay, mainly over royalties. Ham remained musically active and played sax with the Melbourne-based group The Nudist Funk Orchestra until his death. Rees was a music teacher in Melbourne and also played the violin and bass guitar for the band Beggs 2 Differ. Speiser played drums for the band, The Afterburner.

The group won the 1983 Grammy Award for Best New Artist; the other nominees were Asia, Jennifer Holliday, The Human League and Stray Cats. In August 1983 they were given a Crystal Globe Award for $100 million worth of record business by their US label. That same year in Canada they were awarded a Juno Award for "International LP of the Year". Men at Work have sold over 30 million albums worldwide.

At the ARIA Music Awards of 1994 they were inducted into the related Hall of Fame. On 28 May 2001 "Down Under" was listed at No. 4 on the APRA Top 30 Australian songs. In October 2010, "Business as Usual" was listed in the book, "100 Best Australian Albums".

Colin Hay has been the only constant member in all configurations.





</doc>
<doc id="20452" url="https://en.wikipedia.org/wiki?curid=20452" title="Meconium aspiration syndrome">
Meconium aspiration syndrome

Meconium aspiration syndrome (MAS) also known as neonatal aspiration of meconium is a medical condition affecting newborn infants. It describes the spectrum of disorders and pathophysiology of newborns born in meconium-stained amniotic fluid (MSAF) and have meconium within their lungs. Therefore, MAS has a wide range of severity depending on what conditions and complications develop after parturition. Furthermore, the pathophysiology of MAS is multifactorial and extremely complex which is why it is the leading cause of morbidity and mortality in term infants.

The word "meconium" is derived from the Greek word "mēkōnion" meaning "juice from the opium poppy" as the sedative effects it had on the foetus were observed by Aristotle. 

Meconium is a sticky dark-green substance which contains gastrointestinal secretions, amniotic fluid, bile acids, bile, blood, mucus, cholesterol, pancreatic secretions, lanugo, vernix caseosa and cellular debris. Meconium accumulates in the foetal gastrointestinal tract throughout the third trimester of pregnancy and it is the first intestinal discharge released within the first 48 hours after birth. Notably, since meconium and the whole content of the gastrointestinal tract is located ‘extracorporeally,’ its constituents are hidden and normally not recognised by the foetal immune system.

For the meconium within the amniotic fluid to successfully cause MAS, it has to enter the respiratory system during the period when the fluid-filled lungs transition into an air-filled organ capable of gas exchange. 

1 in every 7 pregnancies have MSAF and, of these cases, approximately 5% of these infants develop MAS. MSAF is observed 23-52% in pregnancies at 42 weeks therefore, the frequency of MAS increases as the length of gestation increases, such that the prevalence is greatest in post-term pregnancies. Conversely, preterm births are not frequently associated with MSAF (only approximately 5% in total contain MSAF). The rate of MAS declines in populations where labour is induced in women that have pregnancies exceeding 41 weeks. There are many suspected pre-disposing factors that are thought to increase the risk of MAS. For example, the risk of MSAF is higher in African American, African and Pacific Islander mothers, compared to mothers from other ethnic groups. 

Respiratory distress in an infant born through the darkly coloured MSAF as well as meconium obstructing the airways is usually sufficient enough to diagnose MAS. Additionally, newborns with MAS can have other types of respiratory distress such as tachypnea and hypercapnia. Sometimes it is hard to diagnose MAS as it can be confused with other diseases that also cause respiratory distress, such as pneumonia. Additionally, X-rays and lung ultrasounds can be quick, easy and cheap imaging techniques to diagnose lung diseases like MAS.

The main theories of meconium passage into amniotic fluid are caused by foetal maturity or from foetal stress as a result of hypoxia or infection. Other factors that promote the passage of meconium "in utero" include placental insufficiency, maternal hypertension, pre-eclampsia and maternal drug use of tobacco and cocaine. However, it should be noted that the exact mechanism for meconium passage into the amniotic fluid is not completely understood and it may be a combination of several factors. 

There may be an important association between foetal distress and hypoxia with MSAF. It is believed that foetal distress develops into foetal hypoxia causing the foetus to defecate meconium resulting in MSAF and then perhaps MAS. Other stressors which causes foetal distress, and therefore meconium passage, includes when umbilical vein oxygen saturation is below 30%.

Foetal hypoxic stress during parturition can stimulate colonic activity, by enhancing intestinal peristalsis and relaxing the anal sphincter, which results in the passage of meconium. Then, because of intrauterine gasping or from the first few breaths after delivery, MAS may develop. Furthermore, aspiration of thick meconium leads to obstruction of airways resulting in a more severe hypoxia.

It is important to note that the association between foetal distress and meconium passage is not a definite cause-effect relationship as over ¾ of infants with MSAF are vigorous at birth and do not have any distress or hypoxia. Additionally, foetal distress occurs frequently without the passage of meconium as well.

Although meconium is present in the gastrointestinal tract early in development, MSAF rarely occurs before 34 weeks gestation.

Peristalsis of the foetal intestines is present as early as 8 weeks gestation and the anal sphincter develops at about 20–22 weeks. The control of the anal sphincter is not well known, however the foetus does defecate routinely into the amniotic cavity even in the absence of distress. The presence of intestinal enzymes have been found in the amniotic fluid of women who are as early as 14–22 weeks pregnant. Thus, suggesting there is free passage of the intestinal contents into the amniotic fluid. 

Motilin is found in higher concentrations in post-term than pre-term foetal gastrointestinal tracts. Similarly, intestinal parasympathetic innervation and myelination also increases in later gestations. Therefore, the increased incidence of MAS in post-term pregnancies may reflect the maturation and development of the peristalsis within the gastrointestinal tract in the newborn. 

As MAS describes a spectrum of disorders of newborns born through MSAF, without any congenital respiratory disorders or other underlying pathology, there are numerous hypothesised mechanisms and causes for the onset of this syndrome. Long-term consequences may arise from these disorders, for example, infants that develop MAS have higher rates of developing neurodevelopmental defects due to poor respiration. 

In the first 15 minutes of meconium aspiration, there is obstruction of larger airways which causes increased lung resistance, decreased lung compliance, acute hypoxaemia, hypercapnia, atelectasis and respiratory acidosis. After 60 minutes of exposure, the meconium travels further down into the smaller airways. Once within the terminal bronchioles and alveoli, the meconium triggers inflammation, pulmonary oedema, vasoconstriction, bronchoconstriction, collapse of airways and inactivation of surfactant. 

The lung areas which do not or only partially participate in ventilation, because of obstruction and/or destruction, will become hypoxic and an inflammatory response may consequently occur. Partial obstruction will lead to air trapping and hyperinflation of certain lung areas and pneumothorax may follow. Chronic hypoxia will lead to an increase in pulmonary vascular smooth muscle tone and persistent pulmonary hypertension causing respiratory and circulatory failure. 

Microorganisms, most commonly Gram-negative rods, and endotoxins are found in samples of MSAF at a higher rate than in clear amniotic fluid, for example 46.9% of patients with MSAF also had endotoxins present. A microbial invasion of the amniotic cavity (MIAC) is more common in patients with MSAF and this could ultimately lead to an intra-amniotic inflammatory response. MIAC is associated with high concentrations of cytokines (such as IL-6), chemokines (such as IL-8 and monocyte chemoattractant protein-1), complement, phospholipase A and matrix-degrading enzymes. Therefore, these aforementioned mediators within the amniotic fluid during MIAC and intra-amniotic infection could, when aspirated "in" "utero", induce lung inflammation within the foetus. 

Meconium has a complex chemical composition, so it is difficult to identify a single agent responsible for the several diseases that arise. As meconium is stored inside the intestines, and is partly unexposed to the immune system, when it becomes aspirated the innate immune system recognises as a foreign and dangerous substance. The immune system, which is present at birth, responds within minutes with a low specificity and no memory in order to try to eliminate microbes. Meconium perhaps leads to chemical pneumonitis as it is a potent activator of inflammatory mediators which include cytokines, complement, prostaglandins and reactive oxygen species.

Meconium is a source of pro-inflammatory cytokines, including tumour necrosis factor (TNF) and interleukins (IL-1, IL-6, IL-8), and mediators produced by neutrophils, macrophages and epithelial cells that may injure the lung tissue directly or indirectly. For example, proteolytic enzymes are released from neutrophilic granules and these may damage the lung membrane and surfactant proteins. Additionally, activated leukocytes and cytokines generate reactive nitrogen and oxygen species which have cytotoxic effects. Oxidative stress results in vasoconstriction, bronchoconstriction, platelet aggregation and accelerated cellular apoptosis. Recently, it has been hypothesised that meconium is a potent activator of toll-like receptor (TLRs) and complement, key mediators in inflammation, and may thus contribute to the inflammatory response in MAS. 

Meconium contains high amounts of phospholipase A (PLA), a potent proinflammatory enzyme, which may directly (or through the stimulation of arachidonic acid) lead to surfactant dysfunction, lung epithelium destruction, tissue necrosis and an increase in apoptosis. Meconium can also activate the coagulation cascade, production of platelet-activating factor (PAF) and other vasoactive substances that may lead to destruction of capillary endothelium and basement membranes. Injury to the alveolocapillary membrane results in leakage of liquid, plasma proteins, and cells into the interstitium and alveolar spaces.

Surfactant is synthesised by type II alveolar cells and is made of a complex of phospholipids, proteins and saccharides. It functions to lower surface tension (to allow for lung expansion during inspiration), stabilise alveoli at the end of expiration (to prevent alveolar collapse) and prevents lung oedema. Surfactant also contributes to lung protection and defence as it is also an anti-inflammatory agent. Surfactant enhances the removal of inhaled particles and senescent cells away from the alveolar structure. 

The extent of surfactant inhibition depends on both the concentration of surfactant and meconium. If the surfactant concentration is low, even very highly diluted meconium can inhibit surfactant function whereas, in high surfactant concentrations, the effects of meconium are limited. Meconium may impact surfactant mechanisms by preventing surfactant from spreading over the alveolar surface, decreasing the concentration of surfactant proteins (SP-A and SP-B), and by changing the viscosity and structure of surfactant. Several morphological changes occur after meconium exposure, the most notable being the detachment of airway epithelium from stroma and the shedding of epithelial cells into the airway. These indicate a direct detrimental effect on lung alveolar cells because of the introduction of meconium into the lungs. 
Persistent pulmonary hypertension (PPHN) is the failure of the foetal circulation to adapt to extra-uterine conditions after birth. PPHN is associated with various respiratory diseases, including MAS (as 15-20% of infants with MAS develop PPHN), but also pneumonia and sepsis. A combination of hypoxia, pulmonary vasoconstriction and ventilation/perfusion mismatch can trigger PPHN, depending on the concentration of meconium within the respiratory tract. PPHN in newborns is the leading cause of death in MAS. 

Apoptosis is an important mechanism in the clearance of injured cells and in tissue repair, however too much apoptosis may cause harm, such as acute lung injury. Meconium induces apoptosis and DNA cleavage of lung airway epithelial cells, this is detected by the presence of fragmented DNA within the airways and in alveolar epithelial nuclei. Meconium induces an inflammatory reaction within the lungs as there is an increase of autophagocytic cells and levels of caspase 3 after exposure. After 8 hours of meconium exposure, in rabbit foetuses, the total amount of apoptotic cells is 54%. Therefore, the majority of meconium-induced lung damage may be due to the apoptosis of lung epithelium.

Most infants born through MSAF do not require any treatments (other than routine postnatal care) as they show no signs of respiratory distress, as only approximately 5% of infants born through MSAF develop MAS. However, infants which do develop MAS need to be administered to a neonatal unit where they will be closely observed and provided any treatments needed. Observations include monitoring heart rate, respiratory rate, oxygen saturation and blood glucose (to detect worsening respiratory acidosis or the development of hypoglycemia). In general, treatment of MAS is more supportive in nature.

To clear the airways of meconium, tracheal suctioning can be used however, the efficacy of this method is in question and it can cause harm. 

In cases of MAS, there is a need for supplemental oxygen for at least 12 hours in order to maintain oxygen saturation of haemoglobin at 92% or more. The severity of respiratory distress can vary significantly between newborns with MAS, as some require minimal or no supplemental oxygen requirement and, in severe cases, mechanical ventilation may be needed. The desired oxygen saturation is between 90-95% and PaO may be as high as 90mmHg. In cases where there is thick meconium deep within the lungs, mechanical ventilation may be required. In extreme cases, extracorporeal membrane oxygenation (ECMO) may be utilised in infants who fail to respond to ventilation therapy. While on ECMO, the body can have time to absorb the meconium and for all the associated disorders to resolve. There has been an excellent response to this treatment, as the survival rate of MAS while on ECMO is more than 94%.

Ventilation of infants with MAS can be challenging and, as MAS can affect each individual differently, ventilation administration may need to be customised. Some newborns with MAS can have homogenous lung changes and others can have inconsistent and patchy changes to their lungs. It is common for sedation and muscle relaxants to be used to optimise ventilation and minimise the risk of pneumothorax associated with dyssynchronous breathing.

Inhaled nitric oxide (iNO) acts on vascular smooth muscle causing selective pulmonary vasodilation. This is ideal in the treatment of PPHN as it causes vasodilation within ventilated areas of the lung thus, decreasing the ventilation-perfusion mismatch and thereby, improves oxygenation. Treatment utilising iNO decreases the need for ECMO and mortality in newborns with hypoxic respiratory failure and PPHN as a result of MAS. However, approximately 30-50% of infants with PPHN do not respond to iNO therapy. 

As inflammation is such a huge issue in MAS, treatment has consisted of anti-inflammatories. 

Glucocorticoids (GCs) have a strong anti-inflammatory activity and works to reduce the migration and activation of neutrophils, eosinophils, mononuclears and other cells. GCs reduce the migration of neutrophils into the lungs ergo, decreasing their adherence to the endothelium. Thus, there is a reduction in the action of mediators released from these cells and therefore, a reduced inflammatory response.

GCs also possess a genomic mechanism of action in which, once bound to a glucocorticoid receptor, the activated complex moves into the nucleus and inhibits transcription of mRNA. Ultimately, effecting whether various proteins get produced or not. Inhibiting the transcription of nuclear factor (NF-κB) and protein activator (AP-1) attenuates the expression of pro-inflammatory cytokines (IL-1, IL-6, IL-8 and TNF etc.), enzymes (PLA, COX-2, iNOs etc.) and other biologically active substances. The anti-inflammatory effect of GCs is also demonstrated by enhancing the activity of lipocortines which inhibit the activity of PLA and therefore, decrease the production of arachidonic acid and mediators of lipoxygenase and cyclooxygenase pathways. 

Anti-inflammatories need to be administered as quickly as possible as the effect of these drugs can diminish even just an hour after meconium aspiration. For example, early administration of dexamethasone significantly enhanced gas exchange, reduced ventilatory pressures, decreased the number of neutrophils in the bronchoalveolar area, reduced oedema formation and oxidative lung injury.However, GCs may increase the risk of infection and this risk increases with the dose and duration of glucocorticoid treatment. Other issues can arise, such as aggravation of diabetes mellitus, osteoporosis, skin atrophy and growth retardation in children. 

Phosphodiesterases (PDE) degrades cAMP and cGMP and, within the respiratory system of a newborn with MAS, various isoforms of PDE may be involved due to their pro-inflammatory and smooth muscle contractile activity. Therefore, non-selective and selective inhibitors of PDE could potentially be used in MAS therapy. However, the use of PDE inhibitors can cause cardiovascular side effects. Non-selective PDE inhibitors, such as methylxanthines, increase concentrations of cAMP and cGMP in the cells leading to bronchodilation and vasodilation. Additionally, methylxanthines decreases the concentrations of calcium, acetylcholine and monoamines, this controls the release of various mediators of inflammation and bronchoconstriction, including prostaglandins. Selective PDE inhibitors target one subtype of phosphodiesterase and in MAS the activities of PDE-3, PDE-4, PDE-5 and PDE-7 may become enhanced. For example, Milrinone (a selective PDE3 inhibitor) improved oxygenation and survival of neonates with MAS.

Arachidonic acid is metabolised, via cyclooxygenase (COX) and lipoxygenase, to various substances including prostaglandins and leukotrienes, which exhibit potent pro-inflammatory and vasoactive effects. By inhibiting COX, and more specifically COX-2, (either through selective or non-selective drugs) inflammation and oedema can be reduced. However, COX inhibitors may induce peptic ulcers and cause hyperkalemia and hypernatremia. Additionally, COX inhibitors have not shown any great response in the treatment of MAS.

Meconium is typically sterile however, it can contain various cultures of bacteria so appropriate antibiotics may need to be prescribed. 

Lung lavage with diluted surfactant is a new treatment with potentially beneficial results depending on how early it is administered in newborns with MAS. This treatment shows promise as it has a significant effect on air leaks, pneumothorax, the need for ECMO and death. Early intervention and using it on newborns with mild MAS is more effective. However, there are risks as a large volume of fluid instillation to the lung of a newborn can be dangerous (particularly in cases of severe MAS with pulmonary hypertension) as it can exacerbate hypoxia and lead to mortality.

Originally, it was believed that MAS developed as a result of the meconium being a physical blockage of the airways. Thus, to prevent newborns, who were born through MSAF, from developing MAS, suctioning of the oropharyngeal and nasopharyngeal area before delivery of the shoulders followed by tracheal aspiration was utilised for 20 years. This treatment was believed to be effective as it was reported to significantly decrease the incidence of MAS compared to those newborns born through MSAF who were not treated. This claim was later disproved and future studies concluded that oropharyngeal and nasopharyngeal suctioning, before delivery of the shoulders in infants born through MSAF, does not prevent MAS or its complications. In fact, it can cause more issues and damage (e.g. mucosal damage), thus it is not a recommended preventative treatment. Suctioning may not significantly reduce the incidence of MAS as meconium passage and aspiration may occur "in-utero." Thereby making the suctioning redundant and useless as the meconium may already deep within the lungs at the time of birth. 

Historically, amnioinfusion has been used when MSAF was present, which involves a transcervical infusion of fluid during labour. The idea was to dilute the thick meconium to reduce its potential pathophysiology and reduce cases of MAS, since MAS is more prevalent in cases of thick meconium. However, there are associated risks, such as umbilical cord prolapse and prolongation of labour. The UK National Institute of Health and Clinical Excellence (NICE) Guidelines recommend against the use of amnioinfusion in women with MSAF.

In generally, the incidence of MAS has been significantly reduced over the past two decades as the number of post-term deliveries has minimised. Currently, labour is induced in women who have been pregnant for longer than 41 weeks gestation.

Prevention during pregnancy may include amnioinfusion and antibiotics but the effectiveness of these treatments are questionable. 

As previously mentioned, oropharyngeal and nasopharyngeal suctioning is not an ideal preventative treatment for both vigorous and depressed (not breathing) infants.

Research is being focused on developing both a successful method for preventing MAS as well as an effective treatment. For example, investigations are being made in the efficiency of anti-inflammatory agents, surfactant replacement therapy and antibiotic therapy. More research needs to be conducted on the pharmacological properties of, for example, glucocorticoids, including dosages, administration, timing or any drug interactions. Additionally, there is still research being conducted on whether intubation and suctioning of meconium in newborns with MAS is beneficial, harmful or is simply a redundant and outdated treatment. In generally, there is still no generally accepted therapeutic protocol and effective treatment plan for MAS.




</doc>
<doc id="20453" url="https://en.wikipedia.org/wiki?curid=20453" title="Meconium">
Meconium

Meconium is the earliest stool of a mammalian infant. Unlike later feces, meconium is composed of materials ingested during the time the infant spends in the uterus: intestinal epithelial cells, lanugo, mucus, amniotic fluid, bile, and water. Meconium, unlike later feces, is viscous and sticky like tar, its color usually being a very dark olive green; it is almost odorless. When diluted in amniotic fluid, it may appear in various shades of green, brown, or yellow. It should be completely passed by the end of the first few days after birth, with the stools progressing toward yellow (digested milk).

Meconium is normally retained in the infant's bowel until after birth, but sometimes it is expelled into the amniotic fluid (also called "amniotic liquor") prior to birth or during labor and delivery. The stained amniotic fluid (called "meconium liquor" or "meconium stained liquor") is recognized by medical staff that this may be a sign of fetal distress. Some post-dates pregnancies (where the woman is more than 40 weeks pregnant) may also have meconium stained liquor without fetal distress. Medical staff may aspirate the meconium from the nose and mouth of a newborn immediately after delivery in the event the baby shows signs of respiratory distress to decrease the risk of meconium aspiration syndrome.

Meconium had been thought to be sterile until researchers found bacterial communities in it so developed that they seemed to fall into two categories. Around half of the samples appeared to be dominated by bacteria that produce lactic acid, such as Lactobacillus, while the other half mostly contained a family of so-called enteric bacteria, such as "Escherichia coli".

The Latin term "meconium" derives from the Greek , "mēkōnion", a diminutive of , "mēkōn", i.e. poppy, in reference either to its tarry appearance that may resemble some raw opium preparations, or to Aristotle's belief that it induces sleep in the fetus.

A symptom of both Hirschsprung's disease and cystic fibrosis is the failure to pass meconium.

Meconium can be tested for various drugs, to check for "in utero" exposure. Using meconium, a Canadian research group showed that by measuring a by-product of alcohol (FAEE) they could objectively detect babies exposed to excessive maternal drinking of alcohol in pregnancy. In the USA, the results of meconium testing may be used by child protective services and other law enforcement agencies to determine the eligibility of the parents to keep the newborn.

Most of the time that the amniotic fluid is stained with meconium it will be homogeneously distributed throughout the fluid making it brown. This indicates that the fetus passed the meconium some time ago such that sufficient mixing occurred as to establish the homogeneous mixture. Terminal meconium occurs when the fetus passes the meconium a short enough time before birth/cesarean section that the amniotic fluid remains clear, but individual clumps of meconium are in the fluid.

The meconium sometimes becomes thickened and congested in the intestines, a condition known as meconium ileus. Meconium ileus is often the first sign of cystic fibrosis. In cystic fibrosis, the meconium can form a bituminous black-green mechanical obstruction in a segment of the ileum. Beyond this, there may be a few separate grey-white globular pellets. Below this level, the bowel is a narrow and empty micro-colon. Above the level of the obstruction, there are several loops of hypertrophied bowel distended with fluid. No meconium is passed, and abdominal distension and vomiting appear soon after birth. About 20% of cases of cystic fibrosis present with meconium ileus, while approximately 20% of one series of cases of meconium ileus did not have cystic fibrosis. The presence of meconium ileus is not related to the severity of the cystic fibrosis. The obstruction can be relieved in a number of different ways.

Meconium ileus should be distinguished from meconium plug syndrome, in which a tenacious mass of mucus prevents the meconium from passing and there is no risk of intestinal perforation. Meconium ileus has a significant risk of intestinal perforation. In a barium enema, meconium plug syndrome shows a normal or dilated colon as compared to micro-colon in meconium ileus.



</doc>
<doc id="20454" url="https://en.wikipedia.org/wiki?curid=20454" title="Montreux Convention Regarding the Regime of the Straits">
Montreux Convention Regarding the Regime of the Straits

The Montreux Convention Regarding the Regime of the Straits is a 1936 agreement that gives Turkey control over the Bosporus Straits and the Dardanelles and regulates the transit of naval warships. The Convention gives Turkey full control over the Straits and guarantees the free passage of civilian vessels in peacetime. It restricts the passage of naval ships not belonging to Black Sea states. The terms of the convention have been the source of controversy over the years, most notably concerning the Soviet Union's military access to the Mediterranean Sea.

Signed on 20 July 1936 at the Montreux Palace in Switzerland, it permitted Turkey to remilitarise the Straits. It went into effect on 9 November 1936 and was registered in "League of Nations Treaty Series" on 11 December 1936. It remains in force, with some amendments.

The proposed 21st century Kanal Istanbul project may constitute a possible by-pass to the Montreux Convention and force greater Turkish autonomy with respect to the passage of military ships from the Black Sea to the Sea of Marmara.

The convention was one of a series of agreements in the 19th and 20th centuries that sought to address the long-running "Straits Question" of who should control the strategically vital link between the Black Sea and Mediterranean Sea. In 1923 the Treaty of Lausanne had demilitarised the Dardanelles and opened the Straits to unrestricted civilian and military traffic, under the supervision of the International Straits Commission of the League of Nations.

By the late 1930s, the strategic situation in the Mediterranean had altered with the rise of Fascist Italy, which controlled the Greek-inhabited Dodecanese islands off the west coast of Turkey and had constructed fortifications on Rhodes, Leros and Kos. The Turks feared that Italy would seek to exploit access to the Straits to expand its power into Anatolia and the Black Sea region. There were also fears of Bulgarian rearmament. Although Turkey was not permitted to refortify the Straits, it nonetheless did so secretly.

In April 1935, the Turkish government dispatched a lengthy diplomatic note to the signatories of the Treaty of Lausanne proposing a conference on the agreement of a new regime for the Straits and requested that the League of Nations authorise the reconstruction of the Dardanelles forts. In the note, Turkish foreign minister Tevfik Rüştü Aras explained that the international situation had changed greatly since 1923. At that time, Europe had been moving towards disarmament and an international guarantee to defend the Straits. The Abyssinia Crisis of 1934–35, the denunciation by Germany of the Treaty of Versailles and international moves towards rearmament meant that "the only guarantee intended to guard against the total insecurity of the Straits has just disappeared in its turn." Indeed, Aras said, "the Powers most closely concerned are proclaiming the existence of a threat of general conflagration." The key weaknesses of the present regime were that the machinery for collective guarantees were too slow and ineffective, there was no contingency for a general threat of war and no provision for Turkey to defend itself. Turkey was therefore prepared

The response to the note was generally favourable, and Australia, Bulgaria, France, Germany, Greece, Japan, Romania, the Soviet Union, Turkey, the United Kingdom and Yugoslavia agreed to attend negotiations at Montreux in Switzerland, which began on 22 June 1936. Two major powers were not represented: Italy, whose aggressively expansionist policies had prompted the conference in the first place, refused to attend and the United States declined even to send an observer.

Turkey, the UK and the Soviet Union each put forward their own set of proposals, aimed chiefly at protecting their own interests. The British favoured the continuation of a relatively restrictive approach, while the Turks sought a more liberal regime that reasserted their own control over the Straits and the Soviets proposed a regime that would guarantee absolute freedom of passage. The British, supported by France, sought to exclude the Soviet fleet from the Mediterranean Sea, where it might have threatened the vital shipping lanes to India, Egypt and the Far East. In the end, the British conceded some of their requests while the Soviets succeeded in ensuring that the Black Sea countries – including the USSR – were given some exemptions from the military restrictions imposed on non-Black Sea nations. The agreement was ratified by all of the conference attendees with the exception of Germany, which had not been a signatory to the Treaty of Lausanne, and with reservations by Japan, and came into force on 9 November 1936.

Britain's willingness to make concessions has been attributed to a desire to avoid Turkey being driven to ally itself with, or fall under the influence of, Adolf Hitler or Benito Mussolini. It was thus the first in a series of steps by Britain and France to ensure that Turkey would either remain neutral or tilt towards the Western Allies in the event of any future conflict with the Axis.

The Convention consists of 29 Articles, four annexes and one protocol. Articles 2–7 consider the passage of merchant ships. Articles 8–22 consider the passage of war vessels. The key principle of freedom of passage and navigation is stated in articles 1 and 2. Article 1 provides that "The High Contracting Parties recognise and affirm the principle of freedom of passage and navigation by sea in the Straits". Article 2 states that "In time of peace, merchant vessels shall enjoy complete freedom of passage and navigation in the Straits, by day and by night, under any flag with any kind of cargo."

The International Straits Commission was abolished, authorising the full resumption of Turkish military control over the Straits and the refortification of the Dardanelles. Turkey was authorised to close the Straits to all foreign warships in wartime or when it was threatened by aggression; additionally, it was authorised to refuse transit from merchant ships belonging to countries at war with Turkey.

A number of highly specific restrictions were imposed on what type of warships are allowed passage. No more than nine foreign warships, with a total aggregate tonnage of 15,000 tons, may pass at any one time, therefore a single non-Black Sea state warship passing the straits might not exceed 15,000 tons. An aggregate tonnage of all non-Black Sea warships in the Black Sea must be no more than 30,000 tons (or 45,000 tons under special conditions), and they are permitted to stay in the Black Sea for no longer than twenty-one days. Only Black Sea states may transit capital ships of any tonnage, escorted by no more than two destroyers. 

Under Article 12, Black Sea states are also allowed to send submarines through the Straits, with prior notice, as long as the vessels have been constructed, purchased or sent for repair outside the Black Sea. The less restrictive rules applicable to Black Sea states were agreed as, effectively, a concession to the Soviet Union, the only Black Sea state other than Turkey with any significant number of capital ships or submarines. The passage of civil aircraft between the Mediterranean and Black Seas is permitted, but only along routes authorised by the Turkish government.

The terms of the Convention were largely a reflection of the international situation in the mid-1930s. They largely served Turkish and Soviet interests, enabling Turkey to regain military control of the Straits and assuring Soviet dominance of the Black Sea. Although the Convention restricted the Soviets' ability to send naval forces into the Mediterranean Sea—thereby satisfying British concerns about Soviet intrusion into what was considered a British sphere of influence— it also ensured that outside powers could not exploit the Straits to threaten the Soviet Union. This was to have significant repercussions during World War II when the Montreux regime prevented the Axis powers from sending naval forces through the Straits to attack the Soviet Union. The Axis powers were thus severely limited in naval capability in their Black Sea campaigns, relying principally on small vessels that had been transported overland by rail and canal networks. Auxiliary vessels and armed merchant ships occupied a grey area, however, and the transit of such vessels through the straits led to friction between the Allies and Turkey. Repeated protests from Moscow and London led to the Turkish government banning the movements of "suspicious" Axis ships with effect from June 1944 after a number of German auxiliary ships were permitted to transit the Straits.

Although the Montreux Convention is cited by the Turkish government as prohibiting aircraft carriers in the straits, the treaty actually contains no explicit prohibition on aircraft carriers. However, aircraft carriers are not listed among naval ship classes, that have a right of passage under Articles 10, 11 and 12, and according to Article 10, vessels of war "other than those which fall within the categories specified in the preceding paragraph shall only enjoy a right of transit under the special conditions provided by Articles 11 and 12". In any case, modern aircraft carriers are heavier than the 15,000 ton limit, making it impossible for non-Black Sea powers to transit modern aircraft carriers through the Straits.

Under Article 11, Black Sea states are permitted to transit capital ships of any tonnage through the straits, but Annex II specifically excludes aircraft carriers from the definition of capital ship. In 1936, it was common for battleships to carry observation aircraft. Therefore, aircraft carriers were defined as ships that were "designed or adapted primarily for the purpose of carrying and operating aircraft at sea." The inclusion of aircraft on any other ship does not classify it as an aircraft carrier.

To take advantage of this exception, the Soviet Union designated its Kiev-class and Kuznetsov-class aircraft carriers as "aircraft carrying cruisers." The aircraft carriers were armed with P-500 and P-700 cruise missiles, which were also found on the Slava-class cruiser and the Kirov-class battlecruiser. The result of this is that the Soviet Navy could send its aircraft cruisers through the Straits in compliance with the Convention, while at the same time the Convention denied access to NATO aircraft carriers, which exceeded the 15,000 ton limit.

Turkey chose to accept the designation of the Soviet aircraft carriers as aircraft cruisers. Any revision of the Montreux Convention could leave Turkey with less control over the Turkish Straits. The UN Convention on the Law of the Sea had already established more liberal passage through other straits. By allowing the Soviet aircraft cruisers to transit the Straits, Turkey could leave the more restrictive Montreux Convention in place.

The Convention remains in force, with amendments, though not without dispute. It was repeatedly challenged by the Soviet Union during World War II and the Cold War. As early as 1939, Joseph Stalin sought to reopen the Straits Question and proposed joint Turkish and Soviet control of the Straits, complaining that "a small state [i.e. Turkey] supported by Great Britain held a great state by the throat and gave it no outlet." After the Molotov–Ribbentrop Pact was signed by the Soviet Union and Nazi Germany, the Soviet Foreign Minister Vyacheslav Molotov informed his German counterparts that the USSR wished to take military control of the Straits and establish its own military base there. The Soviets returned to the issue in 1945 and 1946, demanding a revision of the Montreux Convention at a conference excluding most of the Montreux signatories, a permanent Soviet military presence and joint control of the Straits. This was firmly rejected by Turkey, despite an ongoing Soviet "strategy of tension". For several years after World War II, the Soviets exploited the restriction on the number of foreign warships by ensuring that one of theirs was always in the Straits, thus effectively blocking any nation other than Turkey from sending warships through the Straits. Soviet pressure expanded into full on demands to revise the Montreux Convention, which led to the Turkish Straits crisis of 1946, which led to Turkey abandoning its policy of neutrality. In 1947 it became the recipient of US military and economic assistance under the Truman Doctrine of "containment" and joined NATO, along with Greece, in 1952.

The passage of US warships through the Straits also raised controversy, as the convention forbids the transit of non-Black Sea nations' warships with guns of a calibre larger than eight inches (203 mm). In the 1960s, the US sent warships carrying 420 mm calibre ASROC missiles through the Straits, prompting Soviet protests. The Turkish government rejected the Soviet complaints, pointing out that guided missiles were not guns and that such weapons had not even existed at the time of the Convention's agreement so were not restricted.

The United Nations Convention on the Law of the Sea (UNCLOS), which entered into force in November 1994, has prompted calls for the Montreux Convention to be revised and adapted to make it compatible with UNCLOS's regime governing straits used for international navigation. However, Turkey's long-standing refusal to sign UNCLOS has meant that Montreux remains in force without further amendments.

The safety of vessels passing through the Bosporus has become a major concern in recent years as the volume of traffic has increased greatly since the Convention was signed – from 4,500 in 1934 to 49,304 by 1998. As well as obvious environmental concerns, the Straits bisect the city of Istanbul with over 14 million people living on its shores; maritime incidents in the Straits therefore pose a considerable risk to public safety. The Convention does not, however, make any provision for the regulation of shipping for the purposes of safety and environmental protection. In January 1994 the Turkish government adopted new "Maritime Traffic Regulations for the Turkish Straits and the Marmara Region". This introduced a new regulatory regime "in order to ensure the safety of navigation, life and property and to protect the environment in the region" but without violating the Montreux principle of free passage. The new regulations provoked some controversy when Russia, Greece, Cyprus, Romania, Ukraine and Bulgaria raised objections. However, they were approved by the International Maritime Organisation on the grounds that they were not intended to prejudice "the rights of any ship using the Straits under international law". The regulations were revised in November 1998 to address Russian concerns.



</doc>
<doc id="20455" url="https://en.wikipedia.org/wiki?curid=20455" title="Michael Jordan">
Michael Jordan

Michael Jeffrey Jordan (born February 17, 1963), also known by his initials, MJ, is an American former professional basketball player. He played 15 seasons in the National Basketball Association (NBA) for the Chicago Bulls and Washington Wizards. His biography on the official NBA website states: "By acclamation, Michael Jordan is the greatest basketball player of all time." Jordan was one of the most effectively marketed athletes of his generation and was considered instrumental in popularizing the NBA around the world in the 1980s and 1990s. He is currently the principal owner and chairman of the NBA's Charlotte Hornets.

Jordan played three seasons for coach Dean Smith at the University of North Carolina. As a freshman, he was a member of the Tar Heels' national championship team in 1982. Jordan joined the Bulls in 1984 as the third overall draft pick. He quickly emerged as a league star and entertained crowds with his prolific scoring. His leaping ability, demonstrated by performing slam dunks from the free throw line in slam dunk contests, earned him the nicknames Air Jordan and His Airness. He also gained a reputation for being one of the best defensive players in basketball. In 1991, he won his first NBA championship with the Bulls, and followed that achievement with titles in 1992 and 1993, securing a "three-peat". Although Jordan abruptly retired from basketball before the beginning of the 1993–94 NBA season and started a new career playing minor league baseball, he returned to the Bulls in March 1995 and led them to three additional championships in 1996, 1997, and 1998, as well as a then-record 72 regular-season wins in the 1995–96 NBA season. Jordan retired for a second time in January 1999, but returned for two more NBA seasons from 2001 to 2003 as a member of the Wizards.

Jordan's individual accolades and accomplishments include six NBA Finals Most Valuable Player (MVP) Awards, ten scoring titles (both all-time records), five MVP Awards, ten All-NBA First Team designations, nine All-Defensive First Team honors, fourteen NBA All-Star Game appearances, three All-Star Game MVP Awards, three steals titles, and the 1988 NBA Defensive Player of the Year Award. He holds the NBA records for highest career regular season scoring average (30.12 points per game) and highest career playoff scoring average (33.45 points per game). In 1999, he was named the greatest North American athlete of the 20th century by ESPN, and was second to Babe Ruth on the Associated Press's list of athletes of the century. Jordan is a two-time inductee into the Basketball Hall of Fame, having been enshrined in 2009 for his individual career, and again in 2010 as part of the group induction of the 1992 United States men's Olympic basketball team ("The Dream Team"). He became a member of the FIBA Hall of Fame in 2015.

Jordan is also known for his product endorsements. He fueled the success of Nike's Air Jordan sneakers, which were introduced in 1985 and remain popular today. Jordan also starred as himself in the 1996 film "Space Jam". In 2006, he became part-owner and head of basketball operations for the Charlotte Bobcats; he bought a controlling interest in 2010. In 2014, Jordan became the first billionaire player in NBA history. He is the third-richest African-American, behind Robert F. Smith and Oprah Winfrey.

Jordan was born in Brooklyn, New York, to Deloris (née Peoples), who worked in banking, and James R. Jordan Sr., an equipment supervisor. His family moved to Wilmington, North Carolina, when he was a toddler.

Jordan is the fourth of five children. He has two older brothers, Larry Jordan and James R. Jordan, Jr., one older sister, Deloris, and one younger sister, Roslyn. Jordan's brother James retired in 2006 as the Command Sergeant Major of the 35th Signal Brigade of the XVIII Airborne Corps in the U.S. Army.

Jordan attended Emsley A. Laney High School in Wilmington, where he highlighted his athletic career by playing basketball, baseball, and football. He tried out for the varsity basketball team during his sophomore year, but at 5'11" (1.80 m), he was deemed too short to play at that level. His taller friend, Harvest Leroy Smith, was the only sophomore to make the team.

Motivated to prove his worth, Jordan became the star of Laney's junior varsity squad, and tallied several 40-point games. The following summer, he grew four inches (10 cm) and trained rigorously. Upon earning a spot on the varsity roster, Jordan averaged about 20 points per game over his final two seasons of high school play. As a senior, he was selected to the McDonald's All-American Team after averaging a triple-double: 29.2 points, 11.6 rebounds, and 10.1 assists.

Jordan was recruited by numerous college basketball programs, including Duke, North Carolina, South Carolina, Syracuse, and Virginia. In 1981, Jordan accepted a basketball scholarship to North Carolina, where he majored in cultural geography.

As a freshman in coach Dean Smith's team-oriented system, he was named ACC Freshman of the Year after he averaged 13.4 points per game (ppg) on 53.4% shooting (field goal percentage). He made the game-winning jump shot in the 1982 NCAA Championship game against Georgetown, which was led by future NBA rival Patrick Ewing. Jordan later described this shot as the major turning point in his basketball career. During his three seasons at North Carolina, he averaged 17.7 ppg on 54.0% shooting, and added 5.0 rebounds per game (rpg). He was selected by consensus to the NCAA All-American First Team in both his sophomore (1983) and junior (1984) seasons. After winning the Naismith and the Wooden College Player of the Year awards in 1984, Jordan left North Carolina one year before his scheduled graduation to enter the 1984 NBA draft. The Chicago Bulls selected Jordan with the third overall pick, after Hakeem Olajuwon (Houston Rockets) and Sam Bowie (Portland Trail Blazers). One of the primary reasons why Jordan was not drafted sooner was because the first two teams were in need of a center. However, Trail Blazers general manager Stu Inman contended that it was not a matter of drafting a center, but more a matter of taking Sam Bowie over Jordan, in part because Portland already had Clyde Drexler, who was a guard with similar skills to Jordan. ESPN, citing Bowie's injury-laden college career, named the Blazers' choice of Bowie as the worst draft pick in North American professional sports history. Jordan returned to North Carolina to complete his degree in 1986. He graduated the same year with a Bachelor of Arts degree in geography.

During his rookie season with the Bulls, Jordan averaged 28.2 ppg on 51.5% shooting. He quickly became a fan favorite even in opposing arenas, and appeared on the cover of "Sports Illustrated" with the heading "A Star Is Born" just over a month into his professional career. The fans also voted in Jordan as an All-Star starter during his rookie season. Controversy arose before the All-Star game when word surfaced that several veteran players—led by Isiah Thomas—were upset by the amount of attention Jordan was receiving. This led to a so-called "freeze-out" on Jordan, where players refused to pass the ball to him throughout the game. The controversy left Jordan relatively unaffected when he returned to regular season play, and he would go on to be voted Rookie of the Year. The Bulls finished the season 38–44 and lost to the Milwaukee Bucks in four games in the first round of the playoffs.

Jordan's second season was cut short when he broke his foot in the third game of the year, causing him to miss 64 games. Despite Jordan's injury and a 30–52 record (at the time it was fifth worst record of any team to qualify for the playoffs in NBA history), the Bulls made the playoffs. Jordan recovered in time to participate in the playoffs and performed well upon his return. Against a 1985–86 Boston Celtics team that is often considered one of the greatest in NBA history, Jordan set the still-unbroken record for points in a playoff game with 63 in Game 2. The Celtics, however, managed to sweep the series.

Jordan had completely recovered in time for the 1986–87 season, and he had one of the most prolific scoring seasons in NBA history. He became the only player other than Wilt Chamberlain to score 3,000 points in a season, averaging a league high 37.1 points on 48.2% shooting. In addition, Jordan demonstrated his defensive prowess, as he became the first player in NBA history to record 200 steals and 100 blocked shots in a season. Despite Jordan's success, Magic Johnson won the league's Most Valuable Player Award. The Bulls reached 40 wins, and advanced to the playoffs for the third consecutive year. However, they were again swept by the Celtics.

Jordan again led the league in scoring during the 1987–88 season, averaging 35.0 ppg on 53.5% shooting and won his first league MVP Award. He was also named the Defensive Player of the Year, as he had averaged 1.6 blocks and a league high 3.16 steals per game. The Bulls finished 50–32, and made it out of the first round of the playoffs for the first time in Jordan's career, as they defeated the Cleveland Cavaliers in five games. However, the Bulls then lost in five games to the more experienced Detroit Pistons, who were led by Isiah Thomas and a group of physical players known as the "".

In the 1988–89 season, Jordan again led the league in scoring, averaging 32.5 ppg on 53.8% shooting from the field, along with 8 rpg and 8 assists per game (apg). The Bulls finished with a 47–35 record, and advanced to the Eastern Conference Finals, defeating the Cavaliers and New York Knicks along the way. The Cavaliers series included a career highlight for Jordan when he hit "The Shot" over Craig Ehlo at the buzzer in the fifth and final game of the series. However, the Pistons again defeated the Bulls, this time in six games, by utilizing their "Jordan Rules" method of guarding Jordan, which consisted of double and triple teaming him every time he touched the ball.

The Bulls entered the 1989–90 season as a team on the rise, with their core group of Jordan and young improving players like Scottie Pippen and Horace Grant, and under the guidance of new coach Phil Jackson. Jordan averaged a league leading 33.6 ppg on 52.6% shooting, to go with 6.9 rpg and 6.3 apg in leading the Bulls to a 55–27 record. They again advanced to the Eastern Conference Finals after beating the Bucks and Philadelphia 76ers. However, despite pushing the series to seven games, the Bulls lost to the Pistons for the third consecutive season.

In the 1990–91 season, Jordan won his second MVP award after averaging 31.5 ppg on 53.9% shooting, 6.0 rpg, and 5.5 apg for the regular season. The Bulls finished in first place in their division for the first time in 16 years and set a franchise record with 61 wins in the regular season. With Scottie Pippen developing into an All-Star, the Bulls had elevated their play. The Bulls defeated the New York Knicks and the Philadelphia 76ers in the opening two rounds of the playoffs. They advanced to the Eastern Conference Finals where their rival, the Detroit Pistons, awaited them. However, this time the Bulls beat the Pistons in a four-game sweep.

The Bulls advanced to the NBA Finals for the first time in franchise history to face the Los Angeles Lakers, who had Magic Johnson and James Worthy, two formidable opponents. The Bulls won the series four games to one, and compiled a 15–2 playoff record along the way. Perhaps the best known moment of the series came in Game 2 when, attempting a dunk, Jordan avoided a potential Sam Perkins block by switching the ball from his right hand to his left in mid-air to lay the shot into the basket. In his first Finals appearance, Jordan posted per game averages of 31.2 points on 56% shooting from the field, 11.4 assists, 6.6 rebounds, 2.8 steals, and 1.4 blocks. Jordan won his first NBA Finals MVP award, and he cried while holding the NBA Finals trophy.

Jordan and the Bulls continued their dominance in the 1991–92 season, establishing a 67–15 record, topping their franchise record from 1990–91. Jordan won his second consecutive MVP award with averages of 30.1 points, 6.4 rebounds and 6.1 assists per game on 52% shooting. After winning a physical 7-game series over the New York Knicks in the second round of the playoffs and finishing off the Cleveland Cavaliers in the Conference Finals in 6 games, the Bulls met Clyde Drexler and the Portland Trail Blazers in the Finals. The media, hoping to recreate a Magic–Bird rivalry, highlighted the similarities between "Air" Jordan and Clyde "The Glide" during the pre-Finals hype. In the first game, Jordan scored a Finals-record 35 points in the first half, including a record-setting six three-point field goals. After the sixth three-pointer, he jogged down the court shrugging as he looked courtside. Marv Albert, who broadcast the game, later stated that it was as if Jordan was saying, "I can't believe I'm doing this." The Bulls went on to win Game 1, and defeat the Blazers in six games. Jordan was named Finals MVP for the second year in a row and finished the series averaging 35.8 ppg, 4.8 rpg, and 6.5 apg, while shooting 53% from the floor.

In the 1992–93 season, despite a 32.6 ppg, 6.7 rpg, and 5.5 apg campaign, Jordan's streak of consecutive MVP seasons ended as he lost the award to his friend Charles Barkley. Coincidentally, Jordan and the Bulls met Barkley and his Phoenix Suns in the 1993 NBA Finals. The Bulls won their third NBA championship on a game-winning shot by John Paxson and a last-second block by Horace Grant, but Jordan was once again Chicago's leader. He averaged a Finals-record 41.0 ppg during the six-game series, and became the first player in NBA history to win three straight Finals MVP awards. He scored more than 30 points in every game of the series, including 40 or more points in 4 consecutive games. With his third Finals triumph, Jordan capped off a seven-year run where he attained seven scoring titles and three championships, but there were signs that Jordan was tiring of his massive celebrity and all of the non-basketball hassles in his life.

During the Bulls' playoff run in 1993, controversy arose when Jordan was seen gambling in Atlantic City, New Jersey, the night before a game against the New York Knicks. In that same year, he admitted that he had to cover $57,000 in gambling losses, and author Richard Esquinas wrote a book claiming he had won $1.25 million from Jordan on the golf course. In 2005, Jordan talked to Ed Bradley of the CBS evening show "60 Minutes" about his gambling and admitted that he made some reckless decisions. Jordan stated, "Yeah, I've gotten myself into situations where I would not walk away and I've pushed the envelope. Is that compulsive? Yeah, it depends on how you look at it. If you're willing to jeopardize your livelihood and your family, then yeah." When Bradley asked him if his gambling ever got to the level where it jeopardized his livelihood or family, Jordan replied, "No."

On October 6, 1993, Jordan announced his retirement, citing a loss of desire to play the game. Jordan later stated that the death of his father three months earlier also shaped his decision. Jordan's father was murdered on July 23, 1993, at a highway rest area in Lumberton, North Carolina, by two teenagers, Daniel Green and Larry Martin Demery, who carjacked his luxury Lexus bearing the license plate "UNC 0023". His body was dumped in a South Carolina swamp and was not discovered until August 3. The assailants were traced from calls that they made on James Jordan's cell phone. The two criminals were caught, convicted at trial, and sentenced to life in prison. Jordan was close to his father; as a child he had imitated his father's proclivity to stick out his tongue while absorbed in work. He later adopted it as his own signature, displaying it each time he drove to the basket. In 1996, he founded a Chicago area Boys & Girls Club and dedicated it to his father.

In his 1998 autobiography "For the Love of the Game", Jordan wrote that he had been preparing for retirement as early as the summer of 1992. The added exhaustion due to the Dream Team run in the 1992 Olympics solidified Jordan's feelings about the game and his ever-growing celebrity status. Jordan's announcement sent shock waves throughout the NBA and appeared on the front pages of newspapers around the world.

Jordan then further surprised the sports world by signing a minor league baseball contract with the Chicago White Sox on February 7, 1994. He reported to spring training in Sarasota, Florida, and was assigned to the team's minor league system on March 31, 1994. Jordan has stated this decision was made to pursue the dream of his late father, who had always envisioned his son as a Major League Baseball player. The White Sox were another team owned by Bulls owner Jerry Reinsdorf, who continued to honor Jordan's basketball contract during the years he played baseball.

In 1994, Jordan played for the Birmingham Barons, a Double-A minor league affiliate of the Chicago White Sox, batting .202 with three home runs, 51 runs batted in, 30 stolen bases, 114 strikeouts, 51 base on balls, and 11 errors. He also appeared for the Scottsdale Scorpions in the 1994 Arizona Fall League, batting .252 against the top prospects in baseball. On November 1, 1994, his number 23 was retired by the Bulls in a ceremony that included the erection of a permanent sculpture known as "The Spirit" outside the new United Center.

In the 1993–94 season, the Bulls achieved a 55–27 record without Jordan in the lineup, and lost to the New York Knicks in the second round of the playoffs. The 1994–95 Bulls were a shell of the championship team of just two years earlier. Struggling at mid-season to ensure a spot in the playoffs, Chicago was 31–31 at one point in mid-March. The team received help, however, when Jordan decided to return to the Bulls.

In March 1995, Jordan decided to quit baseball due to the ongoing Major League Baseball strike, as he wanted to avoid becoming a potential replacement player. On March 18, 1995, Jordan announced his return to the NBA through a two-word press release: "I'm back." The next day, Jordan took to the court with the Bulls to face the Indiana Pacers in Indianapolis, scoring 19 points. The game had the highest Nielsen rating of a regular season NBA game since 1975. Although he could have opted to wear his normal number in spite of the Bulls having retired it, Jordan instead wore number 45, as he had while playing baseball.

Although he had not played an NBA game in a year and a half, Jordan played well upon his return, making a game-winning jump shot against Atlanta in his fourth game back. He then scored 55 points in the next game against the Knicks at Madison Square Garden on March 28, 1995. Boosted by Jordan's comeback, the Bulls went 13–4 to make the playoffs and advanced to the Eastern Conference Semifinals against the Orlando Magic. At the end of Game 1, Orlando's Nick Anderson stripped Jordan from behind, leading to the game-winning basket for the Magic; he would later comment that Jordan "didn't look like the old Michael Jordan" and that "No. 45 doesn't explode like No. 23 used to."

Jordan responded by scoring 38 points in the next game, which Chicago won. Before the game, Jordan decided that he would immediately resume wearing his former number, 23. The Bulls were fined $25,000 for failing to report the impromptu number change to the NBA. Jordan was fined an additional $5,000 for opting to wear white sneakers when the rest of the Bulls wore black. He averaged 31 points per game in the series, but Orlando won the series in 6 games.

Jordan was freshly motivated by the playoff defeat, and he trained aggressively for the 1995–96 season. The Bulls were strengthened by the addition of rebound specialist Dennis Rodman, and the team dominated the league, starting the season at 41–3. The Bulls eventually finished with the then-best regular season record in NBA history, 72–10; this record was later surpassed by the 2015–16 Golden State Warriors. Jordan led the league in scoring with 30.4 ppg and won the league's regular season and All-Star Game MVP awards.

In the playoffs, the Bulls lost only three games in four series (Miami Heat 3–0, New York Knicks 4–1, Orlando Magic 4–0). They defeated the Seattle SuperSonics 4–2 in the NBA Finals to win their fourth championship. Jordan was named Finals MVP for a record fourth time, surpassing Magic Johnson's three Finals MVP awards. He also achieved only the second sweep of the MVP Awards in the All-Star Game, regular season and NBA Finals, Willis Reed having achieved the first, during the 1969–70 season. Because this was Jordan's first championship since his father's murder, and it was won on Father's Day, Jordan reacted very emotionally upon winning the title, including a memorable scene of him crying on the locker room floor with the game ball.

In the 1996–97 season, the Bulls started out 69–11, but missed out on a second consecutive 70-win season by losing their final two games to finish 69–13. However, this year Jordan was beaten for the NBA MVP Award by Karl Malone. The Bulls again advanced to the Finals, where they faced Malone and the Utah Jazz. The series against the Jazz featured two of the more memorable clutch moments of Jordan's career. He won Game 1 for the Bulls with a buzzer-beating jump shot. In Game 5, with the series tied at 2, Jordan played despite being feverish and dehydrated from a stomach virus. In what is known as the "Flu Game", Jordan scored 38 points, including the game-deciding 3-pointer with 25 seconds remaining. The Bulls won 90–88 and went on to win the series in six games. For the fifth time in as many Finals appearances, Jordan received the Finals MVP award. During the 1997 NBA All-Star Game, Jordan posted the first triple double in All-Star Game history in a victorious effort; however, he did not receive the MVP award.

Jordan and the Bulls compiled a 62–20 record in the 1997–98 season. Jordan led the league with 28.7 points per game, securing his fifth regular-season MVP award, plus honors for All-NBA First Team, First Defensive Team and the All-Star Game MVP. The Bulls won the Eastern Conference Championship for a third straight season, including surviving a seven-game series with the Indiana Pacers in the Eastern Conference Finals; it was the first time Jordan had played in a Game 7 since the 1992 Eastern Conference Semifinals with the Knicks. After winning, they moved on for a rematch with the Jazz in the Finals.

The Bulls returned to the Delta Center for Game 6 on June 14, 1998, leading the series 3–2. Jordan executed a series of plays, considered to be one of the greatest clutch performances in NBA Finals history. With the Bulls trailing 86–83 with 41.9 seconds remaining in the fourth quarter, Phil Jackson called a timeout. When play resumed, Jordan received the inbound pass, drove to the basket, and hit a shot over several Jazz defenders, cutting the Utah lead to 86–85. The Jazz brought the ball upcourt and passed the ball to forward Karl Malone, who was set up in the low post and was being guarded by Rodman. Malone jostled with Rodman and caught the pass, but Jordan cut behind him and took the ball out of his hands for a steal. Jordan then dribbled down the court and paused, eyeing his defender, Jazz guard Bryon Russell. With 10 seconds remaining, Jordan started to dribble right, then crossed over to his left, possibly pushing off Russell, although the officials did not call a foul. With 5.2 seconds left, Jordan gave Chicago an 87–86 lead with a game-winning jumper, the climactic shot of his Bulls career. Afterwards, John Stockton missed a game-winning three-pointer. Jordan and the Bulls won their sixth NBA championship and second three-peat. Once again, Jordan was voted the Finals MVP, having led all scorers averaging 33.5 points per game, including 45 in the deciding Game 6. Jordan's six Finals MVPs is a record; Shaquille O'Neal, Magic Johnson, LeBron James and Tim Duncan are tied for second place with three apiece. The 1998 Finals holds the highest television rating of any Finals series in history. Game 6 also holds the highest television rating of any game in NBA history.

With Phil Jackson's contract expiring, the pending departures of Scottie Pippen and Dennis Rodman looming, and being in the latter stages of an owner-induced lockout of NBA players, Jordan retired for the second time on January 13, 1999. On January 19, 2000, Jordan returned to the NBA not as a player, but as part owner and President of Basketball Operations for the Washington Wizards. Jordan's responsibilities with the Wizards were comprehensive. He controlled all aspects of the Wizards' basketball operations, and had the final say in all personnel matters. Opinions of Jordan as a basketball executive were mixed. He managed to purge the team of several highly paid, unpopular players (such as forward Juwan Howard and point guard Rod Strickland), but used the first pick in the 2001 NBA draft to select high schooler Kwame Brown, who did not live up to expectations and was traded away after four seasons.

Despite his January 1999 claim that he was "99.9% certain" that he would never play another NBA game, in the summer of 2001 Jordan expressed interest in making another comeback, this time with his new team. Inspired by the NHL comeback of his friend Mario Lemieux the previous winter, Jordan spent much of the spring and summer of 2001 in training, holding several invitation-only camps for NBA players in Chicago. In addition, Jordan hired his old Chicago Bulls head coach, Doug Collins, as Washington's coach for the upcoming season, a decision that many saw as foreshadowing another Jordan return.

On September 25, 2001, Jordan announced his return to the NBA to play for the Washington Wizards, indicating his intention to donate his salary as a player to a relief effort for the victims of the September 11, 2001 attacks. In an injury-plagued 2001–02 season, he led the team in scoring (22.9 ppg), assists (5.2 apg), and steals (1.42 spg). However, torn cartilage in his right knee ended Jordan's season after only 60 games, the fewest he had played in a regular season since playing 17 games after returning from his first retirement during the 1994–95 season. Jordan started 53 of his 60 games for the season, averaging 24.3 points, 5.4 assists, and 6.0 rebounds, and shooting 41.9% from the field in his 53 starts. His last seven appearances were in a reserve role, in which he averaged just over 20 minutes per game.

Playing in his 14th and final NBA All-Star Game in 2003, Jordan passed Kareem Abdul-Jabbar as the all-time leading scorer in All-Star Game history (a record since broken by Kobe Bryant). That year, Jordan was the only Washington player to play in all 82 games, starting in 67 of them. He averaged 20.0 points, 6.1 rebounds, 3.8 assists, and 1.5 steals per game. He also shot 45% from the field, and 82% from the free throw line. Even though he turned 40 during the season, he scored 20 or more points 42 times, 30 or more points nine times, and 40 or more points three times. On February 21, 2003, Jordan became the first 40-year-old to tally 43 points in an NBA game. During his stint with the Wizards, all of Jordan's home games at the MCI Center were sold out, and the Wizards were the second most-watched team in the NBA, averaging 20,172 fans a game at home and 19,311 on the road. However, neither of Jordan's final two seasons resulted in a playoff appearance for the Wizards, and Jordan was often unsatisfied with the play of those around him. At several points he openly criticized his teammates to the media, citing their lack of focus and intensity, notably that of the number one draft pick in the 2001 NBA draft, Kwame Brown.

With the recognition that 2002–03 would be Jordan's final season, tributes were paid to him throughout the NBA. In his final game at the United Center in Chicago, which was his old home court, Jordan received a four-minute standing ovation. The Miami Heat retired the number 23 jersey on April 11, 2003, even though Jordan never played for the team. At the 2003 All-Star Game, Jordan was offered a starting spot from Tracy McGrady and Allen Iverson, but refused both. In the end, he accepted the spot of Vince Carter, who decided to give it up under great public pressure.

Jordan played in his final NBA game on April 16, 2003, in Philadelphia. After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter and his team trailing the Philadelphia 76ers, 75–56. Just after the start of the fourth quarter, the First Union Center crowd began chanting "We want Mike!" After much encouragement from coach Doug Collins, Jordan finally rose from the bench and re-entered the game, replacing Larry Hughes with 2:35 remaining. At 1:45, Jordan was intentionally fouled by the 76ers' Eric Snow, and stepped to the line to make both free throws. After the second foul shot, the 76ers in-bounded the ball to rookie John Salmons, who in turn was intentionally fouled by Bobby Simmons one second later, stopping time so that Jordan could return to the bench. Jordan received a three-minute standing ovation from his teammates, his opponents, the officials, and the crowd of 21,257 fans.

Jordan played on two Olympic gold medal-winning American basketball teams. He won a gold medal as a college player in the 1984 Summer Olympics. The team was coached by Bob Knight and featured players such as Patrick Ewing, Sam Perkins, Chris Mullin, Steve Alford, and Wayman Tisdale. Jordan led the team in scoring, averaging 17.1 ppg for the tournament.

In the 1992 Summer Olympics, he was a member of the star-studded squad that included Magic Johnson, Larry Bird, and David Robinson and was dubbed the "Dream Team". Jordan was the only player to start all 8 games in the Olympics. Playing limited minutes due to the frequent blowouts, Jordan averaged 14.9 ppg, finishing second on the team in scoring. Jordan and fellow Dream Team members Patrick Ewing and Chris Mullin are the only American men's basketball players to win Olympic gold medals as amateurs and professionals.

After his third retirement, Jordan assumed that he would be able to return to his front office position as Director of Basketball Operations with the Wizards. However, his previous tenure in the Wizards' front office had produced the aforementioned mixed results and may have also influenced the trade of Richard "Rip" Hamilton for Jerry Stackhouse (although Jordan was not technically Director of Basketball Operations in 2002). On May 7, 2003, Wizards owner Abe Pollin fired Jordan as Washington's President of Basketball Operations. Jordan later stated that he felt betrayed, and that if he had known he would be fired upon retiring he never would have come back to play for the Wizards.

Jordan kept busy over the next few years. He stayed in shape, played golf in celebrity charity tournaments, and spent time with his family in Chicago. He also promoted his Jordan Brand clothing line and rode motorcycles. Since 2004, Jordan has owned Michael Jordan Motorsports, a professional closed-course motorcycle road racing team that competed with two Suzukis in the premier Superbike championship sanctioned by the American Motorcyclist Association (AMA) until the end of the 2013 season. In 2006, Jordan and his wife Juanita pledged $5 million to Chicago's Hales Franciscan High School. The Jordan Brand has made donations to Habitat for Humanity and a Louisiana branch of the Boys & Girls Clubs of America.

On June 15, 2006, Jordan bought a minority stake in the Charlotte Bobcats, becoming the team's second-largest shareholder behind majority owner Robert L. Johnson. As part of the deal, Jordan took full control over the basketball side of the operation, with the title "Managing Member of Basketball Operations." Despite Jordan's previous success as an endorser, he has made an effort not to be included in Charlotte's marketing campaigns. A decade earlier, Jordan had made a bid to become part-owner of Charlotte's original NBA team, the Charlotte Hornets, but talks collapsed when owner George Shinn refused to give Jordan complete control of basketball operations.

In February 2010, it was reported that Jordan was seeking majority ownership of the Bobcats. As February wore on, it became apparent that Jordan and former Houston Rockets president George Postolos were the leading contenders for ownership of the team. On February 27, the Bobcats announced that Johnson had reached an agreement with Jordan and his group, MJ Basketball Holdings, to buy the team pending NBA approval. On March 17, the NBA Board of Governors unanimously approved Jordan's purchase, making him the first former player to become the majority owner of an NBA team. It also made him the league's only African-American majority owner of an NBA team.

During the 2011 NBA lockout, "The New York Times" wrote that Jordan led a group of 10 to 14 hardline owners who wanted to cap the players' share of basketball-related income at 50 percent and as low as 47. Journalists observed that, during the labor dispute in 1998, Jordan had told Washington Wizards then-owner Abe Pollin, "If you can't make a profit, you should sell your team." Jason Whitlock of "FoxSports.com" called Jordan a "sellout" wanting "current players to pay for his incompetence." He cited Jordan's executive decisions to draft disappointing players Kwame Brown and Adam Morrison.

During the 2011–12 NBA season that was shortened to 66 games by the lockout, the Bobcats posted a 7–59 record. Their .106 winning percentage was the worst in NBA history. "I'm not real happy about the record book scenario last year. It's very, very frustrating", Jordan said later that year.

Jordan was a shooting guard who was also capable of playing as a small forward (the position he would primarily play during his second return to professional basketball with the Washington Wizards), and as a point guard. Jordan was known throughout his career for being a strong clutch performer. With the Bulls, he decided 25 games with field goals or free throws in the last 30 seconds, including two NBA Finals games and five other playoff contests. His competitiveness was visible in his prolific trash-talk and well-known work ethic. As the Bulls organization built the franchise around Jordan, management had to trade away players who were not "tough enough" to compete with him in practice. To help improve his defense, he spent extra hours studying film of opponents. On offense, he relied more upon instinct and improvisation at game time. Noted as a durable player, Jordan did not miss four or more games while active for a full season from 1986–87 to 2001–02, when he injured his right knee. He played all 82 games nine times. Jordan has frequently cited David Thompson, Walter Davis, and Jerry West as influences. Confirmed at the start of his career, and possibly later on, Jordan had a special "Love of the Game Clause" written into his contract (unusual at the time) which allowed him to play basketball against anyone at any time, anywhere.

Jordan had a versatile offensive game. He was capable of aggressively driving to the basket, as well as drawing fouls from his opponents at a high rate; his 8,772 free throw attempts are the ninth-highest total of all time. As his career progressed, Jordan also developed the ability to post up his opponents and score with his trademark fadeaway jump shot, using his leaping ability to "fade away" from block attempts. According to Hubie Brown, this move alone made him nearly unstoppable. Despite media criticism as a "selfish" player early in his career, Jordan's 5.3 assists per game also indicate his willingness to defer to his teammates. After shooting under 30% from three-point range in his first five seasons in the NBA, including a career-low 13% in the season, Jordan improved to a career-high 50% in the season. The three-point shot became more of a focus of his game from 1994–95 to 1996–97, when the NBA shortened its three-point line to (from ). His three-point field-goal percentages ranged from 35% to 43% in seasons in which he attempted at least 230 three-pointers between 1989–90 and 1996–97. For a guard, Jordan was also a good rebounder (6.2 per game).

In 1988, Jordan was honored with the NBA's Defensive Player of the Year Award and became the first NBA player to win both the Defensive Player of the Year and MVP awards in a career (since equaled by Hakeem Olajuwon, David Robinson, and Kevin Garnett; Olajuwon is the only player other than Jordan to win both during the same season). In addition, he set both seasonal and career records for blocked shots by a guard, and combined this with his ball-thieving ability to become a standout defensive player. He ranks third in NBA history in total steals with 2,514, trailing John Stockton and Jason Kidd. Jerry West often stated that he was more impressed with Jordan's defensive contributions than his offensive ones. He was also known to have strong eyesight; broadcaster Al Michaels said that he was able to read baseball box scores on a 27-inch television clearly from about 50 feet away.

Jordan's talent was clear from his first NBA season. Bird said that he had "Never seen anyone like him", that he was "One of a kind", and that Jordan was the best player he had ever seen and comparable to Wayne Gretzky as an athlete. In Jordan's first game in Madison Square Garden against the New York Knicks, he received a prolonged standing ovation, a rarity for an opposing player. After Jordan scored a playoff record 63 points against the Boston Celtics on April 20, 1986, Bird described him as "God disguised as Michael Jordan".

Jordan led the NBA in scoring in 10 seasons (NBA record) and tied Wilt Chamberlain's record of seven consecutive scoring titles. He was also a fixture on the NBA All-Defensive First Team, making the roster nine times (NBA record shared with Gary Payton, Kevin Garnett and Kobe Bryant). Jordan also holds the top career regular season and playoff scoring averages of 30.1 and 33.4 points per game, respectively. By 1998, the season of his Finals-winning shot against the Jazz, he was well known throughout the league as a clutch performer. In the regular season, Jordan was the Bulls' primary threat in the final seconds of a close game and in the playoffs; he would always ask for the ball at crunch time. Jordan's total of 5,987 points in the playoffs is the second-highest in NBA history. He retired with 32,292 points in regular season play, placing him fourth on the NBA's all-time scoring list behind Kareem Abdul-Jabbar, Karl Malone, and Kobe Bryant.

With five regular-season MVPs (tied for second place with Bill Russell—only Kareem Abdul-Jabbar has won more, with six), six Finals MVPs (NBA record), and three All-Star Game MVPs, Jordan is the most decorated player in NBA history. Jordan finished among the top three in regular-season MVP voting 10 times, and was named one of the 50 Greatest Players in NBA History in 1996. He is one of only seven players in history to win an NCAA championship, an NBA championship, and an Olympic gold medal (doing so twice with the 1984 and 1992 U.S. men's basketball teams). Since 1976, the year of the NBA's merger with the American Basketball Association, Jordan and Pippen are the only two players to win six NBA Finals playing for one team. In the All-Star Game fan ballot, Jordan received the most votes nine times, more than any other player.
Many of Jordan's contemporaries have said that Jordan is the greatest basketball player of all time. In 1999, an ESPN survey of journalists, athletes and other sports figures ranked Jordan the greatest North American athlete of the 20th century, above such luminaries as Babe Ruth and Muhammad Ali. Jordan placed second to Babe Ruth in the Associated Press's December 1999 list of 20th century athletes. In addition, the Associated Press voted him the greatest basketball player of the 20th century. Jordan has also appeared on the front cover of "Sports Illustrated" a record 50 times. In the September 1996 issue of "Sport", which was the publication's 50th-anniversary issue, Jordan was named the greatest athlete of the past 50 years.

Jordan's athletic leaping ability, highlighted in his back-to-back slam dunk contest championships in 1987 and 1988, is credited by many people with having influenced a generation of young players. Several current NBA players—including LeBron James and Dwyane Wade—have stated that they considered Jordan their role model while they were growing up. In addition, commentators have dubbed a number of next-generation players "the next Michael Jordan" upon their entry to the NBA, including Anfernee "Penny" Hardaway, Grant Hill, Allen Iverson, Kobe Bryant, LeBron James, Vince Carter, and Dwyane Wade. Although Jordan was a well-rounded player, his "Air Jordan" image is also often credited with inadvertently decreasing the jump shooting skills, defense, and fundamentals of young players, a fact Jordan himself has lamented. 

During his heyday, Jordan did much to increase the status of the game, but the popularity of the NBA in the U.S. appears to have declined since his last title. Television ratings in particular increased only during his time in the league, and Finals ratings have not returned to the level reached during his last championship-winning season.
In August 2009, the Basketball Hall of Fame in Springfield, Massachusetts, opened a Michael Jordan exhibit that contained items from his college and NBA careers, as well as from the 1992 "Dream Team". The exhibit also has a batting glove to signify Jordan's short career in minor league baseball. After Jordan received word of his acceptance into the Hall of Fame, he selected Class of 1996 member David Thompson to present him. As Jordan would later explain during his induction speech in September 2009, when he was growing up in North Carolina, he was not a fan of the Tar Heels and greatly admired Thompson, who played at rival North Carolina State. In September, he was inducted into the Hall with several former Bulls teammates in attendance, including Scottie Pippen, Dennis Rodman, Charles Oakley, Ron Harper, Steve Kerr, and Toni Kukoč. Two of Jordan's former coaches, Dean Smith and Doug Collins, were also among those present. His emotional reaction during his speech—when he began to cry—was captured by Associated Press photographer Stephan Savoia and would later go viral on social media as the Crying Jordan Internet meme. In 2016, President Barack Obama honored Jordan with the Presidential Medal of Freedom.

Jordan married Juanita Vanoy in September 1989, and they had two sons, Jeffrey Michael and Marcus James, and a daughter, Jasmine. Jordan and Vanoy filed for divorce on January 4, 2002, citing irreconcilable differences, but reconciled shortly thereafter. They again filed for divorce and were granted a final decree of dissolution of marriage on December 29, 2006, commenting that the decision was made "mutually and amicably". It is reported that Juanita received a $168 million settlement (equivalent to $ million in ), making it the largest celebrity divorce settlement on public record at the time.

In 1991, Jordan purchased a lot in Highland Park, Illinois, to build a 56,000 square foot mansion, which was completed four years later. Jordan listed his Highland Park mansion for sale in 2012. His two sons attended Loyola Academy, a private Roman Catholic high school located in Wilmette, Illinois. Jeffrey graduated as a member of the 2007 graduating class and played his first collegiate basketball game on November 11, 2007, for the University of Illinois. After two seasons, Jeffrey left the Illinois basketball team in 2009. He later rejoined the team for a third season, then received a release to transfer to the University of Central Florida, where Marcus was attending. Marcus transferred to Whitney Young High School after his sophomore year at Loyola Academy and graduated in 2009. He began attending UCF in the fall of 2009, and played three seasons of basketball for the school.

On July 21, 2006, a judge in Cook County, Illinois, determined that Jordan did not owe his alleged former lover Karla Knafel $5 million in a breach of contract claim. Jordan had allegedly paid Knafel $250,000 to keep their relationship a secret. Knafel claimed Jordan promised her $5 million for remaining silent and agreeing not to file a paternity suit after Knafel learned she was pregnant in 1991. A DNA test showed Jordan was not the father of the child.

He proposed to his longtime girlfriend, Cuban-American model Yvette Prieto, on Christmas 2011, and they were married on April 27, 2013, at Bethesda-by-the-Sea Episcopal Church. It was announced on November 30, 2013, that the two were expecting their first child together. On February 11, 2014, Prieto gave birth to identical twin daughters named Victoria and Ysabel.

Jordan is one of the most marketed sports figures in history. He has been a major spokesman for such brands as Nike, Coca-Cola, Chevrolet, Gatorade, McDonald's, Ball Park Franks, Rayovac, Wheaties, Hanes, and MCI. Jordan has had a long relationship with Gatorade, appearing in over 20 commercials for the company since 1991, including the "Be Like Mike" commercials in which a song was sung by children wishing to be like Jordan.

Nike created a signature shoe for him, called the "Air Jordan". One of Jordan's more popular commercials for the shoe involved Spike Lee playing the part of Mars Blackmon. In the commercials Lee, as Blackmon, attempted to find the source of Jordan's abilities and became convinced that "it's gotta be the shoes". The hype and demand for the shoes even brought on a spate of "shoe-jackings" where people were robbed of their sneakers at gunpoint. Subsequently, Nike spun off the Jordan line into its own division named the "Jordan Brand". The company features an impressive list of athletes and celebrities as endorsers. The brand has also sponsored college sports programs such as those of North Carolina, Cal, Georgetown, and Marquette.

Jordan also has been associated with the Looney Tunes cartoon characters. A Nike commercial shown during 1992's Super Bowl XXVI featured Jordan and Bugs Bunny playing basketball. The Super Bowl commercial inspired the 1996 live action/animated film "Space Jam", which starred Jordan and Bugs in a fictional story set during the former's first retirement from basketball. They have subsequently appeared together in several commercials for MCI. Jordan also made an appearance in the music video of Michael Jackson's "Jam" (1992).

Jordan's yearly income from the endorsements is estimated to be over forty million dollars. In addition, when Jordan's power at the ticket gates was at its highest point, the Bulls regularly sold out both their home and road games. Due to this, Jordan set records in player salary by signing annual contracts worth in excess of US $30 million per season. An academic study found that Jordan's first NBA comeback resulted in an increase in the market capitalization of his client firms of more than $1 billion.

Most of Jordan's endorsement deals, including his first deal with Nike, were engineered by his agent, David Falk. Jordan has described Falk as "the best at what he does" and that "marketing-wise, he's great. He's the one who came up with the concept of 'Air Jordan.'"

In June 2010, Jordan was ranked by "Forbes" magazine as the 20th-most powerful celebrity in the world with $55 million earned between June 2009 and June 2010. According to the Forbes article, Jordan Brand generates $1 billion in sales for Nike. In June 2014, Jordan was named the first NBA player to become a billionaire, after he increased his stake in the Charlotte Hornets from 80% to 89.5%. On January 20, 2015, Jordan was honored with the "Charlotte Business Journal"'s Business Person of the Year for 2014. In 2017, he became a part owner of the Miami Marlins of Major League Baseball.

"Forbes" designated Jordan as the athlete with the highest career earnings in 2017. From his Jordan Brand income and endorsements, Jordan's 2015 income was an estimated $110 million, the most of any retired athlete. As of February 2018, his current net worth is estimated at $1.65 billion by "Forbes". Jordan is the third-richest African-American as of 2018, behind Robert F. Smith and Oprah Winfrey.

Jordan co-owns an automotive group which bears his name. The company has a Nissan dealership in Durham, North Carolina, acquired in 1990, and formerly had a Lincoln–Mercury dealership from 1995 until its closure in June 2009. The company also owned a Nissan franchise in Glen Burnie, Maryland. The restaurant industry is another business interest of Jordan's. His restaurants include a steakhouse in New York City's Grand Central Terminal, among others.

Sources:






</doc>
<doc id="20458" url="https://en.wikipedia.org/wiki?curid=20458" title="Musicology">
Musicology

Musicology () is the scholarly analysis and research-based study of music. Musicology is part of the humanities. A scholar who participates in musical research is a musicologist.

Traditionally, historical musicology (commonly termed "music history") has been the most prominent sub-discipline of musicology. In the 2010s, historical musicology is one of several large musicology sub-disciplines. Historical musicology, ethnomusicology, and systematic musicology are approximately equal in size. Ethnomusicology is the study of music in its cultural context. Systematic musicology includes music acoustics, the science and technology of acoustical musical instruments, and the musical implications of physiology, psychology, sociology, philosophy and computing. Cognitive musicology is the set of phenomena surrounding the computational modeling of music. In some countries, music education is a prominent sub-field of musicology, while in others it is regarded as a distinct academic field, or one more closely affiliated with teacher education, educational research, and related fields. Like music education, music therapy is a specialized form of applied musicology which is sometimes considered more closely affiliated with health fields, and other times regarded as part of musicology proper.

The parent disciplines of musicology include:

Musicology also has two central, practically oriented sub-disciplines with no parent discipline: performance practice and research (sometimes viewed as a form of artistic research), and the theory, analysis and composition of music. The disciplinary neighbors of musicology address other forms of art, performance, ritual and communication, including the history and theory of the visual and plastic arts and of architecture; linguistics, literature and theater; religion and theology; and sport. Musical knowledge is applied in medicine, education, and music therapy—which, effectively, are parent disciplines of applied musicology.

Music history or historical musicology is concerned with the composition, performance, reception, and criticism of music over time. Historical studies of music are for example concerned with a composer's life and works, the developments of styles and genres, "e.g.", baroque concertos, the social function of music for a particular group of people, "e.g.", court music, or modes of performance at a particular place and time, "e.g.", Johann Sebastian Bach's choir in Leipzig. Like the comparable field of art history, different branches and schools of historical musicology emphasize different types of musical works and approaches to music. There are also national differences in various definitions of historical musicology. In theory, "music history" could refer to the study of the history of any type or genre of music, "e.g.", the history of Indian music or the history of rock. In practice, these research topics are more often considered within ethnomusicology (see below) and "historical musicology" is typically assumed to imply Western Art music of the European tradition.

The methods of historical musicology include source studies (especially manuscript studies), paleography, philology (especially textual criticism), style criticism, historiography (the choice of historical method), musical analysis (analysis of music to find "inner coherence"), and iconography. The application of musical analysis to further these goals is often a part of music history, though pure analysis or the development of new tools of music analysis is more likely to be seen in the field of music theory. Music historians create a number of written products, ranging from journal articles describing their current research, new editions of musical works, biographies of composers and other musicians, book-length studies or university textbook chapters or entire textbooks. Music historians may examine issues in a close focus, as in the case of scholars who examine the relationship between words and music for a given composer's art songs. On the other hand, some scholars take a broader view, and assess the place of a given type of music, such as the symphony in society using techniques drawn from other fields, such as economics, sociology, or philosophy.

"New musicology" is a term applied since the late 1980s to a wide body of work emphasizing cultural study, analysis, and criticism of music. Such work may be based on feminist, gender studies, queer theory, or postcolonial theory, or the work of Theodor W. Adorno. Although New Musicology emerged from within historical musicology, the emphasis on cultural study within the Western art music tradition places New Musicology at the junction between historical, ethnological and sociological research in music.

New musicology was a reaction against traditional historical musicology, which according to Susan McClary, "fastidiously declares issues of musical signification off-limits to those engaged in legitimate scholarship." Charles Rosen, however, retorts that McClary, "sets up, like so many of the 'new musicologists', a straw man to knock down, the dogma that music has no meaning, and no political or social significance." Today, many musicologists no longer distinguish between musicology and new musicology, since many of the scholarly concerns once associated with new musicology have now become mainstream, and they feel the term "new" no longer applies.

Ethnomusicology, formerly comparative musicology, is the study of music in its cultural context. It is often considered the anthropology or ethnography of music. Jeff Todd Titon has called it the study of "people making music". Although it is most often concerned with the study of non-Western musics, it also includes the study of Western music from an anthropological or sociological perspective, cultural studies and sociology as well as other disciplines in the social sciences and humanities. Some ethnomusicologists primarily conduct historical studies, but the majority are involved in long-term participant observation, or combine ethnographic and historical approaches in their fieldwork. Therefore, ethnomusiological scholarship can be characterized as featuring a substantial, intensive fieldwork component, often involving long-term residence within the community studied.
Closely related to ethnomusiology is the emerging branch of sociomusicology. For instance, Ko (2011) proposed the hypothesis of "Biliterate and Trimusical" in Hong Kong sociomusicology.

Popular music studies, known, "misleadingly," as "popular musicology", emerged in the 1980s as an increasing number of musicologists, ethnomusicologists, and other varieties of historians of American and European culture began to write about popular musics past and present. The first journal focusing on popular music studies was Popular Music, which began publication in 1981. The same year an academic society solely devoted to the topic was formed, the International Association for the Study of Popular Music. The Association's founding was partly motivated by the interdisciplinary agenda of popular musicology though the group has been characterized by a polarized 'musicological' and 'sociological' approach also typical of popular musicology.

Music theory is a field of study that describes the elements of music and includes the development and application of methods for composing and for analyzing music through both notation and, on occasion, musical sound itself. Broadly, theory may include any statement, belief, or conception of or about music (Boretz, 1995). A person who studies or practices music theory is a music theorist.

Some music theorists attempt to explain the techniques composers use by establishing rules and patterns. Others model the experience of listening to or performing music. Though extremely diverse in their interests and commitments, many Western music theorists are united in their belief that the acts of composing, performing, and listening to music may be explicated to a high degree of detail (this, as opposed to a conception of musical expression as fundamentally ineffable except in musical sounds). Generally, works of music theory are both descriptive and prescriptive, attempting both to define practice and to influence later practice. Thus, music theory generally lags behind practice but also points towards future exploration, composition, and performance.

Musicians study music theory to understand the structural relationships in the (nearly always notated) music. Composers study music theory to understand how to produce effects and structure their own works. Composers may study music theory to guide their precompositional and compositional decisions. Broadly speaking, music theory in the Western tradition focuses on harmony and counterpoint, and then uses these to explain large scale structure and the creation of melody.

Music psychology applies the content and methods of all subdisciplines of psychology (perception, cognition, motivation, etc.) to understand how music is created, perceived, responded to, and incorporated into individuals' and societies' daily lives. Its primary branches include cognitive musicology, which emphasizes the use of computational models for human musical abilities and cognition, and the cognitive neuroscience of music, which studies the way that music perception and production manifests in the brain using the methodologies of cognitive neuroscience. While aspects of the field can be highly theoretical, much of modern music psychology seeks to optimize the practices and professions of music performance, composition, education, and therapy.

Performance practice draws on many of the tools of historical musicology to answer the specific question of how music was performed in various places at various times in the past. Although previously confined to early music, recent research in performance practice has embraced questions such as how the early history of recording affected the use of vibrato in classical music, or instruments in Klezmer.

Within the rubric of musicology, performance practice tends to emphasize the collection and synthesis of evidence about how music should be performed. The important other side, learning how to sing authentically or perform a historical instrument is usually part of conservatory or other performance training. However, many top researchers in performance practice are also excellent musicians.

Music performance research (or music performance science) is strongly associated with music psychology. It aims to document and explain the psychological, physiological, sociological and cultural details of how music is actually performed (rather than how it should be performed). The approach to research tends to be systematic and empirical, and to involve the collection and analysis of both quantitative and qualitative data. The findings of music performance research can often be applied in music education.

Musicologists in tenure track professor positions typically hold a Ph.D in musicology. In the 1960s and 1970s, some musicologists obtained professor positions with an M.A. as their highest degree, but in the 2010s, the Ph.D is the standard minimum credential for tenure track professor positions. As part of their initial training, musicologists typically complete a B.Mus or a B.A. in music (or a related field such as history) and in many cases an M.A. in musicology. Some individuals apply directly from a bachelor's degree to a Ph.D, and in these cases, they may not receive an M.A. In the 2010s, given the increasingly interdisciplinary nature of university graduate programs, some applicants for musicology Ph.D programs may have academic training both in music and outside of music (e.g., a student may apply with a B.Mus and an M.A. in psychology). In music education, individuals may hold an M.Ed and an Ed.D.

Most musicologists work as instructors, lecturers or professors in colleges, universities or conservatories. The job market for tenure track professor positions is very competitive. Entry-level applicants must hold a completed Ph.D or the equivalent degree and applicants to more senior professor positions must have a strong record of publishing in peer-reviewed journals. Some Ph.D-holding musicologists are only able to find insecure positions as sessional lecturers. The job tasks of a musicologist are the same as those of a professor in any other humanities discipline: she teaches undergraduate and/or graduate classes in her area of specialization and, in many cases some general courses (such as Music Appreciation or Introduction to Music History), conducts research in her area of expertise, publishes articles about her research in peer-reviewed journals, authors book chapters, books or textbooks, travels to conferences to give talks on her research and learn about research in her field, and, if her program includes a graduate school, supervises M.A. and Ph.D students and gives them guidance on the preparation of their theses and dissertations. Some musicology professors may take on senior administrative positions in their institution, such as Dean or Chair of the School of Music.

The vast majority of major musicologists and music historians from past generations have been men, as in the 19th century and early 20th century; women's involvement in teaching music was mainly in elementary and secondary music teaching. Nevertheless, some women musicologists have reached the top ranks of the profession. Carolyn Abbate (born 1956) is an American musicologist who did her PhD at Princeton University. She has been described by the "Harvard Gazette" as "one of the world's most accomplished and admired music historians". 

Susan McClary (born 1946) is a musicologist associated with the "New Musicology" who incorporates feminist music criticism in her work. McClary holds a PhD from Harvard University. One of her best known works is "Feminine Endings" (1991), which covers musical constructions of gender and sexuality, gendered aspects of traditional music theory, gendered sexuality in musical narrative, music as a gendered discourse and issues affecting women musicians. In the book, McClary suggests that the sonata form (used in symphonies and string quartets) may be a sexist or misogynistic procedure that constructs gender and sexual identity. McClary's "Conventional Wisdom" (2000) argues that the traditional musicological assumption of the existence of "purely musical" elements, divorced from culture and meaning, the social and the body, is a conceit used to veil the social and political imperatives of the worldview that produces the classical canon most prized by supposedly objective musicologists.

Other notable women scholars include:



Although many musicology journals are not available on-line, or are only available through pay-for-access portals, a sampling of peer reviewed journals in various subfields gives some idea of musicological writings:

The following musicology journals can be accessed on-line through JSTOR (requires subscription for full access). Many of them have their latest issues available on-line via publisher portals (usually requiring a fee for access).



</doc>
<doc id="20460" url="https://en.wikipedia.org/wiki?curid=20460" title="Film promotion">
Film promotion

Film promotion is the practice of promotion specifically in the film industry, and usually occurs in coordination with the process of film distribution. Sometimes called the press junket or film junket, film promotion generally includes press releases, advertising campaigns, merchandising, franchising, media and interviews with the key people involved with the making of the film, like actors and directors. As with all business, it is an important part of any release because of the inherent high financial risk; film studios will invest in expensive marketing campaigns to maximize revenue early in the release cycle. Marketing budgets tend to equal about half the production budget. Publicity is generally handled by the distributor and exhibitors.






Film actors, directors, and producers appear for television, cable, radio, print, and online media interviews, which can be conducted in person or remotely. During film production, these can take place "on set". After the film's premiere, key personnel make appearances in major market cities or participate remotely via satellite videoconference or telephone. The purpose of interviews is to encourage journalists to publish stories about their "exclusive interviews" with the film's stars, thereby creating "marketing buzz" around the film and stimulating audience interest in watching the film.

When it comes to feature films picked up by a major film studio for international distribution, promotional tours are notoriously grueling. Key cast and crew are often contracted to travel to several major cities around the world to promote the film and sit for dozens of interviews. In every interview they are supposed to stay "on message" by energetically expressing their enthusiasm for the film in a way that appears candid, fun, and fresh, even though it may be their fifth or sixth interview that day. They are expected to disclose just enough juicy "behind-the-scenes" information about the filmmaking process or the filmmakers' artistic vision to make each journalist feel like he or she got a nice scoop, while at the same time tactfully avoiding disclosure of anything truly negative or embarrassing.

There are seven distinct types of research conducted by film distributors in connection with domestic theatrical releases, according to "Marketing to Moviegoers: Second Edition." Such audience research can cost $1 million per film, especially when scores of TV advertisements are tested and re-tested. The bulk of research is done by major studios for the roughly 170 major releases they mount each year that are supported by tens of millions of advertising buys for each film. Independent film distributors, which typically spend less than $10 million in media buys per film, don’t have the budget or breadth of advertising materials to analyze, so they spend little or nothing on pre-release audience research.
When audience research is conducted for domestic theatrical release, it involves these areas:

Marketing can play a big role in whether or not a film gets the green light. Audience research is a strong factor in determining the ability of a film to sell in theaters, which is ultimately how films make their money. As part of a movie's Marketing strategy, audience research comes into account as producers create promotional materials. These promotional materials consistently change and evolve as a direct consequence of audience research up until the film opens in theaters.


</doc>
<doc id="20463" url="https://en.wikipedia.org/wiki?curid=20463" title="Miltiades (disambiguation)">
Miltiades (disambiguation)

Miltiades the Younger (c. 550 – 489 BC) was tyrant of the Thracian Chersonese and the Athenian commanding general in the Battle of Marathon.

Miltiades may also refer to:




</doc>
<doc id="20468" url="https://en.wikipedia.org/wiki?curid=20468" title="Maggie Out">
Maggie Out

"Maggie Out" was a chant popular during the Miners' Strike, student grant protests, Poll Tax protests and other public demonstrations that fell within the time when Margaret Thatcher was the Prime Minister of the United Kingdom.

The chant called for her to be removed from that role. It was referred to, in that context, during a parliamentary session in 1984.

When Margaret Thatcher felt compelled to resign some people had memories of chanting it for thirteen years. People were passionate about this group activity and associated it with varied political struggles from that time.

It is a variant of the "Oggy Oggy Oggy, Oi Oi Oi" chant. When used in that format, the lyrics were:

<poem>
Maggie, Maggie, Maggie!
Out! Out! Out!

Maggie, Maggie, Maggie!
Out! Out! Out!

Maggie!
Out!

Maggie!
Out!

Maggie, Maggie, Maggie!
Out! Out! Out!
</poem>

The chorus of the chant became the title of a compilation album from Anagram Records (Catalog#:GRAM 28) released in 1987.

The Larks produced a track called "Maggie, Maggie, Maggie (Out, Out, Out)" which was included on the Miners' Benefit LP "Here We Go" on Sterile Records.

Comedian Alexei Sayle remarked humorously that he couldn't find his way around London unless he walked down the middle of the streets shouting the words.

Since 1990 two variants of this song have been heard - adapted for both her successors; replacing 'Major' for 'Maggie' during the tenure of John Major and 'Tony' for 'Maggie' since Tony Blair's plan for the Iraq War in 2003.

The song was occasionally revived to "greet" Thatcher's public outings following her resignation as Prime Minister, but with the word "gone" substituted for "out".

Following the death of Thatcher on 8 April 2013, this chant was revived in the format of "Maggie, Maggie Maggie (Dead, Dead, Dead)" at celebratory parties held in Glasgow, London and Reading.


</doc>
<doc id="20469" url="https://en.wikipedia.org/wiki?curid=20469" title="M25 motorway">
M25 motorway

The M25 or London Orbital Motorway is a motorway that encircles almost all of Greater London, England (with the exception of North Ockendon), in the United Kingdom. An ambitious concept to build four concentric ring roads around London was first mooted in the 1960s. A few sections of the outer two rings were constructed in the early 1970s, but the plan was abandoned and the sections were later integrated to form a single ring which became the M25, aka London Ring Road, finally completed in 1986.

It is one of the busiest of the British motorway network: 196,000 vehicles were recorded on a busy day near Heathrow Airport in 2003 and the western half experienced an average daily flow of 147,000 vehicles in 2007.

The M25, plus the short non-motorway A282 which joins the two ends of the M25 across the River Thames using the Dartford Crossing, is Europe's second longest orbital road after the Berliner Ring, which is .

Originally built almost wholly as a dual three-lane motorway, much of the motorway has been widened: to dual four lanes for almost half, to a dual five-lanes section between junctions 12 and 14 and a dual six-lane section between junctions 14 and 15. Further widening is in progress of minor sections with plans for managed motorways in many others.

To the east of London the two ends of the M25 are joined to complete a loop by the non-motorway A282 Dartford Crossing of the River Thames between Thurrock and Dartford. This crossing, which consists of twin two-lane tunnels and the four-lane QE2 (Queen Elizabeth II) bridge, is named "Canterbury Way". Passage across the bridge or through the tunnels is subject to a toll, its level depending on the kind of vehicle. This stretch being non-motorway, it allows traffic, including that not permitted to use motorways, to cross the River Thames east of the Woolwich Ferry; the only crossing further to the east is a passenger ferry between Gravesend, in Kent, and Tilbury, in Essex. However, in 2017 Highways England published plans to build another motorway-grade Thames tunnel to the east of Gravesend and Grays, the Lower Thames Crossing, in order to relieve congestion on the A282 Dartford Crossing and connect the M25 at North Ockendon in Essex with the M2 in Kent.

At Junction 5, the clockwise carriageway of the M25 is routed off the main north–south dual carriageway onto the main east–west dual carriageway with the main north–south carriageway becoming the A21. In the opposite direction, to the east of the point where the M25 diverges from the main east–west carriageway, that carriageway become the M26 motorway.

The radial distance from London (taken as Charing Cross) varies from in Potters Bar to in Byfleet. Three Greater London boroughs (Enfield, Hillingdon and Havering) have realigned their boundaries to the M25 for minor stretches; while in others, most notably in Essex and Surrey, the radial gap between Greater London and the motorway reaches , neither of which coincide with the Metropolitan Green Belt. Major towns listed as destinations (right), in various counties, adjoin the M25. North Ockendon is the only settlement of Greater London situated outside the M25. In 2004, following an opinion poll, the London Assembly mooted for consultation alignment of the Greater London boundary with the M25. "Inside the M25" and "outside/beyond the M25" are colloquial, looser alternatives to "Greater London" sometimes used in haulage. The Communications Act 2003 explicitly uses the M25 as the boundary in requiring a proportion of television programmes to be made outside the London area.

Two motorway service areas are on the M25, and two others are directly accessible from it. Those on the M25 are Clacket Lane between junctions 5 and 6 (in the south-east) and Cobham between junctions 9 and 10 (in the south-west). Those directly accessible from it are South Mimms off junction 23 (to the north of London) and Thurrock off junction 31 (to the east of London). Cobham services opened on 13 September 2012.

Originally, the M25 was unlit except for sections around Heathrow, major interchanges and Junctions 23–30. Originally, low pressure sodium (SOX) lighting was the most prominent technology used, but widening projects from the 1990s onwards have all used high-pressure sodium (SON) lighting and this has diminished the original installations. By 2014 only one significant stretch was still SOX-lit (Junction 25–26) and the units were removed the same year.

The motorway passes through five counties. Junctions 1A–5 are in Kent, 6–14 are in Surrey, 15–16 are in Buckinghamshire, 17–25 are in Hertfordshire, and 26–31 are in Essex. Policing of the road is carried out by an integrated policing group made up of the Metropolitan, Thames Valley, Essex, Kent, Hertfordshire and Surrey forces.

The M25 is one of Europe's busiest motorways. In 2003, a maximum of 196,000 vehicles a day were recorded on the motorway just south of London Heathrow Airport between junctions 13 and 14.

The idea of an orbital road around London was first proposed early in the 20th century. An outer orbital road around London had first been proposed in 1913, and was re-examined as a motorway route in Sir Charles Bressey's and Sir Edwin Lutyens' "The Highway Development Survey, 1937". Sir Patrick Abercrombie's "County of London Plan, 1943" and "Greater London Plan, 1944" proposed a series of five roads encircling the capital. The northern sections of the M25 follow a similar route to the World War II Outer London Defence Ring, a concentric series of tanks and pillboxes designed to slow down a potential German invasion of the capital.

Little was done to progress these plans until the 1960s when the Greater London Council developed its London Ringways plan consisting of four "rings" around the capital. Sections of the two outer rings – Ringway 3 (the 'M16 motorway') and Ringway 4 – were constructed in the early 1970s and were integrated into the single M25 orbital motorway. But the Ringways plan was hugely controversial owing to the destruction required for the inner two ring roads, (Ringway 1 and Ringway 2). Parts of Ringway 1 were constructed (including West Cross Route), against stiff opposition, before the overall plan was abandoned in 1973 following pressure from residents in the threatened areas.

Construction of parts of the two outer ring roads, Ringways 3 and 4, began in 1973. The first section, between South Mimms and Potters Bar in Hertfordshire (junction 23 to junction 24) opened in September 1975 and was given the temporary general purpose road designation A1178 (a section of motorway-standard-road, originally the M16, which eventually was incorporated into the M25) was completed and operational before this. A Watford-avoiding route between the M1 and the A40 between north Watford and Denham was locally known as the Croxley Green/Rickmansworth bypass, and was operational about 1973/4; a section south of London (junction 6 to junction 8) opened in 1976. A section of Ringway 3 south of the river between Dartford and Swanley (junction 1 to junction 3) was constructed between 1974 and 1977. In 1975 the plans for Ringway 3 were modified to combine it with Ringway 4, the outermost Ringway. The M25 as a component of ringway 4, was first conceived to be an east-west road south of London to relieve the A25, and running parallel to it, with its eastern end following the route of what is now the M26. However, it was subsequently routed northwards towards the Dartford Tunnel to form, in conjunction with similar roads, including the M16 planned to the north of London, part of the London Orbital. The combined motorway was given the designation M25 which had originally been intended for the southern and western part of Ringway 4 and the M16 designation was dropped. The section of Ringway 3 west of South Mimms anti-clockwise around London to Swanley in Kent was cancelled. The stages were not constructed contiguously but in small sections. As the orbital road developed the sections were linked. Each section was presented to planning authorities in its own right and was individually justified, with almost 40 public inquiries relating to sections of the route. Maps at this time depicting these short sections named the route as the M16 but this changed before completion.

The section from Potters Bar to the Dartford Tunnel was constructed between 1979 and 1982. Construction of the M25 continued in stages until its completion in 1986. Prime Minister Margaret Thatcher officially opened the M25 on 29 October 1986, with a ceremony in the section between J22 and J23 (London Colney and South Mimms). The initial tenders for the construction of the M25 totalled £631.9 million. This did not include compulsory purchase of land and subsequent upgrades and repairs.

The ring road around London.

Soon after the motorway opened in 1986 traffic levels exceeded the maximum design capacity and in 1990 the Secretary of State for Transport announced plans to widen the whole of the M25 to four lanes. By 1993 the motorway, which was designed for a maximum of 88,000 vehicles per day, was carrying 200,000 vehicles per day. 15% of UK motorway traffic volume was on the M25 and there were plans to add six lanes to the section from Junctions 12 to 15 as well as widening the rest of the motorway to four lanes.

In parts, particularly the western third this plan went ahead, due to consistent congestion. Again, however, plans to widen further sections to eight lanes (four each way) were scaled back in 2009 in response to rising costs. The plans were reinstated in the agreed Highways Agency 2013-14 business plan.

In 1995 a contract was awarded to widen the section between Junctions 8 and 10 from six to eight lanes for a cost of £93.4 million and a Motorway Incident Detection and Automatic Signalling (MIDAS) system was introduced to the M25 from Junction 10 to Junction 15 at a cost of £13.5m in 1995. This was then extended to Junction 16 at a cost of £11.7m in 2002. This consists of a distributed network of traffic and weather sensors, speed cameras and variable-speed signs that control traffic speeds with little human supervision, and has improved traffic flow slightly, reducing the amount of start-stop driving.
In 1995 there was a proposal to widen the section close to Heathrow Airport to fourteen lanes. This attracted fierce opposition from road protesters opposing the Newbury Bypass and other schemes and it was cancelled shortly afterwards. In 1997, however, the Department of Transport announced new proposals to widen the section between Junction 12 (M3) and Junction 15 (M4) to twelve lanes. At the Terminal Five public inquiry a Highways Agency official said that the widening was needed to accommodate traffic to the proposed new terminal, however the transport minister said that no such evidence had been given. Environmental groups objected to the decision to go ahead with a scheme that would create the widest motorways in the UK without holding a public inquiry. The decision was again deferred. A decision to go-ahead was given for a ten-lane scheme in 1998 and the £148 million 'M25 Jct 12 to 15 Widening' contract was awarded to Balfour Beatty in 2003. The scheme was completed in 2005 as dual-five lanes between Junctions 12 and 14 and dual-six lanes from Junctions 14 to 15.
In 2007 capacity at Junction 25 (A10/Waltham Cross) was increased and the Holmesdale Tunnel was widened to three lanes in an easterly direction at a cost of £75 million.

Work to widen the exit slip-roads in both directions at Junction 28 (A12 road/A1023) was completed in 2008. It was designed to reduce the amount of traffic queueing on the slip roads at busy periods, particularly traffic from the clockwise M25 joining the northbound A12 where the queue extended onto the inside lane of the Motorway.

In 2006 the Highways Agency proposed to widen of M25 from six to eight lanes, between junctions 5–6 and 16–30 as part of a Design, Build, Finance and Operate (DBFO) project. A shortlist of contractors was announced in October 2006 for the project which was expected to cost £4.5 billion. Contractors were asked to resubmit their bids in January 2008 and in June 2009 the new transport minister indicated that the cost had risen to £5.5 billion and the benefit to cost ratio had dropped considerably. In January 2009 the government announced that plans to widen the sections from Junction 5–7 and from 23–27 had been 'scrapped' and that hard shoulder running would be introduced instead. However widening was reinstated to four lanes in the 2013–14 Highways Agency Business Plan.

In 2009 a £6.2 billion M25 DBFO private finance initiative contract was awarded to Connect Plus to widen the sections between junctions 16 and 23 and between junctions 27 and 30 and maintain the M25 and the Dartford Crossing for a 30-year period.
Works to widen the section between Junctions 16 (M40) and 23 (A1(M)) to dual four lanes started in July 2009 at an estimated cost of £580 million. The Junction 16 to 21 (M1) section was completed by July 2011 and the Junction 21 to 23 by June 2012. Works to widen the Junctions 27 (M11) to 30 (A13) section to dual four lanes also started in July 2009. The Junction 27 to 28 (A12) section was completed in July 2010, the Junction 28 to 29 (A127) in June 2011 and finally the Junction 29 to 30 (A13) section opened in May 2012.

Works to introduce managed motorway technology and permanent hard shoulder running on two sections of the M25 began in 2013. The first section between Junctions 5 (A21/M26) and 7 (M23) started construction in May 2013 with the scheme being completed and opened in April 2014. The second section, between Junctions 23 (A1/A1(M)) and 27 (M11), began construction in February 2013 and was completed and opened in November 2014.

In December 2016 Highways England completed the capacity project at Junction 30 (Thurrock) as part of the Thames Gateway Delivery Plan.

The improved junction is said to facilitate billions of pounds of investment in the region, making journeys more reliable and improving safety. In addition, the A13 through the junction has been widened to four lanes in each direction with speed limits capped to 50 mph. New dedicated link roads created and existing slip roads improved to facilitate east bound migration to the Regional Shopping Centre (Lakeside). Drainage, safety barriers and lighting on the M25 have also been upgraded as part of the improvements around Junction 30 and 31 including new electronic gantry signage.

In 2009 the Department for Transport published options for a new Lower Thames Crossing to add capacity to the Dartford Crossing or create a new road and crossing linking to the M2 and M20 motorways.

The M25 is the second-longest ring road in Europe, after the Berlin Ring (A 10), which is longer.

Other cities in the UK encircled by motorways include: Birmingham, using parts of the M5, M6 and M42, and Manchester, using the M60. Additionally, from 2011 Glasgow has an orbital motorway made of the M8, M73 and M74, although one section of the route passes through the centre of the city.

The M25 is one of the busiest motorways in Europe. Here are some comparisons:


Iain Sinclair's 2002 book and film "London Orbital" is based on a year-long journey around the M25 on foot.

The M25 (including the A282 Dartford Crossing) is known for its frequent traffic jams. These have been the subject of so much comment from such an early stage that even at the official opening ceremony Margaret Thatcher complained about "those who carp and criticise". The jams have inspired jokes (e.g., "the world's first circular car park", "the London Orbital Car Park") and songs (e.g., Chris Rea's "The Road to Hell").

The M25 plays a role in the comedy-fantasy novel "Good Omens", as "evidence for the hidden hand of Satan in the affairs of Man". The demon character, Crowley, had manipulated the design of the M25 to resemble a Satanic sigil, and tried to ensure it would anger as many people as possible to drive them off the path of good.

The M25 enjoyed a more positive reputation among ravers in the late 1980s, when this new orbital motorway became a popular route to the parties that took place around the outskirts of London. This use of the M25 for these raves inspired the name of electronic duo Orbital.

The orbital nature of the motorway, in common with racetracks, lent itself to unofficial, and illegal, motor racing. At the end of the 1980s, before the advent of speed enforcement devices, owners of supercars would meet at night at service stations such as South Mimms and conduct time trials. Times below 1 hour were achieved - an average speed of over 117 mph (188 km/h), which included coming to a halt at the Dartford Tunnel road user charge payment booths.

Data from driver location signs provide carriageway identifier information. The numbers on the signs are kilometres from a point near the River Thames, east of London, when travelling clockwise on the motorway. The table below gives details of each junction, including the roads interchanged and the destinations that are signed from the motorway on the blue advance direction signs. Figures in kilometres are from the driver location signs; figures in miles are derived from them.

!scope=col| miles
!scope=col| km
!scope=col| Clockwise exits (A carriageway)
!scope=col| Junction
!scope=col| Anti-clockwise exits (B carriageway)
!scope=col| European Route

Citations
Sources



</doc>
<doc id="20474" url="https://en.wikipedia.org/wiki?curid=20474" title="Mohs scale of mineral hardness">
Mohs scale of mineral hardness

The Mohs scale of mineral hardness is a qualitative ordinal scale characterizing scratch resistance of various minerals through the ability of harder material to scratch softer material. Created in 1812 by German geologist and mineralogist Friedrich Mohs, it is one of several definitions of hardness in materials science, some of which are more quantitative. The method of comparing hardness by seeing which minerals can visibly scratch others is, however, of great antiquity, having been mentioned by Theophrastus in his treatise "On Stones", c. 300 BC, followed by Pliny the Elder in his "Naturalis Historia", c. 77 AD. While greatly facilitating the identification of minerals in the field, the Mohs scale does not show how well hard materials perform in an industrial setting.

Despite its lack of precision, the Mohs scale is highly relevant for field geologists, who use the scale to roughly identify minerals using scratch kits. The Mohs scale hardness of minerals can be commonly found in reference sheets. 

Mohs hardness is useful in milling. It allows assessment of which kind of mill will best reduce a given product whose hardness is known. The scale is also in use at digital electronic manufacturers, for testing the resilience of certain flat panel display components.(like the cover glass for LCDs or encapsulation for OLEDs)

The Mohs scale of mineral hardness is based on the ability of one natural sample of mineral to scratch another mineral visibly. The samples of matter used by Mohs are all different minerals. Minerals are chemically pure solids found in nature. Rocks are made up of one or more minerals. As the hardest known naturally occurring substance when the scale was designed, diamonds are at the top of the scale. The hardness of a material is measured against the scale by finding the hardest material that the given material can scratch, or the softest material that can scratch the given material. For example, if some material is scratched by apatite but not by fluorite, its hardness on the Mohs scale would fall between 4 and 5. "Scratching" a material for the purposes of the Mohs scale means creating non-elastic dislocations visible to the naked eye. Frequently, materials that are lower on the Mohs scale can create microscopic, non-elastic dislocations on materials that have a higher Mohs number. While these microscopic dislocations are permanent and sometimes detrimental to the harder material's structural integrity, they are not considered "scratches" for the determination of a Mohs scale number.

The Mohs scale is a purely ordinal scale. For example, corundum (9) is twice as hard as topaz (8), but diamond (10) is four times as hard as corundum. The table below shows the comparison with the absolute hardness measured by a sclerometer, with pictorial examples.

On the Mohs scale, a streak plate (unglazed porcelain) has a hardness of approximately 7.0. Using these ordinary materials of known hardness can be a simple way to approximate the position of a mineral on the scale.

The table below incorporates additional substances that may fall between levels:

Comparison between hardness (Mohs) and hardness (Vickers):



</doc>
<doc id="20476" url="https://en.wikipedia.org/wiki?curid=20476" title="Murray Gell-Mann">
Murray Gell-Mann

Murray Gell-Mann (; born September 15, 1929) is an American physicist who received the 1969 Nobel Prize in physics for his work on the theory of elementary particles. He is the Robert Andrews Millikan Professor of Theoretical Physics Emeritus at the California Institute of Technology, a distinguished fellow and co-founder of the Santa Fe Institute, a professor of physics at the University of New Mexico, and the Presidential Professor of Physics and Medicine at the University of Southern California.

Gell-Mann has spent several periods at CERN, among others as a John Simon Guggenheim Memorial Foundation fellow in 1972.

Gell-Mann was born in lower Manhattan into a family of Jewish immigrants from the Austro-Hungarian Empire, specifically from Chernivtsi in present-day Ukraine. His parents were Pauline (née Reichstein) and Arthur Isidore Gell-Mann, who taught English as a Second Language (ESL).

Propelled by an intense boyhood curiosity and love for nature and mathematics, he graduated valedictorian from the Columbia Grammar & Preparatory School and subsequently entered Yale College at the age of 15 as a member of Jonathan Edwards College. At Yale, he participated in the William Lowell Putnam Mathematical Competition and was on the team representing Yale University (along with Murray Gerstenhaber and Henry O. Pollak) that won the second prize in 1947. Gell-Mann earned a bachelor's degree in physics from Yale in 1948 and a PhD in physics from Massachusetts Institute of Technology (MIT) in 1951. His supervisor at MIT was Victor Weisskopf.

In 1958, Gell-Mann and Richard Feynman, in parallel with the independent team of George Sudarshan and Robert Marshak, discovered the chiral structures of the weak interaction in physics. This work followed the experimental discovery of the violation of parity by Chien-Shiung Wu, as suggested by Chen Ning Yang and Tsung-Dao Lee, theoretically.

Gell-Mann's work in the 1950s involved recently discovered cosmic ray particles that came to be called kaons and hyperons. Classifying these particles led him to propose that a quantum number called strangeness would be conserved by the strong and the electromagnetic interactions, but not by the weak interactions. Another of Gell-Mann's ideas is the Gell-Mann–Okubo formula, which was, initially, a formula based on empirical results, but was later explained by his quark model. Gell-Mann and Abraham Pais were involved in explaining several puzzling aspects of the physics of these particles.

In 1961, this led him (and Kazuhiko Nishijima) to introduce a classification scheme for hadrons, elementary particles that participate in the strong interaction. (This scheme had been independently proposed by Yuval Ne'eman.) This scheme is now explained by the quark model. Gell-Mann referred to the scheme as the "Eightfold Way", because of the "octets" of particles in the classification. (The term is a reference to the eightfold way of Buddhism.)

In 1964, Gell-Mann and, independently, George Zweig went on to postulate the existence of quarks, particles of which the hadrons of this scheme are composed. The name was coined by Gell-Mann and is a reference to the novel "Finnegans Wake", by James Joyce ("Three quarks for Muster Mark!" book 2, episode 4.) Zweig had referred to the particles as "aces", but Gell-Mann's name caught on. Quarks, antiquarks, and gluons were soon established as the underlying elementary objects in the study of the structure of hadrons. He was awarded a Nobel Prize in physics in 1969 for his contributions and discoveries concerning the classification of elementary particles and their interactions.

In 1972 he and Harald Fritzsch introduced the conserved quantum number "color charge", and later, together with Heinrich Leutwyler, they coined the term quantum chromodynamics (QCD) as the gauge theory of the strong interaction. The quark model is a part of QCD, and it has been robust enough to accommodate in a natural fashion the discovery of new "flavors" of quarks, which superseded the eightfold way scheme.

He is currently the Robert Andrews Millikan Professor of Theoretical Physics Emeritus at California Institute of Technology as well as a University Professor in the Physics and Astronomy Department of the University of New Mexico in Albuquerque, New Mexico, and the Presidential Professor of Physics and Medicine at the University of Southern California. He is a member of the editorial board of the "Encyclopædia Britannica". In 1984 Gell-Mann co-founded the Santa Fe Institute—a non-profit theoretical research institute in Santa Fe, New Mexico—to study complex systems and disseminate the notion of a separate interdisciplinary study of complexity theory.

He was a postdoctoral fellow at the Institute for Advanced Study in 1951, and a visiting research professor at the University of Illinois at Urbana–Champaign from 1952 to 1953. He was a visiting associate professor at Columbia University and an associate professor at the University of Chicago in 1954–55 before moving to the California Institute of Technology, where he taught from 1955 until he retired in 1993.
During the 1990s, Gell-Mann's interest turned to the emerging study of complexity. He played a central role in the founding of the Santa Fe Institute, where he continues to work as a distinguished professor.

He wrote a popular science book about these matters, "The Quark and the Jaguar: Adventures in the Simple and the Complex" (1994). The title of the book is taken from a line of a poem by Arthur Sze: "The world of the quark has everything to do with a jaguar circling in the night".

The author George Johnson has written a biography of Gell-Mann, "Strange Beauty: Murray Gell-Mann, and the Revolution in 20th-Century Physics" (1999), which was shortlisted for the Royal Society Book Prize. Gell-Mann has criticized it as inaccurate. The Nobel Prize–winning physicist Philip Anderson, in his chapter on Gell-Mann from a 2011 book, says that Johnson's biography is excellent. Both Anderson and Johnson say that Gell-Mann is a perfectionist and that his semibiography, "The Quark and the Jaguar" (1994) is consequently incomplete.

In 2012 he and his companion Mary McFadden published the book "Mary McFadden: A Lifetime of Design, Collecting, and Adventure".

Gell-Mann introduced, independently of George Zweig, the quark—constituents of all hadrons—having first identified the SU(3) flavor symmetry of hadrons. This symmetry is now understood to underlie the light quarks, extending isospin to include strangeness, a quantum number which he also discovered.

He developed the V−A theory of the weak interaction in collaboration with Richard Feynman. In the 1960s, he introduced current algebra as a method of systematically exploiting symmetries to extract predictions from quark models, in the absence of reliable dynamical theory. This method led to model-independent sum rules confirmed by experiment and provided starting points underpinning the development of the Standard Model (SM), the widely accepted theory of elementary particles.

Gell-Mann, along with Maurice Lévy, developed the sigma model of pions, which describes low-energy pion interactions. Modifying the integer-charged quark model of Moo-Young Han and Yoichiro Nambu, Harald Fritzsch and Gell-Mann were the first to write down the modern accepted theory of quantum chromodynamics, although they did not anticipate asymptotic freedom. In 1969 he received the Nobel Prize in physics for his contributions and discoveries concerning the classification of elementary particles and their interactions.

Gell-Mann is responsible, together with Pierre Ramond and Richard Slansky, and independently of Peter Minkowski, Rabindra Mohapatra, Goran Senjanovic, Sheldon Lee Glashow, and Tsutomu Yanagida, for the seesaw theory of neutrino masses, that produces masses at the large scale in any theory with a right-handed neutrino. He is also known to have played a large role in keeping string theory alive through the 1970s and early 1980s, supporting that line of research at a time when it was unpopular.

Gell-Mann is a proponent of the consistent histories approach to understanding quantum mechanics.
Gell-Mann married J. Margaret Dow (d. 1981) in 1955: their children are Elizabeth Sarah Gell-Mann (b. 1956) and Nicholas Webster Gell-Mann (b. 1963). Margaret died in 1981, and in 1992 he married Marcia Southwick, with whom he has a stepson, Nicholas Southwick Levis (b. 1978).

Gell-Mann has interests in birdwatching, collecting antiques, ranching, historical linguistics, archaeology, natural history, the psychology of creative thinking, other subjects connected with biological, and cultural evolution and with learning. Along with S. A. Starostin, he established the "Evolution of Human Languages project" at the Santa Fe Institute.

As a humanist and an agnostic, Gell-Mann is a Humanist Laureate in the International Academy of Humanism.

Gell-Mann endorsed Barack Obama for the United States presidency in October 2008.

Together with author Michael Crichton, Gell-Mann is responsible for defining the theoretical psychological phenomenon called the Gell-Mann amnesia effect.

Gell-Mann has won numerous awards and honours including




</doc>
<doc id="20478" url="https://en.wikipedia.org/wiki?curid=20478" title="Magnetopause">
Magnetopause

The magnetopause is the abrupt boundary between a magnetosphere and the surrounding plasma. For planetary science, the magnetopause is the boundary between the planet’s magnetic field and the solar wind. The location of the magnetopause is determined by the balance between the pressure of the dynamic planetary magnetic field and the dynamic pressure of the solar wind. As the solar wind pressure increases and decreases, the magnetopause moves inward and outward in response. Waves (ripples and flapping motion) along the magnetopause move in the direction of the solar wind flow in response to small-scale variations in the solar wind pressure and to Kelvin–Helmholtz instability.

The solar wind is supersonic and passes through a bow shock where the direction of flow is changed so that most of the solar wind plasma is deflected to either side of the magnetopause, much like water is deflected before the bow of a ship. The zone of shocked solar wind plasma is the magnetosheath. At Earth and all the other planets with intrinsic magnetic fields, some solar wind plasma succeeds in entering and becoming trapped within the magnetosphere. At Earth, the solar wind plasma which enters the magnetosphere forms the plasma sheet. The amount of solar wind plasma and energy that enters the magnetosphere is regulated by the orientation of the interplanetary magnetic field, which is embedded in the solar wind.

The Sun and other stars with magnetic fields and stellar winds have a solar magnetopause or heliopause where the stellar environment is bounded by the interstellar environment.

Prior to the age of space exploration, interplanetary space was considered to be a vacuum. The coincidence of the Carrington super flare and the super geomagnetic event of 1859 was evidence that plasma was ejected from the Sun during a flare event. Chapman and Ferraro proposed that a plasma was emitted by the Sun in a burst as part of a flare event which disturbed the planet's magnetic field in a manner known as a geomagnetic storm. The collision frequency of particles in the plasma in the interplanetary medium is very low and the electrical conductivity is so high that it could be approximated to an infinite conductor. A magnetic field in a vacuum cannot penetrate a volume with infinite conductivity. Chapman and Bartels (1940) illustrated this concept by postulating a plate with infinite conductivity placed on the dayside of a planet’s dipole as shown in the schematic. The field lines on the dayside are bent. At low latitudes, the magnetic field lines are pushed inward. At high latitudes, the magnetic field lines are pushed backwards and over the polar regions. The boundary between the region dominated by the planet’s magnetic field (i.e., the magnetosphere) and the plasma in the interplanetary medium is the magnetopause. The configuration equivalent to a flat, infinitely conductive plate is achieved by placing an image dipole (green arrow at left of schematic) at twice the distance from the planet’s dipole to the magnetopause along the planet-Sun line. Since the solar wind is continuously flowing outward, the magnetopause above, below and to the sides of the planet are swept backward into the geomagnetic tail as shown in the artist’s concept. The region (shown in pink in the schematic) which separates field lines from the planet which are pushed inward from those which are pushed backward over the poles is an area of weak magnetic field or day-side cusp. Solar wind particles can enter the planet’s magnetosphere through the cusp region. Because the solar wind exists at all times and not just times of solar flares, the magnetopause is a permanent feature of the space near any planet with a magnetic field.

The magnetic field lines of the planet’s magnetic field are not stationary. They are continuously joining or merging with magnetic field lines of the interplanetary magnetic field. The joined field lines are swept back over the poles into the planetary magnetic tail. In the tail, the field lines from the planet’s magnetic field are re-joined and start moving toward night-side of the planet. The physics of this process was first explained by Dungey (1961) .

If one assumed that magnetopause was just a boundary between a magnetic field in a vacuum and a plasma with a weak magnetic field embedded in it, then the magnetopause would be defined by electrons and ions penetrating one gyroradius into the magnetic field domain. Since the gyro-motion of electrons and ions is in opposite directions, an electric current flows along the boundary. The actual magnetopause is much more complex.

If the pressure from particles within the magnetosphere is neglected, it is possible to estimate the distance to the part of the magnetosphere that faces the Sun. The condition governing this position is that the dynamic ram pressure from the solar wind is equal to the magnetic pressure from the Earth's magnetic field.
"B"("r") is the Magnetic field strength of the planet in SI units ("B" in T, μ in H/m)

Since the dipole magnetic field strength varies with distance as formula_4 the magnetic field strength can be written as formula_5, where formula_6 is the planet's magnetic moment, expressed in formula_7.
Solving this equation for r leads to an estimate of the distance

The distance from Earth to the subsolar magnetopause varies over time due to solar activity, but typical distances range from 6 - 15 Rformula_10. Empirical models using real-time solar wind data can provide a real-time estimate of the magnetopause location. A bow shock stands upstream from the magnetopause. It serves to decelerate and deflect the solar wind flow before it reaches the magnetopause 

Research on the magnetopause is conducted using the LMN coordinate system (which is set of axes like XYZ). N points normal to the magnetopause outward to the magnetosheath, L lies along the projection of the dipole axis onto the magnetopause (positive northward), and M completes the triad by pointing dawnward.

Venus and Mars do not have a planetary magnetic field and do not have a magnetopause. The solar wind interacts with the planet’s atmosphere and a void is created behind the planet. In the case of the Earth’s moon and other bodies without a magnetic field or atmosphere, the body’s surface interacts with the solar wind and a void is created behind the body.



</doc>
