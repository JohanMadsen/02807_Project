<doc id="23591" url="https://en.wikipedia.org/wiki?curid=23591" title="Panentheism">
Panentheism

Panentheism (meaning "all-in-God", from the Ancient Greek "pân", "all", "en", "in" and "Theós", "God") is the belief that the divine pervades and interpenetrates every part of the universe and also extends beyond time and space. The term was coined by the German philosopher Karl Krause in 1828 to distinguish the ideas of Georg Wilhelm Friedrich Hegel (1770–1831) and Friedrich Wilhelm Joseph Schelling (1775–1854) about the relation of God and the universe from the supposed pantheism of Baruch Spinoza. Unlike pantheism, which holds that the divine and the universe are identical, panentheism maintains an ontological distinction between the divine and the non-divine and the significance of both.


The religious beliefs of Neoplatonism can be regarded as panentheistic. Plotinus taught that there was an ineffable transcendent God ("the One", "to En", τὸ Ἕν) of which subsequent realities were emanations. From "the One" emanates the Divine Mind ("Nous", Νοῦς) and the Cosmic Soul ("Psyche", Ψυχή). In Neoplatonism the world itself is God (according to Plato's Timaeus 37). This concept of divinity is associated with that of the "Logos" (Λόγος), which had originated centuries earlier with Heraclitus (c. 535–475 BC). The "Logos" pervades the cosmos, whereby all thoughts and all things originate, or as Heraclitus said: "He who hears not me but the Logos will say: All is one." Neoplatonists such as Iamblichus attempted to reconcile this perspective by adding another hypostasis above the original monad of force or "Dunamis" (Δύναμις). This new all-pervasive monad encompassed all creation and its original uncreated emanations.

Baruch Spinoza later claimed that "Whatsoever is, is in God, and without God nothing can be, or be conceived." "Individual things are nothing but modifications of the attributes of God, or modes by which the attributes of God are expressed in a fixed and definite manner." Though Spinoza has been called the "prophet" and "prince" of pantheism, in a letter to Henry Oldenburg Spinoza states that: "as to the view of certain people that I identify god with nature (taken as a kind of mass or corporeal matter), they are quite mistaken". For Spinoza, our universe (cosmos) is a mode under two attributes of Thought and Extension. God has infinitely many other attributes which are not present in our world.

According to German philosopher Karl Jaspers, when Spinoza wrote "Deus sive Natura" (God or Nature) Spinoza did not mean to say that God and Nature are interchangeable terms, but rather that God's transcendence was attested by his infinitely many attributes, and that two attributes known by humans, namely Thought and Extension, signified God's immanence. Furthermore, Martial Guéroult suggested the term "panentheism", rather than "pantheism" to describe Spinoza's view of the relation between God and the world. The world is not God, but it is, in a strong sense, "in" God. Yet, American philosopher and self-described panentheist Charles Hartshorne referred to Spinoza's philosophy as "classical pantheism" and distinguished Spinoza's philosophy from panentheism.

In 1828, the German philosopher Karl Christian Friedrich Krause (1781–1832) seeking to reconcile monotheism and pantheism, coined the term "panentheism" (from the Ancient Greek expression πᾶν ἐν θεῷ, "pān en theṓ", literally "all in god"). This conception of God influenced New England transcendentalists such as Ralph Waldo Emerson. The term was popularized by Charles Hartshorne in his development of process theology and has also been closely identified with the New Thought. The formalization of this term in the West in the 19th century was not new; philosophical treatises had been written on it in the context of Hinduism for millennia.

Philosophers who embraced panentheism have included Thomas Hill Green (1839–1882), James Ward (1843–1925), Andrew Seth Pringle-Pattison (1856–1931) and Samuel Alexander (1859–1938). Beginning in the 1940s, Hartshorne examined numerous conceptions of God. He reviewed and discarded pantheism, deism, and pandeism in favor of panentheism, finding that such a "doctrine contains all of deism and pandeism except their arbitrary negations". Hartshorne formulated God as a being who could become "more perfect": He has absolute perfection in categories for which absolute perfection is possible, and relative perfection (i. e., is superior to all others) in categories for which perfection cannot be precisely determined.

Earliest reference to panentheistic thought in Hindu philosophy is in a creation myth contained in the later section of Rig Veda called the Purusha Sukta, which was compiled before 1100 BCE. The Purusha Sukta gives a description of the spiritual unity of the cosmos. It presents the nature of Purusha or the cosmic being as both immanent in the manifested world and yet transcendent to it. From this being the sukta holds, the original creative will proceeds, by which this vast universe is projected in space and time.

The most influential and dominant school of Indian philosophy, Advaita Vedanta, rejects theism and dualism by insisting that "Brahman [ultimate reality] is without parts or attributes...one without a second." Since Brahman has no properties, contains no internal diversity and is identical with the whole reality it cannot be understood as an anthropomorphic personal God. The relationship between Brahman and the creation is often thought to be panentheistic.

Panentheism is also expressed in the Bhagavad Gita. In verse IX.4, Krishna states: 

Many schools of Hindu thought espouse monistic theism, which is thought to be similar to a panentheistic viewpoint. Nimbarka's school of differential monism (Dvaitadvaita), Ramanuja's school of qualified monism (Vishistadvaita) and Saiva Siddhanta and Kashmir Shaivism are all considered to be panentheistic. Caitanya's Gaudiya Vaishnavism, which elucidates the doctrine of Acintya Bheda Abheda (inconceivable oneness and difference), is also thought to be panentheistic. In Kashmir Shaivism, all things are believed to be a manifestation of Universal Consciousness (Cit or Brahman). So from the point of view of this school, the phenomenal world ("Śakti") is real, and it exists and has its being in Consciousness ("Cit"). Thus, Kashmir Shaivism is also propounding of theistic monism or panentheism.

Shaktism, or Tantra, is regarded as an Indian prototype of Panentheism. Shakti is considered to be the cosmos itself – she is the embodiment of energy and dynamism, and the motivating force behind all action and existence in the material universe. Shiva is her transcendent masculine aspect, providing the divine ground of all being. "There is no Shiva without Shakti, or Shakti without Shiva. The two ... in themselves are One." Thus, it is She who becomes the time and space, the cosmos, it is She who becomes the five elements, and thus all animate life and inanimate forms. She is the primordial energy that holds all creation and destruction, all cycles of birth and death, all laws of cause and effect within Herself, and yet is greater than the sum total of all these. She is transcendent, but becomes immanent as the cosmos (Mula Prakriti). She, the Primordial Energy, directly becomes Matter.

Taoism says that all is part of the eternal tao, and that all interact through qi.

The Reverend Zen Master Soyen Shaku was the first Zen Buddhist Abbot to tour the United States in 1905–6. He wrote a series of essays collected into the book "Zen For Americans". In the essay titled "The God Conception of Buddhism" he attempts to explain how a Buddhist looks at the ultimate without an anthropomorphic God figure while still being able to relate to the term God in a Buddhist sense:

At the outset, let me state that Buddhism is not atheistic as the term is ordinarily understood. It has certainly a God, the highest reality and truth, through which and in which this universe exists. However, the followers of Buddhism usually avoid the term God, for it savors so much of Christianity, whose spirit is not always exactly in accord with the Buddhist interpretation of religious experience. Again, Buddhism is not pantheistic in the sense that it identifies the universe with God. On the other hand, the Buddhist God is absolute and transcendent; this world, being merely its manifestation, is necessarily fragmental and imperfect. To define more exactly the Buddhist notion of the highest being, it may be convenient to borrow the term very happily coined by a modern German scholar, "panentheism," according to which God is πᾶν καὶ ἕν (all and one) and more than the totality of existence. 
The essay then goes on to explain first utilizing the term "God" for the American audience to get an initial understanding of what he means by "panentheism," and then discusses the terms that Buddhism uses in place of "God" such as Dharmakaya, Buddha or AdiBuddha, and Tathagata.

Panentheism is also a feature of some Christian philosophical theologies and resonates strongly within the theological tradition of the Orthodox Church. It also appears in some Roman Catholic mysticism and in process theology. Process theological thinkers are generally regarded in the Christian West as unorthodox. Furthermore, process philosophical thought is widely believed to have paved the way for open theism, a movement that tends to associate itself primarily with the Evangelical branch of Protestantism, but is also generally considered unorthodox by most Evangelicals.

In Christianity, creation is not considered a literal "part of" God, and divinity is essentially distinct from creation (i.e., transcendent). There is, in other words, an irradicable difference between the uncreated (i. e., God) and the created (i. e., everything else). This does not mean, however, that the creation is wholly separated from God, because the creation exists in and from the divine energies. In Eastern Orthodoxy, these energies or operations are the natural activity of God and are in some sense identifiable with God, but at the same time the creation is wholly distinct from the divine essence. God creates the universe by His will and from His energies. It is, however, not an imprint or emanation of God's own essence ("ousia"), the essence He shares pre-eternally with His Word and Holy Spirit. Neither is it a directly literal outworking or effulgence of the divine, nor any other process which implies that creation is essentially God or a necessary part of God. The use of the term "panentheism" to describe the divine concept in Orthodox Christian theology is problematic for those who would insist that panentheism requires creation to be "part of" God.

God is not merely Creator of the universe, as His dynamic presence is necessary to sustain the existence of every created thing, small and great, visible and invisible. That is, God's energies maintain the existence of the created order and all created beings, even if those agencies have explicitly rejected him. His love for creation is such that He will not withdraw His presence, which would be the ultimate form of annihilation, not merely imposing death, but ending existence altogether. By this token, the entirety of creation is fundamentally "good" in its very being, and is not innately evil either in whole or in part. This does not deny the existence of spiritual or moral evil in a fallen universe, only the claim that it is an intrinsic property of creation. Sin results from the essential freedom of creatures to operate outside the divine order, not as a "necessary" consequence of having inherited human nature.

Many Christians who believe in universalism – mainly expressed in the Universalist Church of America, originating, as a fusion of Pietist and Anabaptist influences, from the American colonies of the 18th century – hold panentheistic views of God in conjunction with their belief in "apocatastasis", also called universal reconciliation. Panentheistic Christian Universalists often believe that all creation's subsistence in God renders untenable the notion of final and permanent alienation from Him, citing Scriptural passages such as Ephesians 4:6 ("[God] is over all and through all and in all") and Romans 11:36 ("from [God] and through him and to him are all things") to justify both panentheism and universalism. Panentheism was also a major force in the Unitarian church for a long time, based in part on Ralph Waldo Emerson's concept of the Over-soul (from the synonymous essay of 1841).

Panentheistic conceptions of God occur amongst some modern theologians. Process theology and Creation Spirituality, two recent developments in Christian theology, contain panentheistic ideas. Charles Hartshorne (1897–2000), who conjoined process theology with panentheism, maintained a lifelong membership in the Methodist church but was also a Unitarian. In later years he joined the Austin, Texas, Unitarian Universalist congregation and was an active participant in that church. Referring to the ideas such as Thomas Oord’s ‘theocosmocentrism’ (2010), the soft panentheism of open theism, Keith Ward’s comparative theology and John Polkinghorne’s critical realism (2009), Raymond Potgieter observes distinctions such as dipolar and bipolar:

The former suggests two poles separated such as God influencing creation and it in turn its creator (Bangert 2006:168), whereas bipolarity completes God’s being implying interdependence between temporal and eternal poles. (Marbaniang 2011:133), in dealing with Whitehead’s approach, does not make this distinction. I use the term bipolar as a generic term to include suggestions of the structural definition of God’s transcendence and immanence; to for instance accommodate a present and future reality into which deity must reasonably fit and function, and yet maintain separation from this world and evil whilst remaining within it.
Some argue that panentheism should also include the notion that God has always been related to some world or another, which denies the idea of creation out of nothing ("creatio ex nihilo"). Nazarene Methodist theologian Thomas Jay Oord (* 1965) advocates panentheism, but he uses the word "theocosmocentrism" to highlight the notion that God and some world or another are the primary conceptual starting blocks for eminently fruitful theology. This form of panentheism helps in overcoming the problem of evil and in proposing that God's love for the world is essential to who God is.

"Gnosticism" is a modern name for a variety of ancient religious ideas and systems prevalent in the first and second century AD. The teachings of the various gnostic groups were very diverse. In his "Dictionary of Gnosticism", Andrew Phillip Smith has written that some branches of Gnosticism taught a panentheistic view of reality, and held to the belief that God exists in the visible world only as sparks of spiritual "light". The goal of human existence is to know the sparks within oneself in order to return to God, who is in the Fullness (or Pleroma).

Gnosticism was panentheistic, believing that the true God is simultaneously both separate from the physical universe and present within it. As Jesus states in the Gospel of Thomas, "I am the light that is over all things. I am all ... . Split a piece of wood; I am there. Lift up the stone, and you will find me there." This seemingly contradictory interpretation of gnostic theology is not without controversy, since one interpretation of dualistic theology holds that a perfect God of pure spirit would not manifest himself through the fallen world of matter.

Manichaeism, being another gnostic sect, preached a very different doctrine in positioning the true Manichaean God against matter as well as other deities, that it described as enmeshed with the world, namely the gods of Jews, Christians and pagans. Nevertheless, this dualistic teaching included an elaborate cosmological myth that narrates the defeat of primal man by the powers of darkness that devoured and imprisoned the particles of light.

Valentinian Gnosticism taught that matter came about through emanations of the supreme being, even if to some this event is held to be more accidental than intentional. To other gnostics, these emanations were akin to the Sephirot of the Kabbalists and deliberate manifestations of a transcendent God through a complex system of intermediaries.

While mainstream Rabbinic Judaism is classically monotheistic, and follows in the footsteps of Maimonides (c. 1135–1204), the panentheistic conception of God can be found among certain mystical Jewish traditions. A leading scholar of Kabbalah, Moshe Idel ascribes this doctrine to the kabbalistic system of Moses ben Jacob Cordovero (1522–1570) and in the eighteenth century to the Baal Shem Tov (c. 1700–1760), founder of the Hasidic movement, as well as his contemporaries, Rabbi Dov Ber, the Maggid of Mezeritch (died 1772), and Menahem Mendel, the Maggid of Bar. This may be said of many, if not most, subsequent Hasidic masters. There is some debate as to whether Isaac Luria (1534–1572) and Lurianic Kabbalah, with its doctrine of tzimtzum, can be regarded as panentheistic.

According to Hasidism, the infinite Ein Sof is incorporeal and exists in a state that is both transcendent and immanent. This appears to be the view of non-Hasidic Rabbi Chaim of Volozhin, as well. Hasidic Judaism merges the elite ideal of nullification to a transcendent God, via the intellectual articulation of inner dimensions through Kabbalah and with emphasis on the panentheistic divine immanence in everything.

Many scholars would argue that "panentheism" is the best single-word description of the philosophical theology of Baruch Spinoza. It is therefore no surprise, that aspects of panentheism are also evident in the theology of Reconstructionist Judaism as presented in the writings of Mordecai Kaplan (1881–1983), who was strongly influenced by Spinoza.

Several Sufi saints and thinkers, primarily Ibn Arabi, held beliefs that have been considered panentheistic. These notions later took shape in the theory of wahdat ul-wujud (the Unity of All Things). Some Sufi Orders, notably the Bektashis and the Universal Sufi movement, continue to espouse panentheistic beliefs. Nizari Ismaili follow panentheism according to Ismaili doctrine. Nevertheless, some Shia Muslims also do believe in different degrees of Panentheism. 

Al-Qayyuum is a Name of God in the Qur'an which translates to "The Self-Existing by Whom all subsist". In Islam the universe can not exist if Allah doesn't exist, and it is only by His power which encompasses everything and which is everywhere that the universe can exist. In Ayaẗ al-Kursii God's throne is described as "extending over the heavens and the earth" and "He feels no fatigue in guarding and preserving them". This does not mean though that the universe is God, or that a creature (like a tree or an animal) is God, because those would be respectively pantheism, which is a heresy in traditional Islam, and the worst heresy in Islam, shirk (polytheism). God is separated by His creation but His creation can not survive without Him.

The Mesoamerican empires of the Mayas, Aztecs as well as the South American Incas (Tahuatinsuyu) have typically been characterized as polytheistic, with strong male and female deities. According to Charles C. Mann's history book "", only the lower classes of Aztec society were polytheistic. Philosopher James Maffie has argued that Aztec metaphysics was pantheistic rather than panentheistic, since Teotl was considered by Aztec philosophers to be the ultimate all-encompassing yet all-transcending force defined by its inherit duality.

Native American beliefs in North America have been characterized as panentheistic in that there is an emphasis on a single, unified divine spirit that is manifest in each individual entity. (North American Native writers have also translated the word for God as the Great Mystery or as the Sacred Other) This concept is referred to by many as the Great Spirit. Philosopher J. Baird Callicott has described Lakota theology as panentheistic, in that the divine both transcends and is immanent in everything.

One exception can be modern Cherokee who are predominantly monotheistic but apparently not panentheistic; yet in older Cherokee traditions many observe both aspects of pantheism and panentheism, and are often not beholden to exclusivity, encompassing other spiritual traditions without contradiction, a common trait among some tribes in the Americas.

The Sikh gurus have described God in numerous ways in their hymns included in the Guru Granth Sahib, the holy scripture of Sikhism, but the oneness of the deity is consistently emphasized throughout. God is described in the Mool Mantar, the first passage in the Guru Granth Sahib, and the basic formula of the faith is:

— ੴ ਸਤਿ ਨਾਮੁ ਕਰਤਾ ਪੁਰਖੁ ਨਿਰਭਉ ਨਿਰਵੈਰੁ ਅਕਾਲ ਮੂਰਤਿ ਅਜੂਨੀ ਸੈਭੰ ਗੁਰਪ੍ਰਸਾਦਿ ॥

"Ik Oankar Satnaam KartaaPurakh Nirbhau Nirvair AkaalMoorat Ajooni Saibhan GurPrasad"

One primal being who made the sound (oan) that expanded and created the world. Truth is the name. Creative being personified. Without fear, without hate. Image of the undying. Beyond birth, self existent. By Guru's grace~

Guru Arjan, the fifth guru of Sikhs, says, "God is beyond colour and form, yet His/Her presence is clearly visible" (Sri Guru Granth Sahib, Ang 74), and "Nanak's Lord transcends the world as well as the scriptures of the east and the west, and yet He/She is clearly manifest" (Sri Guru Granth Sahib, Ang 397).

Knowledge of the ultimate Reality is not a matter for reason; it comes by revelation of the ultimate reality through nadar (grace) and by anubhava (mystical experience). Says Guru Nanak; ""budhi pathi na paiai bahu chaturaiai bhai milai mani bhane."" This translates to "He/She is not accessible through intellect, or through mere scholarship or cleverness at argument; He/She is met, when He/She pleases, through devotion" (GG, 436).

Guru Nanak prefixed the numeral one (ik) to it, making it Ik Oankar or Ek Oankar to stress God's oneness. God is named and known only through his Own immanent nature. The only name which can be said to truly fit God's transcendent state is SatNam ( Sat Sanskrit, Truth), the changeless and timeless Reality. God is transcendent and all-pervasive at the same time. Transcendence and immanence are two aspects of the same single Supreme Reality. The Reality is immanent in the entire creation, but the creation as a whole fails to contain God fully. As says Guru Tegh Bahadur, Nanak IX, "He has himself spread out His/Her Own “maya” (worldly illusion) which He oversees; many different forms He assumes in many colours, yet He stays independent of all" (GG, 537).

In the Bahá'í Faith, God is described as a single, imperishable God, the creator of all things, including all the creatures and forces in the universe. The connection between God and the world is that of the creator to his creation. God is understood to be independent of his creation, and that creation is dependent and contingent on God. Accordingly, the Bahá'í Faith is much more closely aligned with traditions of monotheism than panentheism. God is not seen to be part of creation as he cannot be divided and does not descend to the condition of his creatures. Instead, in the Bahá'í understanding, the world of creation emanates from God, in that all things have been realized by him and have attained to existence. Creation is seen as the expression of God's will in the contingent world, and every created thing is seen as a sign of God's sovereignty, and leading to knowledge of him; the signs of God are most particularly revealed in human beings.

In Konkōkyō God is named “Tenchi Kane no Kami-Sama” which can mean “Golden spirit of the universe” Kami(God) is Also seen as infinitely loving and powerful. 
“This spirit creates new galaxies, winks out brilliant stars, and allowes our hearts to beat.”- “Shine From Within, an introduction to the Konko Faith”

People associated with panentheism:






</doc>
<doc id="23592" url="https://en.wikipedia.org/wiki?curid=23592" title="Paraphilia">
Paraphilia

Paraphilia (previously known as sexual perversion and sexual deviation) is the experience of intense sexual arousal to atypical objects, situations, fantasies, behaviors, or individuals. Such attraction may be labeled sexual fetishism. No consensus has been found for any precise border between unusual sexual interests and paraphilic ones. There is debate over which, if any, of the paraphilias should be listed in diagnostic manuals, such as the "Diagnostic and Statistical Manual of Mental Disorders" (DSM) or the International Classification of Diseases (ICD).

The number and taxonomy of paraphilia is under debate; one source lists as many as 549 types of paraphilia. The DSM-5 has specific listings for eight paraphilic disorders. Several sub-classifications of the paraphilias have been proposed, and some argue that a fully dimensional, spectrum or complaint-oriented approach would better reflect the evidence.

Many terms have been used to describe atypical sexual interests, and there remains debate regarding technical accuracy and perceptions of stigma. Sexologist John Money popularized the term "paraphilia" as a non-pejorative designation for unusual sexual interests. Money described paraphilia as "a sexuoerotic embellishment of, or alternative to the official, ideological norm." Psychiatrist Glen Gabbard writes that despite efforts by Stekel and Money, "the term "paraphilia" remains pejorative in most circumstances."

Coinage of the term "paraphilia" ("paraphilie") has been credited to Friedrich Salomon Krauss in 1903, and it entered the English language in 1913, in reference to Krauss by urologist William J. Robinson. It was used with some regularity by Wilhelm Stekel in the 1920s. The term comes from the Greek παρά ("para") "beside" and φιλία ("-philia") "friendship, love".

In the late 19th century, psychologists and psychiatrists started to categorize various paraphilias as they wanted a more descriptive system than the legal and religious constructs of sodomy and perversion. Before the introduction of the term "paraphilia" in the DSM-III (1980), the term "sexual deviation" was used to refer to paraphilias in the first two editions of the manual. In 1981, an article published in "American Journal of Psychiatry" described paraphilia as "recurrent, intense sexually arousing fantasies, sexual urges, or behaviors generally involving:

Homosexuality, now widely considered a normal variant of human sexuality, was at one time discussed as a sexual deviation. Sigmund Freud and subsequent psychoanalytic thinkers considered homosexuality and paraphilias to result from psychosexual non-normative relations to the Oedipal complex. As such, the term "sexual perversion" or the epithet "pervert" have historically referred to gay men, as well as other non-heterosexuals (people who fall out of the perceived norms of sexual orientation).

By the mid-20th century, mental health practitioners began formalizing "deviant sexuality" classifications into categories. Originally coded as 000-x63, homosexuality was the top of the classification list (Code 302.0) until the American Psychiatric Association removed homosexuality from the DSM in 1974. Martin Kafka writes, "Sexual disorders once considered paraphilias (e.g., homosexuality) are now regarded as variants of normal sexuality."

A 2012 literature study by clinical psychologist James Cantor, when comparing homosexuality with paraphilias, found that both share "the features of onset and course (both homosexuality and paraphilia being life-long), but they appear to differ on sex ratio, fraternal birth order, handedness, IQ and cognitive profile, and neuroanatomy". The research then concluded that the data seemed to suggest paraphilias and homosexuality as two distinct categories, but regarded the conclusion as "quite tentative" given the current limited understanding of paraphilias.

The causes of paraphilic sexual preferences in people are unclear, although a growing body of research points to a possible prenatal neurodevelopmental correlation. A 2008 study analyzing the sexual fantasies of 200 heterosexual men by using the Wilson Sex Fantasy Questionnaire exam, determined that males with a pronounced degree of fetish interest had a greater number of older brothers, a high 2D:4D digit ratio (which would indicate excessive prenatal estrogen exposure), and an elevated probability of being left-handed, suggesting that disturbed hemispheric brain lateralization may play a role in deviant attractions.

Behavioral explanations propose that paraphilias are conditioned early in life, during an experience that pairs the paraphilic stimulus with intense sexual arousal. Susan Nolen-Hoeksema suggests that, once established, masturbatory fantasies about the stimulus reinforce and broaden the paraphilic arousal.

There is scientific and political controversy regarding the continued inclusion of sex-related diagnoses such as the paraphilias in the DSM, due to the stigma of being classified as a mental illness.

Some groups, seeking greater understanding and acceptance of sexual diversity, have lobbied for changes to the legal and medical status of unusual sexual interests and practices. Charles Allen Moser, a physician and advocate for sexual minorities, has argued that the diagnoses should be eliminated from diagnostic manuals.

Albert Eulenburg (1914) noted a commonality across the paraphilias, using the terminology of his time, "All the forms of sexual perversion...have one thing in common: their roots reach down into the matrix of natural and normal sex life; there they are somehow closely connected with the feelings and expressions of our physiological erotism. They are...hyperbolic intensifications, distortions, monstrous fruits of certain partial and secondary expressions of this erotism which is considered 'normal' or at least within the limits of healthy sex feeling."

The clinical literature contains reports of many paraphilias, only some of which receive their own entries in the diagnostic taxonomies of the American Psychiatric Association or the World Health Organization. There is disagreement regarding which sexual interests should be deemed paraphilic disorders versus normal variants of sexual interest. For example, as of May 2000, per DSM-IV-TR, "Because some cases of Sexual Sadism may not involve harm to a victim (e.g., inflicting humiliation on a consenting partner), the wording for sexual sadism involves a hybrid of the DSM-III-R and DSM-IV wording (i.e., "the person has acted on these urges with a non-consenting person, or the urges, sexual fantasies, or behaviors cause marked distress or interpersonal difficulty").

The DSM-IV-TR also acknowledges that the diagnosis and classification of paraphilias across cultures or religions "is complicated by the fact that what is considered deviant in one cultural setting may be more acceptable in another setting”. Some argue that cultural relativism is important to consider when discussing paraphilias, because there is wide variance concerning what is sexually acceptable across cultures.

Consensual adult activities and adult entertainment involving sexual roleplay, novel, superficial, or trivial aspects of sexual fetishism, or incorporating the use of sex toys are not necessarily paraphilic. Paraphilial psychopathology is not the same as psychologically normative adult human sexual behaviors, sexual fantasy, and sex play.

Clinicians distinguish between optional, preferred and exclusive paraphilias, though the terminology is not completely standardized. An "optional" paraphilia is an alternative route to sexual arousal. In preferred paraphilias, a person prefers the paraphilia to conventional sexual activities, but also engages in conventional sexual activities.

The literature includes single-case studies of exceedingly rare and idiosyncratic paraphilias. These include an adolescent male who had a strong fetishistic interest in the exhaust pipes of cars, a young man with a similar interest in a specific type of car, and a man who had a paraphilic interest in sneezing (both his own and the sneezing of others).

In American psychiatry, prior to the publication of the DSM-I, paraphilias were classified as cases of "psychopathic personality with pathologic sexuality". The DSM-I (1952) included sexual deviation as a personality disorder of sociopathic subtype. The only diagnostic guidance was that sexual deviation should have been "reserved for deviant sexuality which [was] not symptomatic of more extensive syndromes, such as schizophrenic or obsessional reactions". The specifics of the disorder were to be provided by the clinician as a "supplementary term" to the sexual deviation diagnosis; there were no restrictions in the DSM-I on what this supplementary term could be. Researcher Anil Aggrawal writes that the now-obsolete DSM-I listed examples of supplementary terms for pathological behavior to include "homosexuality, transvestism, pedophilia, fetishism, and sexual sadism, including rape, sexual assault, mutilation."

The DSM-II (1968) continued to use the term "sexual deviations", but no longer ascribed them under personality disorders, but rather alongside them in a broad category titled "personality disorders and certain other nonpsychotic mental disorders". The types of sexual deviations listed in the DSM-II were: sexual orientation disturbance (homosexuality), fetishism, pedophilia, transvestitism (sic), exhibitionism, voyeurism, sadism, masochism, and "other sexual deviation". No definition or examples were provided for "other sexual deviation", but the general category of sexual deviation was meant to describe the sexual preference of individuals that was "directed primarily toward objects other than people of opposite sex, toward sexual acts not usually associated with coitus, or toward coitus performed under bizarre circumstances, as in necrophilia, pedophilia, sexual sadism, and fetishism." Except for the removal of homosexuality from the DSM-III onwards, this definition provided a general standard that has guided specific definitions of paraphilias in subsequent DSM editions, up to DSM-IV-TR.

The term "paraphilia" was introduced in the DSM-III (1980) as a subset of the new category of "psychosexual disorders."

The DSM-III-R (1987) renamed the broad category to sexual disorders, renamed atypical paraphilia to paraphilia NOS (not otherwise specified), renamed transvestism as transvestic fetishism, added frotteurism, and moved zoophilia to the NOS category. It also provided seven nonexhaustive examples of NOS paraphilias, which besides zoophilia included telephone scatologia, necrophilia, partialism, coprophilia, klismaphilia, and urophilia.

The DSM-IV (1994) retained the sexual disorders classification for paraphilias, but added an even broader category, "sexual and gender identity disorders," which includes them. The DSM-IV retained the same types of paraphilias listed in DSM-III-R, including the NOS examples, but introduced some changes to the definitions of some specific types.

The DSM-IV-TR describes paraphilias as "recurrent, intense sexually arousing fantasies, sexual urges or behaviors generally involving nonhuman objects, the suffering or humiliation of oneself or one's partner, or children or other nonconsenting persons that occur over a period of six months" (criterion A), which "cause clinically significant distress or impairment in social, occupational, or other important areas of functioning" (criterion B). DSM-IV-TR names eight specific paraphilic disorders (exhibitionism, fetishism, frotteurism, pedophilia, sexual masochism, sexual sadism, voyeurism, and transvestic fetishism, plus a residual category, paraphilia—not otherwise specified). Criterion B differs for exhibitionism, frotteurism, and pedophilia to include acting on these urges, and for sadism, acting on these urges with a nonconsenting person. Sexual arousal in association with objects that were designed for sexual purposes is not diagnosable.

Some paraphilias may interfere with the capacity for sexual activity with consenting adult partners.

In the current version of the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV-TR), a paraphilia is not diagnosable as a psychiatric disorder unless it causes distress to the individual or harm to others.

The DSM-5 adds a distinction between "paraphilias" and "paraphilic disorders", stating that paraphilias do not require or justify psychiatric treatment in themselves, and defining "paraphilic disorder" as "a paraphilia that is currently causing distress or impairment to the individual or a paraphilia whose satisfaction has entailed personal harm, or risk of harm, to others".

The DSM-5 Paraphilias Subworkgroup reached a "consensus that paraphilias are not "ipso facto" psychiatric disorders", and proposed "that the DSM-V make a distinction between "paraphilias" and paraphilic "disorders". [...] One would "ascertain" a paraphilia (according to the nature of the urges, fantasies, or behaviors) but "diagnose" a paraphilic disorder (on the basis of distress and impairment). In this conception, having a paraphilia would be a necessary but not a sufficient condition for having a paraphilic disorder." The 'Rationale' page of any paraphilia in the electronic DSM-5 draft continues: "This approach leaves intact the distinction between normative and non-normative sexual behavior, which could be important to researchers, but without automatically labeling non-normative sexual behavior as psychopathological. It also eliminates certain logical absurdities in the DSM-IV-TR. In that version, for example, a man cannot be classified as a transvestite—however much he cross-dresses and however sexually exciting that is to him—unless he is unhappy about this activity or impaired by it. This change in viewpoint would be reflected in the diagnostic criteria sets by the addition of the word "Disorder" to all the paraphilias. Thus, Sexual Sadism would become Sexual Sadism Disorder; Sexual Masochism would become Sexual Masochism Disorder, and so on."

Bioethics professor Alice Dreger interpreted these changes as "a subtle way of saying sexual kinks are basically okay – so okay, the sub-work group doesn’t actually bother to define paraphilia. But a paraphilic disorder is defined: that’s when an atypical sexual interest causes distress or impairment to the individual or harm to others." Interviewed by Dreger, Ray Blanchard, the Chair of the Paraphilias Sub-Work Group, explained: "We tried to go as far as we could in depathologizing mild and harmless paraphilias, while recognizing that severe paraphilias that distress or impair people or cause them to do harm to others are validly regarded as disorders."

Charles Allen Moser pointed out that this change is not really substantive as DSM-IV already acknowledged a difference between paraphilias and non-pathological but unusual sexual interests, a distinction that is virtually identical to what is being proposed for DSM-5, and it is a distinction that, in practice, has often been ignored. Linguist Andrew Clinton Hinderliter argued that "Including some sexual interests—but not others—in the DSM creates a fundamental asymmetry and communicates a negative value judgment against the sexual interests included," and leaves the paraphilias in a situation similar to ego-dystonic homosexuality, which was removed from the DSM because it was realized not to be a mental disorder.

The DSM-5 acknowledges that many dozens of paraphilias exist, but only has specific listings for eight that are forensically important and relatively common. These are voyeuristic disorder, exhibitionistic disorder, frotteuristic disorder, sexual masochism disorder, sexual sadism disorder, pedophilic disorder, fetishistic disorder, and transvestic disorder. Other paraphilias can be diagnosed under the Other Specified Paraphilic Disorder or Unspecified Paraphilic Disorder listings, if accompanied by distress or impairment.

Most clinicians and researchers believe that paraphilic sexual interests cannot be altered, although evidence is needed to support this. Instead, the goal of therapy is normally to reduce the person's discomfort with their paraphilia and limit any criminal behavior. Both psychotherapeutic and pharmacological methods are available to these ends.

Cognitive behavioral therapy, at times, can help people with paraphilias develop strategies to avoid acting on their interests. Patients are taught to identify and cope with factors that make acting on their interests more likely, such as stress. It is currently the only form of psychotherapy for paraphilias supported by randomized double-blind trials, as opposed to case studies and consensus of expert opinion.

Pharmacological treatments can help people control their sexual behaviors, but do not change the content of the paraphilia. They are typically combined with cognitive behavioral therapy for best effect.

Selective serotonin reuptake inhibitors (SSRIs) are used, especially with exhibitionists, non-offending pedophiles, and compulsive masturbators. They are proposed to work by reducing sexual arousal, compulsivity, and depressive symptoms. However, supporting evidence for SSRIs is limited.

Antiandrogens are used in more severe cases. Similar to physical castration, they work by reducing androgen levels, and have thus been described as chemical castration. The antiandrogen cyproterone acetate has been shown to substantially reduce sexual fantasies and offending behaviors. Medroxyprogesterone acetate and gonadotropin-releasing hormone agonists (such as leuprolide acetate) have also been used to lower sex drive. Due to the side effects, the World Federation of Societies of Biological Psychiatry recommends that hormonal treatments only be used when there is a serious risk of sexual violence, or when other methods have failed. Surgical castration has largely been abandoned because these pharmacological alternatives are similarly effective and less invasive.

Research has shown that paraphilias are rarely observed in women. However, there have been some studies on females with paraphilias. Sexual masochism has been found to be the most commonly observed paraphilia in women, with approximately 1 in 20 cases of sexual masochism being female.

Many acknowledge the scarcity of research on female paraphilias. The majority of paraphilia studies are conducted on people who have been convicted of sex crimes. Since the number of male convicted sex offenders far exceeds the number of female convicted sex offenders, research on paraphilic behavior in women is consequently lacking. Some researchers argue that an underrepresentation exists concerning pedophilia in females. Due to the low number of women in studies on pedophilia, most studies are based from "exclusively male samples". This likely underrepresentation may also be attributable to a "societal tendency to dismiss the negative impact of sexual relationships between young boys and adult women". Michele Elliott has done extensive research on child sexual abuse committed by females, publishing the book "Female Sexual Abuse of Children: The Last Taboo" in an attempt to challenge the gender-biased discourse surrounding sex crimes. John Hunsley states that physiological limitations in the study of female sexuality must also be acknowledged when considering research on paraphilias. He states that while a man's sexual arousal can be directly measured from his erection (see penile plethysmograph), a woman's sexual arousal cannot be measured as clearly (see vaginal photoplethysmograph), and therefore research concerning female sexuality is rarely as conclusive as research on men.

In the United States, following a series of landmark cases in the Supreme Court of the United States, persons diagnosed with paraphilias, particularly pedophilia ("Kansas v. Hendricks", 1997) and exhibitionism ("Kansas v. Crane", 2002), with a history of anti-social behavior and related criminal history, can be held indefinitely in civil confinement under various state legislation generically known as sexually violent predator laws and the federal Adam Walsh Act ("United States v. Comstock", 2010).






</doc>
<doc id="23593" url="https://en.wikipedia.org/wiki?curid=23593" title="Pediatrics">
Pediatrics

Pediatrics (also spelled paediatrics or pædiatrics) is the branch of medicine that involves the medical care of infants, children, and adolescents. The American Academy of Pediatrics recommends people be under pediatric care up to the age of 21. A medical doctor who specializes in this area is known as a pediatrician, or paediatrician. The word "pediatrics" and its cognates mean "healer of children"; they derive from two Greek words: ("pais" "child") and ("iatros" "doctor, healer"). Pediatricians work both in hospitals, particularly those working in its subspecialties such as neonatology, and as primary care physicians.

Already Hippocrates, Aristotle, Celsus, Soranus, and Galen understood the differences in growing and maturing organisms that necessitated different treatment: "" ( "In general, boys should not be treated in the same way as men." Celsus).

Some of the oldest traces of pediatrics can be discovered in Ancient India where children's doctors were called "kumara bhrtya". "Sushruta Samhita" an ayurvedic text, composed during the sixth century BC contains the text about pediatrics. Another ayurvedic text from this period is "Kashyapa Samhita".

A second century AD manuscript by the Greek physician and gynecologist Soranus of Ephesus dealt with neonatal pediatrics. Byzantine physicians Oribasius, Aëtius of Amida, Alexander Trallianus, and Paulus Aegineta contributed to the field. The Byzantines also built "brephotrophia" (crêches). Islamic writers served as a bridge for Greco-Roman and Byzantine medicine and added ideas of their own, especially Haly Abbas, Serapion, Avicenna, and Averroes. The Persian philosopher and physician al-Razi (865–925) published a monograph on pediatrics titled Diseases in Children as well as the first definite description of smallpox as a clinical entity. Also among the first books about pediatrics was "Libellus [Opusculum] de aegritudinibus et remediis infantium" 1472 ("Little Book on Children Diseases and Treatment"), by the Italian pediatrician Paolo Bagellardo. In sequence came Bartholomäus Metlinger's "Ein Regiment der Jungerkinder" 1473, Cornelius Roelans (1450–1525) no title Buchlein, or Latin compendium, 1483, and Heinrich von Louffenburg (1391–1460) "Versehung des Leibs" written in 1429 (published 1491), together form the "Pediatric Incunabula", four great medical treatises on children's physiology and pathology.

The Swedish physician Nils Rosén von Rosenstein (1706–1773) is considered to be the founder of modern pediatrics as a medical specialty, while his work "The diseases of children, and their remedies" (1764) is considered to be "the first modern textbook on the subject". Pediatrics as a specialized field of medicine continued to develop in the mid-19th century; German physician Abraham Jacobi (1830–1919) is known as the "father of American pediatrics" because of his many contributions to the field. He received his medical training in Germany and later practiced in New York City.

The first generally accepted pediatric hospital is the "Hôpital des Enfants Malades" (), which opened in Paris in June 1802 on the site of a previous orphanage. From its beginning, this famous hospital accepted patients up to the age of fifteen years, and it continues to this day as the pediatric division of the Necker-Enfants Malades Hospital, created in 1920 by merging with the physically contiguous "Necker Hospital", founded in 1778.

In other European countries, the Charité (a hospital founded in 1710) in Berlin established a separate Pediatric Pavilion in 1830, followed by similar institutions at Sankt Petersburg in 1834, and at Vienna and Breslau (now Wrocław), both in 1837. In 1852 Britain's first pediatric hospital, the Hospital for Sick Children, Great Ormond Street was founded by Charles West. The first Children's hospital in Scotland opened in 1860 in Edinburgh. In the US, the first similar institutions were the Children's Hospital of Philadelphia, which opened in 1855, and then Boston Children's Hospital (1869). Subspecialties in pediatrics were created at the Harriet Lane Home at Johns Hopkins by Edwards A. Park.

The body size differences are paralleled by maturation changes. The smaller body of an infant or neonate is substantially different physiologically from that of an adult. Congenital defects, genetic variance, and developmental issues are of greater concern to pediatricians than they often are to adult physicians. A common adage is that children are not simply "little adults". The clinician must take into account the immature physiology of the infant or child when considering symptoms, prescribing medications, and diagnosing illnesses.

A major difference between the practice of pediatric and adult medicine is that children, in most jurisdictions and with certain exceptions, cannot make decisions for themselves. The issues of guardianship, privacy, legal responsibility and informed consent must always be considered in every pediatric procedure. Pediatricians often have to treat the parents and sometimes, the family, rather than just the child. Adolescents are in their own legal class, having rights to their own health care decisions in certain circumstances. The concept of legal consent combined with the non-legal consent (assent) of the child when considering treatment options, especially in the face of conditions with poor prognosis or complicated and painful procedures/surgeries, means the pediatrician must take into account the desires of many people, in addition to those of the patient.

Aspiring medical students will need 4 years of undergraduate courses at a college or university, which will get them a BS, BA, MBBS or other bachelor's degree. After completing college future pediatricians will need to attend 4 years of medical school and later do 3 more years of residency training, the first year of which is called "internship." After completing the 3 years of residency, physicians are eligible to become certified in pediatrics by passing a rigorous test that deals with medical conditions related to young children.

In high school future pediatricians are required to take basic science classes such as, biology, chemistry, physics, algebra, geometry, and calculus and also foreign language class, preferably Spanish (in the United States), and get involved in high school organizations and extracurricular activities. After high school, college students simply need to fulfill the basic science course requirements that most medical schools recommend and will need to prepare to take the MCAT (Medical College Admission Test) their junior or early senior year in college. Once attending medical school, student courses will focus on basic medical sciences like human anatomy, physiology, chemistry, etc., for the first three years, the second year of which is when medical students start to get hands-on experience with actual patients.

The training of pediatricians varies considerably across the world. Depending on jurisdiction and university, a medical degree course may be either undergraduate-entry or graduate-entry. The former commonly takes five or six years, and has been usual in the Commonwealth. Entrants to graduate-entry courses (as in the US), usually lasting four or five years, have previously completed a three- or four-year university degree, commonly but by no means always in sciences. Medical graduates hold a degree specific to the country and university in and from which they graduated. This degree qualifies that medical practitioner to become licensed or registered under the laws of that particular country, and sometimes of several countries, subject to requirements for "internship" or "conditional registration".

Pediatricians must undertake further training in their chosen field. This may take from four to eleven or more years, (depending on jurisdiction and the degree of specialization).

In the United States, a medical school graduate wishing to specialize in pediatrics must undergo a three-year residency composed of outpatient, inpatient, surgical, and critical care rotations. Specialties within pediatrics require further training in the form of 3-year fellowships. Specialties include critical care, gastroenterology, neurology, infectious disease, hematology/oncology, rheumatology, pulmonology, child abuse, emergency medicine, endocrinology, neonatology, and others.

In most jurisdictions, entry-level degrees are common to all branches of the medical profession, but in some jurisdictions, specialization in pediatrics may begin before completion of this degree. In some jurisdictions, pediatric training is begun immediately following completion of entry-level training. In other jurisdictions, junior medical doctors must undertake generalist (unstreamed) training for a number of years before commencing pediatric (or any other) specialization. Specialist training is often largely under the control of pediatric organizations (see below) rather than universities, and depend on jurisdiction.

Subspecialties of pediatrics include:

Other specialties that care for children include:





</doc>
<doc id="23597" url="https://en.wikipedia.org/wiki?curid=23597" title="Physiology">
Physiology

Physiology (; ) is the scientific study of the functions and mechanisms which work within a living system. 

A sub-discipline of biology, its focus is in how organisms, organ systems, organs, cells, and biomolecules carry out the chemical and physical functions that exist in a living system. 

Central to an understanding of physiological functioning is the investigation of the fundamental biophysical and biochemical phenomena, the coordinated homeostatic control mechanisms, and the continuous communication between cells.

According to the type of investigated organisms, the field can be divided into, animal physiology (including that of humans), plant physiology, cellular physiology and microbial physiology.

The Nobel Prize in Physiology or Medicine is awarded to those who make significant achievements in this discipline by the Royal Swedish Academy of Sciences. The physiologic state is the condition occurring from normal body function, while the pathological state is centered on the abnormalities that occur in animal diseases, including humans.

Human physiology seeks to understand the mechanisms that work to keep the human body alive and functioning, through scientific enquiry into the nature of mechanical, physical, and biochemical functions of humans, their organs, and the cells of which they are composed. The principal level of focus of physiology is at the level of organs and systems within systems. The endocrine and nervous systems play major roles in the reception and transmission of signals. that integrate function in animals. Homeostasis is a major aspect with regard to such interactions within plants as well as animals. The biological basis of the study of physiology, integration refers to the overlap of many functions of the systems of the human body, as well as its accompanied form. It is achieved through communication that occurs in a variety of ways, both electrical and chemical.

Changes in physiology can impact the mental functions of individuals. Examples of this would be the effects of certain medications or toxic levels of substances. Change in behavior as a result of these substances is often used to assess the health of individuals.

Much of the foundation of knowledge in human physiology was provided by animal experimentation. Due to the frequent connection between form and function, physiology and anatomy are intrinsically linked and are studied in tandem as part of a medical curriculum.

Plant physiology is a subdiscipline of botany concerned with the functioning of plants. Closely related fields include plant morphology, plant ecology, phytochemistry, cell biology, genetics, biophysics, and molecular biology. Fundamental processes of plant physiology include photosynthesis, respiration, plant nutrition, tropisms, nastic movements, photoperiodism, photomorphogenesis, circadian rhythms, seed germination, dormancy, and stomata function and transpiration. Absorption of water by roots, production of food in the leaves, and growth of shoots towards light are examples of plant physiology.

Although there are differences between animal, plant, and microbial cells, the basic physiological functions of cells can be divided into the processes of cell division, cell signaling, cell growth, and cell metabolism.

Microorganisms can be found almost everywhere on Earth. Types of microorganisms include archaea, bacteria, eukaryotes, protists, fungi, and micro-plants. Microbes are important in human culture and health in many ways, serving to ferment foods, treat sewage, produce fuel, enzymes and other bioactive compounds. They are essential tools in biology as model organisms and have been put to use in biological warfare and bioterrorism. They are a vital component of fertile soils. In the human body microorganisms make up the human microbiota including the essential gut flora. They are the pathogens responsible for many infectious diseases and as such are the target of hygiene measures. Most microorganisms can reproduce rapidly, and bacteria are also able to freely exchange genes through conjugation, transformation and transduction, even between widely divergent species.

The study of human physiology as a medical field originates in classical Greece, at the time of Hippocrates (late 5th century BC). 
Outside of Western tradition, early forms of physiology or anatomy can be reconstructed as having been present at around the same time in China, India and elsewhere.
Hippocrates incorporated his belief system called the theory of humours, which consisted of four basic substance: earth, water, air and fire. Each substance is known for having a corresponding humour: black bile, phlegm, blood and yellow bile, respectively. Hippocrates also noted some emotional connections to the four humours, which Claudius Galenus would later expand on. The critical thinking of Aristotle and his emphasis on the relationship between structure and function marked the beginning of physiology in Ancient Greece. Like Hippocrates, Aristotle took to the humoral theory of disease, which also consisted of four primary qualities in life: hot, cold, wet and dry. Claudius Galenus (c. ~130–200 AD), known as Galen of Pergamum, was the first to use experiments to probe the functions of the body. Unlike Hippocrates, Galen argued that humoral imbalances can be located in specific organs, including the entire body. His modification of this theory better equipped doctors to make more precise diagnoses. Galen also played off of Hippocrates idea that emotions were also tied to the humours, and added the notion of temperaments: sanguine corresponds with blood; phlegmatic is tied to phlegm; yellow bile is connected to choleric; and black bile corresponds with melancholy. Galen also saw the human body consisting of three connected systems: the brain and nerves, which are responsible for thoughts and sensations; the heart and arteries, which give life; and the liver and veins, which can be attributed to nutrition and growth. Galen was also the founder of experimental physiology. And for the next 1,400 years, Galenic physiology was a powerful and influential tool in medicine.

Jean Fernel (1497–1558), a French physician, introduced the term "physiology". Galen, Ibn al-Nafis, Michael Servetus, Realdo Colombo, Amato Lusitano and William Harvey, are credited as making important discoveries in the circulation of the blood. Santorio Santorio in 1610s was the first to use a device to measure the pulse rate (the "pulsilogium"), and a thermoscope to measure temperature.

In 1791 Luigi Galvani described the role of electricity in nerves of dissected frogs. In 1811, Julien Jean César Legallois studied respiration in animal dissection and lesions and found the center of respiration in the medulla oblongata. In the same year, Charles Bell finished work on what would later become known as the Bell-Magendie law, which compared functional differences between dorsal and ventral roots of the spinal cord. In 1824, François Magendie described the sensory roots and produced the first evidence of the cerebellum’s role in equilibration to complete the Bell-Magendie law.

In the 1820s, the French physiologist Henri Milne-Edwards introduced the notion of physiological division of labor, which allowed to "compare and study living things as if they were machines created by the industry of man." Inspired in the work of Adam Smith, Milne-Edwards wrote that the "body of all living beings, whether animal or plant, resembles a factory ... where the organs, comparable to workers, work incessantly to produce the phenomena that constitute the life of the individual." In more differentiated organisms, the functional labor could be apportioned between different instruments or systems (called by him as "appareils").

In 1858, Joseph Lister studied the cause of blood coagulation and inflammation that resulted after previous injuries and surgical wounds. He later discovered and implemented antiseptics in the operating room, and as a result decreased death rate from surgery by a substantial amount.

The Physiological Society was founded in London in 1876 as a dining club. The American Physiological Society (APS) is a nonprofit organization that was founded in 1887. The Society is, "devoted to fostering education, scientific research, and dissemination of information in the physiological sciences."

In 1891, Ivan Pavlov performed research on "conditional responses" that involved dogs' saliva production in response to a bell and visual stimuli.

In the 19th century, physiological knowledge began to accumulate at a rapid rate, in particular with the 1838 appearance of the Cell theory of Matthias Schleiden and Theodor Schwann. It radically stated that organisms are made up of units called cells. Claude Bernard's (1813–1878) further discoveries ultimately led to his concept of "milieu interieur" (internal environment), which would later be taken up and championed as "homeostasis" by American physiologist Walter B. Cannon in 1929. By homeostasis, Cannon meant "the maintenance of steady states in the body and the physiological processes through which they are regulated." In other words, the body's ability to regulate its internal environment. William Beaumont was the first American to utilize the practical application of physiology.

Nineteenth century physiologists such as Michael Foster, Max Verworn, and Alfred Binet, based on Haeckel's ideas, elaborated what came to be called "general physiology", a unified science of life based on the cell actions, later renamed in the 20th century as cell biology.

In the 20th century, biologists became interested in how organisms other than human beings function, eventually spawning the fields of comparative physiology and ecophysiology. Major figures in these fields include Knut Schmidt-Nielsen and George Bartholomew. Most recently, evolutionary physiology has become a distinct subdiscipline.

In 1920, August Krogh won the Nobel Prize for discovering how, in capillaries, blood flow is regulated.

In 1954, Andrew Huxley and Hugh Huxley, alongside their research team, discovered the sliding filaments in skeletal muscle, known today as the sliding filament theory.

Initially, women were largely excluded from official involvement in any physiological society. The American Physiological Society, for example, was founded in 1887 and included only men in its ranks. In 1902, the American Physiological Society elected Ida Hyde as the first female member of the society. Hyde, a representative of the American Association of University Women and a global advocate for gender equality in education, attempted to promote gender equality in every aspect of science and medicine.

Soon thereafter, in 1913, J.S. Haldane proposed that women be allowed to formally join The Physiological Society, which had been founded in 1876. On 3 July 1915, six women were officially admitted: Florence Buchanan, Winifred Cullis, Ruth C. Skelton, Sarah C. M. Sowton, Constance Leetham Terry, and Enid M. Tribe. The centenary of the election of women was celebrated in 2015 with the publication of the book "Women Physiologists: Centenary Celebrations And Beyond For The Physiological Society." ()

Prominent women physiologists include:

There are many ways to categorize the subdiscplines of physiology:


Human physiology

Animal physiology

Plant physiology

Fungal physiology

Protistan physiology

Algal physiology

Bacterial physiology


</doc>
<doc id="23601" url="https://en.wikipedia.org/wiki?curid=23601" title="Pi">
Pi

The number () is a mathematical constant. Originally defined as the ratio of a circle's circumference to its diameter, it now has various equivalent definitions and appears in many formulas in all areas of mathematics and physics. It is approximately equal to 3.14159. It has been represented by the Greek letter "" since the mid-18th century, though it is also sometimes spelled out as "pi". It is also called Archimedes' constant.

Being an irrational number, cannot be expressed as a common fraction (equivalently, its decimal representation never ends and never settles into a permanently repeating pattern). Still, fractions such as 22/7 and other rational numbers are commonly used to approximate . The digits appear to be randomly distributed. In particular, the digit sequence of is conjectured to satisfy a specific kind of statistical randomness, but to date, no proof of this has been discovered. Also, is a transcendental number; that is, it is not the root of any polynomial having rational coefficients. This transcendence of implies that it is impossible to solve the ancient challenge of squaring the circle with a compass and straightedge.

Ancient civilizations required fairly accurate computed values to approximate for practical reasons, including the Egyptians and Babylonians. Around 250 BC the Greek mathematician Archimedes created an algorithm for calculating it. In the 5th century AD Chinese mathematics approximated to seven digits, while Indian mathematics made a five-digit approximation, both using geometrical techniques. The historically first exact formula for , based on infinite series, was not available until a millennium later, when in the 14th century the Madhava–Leibniz series was discovered in Indian mathematics. In the 20th and 21st centuries, mathematicians and computer scientists discovered new approaches that, when combined with increasing computational power, extended the decimal representation of to many trillions of digits after the decimal point. Practically all scientific applications require no more than a few hundred digits of , and many substantially fewer, so the primary motivation for these computations is the quest to find more efficient algorithms for calculating lengthy numeric series, as well as the desire to break records. The extensive calculations involved have also been used to test supercomputers and high-precision multiplication algorithms.

Because its most elementary definition relates to the circle, is found in many formulae in trigonometry and geometry, especially those concerning circles, ellipses, and spheres. In more modern mathematical analysis, the number is instead defined using the spectral properties of the real number system, as an eigenvalue or a period, without any reference to geometry. It appears therefore in areas of mathematics and the sciences having little to do with the geometry of circles, such as number theory and statistics, as well as in almost all areas of physics. The ubiquity of makes it one of the most widely known mathematical constants both inside and outside the scientific community. Several books devoted to have been published, and record-setting calculations of the digits of often result in news headlines. Attempts to memorize the value of with increasing precision have led to records of over 70,000 digits.

The symbol used by mathematicians to represent the ratio of a circle's circumference to its diameter is the lowercase Greek letter, sometimes spelled out as "pi," and derived from the first letter of the Greek word "perimetros," meaning circumference. In English, is pronounced as "pie" (, ). In mathematical use, the lowercase letter (or π in sans-serif font) is distinguished from its capitalized and enlarged counterpart , which denotes a product of a sequence, analogous to how denotes summation.

The choice of the symbol is discussed in the section "Adoption of the symbol ".

 is commonly defined as the ratio of a circle's circumference to its diameter :
The ratio is constant, regardless of the circle's size. For example, if a circle has twice the diameter of another circle it will also have twice the circumference, preserving the ratio . This definition of implicitly makes use of flat (Euclidean) geometry; although the notion of a circle can be extended to any curved (non-Euclidean) geometry, these new circles will no longer satisfy the formula .

Here, the circumference of a circle is the arc length around the perimeter of the circle, a quantity which can be formally defined independently of geometry using limits, a concept in calculus. For example, one may compute directly the arc length of the top half of the unit circle given in Cartesian coordinates by , as the integral:
An integral such as this was adopted as the definition of by Karl Weierstrass, who defined it directly as an integral in 1841.

Definitions of such as these that rely on a notion of circumference, and hence implicitly on concepts of the integral calculus, are no longer common in the literature. explains that this is because in many modern treatments of calculus, differential calculus typically precedes integral calculus in the university curriculum, so it is desirable to have a definition of that does not rely on the latter. One such definition, due to Richard Baltzer, and popularized by Edmund Landau, is the following: is twice the smallest positive number at which the cosine function equals 0. The cosine can be defined independently of geometry as a power series, or as the solution of a differential equation.

In a similar spirit, can be defined instead using properties of the complex exponential, , of a complex variable . Like the cosine, the complex exponential can be defined in one of several ways. The set of complex numbers at which is equal to one is then an (imaginary) arithmetic progression of the form:
and there is a unique positive real number with this property.
A more abstract variation on the same idea, making use of sophisticated mathematical concepts of topology and algebra, is the following theorem: there is a unique (up to automorphism) continuous isomorphism from the group R/Z of real numbers under addition modulo integers (the circle group) onto the multiplicative group of complex numbers of absolute value one. The number is then defined as half the magnitude of the derivative of this homomorphism.

A circle encloses the largest area that can be attained within a given perimeter. Thus the number is also characterized as the best constant in the isoperimetric inequality (times one-fourth). There are many other, closely related, ways in which appears as an eigenvalue of some geometrical or physical process; see below.

 is an irrational number, meaning that it cannot be written as the ratio of two integers (fractions such as are commonly used to approximate , but no common fraction (ratio of whole numbers) can be its exact value). Because is irrational, it has an infinite number of digits in its decimal representation, and it does not settle into an infinitely repeating pattern of digits. There are several proofs that is irrational; they generally require calculus and rely on the "reductio ad absurdum" technique. The degree to which can be approximated by rational numbers (called the irrationality measure) is not precisely known; estimates have established that the irrationality measure is larger than the measure of or but smaller than the measure of Liouville numbers.

The digits of have no apparent pattern and have passed tests for statistical randomness, including tests for normality; a number of infinite length is called normal when all possible sequences of digits (of any given length) appear equally often. The conjecture that is normal has not been proven or disproven.

Since the advent of computers, a large number of digits of have been available on which to perform statistical analysis. Yasumasa Kanada has performed detailed statistical analyses on the decimal digits of and found them consistent with normality; for example, the frequencies of the ten digits 0 to 9 were subjected to statistical significance tests, and no evidence of a pattern was found. Any random sequence of digits contains arbitrarily long subsequences that appear non-random, by the infinite monkey theorem. Thus, because the sequence of 's digits passes statistical tests for randomness, it contains some sequences of digits that may appear non-random, such as a sequence of six consecutive 9s that begins at the 762nd decimal place of the decimal representation of . This is also called the "Feynman point" in mathematical folklore, after Richard Feynman, although no connection to Feynman is known.

In addition to being irrational, more strongly is a transcendental number, which means that it is not the solution of any non-constant polynomial equation with rational coefficients, such as .

The transcendence of has two important consequences: First, cannot be expressed using any finite combination of rational numbers and square roots or "n"-th roots such as or . Second, since no transcendental number can be constructed with compass and straightedge, it is not possible to "square the circle". In other words, it is impossible to construct, using compass and straightedge alone, a square whose area is exactly equal to the area of a given circle. Squaring a circle was one of the important geometry problems of the classical antiquity. Amateur mathematicians in modern times have sometimes attempted to square the circle and sometimes claim success despite the fact that it is mathematically impossible.

Like all irrational numbers, cannot be represented as a common fraction (also known as a simple or vulgar fraction), by the very definition of "irrational number" (that is, "not a rational number"). But every irrational number, including , can be represented by an infinite series of nested fractions, called a continued fraction:

Truncating the continued fraction at any point yields a rational approximation for ; the first four of these are 3, 22/7, 333/106, and 355/113. These numbers are among the most well-known and widely used historical approximations of the constant. Each approximation generated in this way is a best rational approximation; that is, each is closer to than any other fraction with the same or a smaller denominator. Because is known to be transcendental, it is by definition not algebraic and so cannot be a quadratic irrational. Therefore, cannot have a periodic continued fraction. Although the simple continued fraction for (shown above) also does not exhibit any other obvious pattern, mathematicians have discovered several generalized continued fractions that do, such as:

Some approximations of "pi" include:

Digits in other number systems


Any complex number, say , can be expressed using a pair of real numbers. In the polar coordinate system, one number (radius or "r") is used to represent 's distance from the origin of the complex plane and the other (angle or ) to represent a counter-clockwise rotation from the positive real line as follows:
where is the imaginary unit satisfying = −1. The frequent appearance of in complex analysis can be related to the behavior of the exponential function of a complex variable, described by Euler's formula:

where the constant is the base of the natural logarithm. This formula establishes a correspondence between imaginary powers of and points on the unit circle centered at the origin of the complex plane. Setting = in Euler's formula results in Euler's identity, celebrated by mathematicians because it contains the five most important mathematical constants:

There are different complex numbers satisfying , and these are called the "-th roots of unity". They are given by this formula:

The best-known approximations to dating before the Common Era were accurate to two decimal places; this was improved upon in Chinese mathematics in particular by the mid-first millennium, to an accuracy of seven decimal places.
After this, no further progress was made until the late medieval period.

Some Egyptologists have claimed that the ancient Egyptians used an approximation of as from as early as the Old Kingdom.
This claim has met with skepticism.

The earliest written approximations of are found in Egypt and Babylon, both within one percent of the true value. In Babylon, a clay tablet dated 1900–1600 BC has a geometrical statement that, by implication, treats as  = 3.125. In Egypt, the Rhind Papyrus, dated around 1650 BC but copied from a document dated to 1850 BC, has a formula for the area of a circle that treats as () ≈ 3.1605.

Astronomical calculations in the "Shatapatha Brahmana" (ca. 4th century BC) use a fractional approximation of  ≈ 3.139 (an accuracy of 9×10). Other Indian sources by about 150 BC treat as  ≈ 3.1622.

The first recorded algorithm for rigorously calculating the value of was a geometrical approach using polygons, devised around 250 BC by the Greek mathematician Archimedes. This polygonal algorithm dominated for over 1,000 years, and as a result is sometimes referred to as "Archimedes' constant". Archimedes computed upper and lower bounds of by drawing a regular hexagon inside and outside a circle, and successively doubling the number of sides until he reached a 96-sided regular polygon. By calculating the perimeters of these polygons, he proved that (that is ). Archimedes' upper bound of may have led to a widespread popular belief that is equal to . Around 150 AD, Greek-Roman scientist Ptolemy, in his "Almagest", gave a value for of 3.1416, which he may have obtained from Archimedes or from Apollonius of Perga. Mathematicians using polygonal algorithms reached 39 digits of in 1630, a record only broken in 1699 when infinite series were used to reach 71 digits.
In ancient China, values for included 3.1547 (around 1 AD), (100 AD, approximately 3.1623), and (3rd century, approximately 3.1556). Around 265 AD, the Wei Kingdom mathematician Liu Hui created a polygon-based iterative algorithm and used it with a 3,072-sided polygon to obtain a value of of 3.1416. Liu later invented a faster method of calculating and obtained a value of 3.14 with a 96-sided polygon, by taking advantage of the fact that the differences in area of successive polygons form a geometric series with a factor of 4. The Chinese mathematician Zu Chongzhi, around 480 AD, calculated that (a fraction that goes by the name "Milü" in Chinese), using Liu Hui's algorithm applied to a 12,288-sided polygon. With a correct value for its seven first decimal digits, this value of 3.141592920... remained the most accurate approximation of available for the next 800 years.

The Indian astronomer Aryabhata used a value of 3.1416 in his "Āryabhaṭīya" (499 AD). Fibonacci in c. 1220 computed 3.1418 using a polygonal method, independent of Archimedes. Italian author Dante apparently employed the value .

The Persian astronomer Jamshīd al-Kāshī produced 9 sexagesimal digits, roughly the equivalent of 16 decimal digits, in 1424 using a polygon with 3×2 sides, which stood as the world record for about 180 years. French mathematician François Viète in 1579 achieved 9 digits with a polygon of 3×2 sides. Flemish mathematician Adriaan van Roomen arrived at 15 decimal places in 1593. In 1596, Dutch mathematician Ludolph van Ceulen reached 20 digits, a record he later increased to 35 digits (as a result, was called the "Ludolphian number" in Germany until the early 20th century). Dutch scientist Willebrord Snellius reached 34 digits in 1621, and Austrian astronomer Christoph Grienberger arrived at 38 digits in 1630 using 10 sides, which remains the most accurate approximation manually achieved using polygonal algorithms.

The calculation of was revolutionized by the development of infinite series techniques in the 16th and 17th centuries. An infinite series is the sum of the terms of an infinite sequence. Infinite series allowed mathematicians to compute with much greater precision than Archimedes and others who used geometrical techniques. Although infinite series were exploited for most notably by European mathematicians such as James Gregory and Gottfried Wilhelm Leibniz, the approach was first discovered in India sometime between 1400 and 1500 AD. The first written description of an infinite series that could be used to compute was laid out in Sanskrit verse by Indian astronomer Nilakantha Somayaji in his "Tantrasamgraha", around 1500 AD. The series are presented without proof, but proofs are presented in a later Indian work, "Yuktibhāṣā", from around 1530 AD. Nilakantha attributes the series to an earlier Indian mathematician, Madhava of Sangamagrama, who lived c. 1350 – c. 1425. Several infinite series are described, including series for sine, tangent, and cosine, which are now referred to as the Madhava series or Gregory–Leibniz series. Madhava used infinite series to estimate to 11 digits around 1400, but that value was improved on around 1430 by the Persian mathematician Jamshīd al-Kāshī, using a polygonal algorithm.
The first infinite sequence discovered in Europe was an infinite product (rather than an infinite sum, which are more typically used in calculations) found by French mathematician François Viète in 1593:

The second infinite sequence found in Europe, by John Wallis in 1655, was also an infinite product: 
The discovery of calculus, by English scientist Isaac Newton and German mathematician Gottfried Wilhelm Leibniz in the 1660s, led to the development of many infinite series for approximating . Newton himself used an arcsin series to compute a 15 digit approximation of in 1665 or 1666, later writing "I am ashamed to tell you to how many figures I carried these computations, having no other business at the time."

In Europe, Madhava's formula was rediscovered by Scottish mathematician James Gregory in 1671, and by Leibniz in 1674:

This formula, the Gregory–Leibniz series, equals when evaluated with  = 1. In 1699, English mathematician Abraham Sharp used the Gregory–Leibniz series for formula_13 to compute to 71 digits, breaking the previous record of 39 digits, which was set with a polygonal algorithm. The Gregory–Leibniz for formula_14 series is simple, but converges very slowly (that is, approaches the answer gradually), so it is not used in modern calculations.

In 1706 John Machin used the Gregory–Leibniz series to produce an algorithm that converged much faster:
Machin reached 100 digits of with this formula. Other mathematicians created variants, now known as Machin-like formulae, that were used to set several successive records for calculating digits of . Machin-like formulae remained the best-known method for calculating well into the age of computers, and were used to set records for 250 years, culminating in a 620-digit approximation in 1946 by Daniel Ferguson – the best approximation achieved without the aid of a calculating device.

A record was set by the calculating prodigy Zacharias Dase, who in 1844 employed a Machin-like formula to calculate 200 decimals of in his head at the behest of German mathematician Carl Friedrich Gauss. British mathematician William Shanks famously took 15 years to calculate to 707 digits, but made a mistake in the 528th digit, rendering all subsequent digits incorrect.

Some infinite series for converge faster than others. Given the choice of two infinite series for , mathematicians will generally use the one that converges more rapidly because faster convergence reduces the amount of computation needed to calculate to any given accuracy. A simple infinite series for is the Gregory–Leibniz series:
As individual terms of this infinite series are added to the sum, the total gradually gets closer to , and – with a sufficient number of terms – can get as close to as desired. It converges quite slowly, though – after 500,000 terms, it produces only five correct decimal digits of .

An infinite series for (published by Nilakantha in the 15th century) that converges more rapidly than the Gregory–Leibniz series is:

The following table compares the convergence rates of these two series:

After five terms, the sum of the Gregory–Leibniz series is within 0.2 of the correct value of , whereas the sum of Nilakantha's series is within 0.002 of the correct value of . Nilakantha's series converges faster and is more useful for computing digits of . Series that converge even faster include Machin's series and Chudnovsky's series, the latter producing 14 correct decimal digits per term.

Not all mathematical advances relating to were aimed at increasing the accuracy of approximations. When Euler solved the Basel problem in 1735, finding the exact value of the sum of the reciprocal squares, he established a connection between and the prime numbers that later contributed to the development and study of the Riemann zeta function:

Swiss scientist Johann Heinrich Lambert in 1761 proved that is irrational, meaning it is not equal to the quotient of any two whole numbers. Lambert's proof exploited a continued-fraction representation of the tangent function. French mathematician Adrien-Marie Legendre proved in 1794 that is also irrational. In 1882, German mathematician Ferdinand von Lindemann proved that is transcendental, confirming a conjecture made by both Legendre and Euler. Hardy and Wright states that "the proofs were afterwards modified and simplified by Hilbert, Hurwitz, and other writers".

In the earliest usages, the Greek letter was an abbreviation of the Greek word for periphery (περιφέρεια), and was combined in ratios with δ (for diameter) or ρ (for radius) to form circle constants. (Before then, mathematicians sometimes used letters such as "c" or "p" instead.) The first recorded use is Oughtred's "formula_19", to express the ratio of periphery and diameter in the 1647 and later editions of "Clavis Mathematicae". Barrow likewise used "formula_20" to represent the constant 3.14…, while Gregory instead used "formula_21" to represent 6.28….

The earliest known use of the Greek letter alone to represent the ratio of a circle's circumference to its diameter was by Welsh mathematician William Jones in his 1706 work "Synopsis Palmariorum Matheseos; or, a New Introduction to the Mathematics". The Greek letter first appears there in the phrase "1/2 Periphery ()" in the discussion of a circle with radius one. However, he writes that his equations for are from the "ready pen of the truly ingenious Mr. John Machin", leading to speculation that Machin may have employed the Greek letter before Jones. Jones' notation was not immediately adopted by other mathematicians, with the fraction notation still being used as late as 1767.

Euler started using the single-letter form beginning with his 1727 "Essay Explaining The Properties Of Air", though he used =6.28…, the ratio of radius to periphery, in this and some later writing. Euler first used =3.14… in his 1736 work "Mechanica", and continued in his widely-read 1748 work "Introductio in analysin infinitorum" (he wrote: "for the sake of brevity we will write this number as ; thus is equal to half the circumference of a circle of radius 1"). Because Euler corresponded heavily with other mathematicians in Europe, the use of the Greek letter spread rapidly, and the practice was universally adopted thereafter in the Western world, though the definition still varied between 3.14... and 6.28... as late as 1761.

The development of computers in the mid-20th century again revolutionized the hunt for digits of . American mathematicians John Wrench and Levi Smith reached 1,120 digits in 1949 using a desk calculator. Using an inverse tangent (arctan) infinite series, a team led by George Reitwiesner and John von Neumann that same year achieved 2,037 digits with a calculation that took 70 hours of computer time on the ENIAC computer. The record, always relying on an arctan series, was broken repeatedly (7,480 digits in 1957; 10,000 digits in 1958; 100,000 digits in 1961) until 1 million digits were reached in 1973.

Two additional developments around 1980 once again accelerated the ability to compute . First, the discovery of new iterative algorithms for computing , which were much faster than the infinite series; and second, the invention of fast multiplication algorithms that could multiply large numbers very rapidly. Such algorithms are particularly important in modern computations because most of the computer's time is devoted to multiplication. They include the Karatsuba algorithm, Toom–Cook multiplication, and Fourier transform-based methods.

The iterative algorithms were independently published in 1975–1976 by American physicist Eugene Salamin and Australian scientist Richard Brent. These avoid reliance on infinite series. An iterative algorithm repeats a specific calculation, each iteration using the outputs from prior steps as its inputs, and produces a result in each step that converges to the desired value. The approach was actually invented over 160 years earlier by Carl Friedrich Gauss, in what is now termed the arithmetic–geometric mean method (AGM method) or Gauss–Legendre algorithm. As modified by Salamin and Brent, it is also referred to as the Brent–Salamin algorithm.

The iterative algorithms were widely used after 1980 because they are faster than infinite series algorithms: whereas infinite series typically increase the number of correct digits additively in successive terms, iterative algorithms generally "multiply" the number of correct digits at each step. For example, the Brent-Salamin algorithm doubles the number of digits in each iteration. In 1984, the Canadian brothers John and Peter Borwein produced an iterative algorithm that quadruples the number of digits in each step; and in 1987, one that increases the number of digits five times in each step. Iterative methods were used by Japanese mathematician Yasumasa Kanada to set several records for computing between 1995 and 2002. This rapid convergence comes at a price: the iterative algorithms require significantly more memory than infinite series.

For most numerical calculations involving , a handful of digits provide sufficient precision. According to Jörg Arndt and Christoph Haenel, thirty-nine digits are sufficient to perform most cosmological calculations, because that is the accuracy necessary to calculate the circumference of the observable universe with a precision of one atom. Accounting for additional digits needed to compensate for computational round-off errors, Arndt concludes that a few hundred digits would suffice for any scientific application. Despite this, people have worked strenuously to compute to thousands and millions of digits. This effort may be partly ascribed to the human compulsion to break records, and such achievements with often make headlines around the world. They also have practical benefits, such as testing supercomputers, testing numerical analysis algorithms (including high-precision multiplication algorithms); and within pure mathematics itself, providing data for evaluating the randomness of the digits of .

Modern calculators do not use iterative algorithms exclusively. New infinite series were discovered in the 1980s and 1990s that are as fast as iterative algorithms, yet are simpler and less memory intensive. The fast iterative algorithms were anticipated in 1914, when the Indian mathematician Srinivasa Ramanujan published dozens of innovative new formulae for , remarkable for their elegance, mathematical depth, and rapid convergence. One of his formulae, based on modular equations, is
This series converges much more rapidly than most arctan series, including Machin's formula. Bill Gosper was the first to use it for advances in the calculation of , setting a record of 17 million digits in 1985. Ramanujan's formulae anticipated the modern algorithms developed by the Borwein brothers and the Chudnovsky brothers. The Chudnovsky formula developed in 1987 is
It produces about 14 digits of per term, and has been used for several record-setting calculations, including the first to surpass 1 billion (10) digits in 1989 by the Chudnovsky brothers, 2.7 trillion (2.7×10) digits by Fabrice Bellard in 2009, and 10 trillion (10) digits in 2011 by Alexander Yee and Shigeru Kondo. For similar formulas, see also the Ramanujan–Sato series.

In 2006, Canadian mathematician Simon Plouffe used the PSLQ integer relation algorithm to generate several new formulas for , conforming to the following template:
where is (Gelfond's constant), is an odd number, and are certain rational numbers that Plouffe computed.

Monte Carlo methods, which evaluate the results of multiple random trials, can be used to create approximations of . Buffon's needle is one such technique: If a needle of length is dropped times on a surface on which parallel lines are drawn units apart, and if of those times it comes to rest crossing a line ( > 0), then one may approximate based on the counts:

Another Monte Carlo method for computing is to draw a circle inscribed in a square, and randomly place dots in the square. The ratio of dots inside the circle to the total number of dots will approximately equal .
Another way to calculate using probability is to start with a random walk, generated by a sequence of (fair) coin tosses: independent random variables such that } with equal probabilities. The associated random walk is
so that, for each "n", is drawn from a standard binomial distribution. As "n" varies defines a (discrete) stochastic process. Then can be calculated by
This Monte Carlo method is independent of any relation to circles, and is a consequence of the central limit theorem, discussed above.

These Monte Carlo methods for approximating are very slow compared to other methods, and do not provide any information on the exact number of digits that are obtained. Thus they are never used to approximate when speed or accuracy is desired.

Two algorithms were discovered in 1995 that opened up new avenues of research into . They are called spigot algorithms because, like water dripping from a spigot, they produce single digits of that are not reused after they are calculated. This is in contrast to infinite series or iterative algorithms, which retain and use all intermediate digits until the final result is produced.

American mathematicians Stan Wagon and Stanley Rabinowitz produced a simple spigot algorithm in 1995. Its speed is comparable to arctan algorithms, but not as fast as iterative algorithms.

Another spigot algorithm, the BBP digit extraction algorithm, was discovered in 1995 by Simon Plouffe:
This formula, unlike others before it, can produce any individual hexadecimal digit of without calculating all the preceding digits. Individual binary digits may be extracted from individual hexadecimal digits, and octal digits can be extracted from one or two hexadecimal digits. Variations of the algorithm have been discovered, but no digit extraction algorithm has yet been found that rapidly produces decimal digits. An important application of digit extraction algorithms is to validate new claims of record computations: After a new record is claimed, the decimal result is converted to hexadecimal, and then a digit extraction algorithm is used to calculate several random hexadecimal digits near the end; if they match, this provides a measure of confidence that the entire computation is correct.

Between 1998 and 2000, the distributed computing project PiHex used Bellard's formula (a modification of the BBP algorithm) to compute the quadrillionth (10th) bit of , which turned out to be 0. In September 2010, a Yahoo! employee used the company's Hadoop application on one thousand computers over a 23-day period to compute 256 bits of at the two-quadrillionth (2×10th) bit, which also happens to be zero.

Because is closely related to the circle, it is found in many formulae from the fields of geometry and trigonometry, particularly those concerning circles, spheres, or ellipses. Other branches of science, such as statistics, physics, Fourier analysis, and number theory, also include in some of their important formulae.

 appears in formulae for areas and volumes of geometrical shapes based on circles, such as ellipses, spheres, cones, and tori. Below are some of the more common formulae that involve .

The formulae above are special cases of the volume of the "n"-dimensional ball and the surface area of its boundary, the ("n"−1)-dimensional sphere, given below.

Definite integrals that describe circumference, area, or volume of shapes generated by circles typically have values that involve . For example, an integral that specifies half the area of a circle of radius one is given by:

In that integral the function represents the top half of a circle (the square root is a consequence of the Pythagorean theorem), and the integral computes the area between that half of a circle and the axis.

The trigonometric functions rely on angles, and mathematicians generally use radians as units of measurement. plays an important role in angles measured in radians, which are defined so that a complete circle spans an angle of 2 radians. The angle measure of 180° is equal to radians, and 1° = /180 radians.

Common trigonometric functions have periods that are multiples of ; for example, sine and cosine have period 2, so for any angle and any integer ,

Many of the appearances of in the formulas of mathematics and the sciences have to do with its close relationship with geometry. However, also appears in many natural situations having apparently nothing to do with geometry.

In many applications, it plays a distinguished role as an eigenvalue. For example, an idealized vibrating string can be modelled as the graph of a function on the unit interval , with fixed ends . The modes of vibration of the string are solutions of the differential equation . Here is an associated eigenvalue, which is constrained by Sturm–Liouville theory to take on only certain specific values. It must be positive, since the second derivative is negative definite, so it is convenient to write where is called the wavenumber. Then satisfies the boundary conditions and the differential equation with .

The value is, in fact, the "least" such value of the wavenumber, and is associated with the fundamental mode of vibration of the string. One way to obtain this is by estimating the energy. The energy satisfies an inequality, Wirtinger's inequality for functions, which states that if a function is given such that and and are both square integrable, then the inequality holds:
and the case of equality holds precisely when is a multiple of . So appears as an optimal constant in Wirtinger's inequality, and from this it follows that it is the smallest such wavenumber, using the variational characterization of the eigenvalue. As a consequence, is the smallest singular value of the derivative on the space of functions on vanishing at both endpoints (the Sobolev space formula_32).

The number serves appears in similar eigenvalue problems in higher-dimensional analysis. As mentioned above, it can be characterized via its role as the best constant in the isoperimetric inequality: the area enclosed by a plane Jordan curve of perimeter satisfies the inequality
and equality is clearly achieved for the circle, since in that case and .

Ultimately as a consequence of the isoperimetric inequality, appears in the optimal constant for the critical Sobolev inequality in "n" dimensions, which thus characterizes the role of in many physical phenomena as well, for example those of classical potential theory. In two dimensions, the critical Sobolev inequality is
for "f" a smooth function with compact support in , formula_35 is the gradient of "f", and formula_36 and formula_37 refer respectively to the and -norm. The Sobolev inequality is equivalent to the isoperimetric inequality (in any dimension), with the same best constants.

Wirtinger's inequality also generalizes to higher-dimensional Poincaré inequalities that provide best constants for the Dirichlet energy of an "n"-dimensional membrane. Specifically, is the greatest constant such that
for all convex subsets of of diameter 1, and square-integrable functions "u" on of mean zero. Just as Wirtinger's inequality is the variational form of the Dirichlet eigenvalue problem in one dimension, the Poincaré inequality is the variational form of the Neumann eigenvalue problem, in any dimension.

The constant also appears as a critical spectral parameter in the Fourier transform. This is the integral transform, that takes a complex-valued integrable function on the real line to the function defined as:
There are several different conventions for the Fourier transform, all of which involve a factor of that is placed "somewhere". The appearance of is essential in these formulas, as there is there is no possibility to remove altogether from the Fourier transform and its inverse transform. The definition given above is the most canonical, however, because it describes the unique unitary operator on that is also an algebra homomorphism of to .

The Heisenberg uncertainty principle also contains the number . The uncertainty principle gives a sharp lower bound on the extent to which it is possible to localize a function both in space and in frequency: with our conventions for the Fourier transform,
The physical consequence, about the uncertainty in simultaneous position and momentum observations of a quantum mechanical system, is discussed below. The appearance of in the formulae of Fourier analysis is ultimately a consequence of the Stone–von Neumann theorem, asserting the uniqueness of the Schrödinger representation of the Heisenberg group.

The fields of probability and statistics frequently use the normal distribution as a simple model for complex phenomena; for example, scientists generally assume that the observational error in most experiments follows a normal distribution. The Gaussian function, which is the probability density function of the normal distribution with mean and standard deviation , naturally contains :

For this to be a probability density, the area under the graph of needs to be equal to one. This follows from a change of variables in the Gaussian integral:

which says that the area under the basic bell curve in the figure is equal to the square root of .
The central limit theorem explains the central role of normal distributions, and thus of , in probability and statistics. This theorem is ultimately connected with the spectral characterization of as the eigenvalue associated with the Heisenberg uncertainty principle, and the fact that equality holds in the uncertainty principle only for the Gaussian function. Equivalently, is the unique constant making the Gaussian normal distribution equal to its own Fourier transform. Indeed, according to , the "whole business" of establishing the fundamental theorems of Fourier analysis reduces to the Gaussian integral.

Let be the set of all twice differentiable real functions formula_43 that satisfy the ordinary differential equation formula_44. Then is a two-dimensional real vector space, with two parameters corresponding to a pair of initial conditions for the differential equation. For any formula_45, let formula_46 be the evaluation functional, which associates to each formula_47 the value formula_48 of the function at the real point . Then, for each "t", the kernel of formula_49 is a one-dimensional linear subspace of . Hence formula_50 defines a function from formula_51 from the real line to the real projective line. This function is periodic, and the quantity can be characterized as the period of this map.

The constant appears in the Gauss–Bonnet formula which relates the differential geometry of surfaces to their topology. Specifically, if a compact surface has Gauss curvature "K", then
where is the Euler characteristic, which is an integer. An example is the surface area of a sphere "S" of curvature 1 (so that its radius of curvature, which coincides with its radius, is also 1.) The Euler characteristic of a sphere can be computed from its homology groups and is found to be equal to two. Thus we have
reproducing the formula for the surface area of a sphere of radius 1.

The constant appears in many other integral formulae in topology, in particular, those involving characteristic classes via the Chern–Weil homomorphism.

Vector calculus is a branch of calculus that is concerned with the properties of vector fields, and has many physical applications such as to electricity and magnetism. The Newtonian potential for a point source situated at the origin of a three-dimensional Cartesian coordinate system is
which represents the potential energy of a unit mass (or charge) placed a distance from the source, and is a dimensional constant. The field, denoted here by , which may be the (Newtonian) gravitational field or the (Coulomb) electric field, is the negative gradient of the potential:
Special cases include Coulomb's law and Newton's law of universal gravitation. Gauss' law states that the outward flux of the field through any smooth, simple, closed, orientable surface containing the origin is equal to :

It is standard to absorb this factor of into the constant , but this argument shows why it must appear "somewhere". Furthermore, is the surface area of the unit sphere, but we have not assumed that is the sphere. However, as a consequence of the divergence theorem, because the region away from the origin is vacuum (source-free) it is only the homology class of the surface in that matters in computing the integral, so it can be replaced by any convenient surface in the same homology class, in particular, a sphere, where spherical coordinates can be used to calculate the integral.

A consequence of the Gauss law is that the negative Laplacian of the potential is equal to times the Dirac delta function:
More general distributions of matter (or charge) are obtained from this by convolution, giving the Poisson equation
where is the distribution function.
The constant also plays an analogous role in four-dimensional potentials associated with Einstein's equations, a fundamental formula which forms the basis of the general theory of relativity and describes the fundamental interaction of gravitation as a result of spacetime being curved by matter and energy:
where is the Ricci curvature tensor, is the scalar curvature, is the metric tensor, is the cosmological constant, is Newton's gravitational constant, is the speed of light in vacuum, and is the stress–energy tensor. The left-hand side of Einstein's equation is a non-linear analog of the Laplacian of the metric tensor, and reduces to that in the weak field limit, with the formula_59 term playing the role of a Lagrange multiplier, and the right-hand side is the analog of the distribution function, times .

One of the key tools in complex analysis is contour integration of a function over a positively oriented (rectifiable) Jordan curve . A form of Cauchy's integral formula states that if a point is interior to , then

Although the curve is not a circle, and hence does not have any obvious connection to the constant , a standard proof of this result uses Morera's theorem, which implies that the integral is invariant under homotopy of the curve, so that it can be deformed to a circle and then integrated explicitly in polar coordinates. More generally, it is true that if a rectifiable closed curve does not contain , then the above integral is times the winding number of the curve.

The general form of Cauchy's integral formula establishes the relationship between the values of a complex analytic function on the Jordan curve and the value of at any interior point of :
provided is analytic in the region enclosed by and extends continuously to . Cauchy's integral formula is a special case of the residue theorem, that if is a meromorphic function the region enclosed by and is continuous in a neighborhood of , then
where the sum is of the residues at the poles of .

The factorial function is the product of all of the positive integers through . The gamma function extends the concept of factorial (normally defined only for non-negative integers) to all complex numbers, except the negative real integers. When the gamma function is evaluated at half-integers, the result contains ; for example formula_63 and formula_64.

The gamma function is defined by its Weierstrass product development:
where is the Euler–Mascheroni constant. Evaluated at and squared, the equation reduces to the Wallis product formula. The gamma function is also connected to the Riemann zeta function and identities for the functional determinant, in which the constant plays an important role.

The gamma function is used to calculate the volume of the "n"-dimensional ball of radius "r" in Euclidean "n"-dimensional space, and the surface area of its boundary, the ("n"−1)-dimensional sphere:

Further, it follows from the functional equation that

The gamma function can be used to create a simple approximation to the factorial function for large : formula_69 which is known as Stirling's approximation. Equivalently,

As a geometrical application of Stirling's approximation, let denote the standard simplex in "n"-dimensional Euclidean space, and denote the simplex having all of its sides scaled up by a factor of . Then

Ehrhart's volume conjecture is that this is the (optimal) upper bound on the volume of a convex body containing only one lattice point.

The Riemann zeta function is used in many areas of mathematics. When evaluated at it can be written as
Finding a simple solution for this infinite series was a famous problem in mathematics called the Basel problem. Leonhard Euler solved it in 1735 when he showed it was equal to . Euler's result leads to the number theory result that the probability of two random numbers being relatively prime (that is, having no shared factors) is equal to . This probability is based on the observation that the probability that any number is divisible by a prime is (for example, every 7th integer is divisible by 7.) Hence the probability that two numbers are both divisible by this prime is , and the probability that at least one of them is not is . For distinct primes, these divisibility events are mutually independent; so the probability that two numbers are relatively prime is given by a product over all primes:

This probability can be used in conjunction with a random number generator to approximate using a Monte Carlo approach.

The solution to the Basel problem implies that the geometrically derived quantity is connected in a deep way to the distribution of prime numbers. This is a special case of Weil's conjecture on Tamagawa numbers, which asserts the equality of similar such infinite products of "arithmetic" quantities, localized at each prime "p", and a "geometrical" quantity: the reciprocal of the volume of a certain locally symmetric space. In the case of the Basel problem, it is the hyperbolic 3-manifold .

The zeta function also satisfies Riemann's functional equation, which involves as well as the gamma function:

Furthermore, the derivative of the zeta function satisfies
A consequence is that can be obtained from the functional determinant of the harmonic oscillator. This functional determinant can be computed via a product expansion, and is equivalent to the Wallis product formula. The calculation can be recast in quantum mechanics, specifically the variational approach to the spectrum of the hydrogen atom.

The constant also appears naturally in Fourier series of periodic functions. Periodic functions are functions on the group of fractional parts of real numbers. The Fourier decomposition shows that a complex-valued function on can be written as an infinite linear superposition of unitary characters of . That is, continuous group homomorphisms from to the circle group of unit modulus complex numbers. It is a theorem that every character of is one of the complex exponentials formula_76.

There is a unique character on , up to complex conjugation, that is a group isomorphism. Using the Haar measure on the circle group, the constant is half the magnitude of the Radon–Nikodym derivative of this character. The other characters have derivatives whose magnitudes are positive integral multiples of 2. As a result, the constant is the unique number such that the group T, equipped with its Haar measure, is Pontrjagin dual to the lattice of integral multiples of 2. This is a version of the one-dimensional Poisson summation formula.

The constant is connected in a deep way with the theory of modular forms and theta functions. For example, the Chudnovsky algorithm involves in an essential way the j-invariant of an elliptic curve.

Modular forms are holomorphic functions in the upper half plane characterized by their transformation properties under the modular group formula_77 (or its various subgroups), a lattice in the group formula_78. An example is the Jacobi theta function
which is a kind of modular form called a Jacobi form. This is sometimes written in terms of the nome formula_80.

The constant is the unique constant making the Jacobi theta function an automorphic form, which means that it transforms in a specific way. Certain identities hold for all automorphic forms. An example is
which implies that transforms as a representation under the discrete Heisenberg group. General modular forms and other theta functions also involve , once again because of the Stone–von Neumann theorem.

The Cauchy distribution

is a probability density function. The total probability is equal to one, owing to the integral:

The Shannon entropy of the Cauchy distribution is equal to , which also involves .
The Cauchy distribution plays an important role in potential theory because it is the simplest Furstenberg measure, the classical Poisson kernel associated with a Brownian motion in a half-plane. Conjugate harmonic functions and so also the Hilbert transform are associated with the asymptotics of the Poisson kernel. The Hilbert transform "H" is the integral transform given by the Cauchy principal value of the singular integral

The constant is the unique (positive) normalizing factor such that "H" defines a linear complex structure on the Hilbert space of square-integrable real-valued functions on the real line. The Hilbert transform, like the Fourier transform, can be characterized purely in terms of its transformation properties on the Hilbert space : up to a normalization factor, it is the unique bounded linear operator that commutes with positive dilations and anti-commutes with all reflections of the real line. The constant is the unique normalizing factor that makes this transformation unitary.

An occurrence of in the Mandelbrot set fractal was discovered by David Boll in 1991. He examined the behavior of the Mandelbrot set near the "neck" at (−0.75, 0). If points with coordinates (−0.75, ε) are considered, as ε tends to zero, the number of iterations until divergence for the point multiplied by ε converges to . The point (0.25, ε) at the cusp of the large "valley" on the right side of the Mandelbrot set behaves similarly: the number of iterations until divergence multiplied by the square root of ε tends to .

Although not a physical constant, appears routinely in equations describing fundamental principles of the universe, often because of 's relationship to the circle and to spherical coordinate systems. A simple formula from the field of classical mechanics gives the approximate period of a simple pendulum of length , swinging with a small amplitude ( is the earth's gravitational acceleration):
One of the key formulae of quantum mechanics is Heisenberg's uncertainty principle, which shows that the uncertainty in the measurement of a particle's position (Δ) and momentum (Δ) cannot both be arbitrarily small at the same time (where is Planck's constant):

The fact that is approximately equal to 3 plays a role in the relatively long lifetime of orthopositronium. The inverse lifetime to lowest order in the fine-structure constant is
where is the mass of the electron.

The field of fluid dynamics contains in Stokes' law, which approximates the frictional force exerted on small, spherical objects of radius , moving with velocity in a fluid with dynamic viscosity :

In electromagnetics, the vacuum permeability constant "μ" appears in Maxwell's equations, which describe the properties of electric and magnetic fields and electromagnetic radiation. It is defined as exactly
A relation for the speed of light in vacuum, can be derived from Maxwell's equations in the medium of classical vacuum using a relationship between "μ" and the electric constant (vacuum permittivity), in SI units:

Under ideal conditions (uniform gentle slope on a homogeneously erodible substrate), the sinuosity of a meandering river approaches . The sinuosity is the ratio between the actual length and the straight-line distance from source to mouth. Faster currents along the outside edges of a river's bends cause more erosion than along the inside edges, thus pushing the bends even farther out, and increasing the overall loopiness of the river. However, that loopiness eventually causes the river to double back on itself in places and "short-circuit", creating an ox-bow lake in the process. The balance between these two opposing factors leads to an average ratio of between the actual length and the direct distance between source and mouth.

Piphilology is the practice of memorizing large numbers of digits of , and world-records are kept by the "Guinness World Records". The record for memorizing digits of , certified by Guinness World Records, is 70,000 digits, recited in India by Rajveer Meena in 9 hours and 27 minutes on 21 March 2015. In 2006, Akira Haraguchi, a retired Japanese engineer, claimed to have recited 100,000 decimal places, but the claim was not verified by Guinness World Records.

One common technique is to memorize a story or poem in which the word lengths represent the digits of : The first word has three letters, the second word has one, the third has four, the fourth has one, the fifth has five, and so on. An early example of a memorization aid, originally devised by English scientist James Jeans, is "How I want a drink, alcoholic of course, after the heavy lectures involving quantum mechanics." When a poem is used, it is sometimes referred to as a "piem". Poems for memorizing have been composed in several languages in addition to English. Record-setting memorizers typically do not rely on poems, but instead use methods such as remembering number patterns and the method of loci.

A few authors have used the digits of to establish a new form of constrained writing, where the word lengths are required to represent the digits of . The "Cadaeic Cadenza" contains the first 3835 digits of in this manner, and the full-length book "Not a Wake" contains 10,000 words, each representing one digit of .

Perhaps because of the simplicity of its definition and its ubiquitous presence in formulae, has been represented in popular culture more than other mathematical constructs.

In the 2008 Open University and BBC documentary co-production, "The Story of Maths", aired in October 2008 on BBC Four, British mathematician Marcus du Sautoy shows a visualization of the – historically first exact – formula for calculating when visiting India and exploring its contributions to trigonometry.

In the Palais de la Découverte (a science museum in Paris) there is a circular room known as the "pi room". On its wall are inscribed 707 digits of . The digits are large wooden characters attached to the dome-like ceiling. The digits were based on an 1853 calculation by English mathematician William Shanks, which included an error beginning at the 528th digit. The error was detected in 1946 and corrected in 1949.

In Carl Sagan's novel "Contact" it is suggested that the creator of the universe buried a message deep within the digits of . The digits of have also been incorporated into the lyrics of the song "Pi" from the album "Aerial" by Kate Bush.

In the United States, Pi Day falls on 14 March (written 3/14 in the US style), and is popular among students. and its digital representation are often used by self-described "math geeks" for inside jokes among mathematically and technologically minded groups. Several college cheers at the Massachusetts Institute of Technology include "3.14159". Pi Day in 2015 was particularly significant because the date and time 3/14/15 9:26:53 reflected many more digits of pi.

During the 2011 auction for Nortel's portfolio of valuable technology patents, Google made a series of unusually specific bids based on mathematical and scientific constants, including .
In 1958 Albert Eagle proposed replacing by (tau), where = /2, to simplify formulas. However, no other authors are known to use in this way. Some people use a different value, = 6.283185... = 2, arguing that , as the number of radians in one turn or as the ratio of a circle's circumference to its radius rather than its diameter, is more natural than and simplifies many formulas. Celebrations of this number, because it approximately equals 6.28, by making 28 June "Tau Day" and eating "twice the pie", have been reported in the media. However, this use of has not made its way into mainstream mathematics.

In 1897, an amateur American mathematician attempted to persuade the Indiana legislature to pass the Indiana Pi Bill, which described a method to square the circle and contained text that implied various incorrect values for , including 3.2. The bill is notorious as an attempt to establish a value of scientific constant by legislative fiat. The bill was passed by the Indiana House of Representatives, but rejected by the Senate, meaning it did not become a law.

In contemporary internet culture, individuals and organizations frequently pay homage to the number . For instance, the computer scientist Donald Knuth let the version numbers of his program TeX approach . The versions are 3, 3.1, 3.14, and so forth.

Footnotes
References




</doc>
<doc id="23603" url="https://en.wikipedia.org/wiki?curid=23603" title="Postmodernism">
Postmodernism

Postmodernism is a broad movement that developed in the mid- to late-20th century across philosophy, the arts, architecture, and criticism and that marked a departure from modernism. The term has also more generally been applied to the historical era following modernity and the tendencies of this era. 
While encompassing a wide variety of approaches, postmodernism is generally defined by an attitude of skepticism, irony, or rejection toward the meta-narratives and ideologies of modernism, often calling into question various assumptions of Enlightenment rationality. Consequently, common targets of postmodern critique include universalist notions of objective reality, morality, truth, human nature, reason, language, and social progress. Postmodern thinkers frequently call attention to the contingent or socially-conditioned nature of knowledge claims and value systems, situating them as products of particular political, historical, or cultural discourses and hierarchies. Accordingly, postmodern thought is broadly characterized by tendencies to self-referentiality, epistemological and moral relativism, pluralism, subjectivism, and irreverence.

Postmodern critical approaches gained purchase in the 1980s and 1990s, and have been adopted in a variety of academic and theoretical disciplines, including cultural studies, philosophy of science, economics, linguistics, architecture, feminist theory, and literary criticism, as well as art movements in fields such as literature and music. Postmodernism is often associated with schools of thought such as deconstruction and post-structuralism, as well as philosophers such as Jean-François Lyotard, Jacques Derrida, and Fredric Jameson, though many so-labeled thinkers have criticized the term.

Postmodernism arose after World War II as a reaction to the perceived failings of modernism, whose radical artistic projects had come to be associated with totalitarianism or had been assimilated into mainstream culture. The basic features of what is now called postmodernism can be found as early as the 1940s, most notably in the work of artists such as Jorge Luis Borges. However, most scholars today would agree that postmodernism began to compete with modernism in the late 1950s and gained ascendancy over it in the 1960s. Since then, postmodernism has been a dominant, though not undisputed, force in art, literature, film, music, drama, architecture, history, and continental philosophy. 

Salient features of postmodernism are normally thought to include the ironic play with styles, citations and narrative levels, a metaphysical skepticism or nihilism towards a "grand narrative" of Western culture, a preference for the virtual at the expense of the Real (or more accurately, a fundamental questioning of what 'the real' constitutes) and a "waning of affect" on the part of the subject, who is caught up in the free interplay of virtual, endlessly reproducible signs inducing a state of consciousness similar to schizophrenia.

Since the late 1990s there has been a small but growing feeling both in popular culture and in academia that postmodernism "has gone out of fashion".

Structuralism was a philosophical movement developed by French academics in the 1950s, partly in response to French Existentialism. It has been seen variously as an expression of Modernism, High modernism, or postmodernism . "Post-structuralists" were thinkers who moved away from the strict interpretations and applications of structuralist ideas. Many American academics consider post-structuralism to be part of the broader, less well-defined postmodernist movement, even though many post-structuralists insisted it was not. Thinkers who have been called structuralists include the anthropologist Claude Lévi-Strauss, the linguist Ferdinand de Saussure, the Marxist philosopher Louis Althusser, and the semiotician Algirdas Greimas. The early writings of the psychoanalyst Jacques Lacan and the literary theorist Roland Barthes have also been called structuralist. Those who began as structuralists but became post-structuralists include Michel Foucault, Roland Barthes, Jean Baudrillard, Gilles Deleuze. Other post-structuralists include Jacques Derrida, Pierre Bourdieu, Jean-François Lyotard, Julia Kristeva, Hélène Cixous, and Luce Irigaray. The American cultural theorists, critics and intellectuals whom they influenced include Judith Butler, John Fiske, Rosalind Krauss, Avital Ronell, and Hayden White.

Post-structuralism is not defined by a set of shared axioms or methodologies, but by an emphasis on how various aspects of a particular culture, from its most ordinary, everyday material details to its most abstract theories and beliefs, determine one another. Post-structuralist thinkers reject Reductionism and Epiphenomenalism and the idea that cause-and-effect relationships are top-down or bottom-up. Like structuralists, they start from the assumption that people's identities, values and economic conditions determine each other rather than having "intrinsic" properties that can be understood in isolation. Thus the French structuralists considered themselves to be espousing Relativism and Constructionism. But they nevertheless tended to explore how the subjects of their study might be described, reductively, as a set of "essential" relationships, schematics, or mathematical symbols. (An example is Claude Lévi-Strauss's algebraic formulation of mythological transformation in "The Structural Study of Myth"). Post-structuralists thinkers went further, questioning the existence of any distinction between the nature of a thing and its relationship to other things.

Postmodernist ideas in philosophy and the analysis of culture and society expanded the importance of critical theory and has been the point of departure for works of literature, architecture, and design, as well as being visible in marketing/business and the interpretation of history, law and culture, starting in the late 20th century. These developments—re-evaluation of the entire Western value system (love, marriage, popular culture, shift from industrial to service economy) that took place since the 1950s and 1960s, with a peak in the Social Revolution of 1968—are described with the term "postmodernity", as opposed to "Postmodernism", a term referring to an opinion or movement. Postmodernism has also been used interchangeably with the term post-structuralism out of which postmodernism grew; a proper understanding of postmodernism or doing justice to the postmodernist concept demands an understanding of the post-structuralist movement and the ideas of its advocates. Post-structuralism resulted similarly to postmodernism by following a time of structuralism. It is characterized by new ways of thinking through structuralism, contrary to the original form. "Postmodernist" describes part of a movement; "Postmodern" places it in the period of time since the 1950s, making it a part of contemporary history.

One of the most well-known postmodernist concerns is "deconstruction," a theory for philosophy, literary criticism, and textual analysis developed by Jacques Derrida. The notion of a "deconstructive" approach implies an analysis that questions the already evident understanding of a text in terms of presuppositions, ideological underpinnings, hierarchical values, and frames of reference. A deconstructive approach further depends on the techniques of close reading without reference to cultural, ideological, moral opinions or information derived from an authority over the text such as the author. At the same time Derrida famously writes: "Il n'y a pas d'hors-texte ("there is no such thing as outside-of-the-text")." Derrida implies that the world follows the grammar of a text undergoing its own deconstruction. Derrida's method frequently involves recognizing and spelling out the different, yet similar interpretations of the meaning of a given text and the problematic implications of binary oppositions within the meaning of a text. Derrida's philosophy inspired a postmodern movement called deconstructivism among architects, characterized by the intentional fragmentation, distortion, and dislocation of architectural elements in designing a building. Derrida discontinued his involvement with the movement after the publication of his collaborative project with architect Peter Eisenman in "Chora L Works: Jacques Derrida and Peter Eisenman".

The connection between postmodernism, posthumanism, and cyborgism has led to a challenge of postmodernism, for which the terms "postpostmodernism" and "postpoststructuralism" were first coined in 2003:

More recently metamodernism, post-postmodernism and the "death of postmodernism" have been widely debated: in 2007 Andrew Hoberek noted in his introduction to a special issue of the journal "Twentieth Century Literature" titled "After Postmodernism" that "declarations of postmodernism's demise have become a critical commonplace". A small group of critics has put forth a range of theories that aim to describe culture or society in the alleged aftermath of postmodernism, most notably Raoul Eshelman (performatism), Gilles Lipovetsky (hypermodernity), Nicolas Bourriaud (altermodern), and Alan Kirby (digimodernism, formerly called pseudo-modernism). None of these new theories and labels have so far gained very widespread acceptance. Sociocultural anthropologist Nina Müller-Schwarze offers neostructuralism as a possible direction. The exhibition "Postmodernism – Style and Subversion 1970–1990" at the Victoria and Albert Museum (London, 24 September 2011 – 15 January 2012) was billed as the first show to document postmodernism as a historical movement.

The term "postmodern" was first used around the 1880s. John Watkins Chapman suggested "a Postmodern style of painting" as a way to depart from French Impressionism. J. M. Thompson, in his 1914 article in "The Hibbert Journal" (a quarterly philosophical review), used it to describe changes in attitudes and beliefs in the critique of religion, writing: "The raison d'être of Post-Modernism is to escape from the double-mindedness of Modernism by being thorough in its criticism by extending it to religion as well as theology, to Catholic feeling as well as to Catholic tradition."

In 1921 and 1925, postmodernism had been used to describe new forms of art and music. In 1942 H. R. Hays described it as a new literary form. However, as a general theory for a historical movement it was first used in 1939 by Arnold J. Toynbee: "Our own Post-Modern Age has been inaugurated by the general war of 1914–1918".

In 1949 the term was used to describe a dissatisfaction with modern architecture, and led to the postmodern architecture movement, and a response to the modernist architectural movement known as the International Style. Postmodernism in architecture was initially marked by a re-emergence of surface ornament, reference to surrounding buildings in urban settings, historical reference in decorative forms (eclecticism), and non-orthogonal angles.

Peter Drucker suggested the transformation into a post-modern world happened between 1937 and 1957 (when he was writing). He described an as yet "nameless era" which he characterized as a shift to conceptual world based on pattern, purpose, and process rather than mechanical cause, outlined by four new realities: the emergence of Educated Society, the importance of international development, the decline of the nation state, and the collapse of the viability of non-Western cultures.

In 1971, in a lecture delivered at the Institute of Contemporary Art, London, Mel Bochner described "post-modernism" in art as having started with Jasper Johns, "who first rejected sense-data and the singular point-of-view as the basis for his art, and treated art as a critical investigation".

In 1996, Walter Truett Anderson described postmodernism as belonging to one of four typological world views, which he identifies as either (a) Postmodern-ironist, which sees truth as socially constructed, (b) Scientific-rational, in which truth is found through methodical, disciplined inquiry, (c) Social-traditional, in which truth is found in the heritage of American and Western civilization, or (d) Neo-Romantic, in which truth is found through attaining harmony with nature and/or spiritual exploration of the inner self.

Martin Heidegger rejected the philosophical basis of the concepts of "subjectivity" and "objectivity" and asserted that similar grounding oppositions in logic ultimately refer to one another. Instead of resisting the admission of this paradox in the search for understanding, Heidegger requires that we embrace it through an active process of elucidation he called the "hermeneutic circle". He stressed the historicity and cultural construction of concepts while simultaneously advocating the necessity of an atemporal and immanent apprehension of them. In this vein, he asserted that it was the task of contemporary philosophy to recover the original question of (or "openness to") "Dasein" (translated as Being or Being-there) present in the Presocratic philosophers but normalized, neutered, and standardized since Plato. This was to be done, in part, by tracing the record of "Dasein's" sublimation or forgetfulness through the history of philosophy which meant that we were to ask again what constituted the grounding conditions in ourselves and in the World for the affinity between beings and between the many usages of the term "being" in philosophy. To do this, however, a non-historical and, to a degree, self-referential engagement with whatever set of ideas, feelings or practices would permit (both the non-fixed concept and reality of) such a continuity was required—a continuity permitting the possible experience, possible existence indeed not only of beings but of all differences as they appeared and tended to develop.

Such a conclusion led Heidegger to depart from the phenomenology of his teacher Husserl and prompt instead an (ironically anachronistic) return to the yet-unasked questions of Ontology, a return that in general did not acknowledge an intrinsic distinction between phenomena and noumena or between things in themselves ("de re") and things as they appear (see qualia): Being-in-the-world, or rather, the openness to the process of "Dasein"'s becoming was to bridge the age-old gap between these two. In this latter premise, Heidegger shares an affinity with the late Romantic philosopher, Friedrich Nietzsche, another principal forerunner of post-structuralist and postmodernist thought. Influential to thinkers associated with Postmodernism are Heidegger's critique of the subject–object or sense–knowledge division implicit in Rationalism, Empiricism, and methodological naturalism, his repudiation of the idea that facts exist outside or separately from the process of thinking and speaking them (however, Heidegger is not specifically a nominalist), his related admission that the possibilities of philosophical and scientific discourse are wrapped up in the practices and expectations of a society and that concepts and fundamental constructs are the expression of a lived, historical exercise rather than simple derivations of external, "a priori" conditions independent from historical mind and changing experience (see Johann Gottlieb Fichte, Heinrich von Kleist, Weltanschauung, and social constructionism), and his instrumentalist and negativist notion that Being (and, by extension, reality) is an action, method, tendency, possibility, and question rather than a discrete, positive, identifiable state, answer, or entity (see also process philosophy, dynamism, Instrumentalism, Pragmatism, and Vitalism).

Jacques Derrida re-examined the fundamentals of writing and its consequences on philosophy in general; sought to undermine the language of "presence" or metaphysics in an analytical technique which, beginning as a point of departure from Heidegger's notion of "Destruktion", came to be known as Deconstruction. Derrida used, like Heidegger, references to Greek philosophical notions associated with the Skeptics and the Presocratics, such as Epoché and Aporia to articulate his notion of implicit circularity between premises and conclusions, origins and manifestations, but—in a manner analogous in certain respects to Gilles Deleuze—presented a radical re-reading of canonical philosophical figures such as Plato, Aristotle, and Descartes as themselves being informed by such "destabilizing" notions.

Michel Foucault introduced concepts such as 'discursive regime', or re-invoked those of older philosophers like 'episteme' and 'genealogy' in order to explain the relationship between meaning, power, and social behavior within social orders (see "The Order of Things", "The Archaeology of Knowledge", "Discipline and Punish", and "The History of Sexuality"). In direct contradiction to what have been typified as modernist perspectives on epistemology, Foucault asserted that rational judgment, social practice, and what he called "biopower" are not only inseparable but co-determinant. While Foucault himself was deeply involved in a number of progressive political causes and maintained close personal ties with members of the far-left, he was also controversial with leftist thinkers of his day, including those associated with various Marxist tendencies, proponents of left-libertarianism (such as Noam Chomsky), and supporters of humanism (like Jürgen Habermas), for his rejection of what he deemed to be Enlightenment concepts of freedom, liberation, self-determination, and human nature. Instead, Foucault focused on the ways in which such constructs can foster cultural hegemony, violence, and exclusion.

In line with his rejection of such "positive" tenets of Enlightenment-era humanism, he was active—with Gilles Deleuze and Félix Guattari—in the anti-psychiatry movement, considering much of institutionalized psychiatry and, in particular, Freud's concept of repression central to Psychoanalysis (which was still very influential in France during the 1960s and 1970s), to be both harmful and misplaced. Foucault was known for his controversial aphorisms, such as "language is oppression", meaning that language functions in such a way as to render nonsensical, false, or silent tendencies that might otherwise threaten or undermine the distributions of power backing a society's conventions—even when such distributions purport to celebrate liberation and expression or value minority groups and perspectives. His writings have had a major influence on the larger body of postmodern academic literature.

Jean-François Lyotard identified in "The Postmodern Condition" a crisis in the "discourses of the human sciences" latent in modernism but catapulted to the fore by the advent of the "computerized" or "telematic" era (see information revolution). This crisis, insofar as it pertains to academia, concerns both the motivations and justification procedures for making research claims: unstated givens or values that have validated the basic efforts of academic research since the late 18th century might no longer be valid—particularly, in social science and humanities research, though examples from mathematics are given by Lyotard as well. As formal conjecture about real-world issues becomes inextricably linked to automated calculation, information storage, and retrieval, such knowledge becomes increasingly "exteriorised" from its knowers in the form of information. Knowledge thus becomes materialized and made into a commodity exchanged between producers and consumers; it ceases to be either an idealistic end-in-itself or a tool capable of bringing about liberty or social benefit; it is stripped of its humanistic and spiritual associations, its connection with education, teaching, and human development, being simply rendered as "data"—omnipresent, material, unending, and without any contexts or pre-requisites. Furthermore, the "diversity" of claims made by various disciplines begins to lack any unifying principle or intuition as objects of study become more and more specialized due to the emphasis on specificity, precision, and uniformity of reference that competitive, database-oriented research implies.

The value-premises upholding academic research have been maintained by what Lyotard considers to be quasi-mythological beliefs about human purpose, human reason, and human progress—large, background constructs he calls "metanarratives". These metanarratives still remain in Western society but are now being undermined by rapid Informatization and the commercialization of the university and its functions. The shift of authority from the presence and intuition of knowers—from the good faith of reason to seek diverse knowledge integrated for human benefit or truth fidelity—to the automated database and the market had, in Lyotard's view, the power to unravel the very idea of "justification" or "legitimation" and, with it, the rationale for research altogether, especially in disciplines pertaining to human life, society, and meaning. We are now controlled not by binding extra-linguistic value paradigms defining notions of collective identity and ultimate purpose, but rather by our automatic responses to different species of "language games" (a concept Lyotard imports from J. L. Austin's theory of speech acts). In his vision of a solution to this "vertigo", Lyotard opposes the assumptions of universality, consensus, and generality that he identified within the thought of humanistic, Neo-Kantian philosophers like Jürgen Habermas, and proposes a continuation of experimentation and diversity to be assessed pragmatically in the context of language games rather than via appeal to a resurrected series of transcendentals and metaphysical unities.

Richard Rorty argues in "Philosophy and the Mirror of Nature" that contemporary analytic philosophy mistakenly imitates scientific methods. In addition, he denounces the traditional epistemological perspectives of representationalism and correspondence theory that rely upon the independence of knowers and observers from phenomena and the passivity of natural phenomena in relation to consciousness. As a proponent of anti-foundationalism and anti-essentialism within a pragmatist framework, he echoes the postmodern strain of conventionalism and relativism, but opposes much of postmodern thinking with his commitment to social liberalism.

Jean Baudrillard, in "Simulacra and Simulation", introduced the concept that reality or the principle of "The Real" is short-circuited by the interchangeability of signs in an era whose communicative and semantic acts are dominated by electronic media and digital technologies. Baudrillard proposes the notion that, in such a state, where subjects are detached from the outcomes of events (political, literary, artistic, personal, or otherwise), events no longer hold any particular sway on the subject nor have any identifiable context; they therefore have the effect of producing widespread indifference, detachment, and passivity in industrialized populations. He claimed that a constant stream of appearances and references without any direct consequences to viewers or readers could eventually render the division between appearance and object indiscernible, resulting, ironically, in the "disappearance" of mankind in what is, in effect, a virtual or holographic state, composed only of appearances. For Baudrillard, "simulation is no longer that of a territory, a referential being or a substance. It is the generation by models of a real without origin or a reality: a hyperreal.

Fredric Jameson set forth one of the first expansive theoretical treatments of postmodernism as a historical period, intellectual trend, and social phenomenon in a series of lectures at the Whitney Museum, later expanded as "Postmodernism, or The Cultural Logic of Late Capitalism" (1991). Eclectic in his methodology, Jameson has continued a sustained examination of the role that periodization continues to play as a grounding assumption of critical methodologies in humanities disciplines. He has contributed extensive effort to explicating the importance of concepts of Utopia and Utopianism as driving forces in the cultural and intellectual movements of modernity, and outlining the political and existential uncertainties that may result from the decline or suspension of this trend in the theorized state of postmodernity. Like Susan Sontag, Jameson served to introduce a wide audience of American readers to key figures of the 20th century continental European intellectual left, particularly those associated with the Frankfurt School, structuralism, and post-structuralism. Thus, his importance as a "translator" of their ideas to the common vocabularies of a variety of disciplines in the Anglo-American academic complex is equally as important as his own critical engagement with them.

In "Analysis of the Journey", a journal birthed from postmodernism, Douglas Kellner insists that the "assumptions and procedures of modern theory" must be forgotten. His terms defined in the depth of postmodernism are based on advancement, innovation, and adaptation. Extensively, Kellner analyzes the terms of this theory in real-life experiences and examples. Kellner used science and technology studies as a major part of his analysis; he urged that the theory is incomplete without it. The scale was larger than just postmodernism alone; it must be interpreted through cultural studies where science and technology studies play a huge role. The reality of the September 11 attacks on the United States of America is the catalyst for his explanation. This catalyst is used as a great representation due to the mere fact of the planned ambush and destruction of "symbols of globalization", insinuating the World Trade Center.

The conclusion he depicts is simple: postmodernism, as most use it today, will decide what experiences and signs in one's reality will be one's reality as they know it.

The idea of Postmodernism in architecture began as a response to the perceived blandness and failed Utopianism of the Modern movement. Modern Architecture, as established and developed by Walter Gropius and Le Corbusier, was focused on the pursuit of a perceived ideal perfection, and attempted harmony of form and function, and dismissal of "frivolous ornament," as well as arguing for an architecture that represented the spirit of the age as depicted in cutting-edge technology, be it airplanes, cars, ocean liners or even supposedly artless grain silos. Critics of modernism argued that the attributes of perfection and minimalism themselves were subjective, and pointed out anachronisms in modern thought and questioned the benefits of its philosophy. Definitive postmodern architecture such as the work of Michael Graves and Robert Venturi rejects the notion of a 'pure' form or 'perfect' architectonic detail, instead conspicuously drawing from all methods, materials, forms and colors available to architects.

Modernist Ludwig Mies van der Rohe is associated with the phrase "less is more"; in response Venturi famously said, "Less is a bore." 

The intellectual scholarship regarding postmodernism and architecture is closely linked with the writings of critic-turned-architect Charles Jencks, beginning with lectures in the early 1970s and his essay "The rise of post-modern architecture" from 1975. His "magnum opus", however, is the book "The Language of Post-Modern Architecture", first published in 1977, and since running to seven editions. Jencks makes the point that Post-Modernism (like Modernism) varies for each field of art, and that for architecture it is not just a reaction to Modernism but what he terms "double coding": "Double Coding: the combination of Modern techniques with something else (usually traditional building) in order for architecture to communicate with the public and a concerned minority, usually other architects." Furthermore, Post-Modern architects would for economic reasons be compelled to make use of contemporary technology, hence distinguishing such architects from mere revivalists. Among the Post-Modern architects championed by Jencks were Robert Venturi, Robert Stern, Charles Moore, Michael Graves, Leon Krier, and James Stirling.

Postmodernism is a rejection of 'totality', of the notion that planning could be 'comprehensive', widely applied regardless of context, and rational. In this sense, Postmodernism is a rejection of its predecessor: Modernism. From the 1920s onwards, the Modern movement sought to design and plan cities which followed the logic of the new model of industrial mass production; reverting to large-scale solutions, aesthetic standardisation and prefabricated design solutions (Goodchild 1990). Postmodernism also brought a break from the notion that planning and architecture could result in social reform, which was an integral dimension of the plans of Modernism (Simonsen 1990). Furthermore, Modernism eroded urban living by its failure to recognise differences and aim towards homogenous landscapes (Simonsen 1990, 57). Within Modernism, urban planning represented a 20th-century move towards establishing something stable, structured, and rationalised within what had become a world of chaos, flux and change (Irving 1993, 475). The role of planners predating Postmodernism was one of the 'qualified professional' who believed they could find and implement one single 'right way' of planning new urban establishments (Irving 1993). In fact, after 1945, urban planning became one of the methods through which capitalism could be managed and the interests of developers and corporations could be administered (Irving 1993, 479).

Considering Modernism inclined urban planning to treat buildings and developments as isolated, unrelated parts of the overall urban ecosystems created fragmented, isolated, and homogeneous urban landscapes (Goodchild, 1990). One of the greater problems with Modernist-style of planning was the disregard of resident or public opinion, which resulted in planning being forced upon the majority by a minority consisting of affluent professionals with little to no knowledge of real 'urban' problems characteristic of post-Second World War urban environments: slums, overcrowding, deteriorated infrastructure, pollution and disease, among others (Irving 1993). These were precisely the 'urban ills' Modernism was meant to 'solve', but more often than not, the types of 'comprehensive', 'one size fits all' approaches to planning made things worse., and residents began to show interest in becoming involved in decisions which had once been solely entrusted to professionals of the built environment. Advocacy planning and participatory models of planning emerged in the 1960s to counter these traditional elitist and technocratic approaches to urban planning (Irving 1993; Hatuka & D'Hooghe 2007). Furthermore, an assessment of the 'ills' of Modernism among planners during the 1960s, fuelled development of a participatory model that aimed to expand the range of participants in urban interventions (Hatuka & D'Hooghe 2007, 21).

Jane Jacobs' 1961 book "The Death and Life of Great American Cities" was a sustained critique of urban planning as it had developed within Modernism and marked a transition from modernity to postmodernity in thinking about urban planning (Irving 1993, 479). However, the transition from Modernism to Postmodernism is often said to have happened at 3:32pm on 15 July in 1972, when Pruitt Igoe; a housing development for low-income people in St. Louis designed by architect Minoru Yamasaki, which had been a prize-winning version of Le Corbusier's 'machine for modern living' was deemed uninhabitable and was torn down (Irving 1993, 480). Since then, Postmodernism has involved theories that embrace and aim to create diversity, and it exalts uncertainty, flexibility and change (Hatuka & D'Hooghe 2007). Postmodern planning aims to accept pluralism and heighten awareness of social differences in order to accept and bring to light the claims of minority and disadvantaged groups (Goodchild 1990). It is important to note that urban planning discourse within Modernity and Postmodernity has developed in different contexts, even though they both grew within a capitalist culture. Modernity was shaped by a capitalist ethic of Fordist-Keynesian paradigm of mass, standardized production and consumption, while postmodernity was created out of a more flexible form of capital accumulation, labor markets and organisations (Irving 1993, 60). Also, there is a distinction between a postmodernism of 'reaction' and one of 'resistance'. A postmodernism of 'reaction' rejects Modernism and seeks to return to the lost traditions and history in order to create a new cultural synthesis, while Postmodernity of 'resistance' seeks to deconstruct Modernism and is a critique of the origins without necessarily returning to them (Irving 1993, 60). As a result of Postmodernism, planners are much less inclined to lay a firm or steady claim to there being one single 'right way' of engaging in urban planning and are more open to different styles and ideas of 'how to plan' (Irving 474).

Literary postmodernism was officially inaugurated in the United States with the first issue of "boundary 2", subtitled "Journal of Postmodern Literature and Culture", which appeared in 1972. David Antin, Charles Olson, John Cage, and the Black Mountain College school of poetry and the arts were integral figures in the intellectual and artistic exposition of postmodernism at the time. "boundary 2" remains an influential journal in postmodernist circles today.

Jorge Luis Borges' (1939) short story "Pierre Menard, Author of the Quixote", is often considered as predicting postmodernism and conceiving the ideal of the ultimate parody. Samuel Beckett is sometimes seen as an important precursor and influence. Novelists who are commonly connected with postmodern literature include Vladimir Nabokov, William Gaddis, Umberto Eco, John Hawkes, William S. Burroughs, Giannina Braschi, Kurt Vonnegut, John Barth, Jean Rhys, Donald Barthelme, E.L. Doctorow, Richard Kalich, Jerzy Kosinski, Don DeLillo, Thomas Pynchon (Pynchon's work has also been described as "high modern"), Ishmael Reed, Kathy Acker, Ana Lydia Vega, Jachym Topol and Paul Auster.

In 1971, the Arab-American scholar Ihab Hassan published "The Dismemberment of Orpheus: Toward a Postmodern Literature," an early work of literary criticism from a postmodern perspective, in which the author traces the development of what he calls "literature of silence" through Marquis de Sade, Franz Kafka, Ernest Hemingway, Beckett, and many others, including developments such as the Theatre of the Absurd and the nouveau roman. In 'Postmodernist Fiction' (1987), Brian McHale details the shift from modernism to postmodernism, arguing that the former is characterized by an epistemological dominant, and that postmodern works have developed out of modernism and are primarily concerned with questions of ontology. In "Constructing Postmodernism" (1992), McHale's second book, he provides readings of postmodern fiction and of some of the contemporary writers who go under the label of cyberpunk. McHale's "What Was Postmodernism?" (2007), follows Raymond Federman's lead in now using the past tense when discussing postmodernism.

Postmodern music is either music of the postmodern era, or music that follows aesthetic and philosophical trends of postmodernism. As the name suggests, the postmodernist movement formed partly in reaction to the ideals of the modernist. Because of this, postmodern music is mostly defined in opposition to modernist music, and a work can either be modernist, or postmodern, but not both. Jonathan Kramer posits the idea (following Umberto Eco and Jean-François Lyotard) that postmodernism (including "musical" postmodernism) is less a surface style or historical period (i.e., condition) than an "attitude".

The postmodern impulse in classical music arose in the 1960s with the advent of musical minimalism. Composers such as Terry Riley, Henryk Górecki, Bradley Joseph, John Adams, Steve Reich, Philip Glass, Michael Nyman, and Lou Harrison reacted to the perceived elitism and dissonant sound of atonal academic modernism by producing music with simple textures and relatively consonant harmonies, whilst others, most notably John Cage challenged the prevailing narratives of beauty and objectivity common to Modernism. Some composers have been openly influenced by popular music and world ethnic musical traditions.

Postmodern classical music as well is not a musical "style", but rather refers to music of the postmodern era. It bears the same relationship to postmodernist music that postmodernity bears to postmodernism. Postmodern music, on the other hand, shares characteristics with postmodernist art—that is, art that comes "after" and reacts "against" modernism.

Though representing a general return to certain notions of music-making that are often considered to be classical or romantic, not all postmodern composers have eschewed the experimentalist or academic tenets of modernism. The works of Dutch composer Louis Andriessen, for example, exhibit experimentalist preoccupation that is decidedly anti-romantic. Eclecticism and freedom of expression, in reaction to the rigidity and aesthetic limitations of modernism, are the hallmarks of the postmodern influence in musical composition.

Author on postmodernism, Dominic Strinati, has noted, it is also important "to include in this category the so-called 'art rock' musical innovations and mixing of styles associated with groups like Talking Heads, and performers like Laurie Anderson, together with the self-conscious 'reinvention of disco' by the Pet Shop Boys".

Graphic design in the postmodern age brought forth ideas that challenged the orderly feel of modernism. Graphic designers created works beginning in the 1970s without any set adherence to rational order and formal organization. Designers began experimenting with how shapes, forms and typography could react with one another effectively and interestingly in a less rigid way even if the design was rendered illegible. Some graphic design styles that emerged in the postmodernist era were New Wave Typography, retro and vernacular design, playful design inspired by the Italian Memphis Group, punk rock styles and explorative digital design from the late 1980's. Another characteristic of postmodern graphic design is that "retro, techno, punk, grunge, beach, parody, and pastiche were all conspicuous trends. Each had its own sites and venues, detractors and advocates".

Yet, while postmodern design did not consist of one unified graphic style, the movement was an expressive and playful time for designers who searched for more and more ways to go against the system. Postmodernism did not seek rules but only creative solutions and innovative ideas. The clean orderly grid-based designs of International Typographic Style were interrupted for more exploration and innovation in color, composition, visual communication, and typography. Key influential postmodern graphic designers include Wolfgang Weingart, April Greiman, Jayme Odgers, Tibor Kalman, Dan Friedman, Paula Scher, Neville Brody, Michael Vanderbyl and Jamie Reid.

Criticisms of postmodernism are intellectually diverse, including the assertions that postmodernism is meaningless and promotes obscurantism. For example, Noam Chomsky has argued that postmodernism is meaningless because it adds nothing to analytical or empirical knowledge. He asks why postmodernist intellectuals do not respond like people in other fields when asked, "what are the principles of their theories, on what evidence are they based, what do they explain that wasn't already obvious, etc.?...If [these requests] can't be met, then I'd suggest recourse to Hume's advice in similar circumstances: 'to the flames'."

Christian philosopher William Lane Craig has noted "The idea that we live in a postmodern culture is a myth. In fact, a postmodern culture is an impossibility; it would be utterly unliveable. People are not relativistic when it comes to matters of science, engineering, and technology; rather, they are relativistic and pluralistic in matters of religion and ethics. But, of course, that's not postmodernism; that's modernism!"

Formal, academic critiques of postmodernism can also be found in works such as "Beyond the Hoax" and "Fashionable Nonsense".

However, as for continental philosophy, American academics have tended to label it "postmodernist", especially practitioners of "French Theory". Such a trend might derive from U.S. departments of Comparative Literature. It is interesting to note that Félix Guattari, often considered a "postmodernist", rejected its theoretical assumptions by arguing that the structuralist and postmodernist visions of the world were not flexible enough to seek explanations in psychological, social and environmental domains at the same time.

Analytic philosopher Daniel Dennett declared, "Postmodernism, the school of 'thought' that proclaimed 'There are no truths, only interpretations' has largely played itself out in absurdity, but it has left behind a generation of academics in the humanities disabled by their distrust of the very idea of truth and their disrespect for evidence, settling for 'conversations' in which nobody is wrong and nothing can be confirmed, only asserted with whatever style you can muster."

Daniel A. Farber and Suzanna Sherry criticised Postmodernism for reducing the complexity of the modern world to an expression of power and for undermining truth and reason: "If the modern era begins with the European Enlightenment, the postmodern era that captivates the radical multiculturalists begins with its rejection. According to the new radicals, the Enlightenment-inspired ideas that have previously structured our world, especially the legal and academic parts of it, are a fraud perpetrated and perpetuated by white males to consolidate their own power. Those who disagree are not only blind but bigoted. The Enlightenment's goal of an objective and reasoned basis for knowledge, merit, truth, justice, and the like is an impossibility: "objectivity," in the sense of standards of judgment that transcend individual perspectives, does not exist. Reason is just another code word for the views of the privileged. The Enlightenment itself merely replaced one socially constructed view of reality with another, mistaking power for knowledge. There is naught but power."

H. Sidky pointed out what he sees as several "inherent flaws" of a postmodern antiscience perspective, including the confusion of the authority of science (evidence) with the scientist conveying the knowledge; its self-contradictory claim that all truths are relative; and its strategic ambiguity. He sees 21st-century anti-scientific and pseudo-scientific approaches to knowledge, particularly in the United States, as rooted in a postmodernist "decades-long academic assault on science:" "Many of those indoctrinated in postmodern anti-science went on to become conservative political and religious leaders, policymakers, journalists, journal editors, judges, lawyers, and members of city councils and school boards. Sadly, they forgot the lofty ideals of their teachers, except that science is bogus."







</doc>
<doc id="23604" url="https://en.wikipedia.org/wiki?curid=23604" title="Photography">
Photography

Photography is the science, art, application and practice of creating durable images by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film. Photography is employed in many fields of science, manufacturing (e.g., photolithography), and business, as well as its more direct uses for art, film and video production, recreational purposes, hobby, and mass communication. 

Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically "developed" into a visible image, either negative or positive depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing.

The word "photography" was created from the Greek roots φωτός ("phōtos"), genitive of φῶς ("phōs"), "light" and γραφή ("graphé") "representation by means of lines" or "drawing", together meaning "drawing with light".

Several people may have coined the same new term from these roots independently. Hercules Florence, a French painter and inventor living in Campinas, Brazil, used the French form of the word, "photographie", in private notes which a Brazilian historian believes were written in 1834. This claim is widely reported but apparently has never been independently confirmed as beyond reasonable doubt.

The German newspaper "Vossische Zeitung" of 25 February 1839 contained an article entitled "Photographie", discussing several priority claims – especially Henry Fox Talbot's – regarding Daguerre's claim of invention. The article is the earliest known occurrence of the word in public print. It was signed "J.M.", believed to have been Berlin astronomer Johann von Maedler. 

Credit has traditionally been given to Sir John Herschel both for coining the word and for introducing it to the public. His uses of it in private correspondence prior to 25 February 1839 and at his Royal Society lecture on the subject in London on 14 March 1839 have long been amply documented and accepted as settled facts.

The inventors Nicéphore Niépce, Henry Fox Talbot and Louis Daguerre seem not to have known or used the word "photography", but referred to their processes as "Heliography" (Niépce), "Photogenic Drawing"/"Talbotype"/"Calotype" (Talbot) and "Daguerreotype" (Daguerre).

Photography is the result of combining several technical discoveries. Long before the first photographs were made, ancient Han Chinese philosopher Mo Di from the Mohist School of Logic was the first to discover and develop the scientific principles of optics, camera obscura, and pinhole camera. Later Greek mathematicians Aristotle and Euclid also independently described a pinhole camera in the 5th and 4th centuries BCE. In the 6th century CE, Byzantine mathematician Anthemius of Tralles used a type of camera obscura in his experiments. Both the Han Chinese polymath Shen Kuo (1031–95) and Arab physicist Ibn al-Haytham (Alhazen) (965–1040) independently invented the camera obscura and pinhole camera, Albertus Magnus (1193–1280) discovered silver nitrate, and Georg Fabricius (1516–71) discovered silver chloride. Shen Kuo explains the science of camera obscura and optical physics in his scientific work Dream Pool Essays while the techniques described in Ibn al-Haytham's Book of Optics are capable of producing primitive photographs using medieval materials.

Daniele Barbaro described a diaphragm in 1566. Wilhelm Homberg described how light darkened some chemicals (photochemical effect) in 1694. The fiction book "Giphantie", published in 1760, by French author Tiphaigne de la Roche, described what can be interpreted as photography.

The discovery of the camera obscura that provides an image of a scene dates back to ancient China. Leonardo da Vinci mentions natural camera obscura that are formed by dark caves on the edge of a sunlit valley. A hole in the cave wall will act as a pinhole camera and project a laterally reversed, upside down image on a piece of paper. So the birth of photography was primarily concerned with inventing means to capture and keep the image produced by the camera obscura.

Renaissance painters used the camera obscura which, in fact, gives the optical rendering in color that dominates Western Art. The camera obscura literally means "dark chamber" in Latin. It is a box with a hole in it which allows light to go through and create an image onto the piece of paper.

Around the year 1800, British inventor Thomas Wedgwood made the first known attempt to capture the image in a camera obscura by means of a light-sensitive substance. He used paper or white leather treated with silver nitrate. Although he succeeded in capturing the shadows of objects placed on the surface in direct sunlight, and even made shadow copies of paintings on glass, it was reported in 1802 that "the images formed by means of a camera obscura have been found too faint to produce, in any moderate time, an effect upon the nitrate of silver." The shadow images eventually darkened all over.

The first permanent photoetching was an image produced in 1822 by the French inventor Nicéphore Niépce, but it was destroyed in a later attempt to make prints from it. Niépce was successful again in 1825. In 1826 or 1827, he made the "View from the Window at Le Gras", the earliest surviving photograph from nature (i.e., of the image of a real-world scene, as formed in a camera obscura by a lens).

Because Niépce's camera photographs required an extremely long exposure (at least eight hours and probably several days), he sought to greatly improve his bitumen process or replace it with one that was more practical. In partnership with Louis Daguerre, he worked out post-exposure processing methods that produced visually superior results and replaced the bitumen with a more light-sensitive resin, but hours of exposure in the camera were still required. With an eye to eventual commercial exploitation, the partners opted for total secrecy.

Niépce died in 1833 and Daguerre then redirected the experiments toward the light-sensitive silver halides, which Niépce had abandoned many years earlier because of his inability to make the images he captured with them light-fast and permanent. Daguerre's efforts culminated in what would later be named the daguerreotype process. The essential elements—a silver-plated surface sensitized by iodine vapor, developed by mercury vapor, and "fixed" with hot saturated salt water—were in place in 1837. The required exposure time was measured in minutes instead of hours. Daguerre took the earliest confirmed photograph of a person in 1838 while capturing a view of a Paris street: unlike the other pedestrian and horse-drawn traffic on the busy boulevard, which appears deserted, one man having his boots polished stood sufficiently still throughout the several-minutes-long exposure to be visible. The existence of Daguerre's process was publicly announced, without details, on 7 January 1839. The news created an international sensation. France soon agreed to pay Daguerre a pension in exchange for the right to present his invention to the world as the gift of France, which occurred when complete working instructions were unveiled on 19 August 1839. In that same year, American photographer Robert Cornelius is credited with taking the earliest surviving photographic self-portrait.

In Brazil, Hercules Florence had apparently started working out a silver-salt-based paper process in 1832, later naming it "Photographie".

Meanwhile, a British inventor, William Fox Talbot, had succeeded in making crude but reasonably light-fast silver images on paper as early as 1834 but had kept his work secret. After reading about Daguerre's invention in January 1839, Talbot published his hitherto secret method and set about improving on it. At first, like other pre-daguerreotype processes, Talbot's paper-based photography typically required hours-long exposures in the camera, but in 1840 he created the calotype process, which used the chemical development of a latent image to greatly reduce the exposure needed and compete with the daguerreotype. In both its original and calotype forms, Talbot's process, unlike Daguerre's, created a translucent negative which could be used to print multiple positive copies; this is the basis of most modern chemical photography up to the present day, as Daguerreotypes could only be replicated by rephotographing them with a camera. Talbot's famous tiny paper negative of the Oriel window in Lacock Abbey, one of a number of camera photographs he made in the summer of 1835, may be the oldest camera negative in existence.

British chemist John Herschel made many contributions to the new field. He invented the cyanotype process, later familiar as the "blueprint". He was the first to use the terms "photography", "negative" and "positive". He had discovered in 1819 that sodium thiosulphate was a solvent of silver halides, and in 1839 he informed Talbot (and, indirectly, Daguerre) that it could be used to "fix" silver-halide-based photographs and make them completely light-fast. He made the first glass negative in late 1839.
In the March 1851 issue of "The Chemist", Frederick Scott Archer published his wet plate collodion process. It became the most widely used photographic medium until the gelatin dry plate, introduced in the 1870s, eventually replaced it. There are three subsets to the collodion process; the Ambrotype (a positive image on glass), the Ferrotype or Tintype (a positive image on metal) and the glass negative, which was used to make positive prints on albumen or salted paper.

Many advances in photographic glass plates and printing were made during the rest of the 19th century. In 1891, Gabriel Lippmann introduced a process for making natural-color photographs based on the optical phenomenon of the interference of light waves. His scientifically elegant and important but ultimately impractical invention earned him the Nobel Prize in Physics in 1908.

Glass plates were the medium for most original camera photography from the late 1850s until the general introduction of flexible plastic films during the 1890s. Although the convenience of the film greatly popularized amateur photography, early films were somewhat more expensive and of markedly lower optical quality than their glass plate equivalents, and until the late 1910s they were not available in the large formats preferred by most professional photographers, so the new medium did not immediately or completely replace the old. Because of the superior dimensional stability of glass, the use of plates for some scientific applications, such as astrophotography, continued into the 1990s, and in the niche field of laser holography, it has persisted into the 2010s.

Hurter and Driffield began pioneering work on the light sensitivity of photographic emulsions in 1876. Their work enabled the first quantitative measure of film speed to be devised.

The first flexible photographic roll film was marketed by George Eastman in 1885, but this original "film" was actually a coating on a paper base. As part of the processing, the image-bearing layer was stripped from the paper and transferred to a hardened gelatin support. The first transparent plastic roll film followed in 1889. It was made from highly flammable nitrocellulose ("celluloid"), now usually called "nitrate film".

Although cellulose acetate or "safety film" had been introduced by Kodak in 1908, at first it found only a few special applications as an alternative to the hazardous nitrate film, which had the advantages of being considerably tougher, slightly more transparent, and cheaper. The changeover was not completed for X-ray films until 1933, and although safety film was always used for 16 mm and 8 mm home movies, nitrate film remained standard for theatrical 35 mm motion pictures until it was finally discontinued in 1951.

Films remained the dominant form of photography until the early 21st century when advances in digital photography drew consumers to digital formats. Although modern photography is dominated by digital users, film continues to be used by enthusiasts and professional photographers. The distinctive "look" of film based photographs compared to digital images is likely due to a combination of factors, including: (1) differences in spectral and tonal sensitivity (S-shaped density-to-exposure (H&D curve) with film vs. linear response curve for digital CCD sensors) (2) resolution and (3) continuity of tone.

Originally, all photography was monochrome, or "black-and-white". Even after color film was readily available, black-and-white photography continued to dominate for decades, due to its lower cost and its "classic" photographic look. The tones and contrast between light and dark areas define black-and-white photography. It is important to note that monochromatic pictures are not necessarily composed of pure blacks, whites, and intermediate shades of gray but can involve shades of one particular hue depending on the process. The cyanotype process, for example, produces an image composed of blue tones. The albumen print process first used more than years ago, produces brownish tones.

Many photographers continue to produce some monochrome images, sometimes because of the established archival permanence of well-processed silver-halide-based materials. Some full-color digital images are processed using a variety of techniques to create black-and-white results, and some manufacturers produce digital cameras that exclusively shoot monochrome. Monochrome printing or electronic display can be used to salvage certain photographs taken in color which are unsatisfactory in their original form; sometimes when presented as black-and-white or single-color-toned images they are found to be more effective. Although color photography has long predominated, monochrome images are still produced, mostly for artistic reasons. Almost all digital cameras have an option to shoot in monochrome, and almost all image editing software can combine or selectively discard RGB color channels to produce a monochrome image from one shot in color.

Color photography was explored beginning in the 1840s. Early experiments in color required extremely long exposures (hours or days for camera images) and could not "fix" the photograph to prevent the color from quickly fading when exposed to white light.

The first permanent color photograph was taken in 1861 using the three-color-separation principle first published by Scottish physicist James Clerk Maxwell in 1855. The foundation of virtually all practical color processes, Maxwell's idea was to take three separate black-and-white photographs through red, green and blue filters. This provides the photographer with the three basic channels required to recreate a color image. Transparent prints of the images could be projected through similar color filters and superimposed on the projection screen, an additive method of color reproduction. A color print on paper could be produced by superimposing carbon prints of the three images made in their complementary colors, a subtractive method of color reproduction pioneered by Louis Ducos du Hauron in the late 1860s.

Russian photographer Sergei Mikhailovich Prokudin-Gorskii made extensive use of this color separation technique, employing a special camera which successively exposed the three color-filtered images on different parts of an oblong plate. Because his exposures were not simultaneous, unsteady subjects exhibited color "fringes" or, if rapidly moving through the scene, appeared as brightly colored ghosts in the resulting projected or printed images.

Implementation of color photography was hindered by the limited sensitivity of early photographic materials, which were mostly sensitive to blue, only slightly sensitive to green, and virtually insensitive to red. The discovery of dye sensitization by photochemist Hermann Vogel in 1873 suddenly made it possible to add sensitivity to green, yellow and even red. Improved color sensitizers and ongoing improvements in the overall sensitivity of emulsions steadily reduced the once-prohibitive long exposure times required for color, bringing it ever closer to commercial viability.

Autochrome, the first commercially successful color process, was introduced by the Lumière brothers in 1907. Autochrome plates incorporated a mosaic color filter layer made of dyed grains of potato starch, which allowed the three color components to be recorded as adjacent microscopic image fragments. After an Autochrome plate was reversal processed to produce a positive transparency, the starch grains served to illuminate each fragment with the correct color and the tiny colored points blended together in the eye, synthesizing the color of the subject by the additive method. Autochrome plates were one of several varieties of additive color screen plates and films marketed between the 1890s and the 1950s.

Kodachrome, the first modern "integral tripack" (or "monopack") color film, was introduced by Kodak in 1935. It captured the three color components in a multi-layer emulsion. One layer was sensitized to record the red-dominated part of the spectrum, another layer recorded only the green part and a third recorded only the blue. Without special film processing, the result would simply be three superimposed black-and-white images, but complementary cyan, magenta, and yellow dye images were created in those layers by adding color couplers during a complex processing procedure.

Agfa's similarly structured Agfacolor Neu was introduced in 1936. Unlike Kodachrome, the color couplers in Agfacolor Neu were incorporated into the emulsion layers during manufacture, which greatly simplified the processing. Currently, available color films still employ a multi-layer emulsion and the same principles, most closely resembling Agfa's product.

Instant color film, used in a special camera which yielded a unique finished color print only a minute or two after the exposure, was introduced by Polaroid in 1963.

Color photography may form images as positive transparencies, which can be used in a slide projector, or as color negatives intended for use in creating positive color enlargements on specially coated paper. The latter is now the most common form of film (non-digital) color photography owing to the introduction of automated photo printing equipment. After a transition period centered around 1995–2005, color film was relegated to a niche market by inexpensive multi-megapixel digital cameras. Film continues to be the preference of some photographers because of its distinctive "look".

In 1981, Sony unveiled the first consumer camera to use a charge-coupled device for imaging, eliminating the need for film: the Sony Mavica. While the Mavica saved images to disk, the images were displayed on television, and the camera was not fully digital. In 1991, Kodak unveiled the DCS 100, the first commercially available digital single lens reflex camera. Although its high cost precluded uses other than photojournalism and professional photography, commercial digital photography was born.

Digital imaging uses an electronic image sensor to record the image as a set of electronic data rather than as chemical changes on film. An important difference between digital and chemical photography is that chemical photography resists photo manipulation because it involves film and photographic paper, while digital imaging is a highly manipulative medium. This difference allows for a degree of image post-processing that is comparatively difficult in film-based photography and permits different communicative potentials and applications.

Digital photography dominates the 21st century. More than 99% of photographs taken around the world are through digital cameras, increasingly through smartphones.

Synthesis photography is part of computer-generated imagery (CGI) where the shooting process is modeled on real photography. The CGI, creating digital copies of real universe, requires a visual representation process of these universes. Synthesis photography is the application of analog and digital photography in digital space. With the characteristics of the real photography but not being constrained by the physical limits of real world, synthesis photography allows artists to move into areas beyond the grasp of real photography.

A large variety of photographic techniques and media are used in the process of capturing images for photography. These include the camera; stereoscopy; dualphotography; full-spectrum, ultraviolet and infrared media; light field photography; and other imaging techniques.

The camera is the image-forming device, and a photographic plate, photographic film or a silicon electronic image sensor is the capture medium. The respective recording medium can be the plate or film itself, or a digital magnetic or electronic memory.

Photographers control the camera and lens to "expose" the light recording material to the required amount of light to form a "latent image" (on plate or film) or RAW file (in digital cameras) which, after appropriate processing, is converted to a usable image. Digital cameras use an electronic image sensor based on light-sensitive electronics such as charge-coupled device (CCD) or complementary metal-oxide-semiconductor (CMOS) technology. The resulting digital image is stored electronically, but can be reproduced on a paper.

The camera (or 'camera obscura') is a dark room or chamber from which, as far as possible, all light is excluded except the light that forms the image. It was discovered and used in the 16th century by painters. The subject being photographed, however, must be illuminated. Cameras can range from small to very large, a whole room that is kept dark while the object to be photographed is in another room where it is properly illuminated. This was common for reproduction photography of flat copy when large film negatives were used (see Process camera).

As soon as photographic materials became "fast" (sensitive) enough for taking candid or surreptitious pictures, small "detective" cameras were made, some actually disguised as a book or handbag or pocket watch (the "Ticka" camera) or even worn hidden behind an Ascot necktie with a tie pin that was really the lens.

The movie camera is a type of photographic camera which takes a rapid sequence of photographs on recording medium. In contrast to a still camera, which captures a single snapshot at a time, the movie camera takes a series of images, each called a "frame". This is accomplished through an intermittent mechanism. The frames are later played back in a movie projector at a specific speed, called the "frame rate" (number of frames per second). While viewing, a person's eyes and brain merge the separate pictures together to create the illusion of motion.

Photographs, both monochrome and color, can be captured and displayed through two side-by-side images that emulate human stereoscopic vision. Stereoscopic photography was the first that captured figures in motion. While known colloquially as "3-D" photography, the more accurate term is stereoscopy. Such cameras have long been realized by using film and more recently in digital electronic methods (including cell phone cameras).

Dualphotography consists of photographing a scene from both sides of a photographic device at once (e.g. camera for back-to-back dualphotography, or two networked cameras for portal-plane dualphotography). The dualphoto apparatus can be used to simultaneously capture both the subject and the photographer, or both sides of a geographical place at once, thus adding a supplementary narrative layer to that of a single image.

Ultraviolet and infrared films have been available for many decades and employed in a variety of photographic avenues since the 1960s. New technological trends in digital photography have opened a new direction in full spectrum photography, where careful filtering choices across the ultraviolet, visible and infrared lead to new artistic visions.

Modified digital cameras can detect some ultraviolet, all of the visible and much of the near infrared spectrum, as most digital imaging sensors are sensitive from about 350 nm to 1000 nm. An off-the-shelf digital camera contains an infrared hot mirror filter that blocks most of the infrared and a bit of the ultraviolet that would otherwise be detected by the sensor, narrowing the accepted range from about 400 nm to 700 nm.

Replacing a hot mirror or infrared blocking filter with an infrared pass or a wide spectrally transmitting filter allows the camera to detect the wider spectrum light at greater sensitivity. Without the hot-mirror, the red, green and blue (or cyan, yellow and magenta) colored micro-filters placed over the sensor elements pass varying amounts of ultraviolet (blue window) and infrared (primarily red and somewhat lesser the green and blue micro-filters).

Uses of full spectrum photography are for fine art photography, geology, forensics and law enforcement.

Digital methods of image capture and display processing have enabled the new technology of "light field photography" (also known as synthetic aperture photography). This process allows focusing at various depths of field to be selected "after" the photograph has been captured. As explained by Michael Faraday in 1846, the "light field" is understood as 5-dimensional, with each point in 3-D space having attributes of two more angles that define the direction of each ray passing through that point.

These additional vector attributes can be captured optically through the use of microlenses at each pixel point within the 2-dimensional image sensor. Every pixel of the final image is actually a selection from each sub-array located under each microlens, as identified by a post-image capture focus algorithm.

Besides the camera, other methods of forming images with light are available. For instance, a photocopy or xerography machine forms permanent images but uses the transfer of static electrical charges rather than photographic medium, hence the term electrophotography. Photograms are images produced by the shadows of objects cast on the photographic paper, without the use of a camera. Objects can also be placed directly on the glass of an image scanner to produce digital pictures.

An amateur photographer is one who practices photography as a hobby/passion and not necessarily for profit. The quality of some amateur work is comparable to that of many professionals and may be highly specialized or eclectic in choice of subjects. Amateur photography is often pre-eminent in photographic subjects which have little prospect of commercial use or reward. Amateur photography grew during the late 19th century due to the popularization of the hand-held camera. Nowadays it has spread widely through social media and is carried out throughout different platforms and equipment, switching to the use of cell phone as a key tool for making photography more accessible to everyone.

Commercial photography is probably best defined as any photography for which the photographer is paid for images rather than works of art. In this light, money could be paid for the subject of the photograph or the photograph itself. Wholesale, retail, and professional uses of photography would fall under this definition. The commercial photographic world could include:
The market for photographic services demonstrates the aphorism "A picture is worth a thousand words", which has an interesting basis in the history of photography. Magazines and newspapers, companies putting up Web sites, advertising agencies and other groups pay for photography.

Many people take photographs for commercial purposes. Organizations with a budget and a need for photography have several options: they can employ a photographer directly, organize a public competition, or obtain rights to stock photographs. Photo stock can be procured through traditional stock giants, such as Getty Images or Corbis; smaller microstock agencies, such as Fotolia; or web marketplaces, such as Cutcaster.

During the 20th century, both fine art photography and documentary photography became accepted by the English-speaking art world and the gallery system. In the United States, a handful of photographers, including Alfred Stieglitz, Edward Steichen, John Szarkowski, F. Holland Day, and Edward Weston, spent their lives advocating for photography as a fine art.
At first, fine art photographers tried to imitate painting styles. This movement is called Pictorialism, often using soft focus for a dreamy, 'romantic' look. In reaction to that, Weston, Ansel Adams, and others formed the Group f/64 to advocate 'straight photography', the photograph as a (sharply focused) thing in itself and not an imitation of something else.

The aesthetics of photography is a matter that continues to be discussed regularly, especially in artistic circles. Many artists argued that photography was the mechanical reproduction of an image. If photography is authentically art, then photography in the context of art would need redefinition, such as determining what component of a photograph makes it beautiful to the viewer. The controversy began with the earliest images "written with light"; Nicéphore Niépce, Louis Daguerre, and others among the very earliest photographers were met with acclaim, but some questioned if their work met the definitions and purposes of art.

Clive Bell in his classic essay "Art" states that only "significant form" can distinguish art from what is not art.

On 7 February 2007, Sotheby's London sold the 2001 photograph "99 Cent II Diptychon" for an unprecedented $3,346,456 to an anonymous bidder, making it the most expensive at the time.

Conceptual photography turns a concept or idea into a photograph. Even though what is depicted in the photographs are real objects, the subject is strictly abstract.

Photojournalism is a particular form of photography (the collecting, editing, and presenting of news material for publication or broadcast) that employs images in order to tell a news story. It is now usually understood to refer only to still images, but in some cases the term also refers to video used in broadcast journalism. Photojournalism is distinguished from other close branches of photography (e.g., documentary photography, social documentary photography, street photography or celebrity photography) by complying with a rigid ethical framework which demands that the work be both honest and impartial whilst telling the story in strictly journalistic terms. Photojournalists create pictures that contribute to the news media, and help communities connect with one other. Photojournalists must be well informed and knowledgeable about events happening right outside their door. They deliver news in a creative format that is not only informative, but also entertaining.

The camera has a long and distinguished history as a means of recording scientific phenomena from the first use by Daguerre and Fox-Talbot, such as astronomical events (eclipses for example), small creatures and plants when the camera was attached to the eyepiece of microscopes (in photomicroscopy) and for macro photography of larger specimens. The camera also proved useful in recording crime scenes and the scenes of accidents, such as the Wootton bridge collapse in 1861. The methods used in analysing photographs for use in legal cases are collectively known as forensic photography. Crime scene photos are taken from three vantage point. The vantage points are overview, mid-range, and close-up.

In 1845 Francis Ronalds, the Honorary Director of the Kew Observatory, invented the first successful camera to make continuous recordings of meteorological and geomagnetic parameters. Different machines produced 12- or 24- hour photographic traces of the minute-by-minute variations of atmospheric pressure, temperature, humidity, atmospheric electricity, and the three components of geomagnetic forces. The cameras were supplied to numerous observatories around the world and some remained in use until well into the 20th century. Charles Brooke a little later developed similar instruments for the Greenwich Observatory.

Science uses image technology that has derived from the design of the Pin Hole camera. X-Ray machines are similar in design to Pin Hole cameras with high-grade filters and laser radiation.
Photography has become universal in recording events and data in science and engineering, and at crime scenes or accident scenes. The method has been much extended by using other wavelengths, such as infrared photography and ultraviolet photography, as well as spectroscopy. Those methods were first used in the Victorian era and improved much further since that time.

The first photographed atom was discovered in 2012 by physicists at Griffith University, Australia. They used an electric field to trap an "Ion" of the element, Ytterbium. The image was recorded on a CCD, an electronic photographic film.

There are many ongoing questions about different aspects of photography. In her writing "On Photography" (1977), Susan Sontag discusses concerns about the objectivity of photography. This is a highly debated subject within the photographic community. Sontag argues, "To photograph is to appropriate the thing photographed. It means putting one's self into a certain relation to the world that feels like knowledge, and therefore like power." Photographers decide what to take a photo of, what elements to exclude and what angle to frame the photo, and these factors may reflect a particular socio-historical context. Along these lines, it can be argued that photography is a subjective form of representation.

Modern photography has raised a number of concerns on its effect on society. In Alfred Hitchcock's "Rear Window" (1954), the camera is presented as promoting voyeurism. 'Although the camera is an observation station, the act of photographing is more than passive observing'.
The camera doesn't rape or even possess, though it may presume, intrude, trespass, distort, exploit, and, at the farthest reach of metaphor, assassinate – all activities that, unlike the sexual push and shove, can be conducted from a distance, and with some detachment.
Digital imaging has raised ethical concerns because of the ease of manipulating digital photographs in post-processing. Many photojournalists have declared they will not crop their pictures or are forbidden from combining elements of multiple photos to make "photomontages", passing them as "real" photographs. Today's technology has made image editing relatively simple for even the novice photographer. However, recent changes of in-camera processing allow digital fingerprinting of photos to detect tampering for purposes of forensic photography.

Photography is one of the new media forms that changes perception and changes the structure of society. Further unease has been caused around cameras in regards to desensitization. Fears that disturbing or explicit images are widely accessible to children and society at large have been raised. Particularly, photos of war and pornography are causing a stir. Sontag is concerned that "to photograph is to turn people into objects that can be symbolically possessed." Desensitization discussion goes hand in hand with debates about censored images. Sontag writes of her concern that the ability to censor pictures means the photographer has the ability to construct reality.

One of the practices through which photography constitutes society is tourism. Tourism and photography combine to create a "tourist gaze"
in which local inhabitants are positioned and defined by the camera lens. However, it has also been argued that there exists a "reverse gaze" through which indigenous photographees can position the tourist photographer as a shallow consumer of images.

Additionally, photography has been the topic of many songs in popular culture.

Photography is both restricted as well as protected by the law in many jurisdictions. Protection of photographs is typically achieved through the granting of copyright or moral rights to the photographer. In the United States, photography is protected as a First Amendment right and anyone is free to photograph anything seen in public spaces as long as it is in plain view. In the UK a recent law (Counter-Terrorism Act 2008) increases the power of the police to prevent people, even press photographers, from taking pictures in public places.








</doc>
<doc id="23607" url="https://en.wikipedia.org/wiki?curid=23607" title="Pentateuch (disambiguation)">
Pentateuch (disambiguation)

The Pentateuch is the first part of the Bible, consisting of Genesis, Exodus, Leviticus, Numbers, and Deuteronomy.

It can also refer to:



</doc>
<doc id="23612" url="https://en.wikipedia.org/wiki?curid=23612" title="Postmodern philosophy">
Postmodern philosophy

Postmodern philosophy is a philosophical movement that arose in the second half of the 20 century as a critical response to assumptions allegedly present in modernist philosophical ideas regarding culture, identity, history, or language that were developed during the 18th-century Enlightenment. Postmodernist thinkers developed concepts like difference, repetition, trace, and hyperreality to subvert "grand narratives," univocity of being, and epistemic certainty. Postmodern philosophy questions the importance of power relationships, personalization, and discourse in the "construction" of truth and world views. Many postmodernists appear to deny that an objective reality exists, and appear to deny that there are objective moral values.

Jean-François Lyotard defined philosophical postmodernism in "The Postmodern Condition", writing "Simplifying to the extreme, I define postmodern as incredulity towards metanarratives," where what he means by metanarrative is something like a unified, complete, universal, and epistemically certain story about everything that is. Postmodernists reject metanarratives because they reject the concept of truth that metanarratives presuppose. Postmodernist philosophers in general argue that truth is always contingent on historical and social context rather than being absolute and universal and that truth is always partial and "at issue" rather than being complete and certain.

Postmodern philosophy is often particularly skeptical about simple binary oppositions characteristic of structuralism, emphasizing the problem of the philosopher cleanly distinguishing knowledge from ignorance, social progress from reversion, dominance from submission, good from bad, and presence from absence. But, for the same reasons, postmodern philosophy should often be particularly skeptical about the complex spectral characteristics of things, emphasizing the problem of the philosopher again cleanly distinguishing concepts, for a concept must be understood in the context of its opposite, such as existence and nothingness, normality and abnormality, speech and writing, and the like.

Postmodern philosophy also has strong relations with the substantial literature of critical theory.

Many postmodern claims are a deliberate repudiation of certain 18th-century Enlightenment values. Such a postmodernist believes that there is no objective natural reality, and that logic and reason are mere conceptual constructs that are not universally valid. Two other characteristic postmodern practices are a denial that human nature exists, and a (sometimes moderate) skepticism toward claims that science and technology will change society for the better. Postmodernists also believe there are no objective moral values. Thus, postmodern philosophy suggests equality for all things. One's concept of good and another's concept of evil are to be equally correct, since good and evil are subjective. Since both good and evil are equally correct, a postmodernist then tolerates both concepts, even if he or she disagrees with them subjectively. Postmodern writings often focus on deconstructing the role that power and ideology play in shaping discourse and belief. Postmodern philosophy shares ontological similarities with classical skeptical and relativistic belief systems, and shares political similarities with modern identity politics.

The "Routledge Encyclopedia of Philosophy" states that "The assumption that there is no common denominator in 'nature' or 'truth' ... that guarantees the possibility of neutral or objective thought" is a key assumption of postmodernism. The National Research Council has characterized the belief that "social science research can never generate objective or trustworthy knowledge" as an example of a postmodernist belief. Jean-François Lyotard's seminal 1979 "The Postmodern Condition" stated that its hypotheses "should not be accorded predictive value in relation to reality, but strategic value in relation to the questions raised". Lyotard's statement in 1984 that "I define postmodern as incredulity toward meta-narratives" extends to incredulity toward science. Jacques Derrida, who is generally identified as a postmodernist, stated that "every referent, all reality has the structure of a differential trace". Paul Feyerabend, one of the most famous twentieth-century philosophers of science, is often classified as a postmodernist; Feyerabend held that modern science is no more justified than witchcraft, and has denounced the "tyranny" of "abstract concepts such as 'truth', 'reality', or 'objectivity', which narrow people's vision and ways of being in the world". Feyerabend also defended astrology, adopted alternative medicine, and sympathized with creationism. Defenders of postmodernism state that many descriptions of postmodernism exaggerate its antipathy to science; for example, Feyerabend denied that he was "anti-science", accepted that some scientific "theories" are superior to other theories (even if science itself isn't superior to other modes of inquiry), and attempted conventional medical treatments during his fight against cancer.

Philosopher John Deely has argued for the contentious claim that the label "postmodern" for thinkers such as Derrida "et al." is "premature". Insofar as the "so-called" postmoderns follow the thoroughly "modern" trend of idealism, it is more an "ultra"modernism than anything else. A postmodernism that lives up to its name, therefore, must no longer confine itself to the premodern preoccupation with "things" nor with the modern confinement to "ideas," but must come to terms with the way of signs embodied in the semiotic doctrines of such thinkers as the Portuguese philosopher John Poinsot and the American philosopher Charles Sanders Peirce. Writes Deely,

The epoch of Greek and Latin philosophy was based on "being" in a quite precise sense: the existence exercised by things independently of human apprehension and attitude. The much briefer epoch of modern philosophy based itself rather on the instruments of human knowing, but in a way that unnecessarily compromised being. As the 20th century ends, there is reason to believe that a new philosophical epoch is dawning along with the new century, promising to be the richest epoch yet for human understanding. The postmodern era is positioned to synthesize at a higher level—the level of experience, where the being of things and the activity of the finite knower compenetrate one another and provide the materials whence can be derived knowledge of nature and knowledge of culture in their full symbiosis—the achievements of the ancients and the moderns in a way that gives full credit to the preoccupations of the two. The postmodern era has for its distinctive task in philosophy the exploration of a new path, no longer the ancient way of things nor the modern way of ideas, but the way of signs, whereby the peaks and valleys of ancient and modern thought alike can be surveyed and cultivated by a generation which has yet further peaks to climb and valleys to find.
Postmodern philosophy originated primarily in France during the mid-20th century. However, several philosophical antecedents inform many of postmodern philosophy's concerns.

It was greatly influenced by the writings of Søren Kierkegaard and Friedrich Nietzsche in the 19th century and other early-to-mid 20th-century philosophers, including phenomenologists Edmund Husserl and Martin Heidegger, psychoanalyst Jacques Lacan, structuralist Roland Barthes, Georges Bataille, and the later work of Ludwig Wittgenstein. Postmodern philosophy also drew from the world of the arts and architecture, particularly Marcel Duchamp, John Cage and artists who practiced collage, and the architecture of Las Vegas and the Pompidou Centre.

The most influential early postmodern philosophers were Jean Baudrillard, Jean-François Lyotard, and Jacques Derrida. Michel Foucault is also often cited as an early postmodernist although he personally rejected that label. Following Nietzsche, Foucault argued that knowledge is produced through the operations of "power", and changes fundamentally in different historical periods.

The writings of Lyotard were largely concerned with the role of narrative in human culture, and particularly how that role has changed as we have left modernity and entered a "postindustrial" or postmodern condition. He argued that modern philosophies legitimized their truth-claims not (as they themselves claimed) on logical or empirical grounds, but rather on the grounds of accepted stories (or "metanarratives") about knowledge and the world—comparing these with Wittgenstein's concept of language-games. He further argued that in our postmodern condition, these metanarratives no longer work to legitimize truth-claims. He suggested that in the wake of the collapse of modern metanarratives, people are developing a new "language-game"—one that does not make claims to absolute truth but rather celebrates a world of ever-changing relationships (among people and between people and the world).

Derrida, the father of deconstruction, practiced philosophy as a form of textual criticism. He criticized Western philosophy as privileging the concept of presence and "logos", as opposed to absence and markings or writings.

In the United States, the most famous pragmatist and self-proclaimed postmodernist was Richard Rorty. An analytic philosopher, Rorty believed that combining Willard Van Orman Quine's criticism of the analytic-synthetic distinction with Wilfrid Sellars's critique of the "Myth of the Given" allowed for an abandonment of the view of the thought or language as a mirror of a reality or external world. Further, drawing upon Donald Davidson's criticism of the dualism between conceptual scheme and empirical content, he challenges the sense of questioning whether our particular concepts are related to the world in an appropriate way, whether we can justify our ways of describing the world as compared with other ways. He argued that truth was not about getting it right or representing reality, but was part of a social practice and language was what served our purposes in a particular time; ancient languages are sometimes untranslatable into modern ones because they possess a different vocabulary and are unuseful today. Donald Davidson is not usually considered a postmodernist, although he and Rorty have both acknowledged that there are few differences between their philosophies.

Critics claim that postmodernism is nonsensical or self-contradictory.





</doc>
<doc id="23613" url="https://en.wikipedia.org/wiki?curid=23613" title="Postmodern music">
Postmodern music

Postmodern music is either simply music of the postmodern era, or music that follows aesthetical and philosophical trends of postmodernism. As the name suggests, the postmodernist movement formed partly in reaction to modernism. Even so, postmodern music still does not primarily define itself in opposition to modernist music; this label is applied instead by critics and theorists.

Postmodern music is not a distinct musical style, but rather refers to music of the postmodern era. The terms "postmodern", "postmodernism", "postmodernist", and "postmodernity" are exasperating terms . Indeed, postmodernists question the tight definitions and categories of academic disciplines, which they regard simply as the remnants of modernity .

Postmodernism in music is not a distinct musical style, but rather refers to music of the postmodern era. Postmodernist music, on the other hand, shares characteristics with postmodernist art—that is, art that comes after and reacts against modernism (see Modernism in Music).

Fredric Jameson, a major figure in the thinking on postmodernism and culture, calls postmodernism "the cultural dominant of the logic of late capitalism" , meaning that, through globalization, postmodern culture is tied inextricably with capitalism (Mark Fisher, writing 20 years later, goes further, essentially calling it the sole cultural possibility ). Drawing from Jameson and other theorists, David Beard and Kenneth Gloag argue that, in music, postmodernism is not just an attitude but also an inevitability in the current cultural climate of fragmentation . As early as 1938, Theodor Adorno had already identified a trend toward the dissolution of "a culturally dominant set of values" , citing the commodification of all genres as beginning of the end of genre or value distinctions in music .

In some respects, Postmodern music could be categorized as simply the music of the postmodern era, or music that follows aesthetic and philosophical trends of postmodernism, but with Jameson in mind, it is clear these definitions are inadequate. As the name suggests, the postmodernist movement formed partly in reaction to the ideals of modernism, but in fact postmodern music is more to do with functionality and the effect of globalization than it is with a specific reaction, movement, or attitude . In the face of capitalism, Jameson says, "It is safest to grasp the concept of the postmodern as an attempt to think the present historically in an age that has forgotten how to think historically in the first place" .

Jonathan Kramer posits the idea (following Umberto Eco and Jean-François Lyotard) that postmodernism (including "musical" postmodernism) is less a surface style or historical period (i.e., condition) than an "attitude". Kramer enumerates 16 (arguably subjective) "characteristics of postmodern music, by which I mean music that is understood in a postmodern manner, or that calls forth postmodern listening strategies, or that provides postmodern listening experiences, or that exhibits postmodern compositional practices." According to , postmodern music:


Daniel Albright summarizes the main tendencies of musical postmodernism as :

One author has suggested that the emergence of postmodern music in popular music occurred in the late 1960s, influenced in part by psychedelic rock and one or more of the later Beatles albums . Beard and Gloag support this position, citing Jameson's theory that "the radical changes of musical styles and languages throughout the 1960s [are] now seen as a reflection of postmodernism" (; see also ). Others have placed the beginnings of postmodernism in the arts, with particular reference to music, at around 1930 (; ).







</doc>
<doc id="23615" url="https://en.wikipedia.org/wiki?curid=23615" title="Protocol">
Protocol

Protocol may refer to:






</doc>
<doc id="23617" url="https://en.wikipedia.org/wiki?curid=23617" title="Pump">
Pump

A pump is a device that moves fluids (liquids or gases), or sometimes slurries, by mechanical action. Pumps can be classified into three major groups according to the method they use to move the fluid: "direct lift", "displacement", and "gravity" pumps.

Pumps operate by some mechanism (typically reciprocating or rotary), and consume energy to perform mechanical work for moving the fluid. Pumps operate via many energy sources, including manual operation, electricity, engines, or wind power, come in many sizes, from microscopic for use in medical applications to large industrial pumps.

Mechanical pumps serve in a wide range of applications such as pumping water from wells, aquarium filtering, pond filtering and aeration, in the car industry for water-cooling and fuel injection, in the energy industry for pumping oil and natural gas or for operating cooling towers. In the medical industry, pumps are used for biochemical processes in developing and manufacturing medicine, and as artificial replacements for body parts, in particular the artificial heart and penile prosthesis.

When a casing contains only one revolving impeller, it is called a single stage pump. When a casing contains two or more revolving impellers, it is called a double or multi-stage pump.

In biology, many different types of chemical and bio-mechanical pumps have evolved, and biomimicry is sometimes used in developing new types of mechanical pumps.
Mechanical pumps may be submerged in the fluid they are pumping or be placed external to the fluid.

Pumps can be classified by their method of displacement into positive displacement pumps, impulse pumps, velocity pumps, gravity pumps, steam pumps and valveless pumps. There are two basic types of pumps: positive displacement and centrifugal. Although axial-flow pumps are frequently classified as a separate type, they have essentially the same operating principles as centrifugal pumps.

A positive displacement pump makes a fluid move by trapping a fixed amount and forcing (displacing) that trapped volume into the discharge pipe.

Some positive displacement pumps use an expanding cavity on the suction side and a decreasing cavity on the discharge side. Liquid flows into the pump as the cavity on the suction side expands and the liquid flows out of the discharge as the cavity collapses. The volume is constant through each cycle of operation.

Positive displacement pumps, unlike centrifugal or roto-dynamic pumps, theoretically can produce the same flow at a given speed (RPM) no matter what the discharge pressure. Thus, positive displacement pumps are "constant flow machines". However, a slight increase in internal leakage as the pressure increases prevents a truly constant flow rate.

A positive displacement pump must not operate against a closed valve on the discharge side of the pump, because it has no shutoff head like centrifugal pumps. A positive displacement pump operating against a closed discharge valve continues to produce flow and the pressure in the discharge line increases until the line bursts, the pump is severely damaged, or both.

A relief or safety valve on the discharge side of the positive displacement pump is therefore necessary. The relief valve can be internal or external. The pump manufacturer normally has the option to supply internal relief or safety valves. The internal valve is usually used only as a safety precaution. An external relief valve in the discharge line, with a return line back to the suction line or supply tank provides increased safety.

A positive displacement pump can be further classified according to the mechanism used to move the fluid:


These pumps move fluid using a rotating mechanism that creates a vacuum that captures and draws in the liquid.

"Advantages:" Rotary pumps are very efficient because they can handle highly viscous fluids with higher flow rates as viscosity increases.

"Drawbacks:" The nature of the pump requires very close clearances between the rotating pump and the outer edge, making it rotate at a slow, steady speed. If rotary pumps are operated at high speeds, the fluids cause erosion, which eventually causes enlarged clearances that liquid can pass through, which reduces efficiency.

Rotary positive displacement pumps fall into three main types: 

Reciprocating pumps move the fluid using one or more oscillating pistons, plungers, or membranes (diaphragms), while valves restrict fluid motion to the desired direction. In order for suction to take place, the pump must first pull the plunger in an outward motion to decrease pressure in the chamber. Once the plunger pushes back, it will increase the pressure chamber and the inward pressure of the plunger will then open the discharge valve and release the fluid into the delivery pipe at a high velocity.

Pumps in this category range from "simplex", with one cylinder, to in some cases "quad" (four) cylinders, or more. Many reciprocating-type pumps are "duplex" (two) or "triplex" (three) cylinder. They can be either "single-acting" with suction during one direction of piston motion and discharge on the other, or "double-acting" with suction and discharge in both directions. The pumps can be powered manually, by air or steam, or by a belt driven by an engine. This type of pump was used extensively in the 19th century—in the early days of steam propulsion—as boiler feed water pumps. Now reciprocating pumps typically pump highly viscous fluids like concrete and heavy oils, and serve in special applications that demand low flow rates against high resistance. Reciprocating hand pumps were widely used to pump water from wells. Common bicycle pumps and foot pumps for inflation use reciprocating action.

These positive displacement pumps have an expanding cavity on the suction side and a decreasing cavity on the discharge side. Liquid flows into the pumps as the cavity on the suction side expands and the liquid flows out of the discharge as the cavity collapses. The volume is constant given each cycle of operation and the pump’s volumetric efficiency can be achieved through routine maintenance and inspection of its valves.

Typical reciprocating pumps are:

The positive displacement principle applies in these pumps:


This is the simplest of rotary positive displacement pumps. It consists of two meshed gears that rotate in a closely fitted casing. The tooth spaces trap fluid and force it around the outer periphery. The fluid does not travel back on the meshed part, because the teeth mesh closely in the center. Gear pumps see wide use in car engine oil pumps and in various hydraulic power packs.

A screw pump is a more complicated type of rotary pump that uses two or three screws with opposing thread — e.g., one screw turns clockwise and the other counterclockwise. The screws are mounted on parallel shafts that have gears that mesh so the shafts turn together and everything stays in place. The screws turn on the shafts and drive fluid through the pump. As with other forms of rotary pumps, the clearance between moving parts and the pump's casing is minimal.

Widely used for pumping difficult materials, such as sewage sludge contaminated with large particles, this pump consists of a helical rotor, about ten times as long as its width. This can be visualized as a central core of diameter "x" with, typically, a curved spiral wound around of thickness half "x", though in reality it is manufactured in single casting. This shaft fits inside a heavy duty rubber sleeve, of wall thickness also typically "x". As the shaft rotates, the rotor gradually forces fluid up the rubber sleeve. Such pumps can develop very high pressure at low volumes.

Named after the Roots brothers who invented it, this lobe pump displaces the liquid trapped between two long helical rotors, each fitted into the other when perpendicular at 90°, rotating inside a triangular shaped sealing line configuration, both at the point of suction and at the point of discharge. This design produces a continuous flow with equal volume and no vortex. It can work at low pulsation rates, and offers gentle performance that some applications require.

Applications include:

A "peristaltic pump" is a type of positive displacement pump. It contains fluid within a flexible tube fitted inside a circular pump casing (though linear peristaltic pumps have been made). A number of "rollers", "shoes", or "wipers" attached to a rotor compresses the flexible tube. As the rotor turns, the part of the tube under compression closes (or "occludes"), forcing the fluid through the tube. Additionally, when the tube opens to its natural state after the passing of the cam it draws ("restitution") fluid into the pump. This process is called peristalsis and is used in many biological systems such as the gastrointestinal tract.

"Plunger pumps" are reciprocating positive displacement pumps.

These consist of a cylinder with a reciprocating plunger. The suction and discharge valves are mounted in the head of the cylinder. In the suction stroke the plunger retracts and the suction valves open causing suction of fluid into the cylinder. In the forward stroke the plunger pushes the liquid out of the discharge valve.
Efficiency and common problems: With only one cylinder in plunger pumps, the fluid flow varies between maximum flow when the plunger moves through the middle positions, and zero flow when the plunger is at the end positions. A lot of energy is wasted when the fluid is accelerated in the piping system. Vibration and "water hammer" may be a serious problem. In general the problems are compensated for by using two or more cylinders not working in phase with each other.

Triplex plunger pumps use three plungers, which reduces the pulsation of single reciprocating plunger pumps. Adding a pulsation dampener on the pump outlet can further smooth the "pump ripple", or ripple graph of a pump transducer. The dynamic relationship of the high-pressure fluid and plunger generally requires high-quality plunger seals. Plunger pumps with a larger number of plungers have the benefit of increased flow, or smoother flow without a pulsation damper. The increase in moving parts and crankshaft load is one drawback.

Car washes often use these triplex-style plunger pumps (perhaps without pulsation dampers). In 1968, William Bruggeman reduced the size of the triplex pump and increased the lifespan so that car washes could use equipment with smaller footprints. Durable high-pressure seals, low-pressure seals and oil seals, hardened crankshafts, hardened connecting rods, thick ceramic plungers and heavier duty ball and roller bearings improve reliability in triplex pumps. Triplex pumps now are in a myriad of markets across the world.

Triplex pumps with shorter lifetimes are commonplace to the home user. A person who uses a home pressure washer for 10 hours a year may be satisfied with a pump that lasts 100 hours between rebuilds. Industrial-grade or continuous duty triplex pumps on the other end of the quality spectrum may run for as much as 2,080 hours a year.

The oil and gas drilling industry uses massive semi trailer-transported triplex pumps called mud pumps to pump drilling mud, which cools the drill bit and carries the cuttings back to the surface.
Drillers use triplex or even quintuplex pumps to inject water and solvents deep into shale in the extraction process called "fracking".

One modern application of positive displacement pumps is compressed-air-powered double-diaphragm pumps. Run on compressed air these pumps are intrinsically safe by design, although all manufacturers offer ATEX certified models to comply with industry regulation. These pumps are relatively inexpensive and can perform a wide variety of duties, from pumping water out of bunds to pumping hydrochloric acid from secure storage (dependent on how the pump is manufactured – elastomers / body construction). These double-diaphragm pumps can handle viscous fluids and abrasive materials with a gentle pumping process ideal for transporting shear sensitive media.

Devised in China as chain pumps over 1000 years ago, these pumps can be made from very simple materials: A rope, a wheel and a PVC pipe are sufficient to make a simple rope pump. Rope pump efficiency has been studied by grass roots organizations and the techniques for making and running them have been continuously improved.

Impulse pumps use pressure created by gas (usually air). In some impulse pumps the gas trapped in the liquid (usually water), is released and accumulated somewhere in the pump, creating a pressure that can push part of the liquid upwards.

Conventional impulse pumps include:

Instead of a gas accumulation and releasing cycle, the pressure can be created by burning of hydrocarbons. Such combustion driven pumps directly transmit the impulse form a combustion event through the actuation membrane to the pump fluid. In order to allow this direct transmission, the pump needs to be almost entirely made of an elastomer (e.g. silicone rubber). Hence, the combustion causes the membrane to expand and thereby pumps the fluid out of the adjacent pumping chamber. The first combustion-driven soft pump was developed by ETH Zurich.

A hydraulic ram is a water pump powered by hydropower.

It takes in water at relatively low pressure and high flow-rate and outputs water at a higher hydraulic-head and lower flow-rate. The device uses the water hammer effect to develop pressure that lifts a portion of the input water that powers the pump to a point higher than where the water started.

The hydraulic ram is sometimes used in remote areas, where there is both a source of low-head hydropower, and a need for pumping water to a destination higher in elevation than the source. In this situation, the ram is often useful, since it requires no outside source of power other than the kinetic energy of flowing water.

Rotodynamic pumps (or dynamic pumps) are a type of velocity pump in which kinetic energy is added to the fluid by increasing the flow velocity. This increase in energy is converted to a gain in potential energy (pressure) when the velocity is reduced prior to or as the flow exits the pump into the discharge pipe. This conversion of kinetic energy to pressure is explained by the "First law of thermodynamics", or more specifically by "Bernoulli's principle".

Dynamic pumps can be further subdivided according to the means in which the velocity gain is achieved.

These types of pumps have a number of characteristics:

A practical difference between dynamic and positive displacement pumps is how they operate under closed valve conditions. Positive displacement pumps physically displace fluid, so closing a valve downstream of a positive displacement pump produces a continual pressure build up that can cause mechanical failure of pipeline or pump. Dynamic pumps differ in that they can be safely operated under closed valve conditions (for short periods of time).

Such a pump is also referred to as a centrifugal pump. The fluid enters along the axis or center, is accelerated by the impeller and exits at right angles to the shaft (radially); an example is the centrifugal fan, which is commonly used to implement a vacuum cleaner. Generally, a radial-flow pump operates at higher pressures and lower flow rates than an axial- or a mixed-flow pump.

These are also referred to as All fluid pumps. The fluid is pushed outward or inward and move fluid axially. They operate at much lower pressures and higher flow rates than radial-flow (centripetal) pumps. Axial-flow pumps cannot be run up to speed without special precaution. If at a low flow rate, the total head rise and high torque associated with this pipe would mean that the starting torque would have to become a function of acceleration for the whole mass of liquid in the pipe system. If there is a large amount of fluid in the system, accelerate the pump slowly.
Mixed-flow pumps function as a compromise between radial and axial-flow pumps. The fluid experiences both radial acceleration and lift and exits the impeller somewhere between 0 and 90 degrees from the axial direction. As a consequence mixed-flow pumps operate at higher pressures than axial-flow pumps while delivering higher discharges than radial-flow pumps. The exit angle of the flow dictates the pressure head-discharge characteristic in relation to radial and mixed-flow.

This uses a jet, often of steam, to create a low pressure. This low pressure sucks in fluid and propels it into a higher pressure region.

Gravity pumps include the "syphon" and "Heron's fountain". The "hydraulic ram" is also sometimes called a gravity pump; in a gravity pump the water is lifted by gravitational force.

Steam pumps have been for a long time mainly of historical interest. They include any type of pump powered by a steam engine and also pistonless pumps such as Thomas Savery's or the Pulsometer steam pump.

Recently there has been a resurgence of interest in low power solar steam pumps for use in smallholder irrigation in developing countries. Previously small steam engines have not been viable because of escalating inefficiencies as vapour engines decrease in size. However the use of modern engineering materials coupled with alternative engine configurations has meant that these types of system are now a cost effective opportunity.

Valveless pumping assists in fluid transport in various biomedical and engineering systems. In a valveless pumping system, no valves (or physical occlusions) are present to regulate the flow direction. The fluid pumping efficiency of a valveless system, however, is not necessarily lower than that having valves. In fact, many fluid-dynamical systems in nature and engineering more or less rely upon valveless pumping to transport the working fluids therein. For instance, blood circulation in the cardiovascular system is maintained to some extent even when the heart’s valves fail. Meanwhile, the embryonic vertebrate heart begins pumping blood long before the development of discernible chambers and valves. In microfluidics, valveless impedance pumps have been fabricated, and are expected to be particularly suitable for handling sensitive biofluids. Ink jet printers operating on the Piezoelectric transducer principle also use valveless pumping. The pump chamber is emptied through the printing jet due to reduced flow impedance in that direction and refilled by capillary action..

Examining pump repair records and mean time between failures (MTBF) is of great importance to responsible and conscientious pump users. In view of that fact, the preface to the 2006 Pump User’s Handbook alludes to "pump failure" statistics. For the sake of convenience, these failure statistics often are translated into MTBF (in this case, installed life before failure).

In early 2005, Gordon Buck, John Crane Inc.’s chief engineer for Field Operations in Baton Rouge, LA, examined the repair records for a number of refinery and chemical plants to obtain meaningful reliability data for centrifugal pumps. A total of 15 operating plants having nearly 15,000 pumps were included in the survey. The smallest of these plants had about 100 pumps; several plants had over 2000. All facilities were located in the United States. In addition, considered as "new", others as "renewed" and still others as "established". Many of these plants—but not all—had an alliance arrangement with John Crane. In some cases, the alliance contract included having a John Crane Inc. technician or engineer on-site to coordinate various aspects of the program.

Not all plants are refineries, however, and different results occur elsewhere. In chemical plants, pumps have historically been "throw-away" items as chemical attack limits life. Things have improved in recent years, but the somewhat restricted space available in "old" DIN and ASME-standardized stuffing boxes places limits on the type of seal that fits. Unless the pump user upgrades the seal chamber, the pump only accommodates more compact and simple versions. Without this upgrading, lifetimes in chemical installations are generally around 50 to 60 percent of the refinery values.

Unscheduled maintenance is often one of the most significant costs of ownership, and failures of mechanical seals and bearings are among the major causes. Keep in mind the potential value of selecting pumps that cost more initially, but last much longer between repairs. The MTBF of a better pump may be one to four years longer than that of its non-upgraded counterpart. Consider that published average values of avoided pump failures range from US$2600 to US$12,000. This does not include lost opportunity costs. One pump fire occurs per 1000 failures. Having fewer pump failures means having fewer destructive pump fires.

As has been noted, a typical pump failure, based on actual year 2002 reports, costs US$5,000 on average. This includes costs for material, parts, labor and overhead. Extending a pump's MTBF from 12 to 18 months would save US$1,667 per year — which might be greater than the cost to upgrade the centrifugal pump's reliability.

Pumps are used throughout society for a variety of purposes. Early applications includes the use of the windmill or watermill to pump water. Today, the pump is used for irrigation, water supply, gasoline supply, air conditioning systems, refrigeration (usually called a compressor), chemical movement, sewage movement, flood control, marine services, etc.

Because of the wide variety of applications, pumps have a plethora of shapes and sizes: from very large to very small, from handling gas to handling liquid, from high pressure to low pressure, and from high volume to low volume.

Typically, a liquid pump can't simply draw air. The feed line of the pump and the internal body surrounding the pumping mechanism must first be filled with the liquid that requires pumping: An operator must introduce liquid into the system to initiate the pumping. This is called "priming" the pump. Loss of prime is usually due to ingestion of air into the pump. The clearances and displacement ratios in pumps for liquids, whether thin or more viscous, usually cannot displace air due to its compressibility. This is the case with most velocity (rotodynamic) pumps — for example, centrifugal pumps. For such pumps the position of the pump should always be lower than the suction point, if not the pump should be manually filled with liquid or a secondary pump should be used until all air is removed from the suction line and the pump casing.

Positive–displacement pumps, however, tend to have sufficiently tight sealing between the moving parts and the casing or housing of the pump that they can be described as "self-priming". Such pumps can also serve as "priming pumps", so called when they are used to fulfill that need for other pumps in lieu of action taken by a human operator.

One sort of pump once common worldwide was a hand-powered water pump, or 'pitcher pump'. It was commonly installed over community water wells in the days before piped water supplies.

In parts of the British Isles, it was often called "the parish pump". Though such community pumps are no longer common, people still used the expression "parish pump" to describe a place or forum where matters of local interest are discussed.

Because water from pitcher pumps is drawn directly from the soil, it is more prone to contamination. If such water is not filtered and purified, consumption of it might lead to gastrointestinal or other water-borne diseases. A notorious case is the 1854 Broad Street cholera outbreak. At the time it was not known how cholera was transmitted, but physician John Snow suspected contaminated water and had the handle of the public pump he suspected removed; the outbreak then subsided.

Modern hand-operated community pumps are considered the most sustainable low-cost option for safe water supply in resource-poor settings, often in rural areas in developing countries. A hand pump opens access to deeper groundwater that is often not polluted and also improves the safety of a well by protecting the water source from contaminated buckets. Pumps such as the Afridev pump are designed to be cheap to build and install, and easy to maintain with simple parts. However, scarcity of spare parts for these type of pumps in some regions of Africa has diminished their utility for these areas.

Multiphase pumping applications, also referred to as tri-phase, have grown due to increased oil drilling activity. In addition, the economics of multiphase production is attractive to upstream operations as it leads to simpler, smaller in-field installations, reduced equipment costs and improved production rates. In essence, the multiphase pump can accommodate all fluid stream properties with one piece of equipment, which has a smaller footprint. Often, two smaller multiphase pumps are installed in series rather than having just one massive pump.

For midstream and upstream operations, multiphase pumps can be located onshore or offshore and can be connected to single or multiple wellheads. Basically, multiphase pumps are used to transport the untreated flow stream produced from oil wells to downstream processes or gathering facilities. This means that the pump may handle a flow stream (well stream) from 100 percent gas to 100 percent liquid and every imaginable combination in between. The flow stream can also contain abrasives such as sand and dirt. Multiphase pumps are designed to operate under changing or fluctuating process conditions. Multiphase pumping also helps eliminate emissions of greenhouse gases as operators strive to minimize the flaring of gas and the venting of tanks where possible.

A rotodynamic pump with one single shaft that requires two mechanical seals, this pump uses an open-type axial impeller. It's often called a "Poseidon pump", and can be described as a cross between an axial compressor and a centrifugal pump.

The twin-screw pump is constructed of two inter-meshing screws that move the pumped fluid. Twin screw pumps are often used when pumping conditions contain high gas volume fractions and fluctuating inlet conditions. Four mechanical seals are required to seal the two shafts.

When the pumping application is not suited to a centrifugal pump, a progressive cavity pump is used instead. Progressive cavity pumps are single-screw types typically used in shallow wells or at the surface. This pump is mainly used on surface applications where the pumped fluid may contain a considerable amount of solids such as sand and dirt. The volumetric efficiency and mechanical efficiency of a progressive cavity pump increases as the viscosity of the liquid does.

These pumps are basically multistage centrifugal pumps and are widely used in oil well applications as a method for artificial lift. These pumps are usually specified when the pumped fluid is mainly liquid.

"Buffer tank"
A buffer tank is often installed upstream of the pump suction nozzle in case of a slug flow. The buffer tank breaks the energy of the liquid slug, smooths any fluctuations in the incoming flow and acts as a sand trap.

As the name indicates, multiphase pumps and their mechanical seals can encounter a large variation in service conditions such as changing process fluid composition, temperature variations, high and low operating pressures and exposure to abrasive/erosive media. The challenge is selecting the appropriate mechanical seal arrangement and support system to ensure maximized seal life and its overall effectiveness.

Pumps are commonly rated by horsepower, volumetric flow rate, outlet pressure in metres (or feet) of head, inlet suction in suction feet (or metres) of head.
The head can be simplified as the number of feet or metres the pump can raise or lower a column of water at atmospheric pressure.

From an initial design point of view, engineers often use a quantity termed the specific speed to identify the most suitable pump type for a particular combination of flow rate and head.

The power imparted into a fluid increases the energy of the fluid per unit volume. Thus the power relationship is between the conversion of the mechanical energy of the pump mechanism and the fluid elements within the pump. In general, this is governed by a series of simultaneous differential equations, known as the Navier–Stokes equations. However a more simple equation relating only the different energies in the fluid, known as Bernoulli's equation can be used. Hence the power, P, required by the pump:

where Δp is the change in total pressure between the inlet and outlet (in Pa), and Q, the volume flow-rate of the fluid is given in m/s. The total pressure may have gravitational, static pressure and kinetic energy components; i.e. energy is distributed between change in the fluid's gravitational potential energy (going up or down hill), change in velocity, or change in static pressure. η is the pump efficiency, and may be given by the manufacturer's information, such as in the form of a pump curve, and is typically derived from either fluid dynamics simulation (i.e. solutions to the Navier–Stokes for the particular pump geometry), or by testing. The efficiency of the pump depends upon the pump's configuration and operating conditions (such as rotational speed, fluid density and viscosity etc.)

For a typical "pumping" configuration, the work is imparted on the fluid, and is thus positive. For the fluid imparting the work on the pump (i.e. a turbine), the work is negative. Power required to drive the pump is determined by dividing the output power by the pump efficiency. Furthermore, this definition encompasses pumps with no moving parts, such as a siphon.

Pump efficiency is defined as the ratio of the power imparted on the fluid by the pump in relation to the power supplied to drive the pump. Its value is not fixed for a given pump, efficiency is a function of the discharge and therefore also operating head. For centrifugal pumps, the efficiency tends to increase with flow rate up to a point midway through the operating range (peak efficiency or Best Efficiency Point (BEP) ) and then declines as flow rates rise further. Pump performance data such as this is usually supplied by the manufacturer before pump selection. Pump efficiencies tend to decline over time due to wear (e.g. increasing clearances as impellers reduce in size).

When a system includes a centrifugal pump, an important design issue is matching the "head loss-flow characteristic" with the pump so that it operates at or close to the point of its maximum efficiency.

Pump efficiency is an important aspect and pumps should be regularly tested. Thermodynamic pump testing is one method.



</doc>
<doc id="23618" url="https://en.wikipedia.org/wiki?curid=23618" title="Progressive">
Progressive

Progressive may refer to:










</doc>
<doc id="23619" url="https://en.wikipedia.org/wiki?curid=23619" title="Pressure">
Pressure

Pressure (symbol: "p" or "P") is the force applied perpendicular to the surface of an object per unit area over which that force is distributed. Gauge pressure (also spelled "gage" pressure) is the pressure relative to the ambient pressure.

Various units are used to express pressure. Some of these derive from a unit of force divided by a unit of area; the SI unit of pressure, the pascal (Pa), for example, is one newton per square metre; similarly, the pound-force per square inch (psi) is the traditional unit of pressure in the imperial and US customary systems. Pressure may also be expressed in terms of standard atmospheric pressure; the atmosphere (atm) is equal to this pressure, and the torr is defined as of this. Manometric units such as the centimetre of water, millimetre of mercury, and inch of mercury are used to express pressures in terms of the height of column of a particular fluid in a manometer.

Pressure is the amount of force applied perpendicular to the surface of an object per unit area. The symbol for it is "p" or "P".
The IUPAC recommendation for pressure is a lower-case "p".
However, upper-case "P" is widely used. The usage of "P" vs "p" depends upon the field in which one is working, on the nearby presence of other symbols for quantities such as power and momentum, and on writing style.

Mathematically:
where:

Pressure is a scalar quantity. It relates the vector surface element (a vector normal to the surface) with the normal force acting on it. The pressure is the scalar proportionality constant that relates the two normal vectors:

The minus sign comes from the fact that the force is considered towards the surface element, while the normal vector points outward. The equation has meaning in that, for any surface "S" in contact with the fluid, the total force exerted by the fluid on that surface is the surface integral over "S" of the right-hand side of the above equation.

It is incorrect (although rather usual) to say "the pressure is directed in such or such direction". The pressure, as a scalar, has no direction. The force given by the previous relationship to the quantity has a direction, but the pressure does not. If we change the orientation of the surface element, the direction of the normal force changes accordingly, but the pressure remains the same.

Pressure is distributed to solid boundaries or across arbitrary sections of fluid "normal to" these boundaries or sections at every point. It is a fundamental parameter in thermodynamics, and it is conjugate to volume.

The SI unit for pressure is the pascal (Pa), equal to one newton per square metre (N/m, or kg·m·s). This name for the unit was added in 1971; before that, pressure in SI was expressed simply in newtons per square metre.

Other units of pressure, such as pounds per square inch and bar, are also in common use. The CGS unit of pressure is the barye (Ba), equal to 1 dyn·cm, or 0.1 Pa. Pressure is sometimes expressed in grams-force or kilograms-force per square centimetre (g/cm or kg/cm) and the like without properly identifying the force units. But using the names kilogram, gram, kilogram-force, or gram-force (or their symbols) as units of force is expressly forbidden in SI. The technical atmosphere (symbol: at) is 1 kgf/cm (98.0665 kPa, or 14.223 psi).

Since a system under pressure has the potential to perform work on its surroundings, pressure is a measure of potential energy stored per unit volume. It is therefore related to energy density and may be expressed in units such as joules per cubic metre (J/m, which is equal to Pa).
Mathematically:

Some meteorologists prefer the hectopascal (hPa) for atmospheric air pressure, which is equivalent to the older unit millibar (mbar). Similar pressures are given in kilopascals (kPa) in most other fields, where the hecto- prefix is rarely used. The inch of mercury is still used in the United States. Oceanographers usually measure underwater pressure in decibars (dbar) because pressure in the ocean increases by approximately one decibar per metre depth.
The standard atmosphere (atm) is an established constant. It is approximately equal to typical air pressure at Earth mean sea level and is defined as .

Because pressure is commonly measured by its ability to displace a column of liquid in a manometer, pressures are often expressed as a depth of a particular fluid (e.g., centimetres of water, millimetres of mercury or inches of mercury). The most common choices are mercury (Hg) and water; water is nontoxic and readily available, while mercury's high density allows a shorter column (and so a smaller manometer) to be used to measure a given pressure. The pressure exerted by a column of liquid of height "h" and density "ρ" is given by the hydrostatic pressure equation , where "g" is the gravitational acceleration. Fluid density and local gravity can vary from one reading to another depending on local factors, so the height of a fluid column does not define pressure precisely. When millimetres of mercury or inches of mercury are quoted today, these units are not based on a physical column of mercury; rather, they have been given precise definitions that can be expressed in terms of SI units. One millimetre of mercury is approximately equal to one torr. The water-based units still depend on the density of water, a measured, rather than defined, quantity. These "manometric units" are still encountered in many fields. Blood pressure is measured in millimetres of mercury in most of the world, and lung pressures in centimetres of water are still common.

Underwater divers use the metre sea water (msw or MSW) and foot sea water (fsw or FSW) units of pressure, and these are the standard units for pressure gauges used to measure pressure exposure in diving chambers and personal decompression computers. A msw is defined as 0.1 bar (= 100000 Pa = 10000 Pa), is not the same as a linear metre of depth. 33.066 fsw = 1 atm (1 atm = 101325 Pa / 33.066 = 3064.326 Pa). Note that the pressure conversion from msw to fsw is different from the length conversion: 10 msw = 32.6336 fsw, while 10 m = 32.8083 ft.

Gauge pressure is often given in units with "g" appended, e.g. "kPag", "barg" or "psig", and units for measurements of absolute pressure are sometimes given a suffix of "a", to avoid confusion, for example "kPaa", "psia". However, the US National Institute of Standards and Technology recommends that, to avoid confusion, any modifiers be instead applied to the quantity being measured rather than the unit of measure. For example, rather than .

Differential pressure is expressed in units with "d" appended; this type of measurement is useful when considering sealing performance or whether a valve will open or close.

Presently or formerly popular pressure units include the following:

As an example of varying pressures, a finger can be pressed against a wall without making any lasting impression; however, the same finger pushing a thumbtack can easily damage the wall. Although the force applied to the surface is the same, the thumbtack applies more pressure because the point concentrates that force into a smaller area. Pressure is transmitted to solid boundaries or across arbitrary sections of fluid "normal to" these boundaries or sections at every point. Unlike stress, pressure is defined as a scalar quantity. The negative gradient of pressure is called the force density.

Another example is a knife. If we try to cut a fruit with the flat side, the force is distributed over a large area, and it will not cut. But if we use the edge, it will cut smoothly. The reason is that the flat side has a greater surface area (less pressure), and so it does not cut the fruit. When we take the thin side, the surface area is reduced, and so it cuts the fruit easily and quickly. This is one example of a practical application of pressure.

For gases, pressure is sometimes measured not as an "absolute pressure", but relative to atmospheric pressure; such measurements are called "gauge pressure". An example of this is the air pressure in an automobile tire, which might be said to be "220 kPa (32 psi)", but is actually 220 kPa (32 psi) above atmospheric pressure. Since atmospheric pressure at sea level is about 100 kPa (14.7 psi), the absolute pressure in the tire is therefore about 320 kPa (46.7 psi). In technical work, this is written "a gauge pressure of 220 kPa (32 psi)". Where space is limited, such as on pressure gauges, name plates, graph labels, and table headings, the use of a modifier in parentheses, such as "kPa (gauge)" or "kPa (absolute)", is permitted. In non-SI technical work, a gauge pressure of 32 psi is sometimes written as "32 psig", and an absolute pressure as "32 psia", though the other methods explained above that avoid attaching characters to the unit of pressure are preferred.

Gauge pressure is the relevant measure of pressure wherever one is interested in the stress on storage vessels and the plumbing components of fluidics systems. However, whenever equation-of-state properties, such as densities or changes in densities, must be calculated, pressures must be expressed in terms of their absolute values. For instance, if the atmospheric pressure is 100 kPa, a gas (such as helium) at 200 kPa (gauge) (300 kPa [absolute]) is 50% denser than the same gas at 100 kPa (gauge) (200 kPa [absolute]). Focusing on gauge values, one might erroneously conclude the first sample had twice the density of the second one.

In a static gas, the gas as a whole does not appear to move. The individual molecules of the gas, however, are in constant random motion. Because we are dealing with an extremely large number of molecules and because the motion of the individual molecules is random in every direction, we do not detect any motion. If we enclose the gas within a container, we detect a pressure in the gas from the molecules colliding with the walls of our container. We can put the walls of our container anywhere inside the gas, and the force per unit area (the pressure) is the same. We can shrink the size of our "container" down to a very small point (becoming less true as we approach the atomic scale), and the pressure will still have a single value at that point. Therefore, pressure is a scalar quantity, not a vector quantity. It has magnitude but no direction sense associated with it. Pressure force acts in all directions at a point inside a gas. At the surface of a gas, the pressure force acts perpendicular (at right angle) to the surface.

A closely related quantity is the stress tensor "σ", which relates the vector force formula_7 to the 
vector area formula_8 via the linear relation formula_9.

This tensor may be expressed as the sum of the viscous stress tensor minus the hydrostatic pressure. The negative of the stress tensor is sometimes called the pressure tensor, but in the following, the term "pressure" will refer only to the scalar pressure.

According to the theory of general relativity, pressure increases the strength of a gravitational field (see stress–energy tensor) and so adds to the mass-energy cause of gravity. This effect is unnoticeable at everyday pressures but is significant in neutron stars, although it has not been experimentally tested.

Fluid pressure is most often the compressive stress at some point within a fluid. (The term "fluid" refers to both liquids and gases – for more information specifically about liquid pressure, see section below.)

Fluid pressure occurs in one of two situations:

Pressure in open conditions usually can be approximated as the pressure in "static" or non-moving conditions (even in the ocean where there are waves and currents), because the motions create only negligible changes in the pressure. Such conditions conform with principles of fluid statics. The pressure at any given point of a non-moving (static) fluid is called the hydrostatic pressure. 

Closed bodies of fluid are either "static", when the fluid is not moving, or "dynamic", when the fluid can move as in either a pipe or by compressing an air gap in a closed container. The pressure in closed conditions conforms with the principles of fluid dynamics.

The concepts of fluid pressure are predominantly attributed to the discoveries of Blaise Pascal and Daniel Bernoulli. Bernoulli's equation can be used in almost any situation to determine the pressure at any point in a fluid. The equation makes some assumptions about the fluid, such as the fluid being ideal and incompressible. An ideal fluid is a fluid in which there is no friction, it is inviscid (zero viscosity). The equation for all points of a system filled with a constant-density fluid is

where:


Explosion or deflagration pressures are the result of the ignition of explosive gases, mists, dust/air suspensions, in unconfined and confined spaces.

While pressures are, in general, positive, there are several situations in which negative pressures may be encountered:

Stagnation pressure is the pressure a fluid exerts when it is forced to stop moving. Consequently, although a fluid moving at higher speed will have a lower static pressure, it may have a higher stagnation pressure when forced to a standstill. Static pressure and stagnation pressure are related by:

where 

The pressure of a moving fluid can be measured using a Pitot tube, or one of its variations such as a Kiel probe or Cobra probe, connected to a manometer. Depending on where the inlet holes are located on the probe, it can measure static pressures or stagnation pressures.

There is a two-dimensional analog of pressure – the lateral force per unit length applied on a line perpendicular to the force.

Surface pressure is denoted by π:
and shares many similar properties with three-dimensional pressure. Properties of surface chemicals can be investigated by measuring pressure/area isotherms, as the two-dimensional analog of Boyle's law, , at constant temperature.

Surface tension is another example of surface pressure, but with a reversed sign, because "tension" is the opposite to "pressure".

In an ideal gas, molecules have no volume and do not interact. According to the ideal gas law, pressure varies linearly with temperature and quantity, and inversely with volume:

where:

Real gases exhibit a more complex dependence on the variables of state.

Vapour pressure is the pressure of a vapour in thermodynamic equilibrium with its condensed phases in a closed system. All liquids and solids have a tendency to evaporate into a gaseous form, and all gases have a tendency to condense back to their liquid or solid form.

The atmospheric pressure boiling point of a liquid (also known as the normal boiling point) is the temperature at which the vapor pressure equals the ambient atmospheric pressure. With any incremental increase in that temperature, the vapor pressure becomes sufficient to overcome atmospheric pressure and lift the liquid to form vapour bubbles inside the bulk of the substance. Bubble formation deeper in the liquid requires a higher pressure, and therefore higher temperature, because the fluid pressure increases above the atmospheric pressure as the depth increases.

The vapor pressure that a single component in a mixture contributes to the total pressure in the system is called partial vapor pressure.

When a person swims under the water, water pressure is felt acting on the person's eardrums. The deeper that person swims, the greater the pressure. The pressure felt is due to the weight of the water above the person. As someone swims deeper, there is more water above the person and therefore greater pressure. The pressure a liquid exerts depends on its depth.

Liquid pressure also depends on the density of the liquid. If someone was submerged in a liquid more dense than water, the pressure would be correspondingly greater. thus we can say that the depth, density and liquid pressure are directly proportionate. The pressure due to a liquid in liquid columns of constant density or at a depth within a substance is represented by the following formula:

where:

Another way of saying the same formula is the following:

The pressure a liquid exerts against the sides and bottom of a container depends on the density and the depth of the liquid. If atmospheric pressure is neglected, liquid pressure against the bottom is twice as great at twice the depth; at three times the depth, the liquid pressure is threefold; etc. Or, if the liquid is two or three times as dense, the liquid pressure is correspondingly two or three times as great for any given depth. Liquids are practically incompressible – that is, their volume can hardly be changed by pressure (water volume decreases by only 50 millionths of its original volume for each atmospheric increase in pressure). Thus, except for small changes produced by temperature, the density of a particular liquid is practically the same at all depths.

Atmospheric pressure pressing on the surface of a liquid must be taken into account when trying to discover the "total" pressure acting on a liquid. The total pressure of a liquid, then, is "ρgh" plus the pressure of the atmosphere. When this distinction is important, the term "total pressure" is used. Otherwise, discussions of liquid pressure refer to pressure without regard to the normally ever-present atmospheric pressure.

It is important to recognize that the pressure does not depend on the "amount" of liquid present. Volume is not the important factor – depth is. The average water pressure acting against a dam depends on the average depth of the water and not on the volume of water held back. For example, a wide but shallow lake with a depth of exerts only half the average pressure that a small deep pond does (note that the "total force" applied to the longer dam will be greater, due to the greater total surface area for the pressure to act upon, but for a given 5-foot section of each dam, the 10 ft deep water will apply half the force of 20 ft deep water). A person will feel the same pressure whether his/her head is dunked a metre beneath the surface of the water in a small pool or to the same depth in the middle of a large lake. If four vases contain different amounts of water but are all filled to equal depths, then a fish with its head dunked a few centimetres under the surface will be acted on by water pressure that is the same in any of the vases. If the fish swims a few centimetres deeper, the pressure on the fish will increase with depth and be the same no matter which vase the fish is in. If the fish swims to the bottom, the pressure will be greater, but it makes no difference what vase it is in. All vases are filled to equal depths, so the water pressure is the same at the bottom of each vase, regardless of its shape or volume. If water pressure at the bottom of a vase were greater than water pressure at the bottom of a neighboring vase, the greater pressure would force water sideways and then up the narrower vase to a higher level until the pressures at the bottom were equalized. Pressure is depth dependent, not volume dependent, so there is a reason that water seeks its own level.

Restating this as energy equation, the energy per unit volume in an ideal, incompressible liquid is constant throughout its vessel. At the surface, gravitational potential energy is large but liquid pressure energy is low. At the bottom of the vessel, all the gravitational potential energy is converted to pressure energy. The sum of pressure energy and gravitational potential energy per unit volume is constant throughout the volume of the fluid and the two energy components change linearly with the depth. Mathematically, it is described by Bernoulli's equation, where velocity head is zero and comparisons per unit volume in the vessel are

Terms have the same meaning as in section Fluid pressure.

An experimentally determined fact about liquid pressure is that it is exerted equally in all directions. If someone is submerged in water, no matter which way that person tilts his/her head, the person will feel the same amount of water pressure on his/her ears. Because a liquid can flow, this pressure isn't only downward. Pressure is seen acting sideways when water spurts sideways from a leak in the side of an upright can. Pressure also acts upward, as demonstrated when someone tries to push a beach ball beneath the surface of the water. The bottom of a boat is pushed upward by water pressure (buoyancy).

When a liquid presses against a surface, there is a net force that is perpendicular to the surface. Although pressure doesn't have a specific direction, force does. A submerged triangular block has water forced against each point from many directions, but components of the force that are not perpendicular to the surface cancel each other out, leaving only a net perpendicular point. This is why water spurting from a hole in a bucket initially exits the bucket in a direction at right angles to the surface of the bucket in which the hole is located. Then it curves downward due to gravity. If there are three holes in a bucket (top, bottom, and middle), then the force vectors perpendicular to the inner container surface will increase with increasing depth – that is, a greater pressure at the bottom makes it so that the bottom hole will shoot water out the farthest. The force exerted by a fluid on a smooth surface is always at right angles to the surface. The speed of liquid out of the hole is formula_22, where "h" is the depth below the free surface. This is the same speed the water (or anything else) would have if freely falling the same vertical distance "h".

is the kinematic pressure, where formula_2 is the pressure and formula_25 constant mass density. The SI unit of "P" is m/s. Kinematic pressure is used in the same manner as kinematic viscosity formula_26 in order to compute Navier–Stokes equation without explicitly showing the density formula_25.




</doc>
<doc id="23621" url="https://en.wikipedia.org/wiki?curid=23621" title="Polygon">
Polygon

In elementary geometry, a polygon () is a plane figure that is bounded by a finite chain of straight line segments closing in a loop to form a closed polygonal chain or "circuit". These segments are called its "edges" or "sides", and the points where two edges meet are the polygon's "vertices" (singular: vertex) or "corners". The interior of the polygon is sometimes called its "body". An "n"-gon is a polygon with "n" sides; for example, a triangle is a 3-gon. A polygon is a 2-dimensional example of the more general polytope in any number of dimensions.

The basic geometrical notion of a polygon has been adapted in various ways to suit particular purposes. Mathematicians are often concerned only with the bounding closed polygonal chain and with simple polygons which do not self-intersect, and they often define a polygon accordingly. A polygonal boundary may be allowed to intersect itself, creating star polygons and other self-intersecting polygons. These and other generalizations of polygons are described below.

The word "polygon" derives from the Greek adjective πολύς ("polús") "much", "many" and γωνία ("gōnía") "corner" or "angle". It has been suggested that γόνυ ("gónu") "knee" may be the origin of “gon”.

Polygons are primarily classified by the number of sides. See table below.

Polygons may be characterized by their convexity or type of non-convexity:




Euclidean geometry is assumed throughout.

Any polygon has as many corners as it has sides. Each corner has several angles. The two most important ones are:

For a non-self-intersecting (simple) polygon with vertices formula_6 the signed area and the Cartesian coordinates of the centroid are given by:

where formula_9 is the squared distance between formula_10 and formula_11 and

To close the polygon, the first and last vertices are the same, i.e., "x", "y" = "x", "y". The vertices must be ordered according to positive or negative orientation (counterclockwise or clockwise, respectively); if they are ordered negatively, the value given by the area formula will be negative but correct in absolute value, but when calculating formula_14 and formula_15, the signed value of formula_16 (which in this case is negative) should be used. This is commonly called the shoelace formula or Surveyor's formula.

The area "A" of a simple polygon can also be computed if the lengths of the sides, "a", "a", ..., "a" and the exterior angles, "θ", "θ", ..., "θ" are known, from:

The formula was described by Lopshits in 1963.

If the polygon can be drawn on an equally spaced grid such that all its vertices are grid points, Pick's theorem gives a simple formula for the polygon's area based on the numbers of interior and boundary grid points: the former number plus one-half the latter number, minus 1.

In every polygon with perimeter "p" and area "A ", the isoperimetric inequality formula_18 holds.

If any two simple polygons of equal area are given, then the first can be cut into polygonal pieces which can be reassembled to form the second polygon. This is the Bolyai–Gerwien theorem.

The area of a regular polygon is also given in terms of the radius "r" of its inscribed circle and its perimeter "p" by

This radius is also termed its apothem and is often represented as "a".

The area of a regular "n"-gon with side "s" inscribed in a unit circle is

The area of a regular "n"-gon in terms of the radius "R" of its circumscribed circle and its perimeter "p" is given by

The area of a regular "n"-gon inscribed in a unit-radius circle, with side "s" and interior angle formula_22 can also be expressed trigonometrically as

The lengths of the sides of a polygon do not in general determine the area. However, if the polygon is cyclic the sides "do" determine the area.

Of all "n"-gons with given sides, the one with the largest area is cyclic. Of all "n"-gons with a given perimeter, the one with the largest area is regular (and therefore cyclic).

The area of a self-intersecting polygon can be defined in two different ways, each of which gives a different answer:

The idea of a polygon has been generalized in various ways. Some of the more important include:

The word "polygon" comes from Late Latin "polygōnum" (a noun), from Greek πολύγωνον ("polygōnon/polugōnon"), noun use of neuter of πολύγωνος ("polygōnos/polugōnos", the masculine adjective), meaning "many-angled". Individual polygons are named (and sometimes classified) according to the number of sides, combining a Greek-derived numerical prefix with the suffix "-gon", e.g. "pentagon", "dodecagon". The triangle, quadrilateral and nonagon are exceptions.

Beyond decagons (10-sided) and dodecagons (12-sided), mathematicians generally use numerical notation, for example 17-gon and 257-gon.

Exceptions exist for side counts that are more easily expressed in verbal form (e.g. 20 and 30), or are used by non-mathematicians. Some special polygons also have their own names; for example the regular star pentagon is also known as the pentagram.
To construct the name of a polygon with more than 20 and less than 100 edges, combine the prefixes as follows. The "kai" term applies to 13-gons and higher and was used by Kepler, and advocated by John H. Conway for clarity to concatenated prefix numbers in the naming of quasiregular polyhedra.

Polygons have been known since ancient times. The regular polygons were known to the ancient Greeks, with the pentagram, a non-convex regular polygon (star polygon), appearing as early as the 7th century B.C. on a krater by Aristonothos, found at Caere and now in the Capitoline Museum.

The first known systematic study of non-convex polygons in general was made by Thomas Bradwardine in the 14th century.

In 1952, Geoffrey Colin Shephard generalized the idea of polygons to the complex plane, where each real dimension is accompanied by an imaginary one, to create complex polygons.

Polygons appear in rock formations, most commonly as the flat facets of crystals, where the angles between the sides depend on the type of mineral from which the crystal is made.

Regular hexagons can occur when the cooling of lava forms areas of tightly packed columns of basalt, which may be seen at the Giant's Causeway in Northern Ireland, or at the Devil's Postpile in California.

In biology, the surface of the wax honeycomb made by bees is an array of hexagons, and the sides and base of each cell are also polygons.

In computer graphics, a polygon is a primitive used in modelling and rendering. They are defined in a database, containing arrays of vertices (the coordinates of the geometrical vertices, as well as other attributes of the polygon, such as color, shading and texture), connectivity information, and materials.

Naming conventions differ from those of mathematicians:

Any surface is modelled as a tessellation called polygon mesh. If a square mesh has points (vertices) per side, there are "n" squared squares in the mesh, or 2"n" squared triangles since there are two triangles in a square. There are vertices per triangle. Where "n" is large, this approaches one half. Or, each vertex inside the square mesh connects four edges (lines).

The imaging system calls up the structure of polygons needed for the scene to be created from the database. This is transferred to active memory and finally, to the display system (screen, TV monitors etc.) so that the scene can be viewed. During this process, the imaging system renders polygons in correct perspective ready for transmission of the processed data to the display system. Although polygons are two-dimensional, through the system computer they are placed in a visual scene in the correct three-dimensional orientation.

In computer graphics and computational geometry, it is often necessary to determine whether a given point "P" = ("x","y") lies inside a simple polygon given by a sequence of line segments. This is called the Point in polygon test.





</doc>
<doc id="23622" url="https://en.wikipedia.org/wiki?curid=23622" title="Player character">
Player character

A player character (also known as PC and playable character) is a fictional character in a role-playing game or video game whose actions are directly controlled by a player of the game rather than the rules of the game. The characters that are not controlled by a player are called non-player characters (NPCs). The actions of non-player characters are typically handled by the game itself in video games, or according to rules followed by a gamemaster refereeing tabletop role-playing games. The player character functions as a fictional, alternate body for the player controlling it.

Video games typically have one player character for each person playing the game. Some games offer a group of player characters for the player to choose from, allowing the player to control one of them at a time. Where more than one player character is available, the characters may have different abilities, strengths, and weaknesses to make the game play style different.

A player character may sometimes be based on a real person, especially in sports games that use the names and likenesses of real sports people. Historical people and leaders may sometimes appear as characters too, particularly in strategy or empire building games such as in Sid Meier's "Civilization" series. Curiously, in the case of Civilization, a player's chosen historical character is the same throughout the course of the game despite the fact that a campaign can last several hundred years before and after the lifetime of the real historical persona. Such a player character is more properly an avatar as the player character's name and image typically have little bearing on the game itself. Avatars are also commonly seen in casino game simulations.

In many video games, and especially first-person shooters, the player character is a "blank slate" without any notable characteristics or even backstory. Pac-Man, Crono, Link and Chell are examples of such characters. These characters are generally silent protagonists.

Some games will go even further, never showing or naming the player-character at all. This is somewhat common in first-person videogames, such as in "Myst", but is more often done in strategy video games such as "Dune 2000" and "". In such games, the only real indication that the player has a character (instead of an omnipresent status), is from the cutscenes during which the character is being given a mission briefing or debriefing; the player is usually addressed as "general", "commander", or another military rank.

In gaming culture, such a character was called Ageless, Faceless, Gender-Neutral, Culturally Ambiguous Adventure Person, abbreviated as AFGNCAAP; a term that originated in "" where it is used satirically to refer to the player.

Fighting games typically have a larger number of player characters to choose from, with some basic moves available to all or most characters and some unique moves only available to one or a few characters. Having many different characters to play as and against, all possessing different moves and abilities, is necessary to create a larger gameplay variety in such games.

In role playing games such as "Dungeons and Dragons" or "Final Fantasy," a player typically creates or takes on the identity of a character that may have nothing in common with the player. The character is usually of a certain (often fictional) race and class (such as zombie, berserker, rifleman, elf, or cleric), each with strengths and weaknesses. The attributes of the characters (such as magic and fighting ability) are given as numerical values which can be increased as the gamer progresses and gains rank and experience points through accomplishing goals or fighting enemies.

A secret or unlockable character is a playable character in a video game available only after completing the game or meeting another requirement. In some video games, characters that are not secret but appear only as non-player characters like bosses or enemies become playable characters after completing certain requirements, or sometimes cheating.



</doc>
<doc id="23623" url="https://en.wikipedia.org/wiki?curid=23623" title="Parish">
Parish

A parish is a church territorial entity constituting a division within a diocese. A parish is under the pastoral care and clerical jurisdiction of a parish priest, who might be assisted by one or more curates, and who operates from a parish church. Historically, a parish often covered the same geographical area as a manor. Its association with the parish church remains paramount.

By extension the term "parish" refers not only to the territorial entity but to the people of its community or congregation as well as to church property within it. In England this church property was technically in ownership of the parish priest "ex-officio", vested in him on his institution to that parish.

First attested in English in the late 13th century, the word "parish" comes from the Old French "paroisse", in turn from , the latinisation of the , "sojourning in a foreign land", itself from ("paroikos"), "dwelling beside, stranger, sojourner", which is a compound of ("pará"), "beside, by, near" and οἶκος ("oîkos"), "house".

As an ancient concept, the term "parish" occurs in the long-established Christian denominations: Roman Catholic, Anglican Communion, the Eastern Orthodox Church, and Lutheran churches, and in some Methodist and Presbyterian administrations.

The eighth Archbishop of Canterbury Theodore of Tarsus (c. 602–690) appended the parish structure to the Anglo-Saxon township unit, where it existed, and where minsters catered to the surrounding district.

Broadly speaking, the parish is the standard unit in episcopal polity of church administration, although parts of a parish may be subdivided as a "chapelry", with a chapel of ease or filial church serving as the local place of worship in cases of difficulty to access the main parish church.

In the wider picture of ecclesiastical polity, a "parish" comprises a division of a diocese or see. Parishes within a diocese may be grouped into a deanery or "vicariate forane" (or simply "vicariate"), overseen by a dean or "vicar forane", or in some cases by an archpriest. Some churches of the Anglican Communion have deaneries as units of an archdeaconry.

The Church of England geographical structure uses the local parish church as its basic unit. The parish system survived the Reformation with the Anglican Church's secession from Rome remaining largely untouched, thus it shares its roots with the Catholic Church's system described above. Parishes may extend into different counties or hundreds and historically many parishes comprised extra outlying portions in addition to its principal district, usually being described as 'detached' and intermixed with the lands of other parishes. Church of England parishes nowadays all lie within one of 44 dioceses divided between the provinces of Canterbury, 30 and York, 14.

Each parish normally has its own parish priest (either a vicar or rector, owing to the vagaries of the feudal tithe system: rectories usually having had greater income) and perhaps supported by one or more curates or deacons - although as a result of ecclesiastical pluralism some parish priests might have held more than one parish living, placing a curate in charge of those where they do not reside. Now, however, it is common for a number of neighbouring parishes to be placed under one benefice in the charge of a priest who conducts services by rotation, with additional services being provided by lay readers or other non-ordained members of the church community.

A chapelry was a subdivision of an ecclesiastical parish in England, and parts of Lowland Scotland up to the mid 19th century. It had a similar status to a township but was so named as it had a chapel which acted as a subsidiary place of worship to the main parish church.

In England civil parishes and their governing parish councils evolved in the 19th century as ecclesiastical parishes began to be relieved of what became considered to be civic responsibilities. Thus their boundaries began to diverge. The word "parish" acquired a secular usage. Since 1895, a parish council elected by public vote or a (civil) parish meeting administers a civil parish and is formally recognised as the level of local government below a district council.

The traditional structure of the Church of England with the parish as the basic unit has been exported to other countries and churches throughout the Anglican Communion and Commonwealth but does not necessarily continue to be administered in the same way.

The parish is also the basic level of church administration in the Church of Scotland. Spiritual oversight of each parish church in Scotland is responsibility of the congregation's Kirk Session. Patronage was regulated in 1711 (Patronage Act) and abolished in 1874, with the result that ministers must be elected by members of the congregation. Many parish churches in Scotland today are "linked" with neighbouring parish churches served by a single minister. Since the abolition of parishes as a unit of civil government in Scotland in 1929, Scottish parishes have purely ecclesiastical significance and the boundaries may be adjusted by the local Presbytery.

The church in Wales was disestablished in 1920 and is made up of six dioceses. Parishes were civil administration areas until replaced by communities in 1974.

Although they are more often simply called congregations and have no geographic boundaries, in the United Methodist Church congregations are called parishes. A prominent example of this usage comes in "The Book of Discipline of The United Methodist Church", in which the committee of every local congregation that handles staff support is referred to as the committee on Pastor-Parish Relations. This committee gives recommendations to the bishop on behalf of the parish/congregation since it is the United Methodist Bishop of the episcopal area who appoints a pastor to each congregation. The same is true in the African Methodist Episcopal Church and the Christian Methodist Episcopal Church.

In New Zealand, a local grouping of Methodist churches that share one or more ministers (which in the United Kingdom would be called a circuit) is referred to as a parish.

In the Catholic Church, each parish normally has its own parish priest (in some countries called pastor), who has responsibility and canonical authority over the parish.

What in most English-speaking countries is termed the "parish priest" is referred to as the "pastor" in the United States, where the term "parish priest" is used of any priest assigned to a parish even in a subordinate capacity. These are called "assistant priests", "parochial vicars", "curates", or, in the United States, "associate pastors" and "assistant pastors".

Each diocese (administrative region) is divided into parishes, each with their own central church called the parish church, where religious services take place. Some larger parishes or parishes that have been combined under one parish priest may have two or more such churches, or the parish may be responsible for chapels (or chapels of ease) located at some distance from the mother church for the convenience of distant parishioners.

Normally, a parish comprises all Catholics living within its geographically defined area, but non-territorial parishes can also be established within a defined area on a personal basis for Catholics belonging to a particular rite, language, nationality, or community. An example is that of personal parishes established in accordance with the 7 July 2007 "motu proprio" "Summorum Pontificum" for those attached to the older Extraordinary form of the Roman Rite.

Most Catholic parishes are part of Latin Rite dioceses, which together cover the whole territory of a country. There can also be overlapping parishes of eparchies of Eastern Catholic Churches, personal ordinariates or military ordinariates.







</doc>
<doc id="23624" url="https://en.wikipedia.org/wiki?curid=23624" title="Procopius">
Procopius

Procopius of Caesarea ( "Prokopios ho Kaisareus", ; 500 – 554 AD) was a prominent late antique Greek scholar from Palaestina Prima. Accompanying the Byzantine chief-general Belisarius in the wars of the Emperor Justinian, he became the principal Greek-Byzantine historian of the 6th century, writing the "Wars" (or "Histories"), the "Buildings of Justinian" and the now-celebrated (and infamous) "Secret History". He is commonly held to be the last major historian of the ancient Western world.

Apart from his own writings, the main source for Procopius' life is an entry in the "Suda", a Roman encyclopaedia, written sometime after 975, which tells everything about his early life. He was a native of Caesarea in the Roman Province "Palaestina Prima". He would have received a conventional elite education in the Greek classics and then rhetoric, perhaps at the famous School of Gaza, may have attended law school, possibly at Berytus (modern Beirut) or Constantinople, and became a "rhetor" (barrister or advocate). He evidently knew Latin, as was natural for a man with legal training. In 527, the first year of Eastern Roman Emperor Justinian's reign, he became the "adsessor" (legal adviser) for Belisarius, Justinian's chief military commander who was then beginning a brilliant career.

Procopius was with Belisarius on the eastern front until the latter was defeated at the Battle of Callinicum in 531 and recalled to Constantinople. Procopius witnessed the Nika riots of January, 532, which Belisarius and his fellow general Mundus repressed with a massacre in the Hippodrome. In 533, he accompanied Belisarius on his victorious expedition against the Vandal kingdom in North Africa, took part in the capture of Carthage, and remained in Africa with Belisarius' successor Solomon the Eunuch when Belisarius returned to Constantinople. Procopius recorded a few of the extreme weather events of 535–536, although these were presented as a backdrop to Roman (Byzantine) military activities, such as a mutiny, in and near Carthage. He rejoined Belisarius for his campaign against the Ostrogothic kingdom in Italy and experienced the Gothic siege of Rome that lasted a year and nine days, ending in mid-March 538. He witnessed Belisarius' entry into the Gothic capital, Ravenna, in 540. Book Eight of the "Wars of Justinian", and the "Secret History", suggest that his relationship with Belisarius seems to have cooled thereafter. When Belisarius was sent back to Italy in 544 to cope with a renewal of the war with the Goths, now led by the able king Totila, Procopius appears to have no longer been on Belisarius' staff. 

As "magister militum", Belisarius was a "vir illustris", and Procopius, as his "adsessor", must, therefore, have had at least the rank of a "vir spectabilis". He thus belonged to the middle-ranking group of the "ordo senatorius". However, the "Suda", which is usually well informed in such matters, also describes Procopius himself as ἰλλούστριος . Should this information be correct, then Procopius had a seat in the senate of Constantinople, which was restricted to the "illustres" under Justinian.

It is not known when Procopius himself died, and many historians (James Howard-Johnson, Averil Cameron, Geoffrey Greatrex) date his death to 554, but in 562 there was an urban prefect of Constantinople ("praefectus urbi Constantinopolitanae") who happened to be called Procopius. In that year, Belisarius was implicated in a conspiracy and was brought before this urban prefect.

The writings of Procopius are the primary source of information for the rule of the Eastern Roman emperor Justinian. Procopius was the author of a history in eight books of the wars fought by Justinian I, a panegyric on Justinian's public works throughout the empire, and a book known as the "Secret History" that claims to report the scandals that Procopius could not include in his published history.

Procopius' "Wars of Justinian" (, "Hypèr tōn polémon lógoi"; , "About the Wars") is clearly his most important work, although it is not as well known as the "Secret History". The first seven books, which may have been published as a unit, seem to have been largely completed by 545, but were updated to mid-century before publication, for the latest event mentioned belongs to early 551. The first two books (often known as the "Persian War", Latin "De Bello Persico") deal with the conflict between the Romans and Sassanid Persia in Mesopotamia, Syria, Armenia, Lazica and Caucasian Iberia (roughly modern-day Georgia). It details the campaigns of the Sasanian Shah Kavadh I, the 'Nika' revolt in Constantinople in 532, the war by Kavadh's successor, Khosrau I, in 540 and his destruction of Antioch and the deportation of its inhabitants to Mesopotamia, and the great plague that devastated Constantinople in 542. They also cover the early career of the Roman general Belisarius, Procopius' patron, in some detail. The next two books, the "Vandal War" (Latin "De Bello Vandalico"), cover Belisarius' successful campaign against the Vandal kingdom in Roman Africa. The remaining books cover the "Gothic War" (Latin "De Bello Gothico"), the campaigns by Belisarius and others to recapture Italy, then under the rule of the Ostrogoths. This includes accounts of the sieges of Naples and Rome.

Later, Procopius added an eighth book ("Wars VIII" or "Gothic War IV"), which brings the history to 552/553, when a Roman army led by the eunuch Narses finally destroyed the Ostrogothic kingdom. This eighth book covers campaigns both in Italy and on the Eastern frontier.

The "Wars of Justinian" was influential on later Byzantine history-writing. A continuation of Procopius' work was written after his death by the poet and historian Agathias of Myrina.

The famous "Secret History" ( "Apokrýphe Historía", ) was discovered centuries later in the Vatican Library and published by Niccolò Alamanni in 1623 at Lyons. Its existence was already known from the "Suda", which referred to it as the "Anekdota" (, Latin "Anecdota", "unpublished writings"). The "Secret History" covers roughly the same years as the first seven books of the "History of Justinian's Wars" and appears to have been written after they were published. Current consensus generally dates it to 550 or 558, or maybe even as late as 562.

In the eyes of many scholars, the "Secret History" reveals an author who had become deeply disillusioned with the emperor Justinian and his wife, Empress Theodora, as well as Belisarius, his former commander and patron, and Antonina, Belisarius' wife. The anecdotes claim to expose the secret springs of their public actions, as well as the private lives of the emperor, his wife and their entourage. Justinian is portrayed as cruel, venal, prodigal and incompetent; as for Theodora, the reader is treated to the most detailed and titillating portrayals of vulgarity and insatiable lust combined with shrewish and calculating mean-spiritedness. However, it has been argued that Procopius feared that a conspiracy could overthrow the imperial power, and therefore prepared an exaggerated document in order to clear himself of all accusations of proximity to the former imperial power; if this hypothesis is correct, the "Secret History" cannot be seen as proof that Procopius hated Justinian and Theodora.

Among the more titillating (and doubtful) revelations in the "Secret History" is Procopius' account of Theodora's thespian accomplishments:

Her husband Justinian, meanwhile, was a monster whose head could suddenly vanish—at least according to this passage:

The "Buildings of Justinian" ( "Perì Ktismáton", , "On Buildings") is a panegyric on Justinian's building activity in the empire. The first book may date to before the collapse of the first dome of Hagia Sophia in 557, but some scholars think that it is possible that the work postdates the building of the bridge over the Sangarius in the late 550s. The "Peri ktismaton" (or "De Aedificiis") tells us nothing further about Belisarius, but it takes a sharply different attitude towards Justinian. He is presented as an idealised Christian emperor who built churches for the glory of God and defenses for the safety of his subjects and who showed particular concern for the water supply. He built new aqueducts as well as restoring those that had fallen into disuse.

Historians consider "Buildings" to be an incomplete work, due to evidence of the surviving version being a draft with two possible redactions.

Theodora, who was dead when this panegyric was written, is mentioned only briefly, but Procopius' praise of her beauty is fulsome. The panegyric was likely written at Justinian's behest, however, and it is doubtful that the sentiments expressed are sincere.

Due to the panegyrical nature of "The Buildings", historians have discovered in several occasions discrepancies between claims made by Procopius and other primary sources. A primary example is in Procopius starting the reign of Justinian in 518, which was actually the start of the reign of Justin I, Justinian's predecessor and uncle. This discrepancy can be seen as part of Procopius' panegyric method, as it allowed him to credit buildings constructed under the rule Justin I as Justinian's accomplishments. In this context can be mentioned the renovations to the walls of Edessa after a flood in 525, along with several churches in the region, all of which were completed under Justinian's uncle. Similarly, Procopius falsely credits Justinian for the extensive re-fortifications made in the cities of Tomis and Histria in Scythia Minor, along the Danubian frontier, actual accomplishments of Anastasius I, predecessor of Justin I.

Procopius belongs to the school of late antique secular historians who continued the traditions of the Second Sophistic; they wrote in Attic Greek, their models were Herodotus, Polybius and especially Thucydides, and their subject matter was secular history. They avoided vocabulary unknown to Attic Greek and inserted an explanation when they had to use contemporary words. Thus Procopius explains to his readers that "ekklesia", meaning a Christian church, is the equivalent of a temple or shrine and that monks are "the most temperate of Christians ... whom men are accustomed to call monks" ("Wars" 2.9.14; 1.7.22). In classical Athens, monks had been unknown and an "ekklesia" was the assembly of Athenian citizens that passed the laws.

The secular historians eschewed the history of the Christian church, which they left to ecclesiastical history—a genre that was founded by Eusebius of Caesarea. However, Averil Cameron has argued convincingly that Procopius' works reflect the tensions between the classical and Christian models of history in 6th century Byzantium. This is supported by Mary Whitby's analysis of Procopius' depiction of Constantinople and the Church of Hagia Sophia in comparison to contemporary pagan panegyrics ("Buildings", Book I). Procopius can be seen as depicting Justinian as essentially God's Vicegerent, making the case for buildings being a primarily religious panegyric.

Procopius indicated ("Secret History" 26.18) that he planned to write an ecclesiastical history himself and, if he had, he would probably have followed the rules of that genre. But, as far as it is known, the ecclesiastical history remained unwritten.

A number of historical novels based on Procopius' works (along with other sources) have been written, one of which, "Count Belisarius", was written by poet and novelist Robert Graves in 1938. Procopius himself appears as a minor character in Felix Dahn's "A Struggle for Rome" and in L. Sprague de Camp's alternate history novel "Lest Darkness Fall". The novel's main character, archaeologist Martin Padway, derives most of his knowledge of historical events from the "Secret History".






</doc>
<doc id="23626" url="https://en.wikipedia.org/wiki?curid=23626" title="Property">
Property

Property, in the abstract, is what belongs to or with something, whether as an attribute or as a component of said thing. In the context of this article, it is one or more components (rather than attributes), whether physical or incorporeal, of a person's estate; or so belonging to, as in being owned by, a person or jointly a group of people or a legal entity like a corporation or even a society. Depending on the nature of the property, an owner of property has the right to consume, alter, share, redefine, rent, mortgage, pawn, sell, exchange, transfer, give away or destroy it, or to exclude others from doing these things, as well as to perhaps abandon it; whereas regardless of the nature of the property, the owner thereof has the right to properly use it (as a durable, mean or factor, or whatever), or at the very least exclusively keep it.

In economics and political economy, there are three broad forms of property: private property, public property, and collective property (also called cooperative property).

Property that jointly belongs to more than one party may be possessed or controlled thereby in very similar or very distinct ways, whether simply or complexly, whether equally or unequally. However, there is an expectation that each party's will (rather discretion) with regard to the property be clearly defined and unconditional, so as to distinguish ownership and easement from rent. The parties might expect their wills to be unanimous, or alternately every given one of them, when no opportunity for or possibility of dispute with any other of them exists, may expect his, her, its or their own will to be sufficient and absolute.

The Restatement (First) of Property defines property as anything, tangible or intangible whereby a legal relationship between persons and the state enforces a possessory interest or legal title in that thing. This mediating relationship between individual, property and state is called a property regime.

In sociology and anthropology, property is often defined as a relationship between two or more individuals and an object, in which at least one of these individuals holds a bundle of rights over the object. The distinction between "collective property" and "private property" is regarded as a confusion since different individuals often hold differing rights over a single object.

Important widely recognized types of property include real property (the combination of land and any improvements to or on the land), personal property (physical possessions belonging to a person), private property (property owned by legal persons, business entities or individual natural persons), public property (state owned or publicly owned and available possessions) and intellectual property (exclusive rights over artistic creations, inventions, etc.), although the last is not always as widely recognized or enforced. An article of property may have physical and incorporeal parts. A title, or a right of ownership, establishes the relation between the property and other persons, assuring the owner the right to dispose of the property as the owner sees fit.

Often property is defined by the code of the local sovereignty, and protected wholly or more usually partially by such entity, the owner being responsible for any remainder of protection. The standards of proof concerning proofs of ownerships are also addressed by the code of the local sovereignty, and such entity plays a role accordingly, typically somewhat managerial. Some philosophers assert that property rights arise from social convention, while others find justifications for them in morality or in natural law.

Various scholarly disciplines (such as law, economics, anthropology or sociology) may treat the concept more systematically, but definitions vary, most particularly when involving contracts. Positive law defines such rights, and the judiciary can adjudicate and enforce property rights.

According to Adam Smith, the expectation of profit from "improving one's stock of capital" rests on private property rights. Capitalism has as a central assumption that property rights encourage their holders to develop the property, generate wealth, and efficiently allocate resources based on the operation of markets. From this has evolved the modern conception of property as a right enforced by positive law, in the expectation that this will produce more wealth and better standards of living. However, Smith also expressed a very critical view on the effects of property laws on inequality:

In his text "The Common Law", Oliver Wendell Holmes describes property as having two fundamental aspects. The first, possession, can be defined as control over a resource based on the practical inability of another to contradict the ends of the possessor. The second, title, is the expectation that others will recognize rights to control resource, even when it is not in possession. He elaborates the differences between these two concepts, and proposes a history of how they came to be attached to persons, as opposed to families or to entities such as the church.



Both communism and some kinds of socialism have also upheld the notion that private ownership of capital is inherently illegitimate. This argument centers mainly on the idea that private ownership of capital always benefits one class over another, giving rise to domination through the use of this privately owned capital. Communists do not oppose personal property that is "hard-won, self-acquired, self-earned" (as the Communist Manifesto puts it) by members of the proletariat. Both socialism and communism distinguish carefully between private ownership of capital (land, factories, resources, etc.) and private property (homes, material objects and so forth).

Most legal systems distinguish between different types of property, especially between land (immovable property, estate in land, real estate, real property) and all other forms of property—goods and chattels, movable property or personal property, including the value of legal tender if not the legal tender itself, as the manufacturer rather than the possessor might be the owner. They often distinguish tangible and intangible property. One categorization scheme specifies three species of property: land, improvements (immovable man-made things), and personal property (movable man-made things).

In common law, real property (immovable property) is the combination of interests in land and improvements thereto, and personal property is interest in movable property. Real property rights are rights relating to the land. These rights include ownership and usage. Owners can grant rights to persons and entities in the form of leases, licenses and easements.

Throughout the last centuries of the second millennium, with the development of more complex theories of property, the concept of personal property had become divided into tangible property (such as cars and clothing) and intangible property (such as financial instruments—including stocks and bonds—intellectual property—including patents, copyrights and trademarks—digital files, communication channels, and certain forms of identifier—including Internet domain names, some forms of network address, some forms of handle and again trademarks).

Treatment of intangible property is such that an article of property is, by law or otherwise by traditional conceptualization, subject to expiration even when inheritable, which is a key distinction from tangible property. Upon expiration, the property, if of the intellectual category, becomes a part of public domain, to be used by but not owned by anybody, and possibly used by more than one party simultaneously due the inapplicability of scarcity to intellectual property. Whereas things such as communications channels and pairs of electromagnetic spectrum band and signal transmission power can only be used by a single party at a time, or a single party in a divisible context, if owned or used at all. Thus far or usually those are not considered property, or at least not private property, even though the party bearing right of exclusive use may transfer that right to another.

Of the following, only sale and at-will sharing involve no encumbrance.

The two major justifications given for original property, or the homestead principle, are effort and scarcity. John Locke emphasized effort, "mixing your labor" with an object, or clearing and cultivating virgin land. Benjamin Tucker preferred to look at the telos of property, i.e. What is the purpose of property? His answer: to solve the scarcity problem. Only when items are relatively scarce with respect to people's desires do they become property. For example, hunter-gatherers did not consider land to be property, since there was no shortage of land. Agrarian societies later made arable land property, as it was scarce. For something to be economically scarce it must necessarily have the "exclusivity property"—that use by one person excludes others from using it. These two justifications lead to different conclusions on what can be property. Intellectual property—incorporeal things like ideas, plans, orderings and arrangements (musical compositions, novels, computer programs)—are generally considered valid property to those who support an effort justification, but invalid to those who support a scarcity justification, since the things don't have the exclusivity property (however, those who support a scarcity justification may still support other "intellectual property" laws such as Copyright, as long as these are a subject of contract instead of government arbitration). Thus even ardent propertarians may disagree about IP. By either standard, one's body is one's property.

From some anarchist points of view, the validity of property depends on whether the "property right" requires enforcement by the state. Different forms of "property" require different amounts of enforcement: intellectual property requires a great deal of state intervention to enforce, ownership of distant physical property requires quite a lot, ownership of carried objects requires very little, while ownership of one's own body requires absolutely no state intervention. Some anarchists don't believe in property at all.

Many things have existed that did not have an owner, sometimes called the commons. The term "commons," however, is also often used to mean something quite different: "general collective ownership"—i.e. common ownership. Also, the same term is sometimes used by statists to mean government-owned property that the general public is allowed to access (public property). Law in all societies has tended to develop towards reducing the number of things not having clear owners. Supporters of property rights argue that this enables better protection of scarce resources, due to the tragedy of the commons, while critics argue that it leads to the 'exploitation' of those resources for personal gain and that it hinders taking advantage of potential network effects. These arguments have differing validity for different types of "property"—things that are not scarce are, for instance, not subject to the tragedy of the commons. Some apparent critics advocate general collective ownership rather than ownerlessness.

Things that do not have owners include: ideas (except for intellectual property), seawater (which is, however, protected by anti-pollution laws), parts of the seafloor (see the United Nations Convention on the Law of the Sea for restrictions), gases in Earth's atmosphere, animals in the wild (although in most nations, animals are tied to the land. In the United States and Canada wildlife are generally defined in statute as property of the state. This public ownership of wildlife is referred to as the North American Model of Wildlife Conservation and is based on The Public Trust Doctrine.), celestial bodies and outer space, and land in Antarctica.

The nature of children under the age of majority is another contested issue here. In ancient societies children were generally considered the property of their parents. Children in most modern societies theoretically own their own bodies but are not considered competent to exercise their rights, and their parents or guardians are given most of the actual rights of control over them.

Questions regarding the nature of ownership of the body also come up in the issue of abortion, drugs and euthanasia.

In many ancient legal systems (e.g. early Roman law), religious sites (e.g. temples) were considered property of the God or gods they were devoted to. However, religious pluralism makes it more convenient to have religious sites owned by the religious body that runs them.

Intellectual property and air (airspace, no-fly zone, pollution laws, which can include tradable emissions rights) can be property in some senses of the word.

Ownership of land can be held separately from the ownership of rights over that land, including sporting rights, mineral rights, development rights, air rights, and such other rights as may be worth segregating from simple land ownership.

Ownership laws may vary widely among countries depending on the nature of the property of interest (e.g. firearms, real property, personal property, animals). Persons can own property directly. In most societies legal entities, such as corporations, trusts and nations (or governments) own property.

In many countries women have limited access to property following restrictive inheritance and family laws, under which only men have actual or formal rights to own property.

In the Inca empire, the dead emperors, who were considered gods, still controlled property after death.

Under United States law the principal limitations on whether and the extent to which the State may interfere with property rights are set by the Constitution. The "Takings" clause requires that the government (whether state or federal—for the 14th Amendment's due process clause imposes the 5th Amendment's takings clause on state governments) may take private property only for a public purpose, after exercising due process of law, and upon making "just compensation." If an interest is not deemed a "property" right or the conduct is merely an intentional tort, these limitations do not apply and the doctrine of sovereign immunity precludes relief. Moreover, if the interference does not almost completely make the property valueless, the interference will not be deemed a taking but instead a mere regulation of use. On the other hand, some governmental regulations of property use have been deemed so severe that they have been considered "regulatory takings." Moreover, conduct sometimes deemed only a nuisance or other tort has been held a taking of property where the conduct was sufficiently persistent and severe.

There exist many theories of property. One is the relatively rare first possession theory of property, where ownership of something is seen as justified simply by someone seizing something before someone else does. Perhaps one of the most popular is the natural rights definition of property rights as advanced by John Locke. Locke advanced the theory that God granted dominion over nature to man through Adam in the book of Genesis. Therefore, he theorized that when one mixes one’s labor with nature, one gains a relationship with that part of nature with which the labor is mixed, subject to the limitation that there should be "enough, and as good, left in common for others." (see Lockean proviso)

From the RERUM NOVARUM, Pope Leo XIII wrote "It is surely undeniable that, when a man engages in remunerative labor, the impelling reason and motive of his work is to obtain property, and thereafter to hold it as his very own."

Anthropology studies the diverse systems of ownership, rights of use and transfer, and possession under the term "theories of property." Western legal theory is based, as mentioned, on the owner of property being a legal person. However, not all property systems are founded on this basis.

In every culture studied ownership and possession are the subject of custom and regulation, and "law" where the term can meaningfully be applied. Many tribal cultures balance individual ownership with the laws of collective groups: tribes, families, associations and nations. For example, the 1839 Cherokee Constitution frames the issue in these terms:

Communal property systems describe ownership as belonging to the entire social and political unit. Such arrangements can under certain conditions erode open access resources. This development has been critiqued by the tragedy of the commons.

Corporate systems describe ownership as being attached to an identifiable group with an identifiable responsible individual. The Roman property law was based on such a corporate system. In a well-known paper that contributed to the creation of the field of law and economics in the late 1960s, the American scholar Harold Demsetz described how the concept of property rights makes social interactions easier:

Different societies may have different theories of property for differing types of ownership. Pauline Peters argued that property systems are not isolable from the social fabric, and notions of property may not be stated as such, but instead may be framed in negative terms: for example the taboo system among Polynesian peoples.

In medieval and Renaissance Europe the term "property" essentially referred to land. After much rethinking, land has come to be regarded as only a special case of the property genus. This rethinking was inspired by at least three broad features of early modern Europe: the surge of commerce, the breakdown of efforts to prohibit interest (then called "usury"), and the development of centralized national monarchies.

Urukagina, the king of the Sumerian city-state Lagash, established the first laws that forbade compelling the sale of property.

The Ten Commandments shown in Exodus 20:2–17 and Deuteronomy 5:6–21 stated that the Israelites were not to steal, but the connection between Bronze Age concepts of theft and modern concepts of property is suspect.

Aristotle, in "Politics," advocates "private property." He argues that self-interest leads to neglect of the commons. "[T]hat which is common to the greatest number has the least care bestowed upon it. Every one thinks chiefly of his own, hardly at all of the common interest; and only when he is himself concerned as an individual."

In addition he says that when property is common, there are natural problems that arise due to differences in labor: "If they do not share equally enjoyments and toils, those who labor much and get little will necessarily complain of those who labor little and receive or consume much. But indeed there is always a difficulty in men living together and having all human relations in common, but especially in their having common property." ("Politics, 1261b34")

Cicero held that there is no private property under natural law but only under human law. Seneca viewed property as only becoming necessary when men become avarice. St. Ambrose latter adopted this view and St. Augustine even derided heretics for complaining the Emperor could not confiscate property they had labored for.

The canon law "Decretum Gratiani" maintained that mere human law creates property, repeating the phrases used by St. Augustine. St. Thomas Aquinas agreed with regard to the private consumption of property but modified patristic theory in finding that the private possession of property is necessary. Thomas Aquinas concludes that, given certain detailed provisions,

The principal writings of Thomas Hobbes appeared between 1640 and 1651—during and immediately following the war between forces loyal to King Charles I and those loyal to Parliament. In his own words, Hobbes' reflection began with the idea of "giving to every man his own," a phrase he drew from the writings of Cicero. But he wondered: How can anybody call anything his own? He concluded: My own can only truly be mine if there is one unambiguously strongest power in the realm, and that power treats it as mine, protecting its status as such.

A contemporary of Hobbes, James Harrington, reacted to the same tumult in a different way: he considered property natural but not inevitable. The author of "Oceana", he may have been the first political theorist to postulate that political power is a consequence, not the cause, of the distribution of property. He said that the worst possible situation is one in which the commoners have half a nation's property, with crown and nobility holding the other half—a circumstance fraught with instability and violence. A much better situation (a stable republic) will exist once the commoners own most property, he suggested.

In later years, the ranks of Harrington's admirers included American revolutionary and founder John Adams.

Another member of the Hobbes/Harrington generation, Sir Robert Filmer, reached conclusions much like Hobbes', but through Biblical exegesis. Filmer said that the institution of kingship is analogous to that of fatherhood, that subjects are but children, whether obedient or unruly, and that property rights are akin to the household goods that a father may dole out among his children—his to take back and dispose of according to his pleasure.

In the following generation, John Locke sought to answer Filmer, creating a rationale for a balanced constitution in which the monarch had a part to play, but not an overwhelming part. Since Filmer's views essentially require that the Stuart family be uniquely descended from the patriarchs of the Bible, and since even in the late 17th century that was a difficult view to uphold, Locke attacked Filmer's views in his First Treatise on Government, freeing him to set out his own views in the Second Treatise on Civil Government. Therein, Locke imagined a pre-social world, each of the unhappy residents of which are willing to create a social contract because otherwise "the enjoyment of the property he has in this state is very unsafe, very unsecure," and therefore the "great and chief end, therefore, of men's uniting into commonwealths, and putting themselves under government, is the preservation of their property." They would, he allowed, create a monarchy, but its task would be to execute the will of an elected legislature. "To this end" (to achieve the previously specified goal), he wrote, "it is that men give up all their natural power to the society they enter into, and the community put the legislative power into such hands as they think fit, with this trust, that they shall be governed by declared laws, or else their peace, quiet, and property will still be at the same uncertainty as it was in the state of nature."

Even when it keeps to proper legislative form, though, Locke held that there are limits to what a government established by such a contract might rightly do.

"It cannot be supposed that [the hypothetical contractors] they should intend, had they a power so to do, to give any one or more an absolute arbitrary power over their persons and estates, and put a force into the magistrate's hand to execute his unlimited will arbitrarily upon them; this were to put themselves into a worse condition than the state of nature, wherein they had a liberty to defend their right against the injuries of others, and were upon equal terms of force to maintain it, whether invaded by a single man or many in combination. Whereas by supposing they have given up themselves to the absolute arbitrary power and will of a legislator, they have disarmed themselves, and armed him to make a prey of them when he pleases..."

Note that both "persons "and" estates" are to be protected from the arbitrary power of any magistrate, inclusive of the "power and will of a legislator." In Lockean terms, depredations against an estate are just as plausible a justification for resistance and revolution as are those against persons. In neither case are subjects required to allow themselves to become prey.

To explain the ownership of property Locke advanced a labor theory of property.

In contrast to the figures discussed in this section thus far David Hume lived a relatively quiet life that had settled down to a relatively stable social and political structure. He lived the life of a solitary writer until 1763 when, at 52 years of age, he went off to Paris to work at the British embassy.

In contrast, one might think, to his polemical works on religion and his empiricism-driven skeptical epistemology, Hume's views on law and property were quite conservative.

He did not believe in hypothetical contracts, or in the love of mankind in general, and sought to ground politics upon actual human beings as one knows them. "In general," he wrote, "it may be affirmed that there is no such passion in human mind, as the love of mankind, merely as such, independent of personal qualities, or services, or of relation to ourselves." Existing customs should not lightly be disregarded, because they have come to be what they are as a result of human nature. With this endorsement of custom comes an endorsement of existing governments, because he conceived of the two as complementary: "A regard for liberty, though a laudable passion, ought commonly to be subordinate to a reverence for established government."

Therefore, Hume's view was that there are property rights because of and to the extent that the existing law, supported by social customs, secure them. He offered some practical home-spun advice on the general subject, though, as when he referred to avarice as "the spur of industry," and expressed concern about excessive levels of taxation, which "destroy industry, by engendering despair."

"The property which every man has in his own labour, as it is the original foundation of all other property, so it is the most sacred and inviolable. The patrimony of a poor man lies in the strength and dexterity of his hands; and to hinder him from employing this strength and dexterity in what manner he thinks proper without injury to his neighbour, is a plain violation of this most sacred property. It is a manifest encroachment upon the just liberty both of the workman, and of those who might be disposed to employ him. As it hinders the one from working at what he thinks proper, so it hinders the others from employing whom they think proper. To judge whether he is fit to be employed, may surely be trusted to the discretion of the employers whose interest it so much concerns. The affected anxiety of the law-giver lest they should employ an improper person, is evidently as impertinent as it is oppressive."
— (Source: Adam Smith, "The Wealth of Nations", 1776, Book I, Chapter X, Part II.)

By the mid 19th century, the industrial revolution had transformed England and the United States, and had begun in France. The established conception of what constitutes property expanded beyond land to encompass scarce goods in general. In France, the revolution of the 1790s had led to large-scale confiscation of land formerly owned by church and king. The restoration of the monarchy led to claims by those dispossessed to have their former lands returned.

Section VIII, "Primitive Accumulation" of Capital involves a critique of Liberal Theories of property rights. Marx notes that under Feudal Law, peasants were legally as entitled to their land as the aristocracy was to its manors. Marx cites several historical events in which large numbers of the peasantry were removed from their lands, which were then seized by the aristocracy. This seized land was then used for commercial ventures (sheep heading). Marx sees this "Primitive Accumulation as integral to the creation of English Capitalism. This event created a large unlanded class which had to work for wages in order to survive. Marx asserts that Liberal theories of property are "idyllic" fairy tales that hide a violent historical process.

Charles Comte, in "Traité de la propriété" (1834), attempted to justify the legitimacy of private property in response to the Bourbon Restoration. According to David Hart, Comte had three main points: "firstly, that interference by the state over the centuries in property ownership has had dire consequences for justice as well as for economic productivity; secondly, that property is legitimate when it emerges in such a way as not to harm anyone; and thirdly, that historically some, but by no means all, property which has evolved has done so legitimately, with the implication that the present distribution of property is a complex mixture of legitimately and illegitimately held titles."

Comte, as Proudhon later did, rejected Roman legal tradition with its toleration of slavery. He posited a communal "national" property consisting of non-scarce goods, such as land in ancient hunter-gatherer societies. Since agriculture was so much more efficient than hunting and gathering, private property appropriated by someone for farming left remaining hunter-gatherers with more land per person, and hence did not harm them. Thus this type of land appropriation did not violate the Lockean proviso – there was "still enough, and as good left." Comte's analysis would be used by later theorists in response to the socialist critique on property.

In his 1849 treatise "What is Property?", Pierre Proudhon answers with "Property is theft!" In natural resources, he sees two types of property, "de jure" property (legal title) and "de facto" property (physical possession), and argues that the former is illegitimate. Proudhon's conclusion is that "property, to be just and possible, must necessarily have equality for its condition."

His analysis of the product of labor upon natural resources as property (usufruct) is more nuanced. He asserts that land itself cannot be property, yet it should be held by individual possessors as stewards of mankind with the product of labor being the property of the producer. Proudhon reasoned that any wealth gained without labor was stolen from those who labored to create that wealth. Even a voluntary contract to surrender the product of labor to an employer was theft, according to Proudhon, since the controller of natural resources had no moral right to charge others for the use of that which he did not labor to create and therefore did not own.

Proudhon's theory of property greatly influenced the budding socialist movement, inspiring anarchist theorists such as Mikhail Bakunin who modified Proudhon's ideas, as well as antagonizing theorists like Karl Marx.

Frédéric Bastiat's main treatise on property can be found in chapter 8 of his book "Economic Harmonies" (1850). In a radical departure from traditional property theory, he defines property not as a physical object, but rather as a relationship between people with respect to an object. Thus, saying one owns a glass of water is merely verbal shorthand for "I may justly gift or trade this water to another person". In essence, what one owns is not the object but the value of the object. By "value," Bastiat apparently means "market value"; he emphasizes that this is quite different from utility. ""In our relations with one another, we are not owners of the utility of things, but of their value, and value is the appraisal made of reciprocal services.""

Bastiat theorized that, as a result of technological progress and the division of labor, the stock of communal wealth increases over time; that the hours of work an unskilled laborer expends to buy e.g. 100 liters of wheat decreases over time, thus amounting to "gratis" satisfaction. Thus, private property continually destroys itself, becoming transformed into communal wealth. The increasing proportion of communal wealth to private property results in a tendency toward equality of mankind. ""Since the human race started from the point of greatest poverty, that is, from the point where there were the most obstacles to be overcome, it is clear that all that has been gained from one era to the next has been due to the spirit of property.""

This transformation of private property into the communal domain, Bastiat points out, does not imply that private property will ever totally disappear. This is because man, as he progresses, continually invents new and more sophisticated needs and desires.

Andrew J. Galambos (1924–1997) was an astrophysicist and philosopher who innovated a social structure that seeks to maximize human peace and freedom. Galambos’ concept of property was basic to his philosophy. He defined property as a man’s life and all non-procreative derivatives of his life. (Because the English language is deficient in omitting the feminine from “man” when referring to humankind, it is implicit and obligatory that the feminine is included in the term “man”.)

Galambos taught that property is essential to a non-coercive social structure. That is why he defined freedom as follows: “Freedom is the societal condition that exists when every individual has full (100%) control over his own property.” Galambos defines property as having the following elements:

Property includes all non-procreative derivatives of an individual’s life; this means children are not the property of their parents. and "primary property" (a person's own ideas).

Galambos emphasized repeatedly that true government exists to protect property and that the state attacks property.
For example, the state requires payment for its services in the form of taxes whether or not people desire such services. Since an individual’s money is his property, the confiscation of money in the form of taxes is an attack on property. Military conscription is likewise an attack on a person’s primordial property.

Contemporary political thinkers who believe that natural persons enjoy rights to own property and to enter into contracts espouse two views about John Locke. On the one hand, some admire Locke, such as W.H. Hutt (1956), who praised Locke for laying down the "quintessence of individualism". On the other hand, those such as Richard Pipes regard Locke's arguments as weak, and think that undue reliance thereon has weakened the cause of individualism in recent times. Pipes has written that Locke's work "marked a regression because it rested on the concept of Natural Law" rather than upon Harrington's sociological framework.

Hernando de Soto has argued that an important characteristic of capitalist market economy is the functioning state protection of property rights in a formal property system which clearly records ownership and transactions. These property rights and the whole formal system of property make possible:

All of the above, according to de Soto, enhance economic growth.


Property-giving (legal)


Property-taking (legal)


Property-taking (illegal)
Exclusive or discretionary relational constructs




</doc>
<doc id="23627" url="https://en.wikipedia.org/wiki?curid=23627" title="Police">
Police

A police force is a constituted body of persons empowered by a state to enforce the law, to protect people and property, and to prevent crime and civil disorder. Their powers include the power of arrest and the legitimized use of force. The term is most commonly associated with police services of a sovereign state that are authorized to exercise the police power of that state within a defined legal or territorial area of responsibility. Police forces are often defined as being separate from military or other organizations involved in the defense of the state against foreign aggressors; however, gendarmerie are military units charged with civil policing. The police force is usually a public sector service, funded through taxes.

Law enforcement is only part of policing activity. Policing has included an array of activities in different situations, but the predominant ones are concerned with the preservation of order. In some societies, in the late 18th and early 19th centuries, these developed within the context of maintaining the class system and the protection of private property. Police forces have become ubiquitous in modern societies, though some are involved to varying degrees in corruption, police brutality and the enforcement of authoritarian rule.

Alternative names for a police force include constabulary, gendarmerie, police department, police service, crime prevention, protective services, law enforcement agency, civil guard or civic guard. Members may be referred to as police officers, troopers, sheriffs, constables, rangers, peace officers or civic/civil guards. The word police is most universal and can be seen in many non-English speaking countries. 

As police are often interacting with individuals, slang terms are numerous. Many slang terms for police officers are decades or centuries old with lost etymology. One of the oldest, "cop", has largely lost its slang connotations and become a common colloquial term used both by the public and police officers to refer to their profession.

First attested in English in the early 15th century, initially in a range of senses encompassing '(public) policy; state; public order', the word "police" comes from Middle French "police" ('public order, administration, government'), in turn from Latin "politia", which is the Latinisation of the Greek πολιτεία ("politeia"), "citizenship, administration, civil polity". This is derived from πόλις ("polis"), "city".

Law enforcement in ancient China was carried out by "prefects" for thousands of years since it developed in both the Chu and Jin kingdoms of the Spring and Autumn period. In Jin, dozens of prefects were spread across the state, each having limited authority and employment period. They were appointed by local magistrates, who reported to higher authorities such as governors, who in turn were appointed by the emperor, and they oversaw the civil administration of their "prefecture", or jurisdiction. Under each prefect were "subprefects" who helped collectively with law enforcement in the area. Some prefects were responsible for handling investigations, much like modern police detectives. Prefects could also be women. The concept of the "prefecture system" spread to other cultures such as Korea and Japan.

In ancient Greece, publicly owned slaves were used by magistrates as police. In Athens, a group of 300 Scythian slaves (the , "rod-bearers") was used to guard public meetings to keep order and for crowd control, and also assisted with dealing with criminals, handling prisoners, and making arrests. Other duties associated with modern policing, such as investigating crimes, were left to the citizens themselves.

In the Roman empire, the army, rather than a dedicated police organization, provided security. Local watchmen were hired by cities to provide some extra security. Magistrates such as "procurators fiscal" and "quaestors" investigated crimes. There was no concept of public prosecution, so victims of crime or their families had to organize and manage the prosecution themselves.

Under the reign of Augustus, when the capital had grown to almost one million inhabitants, 14 wards were created; the wards were protected by seven squads of 1,000 men called ""vigiles"", who acted as firemen and nightwatchmen. Their duties included apprehending thieves and robbers and capturing runaway slaves. The vigiles were supported by the Urban Cohorts who acted as a heavy-duty anti-riot force and even the Praetorian Guard if necessary.

In medieval Spain, "Santa Hermandades", or "holy brotherhoods", peacekeeping associations of armed individuals, were a characteristic of municipal life, especially in Castile. As medieval Spanish kings often could not offer adequate protection, protective municipal leagues began to emerge in the twelfth century against banditry and other rural criminals, and against the lawless nobility or to support one or another claimant to a crown.

These organizations were intended to be temporary, but became a long-standing fixture of Spain. The first recorded case of the formation of an "hermandad" occurred when the towns and the peasantry of the north united to police the pilgrim road to Santiago de Compostela in Galicia, and protect the pilgrims against robber knights.

Throughout the Middle Ages such alliances were frequently formed by combinations of towns to protect the roads connecting them, and were occasionally extended to political purposes. Among the most powerful was the league of North Castilian and Basque ports, the Hermandad de las marismas: Toledo, Talavera, and Villarreal.

As one of their first acts after end of the War of the Castilian Succession in 1479, Ferdinand II of Aragon and Isabella I of Castile established the centrally-organized and efficient "Holy Brotherhood" as a national police force. They adapted an existing brotherhood to the purpose of a general police acting under officials appointed by themselves, and endowed with great powers of summary jurisdiction even in capital cases. The original brotherhoods continued to serve as modest local police-units until their final suppression in 1835.

The Vehmic courts of Germany provided some policing in the absence of strong state institutions.

In France during the Middle Ages, there were two Great Officers of the Crown of France with police responsibilities: The Marshal of France and the Constable of France. The military policing responsibilities of the Marshal of France were delegated to the Marshal's provost, whose force was known as the Marshalcy because its authority ultimately derived from the Marshal. The marshalcy dates back to the Hundred Years' War, and some historians trace it back to the early 12th century. Another organisation, the Constabulary (French: Connétablie), was under the command of the Constable of France. The constabulary was regularised as a military body in 1337. Under Francis I of France (who reigned 1515–1547), the Maréchaussée was merged with the Constabulary. The resulting force was also known as the Maréchaussée, or, formally, the Constabulary and Marshalcy of France.

The English system of maintaining public order since the Norman conquest was a private system of tithings, led by a constable, which was based on a social obligation for the good conduct of the others; more common was that local lords and nobles were responsible for maintaining order in their lands, and often appointed a constable, sometimes unpaid, to enforce the law. There was also a system investigative "juries".

The Assize of Arms of 1252, which required the appointment of constables to summon men to arms, quell breaches of the peace, and to deliver offenders to the sheriffs or reeves, is cited as one of the earliest creation of the English police. The Statute of Winchester of 1285 is also cited as the primary legislation regulating the policing of the country between the Norman Conquest and the Metropolitan Police Act 1829.

From about 1500, private watchmen were funded by private individuals and organisations to carry out police functions. They were later nicknamed 'Charlies', probably after the reigning monarch King Charles II. Thief-takers were also rewarded for catching thieves and returning the stolen property.

The first use of the word police ("Polles") in English comes from the book "The Second Part of the Institutes of the Lawes of England" published in 1642.

The first centrally organised police force was created by the government of King Louis XIV in 1667 to police the city of Paris, then the largest city in Europe. The royal edict, registered by the "Parlement" of Paris on March 15, 1667 created the office of "lieutenant général de police" ("lieutenant general of police"), who was to be the head of the new Paris police force, and defined the task of the police as "ensuring the peace and quiet of the public and of private individuals, purging the city of what may cause disturbances, procuring abundance, and having each and everyone live according to their station and their duties".
This office was first held by Gabriel Nicolas de la Reynie, who had 44 "commissaires de police" (police commissioners) under his authority. In 1709, these commissioners were assisted by "inspecteurs de police" (police inspectors). The city of Paris was divided into 16 districts policed by the "commissaires", each assigned to a particular district and assisted by a growing bureaucracy. The scheme of the Paris police force was extended to the rest of France by a royal edict of October 1699, resulting in the creation of lieutenants general of police in all large French cities and towns.

After the French Revolution, Napoléon I reorganized the police in Paris and other cities with more than 5,000 inhabitants on February 17, 1800 as the Prefecture of Police. On March 12, 1829, a government decree created the first uniformed police in France, known as "sergents de ville" ("city sergeants"), which the Paris Prefecture of Police's website claims were the first uniformed policemen in the world.

In 1737, George II began paying some London and Middlesex watchmen with tax monies, beginning the shift to government control. In 1749 Henry Fielding began organizing a force of quasi-professional constables known as the Bow Street Runners. The Macdaniel affair added further impetus for a publicly salaried police force that did not depend on rewards. Nonetheless, In 1828, there were privately financed police units in no fewer than 45 parishes within a 10-mile radius of London.

The word "police" was borrowed from French into the English language in the 18th century, but for a long time it applied only to French and continental European police forces. The word, and the concept of police itself, were "disliked as a symbol of foreign oppression" (according to "Britannica 1911"). Before the 19th century, the first use of the word "police" recorded in government documents in the United Kingdom was the appointment of Commissioners of Police for Scotland in 1714 and the creation of the Marine Police in 1798.

In 1797, Patrick Colquhoun was able to persuade the West Indies merchants who operated at the Pool of London on the River Thames, to establish a police force at the docks to prevent rampant theft that was causing annual estimated losses of £500,000 worth of cargo. The idea of a police, as it then existed in France, was considered as a potentially undesirable foreign import. In building the case for the police in the face of England's firm anti-police sentiment, Colquhoun framed the political rationale on economic indicators to show that a police dedicated to crime prevention was "perfectly congenial to the principle of the British constitution". Moreover, he went so far as to praise the French system, which had reached "the greatest degree of perfection" in his estimation. 
With the initial investment of £4,200, the new trial force of the Thames River Police began with about 50 men charged with policing 33,000 workers in the river trades, of whom Colquhoun claimed 11,000 were known criminals and "on the game". The force was a success after its first year, and his men had "established their worth by saving £122,000 worth of cargo and by the rescuing of several lives". Word of this success spread quickly, and the government passed the Marine Police Bill on 28 July 1800, transforming it from a private to public police agency; now the oldest police force in the world. Colquhoun published a book on the experiment, "The Commerce and Policing of the River Thames". It found receptive audiences far outside London, and inspired similar forces in other cities, notably, New York City, Dublin, and Sydney.

Colquhoun's utilitarian approach to the problem – using a cost-benefit argument to obtain support from businesses standing to benefit – allowed him to achieve what Henry and John Fielding failed for their Bow Street detectives. Unlike the stipendiary system at Bow Street, the river police were full-time, salaried officers prohibited from taking private fees. His other contribution was the concept of preventive policing; his police were to act as a highly visible deterrent to crime by their permanent presence on the Thames. Colquhoun's innovations were a critical development leading up to Robert Peel's "new" police three decades later.

Meanwhile, the authorities in Glasgow, Scotland successfully petitioned the government to pass the Glasgow Police Act establishing the City of Glasgow Police in 1800. Other Scottish towns soon followed suit and set up their own police forces through acts of parliament. In Ireland, the Irish Constabulary Act of 1822 marked the beginning of the Royal Irish Constabulary. The Act established a force in each barony with chief constables and inspectors general under the control of the civil administration at Dublin Castle. By 1841 this force numbered over 8,600 men.

London was fast reaching a size unprecedented in world history, due to the onset of the Industrial Revolution. It became clear that the locally maintained system of volunteer constables and "watchmen" was ineffective, both in detecting and preventing crime. A parliamentary committee was appointed to investigate the system of policing in London. Upon Sir Robert Peel being appointed as Home Secretary in 1822, he established a second and more effective committee, and acted upon its findings.

Royal assent to the Metropolitan Police Act 1829 was given and the Metropolitan Police Service was established on September 29, 1829 in London as the first modern and professional police force in the world.

Peel, widely regarded as the father of modern policing, was heavily influenced by the social and legal philosophy of Jeremy Bentham, who called for a strong and centralised, but politically neutral, police force for the maintenance of social order, for the protection of people from crime and to act as a visible deterrent to urban crime and disorder. Peel decided to standardise the police force as an official paid profession, to organise it in a civilian fashion, and to make it answerable to the public.
Due to public fears concerning the deployment of the military in domestic matters, Peel organised the force along civilian lines, rather than paramilitary. To appear neutral, the uniform was deliberately manufactured in blue, rather than red which was then a military colour, along with the officers being armed only with a wooden truncheon and a rattle to signal the need for assistance. Along with this, police ranks did not include military titles, with the exception of Sergeant.

To distance the new police force from the initial public view of it as a new tool of government repression, Peel publicised the so-called Peelian principles, which set down basic guidelines for ethical policing:

The 1829 Metropolitan Police Act created a modern police force by limiting the purview of the force and its powers, and envisioning it as merely an organ of the judicial system. Their job was apolitical; to maintain the peace and apprehend criminals for the courts to process according to the law. This was very different to the "continental model" of the police force that had been developed in France, where the police force worked within the parameters of the absolutist state as an extension of the authority of the monarch and functioned as part of the governing state.

In 1863, the Metropolitan Police were issued with the distinctive custodian helmet, and in 1884 they switched to the use of whistles that could be heard from much further away. The Metropolitan Police became a model for the police forces in most countries, such as the United States, and most of the British Empire. Bobbies can still be found in many parts of the Commonwealth of Nations.

In Australia the first police force having centralised command as well as jurisdiction over an entire colony was the South Australia Police, formed in 1838 under Henry Inman.

However, whilst the New South Wales Police Force was established in 1862, it was made up from a large number of policing and military units operating within the then Colony of New South Wales and traces its links back to the Royal Marines. The passing of the Police Regulation Act of 1862 essentially tightly regulated and centralised all of the police forces operating throughout the Colony of New South Wales.

The New South Wales Police Force remains the largest police force in Australia in terms of personnel and physical resources. It is also the only police force that requires its recruits to undertake university studies at the recruit level and has the recruit pay for their own education.

In 1566, the first police investigator of Rio de Janeiro was recruited. By the 17th century, most captaincies already had local units with law enforcement functions. On July 9, 1775 a Cavalry Regiment was created in the state of Minas Gerais for maintaining law and order. In 1808, the Portuguese royal family relocated to Brazil, because of the French invasion of Portugal. King João VI established the "Intendência Geral de Polícia" (General Police Intendancy) for investigations. He also created a Royal Police Guard for Rio de Janeiro in 1809. In 1831, after independence, each province started organizing its local "military police", with order maintenance tasks. The Federal Railroad Police was created in 1852, Federal Highway Police, was established in 1928, and Federal Police in 1967.

In Canada, the Royal Newfoundland Constabulary was founded in 1729, making it the first police force in present-day Canada. It was followed in 1834 by the Toronto Police, and in 1838 by police forces in Montreal and Quebec City. A national force, the Dominion Police, was founded in 1868. Initially the Dominion Police provided security for parliament, but its responsibilities quickly grew. The famous Royal Northwest Mounted Police was founded in 1873.
The merger of these two police forces in 1920 formed the world-famous Royal Canadian Mounted Police.

In Lebanon, modern police were established in 1861, with creation of the Gendarmerie.

In India, the police is under the control of respective States and union territories and is known to be under State Police Services (SPS). The candidates selected for the SPS are usually posted as Deputy Superintendent of Police or Assistant Commissioner of Police once their probationary period ends. On prescribed satisfactory service in the SPS, the officers are nominated to the Indian Police Service. The service color is usually dark blue and red, while the uniform color is "Khaki".

In British North America, policing was initially provided by local elected officials. For instance, the New York Sheriff's Office was founded in 1626, and the Albany County Sheriff's Department in the 1660s. In the colonial period, policing was provided by elected sheriffs and local militias.

In 1789 the U.S. Marshals Service was established, followed by other federal services such as the U.S. Parks Police (1791) and U.S. Mint Police (1792). The first city police services were established in Philadelphia in 1751, Richmond, Virginia in 1807, Boston in 1838, and New York in 1845. The U.S. Secret Service was founded in 1865 and was for some time the main investigative body for the federal government.
In the American Old West, policing was often of very poor quality. The Army often provided some policing alongside poorly resourced sheriffs and temporarily organized posses. Public organizations were supplemented by private contractors, notably the Pinkerton National Detective Agency, which was hired by individuals, businessmen, local governments and the federal government. At its height, the Pinkerton Agency's numbers exceeded those of the United States Army.

In recent years, in addition to federal, state, and local forces, some special districts have been formed to provide extra police protection in designated areas. These districts may be known as neighborhood improvement districts, crime prevention districts, or security districts.

In 2005, the Supreme Court of the United States ruled that police do not have a constitutional duty to protect a person from harm.

Michel Foucault claims that the contemporary concept of police as a paid and funded functionary of the state was developed by German and French legal scholars and practitioners in Public administration and Statistics in the 17th and early 18th centuries, most notably with Nicolas Delamare's "Traité de la Police" ("Treatise on the Police"), first published in 1705. The German "Polizeiwissenschaft" (Science of Police) first theorized by Philipp von Hörnigk a 17th-century Austrian Political economist and civil servant and much more famously by Johann Heinrich Gottlob Justi who produced an important theoretical work known as Cameral science on the formulation of police. Foucault cites Magdalene Humpert author of "Bibliographie der Kameralwissenschaften" (1937) in which the author makes note of a substantial bibliography was produced of over 4000 pieces of the practice of Polizeiwissenschaft however, this maybe a mistranslation of Foucault's own work the actual source of Magdalene Humpert states over 14,000 items were produced from the 16th century dates ranging from 1520-1850.

As conceptualized by the "Polizeiwissenschaft", according to Foucault the police had an administrative, economic and social duty ("procuring abundance"). It was in charge of demographic concerns and needed to be incorporated within the western political philosophy system of raison d'état and therefore giving the superficial appearance of empowering the population (and unwittingly supervising the population), which, according to mercantilist theory, was to be the main strength of the state. Thus, its functions largely overreached simple law enforcement activities and included public health concerns, urban planning (which was important because of the miasma theory of disease; thus, cemeteries were moved out of town, etc.), and surveillance of prices.
The concept of preventive policing, or policing to deter crime from taking place, gained influence in the late 18th century. Police Magistrate John Fielding, head of the Bow Street Runners, argued that "...it is much better to prevent even one man from being a rogue than apprehending and bringing forty to justice."

The Utilitarian philosopher, Jeremy Bentham, promoted the views of Italian Marquis Cesare Beccaria, and disseminated a translated version of "Essay on Crime in Punishment". Bentham espoused the guiding principle of "the greatest good for the greatest number:

It is better to prevent crimes than to punish them. This is the chief aim of every good system of legislation, which is the art of leading men to the greatest possible happiness or to the least possible misery, according to calculation of all the goods and evils of life.

Patrick Colquhoun's influential work, "A Treatise on the Police of the Metropolis" (1797) was heavily influenced by Benthamite thought. Colquhoun's Thames River Police was founded on these principles, and in contrast to the Bow Street Runners, acted as a deterrent by their continual presence on the riverfront, in addition to being able to intervene if they spotted a crime in progress.

Edwin Chadwick's 1829 article, "Preventive police" in the "London Review", argued that prevention ought to be the "primary" concern of a police body, which was not the case in practice. The reason, argued Chadwick, was that "A preventive police would act more immediately by placing difficulties in obtaining the objects of temptation." In contrast to a deterrent of punishment, a preventive police force would deter criminality by making crime cost-ineffective - "crime doesn't pay". In the second draft of his 1829 Police Act, the "object" of the new Metropolitan Police, was changed by Robert Peel to the "principal object," which was the "prevention of crime." Later historians would attribute the perception of England's "appearance of orderliness and love of public order" to the preventive principle entrenched in Peel's police system.

Development of modern police forces around the world was contemporary to the formation of the state, later defined by sociologist Max Weber as achieving a "monopoly on the legitimate use of physical force" and which was primarily exercised by the police and the military. Marxist theory situates the development of the modern state as part of the rise of capitalism, in which the police are one component of the bourgeoisie's repressive apparatus for subjugating the working class. By contrast, the Peelian principles argue that "the power of the police...is dependent on public approval of their existence, actions and behavior", a philosophy known as policing by consent.

Police forces include both preventive (uniformed) police and detectives. Terminology varies from country to country. Police functions include protecting life and property, enforcing criminal law, criminal investigations, regulating traffic, crowd control, and other public safety duties. Regardless of size, police forces are generally organized as a hierarchy with multiple ranks. The exact structures and the names of rank vary considerably by country.

Preventive Police, also called Uniform Branch, Uniformed Police, Uniform Division, Administrative Police, Order Police, or Patrol, designates the police that patrol and respond to emergencies and other incidents, as opposed to detective services. As the name "uniformed" suggests, they wear uniforms and perform functions that require an immediate recognition of an officer's legal authority, such as traffic control, stopping and detaining motorists, and more active crime response and prevention. As these must be provided as a 24/7 service, uniformed police are required to do shift work.

Preventive police almost always make up the bulk of a police service's personnel. In Australia and Britain, patrol personnel are also known as "general duties" officers. Atypically, Brazil's preventive police are known as Military Police.

Police detectives are responsible for investigations and detective work. Detectives may be called Investigations Police, Judiciary/Judicial Police, and Criminal Police. In the UK, they are often referred to by the name of their department, the Criminal Investigation Department (CID). Detectives typically make up roughly 15–25% of a police service's personnel.

Detectives, in contrast to uniformed police, typically wear 'business attire' in bureaucratic and investigative functions where a uniformed presence would be either a distraction or intimidating, but a need to establish police authority still exists. "Plainclothes" officers dress in attire consistent with that worn by the general public for purposes of blending in.

In some cases, police are assigned to work "undercover", where they conceal their police identity to investigate crimes, such as organized crime or narcotics crime, that are unsolvable by other means. In some cases this type of policing shares aspects with espionage.

The relationship between detective and uniformed branches varies by country. In the United States, there is high variation within the country itself. Many US police departments require detectives to spend some time on temporary assignments in the patrol division. The argument is that rotating officers helps the detectives to better understand the uniformed officers' work, to promote cross-training in a wider variety of skills, and prevent "cliques" that can contribute to corruption or other unethical behavior. Conversely, some countries regard detective work as being an entirely separate profession, with detectives working in separate agencies and recruited without having to serve in uniform. A common compromise in English-speaking countries is that most detectives are recruited from the uniformed branch, but once qualified they tend to spend the rest of their careers in the detective branch.

Another point of variation is whether detectives have extra status. In some forces, such as the New York Police Department and Philadelphia Police Department, a regular detective holds a higher rank than a regular police officer. In others, such as British police forces and Canadian police forces, a regular detective has equal status with regular uniformed officers. Officers still have to take exams to move to the detective branch, but the move is regarded as being a specialization, rather than a promotion.

Police services often include part-time or volunteer officers, some of whom have other jobs outside policing. These may be paid positions or entirely volunteer. These are known by a variety of names, such as reserves, auxiliary police or special constables.

Other volunteer organizations work with the police and perform some of their duties. Groups in the U.S. including Retired and Senior Volunteer Program, Community Emergency Response Team and the Boy Scout's Police Explorers provide training, traffic and crowd control, disaster response and other policing duties. In the U.S., the Volunteers in Police Service program assists over 200,000 volunteers in almost 2,000 programs. Volunteers may also work on the support staff. Examples of these schemes are Volunteers in Police Service in the US, Police Support Volunteers in the UK and Volunteers in Policing in New South Wales.

Specialized preventive and detective groups, or Specialist Investigation Departments exist within many law enforcement organizations either for dealing with particular types of crime, such as traffic law enforcement and crash investigation, homicide, or fraud; or for situations requiring specialized skills, such as underwater search, aviation, explosive device disposal ("bomb squad"), and computer crime.

Most larger jurisdictions also employ specially selected and trained quasi-military units armed with military-grade weapons for the purposes of dealing with particularly violent situations beyond the capability of a patrol officer response, including high-risk warrant service and barricaded suspects. In the United States these units go by a variety of names, but are commonly known as SWAT (Special Weapons And Tactics) teams.

In counterinsurgency-type campaigns, select and specially trained units of police armed and equipped as light infantry have been designated as police field forces who perform paramilitary-type patrols and ambushes whilst retaining their police powers in areas that were highly dangerous.

Because their situational mandate typically focuses on removing innocent bystanders from dangerous people and dangerous situations, not violent resolution, they are often equipped with non-lethal tactical tools like chemical agents, "flashbang" and concussion grenades, and rubber bullets. The London Metropolitan police's Specialist Firearms Command (CO19) is a group of armed police used in dangerous situations including hostage taking, armed robbery/assault and terrorism.

Police may have administrative duties that are not directly related to enforcing the law, such as issuing firearms licenses. The extent that police have these functions varies among countries, with police in France, Germany, and other continental European countries handling such tasks to a greater extent than British counterparts.

Military police may refer to:


Some Islamic societies have religious police, who enforce the application of Islamic Sharia law. Their authority may include the power to arrest unrelated men and women caught socializing, anyone engaged in homosexual behavior or prostitution; to enforce Islamic dress codes, and store closures during Islamic prayer time.

They enforce Muslim dietary laws, prohibit the consumption or sale of alcoholic beverages and pork, and seize banned consumer products and media regarded as un-Islamic, such as CDs/DVDs of various Western musical groups, television shows and film. In Saudi Arabia, the Mutaween actively prevent the practice or proselytizing of non-Islamic religions within Saudi Arabia, where they are banned.

Most countries are members of the International Criminal Police Organization (Interpol), established to detect and fight transnational crime and provide for international co-operation and co-ordination of other police activities, such as notifying relatives of the death of foreign nationals. Interpol does not conduct investigations or arrests by itself, but only serves as a central point for information on crime, suspects and criminals. Political crimes are excluded from its competencies.

The terms international policing, transnational policing, and/or global policing began to be used from the early 1990s onwards to describe forms of policing that transcended the boundaries of the sovereign nation-state (Nadelmann, 1993), (Sheptycki, 1995). These terms refer in variable ways to practices and forms for policing that, in some sense, transcend national borders. This includes a variety of practices, but international police cooperation, criminal intelligence exchange between police agencies working in different nation-states, and police development-aid to weak, failed or failing states are the three types that have received the most scholarly attention.

Historical studies reveal that policing agents have undertaken a variety of cross-border police missions for many years (Deflem, 2002). For example, in the 19th century a number of European policing agencies undertook cross-border surveillance because of concerns about anarchist agitators and other political radicals. A notable example of this was the occasional surveillance by Prussian police of Karl Marx during the years he remained resident in London. The interests of public police agencies in cross-border co-operation in the control of political radicalism and ordinary law crime were primarily initiated in Europe, which eventually led to the establishment of Interpol before the Second World War. There are also many interesting examples of cross-border policing under private auspices and by municipal police forces that date back to the 19th century (Nadelmann, 1993). It has been established that modern policing has transgressed national boundaries from time to time almost from its inception. It is also generally agreed that in the post–Cold War era this type of practice became more significant and frequent (Sheptycki, 2000).

Not a lot of empirical work on the practices of inter/transnational information and intelligence sharing has been undertaken. A notable exception is James Sheptycki's study of police cooperation in the English Channel region (2002), which provides a systematic content analysis of information exchange files and a description of how these transnational information and intelligence exchanges are transformed into police case-work. The study showed that transnational police information sharing was routinized in the cross-Channel region from 1968 on the basis of agreements directly between the police agencies and without any formal agreement between the countries concerned. By 1992, with the signing of the Schengen Treaty, which formalized aspects of police information exchange across the territory of the European Union, there were worries that much, if not all, of this intelligence sharing was opaque, raising questions about the efficacy of the accountability mechanisms governing police information sharing in Europe (Joubert and Bevers, 1996).

Studies of this kind outside of Europe are even rarer, so it is difficult to make generalizations, but one small-scale study that compared transnational police information and intelligence sharing practices at specific cross-border locations in North America and Europe confirmed that low visibility of police information and intelligence sharing was a common feature (Alain, 2001). Intelligence-led policing is now common practice in most advanced countries (Ratcliffe, 2007) and it is likely that police intelligence sharing and information exchange has a common morphology around the world (Ratcliffe, 2007). James Sheptycki has analyzed the effects of the new information technologies on the organization of policing-intelligence and suggests that a number of 'organizational pathologies' have arisen that make the functioning of security-intelligence processes in transnational policing deeply problematic. He argues that transnational police information circuits help to "compose the panic scenes of the security-control society". The paradoxical effect is that, the harder policing agencies work to produce security, the greater are feelings of insecurity.

Police development-aid to weak, failed or failing states is another form of transnational policing that has garnered attention. This form of transnational policing plays an increasingly important role in United Nations peacekeeping and this looks set to grow in the years ahead, especially as the international community seeks to develop the rule of law and reform security institutions in States recovering from conflict (Goldsmith and Sheptycki, 2007) With transnational police development-aid the imbalances of power between donors and recipients are stark and there are questions about the applicability and transportability of policing models between jurisdictions (Hills, 2009).

Perhaps the greatest question regarding the future development of transnational policing is: in whose interest is it? At a more practical level, the question translates into one about how to make transnational policing institutions democratically accountable (Sheptycki, 2004). For example, according to the Global Accountability Report for 2007 (Lloyd, et al. 2007) Interpol had the lowest scores in its category (IGOs), coming in tenth with a score of 22% on overall accountability capabilities (p. 19). As this report points out, and the existing academic literature on transnational policing seems to confirm, this is a secretive area and one not open to civil society involvement.

In many jurisdictions, police officers carry firearms, primarily handguns, in the normal course of their duties. In the United Kingdom (except Northern Ireland), Iceland, Ireland, Norway, New Zealand, and Malta, with the exception of specialist units, officers do not carry firearms as a matter of course.

Police often have specialist units for handling armed offenders, and similar dangerous situations, and can (depending on local laws), in some extreme circumstances, call on the military (since Military Aid to the Civil Power is a role of many armed forces). Perhaps the most high-profile example of this was, in 1980 the Metropolitan Police handing control of the Iranian Embassy Siege to the Special Air Service.

They can also be armed with non-lethal (more accurately known as "less than lethal" or "less-lethal") weaponry, particularly for riot control. Non-lethal weapons include batons, tear gas, riot control agents, rubber bullets, riot shields, water cannons and electroshock weapons. Police officers often carry handcuffs to restrain suspects. The use of firearms or deadly force is typically a last resort only to be used when necessary to save human life, although some jurisdictions (such as Brazil) allow its use against fleeing felons and escaped convicts. A "shoot-to-kill" policy was recently introduced in South Africa, which allows police to use deadly force against any person who poses a significant threat to them or civilians. With the country having one of the highest rates of violent crime, president Jacob Zuma states that South Africa needs to handle crime differently from other countries.

Modern police forces make extensive use of two-way radio communications equipment, carried both on the person and installed in vehicles, to co-ordinate their work, share information, and get help quickly. In recent years, vehicle-installed mobile data terminals have enhanced the ability of police communications, enabling easier dispatching of calls, criminal background checks on persons of interest to be completed in a matter of seconds, and updating officers' daily activity log and other, required reports on a real-time basis. Other common pieces of police equipment include flashlights/torches, whistles, police notebooks and "ticket books" or citations.

Police vehicles are used for detaining, patrolling and transporting. The average police patrol vehicle is a specially modified, four door sedan (saloon in British English). Police vehicles are usually marked with appropriate logos and are equipped with sirens and flashing light bars to aid in making others aware of police presence.

Unmarked vehicles are used primarily for sting operations or apprehending criminals without alerting them to their presence. Some police forces use unmarked or minimally marked cars for traffic law enforcement, since drivers slow down at the sight of marked police vehicles and unmarked vehicles make it easier for officers to catch speeders and traffic violators. This practice is controversial, with for example, New York State banning this practice in 1996 on the grounds that it endangered motorists who might be pulled over by people impersonating police officers.

Motorcycles are also commonly used, particularly in locations that a car may not be able to reach, to control potential public order situations involving meetings of motorcyclists and often in escort duties where motorcycle police officers can quickly clear a path for escorted vehicles. Bicycle patrols are used in some areas because they allow for more open interaction with the public. In addition, their quieter operation can facilitate approaching suspects unawares and can help in pursuing them attempting to escape on foot.

Police forces use an array of specialty vehicles such as helicopters, airplanes, watercraft, mobile command posts, vans, trucks, all-terrain vehicles, motorcycles, and armored vehicles.

Police cars may also contain fire extinguishers or defibrillators.

The advent of the police car, two-way radio, and telephone in the early 20th century transformed policing into a reactive strategy that focused on responding to calls for service. With this transformation, police command and control became more centralized.

In the United States, August Vollmer introduced other reforms, including education requirements for police officers. O.W. Wilson, a student of Vollmer, helped reduce corruption and introduce professionalism in Wichita, Kansas, and later in the Chicago Police Department. Strategies employed by O.W. Wilson included rotating officers from community to community to reduce their vulnerability to corruption, establishing of a non-partisan police board to help govern the police force, a strict merit system for promotions within the department, and an aggressive recruiting drive with higher police salaries to attract professionally qualified officers. During the professionalism era of policing, law enforcement agencies concentrated on dealing with felonies and other serious crime and conducting visible car patrols in between, rather than broader focus on crime prevention.
The Kansas City Preventive Patrol study in the early 1970s showed flaws in this strategy. It found that aimless car patrols did little to deter crime and often went unnoticed by the public. Patrol officers in cars had insufficient contact and interaction with the community, leading to a social rift between the two. In the 1980s and 1990s, many law enforcement agencies began to adopt community policing strategies, and others adopted problem-oriented policing.

'Broken windows' policing was another, related approach introduced in the 1980s by James Q. Wilson and George L. Kelling, who suggested that police should pay greater attention to minor "quality of life" offenses and disorderly conduct. This method was popularised in the early 1990s by police chief William J. Bratton and New York City Mayor Rudy Giuliani.

The concept behind this method is simple: broken windows, graffiti, and other physical destruction or degradation of property, greatly increases the chances of more criminal activities and destruction of property. When criminals see the abandoned vehicles, trash, and deplorable property, they assume that authorities do not care and do not take active approaches to correct problems in these areas. Therefore, correcting the small problems prevents more serious criminal activity.

Building upon these earlier models, intelligence-led policing has emerged as the dominant philosophy guiding police strategy. Intelligence-led policing and problem-oriented policing are complementary strategies, both of which involve systematic use of information. Although it still lacks a universally accepted definition, the crux of intelligence-led policing is an emphasis on the collection and analysis of information to guide police operations, rather than the reverse.

A related development is evidence-based policing. In a similar vein to evidence-based policy, evidence-based policing is the use of controlled experiments to find which methods of policing are more effective. Leading advocates of evidence-based policing include the criminologist Lawrence W. Sherman and philanthropist Jerry Lee. Findings from controlled experiments include the Minneapolis Domestic Violence Experiment, evidence that patrols deter crime if they are concentrated in crime hotspots and that restricting police powers to shoot suspects does not cause an increase in crime or violence against police officers. Use of experiments to assess the usefulness of strategies has been endorsed by many police services and institutions, including the US Police Foundation and the UK College of Policing.

In many nations, criminal procedure law has been developed to regulate officers' discretion, so that they do not arbitrarily or unjustly exercise their powers of arrest, search and seizure, and use of force. In the United States, "Miranda v. Arizona" led to the widespread use of Miranda warnings or constitutional warnings.

In "Miranda" the court created safeguards against self-incriminating statements made after an arrest. The court held that "The prosecution may not use statements, whether exculpatory or inculpatory, stemming from questioning initiated by law enforcement officers after a person has been taken into custody or otherwise deprived of his freedom of action in any significant way, unless it demonstrates the use of procedural safeguards effective to secure the Fifth Amendment's privilege against self-incrimination"

Police in the United States are also prohibited from holding criminal suspects for more than a reasonable amount of time (usually 24–48 hours) before arraignment, using torture, abuse or physical threats to extract confessions, using excessive force to effect an arrest, and searching suspects' bodies or their homes without a warrant obtained upon a showing of probable cause. The four exceptions to the constitutional requirement of a search warrant are:

In Terry v. Ohio (1968) the court divided seizure into two parts, the investigatory stop and arrest. The court further held that during an investigatory stop a police officer's search " [is] confined to what [is] minimally necessary to determine whether [a suspect] is armed, and the intrusion, which [is] made for the sole purpose of protecting himself and others nearby, [is] confined to ascertaining the presence of weapons" (U.S. Supreme Court). Before Terry, every police encounter constituted an arrest, giving the police officer the full range of search authority. Search authority during a Terry stop (investigatory stop) is limited to weapons only.

Using deception for confessions is permitted, but not coercion. There are exceptions or exigent circumstances such as an articulated need to disarm a suspect or searching a suspect who has already been arrested (Search Incident to an Arrest). The Posse Comitatus Act severely restricts the use of the military for police activity, giving added importance to police SWAT units.

British police officers are governed by similar rules, such as those introduced to England and Wales under the Police and Criminal Evidence Act 1984 (PACE), but generally have greater powers. They may, for example, legally search any suspect who has been arrested, or their vehicles, home or business premises, without a warrant, and may seize anything they find in a search as evidence.

All police officers in the United Kingdom, whatever their actual rank, are 'constables' in terms of their legal position. This means that a newly appointed constable has the same arrest powers as a Chief Constable or Commissioner. However, certain higher ranks have additional powers to authorize certain aspects of police operations, such as a power to authorize a search of a suspect's house (section 18 PACE in England and Wales) by an officer of the rank of Inspector, or the power to authorize a suspect's detention beyond 24 hours by a Superintendent.

Police services commonly include units for investigating crimes committed by the police themselves. These units are typically called Inspectorate-General, or in the US, "internal affairs". In some countries separate organizations outside the police exist for such purposes, such as the British Independent Office for Police Conduct.

Likewise, some state and local jurisdictions, for example, Springfield, Illinois have similar outside review organizations. The Police Service of Northern Ireland is investigated by the Police Ombudsman for Northern Ireland, an external agency set up as a result of the Patten report into policing the province. In the Republic of Ireland the Garda Síochána is investigated by the Garda Síochána Ombudsman Commission, an independent commission that replaced the Garda Complaints Board in May 2007.

The Special Investigations Unit of Ontario, Canada, is one of only a few civilian agencies around the world responsible for investigating circumstances involving police and civilians that have resulted in a death, serious injury, or allegations of sexual assault. The agency has made allegations of insufficient cooperation from various police services hindering their investigations.

In Hong Kong, any allegations of corruption within the police will be investigated by the Independent Commission Against Corruption and the Independent Police Complaints Council, two agencies which are independent of the police force.

Due to a long-term decline in public confidence for law enforcement in the United States, body cameras worn by police officers are under consideration.

Police forces also find themselves under criticism for their use of force, particularly deadly force. Specifically, tension increases when a police officer of one ethnic group harms or kills a suspect of another one. In the United States, such events occasionally spark protests and accusations of racism against police and allegations that police departments practice racial profiling.
In the United States since the 1960s, concern over such issues has increasingly weighed upon law enforcement agencies, courts and legislatures at every level of government. Incidents such as the 1965 Watts Riots, the videotaped 1991 beating by Los Angeles Police officers of Rodney King, and the riot following their acquittal have been suggested by some people to be evidence that U.S. police are dangerously lacking in appropriate controls.
The fact that this trend has occurred contemporaneously with the rise of the civil rights movement, the "War on Drugs", and a precipitous rise in violent crime from the 1960s to the 1990s has made questions surrounding the role, administration and scope of police authority increasingly complicated.

Police departments and the local governments that oversee them in some jurisdictions have attempted to mitigate some of these issues through community outreach programs and community policing to make the police more accessible to the concerns of local communities, by working to increase hiring diversity, by updating training of police in their responsibilities to the community and under the law, and by increased oversight within the department or by civilian commissions.

In cases in which such measures have been lacking or absent, civil lawsuits have been brought by the United States Department of Justice against local law enforcement agencies, authorized under the 1994 Violent Crime Control and Law Enforcement Act. This has compelled local departments to make organizational changes, enter into consent decree settlements to adopt such measures, and submit to oversight by the Justice Department.

Since 1855, the Supreme Court of the United States has consistently ruled that law enforcement officers have no duty to protect any individual, despite the motto "protect and serve". Their duty is to enforce the law in general. The first such case was in 1855 ("") and the most recent in 2005 ("Town of Castle Rock v. Gonzales").

In contrast, the police are entitled to protect private rights in some jurisdictions. To ensure that the police would not interfere in the regular competencies of the courts of law, some police acts require that the police may only interfere in such cases where protection from courts cannot be obtained in time, and where, without interference of the police, the realization of the private right would be impeded. This would, for example, allow police to establish a restaurant guest's identity and forward it to the innkeeper in a case where the guest cannot pay the bill at nighttime because his wallet had just been stolen from the restaurant table.

In addition, there are Federal law enforcement agencies in the United States whose mission includes providing protection for executives such as the President and accompanying family members, visiting foreign dignitaries, and other high-ranking individuals. Such agencies include the United States Secret Service and the United States Park Police.

Police forces are usually organized and funded by some level of government. The level of government responsible for policing varies from place to place, and may be at the national, regional or local level. Some countries have police forces that serve the same territory, with their jurisdiction depending on the type of crime or other circumstances. Other countries, such as Austria, Chile, Israel, New Zealand, the Philippines, South Africa and Sweden, have a single national police force.

In some places with multiple national police forces, one common arrangement is to have a civilian police force and a paramilitary gendarmerie, such as the Police Nationale and National Gendarmerie in France. The French policing system spread to other countries through the Napoleonic Wars and the French colonial empire. Another example is the Policía Nacional and Guardia Civil in Spain. In both France and Spain, the civilian force polices urban areas and the paramilitary force polices rural areas. Italy has a similar arrangement with the Polizia di Stato and Carabinieri, though their jurisdictions overlap more. Some countries have separate agencies for uniformed police and detectives, such as the Military Police and Civil Police in Brazil and the Carabineros and Investigations Police in Chile.

Other countries have sub-national police forces, but for the most part their jurisdictions do not overlap. In many countries, especially federations, there may be two or more tiers of police force, each serving different levels of government and enforcing different subsets of the law. In Australia and Germany, the majority of policing is carried out by state (i.e. provincial) police forces, which are supplemented by a federal police force. Though not a federation, the United Kingdom has a similar arrangement, where policing is primarily the responsibility of a regional police force and specialist units exist at the national level. In Canada, the Royal Canadian Mounted Police (RCMP) are the federal police, while municipalities can decide whether to run a local police service or to contract local policing duties to a larger one. Most urban areas have a local police service, while most rural areas contract it to the RCMP, or to the provincial police in Ontario and Quebec.

The United States has a highly decentralized and fragmented system of law enforcement, with over 17,000 state and local law enforcement agencies. These agencies include local police, county police (often in the form of a sheriff's office), state police and federal law enforcement agencies. Federal agencies, such as the FBI, only have jurisdiction over federal crimes or those that involve more than one state. Other federal agencies have jurisdiction over a specific type of crime. Examples include the Federal Protective Service, which patrols and protects government buildings; the postal police, which protect postal buildings, vehicles and items; the Park Police, which protect national parks; and Amtrak Police, which patrol Amtrak stations and trains. There are also some government agencies that perform police functions in addition to other duties, such as the Coast Guard.





</doc>
<doc id="23628" url="https://en.wikipedia.org/wiki?curid=23628" title="PDP-10">
PDP-10

The PDP-10 is a mainframe computer family manufactured by Digital Equipment Corporation (DEC) from 1966 into the 1980s. Later models were marketed under the DECsystem-10 name, especially when the TOPS-10 operating system became widely used.

The PDP-10 architecture is almost identical to the earlier PDP-6 architecture, sharing the same 36-bit word length and slightly extending the instruction set (but with improved hardware implementation). Some aspects of the instruction set are unusual, most notably the "byte" instructions, which operated on bit fields of any size from 1 to 36 bits inclusive according to the general definition of a byte as "a contiguous sequence of a fixed number of bits".

The PDP-10 is the machine that made time-sharing common, and this and other features made it a common fixture in many university computing facilities and research labs during the 1970s, the most notable being Harvard's Aiken Lab, MIT's AI Lab and Project MAC, Stanford's SAIL, Computer Center Corporation (CCC), ETH (ZIR), and Carnegie Mellon University. Its main operating systems, TOPS-10 and TENEX, were used to build out the early ARPANET. For these reasons, the PDP-10 looms large in early hacker folklore.

Projects to extend the PDP-10 line were eclipsed by the success of the unrelated VAX superminicomputer, and the cancellation of the PDP-10 line was announced in 1983.

The original PDP-10 processor is the KA10, introduced in 1968. It uses discrete transistors packaged in DEC's Flip-Chip technology, with backplanes wire wrapped via a semi-automated manufacturing process. Its cycle time is 1 μs and its add time 2.1 μs. In 1973, the KA10 was replaced by the KI10, which uses TTL SSI. This was joined in 1975 by the higher-performance KL10 (later faster variants), which is built from ECL, microprogrammed, and has cache memory. The KL10's performance was about 1 megaflops using 36-bit floating point numbers on matrix row reduction. It was slightly faster than the newer VAX-750, although more limited in memory.

A smaller, less expensive model, the KS10, was introduced in 1978, using TTL and Am2901 bit-slice components and including the PDP-11 Unibus to connect peripherals. The KS was marketed as the DECsystem-2020, DEC's entry in the distributed processing arena, and it was introduced as "the word's lowest cost mainframe computer system."

The KA10 has a maximum main memory capacity (both virtual and physical) of 256 kilowords (equivalent to 1152 kilobytes). As supplied by DEC, it did not include paging hardware; memory management consisted of two sets of protection and relocation registers, called "base and bounds" registers. This allows each half of a user's address space to be limited to a set section of main memory, designated by the base physical address and size. This allows the model of separate read-only shareable code segment (normally the high segment) and read-write data/stack segment (normally the low segment) used by TOPS-10 and later adopted by Unix. Some KA10 machines, first at MIT, and later at Bolt, Beranek and Newman (BBN), were modified to add support for demand paged virtual memory, as well as more physical memory.

The 10/50 was the top-of-the-line KA machine at the time when the PA1050 software package was introduced. Two other models were 10/40, 10/55.

The KI10 and later processors offer paged memory management, and also support a larger physical address space of 4 megawords. KI10 models include 1060, 1070 and 1077, the latter incorporating two CPUs.
The original KL10 PDP-10 (also marketed as DECsystem-10) models (1080, 1088, etc.) use the original PDP-10 memory bus, with external memory modules. Module in this context meant a cabinet, dimensions roughly (WxHxD) 30 x 75 x 30 in. with a capacity of 32 to 256 kWords of magnetic core memory (the picture on the right hand side of the introduction shows six of these cabinets). The processors used in the DECSYSTEM-20 (2040, 2050, 2060, 2065), commonly but incorrectly called "KL20", use internal memory, mounted in the same cabinet as the CPU. The 10xx models also have different packaging; they come in the original tall PDP-10 cabinets, rather than the short ones used later on for the DECSYSTEM-20. The differences between the 10xx and 20xx models are more cosmetic than real; some 10xx systems have "20-style" internal memory and I/O, and some 20xx systems have "10-style" external memory and an I/O bus. In particular, all ARPAnet TOPS-20 systems had an I/O bus because the AN20 IMP interface was an I/O bus device. Both could run either TOPS-10 or TOPS-20 microcode and thus the corresponding operating system.

Later, the "Model B" version of the 2060 processors removed the 256 kiloword limitation on the virtual address space, by allowing the use of up to 32 "sections" of up to 256 kilowords each, along with substantial changes to the instruction set. "Model A" and "Model B" KL10 processors can be thought of as being different CPUs. The first operating system that took advantage of the Model B's capabilities was TOPS-20 release 3, and user mode extended addressing was offered in TOPS-20 release 4. TOPS-20 versions after release 4.1 would only run on a Model B.

TOPS-10 versions 7.02 and 7.03 also use extended addressing when run on a 1090 (or 1091) Model B processor running TOPS-20 microcode.

The final upgrade to the KL10 was the MCA25 upgrade of a 2060 to 2065 (or a 1091 to 1095), which gave some performance increases for programs which run in multiple sections.

The KS10 design was crippled to be a Model A even though most of the necessary data paths needed to support the Model B architecture were present. This was no doubt intended to segment the market, but it greatly shortened the KS10's product life.

Frontend processors are computers that extend the functionality of the systems to which they 
are connected.

The KL class machines cannot be started without the assistance of a PDP-11/40 frontend computer installed in every system. The PDP-11 is booted from a dual-ported RP06 disk drive (or alternatively from an 8" floppy disk drive or DECtape), and then commands can be given to the PDP-11 to start the main processor, which is typically booted from the same RP06 disk drive as the PDP-11. The PDP-11 performs watchdog functions once the main processor is running.

Networking and Remote Job Entry (RJE) were accomplished via a PDP-11/34a frontend,
for which was DEC used the terms DN60 and D60SPL

The KS system uses a similar boot procedure. An 8080 CPU loads the microcode from an RM03, RM80, or RP06 disk or magnetic tape and then starts the main processor. The 8080 switches modes after the operating system boots and controls the console and remote diagnostic serial ports.

The I/O architecture of the 20xx series KL machines is based on a DEC bus design called the MASSbus. While many attributed the success of the PDP-11 to DEC's decision to make the PDP-11 Unibus an open architecture, DEC reverted to prior philosophy with the KL, making MASSbus both unique and proprietary. Consequently, there were no aftermarket peripheral manufacturers who made devices for the MASSbus, and DEC chose to price their own MASSbus devices, notably the RP06 disk drive, at a substantial premium above comparable IBM-compatible devices. CompuServe for one, designed its own alternative disk controller that could operate on the MASSbus, but connect to IBM style 3330 disk subsystems.

Two models of tape drives were supported by the TM10 Magnetic Tape Control subsystem:
A mix of up to eight of these could be supported, providing seven-track &/or nine-track devices.
The TU20 and TU30 each came in A (9 track) and B (7 track) versions, and all of the aforementioned tape drives could read/write from/to 200 BPI, 556 BPI and 800 BPI IBM-compatible tapes. 

The TM10 Magtape controller was available in two submodels:

From the first PDP-6s to the Model A KL-10s, the user-mode instruction set architecture is largely the same. This section covers that architecture. (Multi-section extended addressing is covered in the "DECsystem-10/DECSYSTEM-20 Processor Reference Manual".)

The PDP-10 has 36-bit words and 18-bit word addresses. In supervisor mode, instruction addresses correspond directly to physical memory. In user mode, addresses are translated to physical memory. Earlier models give a user process a "high" and a "low" memory: addresses with a 0 top bit used one base register, and higher addresses used another. Each segment is contiguous. Later architectures have paged memory access, allowing non-contiguous address spaces. The CPU's general-purpose registers can also be addressed as memory locations 0-15.

There are 16 general-purpose, 36-bit registers. The right half of these registers (other than register 0) may be used for indexing. A few instructions operate on pairs of registers. The "PC Word" consists of a 13-bit condition register (plus 5 always zero bits) in the left half and an 18-bit Program Counter in the right half. The condition register, which records extra bits from the results of arithmetic operations ("e.g." overflow), can be accessed by only a few instructions.

There are two operational modes, supervisor and user mode. Besides the difference in memory referencing described above, supervisor-mode programs can execute input/output operations.

Communication from user-mode to supervisor-mode is done through Unimplemented User Operations (UUOs): instructions which are not defined by the hardware, and are trapped by the supervisor. This mechanism is also used to emulate operations which may not have hardware implementations in cheaper models.

The major datatypes which are directly supported by the architecture are two's complement 36-bit integer arithmetic (including bitwise operations), 36-bit floating-point, and halfwords. Extended, 72-bit, floating point is supported through special instructions designed to be used in multi-instruction sequences. Byte pointers are supported by special instructions. A word structured as a "count" half and a "pointer" half facilitates the use of bounded regions of memory, notably stacks.

The instruction set is very symmetrical. Every instruction consists of a 9-bit opcode, a 4-bit register code, and a 23-bit effective address field, which consists in turn of a 1-bit indirect bit, a 4-bit register code, and an 18-bit offset. Instruction execution begins by calculating the effective address. It adds the contents of the given register (if non-zero) to the offset; then, if the indirect bit is 1, fetches the word at the calculated address and repeats the effective address calculation until an effective address with a zero indirect bit is reached. The resulting effective address can be used by the instruction either to fetch memory contents, or simply as a constant. Thus, for example, MOVEI A,3(C) adds 3 to the 18 lower bits of register C and puts the result in register A, without touching memory.

There are three main classes of instruction: arithmetic, logical, and move; conditional jump; conditional skip (which may have side effects). There are also several smaller classes.

The arithmetic, logical, and move operations include variants which operate immediate-to-register, memory-to-register, register-to-memory, register-and-memory-to-both or memory-to-memory. Since registers may be addressed as part of memory, register-to-register operations are also defined. (Not all variants are useful, though they are well-defined.) For example, the ADD operation has as variants ADDI (add an 18-bit "I"mmediate constant to a register), ADDM (add register contents to a "M"emory location), ADDB (add to "B"oth, that is, add register contents to memory and also put the result in the register). A more elaborate example is HLROM ("H"alf "L"eft to "R"ight, "O"nes to "M"emory), which takes the Left half of the register contents, places them in the Right half of the memory location, and replaces the left half of the memory location with Ones. Halfword instructions are also used for linked lists: HLRZ is the Lisp CAR operator; HRRZ is CDR.

The conditional jump operations examine register contents and jump to a given location depending on the result of the comparison. The mnemonics for these instructions all start with JUMP, JUMPA meaning "jump always" and JUMP meaning "jump never" - as a consequence of the symmetrical design of the instruction set, it contains several no-ops such as JUMP. For example, JUMPN A,LOC jumps to the address LOC if the contents of register A is non-zero. There are also conditional jumps based on the processor's condition register using the JRST instruction. On the KA10 and KI10, JRST is faster than JUMPA, so the standard unconditional jump is JRST.

The conditional skip operations compare register and memory contents and skip the next instruction (which is often an unconditional jump) depending on the result of the comparison. A simple example is CAMN A,LOC which compares the contents of register A with the contents of location LOC and skips the next instruction if they are not equal. A more elaborate example is TLCE A,LOC (read "Test Left Complement, skip if Equal"), which using the contents of LOC as a mask, selects the corresponding bits in the left half of register A. If all those bits are "E"qual to zero, skip the next instruction; and in any case, replace those bits by their boolean complement.

Some smaller instruction classes include the shift/rotate instructions and the procedure call instructions. Particularly notable are the stack instructions PUSH and POP, and the corresponding stack call instructions PUSHJ and POPJ. The byte instructions use a special format of indirect word to extract and store arbitrary-sized bit fields, possibly advancing a pointer to the next unit.

The original PDP-10 operating system was simply called "Monitor", but was later renamed TOPS-10. Eventually the PDP-10 system itself was renamed the DECsystem-10. Early versions of Monitor and TOPS-10 formed the basis of Stanford's WAITS operating system and the Compuserve time-sharing system.

Over time, some PDP-10 operators began running operating systems assembled from major components developed outside DEC. For example, the main Scheduler might come from one university, the Disk Service from another, and so on. The commercial timesharing services such as CompuServe, On-Line Systems (OLS), and Rapidata maintained sophisticated inhouse systems programming groups so that they could modify the operating system as needed for their own businesses without being dependent on DEC or others. There are also strong user communities such as DECUS through which users can share software that they have developed.

BBN developed their own alternative operating system, TENEX, which fairly quickly became the de facto standard in the research community. DEC later ported Tenex to the KL10, enhanced it considerably, and named it TOPS-20, forming the DECSYSTEM-20 line. MIT also had developed their own influential system, the Incompatible Timesharing System (ITS), whose name was a joke on the Compatible Time-Sharing System, developed at MIT for a modified IBM 7094.

Tymshare developed TYMCOM-X, derived from TOPS-10 but using a page-based file system like TOPS-20.

In 1971 to 1972 researchers at Xerox PARC were frustrated by top company management's refusal to let them purchase a PDP-10. Xerox had just bought Scientific Data Systems in 1969, and wanted PARC to use an SDS machine.
Instead, a group led by Charles P. Thacker designed and constructed two PDP-10 clone systems named "MAXC" (pronounced "Max", in honour of Max Palevsky, who had sold SDS to Xerox) for their own use. MAXC was also a backronym for Multiple Access Xerox Computer.
MAXC ran a modified version of TENEX.

Third-party attempts to sell PDP-10 clones were relatively unsuccessful; see Foonly, Systems Concepts, and XKL.

One of the largest collections of DECsystem-10 architecture systems ever assembled was at CompuServe, which at its peak operated over 200 loosely coupled systems in three data centers in Columbus, Ohio. CompuServe used these systems as 'hosts', providing access to commercial applications as well as the CompuServe Information Service. While the first such systems were purchased from DEC, when DEC abandoned the PDP-10 architecture in favor of the VAX, CompuServe and other PDP-10 customers began purchasing plug compatible computers from Systems Concepts. As of January 2007, CompuServe continued to operate a small number of PDP-10 architecture machines to perform some billing and routing functions.

The main power supplies used in the KL-series machines were so inefficient that CompuServe engineers designed a replacement power supply that consumed about half the energy. CompuServe offered to license the design for its KL power supply to DEC for free if DEC would promise that any new KL purchased by CompuServe would have the more efficient power supply installed. DEC declined the offer.
Another modification made to the PDP-10 by CompuServe engineers was the replacement of the hundreds of incandescent indicator lamps on the KI10 processor cabinet with LED lamp modules. The cost of the conversion was easily offset by the cost savings in electric consumption, the reduction of heat, and the manpower required to replace burned-out lamps. Digital followed this step all over the world. The picture on the right hand side shows the light panel of the MF10 memory which is contemporaneous with the KI10 CPU. This item is part of a computer museum, and was populated with LEDs in 2008 for demonstration purposes only. There were no similar banks of indicator lamps on KL and KS processors.

The PDP-10 was eventually eclipsed by the VAX superminicomputer machines (descendants of the PDP-11) when DEC recognized that the PDP-10 and VAX product lines were competing with each other and decided to concentrate its software development effort on the more profitable VAX. The PDP-10 product line cancellation was announced in 1983, including cancelling the ongoing Jupiter project to produce a new high-end PDP-10 processor (despite that project being in good shape at the time of the cancellation) and the Minnow project to produce a desktop PDP-10, which may then have been at the prototyping stage.

This event spelled the doom of ITS and the technical cultures that had spawned the original jargon file, but by the 1990s it had become something of a badge of honor among old-time hackers to have cut one's teeth on a PDP-10.

The PDP-10 assembly language instructions LDB and DPB (load/deposit byte) live on as functions in the programming language Common Lisp. See the "References" section on the LISP article — the 36-bit word size of the PDP-6 and PDP-10 was influenced by the programming convenience of having 2 LISP pointers, each 18 bits, in one word.

Will Crowther created "Adventure", the prototypical computer adventure game, for a PDP-10. Don Daglow created the first computer baseball game (1971) and "Dungeon" (1975), the first role-playing video game on a PDP-10. Walter Bright originally created "Empire" for the PDP-10. Roy Trubshaw and Richard Bartle created the first MUD on a PDP-10. In addition, "Zork" was written on the PDP-10, and Infocom used several PDP-10s for game development and testing.

Bill Gates and Paul Allen originally wrote Altair BASIC using an Intel 8080 emulator running on a PDP-10 at Harvard University. They founded Microsoft shortly after.

The software for simulation of historical computers SIMH contains a module to emulate the KS10 CPU on a Windows or Unix-based machine. Copies of DEC's original distribution tapes are available as downloads from the Internet so that a running TOPS-10 or TOPS-20 system may be established. ITS is also available for SIMH.

Ken Harrenstien's KLH10 software for Unix-like systems emulates a KL10B processor with extended addressing and 4 MW of memory or a KS10 processor with 512 KW of memory. The KL10 emulation supports v.442 of the KL10 microcode, which enables it to run the final versions of both TOPS-10 and TOPS-20. The KS10 emulation supports both ITS v.262 microcode for the final version of KS10 ITS and DEC v.130 microcode for the final versions of KS TOPS-10 and TOPS-20.








</doc>
<doc id="23629" url="https://en.wikipedia.org/wiki?curid=23629" title="DECSYSTEM-20">
DECSYSTEM-20

The DECSYSTEM-20 was a 36-bit Digital Equipment Corporation PDP-10 mainframe computer running the TOPS-20 operating system (products introduced in 1977).

PDP-10 computers running the TOPS-10 operating system were labeled "DECsystem-10" as a way of differentiating them from the PDP-11. Later on, those systems running TOPS-20 (on the KL10 PDP-10 processors) were labeled "DECSYSTEM-20" (the block capitals being the result of a lawsuit brought against DEC by Singer, which once made a computer called "system-10"). The DECSYSTEM-20 was sometimes called PDP-20, although this designation was never used by DEC.

The following models were produced:


The only significant difference the user could see between a DECsystem-10 and a DECSYSTEM-20 was the operating system and the color of the paint. Most (but not all) machines sold to run TOPS-10 were painted "Blasi Blue", whereas most TOPS-20 machines were painted "Terracotta" (often mistakenly called "Chinese Red" or orange; the actual name of the color on the paint cans was Terracotta).

There were some significant internal differences between the earlier KL10 Model A processors, used in the earlier DECsystem-10s running on KL10 processors, and the later KL10 Model Bs, used for the DECSYSTEM-20s. Model As used the original PDP-10 memory bus, with external memory modules. The later Model B processors used in the DECSYSTEM-20 used internal memory, mounted in the same cabinet as the CPU. The Model As also had different packaging; they came in the original tall PDP-10 cabinets, rather than the short ones used later on for the DECSYSTEM-20.

The last released implementation of DEC's 36-bit architecture was the single cabinet DECSYSTEM-2020, using a KS10 processor.

The DECSYSTEM-20 was primarily designed and used as a small mainframe for timesharing. That is, multiple users would concurrently log on to individual user accounts and share use of the main processor to compile and run applications. Separate disk allocations were maintained for all users by the operating system, and various levels of protection could be maintained by for System, Owner, Group, and World users. A model 2060, for example, could typically host up to 40 to 60 simultaneous users before exhibiting noticeably reduced response time.

The Living Computer Museum of Seattle, Washington maintains a 2065 running TOPS-10, which is available to interested parties via telnet upon registration (at no cost) at their website.




</doc>
<doc id="23630" url="https://en.wikipedia.org/wiki?curid=23630" title="Programmed Data Processor">
Programmed Data Processor

Programmed Data Processor (PDP), referred to by some customers, media and authors as "Programmable Data Processor, is a term used by the Digital Equipment Corporation from 1957 to 1990 for several lines of minicomputers. The name "PDP" intentionally avoids the use of the term "computer" because, at the time of the first PDPs, computers had a reputation of being large, complicated, and expensive machines, and the venture capitalists behind Digital (especially Georges Doriot) would not support Digital's attempting to build a "computer"; the word "minicomputer" had not yet been coined. So instead, Digital used their existing line of logic modules to build a "Programmed Data Processor" and aimed it at a market that could not afford the larger computers.

The various PDP machines can generally be grouped into families based on word length.

Members of the PDP series include:










Various sites list documents by Charles Lasner, the creator of the alt.sys.pdp8 discussion group, and related documents by various members of the alt.sys.pdp8 readership with even more authoritative information about the various models, especially detailed focus upon the various members of the PDP-8 "family" of computers both made and not made by DEC.


</doc>
<doc id="23631" url="https://en.wikipedia.org/wiki?curid=23631" title="Primary mirror">
Primary mirror

A primary mirror (or primary) is the principal light-gathering surface (the objective) of a reflecting telescope.

The primary mirror of a reflecting telescope is a spherical or parabolic shaped disks of polished reflective metal (speculum metal up to the mid 19th century), or in later telescopes, glass or other material coated with a reflective layer. One of the first known reflecting telescopes, Newton's reflector of 1668, used a 3.3 cm polished metal primary mirror. The next major change was to use silver on glass rather than metal, in the 19th century such was with the Crossley reflector. This was changed to vacuum deposited aluminum on glass, used on the 200-inch Hale telescope.

Solid primary mirrors have to sustain their own weight and not deform under gravity, which limits the maximum size for a single piece primary mirror.

Segmented mirror configurations are used to get around the size limitation on single primary mirrors. For example, the Giant Magellan Telescope will have seven 8.4 meter primary mirrors, with the resolving power equivalent to a optical aperture.

The largest optical telescope in the world as of 2009 to use a non-segmented single-mirror as its primary mirror is the 8.2 m (26.9 ft) Subaru telescope of the National Astronomical Observatory of Japan, located in Mauna Kea Observatory on Hawaii since 1997; however, this is not the largest diameter single mirror in a telescope, the U.S./German/Italian Large Binocular Telescope has two 8.4 m (27.6 ft) mirrors (which can be used together for interferometric mode). Both of these are smaller than the 10 m segmented primary mirrors on the two Keck telescope. The Hubble Space Telescope has a 2.4 m (7 ft 10 in) primary mirror.

Radio and submillimeter telescopes use much larger dishes or antennae, which do not have to be made as precisely as the mirrors used in optical telescopes. The Arecibo Observatory uses a 305 m dish, which is the world largest single-dish radio telescope fixed to the ground. The Green Bank Telescope has the world's largest steerable single radio dish with 100 m in diameter. There are larger radio arrays, composed of multiple dishes which have better image resolution but less sensitivity.



</doc>
<doc id="23632" url="https://en.wikipedia.org/wiki?curid=23632" title="Platonic idealism">
Platonic idealism

Platonic idealism usually refers to Plato's theory of forms or doctrine of ideas.

Some commentators hold that Plato argued that truth is an abstraction. In other words, we are urged to believe that Plato's theory of ideals is an abstraction, divorced from the so-called external world, of modern European philosophy, despite the fact Plato taught that ideals are ultimately real, and different from non-ideal things—indeed, he argued for a distinction between the ideal and non-ideal realm.

These commentators speak thus: for example, a particular tree, with a branch or two missing, possibly alive, possibly dead, and with the initials of two lovers carved into its bark, is distinct from the abstract form of Tree-ness. A Tree is the ideal that each of us holds that allows us to identify the imperfect reflections of trees all around us.

Plato gives the divided line as an outline of this theory. At the top of the line, the Form of the Good
is found, directing everything underneath.

Some contemporary linguistic philosophers construe "Platonism" to mean the proposition that universals exist independently of particulars (a universal is anything that can be predicated of a particular).

Platonism is an ancient school of philosophy, founded by Plato; at the beginning, this school had a physical existence at a site just outside the walls of Athens called the Academy, as well as the intellectual unity of a shared approach to philosophizing.

Platonism is usually divided into three periods:


Plato's students used the hypomnemata as the foundation to his philosophical approach to knowledge. The hypomnemata constituted a material memory of things read, heard, or thought, thus offering these as an accumulated treasure for rereading and later meditation. For the Neoplatonist they also formed a raw material for the writing of more systematic treatises in which were given arguments and means by which to struggle against some defect (such as anger, envy, gossip, flattery) or to overcome some difficult circumstance (such as a mourning, an exile, downfall, disgrace).

Platonism is considered to be, in mathematics departments the world over, the predominant philosophy of mathematics, especially regarding the foundations of mathematics.

One statement of this philosophy is the thesis that mathematics is not created but discovered.
A lucid statement of this is found in an essay written by the British mathematician G. H. Hardy in defense of pure mathematics. 

The absence in this thesis of clear distinction between mathematical and non-mathematical "creation" leaves open the inference that it applies to allegedly creative endeavors in art, music, and literature.

It is unknown if Plato's ideas of idealism have some earlier origin, but Plato held Pythagoras in high regard, and Pythagoras as well as his followers in the movement known as Pythagoreanism claimed the world was literally built up from numbers, an abstract, absolute form.




</doc>
<doc id="23633" url="https://en.wikipedia.org/wiki?curid=23633" title="List of physicists">
List of physicists

Following is a list of physicists who are notable for their achievements.





























</doc>
<doc id="23634" url="https://en.wikipedia.org/wiki?curid=23634" title="Protein">
Protein

Proteins are large biomolecules, or macromolecules, consisting of one or more long chains of amino acid residues. Proteins perform a vast array of functions within organisms, including catalysing metabolic reactions, DNA replication, responding to stimuli, and transporting molecules from one location to another. Proteins differ from one another primarily in their sequence of amino acids, which is dictated by the nucleotide sequence of their genes, and which usually results in protein folding into a specific three-dimensional structure that determines its activity.

A linear chain of amino acid residues is called a polypeptide. A protein contains at least one long polypeptide. Short polypeptides, containing less than 20–30 residues, are rarely considered to be proteins and are commonly called peptides, or sometimes oligopeptides. The individual amino acid residues are bonded together by peptide bonds and adjacent amino acid residues. The sequence of amino acid residues in a protein is defined by the sequence of a gene, which is encoded in the genetic code. In general, the genetic code specifies 20 standard amino acids; however, in certain organisms the genetic code can include selenocysteine and—in certain archaea—pyrrolysine. Shortly after or even during synthesis, the residues in a protein are often chemically modified by post-translational modification, which alters the physical and chemical properties, folding, stability, activity, and ultimately, the function of the proteins. Sometimes proteins have non-peptide groups attached, which can be called prosthetic groups or cofactors. Proteins can also work together to achieve a particular function, and they often associate to form stable protein complexes.

Once formed, proteins only exist for a certain period and are then degraded and recycled by the cell's machinery through the process of protein turnover. A protein's lifespan is measured in terms of its half-life and covers a wide range. They can exist for minutes or years with an average lifespan of 1–2 days in mammalian cells. Abnormal or misfolded proteins are degraded more rapidly either due to being targeted for destruction or due to being unstable.

Like other biological macromolecules such as polysaccharides and nucleic acids, proteins are essential parts of organisms and participate in virtually every process within cells. Many proteins are enzymes that catalyse biochemical reactions and are vital to metabolism. Proteins also have structural or mechanical functions, such as actin and myosin in muscle and the proteins in the cytoskeleton, which form a system of scaffolding that maintains cell shape. Other proteins are important in cell signaling, immune responses, cell adhesion, and the cell cycle. In animals, proteins are needed in the diet to provide the essential amino acids that cannot be synthesized. Digestion breaks the proteins down for use in the metabolism.

Proteins may be purified from other cellular components using a variety of techniques such as ultracentrifugation, precipitation, electrophoresis, and chromatography; the advent of genetic engineering has made possible a number of methods to facilitate purification. Methods commonly used to study protein structure and function include immunohistochemistry, site-directed mutagenesis, X-ray crystallography, nuclear magnetic resonance and mass spectrometry.

Most proteins consist of linear polymers built from series of up to 20 different -α-amino acids. All proteinogenic amino acids possess common structural features, including an α-carbon to which an amino group, a carboxyl group, and a variable side chain are bonded. Only proline differs from this basic structure as it contains an unusual ring to the N-end amine group, which forces the CO–NH amide moiety into a fixed conformation. The side chains of the standard amino acids, detailed in the list of standard amino acids, have a great variety of chemical structures and properties; it is the combined effect of all of the amino acid side chains in a protein that ultimately determines its three-dimensional structure and its chemical reactivity.
The amino acids in a polypeptide chain are linked by peptide bonds. Once linked in the protein chain, an individual amino acid is called a "residue," and the linked series of carbon, nitrogen, and oxygen atoms are known as the "main chain" or "protein backbone."

The peptide bond has two resonance forms that contribute some double-bond character and inhibit rotation around its axis, so that the alpha carbons are roughly coplanar. The other two dihedral angles in the peptide bond determine the local shape assumed by the protein backbone. The end with a free amino group is known as the N-terminus or amino terminus, whereas the end of the protein with a free carboxyl group is known as the C-terminus or carboxy terminus (the sequence of the protein is written from N-terminus to C-terminus, from left to right).

The words "protein", "polypeptide," and "peptide" are a little ambiguous and can overlap in meaning. "Protein" is generally used to refer to the complete biological molecule in a stable conformation, whereas "peptide" is generally reserved for a short amino acid oligomers often lacking a stable three-dimensional structure. However, the boundary between the two is not well defined and usually lies near 20–30 residues. "Polypeptide" can refer to any single linear chain of amino acids, usually regardless of length, but often implies an absence of a defined conformation.

Proteins can interact with many types of molecules, including with other proteins, with lipids, with carboyhydrates, and with DNA.

It has been estimated that average-sized bacteria contain about 2 million proteins per cell (e.g. "E. coli" and "Staphylococcus aureus"). Smaller bacteria, such as "Mycoplasma" or "spirochetes" contain fewer molecules, on the order of 50,000 to 1 million. By contrast, eukaryotic cells are larger and thus contain much more protein. For instance, yeast cells have been estimated to contain about 50 million proteins and human cells on the order of 1 to 3 billion. The concentration of individual protein copies ranges from a few molecules per cell up to 20 million. Not all genes coding proteins are expressed in most cells and their number depends on, for example, cell type and external stimuli. For instance, of the 20,000 or so proteins encoded by the human genome, only 6,000 are detected in lymphoblastoid cells. Moreover, the number of proteins the genome encodes correlates well with the organism complexity. Eukaryotes, bacteria, archaea and viruses have on average 15145, 3200, 2358 and 42 proteins respectively coded in their genomes.

Proteins are assembled from amino acids using information encoded in genes. Each protein has its own unique amino acid sequence that is specified by the nucleotide sequence of the gene encoding this protein. The genetic code is a set of three-nucleotide sets called codons and each three-nucleotide combination designates an amino acid, for example AUG (adenine-uracil-guanine) is the code for methionine. Because DNA contains four nucleotides, the total number of possible codons is 64; hence, there is some redundancy in the genetic code, with some amino acids specified by more than one codon. Genes encoded in DNA are first transcribed into pre-messenger RNA (mRNA) by proteins such as RNA polymerase. Most organisms then process the pre-mRNA (also known as a "primary transcript") using various forms of Post-transcriptional modification to form the mature mRNA, which is then used as a template for protein synthesis by the ribosome. In prokaryotes the mRNA may either be used as soon as it is produced, or be bound by a ribosome after having moved away from the nucleoid. In contrast, eukaryotes make mRNA in the cell nucleus and then translocate it across the nuclear membrane into the cytoplasm, where protein synthesis then takes place. The rate of protein synthesis is higher in prokaryotes than eukaryotes and can reach up to 20 amino acids per second.

The process of synthesizing a protein from an mRNA template is known as translation. The mRNA is loaded onto the ribosome and is read three nucleotides at a time by matching each codon to its base pairing anticodon located on a transfer RNA molecule, which carries the amino acid corresponding to the codon it recognizes. The enzyme aminoacyl tRNA synthetase "charges" the tRNA molecules with the correct amino acids. The growing polypeptide is often termed the "nascent chain". Proteins are always biosynthesized from N-terminus to C-terminus.

The size of a synthesized protein can be measured by the number of amino acids it contains and by its total molecular mass, which is normally reported in units of "daltons" (synonymous with atomic mass units), or the derivative unit kilodalton (kDa). The average size of a protein increases from Archaea to Bacteria to Eukaryote (283, 311, 438 residues and 31, 34, 49 kDa respecitvely) due to a bigger number of protein domains constituting proteins in higher organisms. For instance, yeast proteins are on average 466 amino acids long and 53 kDa in mass. The largest known proteins are the titins, a component of the muscle sarcomere, with a molecular mass of almost 3,000 kDa and a total length of almost 27,000 amino acids.

Short proteins can also be synthesized chemically by a family of methods known as peptide synthesis, which rely on organic synthesis techniques such as chemical ligation to produce peptides in high yield. Chemical synthesis allows for the introduction of non-natural amino acids into polypeptide chains, such as attachment of fluorescent probes to amino acid side chains. These methods are useful in laboratory biochemistry and cell biology, though generally not for commercial applications. Chemical synthesis is inefficient for polypeptides longer than about 300 amino acids, and the synthesized proteins may not readily assume their native tertiary structure. Most chemical synthesis methods proceed from C-terminus to N-terminus, opposite the biological reaction.

Most proteins fold into unique 3-dimensional structures. The shape into which a protein naturally folds is known as its native conformation. Although many proteins can fold unassisted, simply through the chemical properties of their amino acids, others require the aid of molecular chaperones to fold into their native states. Biochemists often refer to four distinct aspects of a protein's structure:

Proteins are not entirely rigid molecules. In addition to these levels of structure, proteins may shift between several related structures while they perform their functions. In the context of these functional rearrangements, these tertiary or quaternary structures are usually referred to as "conformations", and transitions between them are called "conformational changes." Such changes are often induced by the binding of a substrate molecule to an enzyme's active site, or the physical region of the protein that participates in chemical catalysis. In solution proteins also undergo variation in structure through thermal vibration and the collision with other molecules.

Proteins can be informally divided into three main classes, which correlate with typical tertiary structures: globular proteins, fibrous proteins, and membrane proteins. Almost all globular proteins are soluble and many are enzymes. Fibrous proteins are often structural, such as collagen, the major component of connective tissue, or keratin, the protein component of hair and nails. Membrane proteins often serve as receptors or provide channels for polar or charged molecules to pass through the cell membrane.

A special case of intramolecular hydrogen bonds within proteins, poorly shielded from water attack and hence promoting their own dehydration, are called dehydrons.

Many proteins are composed of several protein domains, i.e. segments of a protein that fold into distinct structural units. Domains usually also have specific functions, such as enzymatic activities (e.g. kinase) or they serve as binding modules (e.g. the SH3 domain binds to proline-rich sequences in other proteins).

Short amino acid sequences within proteins often act as recognition sites for other proteins. For instance, SH3 domains typically bind to short PxxP motifs (i.e. 2 prolines [P], separated by 2 unspecified amino acids [x], although the surrounding amino acids may determine the exact binding specificity). A large number of such motifs has been collected in the Eukaryotic Linear Motif (ELM) database.

Proteins are the chief actors within the cell, said to be carrying out the duties specified by the information encoded in genes. With the exception of certain types of RNA, most other biological molecules are relatively inert elements upon which proteins act. Proteins make up half the dry weight of an "Escherichia coli" cell, whereas other macromolecules such as DNA and RNA make up only 3% and 20%, respectively. The set of proteins expressed in a particular cell or cell type is known as its proteome.

The chief characteristic of proteins that also allows their diverse set of functions is their ability to bind other molecules specifically and tightly. The region of the protein responsible for binding another molecule is known as the binding site and is often a depression or "pocket" on the molecular surface. This binding ability is mediated by the tertiary structure of the protein, which defines the binding site pocket, and by the chemical properties of the surrounding amino acids' side chains. Protein binding can be extraordinarily tight and specific; for example, the ribonuclease inhibitor protein binds to human angiogenin with a sub-femtomolar dissociation constant (<10 M) but does not bind at all to its amphibian homolog onconase (>1 M). Extremely minor chemical changes such as the addition of a single methyl group to a binding partner can sometimes suffice to nearly eliminate binding; for example, the aminoacyl tRNA synthetase specific to the amino acid valine discriminates against the very similar side chain of the amino acid isoleucine.

Proteins can bind to other proteins as well as to small-molecule substrates. When proteins bind specifically to other copies of the same molecule, they can oligomerize to form fibrils; this process occurs often in structural proteins that consist of globular monomers that self-associate to form rigid fibers. Protein–protein interactions also regulate enzymatic activity, control progression through the cell cycle, and allow the assembly of large protein complexes that carry out many closely related reactions with a common biological function. Proteins can also bind to, or even be integrated into, cell membranes. The ability of binding partners to induce conformational changes in proteins allows the construction of enormously complex signaling networks.
As interactions between proteins are reversible, and depend heavily on the availability of different groups of partner proteins to form aggregates that are capable to carry out discrete sets of function, study of the interactions between specific proteins is a key to understand important aspects of cellular function, and ultimately the properties that distinguish particular cell types.

The best-known role of proteins in the cell is as enzymes, which catalyse chemical reactions. Enzymes are usually highly specific and accelerate only one or a few chemical reactions. Enzymes carry out most of the reactions involved in metabolism, as well as manipulating DNA in processes such as DNA replication, DNA repair, and transcription. Some enzymes act on other proteins to add or remove chemical groups in a process known as posttranslational modification. About 4,000 reactions are known to be catalysed by enzymes. The rate acceleration conferred by enzymatic catalysis is often enormous—as much as 10-fold increase in rate over the uncatalysed reaction in the case of orotate decarboxylase (78 million years without the enzyme, 18 milliseconds with the enzyme).

The molecules bound and acted upon by enzymes are called substrates. Although enzymes can consist of hundreds of amino acids, it is usually only a small fraction of the residues that come in contact with the substrate, and an even smaller fraction—three to four residues on average—that are directly involved in catalysis. The region of the enzyme that binds the substrate and contains the catalytic residues is known as the active site.

Dirigent proteins are members of a class of proteins that dictate the stereochemistry of a compound synthesized by other enzymes.

Many proteins are involved in the process of cell signaling and signal transduction. Some proteins, such as insulin, are extracellular proteins that transmit a signal from the cell in which they were synthesized to other cells in distant tissues. Others are membrane proteins that act as receptors whose main function is to bind a signaling molecule and induce a biochemical response in the cell. Many receptors have a binding site exposed on the cell surface and an effector domain within the cell, which may have enzymatic activity or may undergo a conformational change detected by other proteins within the cell.

Antibodies are protein components of an adaptive immune system whose main function is to bind antigens, or foreign substances in the body, and target them for destruction. Antibodies can be secreted into the extracellular environment or anchored in the membranes of specialized B cells known as plasma cells. Whereas enzymes are limited in their binding affinity for their substrates by the necessity of conducting their reaction, antibodies have no such constraints. An antibody's binding affinity to its target is extraordinarily high.

Many ligand transport proteins bind particular small biomolecules and transport them to other locations in the body of a multicellular organism. These proteins must have a high binding affinity when their ligand is present in high concentrations, but must also release the ligand when it is present at low concentrations in the target tissues. The canonical example of a ligand-binding protein is haemoglobin, which transports oxygen from the lungs to other organs and tissues in all vertebrates and has close homologs in every biological kingdom. Lectins are sugar-binding proteins which are highly specific for their sugar moieties. Lectins typically play a role in biological recognition phenomena involving cells and proteins. Receptors and hormones are highly specific binding proteins.

Transmembrane proteins can also serve as ligand transport proteins that alter the permeability of the cell membrane to small molecules and ions. The membrane alone has a hydrophobic core through which polar or charged molecules cannot diffuse. Membrane proteins contain internal channels that allow such molecules to enter and exit the cell. Many ion channel proteins are specialized to select for only a particular ion; for example, potassium and sodium channels often discriminate for only one of the two ions.

Structural proteins confer stiffness and rigidity to otherwise-fluid biological components. Most structural proteins are fibrous proteins; for example, collagen and elastin are critical components of connective tissue such as cartilage, and keratin is found in hard or filamentous structures such as hair, nails, feathers, hooves, and some animal shells. Some globular proteins can also play structural functions, for example, actin and tubulin are globular and soluble as monomers, but polymerize to form long, stiff fibers that make up the cytoskeleton, which allows the cell to maintain its shape and size.

Other proteins that serve structural functions are motor proteins such as myosin, kinesin, and dynein, which are capable of generating mechanical forces. These proteins are crucial for cellular motility of single celled organisms and the sperm of many multicellular organisms which reproduce sexually. They also generate the forces exerted by contracting muscles and play essential roles in intracellular transport.

The activities and structures of proteins may be examined "in vitro," "in vivo, and in silico". In vitro studies of purified proteins in controlled environments are useful for learning how a protein carries out its function: for example, enzyme kinetics studies explore the chemical mechanism of an enzyme's catalytic activity and its relative affinity for various possible substrate molecules. By contrast, in vivo experiments can provide information about the physiological role of a protein in the context of a cell or even a whole organism. In silico studies use computational methods to study proteins.

To perform "in vitro" analysis, a protein must be purified away from other cellular components. This process usually begins with cell lysis, in which a cell's membrane is disrupted and its internal contents released into a solution known as a crude lysate. The resulting mixture can be purified using ultracentrifugation, which fractionates the various cellular components into fractions containing soluble proteins; membrane lipids and proteins; cellular organelles, and nucleic acids. Precipitation by a method known as salting out can concentrate the proteins from this lysate. Various types of chromatography are then used to isolate the protein or proteins of interest based on properties such as molecular weight, net charge and binding affinity. The level of purification can be monitored using various types of gel electrophoresis if the desired protein's molecular weight and isoelectric point are known, by spectroscopy if the protein has distinguishable spectroscopic features, or by enzyme assays if the protein has enzymatic activity. Additionally, proteins can be isolated according to their charge using electrofocusing.

For natural proteins, a series of purification steps may be necessary to obtain protein sufficiently pure for laboratory applications. To simplify this process, genetic engineering is often used to add chemical features to proteins that make them easier to purify without affecting their structure or activity. Here, a "tag" consisting of a specific amino acid sequence, often a series of histidine residues (a "His-tag"), is attached to one terminus of the protein. As a result, when the lysate is passed over a chromatography column containing nickel, the histidine residues ligate the nickel and attach to the column while the untagged components of the lysate pass unimpeded. A number of different tags have been developed to help researchers purify specific proteins from complex mixtures.

The study of proteins "in vivo" is often concerned with the synthesis and localization of the protein within the cell. Although many intracellular proteins are synthesized in the cytoplasm and membrane-bound or secreted proteins in the endoplasmic reticulum, the specifics of how proteins are targeted to specific organelles or cellular structures is often unclear. A useful technique for assessing cellular localization uses genetic engineering to express in a cell a fusion protein or chimera consisting of the natural protein of interest linked to a "reporter" such as green fluorescent protein (GFP). The fused protein's position within the cell can be cleanly and efficiently visualized using microscopy, as shown in the figure opposite.

Other methods for elucidating the cellular location of proteins requires the use of known compartmental markers for regions such as the ER, the Golgi, lysosomes or vacuoles, mitochondria, chloroplasts, plasma membrane, etc. With the use of fluorescently tagged versions of these markers or of antibodies to known markers, it becomes much simpler to identify the localization of a protein of interest. For example, indirect immunofluorescence will allow for fluorescence colocalization and demonstration of location. Fluorescent dyes are used to label cellular compartments for a similar purpose.

Other possibilities exist, as well. For example, immunohistochemistry usually utilizes an antibody to one or more proteins of interest that are conjugated to enzymes yielding either luminescent or chromogenic signals that can be compared between samples, allowing for localization information. Another applicable technique is cofractionation in sucrose (or other material) gradients using isopycnic centrifugation. While this technique does not prove colocalization of a compartment of known density and the protein of interest, it does increase the likelihood, and is more amenable to large-scale studies.

Finally, the gold-standard method of cellular localization is immunoelectron microscopy. This technique also uses an antibody to the protein of interest, along with classical electron microscopy techniques. The sample is prepared for normal electron microscopic examination, and then treated with an antibody to the protein of interest that is conjugated to an extremely electro-dense material, usually gold. This allows for the localization of both ultrastructural details as well as the protein of interest.

Through another genetic engineering application known as site-directed mutagenesis, researchers can alter the protein sequence and hence its structure, cellular localization, and susceptibility to regulation. This technique even allows the incorporation of unnatural amino acids into proteins, using modified tRNAs, and may allow the rational design of new proteins with novel properties.

The total complement of proteins present at a time in a cell or cell type is known as its proteome, and the study of such large-scale data sets defines the field of proteomics, named by analogy to the related field of genomics. Key experimental techniques in proteomics include 2D electrophoresis, which allows the separation of a large number of proteins, mass spectrometry, which allows rapid high-throughput identification of proteins and sequencing of peptides (most often after in-gel digestion), protein microarrays, which allow the detection of the relative levels of a large number of proteins present in a cell, and two-hybrid screening, which allows the systematic exploration of protein–protein interactions. The total complement of biologically possible such interactions is known as the interactome. A systematic attempt to determine the structures of proteins representing every possible fold is known as structural genomics.

A vast array of computational methods have been developed to analyze the structure, function, and evolution of proteins.

The development of such tools has been driven by the large amount of genomic and proteomic data available for a variety of organisms, including the human genome. It is simply impossible to study all proteins experimentally, hence only a few are subjected to laboratory experiments while computational tools are used to extrapolate to similar proteins. Such homologous proteins can be efficiently identified in distantly related organisms by sequence alignment. Genome and gene sequences can be searched by a variety of tools for certain properties. Sequence profiling tools can find restriction enzyme sites, open reading frames in nucleotide sequences, and predict secondary structures. Phylogenetic trees can be constructed and evolutionary hypotheses developed using special software like ClustalW regarding the ancestry of modern organisms and the genes they express. The field of bioinformatics is now indispensable for the analysis of genes and proteins.

Discovering the tertiary structure of a protein, or the quaternary structure of its complexes, can provide important clues about how the protein performs its function. Common experimental methods of structure determination include X-ray crystallography and NMR spectroscopy, both of which can produce information at atomic resolution. However, NMR experiments are able to provide information from which a subset of distances between pairs of atoms can be estimated, and the final possible conformations for a protein are determined by solving a distance geometry problem. Dual polarisation interferometry is a quantitative analytical method for measuring the overall protein conformation and conformational changes due to interactions or other stimulus. Circular dichroism is another laboratory technique for determining internal β-sheet / α-helical composition of proteins. Cryoelectron microscopy is used to produce lower-resolution structural information about very large protein complexes, including assembled viruses; a variant known as electron crystallography can also produce high-resolution information in some cases, especially for two-dimensional crystals of membrane proteins. Solved structures are usually deposited in the Protein Data Bank (PDB), a freely available resource from which structural data about thousands of proteins can be obtained in the form of Cartesian coordinates for each atom in the protein.

Many more gene sequences are known than protein structures. Further, the set of solved structures is biased toward proteins that can be easily subjected to the conditions required in X-ray crystallography, one of the major structure determination methods. In particular, globular proteins are comparatively easy to crystallize in preparation for X-ray crystallography. Membrane proteins, by contrast, are difficult to crystallize and are underrepresented in the PDB. Structural genomics initiatives have attempted to remedy these deficiencies by systematically solving representative structures of major fold classes. Protein structure prediction methods attempt to provide a means of generating a plausible structure for proteins whose structures have not been experimentally determined.

Complementary to the field of structural genomics, "protein structure prediction" develops efficient mathematical models of proteins to computationally predict the molecular formations in theory, instead of detecting structures with laboratory observation. The most successful type of structure prediction, known as homology modeling, relies on the existence of a "template" structure with sequence similarity to the protein being modeled; structural genomics' goal is to provide sufficient representation in solved structures to model most of those that remain. Although producing accurate models remains a challenge when only distantly related template structures are available, it has been suggested that sequence alignment is the bottleneck in this process, as quite accurate models can be produced if a "perfect" sequence alignment is known. Many structure prediction methods have served to inform the emerging field of protein engineering, in which novel protein folds have already been designed. A more complex computational problem is the prediction of intermolecular interactions, such as in molecular docking and protein–protein interaction prediction.

Mathematical models to simulate dynamic processes of protein folding and binding involve molecular mechanics, in particular, molecular dynamics. Monte Carlo techniques facilitate the computations, which exploit advances in parallel and distributed computing (for example, the Folding@home project which performs molecular modeling on GPUs). "In silico" simulations discovered the folding of small α-helical protein domains such as the villin headpiece and the HIV accessory protein. Hybrid methods combining standard molecular dynamics with quantum mechanical mathematics explored the electronic states of rhodopsins.

Many proteins (in Eucaryota ~33%) contain large unstructured but biologically functional segments and can be classified as intrinsically disordered proteins. Predicting and analysing protein disorder is, therefore, an important part of protein structure characterisation.

Most microorganisms and plants can biosynthesize all 20 standard amino acids, while animals (including humans) must obtain some of the amino acids from the diet. The amino acids that an organism cannot synthesize on its own are referred to as essential amino acids. Key enzymes that synthesize certain amino acids are not present in animals — such as aspartokinase, which catalyses the first step in the synthesis of lysine, methionine, and threonine from aspartate. If amino acids are present in the environment, microorganisms can conserve energy by taking up the amino acids from their surroundings and downregulating their biosynthetic pathways.

In animals, amino acids are obtained through the consumption of foods containing protein. Ingested proteins are then broken down into amino acids through digestion, which typically involves denaturation of the protein through exposure to acid and hydrolysis by enzymes called proteases. Some ingested amino acids are used for protein biosynthesis, while others are converted to glucose through gluconeogenesis, or fed into the citric acid cycle. This use of protein as a fuel is particularly important under starvation conditions as it allows the body's own proteins to be used to support life, particularly those found in muscle.

In animals such as dogs and cats, protein maintains the health and quality of the skin by promoting hair follicle growth and keratinization, and thus reducing the likelihood of skin problems producing malodours. Poor-quality proteins also have a role regarding gastrointestinal health, increasing the potential for flatulence and odorous compounds in dogs because when proteins reach the colon in an undigested state, they are fermented producing hydrogen sulfide gas, indole, and skatole. Dogs and cats digest animal proteins better than those from plants but products of low-quality animal origin are poorly digested, including skin, feathers, and connective tissue.

Proteins were recognized as a distinct class of biological molecules in the eighteenth century by Antoine Fourcroy and others, distinguished by the molecules' ability to coagulate or flocculate under treatments with heat or acid. Noted examples at the time included albumin from egg whites, blood serum albumin, fibrin, and wheat gluten.

Proteins were first described by the Dutch chemist Gerardus Johannes Mulder and named by the Swedish chemist Jöns Jacob Berzelius in 1838. Mulder carried out elemental analysis of common proteins and found that nearly all proteins had the same empirical formula, CHNOPS. He came to the erroneous conclusion that they might be composed of a single type of (very large) molecule. The term "protein" to describe these molecules was proposed by Mulder's associate Berzelius; protein is derived from the Greek word πρώτειος ("proteios"), meaning "primary", "in the lead", or "standing in front", + "-in". Mulder went on to identify the products of protein degradation such as the amino acid leucine for which he found a (nearly correct) molecular weight of 131 Da. Prior to "protein", other names were used, like "albumins" or "albuminous materials" ("Eiweisskörper", in German).

Early nutritional scientists such as the German Carl von Voit believed that protein was the most important nutrient for maintaining the structure of the body, because it was generally believed that "flesh makes flesh." Karl Heinrich Ritthausen extended known protein forms with the identification of glutamic acid. At the Connecticut Agricultural Experiment Station a detailed review of the vegetable proteins was compiled by Thomas Burr Osborne. Working with Lafayette Mendel and applying Liebig's law of the minimum in feeding laboratory rats, the nutritionally essential amino acids were established. The work was continued and communicated by William Cumming Rose. The understanding of proteins as polypeptides came through the work of Franz Hofmeister and Hermann Emil Fischer in 1902. The central role of proteins as enzymes in living organisms was not fully appreciated until 1926, when James B. Sumner showed that the enzyme urease was in fact a protein.

The difficulty in purifying proteins in large quantities made them very difficult for early protein biochemists to study. Hence, early studies focused on proteins that could be purified in large quantities, e.g., those of blood, egg white, various toxins, and digestive/metabolic enzymes obtained from slaughterhouses. In the 1950s, the Armour Hot Dog Co. purified 1 kg of pure bovine pancreatic ribonuclease A and made it freely available to scientists; this gesture helped ribonuclease A become a major target for biochemical study for the following decades.

Linus Pauling is credited with the successful prediction of regular protein secondary structures based on hydrogen bonding, an idea first put forth by William Astbury in 1933. Later work by Walter Kauzmann on denaturation, based partly on previous studies by Kaj Linderstrøm-Lang, contributed an understanding of protein folding and structure mediated by hydrophobic interactions.

The first protein to be sequenced was insulin, by Frederick Sanger, in 1949. Sanger correctly determined the amino acid sequence of insulin, thus conclusively demonstrating that proteins consisted of linear polymers of amino acids rather than branched chains, colloids, or cyclols. He won the Nobel Prize for this achievement in 1958.

The first protein structures to be solved were hemoglobin and myoglobin, by Max Perutz and Sir John Cowdery Kendrew, respectively, in 1958. , the Protein Data Bank has over 126,060 atomic-resolution structures of proteins. In more recent times, cryo-electron microscopy of large macromolecular assemblies and computational protein structure prediction of small protein domains are two methods approaching atomic resolution.





</doc>
<doc id="23635" url="https://en.wikipedia.org/wiki?curid=23635" title="Physical chemistry">
Physical chemistry

Physical Chemistry is the study of macroscopic, atomic, subatomic, and particulate phenomena in chemical systems in terms of the principles, practices, and concepts of physics such as motion, energy, force, time, thermodynamics, quantum chemistry, statistical mechanics, analytical dynamics and chemical equilibrium.

Physical chemistry, in contrast to chemical physics, is predominantly (but not always) a macroscopic or supra-molecular science, as the majority of the principles on which it was founded relate to the bulk rather than the molecular/atomic structure alone (for example, chemical equilibrium and colloids).

Some of the relationships that physical chemistry strives to resolve include the effects of:

The key concepts of physical chemistry are the ways in which pure physics is applied to chemical problems.

One of the key concepts in classical chemistry is that all chemical compounds can be described as groups of atoms bonded together and chemical reactions can be described as the making and breaking of those bonds. Predicting the properties of chemical compounds from a description of atoms and how they bond is one of the major goals of physical chemistry. To describe the atoms and bonds precisely, it is necessary to know both where the nuclei of the atoms are, and how electrons are distributed around them.<br>
Quantum chemistry, a subfield of physical chemistry especially concerned with the application of quantum mechanics to chemical problems, provides tools to determine how strong and what shape bonds are, how nuclei move, and how light can be absorbed or emitted by a chemical compound. Spectroscopy is the related sub-discipline of physical chemistry which is specifically concerned with the interaction of electromagnetic radiation with matter.

Another set of important questions in chemistry concerns what kind of reactions can happen spontaneously and which properties are possible for a given chemical mixture. This is studied in chemical thermodynamics, which sets limits on quantities like how far a reaction can proceed, or how much energy can be converted into work in an internal combustion engine, and which provides links between properties like the thermal expansion coefficient and rate of change of entropy with pressure for a gas or a liquid. It can frequently be used to assess whether a reactor or engine design is feasible, or to check the validity of experimental data. To a limited extent, quasi-equilibrium and non-equilibrium thermodynamics can describe irreversible changes. However, classical thermodynamics is mostly concerned with systems in equilibrium and reversible changes and not what actually does happen, or how fast, away from equilibrium.

Which reactions do occur and how fast is the subject of chemical kinetics, another branch of physical chemistry. A key idea in chemical kinetics is that for reactants to react and form products, most chemical species must go through transition states which are higher in energy than either the reactants or the products and serve as a barrier to reaction. In general, the higher the barrier, the slower the reaction. A second is that most chemical reactions occur as a sequence of elementary reactions, each with its own transition state. Key questions in kinetics include how the rate of reaction depends on temperature and on the concentrations of reactants and catalysts in the reaction mixture, as well as how catalysts and reaction conditions can be engineered to optimize the reaction rate.

The fact that how fast reactions occur can often be specified with just a few concentrations and a temperature, instead of needing to know all the positions and speeds of every molecule in a mixture, is a special case of another key concept in physical chemistry, which is that to the extent an engineer needs to know, everything going on in a mixture of very large numbers (perhaps of the order of the Avogadro constant, 6 x 10) of particles can often be described by just a few variables like pressure, temperature, and concentration. The precise reasons for this are described in statistical mechanics, a specialty within physical chemistry which is also shared with physics. Statistical mechanics also provides ways to predict the properties we see in everyday life from molecular properties without relying on empirical correlations based on chemical similarities.

The term "physical chemistry" was coined by Mikhail Lomonosov in 1752, when he presented a lecture course entitled "A Course in True Physical Chemistry" (Russian: «Курс истинной физической химии») before the students of Petersburg University. In the preamble to these lectures he gives the definition: "Physical chemistry is the science that must explain under provisions of physical experiments the reason for what is happening in complex bodies through chemical operations".

Modern physical chemistry originated in the 1860s to 1880s with work on chemical thermodynamics, electrolytes in solutions, chemical kinetics and other subjects. One milestone was the publication in 1876 by Josiah Willard Gibbs of his paper, "On the Equilibrium of Heterogeneous Substances". This paper introduced several of the cornerstones of physical chemistry, such as Gibbs energy, chemical potentials, and Gibbs' phase rule. Other milestones include the subsequent naming and accreditation of enthalpy to Heike Kamerlingh Onnes and to macromolecular processes. 

The first scientific journal specifically in the field of physical chemistry was the German journal, "Zeitschrift für Physikalische Chemie", founded in 1887 by Wilhelm Ostwald and Jacobus Henricus van 't Hoff. Together with Svante August Arrhenius, these were the leading figures in physical chemistry in the late 19th century and early 20th century. All three were awarded the Nobel Prize in Chemistry between 1901–1909.

Developments in the following decades include the application of statistical mechanics to chemical systems and work on colloids and surface chemistry, where Irving Langmuir made many contributions. Another important step was the development of quantum mechanics into quantum chemistry from the 1930s, where Linus Pauling was one of the leading names. Theoretical developments have gone hand in hand with developments in experimental methods, where the use of different forms of spectroscopy, such as infrared spectroscopy, microwave spectroscopy, electron paramagnetic resonance and nuclear magnetic resonance spectroscopy, is probably the most important 20th century development.

Further development in physical chemistry may be attributed to discoveries in nuclear chemistry, especially in isotope separation (before and during World War II), more recent discoveries in astrochemistry, as well as the development of calculation algorithms in the field of "additive physicochemical properties" (practically all physicochemical properties, such as boiling point, critical point, surface tension, vapor pressure, etc.—more than 20 in all—can be precisely calculated from chemical structure alone, even if the chemical molecule remains unsynthesized), and herein lies the practical importance of contemporary physical chemistry.

See Group contribution method, Lydersen method, Joback method, Benson group increment theory, quantitative structure–activity relationship

Some journals that deal with physical chemistry include Zeitschrift für Physikalische Chemie (1887); Journal of Physical Chemistry A (from 1896 as "Journal of Physical Chemistry", renamed in 1997); Physical Chemistry Chemical Physics (from 1999, formerly Faraday Transactions with a history dating back to 1905); Macromolecular Chemistry and Physics (1947); Annual Review of Physical Chemistry (1950); Molecular Physics (1957); Journal of Physical Organic Chemistry (1988); Journal of Physical Chemistry B (1997); ChemPhysChem (2000); Journal of Physical Chemistry C (2007); and Journal of Physical Chemistry Letters (from 2010, combined letters previously published in the separate journals)

Historical journals that covered both chemistry and physics include Annales de chimie et de physique (started in 1789, published under the name given here from 1815–1914).




</doc>
<doc id="23636" url="https://en.wikipedia.org/wiki?curid=23636" title="Perimeter">
Perimeter

A perimeter is a path that surrounds a two-dimensional shape. The term may be used either for the path or its length—it can be thought of as the length of the outline of a shape. The perimeter of a circle or ellipse is called its circumference.

Calculating the perimeter has several practical applications. A calculated perimeter is the length of fence required to surround a yard or garden. The perimeter of a wheel (its circumference) describes how far it will roll in one revolution. Similarly, the amount of string wound around a spool is related to the spool's perimeter.

The perimeter is the distance around a shape. Perimeters for more general shapes can be calculated, as any path, with formula_1, where formula_2 is the length of the path and formula_3 is an infinitesimal line element. Both of these must be replaced with by algebraic forms in order to be practically calculated. If the perimeter is given as a closed piecewise smooth plane curve formula_4 with
then its length formula_2 can be computed as follows:

A generalized notion of perimeter, which includes hypersurfaces bounding volumes in formula_8-dimensional Euclidean spaces, is described by the theory of Caccioppoli sets.

Polygons are fundamental to determining perimeters, not only because they are the simplest shapes but also because the perimeters of many shapes are calculated by approximating them with sequences of polygons tending to these shapes. The first mathematician known to have used this kind of reasoning is Archimedes, who approximated the perimeter of a circle by surrounding it with regular polygons.

The perimeter of a polygon equals the sum of the lengths of its sides (edges). In particular, the perimeter of a rectangle of width formula_9 and length formula_10 equals formula_11

An equilateral polygon is a polygon which has all sides of the same length (for example, a rhombus is a 4-sided equilateral polygon). To calculate the perimeter of an equilateral polygon, one must multiply the common length of the sides by the number of sides.

A regular polygon may be characterized by the number of its sides and by its circumradius, that is to say, the constant distance between its centre and each of its vertices. The length of its sides can be calculated using trigonometry. If is a regular polygon's radius and is the number of its sides, then its perimeter is 

A splitter of a triangle is a cevian (a segment from a vertex to the opposite side) that divides the perimeter into two equal lengths, this common length being called the semiperimeter of the triangle. The three splitters of a triangle all intersect each other at the Nagel point of the triangle.

A cleaver of a triangle is a segment from the midpoint of a side of a triangle to the opposite side such that the perimeter is divided into two equal lengths. The three cleavers of a triangle all intersect each other at the triangle's Spieker center.

The perimeter of a circle, often called the circumference, is proportional to its diameter and its radius. That is to say, there exists a constant number pi, (the Greek "p" for perimeter), such that if is the circle's perimeter and its diameter then,

In terms of the radius of the circle, this formula becomes,

To calculate a circle's perimeter, knowledge of its radius or diameter and the number suffices. The problem is that is not rational (it cannot be expressed as the quotient of two integers), nor is it algebraic (it is not a root of a polynomial equation with rational coefficients). So, obtaining an accurate approximation of is important in the calculation. The computation of the digits of is relevant to many fields, such as mathematical analysis, algorithmics and computer science.

The perimeter and the area are two main measures of geometric figures. Confusing them is a common error, as well as believing that the greater one of them is, the greater the other must be. Indeed, a commonplace observation is that an enlargement (or a reduction) of a shape make its area grow (or decrease) as well as its perimeter. For example, if a field is drawn on a 1/ scale map, the actual field perimeter can be calculated multiplying the drawing perimeter by . The real area is times the area of the shape on the map. Nevertheless, there is no relation between the area and the perimeter of an ordinary shape. For example, the perimeter of a rectangle of width 0.001 and length 1000 is slightly above 2000, while the perimeter of a rectangle of width 0.5 and length 2 is 5. Both areas equal to 1.

Proclus (5th century) reported that Greek peasants "fairly" parted fields relying on their perimeters. However, a field's production is proportional to its area, not to its perimeter, so many naive peasants may have gotten fields with long perimeters but small areas (thus, few crops).

If one removes a piece from a figure, its area decreases but its perimeter may not. In the case of very irregular shapes, confusion between the perimeter and the convex hull may arise. The convex hull of a figure may be visualized as the shape formed by a rubber band stretched around it. In the animated picture on the left, all the figures have the same convex hull; the big, first hexagon.

The isoperimetric problem is to determine a figure with the largest area, amongst those having a given perimeter. The solution is intuitive; it is the circle. In particular, this can be used to explain why drops of fat on a broth surface are circular.

This problem may seem simple, but its mathematical proof requires some sophisticated theorems. The isoperimetric problem is sometimes simplified by restricting the type of figures to be used. In particular, to find the quadrilateral, or the triangle, or another particular figure, with the largest area amongst those with the same shape having a given perimeter. The solution to the quadrilateral isoperimetric problem is the square, and the solution to the triangle problem is the equilateral triangle. In general, the polygon with sides having the largest area and a given perimeter is the regular polygon, which is closer to being a circle than is any irregular polygon with the same number of sides.

The word comes from the Greek περίμετρος "perimetros" from περί "peri" "around" and μέτρον "metron" "measure".




</doc>
<doc id="23637" url="https://en.wikipedia.org/wiki?curid=23637" title="Phase (matter)">
Phase (matter)

In the physical sciences, a phase is a region of space (a thermodynamic system), throughout which all physical properties of a material are essentially uniform. Examples of physical properties include density, index of refraction, magnetization and chemical composition. A simple description is that a phase is a region of material that is chemically uniform, physically distinct, and (often) mechanically separable. In a system consisting of ice and water in a glass jar, the ice cubes are one phase, the water is a second phase, and the humid air is a third phase over the ice and water. The glass of the jar is another separate phase. (See state of matter#Glass)

The term "phase" is sometimes used as a synonym for state of matter, but there can be several immiscible phases of the same state of matter. Also, the term "phase" is sometimes used to refer to a set of equilibrium states demarcated in terms of state variables such as pressure and temperature by a phase boundary on a phase diagram. Because phase boundaries relate to changes in the organization of matter, such as a change from liquid to solid or a more subtle change from one crystal structure to another, this latter usage is similar to the use of "phase" as a synonym for state of matter. However, the state of matter and phase diagram usages are not commensurate with the formal definition given above and the intended meaning must be determined in part from the context in which the term is used.

Distinct phases may be described as different states of matter such as gas, liquid, solid, plasma or Bose–Einstein condensate. Useful mesophases between solid and liquid form other states of matter.

Distinct phases may also exist within a given state of matter. As shown in the diagram for iron alloys, several phases exist for both the solid and liquid states. Phases may also be differentiated based on solubility as in polar (hydrophilic) or non-polar (hydrophobic). A mixture of water (a polar liquid) and oil (a non-polar liquid) will spontaneously separate into two phases. Water has a very low solubility (is insoluble) in oil, and oil has a low solubility in water. Solubility is the maximum amount of a solute that can dissolve in a solvent before the solute ceases to dissolve and remains in a separate phase. A mixture can separate into more than two liquid phases and the concept of phase separation extends to solids, i.e., solids can form solid solutions or crystallize into distinct crystal phases. Metal pairs that are mutually soluble can form alloys, whereas metal pairs that are mutually insoluble cannot.

As many as eight immiscible liquid phases have been observed. Mutually immiscible liquid phases are formed from water (aqueous phase), hydrophobic organic solvents, perfluorocarbons (fluorous phase), silicones, several different metals, and also from molten phosphorus. Not all organic solvents are completely miscible, e.g. a mixture of ethylene glycol and toluene may separate into two distinct organic phases.

Phases do not need to macroscopically separate spontaneously. Emulsions and colloids are examples of immiscible phase pair combinations that do not physically separate.
Left to equilibration, many compositions will form a uniform single phase, but depending on the temperature and pressure even a single substance may separate into two or more distinct phases. Within each phase, the properties are uniform but between the two phases properties differ.

Water in a closed jar with an air space over it forms a two phase system. Most of the water is in the liquid phase, where it is held by the mutual attraction of water molecules. Even at equilibrium molecules are constantly in motion and, once in a while, a molecule in the liquid phase gains enough kinetic energy to break away from the liquid phase and enter the gas phase. Likewise, every once in a while a vapor molecule collides with the liquid surface and condenses into the liquid. At equilibrium, evaporation and condensation processes exactly balance and there is no net change in the volume of either phase.

At room temperature and pressure, the water jar reaches equilibrium when the air over the water has a humidity of about 3%. This percentage increases as the temperature goes up. At 100 °C and atmospheric pressure, equilibrium is not reached until the air is 100% water. If the liquid is heated a little over 100 °C, the transition from liquid to gas will occur not only at the surface, but throughout the liquid volume: the water boils.

For a given composition, only certain phases are possible at a given temperature and pressure. The number and type of phases that will form is hard to predict and is usually determined by experiment. The results of such experiments can be plotted in phase diagrams.

The phase diagram shown here is for a single component system. In this simple system, which phases that are possible depends only on pressure and temperature. The markings show points where two or more phases can co-exist in equilibrium. At temperatures and pressures away from the markings, there will be only one phase at equilibrium.

In the diagram, the blue line marking the boundary between liquid and gas does not continue indefinitely, but terminates at a point called the critical point. As the temperature and pressure approach the critical point, the properties of the liquid and gas become progressively more similar. At the critical point, the liquid and gas become indistinguishable. Above the critical point, there are no longer separate liquid and gas phases: there is only a generic fluid phase referred to as a supercritical fluid. In water, the critical point occurs at around 647 K (374 °C or 705 °F) and 22.064 MPa.

An unusual feature of the water phase diagram is that the solid–liquid phase line (illustrated by the dotted green line) has a negative slope. For most substances, the slope is positive as exemplified by the dark green line. This unusual feature of water is related to ice having a lower density than liquid water. Increasing the pressure drives the water into the higher density phase, which causes melting.

Another interesting though not unusual feature of the phase diagram is the point where the solid–liquid phase line meets the liquid–gas phase line. The intersection is referred to as the triple point. At the triple point, all three phases can coexist.

Experimentally, the phase lines are relatively easy to map due to the interdependence of temperature and pressure that develops when multiple phases forms. See Gibbs' phase rule. Consider a test apparatus consisting of a closed and well insulated cylinder equipped with a piston. By charging the right amount of water and applying heat, the system can be brought to any point in the gas region of the phase diagram. If the piston is slowly lowered, the system will trace a curve of increasing temperature and pressure within the gas region of the phase diagram. At the point where gas begins to condense to liquid, the direction of the temperature and pressure curve will abruptly change to trace along the phase line until all of the water has condensed.

Between two phases in equilibrium there is a narrow region where the properties are not that of either phase. Although this region may be very thin, it can have significant and easily observable effects, such as causing a liquid to exhibit surface tension. In mixtures, some components may preferentially move toward the interface. In terms of modeling, describing, or understanding the behavior of a particular system, it may be efficacious to treat the interfacial region as a separate phase.

A single material may have several distinct solid states capable of forming separate phases. Water is a well-known example of such a material. For example, water ice is ordinarily found in the hexagonal form ice I, but can also exist as the cubic ice I, the rhombohedral ice II, and many other forms. Polymorphism is the ability of a solid to exist in more than one crystal form. For pure chemical elements, polymorphism is known as allotropy. For example, diamond, graphite, and fullerenes are different allotropes of carbon.

When a substance undergoes a phase transition (changes from one state of matter to another) it usually either takes up or releases energy. For example, when water evaporates, the increase in kinetic energy as the evaporating molecules escape the attractive forces of the liquid is reflected in a decrease in temperature. The energy required to induce the phase transition is taken from the internal thermal energy of the water, which cools the liquid to a lower temperature; hence evaporation is useful for cooling. See Enthalpy of vaporization. The reverse process, condensation, releases heat. The heat energy, or enthalpy, associated with a solid to liquid transition is the enthalpy of fusion and that associated with a solid to gas transition is the enthalpy of sublimation.




</doc>
<doc id="23638" url="https://en.wikipedia.org/wiki?curid=23638" title="Outline of physical science">
Outline of physical science

Physical science is a branch of natural science that studies non-living systems, in contrast to life science. It in turn has many branches, each referred to as a "physical science", together called the "physical sciences". However, the term "physical" creates an unintended, somewhat arbitrary distinction, since many branches of physical science also study biological phenomena and branches of chemistry such as organic chemistry.

Physical science can be described as all of the following:


History of physical science – history of the branch of natural science that studies non-living systems, in contrast to the life sciences. It in turn has many branches, each referred to as a "physical science", together called the "physical sciences". However, the term "physical" creates an unintended, somewhat arbitrary distinction, since many branches of physical science also study biological phenomena (organic chemistry, for example).


Physics – branch of science that studies matter and its motion through space and time, along with related concepts such as energy and force. Physics is one of the "fundamental sciences" because the other natural sciences (like biology, geology etc.) deal with systems that seem to obey the laws of physics. According to physics, the physical laws of matter, energy and the fundamental forces of nature govern the interactions between particles and physical entities (such as planets, molecules, atoms or the subatomic particles). Some of the basic pursuits of physics, which include some of the most prominent developments in modern science in the last millennium, include:


Astronomy – science of celestial bodies and their interactions in space. Its studies includes the following:


Chemistry – branch of science that studies the composition, structure, properties and change of matter. Chemistry is chiefly concerned with atoms and molecules and their interactions and transformations, for example, the properties of the chemical bonds formed between atoms to create chemical compounds. As such, chemistry studies the involvement of electrons and various forms of energy in photochemical reactions, oxidation-reduction reactions, changes in phases of matter, and separation of mixtures. Preparation and properties of complex substances, such as alloys, polymers, biological molecules, and pharmaceutical agents are considered in specialized fields of chemistry.


Earth science – the science of the planet Earth, the only identified life-bearing planet. Its studies include the following:








</doc>
<doc id="23639" url="https://en.wikipedia.org/wiki?curid=23639" title="Gasoline">
Gasoline

Gasoline (American English), or petrol (British English), is a transparent, petroleum-derived liquid that is used primarily as a fuel in spark-ignited internal combustion engines. It consists mostly of organic compounds obtained by the fractional distillation of petroleum, enhanced with a variety of additives. On average, a 42-gallon barrel of crude oil (159 L) yields about of gasoline when processed in an oil refinery, though this varies based on the crude oil source's assay.

The characteristic of a particular gasoline blend to resist igniting too early (which causes knocking and reduces efficiency in reciprocating engines) is measured by its octane rating. Gasoline is produced in several grades of octane rating. Tetraethyllead and other lead compounds are no longer used in most areas to regulate and increase octane-rating, but many other additives are put into gasoline to improve its chemical stability, control corrosiveness, provide fuel system cleaning, and determine performance characteristics under intended use. Sometimes, gasoline also contains ethanol as an alternative fuel, for economic, political or environmental reasons.

Gasoline used in internal combustion engines has a significant effect on the environment, both in local effects (e.g., smog) and in global effects (e.g., effect on the climate). Gasoline may also enter the environment uncombusted, as liquid and as vapors, from leakage and handling during production, transport and delivery, from storage tanks, from spills, etc. As an example of efforts to control such leakage, many (underground) storage tanks are required to have extensive measures in place to detect and prevent such leaks. Gasoline contains benzene and other known carcinogens.

"Gasoline" is a North America word that refers to fuel for automobiles. The "Oxford English Dictionary" dates its first recorded use to 1863 when it was spelled "gasolene". The term "gasoline" was first used in North America in 1864. The words is a derivation from the word "gas" and the chemical suffixes "-ol" and "-ine" or "-ene". 

However, the term may also have been influenced by the trademark "Cazeline" or "Gazeline". On 27 November 1862, the British publisher, coffee merchant, and social campaigner John Cassell placed an advertisement in "The Times" of London:

"Petrol" is used in most Commonwealth countries. "Petrol" was first used as the name of a refined petroleum product around 1870 by British wholesaler Carless, Capel & Leonard, who marketed it as a solvent. When the product later found a new use as a motor fuel, Frederick Simms, an associate of Gottlieb Daimler, suggested to Carless that they register the trade mark "petrol", but by this time the word was already in general use, possibly inspired by the French "pétrole", and the registration was not allowed. Carless registered a number of alternative names for the product, but "petrol" became the common term for the fuel in the British Commonwealth.

British refiners originally used "motor spirit" as a generic name for the automotive fuel and "aviation spirit" for aviation gasoline. When Carless was denied a trademark on "petrol" in the 1930s, its competitors switched to the more popular name "petrol". However, "motor spirit" had already made its way into laws and regulations, so the term remains in use as a formal name for petrol. The term is used most widely in Nigeria, where the largest petroleum companies call their product "premium motor spirit". Although "petrol" has made inroads into Nigerian English, "premium motor spirit" remains the formal name that is used in scientific publications, government reports, and newspapers.

The use of the word "gasoline" instead of "petrol" outside North America can often be confusing. Shortening "gasoline" to "gas", which happens often, causes confusion with various forms of gas used as car fuel (compressed natural gas (CNG), liquefied natural gas (LNG) and liquefied petroleum gas (LPG)). In many countries, gasoline has a colloquial name derived from that of the chemical benzene ("e.g.", German "Benzin", Czech "benzín", Dutch "benzine", Italian "benzina", Russian бензин "benzin", Polish "benzyna", Chilean Spanish "bencina", Thai เบนซิน "bensin", Greek βενζίνη "venzini", Romanian "benzină", Hebrew בנזין "benzin", Swedish "bensin", Arabic بنزين "binzīn", Catalan "benzina"). Argentina, Uruguay and Paraguay use the colloquial name "nafta" derived from that of the chemical naphtha.

The first automotive combustion engines, so-called Otto engines, were developed in the last quarter of the 19th century in Germany. The fuel was a relatively volatile hydrocarbon obtained from coal gas. With a boiling point near 85 °C (octanes boil about 40 °C higher), it was well suited for early carburetors (evaporators). The development of a "spray nozzle" carburetor enabled the use of less volatile fuels. Further improvements in engine efficiency were attempted at higher compression ratios, but early attempts were blocked by knocking (premature explosion of fuel).

During the early period of gasoline engine development aircraft were forced to use motor vehicle gasoline since aviation gasoline did not exist. These early fuels were termed straight run gasolines and were byproducts from the distillation of a single crude oil to produce kerosene which was the principal product sought for lighting in kerosene lamps. Gasoline production would not surpass kerosene production until 1916. The earliest straight run gasolines were the result of distilling eastern crude oils and there was no mixing of distillates from different crudes. The composition of these early fuels was unknown and the quality varied greatly as crude oils from different oil fields created different mixtures of hydrocarbons in different ratios. The engine effects produced by abnormal combustion (engine knocking and pre-ignition) due to inferior fuels had not yet been identified and as a result there was no rating of gasoline in terms of its resistance to abnormal combustion. The general specification of early gasolines was that of specific gravity via the Baumé scale and later the volatility (ability to vaporize) specified in terms of boiling points which would be the primary focus of the producers. These early eastern crude oil gasolines had relatively high Baumé test results (65 to 80 degrees Baumé) and were called Pennsylvania "High-Test" or simply "High-Test" gasolines and these would often be used in aircraft engines.

By 1910 increased automobile production and the resultant increased gasoline consumption combined with the growing electrification of lighting producing a drop in kerosene demand created a supply problem. It appeared that the oil industry would be trapped into over producing kerosene and under producing gasoline since simple distillation could not alter the ratio of the two products from any given crude. The solution appeared in 1911 when the Burton process created thermal cracking of crude oils which increased the percent yield of gasoline from the heavier hydrocarbons and this was combined with expansion of foreign markets for the export of surplus kerosene which the domestic market no longer needed. These new thermally "cracked" gasolines were believed to have no harmful effects and would be added to straight run gasolines. There also was the practice of mixing heavy and light distillates to achieve a desired Baumé reading and collectively these were called "blended" gasolines. Gradually volatility gained favor over the Baumé test though both would be used in combination to specify a gasoline. As late as June, 1917 Standard Oil (the largest refiner of crude oil in the United States at this time) would state that the most important property of a gasoline was its volatility. It is estimated that the rating equivalent of these straight run gasolines varied from 40 to 60 octane and that the "High-Test" (sometimes referred to as "fighting grade") probably averaged 50 to 65 octane.

Prior to the American entry into World War I the European Allies were using fuels derived from crude oils from Borneo, Java and Sumatra which gave satisfactory performance in their military aircraft. With the United States entry in April, 1917, the U.S. became the principal supplier of aviation gasoline to the Allies and a decrease in engine performance was noted. Soon it was realized that motor vehicle fuels were unsatisfactory for aviation and after the loss of a number of combat aircraft attention turned to the quality of the gasolines being used. Later flight tests conducted in 1937 showed that an octane reduction of 13 points (from 100 down to 87 octane) decreased engine performance by 20% and take-off distance was increased 45 percent. If abnormal combustion were to occur the engine could lose enough power to make getting airborne impossible and a take-off roll became a threat to the pilot and aircraft. On August 2, 1917, the Bureau of Mines arranged to study fuels for aircraft in cooperation with the Aviation Section of the Signal Corps and a general survey concluded that no reliable data existed for the proper fuels for aircraft. As a result, flight tests began at Langley, McCook and Wright fields to determine how different gasolines performed under different conditions. These tests showed that in certain aircraft, motor vehicle gasolines performed as well as "High-Test" but in other types resulted in hot-running engines. Also, gasolines from aromatic and naphthenic base crude oils from California, South Texas and Venezuela resulted in smooth running engines. These tests resulted in the first government specifications for motor gasolines (aviation gasolines used the same specifications as motor gasolines) in late 1917. 

Engine designers knew that according to the Otto cycle power and efficiency increased with compression ratio but experience with these early gasolines during WW I showed that higher compression ratios increased the risk of abnormal combustion producing lower power, lower efficiency, hot running engines, and could lead to severe engine damage. To compensate for these poor fuels early engines used low compression ratios and this required relatively large, heavy engines to produce limited power and efficiency. The Wright Brothers first engine used a compression ratio as low as 4.7 to one and developed only 12 horsepower from 201 cubic inches and weighed 180 pounds. . This was a major concern for aircraft designers and the needs of the aviation industry led the search for fuels that could be used in higher compression engines.

Between 1917 and 1919 the amount of thermally cracked gasoline utilized almost doubled. Also, the use of Natural gasoline increased greatly. During this period many states established specifications for motor gasoline but none of these agreed and were unsatisfactory from one standpoint or another. Larger oil refiners began to specify unsaturated material percentage (thermally cracked products caused gummming in both use and storage. See Saturated and unsaturated compounds and unsaturated hydrocarbons are more reactive and tend to combine with impurities leading to gumming). In 1922 the government published the first specifications for aviation gasolines (two grades were designated as "Fighting" and "Domestic" and were governed by boiling points, color, sulphur content and a gum formation test) along with one "Motor" grade for automobiles. The gum test essentially eliminated thermally cracked gasoline from aviation and thus aviation gasolines reverted back to fractionating straight-run naphthas or blending straight-run and highly treated thermally cracked naphthas. This situation persisted until 1929. 

The automobile industry reacted to the increase in thermally cracked gasoline with alarm. Thermal cracking produced large amounts of both mono- and diolefins (unsaturated hydrocarbons) which increased the risk of gumming. Also the volatility was decreasing to the point that fuel did not vaporize and was sticking to spark plugs and fouling them, creating hard starting and rough running in winter and sticking to cylinder walls, bypassing the pistons and rings and going into the crankcase oil. One journal stated, "...on a multi-cylinder engine in a high-priced car we are diluting the oil in the crankcase as much as 40 percent in a 200-mile run, as the analysis of the oil in the oil-pan shows." Being very unhappy with the consequent reduction in overall gasoline quality the automobile manufacturers suggested imposing a quality standard on the oil suppliers. The oil industry accused the automakers of not doing enough to improve vehicle economy and this became known within the two industries as ‘The Fuel Problem’. Animosity grew between the industries, each accusing the other of not doing
anything to resolve matters and relationships deteriorated. The situation was resolved when the American Petroleum Institute (API)
initiated a conference to address ‘The Fuel Problem’ and a Cooperative Fuel Research (CFR) Committee was established in 1920 to oversee joint investigative programs and solutions. Apart from representatives of the two industries the Society of Automotive Engineers (SAE) also played an instrumental role with the American Bureau of Standards being chosen, as an impartial research organization, to carry out many of the studies. Initially all the programs were related to volatility and fuel consumption, ease of starting, crankcase oil dilution and acceleration.

As early as 1917-1918, researchers such as Gibson, Ricardo, Midgely and Boyd began to investigate abnormal combustion and this led to the discovery in the 1920s of antiknock compounds, the most important being that of Thomas Midgley Jr. and Boyd, specifically tetraethyllead (TEL). This innovation started a cycle of improvements in fuel efficiency that coincided with the large-scale development of oil refining to provide more products in the boiling range of gasoline.

With the increased use of thermally cracked gasolines came an increased concern over its effects on abnormal combustion and this led to research for antiknock additives. Beginning in 1916 Charles F. Kettering began investigating additives based on two paths, the "high percentage" solution where large quantities of ethanol were added and the "low percentage" solution which led to the discovery of tetraethyllead (TEL) in December, 1921 where only 2-4 grams per gallon were needed. Ethanol could not be patented but TEL could so Kettering secured a patent for TEL and began promoting it instead of other options. The dangers of lead were well established by then and Kettering was directly warned by Robert Wilson of MIT, Reid Hunt of Harvard,
Yandell Henderson of Yale, and Charles Kraus of the University of Pottsdam in Germany about its use. Kraus had worked on tetraethyl lead for many years and called it “a creeping and malicious poison” that had killed a member of his dissertation committee. On October 27, 1924 newspaper articles around the nation told of the workers at the Standard Oil refinery near Elizabeth, New Jersey who were producing TEL and were suffering from lead poisoning. By October 30, 1924 the death toll had reached five. In November the New Jersey Labor Commission closed the Bayway refinery and a grand jury investigation was started which resulted in no charges by February, 1925. Leaded gasoline sales were banned in New York City, Philadelphia, and New Jersey. GM, DuPont, and Standard Oil who were parteners in Ethyl, the company created to produce TEL, began to argue that there were no alternatives to leaded gasoline. After flawed studies determined that TEL treated gasoline was not a public health issue, the controversy subsided.

In the 5-year period prior to 1929, a great amount of experimentation was being conducted on different testing methods to determine fuel resistance to abnormal combustion. Engine 'knocking' was dependent on a wide variety of parameters including compression, cylinder temperature, air cooled or water cooled engines, chamber shapes, intake temperatures, lean or rich mixtures and more. This led to a confusing variety of test engines that gave conflicting results and no standard rating scale existed. By 1929 it was recognized by most aviation gasoline manufacturers and users that some kind of antiknock rating must be included in specifications. In 1929 the Octane rating scale was adopted and in 1930 the first octane specification for aviation fuels was established. In the same year the Army Air Force specified 87 octane for its aircraft as a result of studies it conducted.

During this period research showed that hydrocarbon structure was extremely important to antiknocking properties of fuel. Straight-chain paraffins in the boiling range of gasoline had low antiknock qualities while ring-shaped molecules such as aromatic hydrocarbons (an example is Benzene) had higher resistance to knocking. This development led to the search for processes that would produce more of these compounds from crude oils than achieved under straight distillation or thermal cracking. Research by the major refiners into conversion processes yielded isomerization, dehydration, and alkylation that would change the cheap and abundant butane into isooctane, which became an important component in aviation fuel blending. To further complicate the situation, as engine performance increased, the altitude that aircraft could reach increased which resulted in concerns about fuel freezing. The average temperature decrease is 3.6 degrees Fahrenheit per 1,000-ft increase in altitude and at 40,000-feet the temperature can approach -70 degrees. The presence of additives like benzene with a freezing point of 42 degrees would freeze in the gasoline and plug fuel lines. Substitute aromatics such as toluene, xylene, or cumene combined with limited benzene would solve the problem. 

By 1935 there were seven different aviation grades based on octane rating, two Army grades, four Navy grades and three commercial grades. By 1937 and the introduction of 100 octane gasoline, the confusion increased to 14 different grades in addition to 11 in foreign countries. With some companies required to stock 14 grades of aviation fuel, none of which could be interchanged, the effect on the refiners was negative. The refining industry could not concentrate on large capacity conversion processes for so many different grades and a solution had to be found. By 1941, principally through the efforts of the Cooperative Fuel Research Committee, the number of grades was reduced to three; 73, 91, and 100 octane. 

In 1937 Eugene Houdry developed the Houdry process of catalytic cracking which produced a high octane base stock of gasoline which was superior to the thermally cracked product since it did not contain the high concentration of olefins. In 1940 there were only 14 Houdry units in operation in the U.S. By 1943 this had increased to 77, either of the Houdry process or of the Thermofor Catalytic or Fluid Catalyst type.

Oil and its byproducts, especially high octane aviation gasoline would be a driving concern for how Germany conducted the war. As a result of the lessons of WW I, Germany had stockpiled oil and gasoline for its Blitzkrieg offensive and had annexed Austria adding 18,000 barrels per day of oil production but this was not sufficient to sustain the planned conquest of Europe. Because captured supplies and oil fields would be necessary to fuel the campaign, the German high command had created a special squad of oil-field experts drawn from the ranks of domestic oil industries. They would be sent in to put out oil-field fires and get production going again as soon as possible. But capturing oil fields would be an obstacle throughout the war. In the Invasion of Poland German estimates of gasoline consumption turned out to be vastly underestimated. Heinz Guderian and his Panzer divisions consumed 1,000 gallons of gasoline per mile on the drive to Vienna. But when they were engaged in combat across open country, gasoline consumption almost doubled. On the second day of battle, a unit of the XIX Corps was forced to halt when it ran out of gasoline.

In the 1950s oil refineries started to focus on high octane fuels, and then detergents were added to gasoline to clean the jets in carburetors. The 1970s witnessed greater attention to the environmental consequences of burning gasoline. These considerations led to the phasing out of TEL and its replacement by other antiknock compounds. Subsequently, low-sulfur gasoline was introduced, in part to preserve the catalysts in modern exhaust systems.

Spark ignition engines are designed to burn gasoline in a controlled process called deflagration. However, the unburned mixture may autoignite by detonating from pressure and heat alone, rather than ignite from the spark plug at exactly the right time. This causes a rapid pressure rise which can damage the engine. This is often referred to as engine knocking or end-gas knock. Knocking can be reduced by increasing the gasoline's resistance to autoignition, which is expressed by its octane rating.

Octane rating is measured relative to a mixture of 2,2,4-trimethylpentane (an isomer of octane) and n-heptane. There are different conventions for expressing octane ratings, so the same physical fuel may have several different octane ratings based on the measure used. One of the best known is the research octane number (RON).

The octane rating of typical commercially available gasoline varies by country. In Finland, Sweden, and Norway, 95 RON is the standard for regular unleaded gasoline and 98 RON is also available as a more expensive option. In the UK, ordinary regular unleaded gasoline is 95 RON (commonly available), premium unleaded gasoline is always 97 RON, and super unleaded is usually 97–98 RON. However, both Shell and BP produce fuel at 102 RON for cars with high-performance engines and in 2006 the supermarket chain Tesco began to sell super unleaded gasoline rated at 99 RON. In the US, octane ratings in unleaded fuels can vary between 85 and 87 AKI (91–92 RON) for regular, through 89–90 AKI (94–95 RON) for mid-grade (equivalent to European regular), up to 90–94 AKI (95–99 RON) for premium (European premium).

As South Africa's largest city, Johannesburg, is located on the Highveld at above sea level, the South African AA recommends 95 octane gasoline (petrol) at low altitude and 93 octane for use in Johannesburg because "The higher the altitude the lower the air pressure, and the lower the need for a high octane fuel as there is no real performance gain".

The octane rating became important as the military sought higher output for aircraft engines in the late 1930s and the 1940s. A higher octane rating allows a higher compression ratio or supercharger boost, and thus higher temperatures and pressures, which translate to higher power output. Some scientists even predicted that a nation with a good supply of high octane gasoline would have the advantage in air power. In 1943, the Rolls-Royce Merlin aero engine produced 1,320 horsepower (984 kW) using 100 RON fuel from a modest 27 liter displacement. By the time of Operation Overlord during World War II both the RAF and USAAF were conducting some operations in Europe using 150 RON fuel (100/150 avgas), obtained by adding 2.5% aniline to 100 octane avgas. By this time the Rolls-Royce Merlin 66 was developing 2,000 hp using this fuel.

Quality gasoline should be stable for six months if stored properly but gasoline will break down slowly over time due to the separation of the components. Gasoline stored for a year will most likely be able to be burned in an internal combustion engine without too much trouble but the effects of long term storage will become more noticeable with each passing month until a time comes when the gasoline should be diluted with ever-increasing amounts of freshly made fuel so that the older gasoline may be used up. If left undiluted, improper operation will occur and this may include engine damage from misfiring and/or the lack of proper action of the fuel within a fuel injection system and from an onboard computer attempting to compensate (if applicable to the vehicle). Storage should be in an airtight container (to prevent oxidation or water vapors mixing in with the gas) that can withstand the vapor pressure of the gasoline without venting (to prevent the loss of the more volatile fractions) at a stable cool temperature (to reduce the excess pressure from liquid expansion, and to reduce the rate of any decomposition reactions). When gasoline is not stored correctly, gums and solids may be created, which can corrode system components and accumulate on wetted surfaces, resulting in a condition called “stale fuel”. Gasoline containing ethanol is especially subject to absorbing atmospheric moisture, then forming gums, solids, or two phases (a hydrocarbon phase floating on top of a water-alcohol phase).

The presence of these degradation products in the fuel tank, fuel lines plus a carburetor or fuel injection components makes it harder to start the engine or causes reduced engine performance. On resumption of regular engine use, the buildup may or may not be eventually cleaned out by the flow of fresh gasoline. The addition of a fuel stabilizer to gasoline can extend the life of fuel that is not or cannot be stored properly though removal of all fuel from a fuel system is the only real solution to the problem of long term storage of an engine or a machine or vehicle. Some typical fuel stabilizers are proprietary mixtures containing mineral spirits, isopropyl alcohol, 1,2,4-trimethylbenzene, or other additives. Fuel stabilizer is commonly used for small engines, such as lawnmower and tractor engines, especially when their use is seasonal (low to no use for one or more seasons of the year). Users have been advised to keep gasoline containers more than half full and properly capped to reduce air exposure, to avoid storage at high temperatures, to run an engine for ten minutes to circulate the stabilizer through all components prior to storage, and to run the engine at intervals to purge stale fuel from the carburetor.

Gasoline stability requirements are set in standard ASTM D4814. The standard describes the various characteristics and requirements of automotive fuels for use over a wide range of operating conditions in ground vehicles equipped with spark-ignition engines.

A gasoline-fueled internal combustion engine obtains energy from combustion of gasoline's various hydrocarbons with oxygen from the ambient air, yielding carbon dioxide and water exhaust. The combustion of octane, a representative species, performs the chemical reaction:

<chem>2 C8H18 + 25 O2 -> 16 CO2 + 18 H2O
</chem>

Gasoline contains about 46.7 MJ/kg (127 MJ/US gal, 35.3 kWh/US gal, 13.0 kWh/kg, 120,405 BTU/US gal), quoting the lower heating value. Gasoline blends differ, and therefore actual energy content varies according to the season and producer by up to 1.75% more or less than the average. On average, about 74 L of gasoline (19.5 US gal, 16.3 imp gal) are available from a barrel of crude oil (about 46% by volume), varying due to quality of crude and grade of gasoline. The remainder are products ranging from tar to naphtha.

A high-octane-rated fuel, such as liquefied petroleum gas (LPG) has an overall lower power output at the typical 10:1 compression ratio of an engine design optimized for gasoline fuel. An engine tuned for LPG fuel via higher compression ratios (typically 12:1) improves the power output. This is because higher-octane fuels allow for a higher compression ratio without knocking, resulting in a higher cylinder temperature, which improves efficiency. Also, increased mechanical efficiency is created by a higher compression ratio through the concomitant higher expansion ratio on the power stroke, which is by far the greater effect. The higher expansion ratio extracts more work from the high-pressure gas created by the combustion process. An Atkinson cycle engine uses the timing of the valve events to produce the benefits of a high expansion ratio without the disadvantages, chiefly detonation, of a high compression ratio. A high expansion ratio is also one of the two key reasons for the efficiency of diesel engines, along with the elimination of pumping losses due to throttling of the intake air flow.

The lower energy content of LPG by liquid volume in comparison to gasoline is due mainly to its lower density. This lower density is a property of the lower molecular weight of propane (LPG's chief component) compared to gasoline's blend of various hydrocarbon compounds with heavier molecular weights than propane. Conversely, LPG energy content by weight is higher than gasoline due to a higher hydrogen to carbon ratio.

Molecular weights of the representative octane combustion are CH 114, O 32, CO 44, HO 18; therefore 1 kg of fuel reacts with 3.51 kg of oxygen to produce 3.09 kg of carbon dioxide and 1.42 kg of water.

The density of gasoline ranges from 0.71–0.77 kg/L ( ; 0.026 lb/in; 6.073 lb/US gal; 7.29 lb/imp gal), higher densities having a greater volume of aromatics. Since gasoline floats on water, water cannot generally be used to extinguish a gasoline fire unless used in a fine mist.
Finished marketable gasoline is traded with a standard reference of 0.755 kg/L, and its price is escalated/de-escalated according to its actual density.

Gasoline is produced in oil refineries. Roughly 19 US gallons (72 L) of gasoline is derived from a 42-gallon (159 L) barrel of crude oil. Material separated from crude oil via distillation, called virgin or straight-run gasoline, does not meet specifications for modern engines (particularly the octane rating, see below), but can be pooled to the gasoline blend.

The bulk of a typical gasoline consists of hydrocarbons with between 4 and 12 carbon atoms per molecule (commonly referred to as C4-C12). It is a mixture of paraffins (alkanes), cycloalkanes (naphthenes), and olefins (alkenes), where the usage of the terms paraffin and olefin is particular to the oil industry. The actual ratio depends on:

The various refinery streams blended to make gasoline have different characteristics. Some important streams are:

The terms above are the jargon used in the oil industry and terminology varies.

Currently, many countries set limits on gasoline aromatics in general, benzene in particular, and olefin (alkene) content. Such regulations led to increasing preference for high octane pure paraffin (alkane) components, such as alkylate, and is forcing refineries to add processing units to reduce benzene content. In the EU the benzene limit is set at 1% volume for all grade of automotive gasoline.

Gasoline can also contain other organic compounds, such as organic ethers (deliberately added), plus small levels of contaminants, in particular organosulfur compounds, but these are usually removed at the refinery.

Almost all countries in the world have phased out automotive leaded fuel. In 2011 six countries were still using leaded gasoline: Afghanistan, Myanmar, North Korea, Algeria, Iraq and Yemen. It was expected that by the end of 2013 those countries would ban leaded gasoline, but it has not occurred. Algeria will replace leaded with unleaded automotive fuel only in 2015.
Different additives have replaced the lead compounds. The most popular additives include aromatic hydrocarbons, ethers and alcohol (usually ethanol or methanol).
For technical reasons the use of leaded additives is still permitted worldwide for the formulation of some grades of aviation gasoline such as 100LL, because the required octane rating would be technically infeasible to reach without the use of leaded additives.
Gasoline, when used in high-compression internal combustion engines, tends to autoignite ("detonate") causing damaging "engine knocking" (also called "pinging" or "pinking"). To address this problem, tetraethyllead (TEL) was widely adopted as an additive for gasoline in the 1920s. With the discovery of the extent of environmental and health damage caused by the lead, however, and the incompatibility of lead with catalytic converters, leaded gasoline was phased out in the USA beginning in 1973. By 1995, leaded fuel accounted for only 0.6% of total gasoline sales and under 2000 short tons (1814 t) of lead per year in the USA. From 1 January 1996, the U.S. Clean Air Act banned the sale of leaded fuel for use in on-road vehicles in the USA. The use of TEL also necessitated other additives, such as dibromoethane.
First European countries started replacing lead by the end of the 1980s and by the end of the 1990s leaded gasoline was banned within the entire European Union. Reduction in the average blood lead level is believed to have been a major cause for falling violent crime rates in the United States and South Africa. A statistically significant correlation has been found between the usage rate of leaded gasoline and violent crime: taking into account a 22-year time lag, the violent crime curve virtually tracks the lead exposure curve.

Lead replacement petrol (LRP) was developed for vehicles designed to run on leaded fuel and incompatible with unleaded. Rather than tetraethyl lead it contains other metals such as potassium compounds or methylcyclopentadienyl manganese tricarbonyl (MMT); these are purported to buffer soft exhaust valves and seats so that they do not suffer recession due to the use of unleaded fuel.

LRP was marketed during and after the phaseout of leaded motor fuels in the United Kingdom, Australia, South Africa and some other countries. Consumer confusion led to widespread mistaken preference for LRP rather than unleaded, and LRP was phased out 8 to 10 years after the introduction of unleaded.

Leaded gasoline was withdrawn from sale in Britain after 31 December 1999, seven years after EEC regulations signalled the end of production for cars using leaded gasoline in member states. At this stage, a large percentage of cars from the 1980s and early 1990s which ran on leaded gasoline were still in use, along with cars which could run on unleaded fuel. However, the declining number of such cars on British roads saw many gasoline stations withdrawing LRP from sale by 2003.

Methylcyclopentadienyl manganese tricarbonyl (MMT) is used in Canada and in Australia to boost octane. It also helps old cars designed for leaded fuel run on unleaded fuel without need for additives to prevent valve problems. Its use in the US has been restricted by regulations. Its use in the EU is restricted by Article 8a of the Fuel Quality Directive following its testing under the Protocol for the evaluation of effects of metallic fuel-additives on the emissions performance of vehicles.

Gummy, sticky resin deposits result from oxidative degradation of gasoline upon long term storage. These harmful deposits arise from the oxidation of alkenes and other minor components in gasoline (see drying oils). Improvements in refinery techniques have generally reduced the susceptibility of gasolines to these problems. Previously, catalytically or thermally cracked gasolines are most susceptible to oxidation. The formation of these gums is accelerated by copper salts, which can be neutralized by additives called metal deactivators.

This degradation can be prevented through the addition of 5–100 ppm of antioxidants, such as phenylenediamines and other amines. Hydrocarbons with a bromine number of 10 or above can be protected with the combination of unhindered or partially hindered phenols and oil-soluble strong amine bases, such as hindered phenols. "Stale" gasoline can be detected by a colorimetric enzymatic test for organic peroxides produced by oxidation of the gasoline.

Gasolines are also treated with metal deactivators, which are compounds that sequester (deactivate) metal salts that otherwise accelerate the formation of gummy residues. The metal impurities might arise from the engine itself or as contaminants in the fuel.

Gasoline, as delivered at the pump, also contains additives to reduce internal engine carbon buildups, improve combustion, and to allow easier starting in cold climates. High levels of detergent can be found in Top Tier Detergent Gasolines. The specification for Top Tier Detergent gasolines was developed by four automakers: GM, Honda, Toyota and BMW. According to the bulletin, the minimal EPA requirement is not sufficient to keep engines clean. Typical detergents include alkylamines and alkyl phosphates at the level of 50–100 ppm.

In the EU, 5% ethanol can be added within the common gasoline spec (EN 228). Discussions are ongoing to allow 10% blending of ethanol (available in Finnish, French and German gas stations). In Finland most gasoline stations sell 95E10, which is 10% of ethanol; and 98E5, which is 5% ethanol. Most gasoline sold in Sweden has 5–15% ethanol added.

In Brazil, the Brazilian National Agency of Petroleum, Natural Gas and Biofuels (ANP) requires gasoline for automobile use to have 27.5% of ethanol added to its composition. Pure hydrated ethanol is also available as a fuel.

Legislation requires retailers to label fuels containing ethanol on the dispenser, and limits ethanol use to 10% of gasoline in Australia. Such gasoline is commonly called E10 by major brands, and it is cheaper than regular unleaded gasoline.

The federal Renewable Fuel Standard (RFS) effectively requires refiners and blenders to blend renewable biofuels (mostly ethanol) with gasoline, sufficient to meet a growing annual target of total gallons blended. Although the mandate does not require a specific percentage of ethanol, annual increases in the target combined with declining gasoline consumption has caused the typical ethanol content in gasoline to approach 10%. Most fuel pumps display a sticker that states that the fuel may contain up to 10% ethanol, an intentional disparity that reflects the varying actual percentage. Until late 2010, fuels retailers were only authorized to sell fuel containing up to 10 percent ethanol (E10), and most vehicle warranties (except for flexible fuel vehicles) authorize fuels that contain no more than 10 percent ethanol. In parts of the United States, ethanol is sometimes added to gasoline without an indication that it is a component.

The Government of India in October 2007 decided to make 5% ethanol blending (with gasoline) mandatory. Currently, 10% Ethanol blended product (E10) is being sold in various parts of the country.

Ethanol has been found in at least one study to damage catalytic converters.

In Australia, the lowest grade of gasoline (RON 91) is dyed a light shade of red/orange and the medium grade (RON 95) is dyed yellow.

In the United States, aviation gasoline (avgas) is dyed to identify its octane rating and to distinguish it from kerosene-based jet fuel, which is clear.

In Canada the gasoline for marine and farm use is dyed red and is not subject to sales tax. 

Oxygenate blending adds oxygen-bearing compounds such as MTBE, ETBE, ethanol, and biobutanol. The presence of these oxygenates reduces the amount of carbon monoxide and unburned fuel in the exhaust gas. In many areas throughout the US, oxygenate blending is mandated by EPA regulations to reduce smog and other airborne pollutants. For example, in Southern California, fuel must contain 2% oxygen by weight, resulting in a mixture of 5.6% ethanol in gasoline. The resulting fuel is often known as reformulated gasoline (RFG) or oxygenated gasoline, or in the case of California, California reformulated gasoline. The federal requirement that RFG contain oxygen was dropped on 6 May 2006 because the industry had developed VOC-controlled RFG that did not need additional oxygen.

MTBE was phased out in the US due to ground water contamination and the resulting regulations and lawsuits. Ethanol and, to a lesser extent, the ethanol-derived ETBE are common replacements. A common ethanol-gasoline mix of 10% ethanol mixed with gasoline is called gasohol or E10, and an ethanol-gasoline mix of 85% ethanol mixed with gasoline is called E85. The most extensive use of ethanol takes place in Brazil, where the ethanol is derived from sugarcane. In 2004, over 3.4 billion US gallons (2.8 billion imp gal/13 million m³) of ethanol was produced in the United States for fuel use, mostly from corn, and E85 is slowly becoming available in much of the United States, though many of the relatively few stations vending E85 are not open to the general public.

The use of bioethanol, either directly or indirectly by conversion of such ethanol to bio-ETBE, is encouraged by the European Union Directive on the Promotion of the use of biofuels and other renewable fuels for transport. Since producing bioethanol from fermented sugars and starches involves distillation, though, ordinary people in much of Europe cannot legally ferment and distill their own bioethanol at present (unlike in the US, where getting a BATF distillation permit has been easy since the 1973 oil crisis).

Combustion of of gasoline produces of carbon dioxide (2.3 kg/l), a greenhouse gas.

The main concern with gasoline on the environment, aside from the complications of its extraction and refining, is the effect on the climate through the production of carbon dioxide . Unburnt gasoline and evaporation from the tank, when in the atmosphere, reacts in sunlight to produce photochemical smog. Vapor pressure initially rises with some addition of ethanol to gasoline, but the increase is greatest at 10% by volume. At higher concentrations of ethanol above 10%, the vapor pressure of the blend starts to decrease. At a 10% ethanol by volume, the rise in vapor pressure may potentially increase the problem of photochemical smog. This rise in vapor pressure could be mitigated by increasing or decreasing the percentage of ethanol in the gasoline mixture.

The chief risks of such leaks come not from vehicles, but from gasoline delivery truck accidents and leaks from storage tanks. Because of this risk, most (underground) storage tanks now have extensive measures in place to detect and prevent any such leaks, such as monitoring systems (Veeder-Root, Franklin Fueling).

Production of gasoline consumes 0.63 gallons of water per mile driven.

The safety data sheet for unleaded gasoline shows at least 15 hazardous chemicals occurring in various amounts, including benzene (up to 5% by volume), toluene (up to 35% by volume), naphthalene (up to 1% by volume), trimethylbenzene (up to 7% by volume), methyl "tert"-butyl ether (MTBE) (up to 18% by volume, in some states) and about ten others. Hydrocarbons in gasoline generally exhibit low acute toxicities, with LD50 of 700–2700 mg/kg for simple aromatic compounds. Benzene and many antiknocking additives are carcinogenic.

People can be exposed to gasoline in the workplace by swallowing it, breathing in vapors, skin contact, and eye contact. The National Institute for Occupational Safety and Health (NIOSH) has designated gasoline as a carcinogen.

Inhaled (huffed) gasoline vapor is a common intoxicant. Users concentrate and inhale gasoline vapour in a manner not intended by the manufacturer to produce euphoria and intoxication. Gasoline inhalation has become epidemic in some poorer communities and indigenous groups in Australia, Canada, New Zealand, and some Pacific Islands. The practice is thought to cause severe organ damage, including mental retardation.

In Canada, Native children in the isolated Northern Labrador community of Davis Inlet were the focus of national concern in 1993, when many were found to be sniffing gasoline. The Canadian and provincial Newfoundland and Labrador governments intervened on a number of occasions, sending many children away for treatment. Despite being moved to the new community of Natuashish in 2002, serious inhalant abuse problems have continued. Similar problems were reported in Sheshatshiu in 2000 and also in Pikangikum First Nation. In 2012, the issue once again made the news media in Canada.
Australia has long faced a petrol (gasoline) sniffing problem in isolated and impoverished aboriginal communities. Although some sources argue that sniffing was introduced by United States servicemen stationed in the nation's Top End during World War II or through experimentation by 1940s-era Cobourg Peninsula sawmill workers, other sources claim that inhalant abuse (such as glue inhalation) emerged in Australia in the late 1960s. Chronic, heavy petrol sniffing appears to occur among remote, impoverished indigenous communities, where the ready accessibility of petrol has helped to make it a common substance for abuse.

In Australia, petrol sniffing now occurs widely throughout remote Aboriginal communities in the Northern Territory, Western Australia, northern parts of South Australia and Queensland. The number of people sniffing petrol goes up and down over time as young people experiment or sniff occasionally. "Boss", or chronic, sniffers may move in and out of communities; they are often responsible for encouraging young people to take it up. In 2005, the Government of Australia and BP Australia began the usage of opal fuel in remote areas prone to petrol sniffing. Opal is a non-sniffable fuel (which is much less likely to cause a high) and has made a difference in some indigenous communities.

Like other hydrocarbons, gasoline burns in a limited range of its vapor phase and, coupled with its volatility, this makes leaks highly dangerous when sources of ignition are present. Gasoline has a lower explosive limit of 1.4% by volume and an upper explosive limit of 7.6%. If the concentration is below 1.4%, the air-gasoline mixture is too lean and does not ignite. If the concentration is above 7.6%, the mixture is too rich and also does not ignite. However, gasoline vapor rapidly mixes and spreads with air, making unconstrained gasoline quickly flammable.

The United States accounts for about 44% of the world’s gasoline consumption. In 2003, the United States consumed , which equates to of gasoline each day. The United States used about of gasoline in 2006, of which 5.6% was mid-grade and 9.5% was premium grade.

Countries in Europe impose substantially higher taxes on fuels such as gasoline, when compared to the USA. The price of gasoline in Europe is typically higher than that in the US due to this difference.

From 1998 to 2004, the price of gasoline fluctuated between US$1 and US$2 per U.S. gallon. After 2004, the price increased until the average gas price reached a high of $4.11 per U.S. gallon in mid-2008, but receded to approximately $2.60 per U.S. gallon by September 2009. More recently, the U.S. experienced an upswing in gasoline prices through 2011, and by 1 March 2012, the national average was $3.74 per gallon.

In the United States, most consumer goods bear pre-tax prices, but gasoline prices are posted with taxes included. Taxes are added by federal, state, and local governments. As of 2009, the federal tax is 18.4¢ per gallon for gasoline and 24.4¢ per gallon for diesel (excluding red diesel). Among states, the highest gasoline tax rates, including the federal taxes as of 2005, are New York (62.9¢/gal), Hawaii (60.1¢/gal), and California (60¢/gal).

About 9% of all gasoline sold in the US in May 2009 was premium grade, according to the Energy Information Administration. "Consumer Reports" magazine says, "If [your owner’s manual] says to use regular fuel, do so—there's no advantage to a higher grade." The Associated Press said premium gas—which is a higher octane and costs more per gallon than regular unleaded—should be used only if the manufacturer says it is "required". Cars with turbocharged engines and high compression ratios often specify premium gas because higher octane fuels reduce the incidence of "knock", or fuel pre-detonation. The price of gas varies during the summer and winter months.

About 19.64 pounds (8.91 kg) of carbon dioxide (CO2) are produced from burning a (US) gallon (3.78l) of gasoline that does not contain ethanol (2.36 kg/l). About 22.38 pounds (10.15 kg) of CO2 are produced from burning a (US) gallon (3.78l) of diesel fuel (2.69 kg/l).

The US EIA estimates that U.S. motor gasoline and diesel (distillate) fuel consumption for transportation in 2015 resulted in the emission of about 1,105 million metric tons of CO2 and 440 million metric tons of CO2, respectively, for a total of 1,545 million metric tons of CO2. This total was equivalent to 83% of total U.S. transportation sector CO2 emissions and equivalent to 29% of total U.S. energy-related CO2 emissions in 2015.

Most of the retail gasoline now sold in the United States contains about 10% fuel ethanol (or E10) by volume. Burning a gallon of E10 produces about 17.68 pounds of CO2 that is emitted from the fossil fuel content. If the CO2 emissions from ethanol combustion are considered, then about 18.95 pounds of CO2 are produced when a gallon of E10 is combusted. About 12.73 pounds of CO2 are produced when a gallon of pure ethanol is combusted.

Volumetric and mass energy density of some fuels compared with gasoline (in the rows with gross and net, they are from):


Images


</doc>
<doc id="23640" url="https://en.wikipedia.org/wiki?curid=23640" title="Pentose">
Pentose

A pentose is a monosaccharide with five carbon atoms. Pentoses are organized into two groups: Aldopentoses have an aldehyde functional group at position 1. Ketopentoses have a ketone functional group at position 2 or 3. In the cell, pentoses have a higher metabolic stability than hexoses.

The aldopentoses have three chiral centers; therefore, eight (2) different stereoisomers are possible.
Ribose is a constituent of RNA, and the related molecule, deoxyribose, is a constituent of DNA. Phosphorylated pentoses are important products of the pentose phosphate pathway, most importantly ribose 5-phosphate (R5P), which is used in the synthesis of nucleotides and nucleic acids, and erythrose 4-phosphate (E4P), which is used in the synthesis of aromatic amino acids. 

The 2-ketopentoses have two chiral centers; therefore, four (2) different stereoisomers are possible. The 3-ketopentoses are rare.

The one deoxypentose has two steroisomers, for two total steroisomers.
The aldehyde and ketone functional groups in these carbohydrates react with neighbouring hydroxyl functional groups to form intramolecular hemiacetals and hemiketals, respectively. The resulting ring structure is related to furan, and is termed a furanose. The ring spontaneously opens and closes, allowing rotation to occur about the bond between the carbonyl group and the neighbouring carbon atom — yielding two distinct configurations (α and β). This process is termed mutarotation.

A polymer composed of pentose sugars is called a pentosan.

The most important tests for pentoses rely on converting the pentose to furfural, which then reacts with a chromophore. In Tollens’ test for pentoses (not to be confused with Tollens' silver-mirror test for reducing sugars) the furfural ring reacts with phloroglucinol to produce a colored compound; in the aniline acetate test with aniline acetate; and in Bial's test, with orcinol. In each of these tests, pentoses react much more strongly and quickly than hexoses.


</doc>
<doc id="23643" url="https://en.wikipedia.org/wiki?curid=23643" title="Propane">
Propane

Propane () is a three-carbon alkane with the molecular formula CH. It is a gas at standard temperature and pressure, but compressible to a transportable liquid. A by-product of natural gas processing and petroleum refining, it is commonly used as a fuel. Propane is one of a group of liquefied petroleum gases (LP gases). The others include butane, propylene, butadiene, butylene, isobutylene, and mixtures thereof.

Propane was discovered by the French chemist Marcellin Berthelot in 1857. It was first identified as a volatile component in gasoline by Walter O. Snelling of the U.S. Bureau of Mines in 1910. Although the compound was known long before this, Snelling's work was the beginning of the propane industry in the United States. The volatility of these lighter hydrocarbons caused them to be known as "wild" because of the high vapor pressures of unrefined gasoline. On March 31, 1912, "The New York Times" reported on Snelling's work with liquefied gas, saying "a steel bottle will carry enough gas to light an ordinary home for three weeks".

It was during this time that Snelling, in cooperation with Frank P. Peterson, Chester Kerr, and Arthur Kerr, created ways to liquefy the LP gases during the refining of natural gasoline. Together, they established American Gasol Co., the first commercial marketer of propane. Snelling had produced relatively pure propane by 1911, and on March 25, 1913, his method of processing and producing LP gases was issued patent #1,056,845. A separate method of producing LP gas through compression was created by Frank Peterson and its patent granted on July 2, 1912.

The 1920s saw increased production of LP gas, with the first year of recorded production totaling in 1922. In 1927, annual marketed LP gas production reached , and by 1935, the annual sales of LP gas had reached . Major industry developments in the 1930s included the introduction of railroad tank car transport, gas odorization, and the construction of local bottle-filling plants. The year 1945 marked the first year that annual LP gas sales reached a billion gallons. By 1947, 62% of all U.S. homes had been equipped with either natural gas or propane for cooking.

In 1950, 1,000 propane-fueled buses were ordered by the Chicago Transit Authority, and by 1958, sales in the U.S. had reached annually. In 2004, it was reported to be a growing $8-billion to $10-billion industry with over of propane being used annually in the U.S.

The "prop-" root found in "propane" and names of other compounds with three-carbon chains was derived from "propionic acid", which in turn was named after the Greek words protos (meaning first) and pion (fat).

Propane is produced as a by-product of two other processes, natural gas processing and petroleum refining. The processing of natural gas involves removal of butane, propane, and large amounts of ethane from the raw gas, in order to prevent condensation of these volatiles in natural gas pipelines. Additionally, oil refineries produce some propane as a by-product of cracking petroleum into gasoline or heating oil.

The supply of propane cannot easily be adjusted to meet increased demand, because of the by-product nature of propane production. About 90% of U.S. propane is domestically produced. The United States imports about 10% of the propane consumed each year, with about 70% of that coming from Canada via pipeline and rail. The remaining 30% of imported propane comes to the United States from other sources via ocean transport.

After it is produced, North American propane is stored in huge salt caverns. Examples of these are Fort Saskatchewan, Alberta; Mont Belvieu, Texas; and Conway, Kansas. These salt caverns were hollowed out in the 1940s, and they can store or more of propane. When the propane is needed, much of it is shipped by pipelines to other areas of the United States. Propane is also shipped by truck, ship, barge, and railway to many U.S. areas.

Propane can also be produced as a biofuel.

Propane undergoes combustion reactions in a similar fashion to other alkanes. In the presence of excess oxygen, propane burns to form water and carbon dioxide.<chem display="block">C3H8 + 5O2 -> 3CO2 + 4H2O + heat </chem> When not enough oxygen or too much oxygen is present for complete combustion, incomplete combustion occurs, allowing carbon monoxide and/or soot (carbon) to be formed as well:
<chem display="block">2C3H8 + 9O2 -> 4CO2 + 2CO + 8H2O + heat </chem><chem display="block">C3H8 + 2O2 -> 3C + 4H2O + heat </chem> Complete combustion of propane produces about 50 MJ/kg of heat.

Propane combustion is much cleaner than gasoline combustion, though not as clean as natural gas combustion. The presence of C–C bonds, plus the multiple bonds of propylene and butylene, create organic exhausts besides carbon dioxide and water vapor during typical combustion. These bonds also cause propane to burn with a visible flame.

The enthalpy of combustion of propane gas where all products return to standard state, for example where water returns to its liquid state at standard temperature (known as higher heating value), is (2219.2 ± 0.5) kJ/mol, or (50.33 ± 0.01) MJ/kg.
The enthalpy of combustion of propane gas where products do not return to standard state, for example where the hot gases including water vapor exit a chimney, (known as lower heating value) is −2043.455 kJ/mol. The lower heat value is the amount of heat available from burning the substance where the combustion products are vented to the atmosphere. For example, the heat from a fireplace when the flue is open.

The density of liquid propane at 25 °C (77 °F) is 0.493 g/cm, which is equivalent to 4.11 pounds per U.S. liquid gallon or 493 g/L. Propane expands at 1.5% per 10 °F. Thus, liquid propane has a density of approximately 4.2 pounds per gallon (504 g/L) at 60 °F (15.6 °C).

Propane is a popular choice for barbecues and portable stoves because the low boiling point of makes it vaporize as soon as it is released from its pressurized container. Therefore, no carburetor or other vaporizing device is required; a simple metering nozzle suffices. Propane powers some locomotives, buses, forklifts, taxis, outboard boat motors, and ice resurfacing machines and is used for heat and cooking in recreational vehicles and campers. Since it can be transported easily, it is a popular fuel for home heat and backup electrical generation in sparsely populated areas that do not have natural gas pipelines.
Propane is generally stored and transported in steel cylinders as a liquid with a vapor space above the liquid. The vapor pressure in the cylinder is a function of temperature. When gaseous propane is drawn at a high rate, the latent heat of vaporisation required to create the gas will cause the bottle to cool. (This is why water often condenses on the sides of the bottle and then freezes). In addition, the lightweight, high-octane compounds vaporize before the heavier, low-octane ones. Thus, the ignition properties change as the cylinder empties. For these reasons, the liquid is often withdrawn using a dip tube. Propane is used as fuel in furnaces for heat, in cooking, as an energy source for water heaters, laundry dryers, barbecues, portable stoves, and motor vehicles.

Commercially available "propane" fuel, or LPG, is not pure. Typically in the United States and Canada, it is primarily propane (at least 90%), with the rest mostly ethane, propylene, butane, and odorants including ethyl mercaptan. This is the HD-5 standard, (Heavy Duty-5% maximum allowable propylene content, and no more than 5% butanes and ethane) defined by the American Society for Testing and Materials by its Standard 1835 for internal combustion engines. Not all products labeled "LPG" conform to this standard however. In Mexico, for example, gas labeled "LPG" may consist of 60% propane and 40% butane. "The exact proportion of this combination varies by country, depending on international prices, on the availability of components and, especially, on the climatic conditions that favor LPG with higher butane content in warmer regions and propane in cold areas".

Propane use is growing rapidly in non-industrialized areas of the world. Propane has replaced many older other traditional fuel sources.

North American industries using propane include glass makers, brick kilns, poultry farms and other industries that need portable heat.

In rural areas of North America, as well as northern Australia, propane is used to heat livestock facilities, in grain dryers, and other heat-producing appliances. When used for heating or grain drying it is usually stored in a large, permanently-placed cylinder which is recharged by a propane-delivery truck. , 6.2 million American households use propane as their primary heating fuel.

In North America, local delivery trucks with an average cylinder size of , fill up large cylinders that are permanently installed on the property, or other service trucks exchange empty cylinders of propane with filled cylinders. Large tractor-trailer trucks, with an average cylinder size of , transport the propane from the pipeline or refinery to the local bulk plant. The bobtail and transport are not unique to the North American market, though the practice is not as common elsewhere, and the vehicles are generally called "tankers". In many countries, propane is delivered to consumers via small or medium-sized individual cylinders, while empty cylinders are removed for refilling at a central location.

Propene (also called propylene) can be a contaminant of commercial propane. Propane containing too much propene is not suited for most vehicle fuels. HD-5 is a specification that establishes a maximum concentration of 5% propene in propane. Propane and other LP gas specifications are established in ASTM D-1835. All propane fuels include an odorant, almost always ethanethiol, so that people can easily smell the gas in case of a leak. Propane as HD-5 was originally intended for use as vehicle fuel. HD-5 is currently being used in all propane applications.

Propane is also instrumental in providing off-the-grid refrigeration, as the energy source for a gas absorption refrigerator and is commonly used for camping and recreational vehicles.
In addition, blends of pure, dry "isopropane" (R-290a) (isobutane/propane mixtures) and isobutane (R-600a) can be used as the circulating refrigerant in suitably constructed compressor-based refrigeration. Compared to fluorocarbons, propane has a negligible ozone depletion potential and very low global warming potential (having a value of only 3.3 times the GWP of carbon dioxide) and can serve as a functional replacement for R-12, R-22, R-134a, and other chlorofluorocarbon or hydrofluorocarbon refrigerants in conventional stationary refrigeration and air conditioning systems. Because its global warming effect is far less than current refrigerants, propane was chosen as one of five replacement refrigerants approved by the EPA in 2015, for use in systems specially designed to handle its flammability.

Such substitution is widely prohibited or discouraged in motor vehicle air conditioning systems, on the grounds that using flammable hydrocarbons in systems originally designed to carry non-flammable refrigerant presents a significant risk of fire or explosion.

Vendors and advocates of hydrocarbon refrigerants argue against such bans on the grounds that there have been very few such incidents relative to the number of vehicle air conditioning systems filled with hydrocarbons.

Propane is also being used increasingly for vehicle fuels. In the U.S., over 190,000 on-road vehicles use propane, and over 450,000 forklifts use it for power. It is the third most popular vehicle fuel in the world, behind gasoline and Diesel fuel. In other parts of the world, propane used in vehicles is known as autogas. In 2007, approximately 13 million vehicles worldwide use autogas.

The advantage of propane in cars is its liquid state at a moderate pressure. This allows fast refill times, affordable fuel cylinder construction, and price ranges typically just over half that of gasoline. Meanwhile, it is noticeably cleaner (both in handling, and in combustion), results in less engine wear (due to carbon deposits) without diluting engine oil (often extending oil-change intervals), and until recently was a relative bargain in North America. The octane rating of propane is relatively high at 110. In the United States the propane fueling infrastructure is the most developed of all alternative vehicle fuels. Many converted vehicles have provisions for topping off from "barbecue bottles". Purpose-built vehicles are often in commercially owned fleets, and have private fueling facilities. A further saving for propane fuel vehicle operators, especially in fleets, is that pilferage is much more difficult than with gasoline or diesel fuels.

Propane is also used as fuel for small engines, especially those used indoors or in areas with insufficient fresh air and ventilation to carry away the more toxic exhaust of an engine running on gasoline or Diesel fuel. More recently, there have been lawn care products like string trimmers, lawn mowers and leaf blowers intended for outdoor use, but fueled by propane in order to reduce air pollution.

Propane and propane cylinders have been used as improvised explosive devices in attacks and attempted attacks against schools and terrorist targets such as the Columbine High School massacre, 2012 Brindisi school bombing, the Discovery Communications headquarters hostage crisis and in car bombs.


Propane is a simple asphyxiant. Unlike natural gas, propane is denser than air. It may accumulate in low spaces and near the floor. When abused as an inhalant, it may cause hypoxia (lack of oxygen), pneumonia, cardiac failure or cardiac arrest. Propane has low toxicity since it is not readily absorbed and is not biologically active. Commonly stored under pressure at room temperature, propane and its mixtures will flash evaporate at atmospheric pressure and cool well below the freezing point of water. The cold gas, which appears white due to moisture condensing from the air, may cause frostbite.

Propane is denser than air. If a leak in a propane fuel system occurs, the gas will have a tendency to sink into any enclosed area and thus poses a risk of explosion and fire. The typical scenario is a leaking cylinder stored in a basement; the propane leak drifts across the floor to the pilot light on the furnace or water heater, and results in an explosion or fire. This property makes propane generally unsuitable as a fuel for boats.

One hazard associated with propane storage and transport is known as a BLEVE or boiling liquid expanding vapor explosion. The Kingman Explosion involved a railroad tank car in Kingman, Arizona in 1973 during a propane transfer. The fire and subsequent explosions resulted in twelve fatalities and numerous injuries.

Propane is bought and stored in a liquid form (LPG), and thus fuel energy can be stored in a relatively small space. Compressed natural gas (CNG), largely methane, is another gas used as fuel, but it cannot be liquefied by compression at normal temperatures, as these are well above its critical temperature. As a gas, very high pressure is required to store useful quantities. This poses the hazard that, in an accident, just as with any compressed gas cylinder (such as a CO cylinder used for a soda concession) a CNG cylinder may burst with great force, or leak rapidly enough to become a self-propelled missile. Therefore, CNG is much less efficient to store, due to the large cylinder volume required. An alternative means of storing natural gas is as a cryogenic liquid in an insulated container as liquefied natural gas (LNG). This form of storage is at low pressure and is around 3.5 times as efficient as storing it as CNG. Unlike propane, if a spill occurs, CNG will evaporate and dissipate harmlessly because it is lighter than air. Propane is much more commonly used to fuel vehicles than is natural gas, because the equipment required costs less. Propane requires just of pressure to keep it liquid at .
, the retail cost of propane was approximately $2.37 per gallon, or roughly $25.95 per 1 million BTUs. This means that filling a 500-gallon propane tank, which is what households that use propane as their main source of energy usually require, costs $948 (80% of 500 gallons or 400 gallons), a 7.5% increase on the 2012–2013 winter season average US price. However, propane costs per gallon change significantly from one state to another: the Energy Information Administration (EIA) quotes a $2.995 per gallon average on the East Coast for October 2013, while the figure for the Midwest was $1.860 for the same period.




</doc>
<doc id="23645" url="https://en.wikipedia.org/wiki?curid=23645" title="Precambrian">
Precambrian

The Precambrian (or Pre-Cambrian, sometimes abbreviated pЄ, or Cryptozoic) is the earliest part of Earth's history, set before the current Phanerozoic Eon. The Precambrian is so named because it preceded the Cambrian, the first period of the Phanerozoic eon, which is named after Cambria, the Latinised name for Wales, where rocks from this age were first studied. The Precambrian accounts for 88% of the Earth's geologic time.

The Precambrian (colored green in the timeline figure) is an informal unit of geologic time, subdivided into three eons (Hadean, Archean, Proterozoic) of the geologic time scale. It spans from the formation of Earth about 4.6 billion years ago (Ga) to the beginning of the Cambrian Period, about million years ago (Ma), when hard-shelled creatures first appeared in abundance. 

Relatively little is known about the Precambrian, despite it making up roughly seven-eighths of the Earth's history, and what is known has largely been discovered from the 1960s onwards. The Precambrian fossil record is poorer than that of the succeeding Phanerozoic, and fossils from the Precambrian (e.g. stromatolites) are of limited biostratigraphic use. This is because many Precambrian rocks have been heavily metamorphosed, obscuring their origins, while others have been destroyed by erosion, or remain deeply buried beneath Phanerozoic strata.

It is thought that the Earth coalesced from material in orbit around the Sun at roughly 4,543 Ma, and may have been struck by a very large (Mars-sized) planetesimal shortly after it formed, splitting off material that formed the Moon (see Giant impact hypothesis). A stable crust was apparently in place by 4,433 Ma, since zircon crystals from Western Australia have been dated at 4,404 ± 8 Ma.

The term "Precambrian" is recognized by the International Commission on Stratigraphy as the only "supereon" in geologic time; it is so called because it includes the Hadean (~4.6—4 billion), Archean (4—2.5 billion), and Proterozoic (2.5 billion—541 million) eons. (There is only one other eon: the Phanerozoic, 541 million-present.) "Precambrian" is still used by geologists and paleontologists for general discussions not requiring the more specific eon names. , the United States Geological Survey considers the term informal, lacking a stratigraphic rank.

A specific date for the origin of life has not been determined. Carbon found in 3.8 billion-year-old rocks (Archean eon) from islands off western Greenland may be of organic origin. Well-preserved microscopic fossils of bacteria older than 3.46 billion years have been found in Western Australia. Probable fossils 100 million years older have been found in the same area. However, there is evidence that live could have evolved over 4.280 billion years ago. There is a fairly solid record of bacterial life throughout the remainder (Proterozoic eon) of the Precambrian.

Excluding a few contested reports of much older forms from North America and India, the first complex multicellular life forms seem to have appeared at roughly 1500 Ma, in the Mesoproterozoic era of the Proterozoic eon. Fossil evidence from the later Ediacaran period of such complex life comes from the Lantian formation, at least 580 million years ago. A very diverse collection of soft-bodied forms is found in a variety of locations worldwide and date to between 635 and 542 Ma. These are referred to as Ediacaran or Vendian biota. Hard-shelled creatures appeared toward the end of that time span, marking the beginning of the Phanerozoic eon. By the middle of the following Cambrian period, a very diverse fauna is recorded in the Burgess Shale, including some which may represent stem groups of modern taxa. The increase in diversity of lifeforms during the early Cambrian is called the Cambrian explosion of life.

While land seems to have been devoid of plants and animals, cyanobacteria and other microbes formed prokaryotic mats that covered terrestrial areas.

Tracks from an animal with leg like appendages have been found in what was mud 551 million years ago.

Evidence of the details of plate motions and other tectonic activity in the Precambrian has been poorly preserved. It is generally believed that small proto-continents existed prior to 4280 Ma, and that most of the Earth's landmasses collected into a single supercontinent around 1130 Ma. The supercontinent, known as Rodinia, broke up around 750 Ma. A number of glacial periods have been identified going as far back as the Huronian epoch, roughly 2400–2100 Ma. One of the best studied is the Sturtian-Varangian glaciation, around 850–635 Ma, which may have brought glacial conditions all the way to the equator, resulting in a "Snowball Earth".

The atmosphere of the early Earth is not well understood. Most geologists believe it was composed primarily of nitrogen, carbon dioxide, and other relatively inert gases, and was lacking in free oxygen. There is, however, evidence that an oxygen-rich atmosphere existed since the early Archean.

At present, it is still believed that molecular oxygen was not a significant fraction of Earth's atmosphere until after photosynthetic life forms evolved and began to produce it in large quantities as a byproduct of their metabolism. This radical shift from a chemically inert to an oxidizing atmosphere caused an ecological crisis, sometimes called the oxygen catastrophe. At first, oxygen would have quickly combined with other elements in Earth's crust, primarily iron, removing it from the atmosphere. After the supply of oxidizable surfaces ran out, oxygen would have begun to accumulate in the atmosphere, and the modern high-oxygen atmosphere would have developed. Evidence for this lies in older rocks that contain massive banded iron formations that were laid down as iron oxides.

A terminology has evolved covering the early years of the Earth's existence, as radiometric dating has allowed real dates to be assigned to specific formations and features. The Precambrian is divided into three eons: the Hadean (– Ma), Archean (- Ma) and Proterozoic (- Ma). See Timetable of the Precambrian.
It has been proposed that the Precambrian should be divided into eons and eras that reflect stages of planetary evolution, rather than the current scheme based upon numerical ages. Such a system could rely on events in the stratigraphic record and be demarcated by GSSPs. The Precambrian could be divided into five "natural" eons, characterized as follows:

The movement of Earth's plates has caused the formation and break-up of continents over time, including occasional formation of a supercontinent containing most or all of the landmass. The earliest known supercontinent was Vaalbara. It formed from proto-continents and was a supercontinent 3.636 billion years ago. Vaalbara broke up c. 2.845–2.803 Ga ago. The supercontinent Kenorland was formed c. 2.72 Ga ago and then broke sometime after 2.45–2.1 Ga into the proto-continent cratons called Laurentia, Baltica, Yilgarn craton, and Kalahari. The supercontinent Columbia or Nuna formed 2.06–1.82 billion years ago and broke up about 1.5–1.35 billion years ago. The supercontinent Rodinia is thought to have formed about 1.13–1.071 billion years ago, to have embodied most or all of Earth's continents and to have broken up into eight continents around 750–600 million years ago.





</doc>
<doc id="23647" url="https://en.wikipedia.org/wiki?curid=23647" title="Polymerase chain reaction">
Polymerase chain reaction

Polymerase chain reaction (PCR) is a technique used in molecular biology to amplify a single copy or a few copies of a segment of DNA across several orders of magnitude, generating thousands to millions of copies of a particular DNA sequence. Developed in 1983 by Kary Mullis, who was an employee of the Cetus Corporation, and also the winner of Nobel Prize in Chemistry in 1993, it is an easy, cheap, and reliable way to repeatedly replicate a focused segment of DNA, a concept which is applicable to numerous fields in modern biology and related sciences. PCR is probably the most widely used technique in molecular biology. This technique is used in biomedical research, criminal forensics, and molecular archaeology.

PCR is now a common and often indispensable technique used in clinical and research laboratories for a broad variety of applications. These include DNA cloning for sequencing, gene cloning and manipulation, gene mutagenesis; construction of DNA-based phylogenies, or functional analysis of genes; diagnosis and monitoring of hereditary diseases; amplification of ancient DNA; analysis of genetic fingerprints for DNA profiling (for example, in forensic science and parentage testing); and detection of pathogens in nucleic acid tests for the diagnosis of infectious diseases. In 1993, Mullis was awarded the Nobel Prize in Chemistry along with Michael Smith for his work on PCR. 

The vast majority of PCR methods rely on thermal cycling, which involves exposing the reactants to cycles of repeated heating and cooling, permitting different temperature-dependent reactions—specifically, DNA melting and enzyme-driven DNA replication—to quickly proceed many times in sequence. Primers (short DNA fragments) containing sequences complementary to the target region, along with a DNA polymerase (e.g. Taq polymerase), after which the method is named, enable selective and repeated amplification. As PCR progresses, the DNA generated is itself used as a template for replication, setting in motion a chain reaction in which the original DNA template is exponentially amplified. The simplicity of the basic principle underlying PCR means it can be extensively modified to perform a wide array of genetic manipulations. PCR is not generally considered to be a recombinant DNA method, as it does not involve cutting and pasting DNA, only amplification of existing sequences.

Almost all PCR applications employ a heat-stable DNA polymerase, such as Taq polymerase, an enzyme originally isolated from the thermophilic bacterium "Thermus aquaticus". If heat-susceptible DNA polymerase is used, it will denature every cycle at the denaturation step. Before the use of Taq polymerase, DNA polymerase had to be manually added every cycle, which was a tedious and costly process. This DNA polymerase enzymatically assembles a new DNA strand from free nucleotides, the building blocks of DNA, by using single-stranded DNA as a template and DNA oligonucleotides (the primers mentioned above) to initiate DNA synthesis.

In the first step, the two strands of the DNA double helix are physically separated at a high temperature in a process called DNA melting. In the second step, the temperature is lowered and the two DNA strands become templates for DNA polymerase to selectively amplify the target DNA. The selectivity of PCR results from the use of primers that are complementary to sequence around the DNA region targeted for amplification under specific thermal cycling conditions.

PCR, like recombinant DNA technology, has had an enormous impact in both basic and diagnostic aspects of molecular biology because it can produce large amounts of a specific DNA fragment from small amounts of a complex template. Recombinant DNA techniques create molecular clones by conferring on a specific sequence the ability to replicate by inserting it into a vector and introducing the vector into a host cell. PCR represents a form of “"in vitro" cloning” that can generate, as well as modify, DNA fragments of defined length and sequence in a simple automated reaction. In addition to its many applications in basic molecular biological research, PCR promises to play a critical role in the identification of medically important sequences as well as an important diagnostic one in their detection.

PCR amplifies a specific region of a DNA strand (the DNA target). Most PCR methods amplify DNA fragments of between 0.1 and 10 kilo base pairs (kbp), although some techniques allow for amplification of fragments up to 40 kbp in size. The amount of amplified product is determined by the available substrates in the reaction, which become limiting as the reaction progresses.

A basic PCR set-up requires several components and reagents, including:

The reaction is commonly carried out in a volume of 10–200 μl in small reaction tubes (0.2–0.5 ml volumes) in a thermal cycler. The thermal cycler heats and cools the reaction tubes to achieve the temperatures required at each step of the reaction (see below). Many modern thermal cyclers make use of the Peltier effect, which permits both heating and cooling of the block holding the PCR tubes simply by reversing the electric current. Thin-walled reaction tubes permit favorable thermal conductivity to allow for rapid thermal equilibration. Most thermal cyclers have heated lids to prevent condensation at the top of the reaction tube. Older thermal cyclers lacking a heated lid require a layer of oil on top of the reaction mixture or a ball of wax inside the tube.

Typically, PCR consists of a series of 20–40 repeated temperature changes, called cycles, with each cycle commonly consisting of two or three discrete temperature steps (see figure below). The cycling is often preceded by a single temperature step at a very high temperature (>), and followed by one hold at the end for final product extension or brief storage. The temperatures used and the length of time they are applied in each cycle depend on a variety of parameters, including the enzyme used for DNA synthesis, the concentration of bivalent ions and dNTPs in the reaction, and the melting temperature ("Tm") of the primers. The individual steps common to most PCR methods are as follows:




To check whether the PCR successfully generated the anticipated DNA target region (also sometimes referred to as the amplimer or amplicon), agarose gel electrophoresis may be employed for size separation of the PCR products. The size(s) of PCR products is determined by comparison with a DNA ladder, a molecular weight marker which contains DNA fragments of known size run on the gel alongside the PCR products.
As with other chemical reactions, the reaction rate and efficiency of PCR are affected by limiting factors. Thus, the entire PCR process can further be divided into three stages based on reaction progress:

In practice, PCR can fail for various reasons, in part due to its sensitivity to contamination causing amplification of spurious DNA products. Because of this, a number of techniques and procedures have been developed for optimizing PCR conditions. Contamination with extraneous DNA is addressed with lab protocols and procedures that separate pre-PCR mixtures from potential DNA contaminants. This usually involves spatial separation of PCR-setup areas from areas for analysis or purification of PCR products, use of disposable plasticware, and thoroughly cleaning the work surface between reaction setups. Primer-design techniques are important in improving PCR product yield and in avoiding the formation of spurious products, and the usage of alternate buffer components or polymerase enzymes can help with amplification of long or otherwise problematic regions of DNA. Addition of reagents, such as formamide, in buffer systems may increase the specificity and yield of PCR. Computer simulations of theoretical PCR results (Electronic PCR) may be performed to assist in primer design.

PCR allows isolation of DNA fragments from genomic DNA by selective amplification of a specific region of DNA. This use of PCR augments many ways, such as generating hybridization probes for Southern or northern hybridization and DNA cloning, which require larger amounts of DNA, representing a specific DNA region. PCR supplies these techniques with high amounts of pure DNA, enabling analysis of DNA samples even from very small amounts of starting material.

Other applications of PCR include DNA sequencing to determine unknown PCR-amplified sequences in which one of the amplification primers may be used in Sanger sequencing, isolation of a DNA sequence to expedite recombinant DNA technologies involving the insertion of a DNA sequence into a plasmid, phage, or cosmid (depending on size) or the genetic material of another organism. Bacterial colonies "(such as E. coli)" can be rapidly screened by PCR for correct DNA vector constructs. PCR may also be used for genetic fingerprinting; a forensic technique used to identify a person or organism by comparing experimental DNAs through different PCR-based methods.

Some PCR 'fingerprints' methods have high discriminative power and can be used to identify genetic relationships between individuals, such as parent-child or between siblings, and are used in paternity testing (Fig. 4). This technique may also be used to determine evolutionary relationships among organisms when certain molecular clocks are used (i.e., the 16S rRNA and recA genes of microorganisms).

Because PCR amplifies the regions of DNA that it targets, PCR can be used to analyze extremely small amounts of sample. This is often critical for forensic analysis, when only a trace amount of DNA is available as evidence. PCR may also be used in the analysis of ancient DNA that is tens of thousands of years old. These PCR-based techniques have been successfully used on animals, such as a forty-thousand-year-old mammoth, and also on human DNA, in applications ranging from the analysis of Egyptian mummies to the identification of a Russian tsar and the body of English king Richard III.

Quantitative PCR or Real Time Quantitative PCR (RT-qPCR) methods allow the estimation of the amount of a given sequence present in a sample—a technique often applied to quantitatively determine levels of gene expression. Quantitative PCR is an established tool for DNA quantification that measures the accumulation of DNA product after each round of PCR amplification.

RT-qPCR allows the quantification and detection of a specific DNA sequence in real time since it measures concentration while the synthesis process is taking place. There are two methods for simultaneous detection and quantification. The first method consists of using fluorescent dyes that are retained nonspecifically in between the double strands. The second method involves probes that code for specific sequences and are fluorescently labeled. Detection of DNA using these methods can only be seen after the hybridization of probes with its complementary DNA takes place. An interesting technique combination is real-time PCR and reverse transcription. This sophisticated technique allows for the quantification of a small quantity of RNA. Through this combined technique, mRNA is converted to cDNA, which is further quantified using qPCR. This technique lowers the possibility of error at the end point of PCR, increasing chances for detection of genes associated with genetic diseases such as cancer. Laboratories use RT-qPCR for the purpose of sensitively measuring gene regulation.

. Prospective parents can be tested for being genetic carriers, or their children might be tested for actually being affected by a disease. DNA samples for prenatal testing can be obtained by amniocentesis, chorionic villus sampling, or even by the analysis of rare fetal cells circulating in the mother's bloodstream. PCR analysis is also essential to preimplantation genetic diagnosis, where individual cells of a developing embryo are tested for mutations.

PCR allows for rapid and highly specific diagnosis of infectious diseases, including those caused by bacteria or viruses. PCR also permits identification of non-cultivatable or slow-growing microorganisms such as mycobacteria, anaerobic bacteria, or viruses from tissue culture assays and animal models. The basis for PCR diagnostic applications in microbiology is the detection of infectious agents and the discrimination of non-pathogenic from pathogenic strains by virtue of specific genes.

Characterization and detection of infectious disease organisms have been revolutionized by PCR in the following ways:


The development of PCR-based genetic (or DNA) fingerprinting protocols has seen widespread application in forensics:


PCR has been applied to many areas of research in molecular genetics:


PCR has a number of advantages. It is fairly simple to understand and to use, and produces results rapidly. The technique is highly sensitive with the potential to produce millions to billions of copies of a specific product for sequencing, cloning, and analysis. qRT-PCR shares the same advantages as the PCR, with an added advantage of quantification of the synthesized product. Therefore, it has its uses to analyze alterations of gene expression levels in tumors, microbes, or other disease states.

PCR is a very powerful and practical research tool. The sequencing of unknown etiologies of many diseases are being figured out by the PCR. The technique can help identify the sequence of previously unknown viruses related to those already known and thus give us a better understanding of the disease itself. If the procedure can be further simplified and sensitive non radiometric detection systems can be developed, the PCR will assume a prominent place in the clinical laboratory for years to come.

One major limitation of PCR is that prior information about the target sequence is necessary in order to generate the primers that will allow its selective amplification.<ref name="10.1038/jid.2013.1"></ref> This means that, typically, PCR users must know the precise sequence(s) upstream of the target region on each of the two single-stranded templates in order to ensure that the DNA polymerase properly binds to the primer-template hybrids and subsequently generates the entire target region during DNA synthesis.

Like all enzymes, DNA polymerases are also prone to error, which in turn causes mutations in the PCR fragments that are generated.

Another limitation of PCR is that even the smallest amount of contaminating DNA can be amplified, resulting in misleading or ambiguous results. To minimize the chance of contamination, investigators should reserve separate rooms for reagent preparation, the PCR, and analysis of product. Reagents should be dispensed into single-use aliquots. Pipetters with disposable plungers and extra-long pipette tips should be routinely used.



A 1971 paper in the "Journal of Molecular Biology" by and co-workers in the laboratory of H. Gobind Khorana first described a method using an enzymatic assay to replicate a short DNA template with primers "in vitro". However, this early manifestation of the basic PCR principle did not receive much attention at the time, and the invention of the polymerase chain reaction in 1983 is generally credited to Kary Mullis.

When Mullis developed the PCR in 1983, he was working in Emeryville, California for Cetus Corporation, one of the first biotechnology companies. There, he was responsible for synthesizing short chains of DNA. Mullis has written that he conceived of PCR while cruising along the Pacific Coast Highway one night in his car. He was playing in his mind with a new way of analyzing changes (mutations) in DNA when he realized that he had instead invented a method of amplifying any DNA region through repeated cycles of duplication driven by DNA polymerase. In "Scientific American", Mullis summarized the procedure: "Beginning with a single molecule of the genetic material DNA, the PCR can generate 100 billion similar molecules in an afternoon. The reaction is easy to execute. It requires no more than a test tube, a few simple reagents, and a source of heat." DNA fingerprinting first become used for paternity testing in 1988.

Mullis was awarded the Nobel Prize in Chemistry in 1993 for his invention, seven years after he and his colleagues at Cetus first put his proposal to practice. Mullis’s 1985 paper with R. K. Saiki and H. A. Erlich, “Enzymatic Amplification of β-globin Genomic Sequences and Restriction Site Analysis for Diagnosis of Sickle Cell Anemia”—the polymerase chain reaction invention (PCR) -- was honored by a Citation for Chemical Breakthrough Award from the Division of History of Chemistry of the American Chemical Society in 2017.

Some controversies have remained about the intellectual and practical contributions of other scientists to Mullis' work, and whether he had been the sole inventor of the PCR principle (see below).

At the core of the PCR method is the use of a suitable DNA polymerase able to withstand the high temperatures of > required for separation of the two DNA strands in the DNA double helix after each replication cycle. The DNA polymerases initially employed for in vitro experiments presaging PCR were unable to withstand these high temperatures. So the early procedures for DNA replication were very inefficient and time-consuming, and required large amounts of DNA polymerase and continuous handling throughout the process.

The discovery in 1976 of Taq polymerase—a DNA polymerase purified from the thermophilic bacterium, "Thermus aquaticus", which naturally lives in hot () environments such as hot springs—paved the way for dramatic improvements of the PCR method. The DNA polymerase isolated from "T. aquaticus" is stable at high temperatures remaining active even after DNA denaturation, thus obviating the need to add new DNA polymerase after each cycle. This allowed an automated thermocycler-based process for DNA amplification.

The PCR technique was patented by Kary Mullis and assigned to Cetus Corporation, where Mullis worked when he invented the technique in 1983. The "Taq" polymerase enzyme was also covered by patents. There have been several high-profile lawsuits related to the technique, including an unsuccessful lawsuit brought by DuPont. The pharmaceutical company Hoffmann-La Roche purchased the rights to the patents in 1992 and currently holds those that are still protected.

A related patent battle over the Taq polymerase enzyme is still ongoing in several jurisdictions around the world between Roche and Promega. The legal arguments have extended beyond the lives of the original PCR and Taq polymerase patents, which expired on March 28, 2005.




</doc>
<doc id="23648" url="https://en.wikipedia.org/wiki?curid=23648" title="Polymerase">
Polymerase

A polymerase is an enzyme (EC 2.7.7.6/7/19/48/49) that synthesizes long chains of polymers or nucleic acids. DNA polymerase and RNA polymerase are used to assemble DNA and RNA molecules, respectively, by copying a DNA template strand using base-pairing interactions or RNA by half ladder replication.

A polymerase from the thermophilic bacterium, "Thermus aquaticus" ("Taq") (PDB 1BGX, EC 2.7.7.7) is used in the polymerase chain reaction, an important technique of molecular biology.

Other well-known polymerases include:




</doc>
<doc id="23649" url="https://en.wikipedia.org/wiki?curid=23649" title="Pacific Scandal">
Pacific Scandal

The Pacific Scandal was a political scandal in Canada involving bribes being accepted by 150 members of the Conservative government in the attempts of private interests to influence the bidding for a national rail contract. As part of British Columbia's 1871 agreement to join Canadian Confederation, the government had agreed to build a transcontinental railway linking the Pacific Province to the eastern provinces. 

The scandal led to the resignation of Canada's first Prime Minister, Sir John A. Macdonald, and a transfer of power from his Conservative government to a Liberal government led by Alexander Mackenzie. One of the new government's first measures was to introduce secret ballots in an effort to improve the integrity of future elections. After the scandal broke the railway plan collapsed and the proposed line was not built. An entirely different operation later built the Canadian Pacific Railway railway to the Pacific.
For a young and loosely defined nation, the building of a national railway was an active attempt at state-making, as well as an aggressive capitalist venture. Canada, a nascent country with a population of 3.5 million in 1871, lacked the means to exercise meaningful "de facto" control within the "de jure" political boundaries of the recently acquired Rupert's Land; building a transcontinental railway was national policy of high order to change this situation. Moreover, after the American Civil War the American frontier rapidly expanded west with land-hungry settlers, exacerbating talk of annexation. Indeed, sentiments of Manifest Destiny were abuzz in this time: in 1867, year of Confederation, US Secretary of State W. H. Seward surmised that the whole North American continent "shall be, sooner or later, within the magic circle of the American Union". Therefore, preventing American investment into the project was considered as being in Canada's national interest. Thus the federal government favoured an "all Canadian route" through the rugged Canadian Shield of northern Ontario, refusing to consider a less costly route passing south through Wisconsin and Minnesota.

However, a route across the Canadian Shield was highly unpopular with potential investors, not only in the United States but also in Canada and especially Great Britain, the only other viable source of financing. For would-be investors, the objections were not primarily based on politics or nationalism but economics. At the time, national governments lacked the finances needed to undertake such large projects. For the First Transcontinental Railroad, the United States government had made extensive grants of public land to the railway's builders, inducing private financiers to fund the railway on the understanding that they would acquire rich farmland along the route, which could then be sold for a large profit. However, the eastern terminus of the proposed Canadian Pacific route, unlike that of the First Transcontinental, was not in rich Nebraskan farmland, but deep within the Canadian Shield. Copying the American financing model whilst insisting on an all-Canadian route would require the railway's backers to build hundreds of miles of track across rugged shield terrain (with little economic value) at considerable expense before they could expect to access lucrative farmland in Manitoba and the newly created Northwest Territories. Many financiers, who had expected to make a relatively quick profit, were not willing to make this sort of long-term commitment.

Nevertheless, the Montreal capitalist Hugh Allan, with his syndicate Canada Pacific Railway Company, sought the potentially lucrative charter for the project. The problem lay in that Allan and Macdonald highly, and secretly, were in cahoots with American financiers such as George W. McMullen and Jay Cooke, men who were deeply interested in the rival American undertaking, the Northern Pacific Railroad.

Two groups competed for the contract to build the railway, Hugh Allan's Canada Pacific Railway Company and David Lewis Macpherson's Inter-Oceanic Railway Company. On April 2, 1873, Lucius Seth Huntington, a Liberal Member of Parliament, created an uproar in the House of Commons. He announced he had uncovered evidence that Allan and his associates had been granted the Canadian Pacific Railway contract in return for political donations of $360,000.

In 1873, it became known that Allan had contributed a large sum of money to the Conservative government's re-election campaign of 1872; some sources quote a sum over $360,000. Allan had promised to keep American capital out of the railway deal, but had lied to Macdonald over this vital point, and Macdonald later discovered the lie. The Liberal party, at this time the opposition party in Parliament, accused the Conservatives of having made a tacit agreement to give the contract to Hugh Allan in exchange for money.

In making such allegations, the Liberals and their allies in the press (in particular, George Brown's newspaper the Globe) presumed that most of the money had been used to bribe voters in the 1872 election. The secret ballot, then considered a novelty, had not yet been introduced in Canada. Although it was illegal to offer, solicit or accept bribes in exchange for votes, effective enforcement of this prohibition proved impossible.

Despite Macdonald's claims that he was innocent, evidence came to light showing receipts of money from Allan to Macdonald and some of his political colleagues. Perhaps even more damaging to Macdonald was when the Liberals discovered a telegram, through a former employee of Allan, which was thought to have been stolen from the safe of Allan's lawyer, John Abbott.

The scandal proved fatal to Macdonald's government. Macdonald's control of Parliament was already tenuous following the 1872 election. In a time when party discipline was not as strong as it is today, once Macdonald's culpability in the scandal became known he could no longer expect to retain the confidence of the House of Commons.

Macdonald resigned as prime minister on 5 November 1873. He also offered his resignation as the head of the Conservative party, but it was not accepted and he was convinced to stay. Perhaps as a direct result of this scandal, the Conservative party fell in the eyes of the public and was relegated to being the Official Opposition in the federal election of 1874. This election, in which secret ballots were used for the first time, gave Alexander Mackenzie a firm mandate to succeed Macdonald as the new prime minister of Canada.

Despite the short-term defeat, the scandal was not a mortal wound to Macdonald, the Conservative Party, or the Canadian Pacific Railway. An economic depression gripped Canada shortly after Macdonald left office, and although the causes of the depression were largely external to Canada many Canadians nevertheless blamed Mackenzie for the ensuing hard times. Macdonald would return as prime minister in the 1878 election thanks to his National Policy. He would hold the office of prime minister to his death in 1891, and the Canadian Pacific would be completed by 1885 with Macdonald still in office.




</doc>
<doc id="23650" url="https://en.wikipedia.org/wiki?curid=23650" title="Primer (molecular biology)">
Primer (molecular biology)

A primer is a short strand of RNA or DNA (generally about 18-22 bases) that serves as a starting point for DNA synthesis. It is required for DNA replication because the enzymes that catalyze this process, DNA polymerases, can only add new nucleotides to an existing strand of DNA. The polymerase starts replication at the 3′-end of the primer, and copies the opposite strand.

In vivo DNA replication utilizes short strands of RNA called RNA primers to initiate DNA synthesis on both the leading and lagging strands – DNA primers are not seen in vivo in humans. These RNA primers can be made "de novo".

On the other hand, many of the in vitro laboratory techniques that involve DNA polymerase in biochemistry and molecular biology (such as DNA sequencing and the polymerase chain reaction), use DNA primers because they are more temperature stable. In experiments, it is often important to use a primer with a similar Tm (melting temperature) to the template strand it will be hybridizing to. A primer with a Tm significantly higher than the reaction's annealing temperature may mishybridize and extend at an incorrect location along the DNA sequence, while one with a Tm significantly lower than the annealing temperature may fail to anneal and extend at all. These primers are usually short, chemically synthesized oligonucleotides, with a length of about twenty bases. They are hybridized to a target DNA, which is then copied by the polymerase.

The lagging strand of DNA is that strand of the DNA double helix that is orientated in a 5′ to 3′ manner. Therefore, its complement must be synthesized in a 3′→5′ manner. Because DNA polymerase III cannot synthesize in the 5′→3′ direction, the lagging strand is synthesized in short segments known as Okazaki fragments. Along the lagging strand's template, primase builds RNA primers in short bursts. DNA polymerases are then able to use the free 3′-OH groups on the RNA primers to synthesize DNA in the 5′→3′ direction.

The RNA fragments are then removed by DNA polymerase I for prokaryotes or DNA polymerase δ for eukaryotes (different mechanisms are used in eukaryotes and prokaryotes) and new deoxyribonucleotides are added to fill the gaps where the RNA was present. DNA ligase then joins the deoxyribonucleotides together, completing the synthesis of the lagging strand.

In eukaryotic primer removal, DNA polymerase δ extends the Okazaki fragment in 5′ to 3′ direction, and when it encounters the RNA primer from the previous Okazaki fragment, it displaces the 5′ end of the primer into a single-stranded RNA flap, which is removed by nuclease cleavage. Cleavage of the RNA flaps involves either flap structure-specific endonuclease 1 (FEN1) cleavage of short flaps, or coating of long flaps by the single-stranded DNA binding protein replication protein A (RPA) and sequential cleavage by Dna2 nuclease and FEN1.

This mechanism is a potential explanation of how the HIV virus can transform its genome into double-stranded DNA from the RNA-DNA formed after reverse transcription of its RNA. However, the HIV-encoded reverse transcriptase has its own ribonuclease activity that degrades the viral RNA during the synthesis of cDNA, as well as DNA-dependent DNA polymerase activity that copies the sense cDNA strand into "antisense" DNA to form a double-stranded DNA intermediate.

DNA sequencing is used to determine the nucleotides in a DNA strand. The Sanger chain termination method of sequencing uses a primer to start the chain reaction.

In PCR, primers are used to determine the DNA fragment to be amplified by the PCR process. The length of primers is usually not more than 30 (usually 18–24) nucleotides, and they need to match the beginning and the end of the DNA fragment to be amplified. They direct replication towards each other – the extension of one primer by polymerase then becomes the template for the other, leading to an exponential increase in the target segment.

It is worth noting that primers are not always for DNA synthesis, but can in fact be used by viral polymerases, e.g. influenza, for RNA synthesis.

Pairs of primers should have similar melting temperatures since annealing in a PCR occurs for both simultaneously. A primer with a "T" (melting temperature) significantly higher than the reaction's annealing temperature may mishybridize and extend at an incorrect location along the DNA sequence, while "T" significantly lower than the annealing temperature may fail to anneal and extend at all.

Primer sequences need to be chosen to uniquely select for a region of DNA, avoiding the possibility of mishybridization to a similar sequence nearby. A commonly used method is BLAST search whereby all the possible regions to which a primer may bind can be seen. Both the nucleotide sequence as well as the primer itself can be BLAST searched. The free NCBI tool Primer-BLAST integrates primer design and BLAST search into one application, as do commercial software products such as ePrime and Beacon Designer. Computer simulations of theoretical PCR results (Electronic PCR) may be performed to assist in primer design.

Many online tools are freely available for primer design, some of which focus on specific applications of PCR. The popular tools Primer3Plus and PrimerQuest can be used to find primers matching a wide variety of specifications. Highly degenerate primers for targeting a wide variety of DNA templates can be interactively designed using GeneFISHER. Primers with high specificity for a subset of DNA templates in the presence of many similar variants can be designed using DECIPHER. Primer design aims to generate a balance between specificity and efficiency of amplification.

Mononucleotide and dinucleotide repeats should be avoided, as loop formation can occur and contribute to mishybridization. Primers should not easily anneal with other primers in the mixture (either other copies of same or the reverse direction primer); this phenomenon can lead to the production of 'primer dimer' products contaminating the mixture. Primers should also not anneal strongly to themselves, as internal hairpins and loops could hinder the annealing with the template DNA.

When designing a primer for use in TA cloning, efficiency can be increased by adding AG tails to the 5′ and the 3′ end.

The reverse primer has to be the reverse complement of the given cDNA sequence. The reverse complement can be easily determined, e.g. with online calculators.

Sometimes "degenerate primers" are used. These are actually mixtures of similar, but not identical primers. They may be convenient if the same gene is to be amplified from different organisms, as the genes themselves are probably similar but not identical. The other use for degenerate primers is when primer design is based on protein sequence. As several different codons can code for one amino acid, it is often difficult to deduce which codon is used in a particular case. Therefore, primer sequence corresponding to the amino acid isoleucine might be "ATH", where A stands for adenine, T for thymine, and H for adenine, thymine, or cytosine, according to the genetic code for each codon, using the IUPAC symbols for degenerate bases. Use of degenerate primers can greatly reduce the specificity of the PCR amplification. The problem can be partly solved by using touchdown PCR.

"Degenerate primers" are widely used and extremely useful in the field of microbial ecology. They allow for the amplification of genes from thus far uncultivated microorganisms or allow the recovery of genes from organisms where genomic information is not available. Usually, degenerate primers are designed by aligning gene sequencing found in GenBank. Differences among sequences are accounted for by using IUPAC degeneracies for individual bases. PCR primers are then synthesized as a mixture of primers corresponding to all permutations.

Excision of Rna primers : once a small segment of an okazaki fragment has been formed, the rna primers are removed from the 5 by the action of 5 3 exonuclease activity of dna polymerase I

There are a number of programs available to perform these primer predictions:




</doc>
<doc id="23652" url="https://en.wikipedia.org/wiki?curid=23652" title="Purine">
Purine

A purine is a heterocyclic aromatic organic compound that consists of a pyrimidine ring fused to an imidazole ring. Purine gives its name to the wider class of molecules, purines, which include substituted purines and their tautomers, are the most widely occurring nitrogen-containing heterocycle in nature. Purine is water soluble.

Purines are found in high concentration in meat and meat products, especially internal organs such as liver and kidney. In general, plant-based diets are low in purines. Examples of high-purine sources include: sweetbreads, anchovies, sardines, liver, beef kidneys, brains, meat extracts (e.g., Oxo, Bovril), herring, mackerel, scallops, game meats, beer (from the yeast) and gravy.

A moderate amount of purine is also contained in red meat, beef, pork, poultry, fish and seafood, asparagus, cauliflower, spinach, mushrooms, green peas, lentils, dried peas, beans, oatmeal, wheat bran, wheat germ, and haws.

Purines and pyrimidines make up the two groups of nitrogenous bases, including the two groups of nucleotide bases. Two of the four deoxyribonucleotides (deoxyadenosine and deoxyguanosine) and two of the four ribonucleotides (adenosine, or AMP, and guanosine, or GMP), the respective building blocks of DNA and RNA, are purines. In order to form DNA and RNA, both purines and pyrimidines are needed by the cell in approximately equal quantities. Both purine and pyrimidine are self-inhibiting and activating. When purines are formed, they inhibit the enzymes required for more purine formation. This self-inhibition occurs as they also activate the enzymes needed for pyrimidine formation. Pyrimidine simultaneously self-inhibits and activates purine in similar manner. Because of this, there is nearly an equal amount of both substances in the cell at all times.

Purine is both a very weak acid (pK 2.39) and an even weaker base (pK 8.93). If dissolved in pure water, the pH will be half way between these two pK values.

There are many naturally occurring purines. They include the nucleobases adenine (2) and guanine (3). In DNA, these bases form hydrogen bonds with their complementary pyrimidines, thymine and cytosine, respectively. This is called complementary base pairing. In RNA, the complement of adenine is uracil instead of thymine.

Other notable purines are hypoxanthine (4), xanthine (5), theobromine (6), caffeine (7), uric acid (8) and isoguanine (9).

Aside from the crucial roles of purines (adenine and guanine) in DNA and RNA, purines are also significant components in a number of other important biomolecules, such as ATP, GTP, cyclic AMP, NADH, and coenzyme A. Purine (1) itself, has not been found in nature, but it can be produced by organic synthesis.

They may also function directly as neurotransmitters, acting upon purinergic receptors. Adenosine activates adenosine receptors.

The word "purine" ("pure urine") was coined by the German chemist Emil Fischer in 1884. He synthesized it for the first time in 1898. The starting material for the reaction sequence was uric acid (8), which had been isolated from kidney stones by Carl Wilhelm Scheele in 1776. Uric acid (8) was reacted with PCl to give 2,6,8-trichloropurine (10), which was converted with HI and PHI to give 2,6-diiodopurine (11). The product was reduced to purine (1) using zinc dust.

Many organisms have metabolic pathways to synthesize and break down purines.

Purines are biologically synthesized as nucleosides (bases attached to ribose).

Accumulation of modified purine nucleotides is defective to various cellular processes, especially those involving DNA and RNA. To be viable, organisms possess a number of (deoxy)purine phosphohydrolases, which hydrolyze these purine derivatives removing them from the active NTP and dNTP pools. Deamination of purine bases can result in accumulation of such nucleotides as ITP, dITP, XTP and dXTP.

Defects in enzymes that control purine production and breakdown can severely alter a cell’s DNA sequences, which may explain why people who carry certain genetic variants of purine metabolic enzymes have a higher risk for some types of cancer.

Higher levels of meat and seafood consumption are associated with an increased risk of gout, whereas a higher level of consumption of dairy products is associated with a decreased risk. Moderate intake of purine-rich vegetables or protein is not associated with an increased risk of gout.

In addition to "in vivo" synthesis of purines in purine metabolism, purine can also be created artificially.

Purine (1) is obtained in good yield when formamide is heated in an open vessel at 170 °C for 28 hours.

This remarkable reaction and others like it have been discussed in the context of the origin of life.

Oro, Orgel and co-workers have shown that four molecules of HCN tetramerize to form diaminomaleodinitrile (12), which can be converted into almost all naturally-occurring purines. For example, five molecules of HCN condense in an exothermic reaction to make adenine, especially in the presence of ammonia.

The Traube purine synthesis (1900) is a classic reaction (named after Wilhelm Traube) between an amine-substituted pyrimidine and formic acid.




</doc>
<doc id="23653" url="https://en.wikipedia.org/wiki?curid=23653" title="Pyrimidine">
Pyrimidine

Pyrimidine is an aromatic heterocyclic organic compound similar to pyridine. One of the three diazines (six-membered heterocyclics with two nitrogen atoms in the ring), it has the nitrogen atoms at positions 1 and 3 in the ring. The other diazines are pyrazine (nitrogen atoms at the 1 and 4 positions) and pyridazine (nitrogen atoms at the 1 and 2 positions). In nucleic acids, three types of nucleobases are pyrimidine derivatives: cytosine (C), thymine (T), and uracil (U).

The pyrimidine ring system has wide occurrence in nature
as substituted and ring fused compounds and derivatives, including the nucleotides cytosine, thymine and uracil, thiamine (vitamin B1) and alloxan. It is also found in many synthetic compounds such as barbiturates and the HIV drug, zidovudine. Although pyrimidine derivatives such as uric acid and alloxan were known in the early 19th century, a laboratory synthesis of a pyrimidine was not carried out until 1879, when Grimaux reported the preparation of barbituric acid from urea and malonic acid in the presence of phosphorus oxychloride.
The systematic study of pyrimidines began in 1884 with Pinner,
who synthesized derivatives by condensing ethyl acetoacetate with amidines. Pinner first proposed the name “pyrimidin” in 1885. The parent compound was first prepared by Gabriel and Colman in 1900,

by conversion of barbituric acid to 2,4,6-trichloropyrimidine followed by reduction using zinc dust in hot water.

The nomenclature of pyrimidines is straightforward. However, like other heterocyclics, tautomeric hydroxyl groups yield complications since they exist primarily in the cyclic amide form. For example, 2-hydroxypyrimidine is more properly named 2-pyrimidone. A partial list of trivial names of various pyrimidines exists.

Physical properties are shown in the data box. A more extensive discussion, including spectra, can be found in Brown "et al."

Per the classification by Albert six-membered heterocycles can be described as π-deficient. Substitution by electronegative groups or additional nitrogen atoms in the ring significantly increase the π-deficiency. These effects also decrease the basicity.

Like pyridines, in pyrimidines the π-electron density is decreased to an even greater extent. Therefore, electrophilic aromatic substitution is more difficult while nucleophilic aromatic substitution is facilitated. An example of the last reaction type is the displacement of the amino group in 2-aminopyrimidine by chlorine and its reverse.

Electron lone pair availability (basicity) is decreased compared to pyridine. Compared to pyridine, "N"-alkylation and "N"-oxidation are more difficult. The p"K" value for protonated pyrimidine is 1.23 compared to 5.30 for pyridine. Protonation and other electrophilic additions will occur at only one nitrogen due to further deactivation by the second nitrogen. The 2-, 4-, and 6- positions on the pyrimidine ring are electron deficient analogous to those in pyridine and nitro- and dinitrobenzene. The 5-position is less electron deficient and substituents there are quite stable. However, electrophilic substitution is relatively facile at the 5-position, including nitration and halogenation.

Reduction in resonance stabilization of pyrimidines may lead to addition and ring cleavage reactions rather than substitutions. One such manifestation is observed in the Dimroth rearrangement.

Pyrimidine is also found in meteorites, but scientists still do not know its origin. Pyrimidine also photolytically decomposes into uracil under ultraviolet light.

As is often the case with parent heterocyclic ring systems, the synthesis of pyrimidine is not that common and is usually performed by removing functional groups from derivatives. Primary syntheses in quantity involving formamide have been reported.

As a class, pyrimidines are typically synthesized by the principal synthesis involving cyclization of β-dicarbonyl compounds with N–C–N compounds. Reaction of the former with amidines to give 2-substituted pyrimidines, with urea to give 2-pyrimidinones, and guanidines to give 2-aminopyrimidines are typical.

Pyrimidines can be prepared via the Biginelli reaction. Many other methods rely on condensation of carbonyls with diamines for instance the synthesis of 2-thio-6-methyluracil from thiourea and ethyl acetoacetate or the synthesis of 4-methylpyrimidine with 4,4-dimethoxy-2-butanone and formamide.

A novel method is by reaction of "N"-vinyl and "N"-aryl amides with carbonitriles under electrophilic activation of the amide with 2-chloro-pyridine and trifluoromethanesulfonic anhydride:

Because of the decreased basicity compared to pyridine, electrophilic substitution of pyrimidine is less facile. Protonation or alkylation typically takes place at only one of the ring nitrogen atoms. Mono-"N"-oxidation occurs by reaction with peracids.

Electrophilic "C"-substitution of pyrimidine occurs at the 5-position, the least electron-deficient. Nitration, nitrosation, azo coupling, halogenation, sulfonation, formylation, hydroxymethylation, and aminomethylation have been observed with substituted pyrimidines.

Nucleophilic "C"-substitution should be facilitated at the 2-, 4-, and 6-positions but there are only a few examples. Amination and hydroxylation has been observed for substituted pyrimidines. Reactions with Grignard or alkyllithium reagents yield 4-alkyl- or 4-aryl pyrimidine after aromatization.

Free radical attack has been observed for pyrimidine and photochemical reactions have been observed for substituted pyrimidines. Pyrimidine can be hydrogenated to give tetrahydropyrimidine.

Three nucleobases found in nucleic acids, cytosine (C), thymine (T), and uracil (U), are pyrimidine derivatives:

In DNA and RNA, these bases form hydrogen bonds with their complementary purines. Thus, in DNA, the purines adenine (A) and guanine (G) pair up with the pyrimidines thymine (T) and cytosine (C), respectively.

In RNA, the complement of adenine (A) is uracil (U) instead of thymine (T), so the pairs that form are adenine:uracil and guanine:cytosine.

Very rarely, thymine can appear in RNA, or uracil in DNA, but when the other three major pyrimidine bases are represented, some minor pyrimidine bases can also occur in nucleic acids. These minor pyrimidines are usually methylated versions of major ones and are postulated to have regulatory functions.

These hydrogen bonding modes are for classical Watson–Crick base pairing. Other hydrogen bonding modes ("wobble pairings") are available in both DNA and RNA, although the additional 2′-hydroxyl group of RNA expands the configurations, through which RNA can form hydrogen bonds.

In March 2015, NASA Ames scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the universe, may have been formed in red giants or in interstellar dust and gas clouds.


</doc>
<doc id="23654" url="https://en.wikipedia.org/wiki?curid=23654" title="Play-by-mail game">
Play-by-mail game

Play-by-mail games, or play-by-post games, are games, of any type, played through postal mail or email.

Correspondence chess has been played by mail for centuries. The boardgame "Diplomacy" has been played by mail since the 1960s, starting with a printed newsletter (a fanzine) written by John Boardman. More complex games, moderated entirely or partially by computer programs, were pioneered by Rick Loomis of Flying Buffalo in 1970. The first such game offered via major e-mail services was "WebWar II" (based on Starweb and licensed from Flying Buffalo) from Neolithic Enterprises who accepted e-mail turns from all of the major e-mail services including CompuServe in 1983.

Play by mail games are often referred to as PBM games, and play by email is sometimes abbreviated PBeM—as opposed to face to face (FTF) or over the board (OTB) games which are played in person. Another variation on the name is play-by-Internet (PBI) or play-by-web (PBW). In all of these examples, player instructions can be either executed by a human moderator, a computer program, or a combination of the two.

In the 1980s, play-by-mail games reached their peak of popularity with the advent of "Gaming Universal", "Paper Mayhem" and "Flagship magazine", the first professional magazines devoted to play-by-mail games. (An earlier fanzine, "The Nuts & Bolts of PBM", was the first publication to exclusively cover the hobby.) Bob McLain, the publisher and editor of Gaming Universal, further popularized the hobby by writing articles that appeared in many of the leading mainstream gaming magazines of the time. Flagship later bought overseas right to Gaming Universal, making it the leading magazine in the field. "Flagship magazine" was founded by Chris Harvey and Nick Palmer. The magazine still exists, under a new editor, but health concerns have led to worries over the publication's long term viability.

In the late 1990s, computer and Internet games marginalized play-by-mail conducted by actual postal mail, but the postal hobby still exists with an estimated 2000–3000 adherents worldwide.

Postal gaming developed as a way for geographically separated gamers to compete with each other. It was especially useful for those living in isolated areas and those whose tastes in games were uncommon.

In the case of a two player game such as chess, players would simply send their moves to each other alternately. In the case of a multi-player game such as Diplomacy, a central game master would run the game, receiving the moves and publishing adjudications. Such adjudications were often published in postal game zines, some of which contained far more than just games.

The commercial market for play-by-mail games grew to involve computer servers set up to host potentially thousands of players at once. Players would typically be split up into parallel games in order to keep the number of players per game at a reasonable level, with new games starting as old games ended. A typical closed game session might involve one to two dozen players, although some games claimed to have as many as five hundred people simultaneously competing in the same game world. While the central company was responsible for feeding in moves and mailing the processed output back to players, players were also provided with the mailing addresses of others so that direct contact could be made and negotiations performed. With turns being processed every few weeks (a two-week turnaround being standard), more advanced games could last over a year.

Game themes are heavily varied, and may range from those based on historical or real events to those taking place in alternate or fictional worlds.

One of the most successful and longest running PBM games is TribeNet, a strategy game with themes of exploration, trade and warfare. TribeNet was launched by Jeff Perkins as a PBM in 1985 and was transformed by Peter Rzechorzek into a PBeM when he took over the game in 1997. Peter has remained in charge for over twenty years. 

The onset of the computer-moderated PBM game (primarily the "Legends" game system) inevitably meant that the human moderated games became "boutique" games with little chance of matching the gross revenues that larger, automated games could produce.

The mechanics of play-by-mail games require that players think and plan carefully before making moves. Because planned actions can typically only be submitted at a fixed maximum frequency (e.g., once every few days or every few weeks), the number of discrete actions is limited compared to real-time games. As a result, players are provided with a variety of resources to assist in turn planning, including game aids, maps, and results from previous turns. Using this material, planning a single turn may take a number of hours.

Actual move/turn submission is traditionally carried out by filling in a "turn card". This card has formatted entry areas where players enter their planned actions (using some form of encoding) for the upcoming turn. Players are limited to some finite number of actions, and in some cases must split their resources between these actions (so that additional actions make each less effective). The way the card is filled in often implies an ordering between each command, so that they are processed in-order, one after another. Once completed, the card is then mailed (or, in more modern times, emailed) to the game master, where it is either processed, or held until the next turn processing window begins.

By gathering turn cards from a number of players and processing them all at the same time, games can provide simultaneous actions for all players. However, for this same reason, co-ordination between players can be difficult to achieve. For example, player A might attempt to move to player B's current location to do something with (or to) player B, while player B might simultaneously attempt to move to player A's current location. As such, the output/results of the turn can differ significantly from the submitted plan. Whatever the results, they are mailed back to the player to be studied and used as the basis for the next turn (often along with a new blank turn card).

While billing is sometimes done using a flat per-game rate (when the length of the game is known and finite), games more typically use a per-turn cost schedule. In such cases, each turn submitted depletes a pool of credit which must periodically be replenished in order to keep playing. Some games have multiple fee schedules, where players can pay more to perform advanced actions, or to take a greater number of actions in a turn.

Some role-playing PBM games also include an element whereby the player may describe actions of their characters in a free text form. The effect and effectiveness of the action is then based on the judgement of the GM who may allow or partially allow the action. This gives the player more flexibility beyond the normal fixed actions at the cost of more complexity and, usually, expense.

With the rise of the Internet, email and websites have largely replaced postal gaming and postal games zines. Play-by-email (PBEM) games differ from popular online multiplayer games in that, for most computerized multiplayer games, the players have to be online at the same time - also known as synchronous play. With a play-by-mail game, the players can play whenever they choose, since responses need not be immediate; this is sometimes referred to as turn-based gaming and is common among browser-based games. Some video games can be played in turn-based mode: one player makes a move, declaring it by email, and then the turn passes to another player who to makes his or her move. Depending on the game, a PBEM Aide may be available as an alternative to describing the move with text. Such an aide generally consists of a picture of the board along with movable playing pieces, cards, etc. A player moves the pieces as desired and then can either save a static picture of the new game state, or a replay sequentially showing all changes that transpired during his turn, and send this to his opponent.

Several non-commercial email games played on the Internet and BITNET predate these. Some cards games like poker can also be played by email using cryptography, such as with FXTOP.

An increasingly popular format for play-by-email games is play-by-web. As with play-by-email games the players are notified by email when it becomes their turn, but they must then return to the game's website to continue playing what is essentially a browser-based game. The main advantage of this is that the players can be presented with a graphical representation of the game and an interactive interface to guide them through their turn. Since the notifications only have to remind the players that it is their turn they can just as easily be sent via instant messaging.

Some sites have extended this gaming style by allowing the players to see each other's actions as they are made. This allows for real time playing while everyone is online and active, or slower progress if not.

Increasingly, this format is being adopted by social and mobile games, often described using the term "asynchronous multiplayer".




</doc>
<doc id="23658" url="https://en.wikipedia.org/wiki?curid=23658" title="Philip K. Dick Award">
Philip K. Dick Award

The Philip K. Dick Award is a science fiction award given annually at Norwescon and sponsored by the Philadelphia Science Fiction Society and (since 2005) the Philip K. Dick Trust. Named after science fiction and fantasy writer Philip K. Dick, it has been awarded since 1983, the year after his death. It is awarded to the best original paperback published each year in the US.

The award was founded by Thomas Disch with assistance from David G. Hartwell, Paul S. Williams, and Charles N. Brown. As of 2016, it is administered by Gordon Van Gelder. Past administrators include Algis Budrys, David G. Hartwell, and David Alexander Smith.

Winners are listed in bold.<br>
Authors of special citation entries are listed in "italics". The year in the table below indicates the year the book was published – winners are announced the following year.



</doc>
<doc id="23659" url="https://en.wikipedia.org/wiki?curid=23659" title="Plug-in (computing)">
Plug-in (computing)

In computing, a plug-in (or plugin, add-in, addin, add-on, addon, or extension) is a software component that adds a specific feature to an existing computer program. When a program supports plug-ins, it enables customization. The common examples are the plug-ins used in web browsers to add new features such as search-engines, virus scanners, or the ability to use a new file type such as a new video format. Well-known browser plug-ins include the Adobe Flash Player, the QuickTime Player, and the Java plug-in, which can launch a user-activated Java applet on a web page to its execution on a local Java virtual machine.

A theme or skin is a preset package containing additional or changed graphical appearance details, achieved by the use of a graphical user interface (GUI) that can be applied to specific software and websites to suit the purpose, topic, or tastes of different users to customize the look and feel of a piece of computer software or an operating system front-end GUI (and window managers).

Applications support plug-ins for many reasons. Some of the main reasons include:

Types of applications and why they use plug-ins:

The host application provides services which the plug-in can use, including a way for plug-ins to register themselves with the host application and a protocol for the exchange of data with plug-ins. Plug-ins depend on the services provided by the host application and do not usually work by themselves. Conversely, the host application operates independently of the plug-ins, making it possible for end-users to add and update plug-ins dynamically without needing to make changes to the host application.

Programmers typically implement plug-in functionality using shared libraries, which get dynamically loaded at run time, installed in a place prescribed by the host application. HyperCard supported a similar facility, but more commonly included the plug-in code in the HyperCard documents (called "stacks") themselves. Thus the HyperCard stack became a self-contained application in its own right, distributable as a single entity that end-users could run without the need for additional installation-steps. Programs may also implement plugins by loading a directory of simple script files written in a scripting language like Python or Lua.

In Mozilla Foundation definitions, the words "add-on", "extension" and "plug-in" are not synonyms. "Add-on" can refer to anything that extends the functions of a Mozilla application. Extensions comprise a subtype, albeit the most common and the most powerful one. Mozilla applications come with integrated add-on managers that, similar to package managers, install, update and manage extensions. The term, "Plug-in", however, strictly refers to NPAPI-based web content renderers. Plug-ins are being deprecated.

Plug-ins appeared as early as the mid 1970s, when the EDT text editor running on the Unisys VS/9 operating system using the UNIVAC Series 90 mainframe computers provided the ability to run a program from the editor and to allow such a program to access the editor buffer, thus allowing an external program to access an edit session in memory. The plug-in program could make calls to the editor to have it perform text-editing services upon the buffer that the editor shared with the plug-in. The Waterloo Fortran compiler used this feature to allow interactive compilation of Fortran programs edited by EDT.

Very early PC software applications to incorporate plug-in functionality included HyperCard and QuarkXPress on the Macintosh, both released in 1987. In 1988, Silicon Beach Software included plug-in functionality in Digital Darkroom and SuperPaint, and Ed Bomke coined the term "plug-in".



</doc>
<doc id="23660" url="https://en.wikipedia.org/wiki?curid=23660" title="Pierre Teilhard de Chardin">
Pierre Teilhard de Chardin

Pierre Teilhard de Chardin ( (); 1 May 1881 – 10 April 1955) was a French idealist philosopher and Jesuit priest who trained as a paleontologist and geologist and took part in the discovery of Peking Man. He conceived the vitalist idea of the Omega Point (a maximum level of complexity and consciousness towards which he believed the universe was evolving) and developed Vladimir Vernadsky's concept of noosphere.

Although many of Teilhard's writings were censored by the Catholic Church during his lifetime because of his views on original sin, Teilhard has been posthumously praised by Pope Benedict XVI and other eminent Catholic figures, and his theological teachings were cited by Pope Francis in the 2015 encyclical, "Laudato si'". The response to his writings by evolutionary biologists has been, with some exceptions, decidedly negative.

Pierre Teilhard de Chardin was born in the Château of Sarcenat at Orcines, close to Clermont-Ferrand, France, on 1 May 1881. On the Teilhard side he was descended from an ancient family of magistrates from Auvergne originating in Murat, Cantal, and on the de Chardin side he was descended from a family that was ennobled under Louis XVIII. He was the fourth of eleven children. His father, Emmanuel Teilhard (1844–1932), an amateur naturalist, collected stones, insects and plants and promoted the observation of nature in the household. Pierre Teilhard's spirituality was awakened by his mother, Berthe de Dompiere the grand-niece of the great philosopher of the previous century, Voltaire. When he was 12, he went to the Jesuit college of Mongré, in Villefranche-sur-Saône, where he completed baccalaureates of philosophy and mathematics. Then, in 1899, he entered the Jesuit novitiate at Aix-en-Provence, where he began a philosophical, theological and spiritual career.

When the Associations Bill of 1901 required congregational associations to submit their properties to state control, some of the Jesuits exiled themselves in the United Kingdom. Young Jesuit students continued their studies in Jersey. In the meantime, Teilhard earned a licentiate in literature in Caen in 1902.

From 1905 to 1908, he taught physics and chemistry in Cairo, Egypt, at the Jesuit College of the Holy Family. He wrote "... it is the dazzling of the East foreseen and drunk greedily ... in its lights, its vegetation, its fauna and its deserts."

Teilhard studied theology in Hastings, in Sussex (United Kingdom), from 1908 to 1912. There he synthesized his scientific, philosophical and theological knowledge in the light of evolution. At that time he read "L'Évolution Créatrice" (The Creative Evolution) by Henri Bergson, about which he wrote that "the only effect that brilliant book had upon me was to provide fuel at just the right moment, and very briefly, for a fire that was already consuming my heart and mind." In short, Bergson's ideas helped him to unify his views on matter, life, and energy into a coherent and organic whole.

From 1912 to 1914, Teilhard worked in the paleontology laboratory of the Museum National d'Histoire Naturelle in Paris, studying the mammals of the middle tertiary period. Later he studied elsewhere in Europe. In June 1912 he formed part of the original digging team, with Arthur Smith Woodward and Charles Dawson, at the Piltdown site, after the discovery of the first fragments of the (fraudulent) "Piltdown Man". Some have suggested he participated in the hoax. Marcellin Boule, a specialist in Neanderthal studies, who as early as 1915 had recognized the non-hominid origins of the Piltdown finds, gradually guided Teilhard towards human paleontology. At the museum's Institute of Human Paleontology, he became a friend of Henri Breuil and in 1913 took part with him in excavations at the prehistoric painted Caves of Castillo in northwest Spain.

Mobilised in December 1914, Teilhard served in World War I as a stretcher-bearer in the 8th Moroccan Rifles. For his valour, he received several citations including the Médaille militaire and the Legion of Honour.

During the war he developed his reflections in his diaries and in letters to his cousin, Marguerite Teillard-Chambon, who later published a collection of them. He later wrote: "...the war was a meeting ... with the Absolute." In 1916, he wrote his first essay: "La Vie Cosmique" ("Cosmic life"), where his scientific and philosophical thought was revealed just as his mystical life. While on leave from the military he pronounced his solemn vows as a Jesuit in Sainte-Foy-lès-Lyon on 26 May 1918. In August 1919, in Jersey, he wrote "Puissance spirituelle de la Matière" ("The Spiritual Power of Matter").

At the Sorbonne Teilhard pursued three unit degrees of natural science: geology, botany and zoology. His thesis treated the mammals of the French lower Eocene and their stratigraphy. After 1920, he lectured in geology at the Catholic Institute of Paris and after earning a science doctorate in 1922 became an assistant professor there.

In 1923 he travelled to China with Father Emile Licent, who was in charge in Tianjin of a significant laboratory collaboration between the Natural History Museum in Paris and Marcellin Boule's laboratory. Licent carried out considerable basic work in connection with missionaries who accumulated observations of a scientific nature in their spare time. He was known as 德日進 (pinyin: Dérìjìn) in China.

Teilhard wrote several essays, including "La Messe sur le Monde" (the "Mass on the World"), in the Ordos Desert. In the following year he continued lecturing at the Catholic Institute and participated in a cycle of conferences for the students of the Engineers' Schools. Two theological essays on Original Sin sent to a theologian at his request on a purely personal basis were wrongly understood.


The Church required him to give up his lecturing at the Catholic Institute in order to continue his geological research in China.

Teilhard traveled again to China in April 1926. He would remain there for about twenty years, with many voyages throughout the world. He settled until 1932 in Tientsin with Emile Licent, then in Beijing. Teilhard made five geological research expeditions in China between 1926 and 1935. They enabled him to establish a general geological map of China. That same year, Teilhard's superiors in the Jesuit Order forbade him to teach any longer.

In 1926–27, after a missed campaign in Gansu, Teilhard traveled in the Sang-Kan-Ho valley near Kalgan (Zhangjiakou) and made a tour in Eastern Mongolia. He wrote "Le Milieu Divin" ("The Divine Milieu"). Teilhard prepared the first pages of his main work "Le Phénomène Humain" ("The Phenomenon of Man"). The Holy See refused the Imprimatur for "Le Milieu Divin" in 1927. He joined the ongoing excavations of the Peking Man Site at Zhoukoudian as an advisor in 1926 and continued in the role for the Cenozoic Research Laboratory of the Geological Survey of China following its founding in 1928. Teilhard resided in Manchuria with Emile Licent, then stayed in Western Shansi (Shanxi) and northern Shensi (Shaanxi) with the Chinese paleontologist C. C. Young and with Davidson Black, Chairman of the Geological Survey of China.

After a tour in Manchuria in the area of Great Khingan with Chinese geologists, Teilhard joined the team of American Expedition Center-Asia in the Gobi Desert organised in June and July, by the American Museum of Natural History with Roy Chapman Andrews. Henri Breuil and Teilhard discovered that the Peking Man, the nearest relative of "Pithecanthropus" from Java, was a "faber" (worker of stones and controller of fire). Teilhard wrote "L'Esprit de la Terre" ("The Spirit of the Earth").

Teilhard took part as a scientist in the Croisière Jaune (Yellow Cruise) financed by André Citroën in Central Asia. Northwest of Beijing in Kalgan, he joined the Chinese group who joined the second part of the team, the Pamir group, in Aksu. He remained with his colleagues for several months in Ürümqi, capital of Sinkiang. The following year the Sino-Japanese War (1937–1945) began.

In 1933, Rome ordered him to give up his post in Paris. Teilhard subsequently undertook several explorations in the south of China. He traveled in the valleys of Yangtze River and Sichuan in 1934, then, the following year, in Kwang-If and Guangdong. The relationship with Marcellin Boule was disrupted; the museum cut its financing on the grounds that Teilhard worked more for the Chinese Geological Service than for the museum.

During all these years, Teilhard contributed considerably to the constitution of an international network of research in human paleontology related to the whole of eastern and southeastern Asia. He would be particularly associated in this task with two friends, the English/Canadian Davidson Black and the Scot George B. Barbour. Often he would visit France or the United States, only to leave these countries for further expeditions.

From 1927 to 1928, Teilhard based himself in Paris. He journeyed to Leuven, Belgium, and to Cantal and Ariège, France. Between several articles in reviews, he met new people such as Paul Valéry and Bruno de Solages, who were to help him in issues with the Catholic Church.

Answering an invitation from Henry de Monfreid, Teilhard undertook a journey of two months in Obock, in Harrar and in Somalia with his colleague Pierre Lamarre, a geologist, before embarking in Djibouti to return to Tianjin. While in China, Teilhard developed a deep and personal friendship with Lucile Swan.

During 1930–1931, Teilhard stayed in France and in the United States. During a conference in Paris, Teilhard stated: "For the observers of the Future, the greatest event will be the sudden appearance of a collective humane conscience and a human work to make." From 1932–1933, he began to meet people to clarify issues with the Congregation for the Doctrine of the Faith, regarding "Le Milieu divin" and "L'Esprit de la Terre". He met Helmut de Terra, a German geologist in the International Geology Congress in Washington, D.C..

Teilhard participated in the 1935 Yale–Cambridge expedition in northern and central India with the geologist Helmut de Terra and Patterson, who verified their assumptions on Indian Paleolithic civilisations in Kashmir and the Salt Range Valley. He then made a short stay in Java, on the invitation of Dutch paleontologist Ralph von Koenigswald to the site of Java man. A second cranium, more complete, was discovered. Professor von Koenigswald had also found a tooth in a Chinese apothecary shop in 1934 that he believed belonged to a three-meter-tall ape, "Gigantopithecus," which lived between one hundred thousand and around a million years ago. Fossilized teeth and bone ("dragon bones") are often ground into powder and used in some branches of traditional Chinese medicine.

In 1937, Teilhard wrote "Le Phénomène spirituel" ("The Phenomenon of the Spirit") on board the boat Empress of Japan, where he met the Raja of Sarawak. The ship conveyed him to the United States. He received the Mendel Medal granted by Villanova University during the Congress of Philadelphia, in recognition of his works on human paleontology. He made a speech about evolution, the origins and the destiny of man. "The New York Times" dated 19 March 1937 presented Teilhard as the Jesuit who held that man descended from monkeys. Some days later, he was to be granted the "Doctor Honoris Causa" distinction from Boston College. Upon arrival in that city, he was told that the award had been cancelled.

Rome banned his work "L’Énergie Humaine" in 1939. By this point Teilhard was based again in France, where he was immobilized by malaria. During his return voyage to Beijing he wrote "L'Energie spirituelle de la Souffrance" ("Spiritual Energy of Suffering") (Complete Works, tome VII).

In 1941, Teilhard submitted to Rome his most important work, "Le Phénomène Humain". By 1947, Rome forbade him to write or teach on philosophical subjects. The next year, Teilhard was called to Rome by the Superior General of the Jesuits who hoped to acquire permission from the Holy See for the publication of "Le Phénomène Humain". However, the prohibition to publish it that was previously issued in 1944 was again renewed. Teilhard was also forbidden to take a teaching post in the Collège de France. Another setback came in 1949, when permission to publish "Le Groupe Zoologique" was refused.

Teilhard was nominated to the French Academy of Sciences in 1950. He was forbidden by his Superiors to attend the International Congress of
Paleontology in 1955. The Supreme Authority of the Holy Office, in a decree dated 15 November 1957, forbade the works of de Chardin to be retained in libraries, including those of religious institutes. His books were not to be sold in Catholic bookshops and were not to be translated into other languages.

Further resistance to Teilhard's work arose elsewhere. In April 1958, all Jesuit publications in Spain ("Razón y Fe", "Sal Terrae","Estudios de Deusto", etc.) carried a notice from the Spanish Provincial of the Jesuits that Teilhard's works had been published in Spanish without previous ecclesiastical examination and in defiance of the decrees of the Holy See. A decree of the Holy Office dated 30 June 1962, under the authority of Pope John XXIII, warned that "... it is obvious that in philosophical and theological matters, the said works [Teilhard's] are replete with ambiguities or rather with serious errors which offend Catholic doctrine. That is why... the Rev. Fathers of the Holy Office urge all Ordinaries, Superiors, and Rectors... to effectively protect, especially the minds of the young, against the dangers of the works of Fr. Teilhard de Chardin and his followers" (AAS, 6 August 1962).

The Diocese of Rome on 30 September 1963 required Catholic booksellers in Rome to withdraw his works as well as those that supported his views.

Teilhard died in New York City, where he was in residence at the Jesuit Church of St. Ignatius Loyola, Park Avenue. On 15 March 1955, at the house of his diplomat cousin Jean de Lagarde, Teilhard told friends he hoped he would die on Easter Sunday. On the evening of Easter Sunday, 10 April 1955, during an animated discussion at the apartment of Rhoda de Terra, his personal assistant since 1949, Teilhard suffered a heart attack and died. He was buried in the cemetery for the New York Province of the Jesuits at the Jesuit novitiate, St. Andrew-on-Hudson, in Hyde Park, New York, With the moving of the novitiate, the property was sold to the Culinary Institute of America in 1970.

Teilhard de Chardin has two comprehensive works, "The Phenomenon of Man" and "The Divine Milieu".

His posthumously published book, "The Phenomenon of Man", set forth a sweeping account of the unfolding of the cosmos and the evolution of matter to humanity, to ultimately a reunion with Christ. In the book, Teilhard abandoned literal interpretations of creation in the Book of Genesis in favor of allegorical and theological interpretations. The unfolding of the material cosmos is described from primordial particles to the development of life, human beings and the noosphere, and finally to his vision of the Omega Point in the future, which is "pulling" all creation towards it. He was a leading proponent of orthogenesis, the idea that evolution occurs in a directional, goal-driven way. Teilhard argued in Darwinian terms with respect to biology, and supported the synthetic model of evolution, but argued in Lamarckian terms for the development of culture, primarily through the vehicle of education. Teilhard made a total commitment to the evolutionary process in the 1920s as the core of his spirituality, at a time when other religious thinkers felt evolutionary thinking challenged the structure of conventional Christian faith. He committed himself to what the evidence showed.

Teilhard made sense of the universe by assuming it had a vitalist evolutionary process. He interprets complexity as the axis of evolution of matter into a geosphere, a biosphere, into consciousness (in man), and then to supreme consciousness (the Omega Point).

Teilhard's unique relationship to both paleontology and Catholicism allowed him to develop a highly progressive, cosmic theology which takes into account his evolutionary studies. Teilhard recognized the importance of bringing the Church into the modern world, and approached evolution as a way of providing ontological meaning for Christianity, particularly creation theology. For Teilhard, evolution was "the natural landscape where the history of salvation is situated."

Teilhard's cosmic theology is largely predicated on his interpretation of Pauline scripture, particularly Colossians 1:15-17 (especially verse 1:17b) and 1 Corinthians 15:28. He drew on the Christocentrism of these two Pauline passages to construct a cosmic theology which recognizes the absolute primacy of Christ. He understood creation to be "a teleological process towards union with the Godhead, effected through the incarnation and redemption of Christ, 'in whom all things hold together' (Col. 1:17)." He further posited that creation would not be complete until each "participated being is totally united with God through Christ in the Pleroma, when God will be 'all in all' (1Cor. 15:28)." 
Teilhard's life work was predicated on his conviction that human spiritual development is moved by the same universal laws as material development. He wrote, "...everything is the sum of the past" and "...nothing is comprehensible except through its history. 'Nature' is the equivalent of 'becoming', self-creation: this is the view to which experience irresistibly leads us. ... There is nothing, not even the human soul, the highest spiritual manifestation we know of, that does not come within this universal law." "The Phenomenon of Man" represents Teilhard's attempt at reconciling his religious faith with his academic interests as a paleontologist. One particularly poignant observation in Teilhard's book entails the notion that evolution is becoming an increasingly optional process. Teilhard points to the societal problems of isolation and marginalization as huge inhibitors of evolution, especially since evolution requires a unification of consciousness. He states that "no evolutionary future awaits anyone except in association with everyone else." Teilhard argued that the human condition necessarily leads to the psychic unity of humankind, though he stressed that this unity can only be voluntary; this voluntary psychic unity he termed "unanimization." Teilhard also states that "evolution is an ascent toward consciousness", giving encephalization as an example of early stages, and therefore, signifies a continuous upsurge toward the Omega Point which, for all intents and purposes, is God.

Teilhard also used his perceived correlation between spiritual and material to describe Christ, arguing that Christ not only has a mystical dimension but also takes on a physical dimension as he becomes the organizing principle of the universe—that is, the one who "holds together" the universe (Col. 1:17b). For Teilhard, Christ forms not only the eschatological end toward which his mystical/ecclesial body is oriented, but he also "operates physically in order to regulate all things" becoming "the one from whom all creation receives its stability." In other words, as the one who holds all things together, "Christ exercises a supremacy over the universe which is physical, not simply juridical. He is the unifying centre of the universe and its goal. The function of holding all things together indicates that Christ is not only man and God; he also possesses a third aspect—indeed, a third nature—which is cosmic." In this way, the Pauline description of the Body of Christ is not simply a mystical or ecclesial concept for Teilhard; it is cosmic. This cosmic Body of Christ "extend[s] throughout the universe and compris[es] all things that attain their fulfillment in Christ [so that] ... the Body of Christ is the one single thing that is being made in creation." Teilhard describes this cosmic amassing of Christ as "Christogenesis." According to Teilhard, the universe is engaged in Christogenesis as it evolves toward its full realization at Omega, a point which coincides with the fully realized Christ. It is at this point that God will be "all in all" (1Cor. 15:28c).

In 1925, Teilhard was ordered by the Jesuit Superior General Wlodimir Ledóchowski to leave his teaching position in France and to sign a statement withdrawing his controversial statements regarding the doctrine of original sin. Rather than leave the Society of Jesus, Teilhard signed the statement and left for China.

This was the first of a series of condemnations by certain ecclesiastical officials that would continue until after Teilhard's death. The climax of these condemnations was a 1962 "monitum" (warning) of the Holy Office cautioning on Teilhard's works. It said:

The Holy Office did not place any of Teilhard's writings on the "Index Librorum Prohibitorum" (Index of Forbidden Books), which existed during Teilhard's lifetime and at the time of the 1962 decree.

Shortly thereafter, prominent clerics mounted a strong theological defense of Teilhard's works. Henri de Lubac (later a Cardinal) wrote three comprehensive books on the theology of Teilhard de Chardin in the 1960s. While de Lubac mentioned that Teilhard was less than precise in some of his concepts, he affirmed the orthodoxy of Teilhard de Chardin and responded to Teilhard's critics: "We need not concern ourselves with a number of detractors of Teilhard, in whom emotion has blunted intelligence". Later that decade Joseph Ratzinger, a German theologian who became Pope Benedict XVI, spoke glowingly of Teilhard's Christology in Ratzinger's "Introduction to Christianity":

Over the next several decades prominent theologians and Church leaders, including leading Cardinals, Pope John Paul II and Pope Benedict XVI all wrote approvingly of Teilhard's ideas. In 1981, Cardinal Agostino Casaroli, on behalf of Pope John Paul II, wrote on the front page of the Vatican newspaper, "l'Osservatore Romano":
Cardinal Avery Dulles said in 2004:
Cardinal Christoph Schönborn wrote in 2007:
Pope Benedict XVI in his book "Spirit of the Liturgy" incorporates Teilhard's vision as a touchstone of the Catholic Mass:
in July 2009, Vatican spokesman Fr. Federico Lombardi said, "By now, no one would dream of saying that [Teilhard] is a heterodox author who shouldn’t be studied."

Pope Francis refers to Teilhard's eschatological contribution in his encyclical Laudato si'.

According to Daniel Dennett, "it has become clear to the point of unanimity among scientists that Teilhard offered nothing serious in the way of an alternative to orthodoxy; the ideas that were peculiarly his were confused, and the rest was just bombastic redescription of orthodoxy." Similarly, Steven Rose wrote that "Teilhard is revered as a mystic of genius by some, but amongst most biologists is seen as little more than a charlatan."

In 1961, the Nobel Prize-winner Peter Medawar, a British immunologist, wrote a scornful review of "The Phenomenon Of Man" for the journal "Mind": "the greater part of it, I shall show, is nonsense, tricked out with a variety of metaphysical conceits, and its author can be excused of dishonesty only on the grounds that before deceiving others he has taken great pains to deceive himself". The evolutionary biologist Richard Dawkins called Medawar's review "devastating" and "The Phenomenon of Man" "the quintessence of bad poetic science".

Sir Julian Huxley, the evolutionary biologist, praised the thought of Teilhard de Chardin for looking at the way in which human development needs to be examined within a larger integrated universal sense of evolution, though admitting he could not follow Teilhard all the way. Theodosius Dobzhansky drew upon Teilhard's insistence that evolutionary theory provides the core of how man understands his relationship to nature, calling him "one of the great thinkers of our age".

George Gaylord Simpson, however, felt that if Teilhard were right, the lifework "of Huxley, Dobzhansky, and hundreds of others was not only wrong, but meaningless", and was mystified by their public support for him. He considered Teilhard a friend and his work in paleontology extensive and important, but expressed strongly adverse views of his contributions as scientific theorist and philosopher.

David Sloan Wilson, an evolutionary biologist, wrote that Teilhard has been "largely forgotten as a scientist" but believes that he remains "amazingly relevan[t]" and anticipated his own work in multilevel selection theory.

Brian Swimme wrote "Teilhard was one of the first scientists to realize that the human and the universe are inseparable. The only universe we know about is a universe that brought forth the human." 

Pierre Teilhard de Chardin is honored with a feast day on the liturgical calendar of the Episcopal Church (USA) on 10 April. George Gaylord Simpson named the most primitive and ancient genus of true primate, the Eocene genus "Teilhardina".

Teilhard and his work continue to influence the arts and culture. Characters based on Teilhard appear in several novels, including Jean Telemond in Morris West's "The Shoes of the Fisherman" (mentioned by name and quoted by Oskar Werner playing Fr. Telemond in the movie version of the novel) and Father Lankester Merrin in William Peter Blatty's "The Exorcist". In Dan Simmons' 1989–97 "Hyperion Cantos", Teilhard de Chardin has been canonized a saint in the far future. His work inspires the anthropologist priest character, Paul Duré. When Duré becomes Pope, he takes "Teilhard I" as his regnal name. Teilhard appears as a minor character in the play "Fake" by Eric Simonson, staged by Chicago's Steppenwolf Theatre Company in 2009, involving a fictional solution to the infamous Piltdown Man hoax.

References range from occasional quotations—an auto mechanic quotes Teilhard in Philip K. Dick's "A Scanner Darkly"—to serving as the philosophical underpinning of the plot, as Teilhard's work does in Julian May's 1987–94 Galactic Milieu Series. Teilhard also plays a major role in Annie Dillard's 1999 "For the Time Being". Teilhard is mentioned by name and the Omega Point briefly explained in Arthur C. Clarke's and Stephen Baxter's "The Light of Other Days".
The title of the short-story collection "Everything That Rises Must Converge" by Flannery O'Connor is a reference to Teilhard's work. The American novelist Don DeLillo's 2010 novel "Point Omega" borrows its title and some of its ideas from Teilhard de Chardin. Robert Wright, in his book "", compares his own naturalistic thesis that biological and cultural evolution are directional and, possibly, purposeful, with Teilhard's ideas.

Teilhard's work also inspired philosophical ruminations by Italian laureate architect Paolo Soleri, artworks such as French painter Alfred Manessier's "L'Offrande de la terre ou Hommage à Teilhard de Chardin" and American sculptor Frederick Hart's acrylic sculpture "The Divine Milieu: Homage to Teilhard de Chardin". A sculpture of the Omega Point by Henry Setter, with a quote from Teilhard de Chardin, can be found at the entrance to the Roesch Library at the University of Dayton. The Spanish painter Salvador Dali was fascinated by Teilhard de Chardin and the Omega Point theory. His 1959 painting The Ecumenical Council (painting) is said to represent the "interconnectedness" of the Omega Point. Edmund Rubbra's 1968 Symphony No. 8 is titled "Hommage à Teilhard de Chardin".

Several college campuses honor Teilhard. A building at the University of Manchester is named after him, as are residence dormitories at Gonzaga University and Seattle University.

"The De Chardin Project", a play celebrating Teilhard's life, ran from 20 November to 14 December 2014 in Toronto, Canada. "The Evolution of Teilhard de Chardin", a documentary film on Teilhard's life, was scheduled for released in 2015.

Founded in 1978, George Addair based much of Omega Vector on Teilhard's work.

The American physicist Frank J. Tipler has further developed Teilhard's Omega Point concept in two controversial books, The Physics of Immortality and the more theologically based Physics of Christianity. While keeping the central premise of Teilhard's Omega Point (i.e. a universe evolving towards a maximum state of complexity and consciousness) Tipler has supplanted some of the more mystical/ theological elements of the OPT with his own scientific and mathematical observations (as well as some elements borrowed from Freeman Dyson's eternal intelligence theory). 
In 1972, the Uruguayan priest Juan Luis Segundo, in his five-volume series "A Theology for Artisans of a New Humanity," wrote that Teilhard "noticed the profound analogies existing between the conceptual elements used by the natural sciences — all of them being based on the hypothesis of a general evolution of the universe."

The philosopher Dietrich von Hildebrand criticized severely the work of Teilhard. According to this philosopher, in a conversation after a lecture by Teilhard: "He (Teilhard) ignored completely the decisive difference between nature and supernature. After a lively discussion in which I ventured a criticism of his ideas, I had an opportunity to speak to Teilhard privately. When our talk touched on St. Augustine, he exclaimed violently: 'Don’t mention that unfortunate man; he spoiled everything by introducing the supernatural.'" 
Von Hildebrand writes that Theihardism is incompatible with Christianity, substitutes efficiency for sanctity, deshumanizes man, and describes love as merely cosmic energy.

Teilhard has had a profound influence on the New Age movement and has been described as "perhaps the man most responsible for the spiritualization of evolution in a global and cosmic context". New Age figure and self-described evolutionary biologist Jeremy Griffith described Teilhard as a "visionary" philosopher and a contemporary "truth-sayer" or "prophet".

Teilhard’s quote on likening the discovery of the power of love to the second time man will have discovered the power of fire was quoted in the sermon of the Most Reverend Michael Curry, Presiding Bishop of the Episcopal Church, during the wedding of Prince Harry to Meghan Markle on May 20. 2018.

The dates in parentheses are the dates of first publication in French and English. Most of these works were written years earlier, but Teilhard's ecclesiastical order forbade him to publish them because of their controversial nature. The essay collections are organized by subject rather than date, thus each one typically spans many years.






</doc>
<doc id="23661" url="https://en.wikipedia.org/wiki?curid=23661" title="Phutball">
Phutball

Phutball (short for Philosopher's Football) is a two-player strategy board game described in Elwyn Berlekamp, John Horton Conway, and Richard K. Guy's "Winning Ways for your Mathematical Plays".

Phutball is played on the intersections of a 19×15 grid using one white stone and as many black stones as needed. 
In this article the two players are named Ohs (O) and Eks (X).
The board is labeled A through P (omitting I) from left to right and 1 to 19 from bottom to top from Ohs' perspective. 
Rows 0 and 20 represent "off the board" beyond rows 1 and 19 respectively.

As specialized phutball boards are hard to come by, the game is usually played on a 19×19 Go board, with a white stone representing the football and black stones representing the men.

The objective is to score goals by using the men (the black stones) to move the football (the white stone) onto or over the opponent's goal line. Ohs tries to move the football to rows 19 or 20 and Eks to rows 1 or 0. 
At the start of the game the football is placed on the central point, unless one player gives the other a handicap, in which case the ball starts nearer one player's goal.

Players alternate making moves. 
A move is either to add a man to any vacant point on the board or to move the ball. 
There is no difference between men played by Ohs and those played by Eks.

The football is moved by a series of jumps over adjacent men. 
Each jump is to the first vacant point in a straight line horizontally, vertically, or diagonally over one or more men. 
The jumped men are then removed from the board (before any subsequent jump occurs). 
This process repeats for as long as there remain men available to be jumped and the player desires. Jumping is optional: there is no requirement to jump. 
In contrast to checkers, multiple men in a row are jumped and removed as a group.

The diagram on the right illustrates a jump. 

If the football ends the move on or over the opponent's goal line then a goal has been scored. 
If the football passes through a goal line, but ends up elsewhere due to further jumps, the game continues.

The game is sufficiently complex that checking whether there is a win in one (on an m×n board) is NP-complete. It is not known whether any player has a winning strategy or both players have a drawing strategy.
Given an arbitrary board position, with initially a black stone placed in the center, determining whether the current player has a winning strategy is PSPACE-hard.




</doc>
<doc id="23664" url="https://en.wikipedia.org/wiki?curid=23664" title="Papyrus">
Papyrus

Papyrus is a material similar to thick paper that was used in ancient times as a writing surface. It was made from the pith of the papyrus plant, "Cyperus papyrus", a wetland sedge. "Papyrus" (plural: "papyri") can also refer to a document written on sheets of such material, joined together side by side and rolled up into a scroll, an early form of a book. 
Papyrus is first known to have been used in ancient Egypt (at least as far back as the First Dynasty), as the papyrus plant was once abundant across the Nile Delta. It was also used throughout the Mediterranean region and in the Kingdom of Kush. Apart from a writing material, ancient Egyptians employed papyrus in the construction of other artifacts, such as reed boats, mats, rope, sandals, and baskets.

Papyrus was first manufactured in Egypt as far back as the fourth millennium BCE. The earliest archaeological evidence of papyrus was excavated in 2012 and 2013 at Wadi al-Jarf, an ancient Egyptian harbor located on the Red Sea coast. These documents date from c. 2560–2550 BCE (end of the reign of Khufu). The papyrus rolls describe the last years of building the Great Pyramid of Giza. In the first centuries BCE and CE, papyrus scrolls gained a rival as a writing surface in the form of parchment, which was prepared from animal skins. Sheets of parchment were folded to form quires from which book-form codices were fashioned. Early Christian writers soon adopted the codex form, and in the Græco-Roman world, it became common to cut sheets from papyrus rolls to form codices.

Codices were an improvement on the papyrus scroll, as the papyrus was not pliable enough to fold without cracking and a long roll, or scroll, was required to create large-volume texts. Papyrus had the advantage of being relatively cheap and easy to produce, but it was fragile and susceptible to both moisture and excessive dryness. Unless the papyrus was of perfect quality, the writing surface was irregular, and the range of media that could be used was also limited.

Papyrus was replaced in Europe by the cheaper, locally produced products parchment and vellum, of significantly higher durability in moist climates, though Henri Pirenne's connection of its disappearance with the Muslim conquest of Egypt is contested. Its last appearance in the Merovingian chancery is with a document of 692, though it was known in Gaul until the middle of the following century. The latest certain dates for the use of papyrus are 1057 for a papal decree (typically conservative, all papal bulls were on papyrus until 1022), under Pope Victor II, and 1087 for an Arabic document. Its use in Egypt continued until it was replaced by more inexpensive paper introduced by Arabs who originally learned of it from the Chinese. By the 12th century, parchment and paper were in use in the Byzantine Empire, but papyrus was still an option.

Papyrus was made in several qualities and prices. Pliny the Elder and Isidore of Seville described six variations of papyrus which were sold in the Roman market of the day. These were graded by quality based on how fine, firm, white, and smooth the writing surface was. Grades ranged from the superfine Augustan, which was produced in sheets of 13 digits (10 inches) wide, to the least expensive and most coarse, measuring six digits (four inches) wide. Materials deemed unusable for writing or less than six digits were considered commercial quality and were pasted edge to edge to be used only for wrapping.

Until the middle of the 19th century, only some isolated documents written on papyrus were known, and that museums simply displayed them as curiosities. They did not contain literary works. The first modern discovery of papyri rolls was made at Herculaneum in 1752. Until then, the only papyri known had been a few surviving from medieval times. Scholarly investigations began with the Dutch historian Caspar Jacob Christiaan Reuvens (1793–1835). He wrote about the content of the Leyden papyrus, published in 1830. The first publication has been credited to the British scholar Charles Wycliffe Goodwin (1817–1878), who published for the Cambridge Antiquarian Society, one of the Papyri Graecae Magicae V, translated into English with commentary in 1853.

The English word "papyrus" derives, via Latin, from Greek πάπυρος ("papyros"), a loanword of unknown (perhaps Pre-Greek) origin. Greek has a second word for it, βύβλος ("byblos", said to derive from the name of the Phoenician city of Byblos). The Greek writer Theophrastus, who flourished during the 4th century BCE, uses "papyros" when referring to the plant used as a foodstuff and "byblos" for the same plant when used for nonfood products, such as cordage, basketry, or writing surfaces. The more specific term βίβλος "biblos", which finds its way into English in such words as 'bibliography', 'bibliophile', and 'bible', refers to the inner bark of the papyrus plant. "Papyrus" is also the etymon of 'paper', a similar substance.

In the Egyptian language, papyrus was called "wadj" ("w3ḏ"), "tjufy" ("ṯwfy"), or "djet" ("ḏt").

The word for the material papyrus is also used to designate documents written on sheets of it, often rolled up into scrolls. The plural for such documents is papyri. Historical papyri are given identifying names—generally the name of the discoverer, first owner or institution where they are kept—and numbered, such as "Papyrus Harris I". Often an abbreviated form is used, such as "pHarris I". These documents provide important information on ancient writings; they give us the only extant copy of Menander, the Egyptian Book of the Dead, Egyptian treatises on medicine (the Ebers Papyrus) and on surgery (the Edwin Smith papyrus), Egyptian mathematical treatises (the Rhind papyrus), and Egyptian folk tales (the Westcar papyrus). When, in the 18th century, a library of ancient papyri was found in Herculaneum, ripples of expectation spread among the learned men of the time. However, since these papyri were badly charred, their unscrolling and deciphering is still going on today.

Papyrus is made from the stem of the papyrus plant, "Cyperus papyrus". The outer rind is first removed, and the sticky fibrous inner pith is cut lengthwise into thin strips of about long. The strips are then placed side by side on a hard surface with their edges slightly overlapping, and then another layer of strips is laid on top at a right angle. The strips may have been soaked in water long enough for decomposition to begin, perhaps increasing adhesion, but this is not certain. The two layers possibly were glued together. While still moist, the two layers are hammered together, mashing the layers into a single sheet. The sheet is then dried under pressure. After drying, the sheet is polished with some rounded object, possibly a stone or seashell or round hardwood.

Sheets could be cut to fit the obligatory size or glued together to create a longer roll. A wooden stick would be attached to the last sheet in a roll, making it easier to handle. To form the long strip scrolls required, a number of such sheets were united, placed so all the horizontal fibres parallel with the roll's length were on one side and all the vertical fibres on the other. Normally, texts were first written on the "recto", the lines following the fibres, parallel to the long edges of the scroll. Secondarily, papyrus was often reused, writing across the fibres on the "verso". Pliny the Elder describes the methods of preparing papyrus in his "Naturalis Historia".

In a dry climate, like that of Egypt, papyrus is stable, formed as it is of highly rot-resistant cellulose; but storage in humid conditions can result in molds attacking and destroying the material. Library papyrus rolls were stored in wooden boxes and chests made in the form of statues. Papyrus scrolls were organized according to subject or author, and identified with clay labels that specified their contents without having to unroll the scroll. In European conditions, papyrus seems to have lasted only a matter of decades; a 200-year-old papyrus was considered extraordinary. Imported papyrus once commonplace in Greece and Italy has since deteriorated beyond repair, but papyri are still being found in Egypt; extraordinary examples include the Elephantine papyri and the famous finds at Oxyrhynchus and Nag Hammadi. The Villa of the Papyri at Herculaneum, containing the library of Lucius Calpurnius Piso Caesoninus, Julius Caesar's father-in-law, was preserved by the eruption of Mount Vesuvius, but has only been partially excavated.

Sporadic attempts to revive the manufacture of papyrus have been made since the mid-18th century. Scottish explorer James Bruce experimented in the late 18th century with papyrus plants from the Sudan, for papyrus had become extinct in Egypt. Also in the 18th century, Sicilian Saverio Landolina manufactured papyrus at Syracuse, where papyrus plants had continued to grow in the wild. During the 1920s, when Egyptologist Battiscombe Gunn lived in Maadi, outside Cairo, he experimented with the manufacture of papyrus, growing the plant in his garden. He beat the sliced papyrus stalks between two layers of linen, and produced successful examples of papyrus, one of which was exhibited in the Egyptian Museum in Cairo. The modern technique of papyrus production used in Egypt for the tourist trade was developed in 1962 by the Egyptian engineer Hassan Ragab using plants that had been reintroduced into Egypt in 1872 from France. Both Sicily and Egypt have centres of limited papyrus production.

Papyrus is still used by communities living in the vicinity of swamps, to the extent that rural householders derive up to 75% of their income from swamp goods. Particularly in East and Central Africa, people harvest papyrus, which is used to manufacture items that are sold or used locally. Examples include baskets, hats, fish traps, trays or winnowing mats, and floor mats. Papyrus is also used to make roofs, ceilings, rope and fences. Although alternatives, such as eucalyptus, are increasingly available, papyrus is still used as fuel.



Other ancient writing materials:




</doc>
<doc id="23665" url="https://en.wikipedia.org/wiki?curid=23665" title="Pixel">
Pixel

In digital imaging, a pixel, pel, dots, or picture element is a physical point in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

Each pixel is a sample of an original image; more samples typically provide more accurate representations of the original. The intensity of each pixel is variable. In color imaging systems, a color is typically represented by three or four component intensities such as red, green, and blue, or cyan, magenta, yellow, and black.

In some contexts (such as descriptions of camera sensors), "pixel" refers to a single scalar element of a multi-component representation (called a "photosite" in the camera sensor context, although "sensel" is sometimes used), while in yet other contexts it may refer to the set of component intensities for a spatial position. 

The word "pixel" is a portmanteau of "pix" (from "pictures", shortened to "pics") and "el" (for "element"); similar formations with '"el"' include the words "voxel" and "texel".

The word "pixel" was first published in 1965 by Frederic C. Billingsley of JPL, to describe the picture elements of video images from space probes to the Moon and Mars. Billingsley had learned the word from Keith E. McFarland, at the Link Division of General Precision in Palo Alto, who in turn said he did not know where it originated. McFarland said simply it was "in use at the time" (circa 1963).

The word is a combination of "pix", for picture, and "element". The word "pix" appeared in "Variety" magazine headlines in 1932, as an abbreviation for the word "pictures", in reference to movies. By 1938, "pix" was being used in reference to still pictures by photojournalists.

The concept of a "picture element" dates to the earliest days of television, for example as ""Bildpunkt"" (the German word for "pixel", literally 'picture point') in the 1888 German patent of Paul Nipkow. According to various etymologies, the earliest publication of the term "picture element" itself was in "Wireless World" magazine in 1927, though it had been used earlier in various U.S. patents filed as early as 1911.

Some authors explain "pixel" as "picture cell," as early as 1972. In graphics and in image and video processing, "pel" is often used instead of "pixel". For example, IBM used it in their Technical Reference for the original PC.

Pixels, abbreviated as 'px,' are also a unit of measurement commonly used in graphic and web design, equivalent to roughly . This measurement is used to make sure a given element will display as the same size no matter what screen resolution views it.

Pixilation, spelled with a second "i", is an unrelated filmmaking technique that dates to the beginnings of cinema, in which live actors are posed frame by frame and photographed to create stop-motion animation. An archaic British word meaning "possession by spirits (pixies)", the term has been used to describe the animation process since the early 1950s; various animators, including Norman McLaren and Grant Munro, are credited with popularizing it.

 thought of as the smallest single component of a digital image. However, the definition is highly context-sensitive. For example, there can be "printed pixels" in a page, or pixels carried by electronic signals, or represented by digital values, or pixels on a display device, or pixels in a digital camera (photosensor elements). This list is not exhaustive and, depending on context, synonyms include pel, sample, byte, bit, dot, and spot. "Pixels" can be used as a unit of measure such as: 2400 pixels per inch, 640 pixels per line, or spaced 10 pixels apart.

The measures dots per inch (dpi) and pixels per inch (ppi) are sometimes used interchangeably, but have distinct meanings, especially for printer devices, where dpi is a measure of the printer's density of dot (e.g. ink droplet) placement. For example, a high-quality photographic image may be printed with 600 ppi on a 1200 dpi inkjet printer. Even higher dpi numbers, such as the 4800 dpi quoted by printer manufacturers since 2002, do not mean much in terms of achievable resolution.

The more pixels used to represent an image, the closer the result can resemble the original. The number of pixels in an image is sometimes called the resolution, though resolution has a more specific definition. Pixel counts can be expressed as a single number, as in a "three-megapixel" digital camera, which has a nominal three million pixels, or as a pair of numbers, as in a "640 by 480 display", which has 640 pixels from side to side and 480 from top to bottom (as in a VGA display), and therefore has a total number of 640×480 = 307,200 pixels or 0.3 megapixels.

The pixels, or color samples, that form a digitized image (such as a JPEG file used on a web page) may or may not be in one-to-one correspondence with screen pixels, depending on how a computer displays an image. In computing, an image composed of pixels is known as a "bitmapped image" or a "raster image". The word "raster" originates from television scanning patterns, and has been widely used to describe similar halftone printing and storage techniques.

For convenience, pixels are normally arranged in a regular two-dimensional grid. By using this arrangement, many common operations can be implemented by uniformly applying the same operation to each pixel independently. Other arrangements of pixels are possible, with some sampling patterns even changing the shape (or kernel) of each pixel across the image. For this reason, care must be taken when acquiring an image on one device and displaying it on another, or when converting image data from one pixel format to another.

For example:


Computers can use pixels to display an image, often an abstract image that represents a GUI. The resolution of this image is called the display resolution and is determined by the video card of the computer. LCD monitors also use pixels to display an image, and have a native resolution. Each pixel is made up of triads, with the number of these triads determining the native resolution. On some CRT monitors, the beam sweep rate may be fixed, resulting in a fixed native resolution. Most CRT monitors do not have a fixed beam sweep rate, meaning they do not have a native resolution at all - instead they have a set of resolutions that are equally well supported.
To produce the sharpest images possible on an LCD, the user must ensure the display resolution of the computer matches the native resolution of the monitor.

The pixel scale used in astronomy is the angular distance between two objects on the sky that fall one pixel apart on the detector (CCD or infrared chip). The scale "s" measured in radians is the ratio of the pixel spacing "p" and focal length "f" of the preceding optics, "s"="p/f". (The focal length is the product of the focal ratio by the diameter of the associated lens or mirror.)
Because "p" is usually expressed in units of arcseconds per pixel, because 1 radian equals "180/π*3600≈206,265" arcseconds, and because diameters are often given in millimeters and pixel sizes in micrometers which yields another factor of 1,000, the formula is often quoted as "s=206p/f".

The number of distinct colors that can be represented by a pixel depends on the number of bits per pixel (bpp). A 1 bpp image uses 1-bit for each pixel, so each pixel can be either on or off. Each additional bit doubles the number of colors available, so a 2 bpp image can have 4 colors, and a 3 bpp image can have 8 colors:

For color depths of 15 or more bits per pixel, the depth is normally the sum of the bits allocated to each of the red, green, and blue components. Highcolor, usually meaning 16 bpp, normally has five bits for red and blue each, and six bits for green, as the human eye is more sensitive to errors in green than in the other two primary colors. For applications involving transparency, the 16 bits may be divided into five bits each of red, green, and blue, with one bit left for transparency. A 24-bit depth allows 8 bits per component. On some systems, 32-bit depth is available: this means that each 24-bit pixel has an extra 8 bits to describe its opacity (for purposes of combining with another image).

Many display and image-acquisition systems are, for various reasons, not capable of displaying or sensing the different color channels at the same site. Therefore, the pixel grid is divided into single-color regions that contribute to the displayed or sensed color when viewed at a distance. In some displays, such as LCD, LED, and plasma displays, these single-color regions are separately addressable elements, which have come to be known as subpixels. For example, LCDs typically divide each pixel vertically into three subpixels. When the square pixel is divided into three subpixels, each subpixel is necessarily rectangular. In display industry terminology, subpixels are often referred to as "pixels", as they are the basic addressable elements in a viewpoint of hardware, and hence "pixel circuits" rather than "subpixel circuits" is used.

Most digital camera image sensors use single-color sensor regions, for example using the Bayer filter pattern, and in the camera industry these are known as "pixels" just like in the display industry, not "subpixels".

For systems with subpixels, two different approaches can be taken:

This latter approach, referred to as subpixel rendering, uses knowledge of pixel geometry to manipulate the three colored subpixels separately, producing an increase in the apparent resolution of color displays. While CRT displays use red-green-blue-masked phosphor areas, dictated by a mesh grid called the shadow mask, it would require a difficult calibration step to be aligned with the displayed pixel raster, and so CRTs do not currently use subpixel rendering.

The concept of subpixels is related to samples.

A megapixel (MP) is a million pixels; the term is used not only for the number of pixels in an image, but also to express the number of image sensor elements of digital cameras or the number of display elements of digital displays. For example, a camera that makes a 2048×1536 pixel image (3,145,728 finished image pixels) typically uses a few extra rows and columns of sensor elements and is commonly said to have "3.2 megapixels" or "3.4 megapixels", depending on whether the number reported is the "effective" or the "total" pixel count.

Digital cameras use photosensitive electronics, either charge-coupled device (CCD) or complementary metal–oxide–semiconductor (CMOS) image sensors, consisting of a large number of single sensor elements, each of which records a measured intensity level. In most digital cameras, the sensor array is covered with a patterned color filter mosaic having red, green, and blue regions in the Bayer filter arrangement, so that each sensor element can record the intensity of a single primary color of light. The camera interpolates the color information of neighboring sensor elements, through a process called demosaicing, to create the final image. These sensor elements are often called "pixels", even though they only record 1 channel (only red, or green, or blue) of the final color image. Thus, two of the three color channels for each sensor must be interpolated and a so-called "N-megapixel" camera that produces an N-megapixel image provides only one-third of the information that an image of the same size could get from a scanner. Thus, certain color contrasts may look fuzzier than others, depending on the allocation of the primary colors (green has twice as many elements as red or blue in the Bayer arrangement).

DxO Labs invented the Perceptual MegaPixel (P-MPix) to measure the sharpness that a camera produces when paired to a particular lens – as opposed to the MP a manufacturer states for a camera product which is based only on the camera's sensor. The new P-MPix claims to be a more accurate and relevant value for photographers to consider when weighing-up camera sharpness. As of mid-2013, the Sigma 35mm f/1.4 DG HSM lens mounted on a Nikon D800 has the highest measured P-MPix. However, with a value of 23 MP, it still wipes-off more than one-third of the D800's 36.3 MP sensor.

One new method to add Megapixels has been introduced in a Micro Four Thirds System camera which only uses 16MP sensor, but can produce 64MP RAW (40MP JPEG) by making two exposures, shifting the sensor by a half pixel between them. Using a tripod to take level multi-shots within an instance, the multiple 16MP images are then generated into a unified 64MP image.



</doc>
<doc id="23666" url="https://en.wikipedia.org/wiki?curid=23666" title="Prime number">
Prime number

A prime number (or a prime) is a natural number greater than 1 that cannot be formed by multiplying two smaller natural numbers. A natural number greater than 1 that is not prime is called a composite number. For example, 5 is prime because the only ways of writing it as a product, or , involve 5 itself.
However, 6 is composite because it is the product of two numbers () that are both smaller than 6. Primes are central in number theory because of the fundamental theorem of arithmetic: every natural number greater than 1 is either a prime itself or can be factorized as a product of primes that is unique up to their order.

The property of being prime is called primality. A simple but slow method of checking the primality of a given number formula_1, called trial division, tests whether formula_1 is a multiple of any integer between 2 and formula_3. Faster algorithms include the Miller–Rabin primality test, which is fast but has a small chance of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical. Particularly fast methods are available for numbers of special forms, such as Mersenne numbers. , the largest known prime number has 23,249,425 decimal digits.

There are infinitely many primes, as demonstrated by Euclid around 300 BC. No known simple formula separates prime numbers from composite numbers. However, the distribution of primes within the natural numbers in the large can be statistically modelled. The first result in that direction is the prime number theorem, proven at the end of the 19th century, which says that the probability of a randomly chosen number being prime is inversely proportional to its number of digits, that is, to its logarithm.

Several historical questions regarding prime numbers are still unsolved. These include Goldbach's conjecture, that every even integer greater than 2 can be expressed as the sum of two primes, and the twin prime conjecture, that there are infinitely many pairs of primes having just one even number between them. Such questions spurred the development of various branches of number theory, focusing on analytic or algebraic aspects of numbers. Primes are used in several routines in information technology, such as public-key cryptography, which relies on the difficulty of factoring large numbers into their prime factors. In abstract algebra, objects that behave in a generalized way like prime numbers include prime elements and prime ideals.

A natural number (1, 2, 3, 4, 5, 6, etc.) is called a prime number (or a prime) if it is greater than 1 and cannot be written as a product of two natural numbers that are both smaller than it. The numbers greater than 1 that are not prime are called composite numbers. In other words, formula_1 is prime if formula_1 items cannot be divided up into smaller equal-size groups of more than one item, or if it is not possible to arrange formula_1 dots into a rectangular grid that is more than one dot wide and more than one dot high.
For example, among the numbers 1 through 6, the numbers 2, 3, and 5 are the prime numbers, as there are no other numbers that divide them evenly (without a remainder).
1 is not prime, as it is specifically excluded in the definition. and are both composite.
The divisors of a natural number formula_1 are the numbers that divide formula_1 evenly.
Every natural number has both 1 and itself as a divisor. If it has any other divisor, it cannot be prime. This idea leads to a different but equivalent definition of the primes: they are the numbers with exactly two positive divisors, 1 and the number itself.
Yet another way to say the same thing is that a number formula_1 is prime if it is greater than one and if none of the numbers formula_10 divides formula_1 evenly.

The first 25 prime numbers (all the prime numbers less than 100) are:

No even number formula_1 greater than 2 is prime because any such number can be expressed as the product formula_13. Therefore, every prime number other than 2 is an odd number, and is called an "odd prime". Similarly, when written in the usual decimal system, all prime numbers larger than 5 end in 1, 3, 7, or 9. The numbers that end with other digits are all composite:
decimal numbers that end in 0, 2, 4, 6, or 8 are even, and decimal numbers that end in 0 or 5 are divisible by 5.

The set of all primes is sometimes denoted by formula_14 (a boldface capital P) or by formula_15 (a blackboard bold capital P).

The Rhind Mathematical Papyrus, from around 1550 BC, has Egyptian fraction expansions of different forms for prime and composite numbers. However, the earliest surviving records of the explicit study of prime numbers come from Ancient Greek mathematics. Euclid's "Elements" (circa 300 BC) proves the infinitude of primes and the fundamental theorem of arithmetic, and shows how to construct a perfect number from a Mersenne prime. Another Greek invention, the Sieve of Eratosthenes, is still used to construct lists of primes.
Around 1000 AD, the Islamic mathematician Alhazen found Wilson's theorem, characterizing the prime numbers as the numbers formula_1 that evenly divide formula_17. Alhazen also conjectured that all even perfect numbers come from Euclid's construction using Mersenne primes, but was unable to prove it. Another Islamic mathematician, Ibn al-Banna' al-Marrakushi, observed that the sieve of Eratosthenes can be sped up by testing only the divisors up to the square root of the largest number to be tested. Fibonacci brought the innovations from Islamic mathematics back to Europe. His book "Liber Abaci" (1202) was the first to describe trial division for testing primality, again using divisors only up to the square root.

In 1640 Pierre de Fermat stated (without proof) Fermat's little theorem (later proved by Leibniz and Euler). Fermat also investigated the primality of the Fermat numbers 
formula_18, and Marin Mersenne studied the Mersenne primes, prime numbers of the form formula_19 with formula_20 itself a prime. Christian Goldbach formulated Goldbach's conjecture, that every even number is the sum of two primes, in a 1742 letter to Euler. Euler proved Alhazen's conjecture (now the Euclid–Euler theorem) that all even perfect numbers can be constructed from Mersenne primes. He introduced methods from mathematical analysis to this area in his proofs of the infinitude of the primes and the divergence of the sum of the reciprocals of the primes formula_21.
At the start of the 19th century, Legendre and Gauss conjectured that as formula_22 tends to infinity, the number of primes up to formula_22 is asymptotic to formula_24, where formula_25 is the natural logarithm of formula_22. Ideas of Riemann in his 1859 paper on the zeta-function sketched an outline for proving this. Although the closely related Riemann hypothesis remains unproven, Riemann's outline was completed in 1896 by Hadamard and de la Vallée Poussin, and the result is now known as the prime number theorem. Another important 19th-century result was Dirichlet's theorem on arithmetic progressions, that certain arithmetic progressions contain infinitely many primes.

Many mathematicians have worked on primality tests for numbers larger than those where trial division is practicably applicable. Methods that are restricted to specific number forms include Pépin's test for Fermat numbers (1877), Proth's theorem (around 1878), the Lucas–Lehmer primality test (originated 1856), and the generalized Lucas primality test.
Since 1951 all the largest known primes have been found using these tests on computers. The search for ever larger primes has generated interest outside mathematical circles, through the Great Internet Mersenne Prime Search and other distributed computing projects. The idea that prime numbers had few applications outside of pure mathematics was shattered in the 1970s when public-key cryptography and the RSA cryptosystem were invented, using prime numbers as their basis.
The increased practical importance of computerized primality testing and factorization led to the development of improved methods capable of handling large numbers of unrestricted form. The mathematical theory of prime numbers also moved forward with the Green–Tao theorem (2004) on long arithmetic progressions of prime numbers, and Yitang Zhang's 2013 proof that there exist infinitely many prime gaps of bounded size.

Most early Greeks did not even consider 1 to be a number, so they could not consider its primality. A few mathematicians from this time also considered the prime numbers to be a subdivision of the odd numbers, so they also did not consider 2 to be prime. However, Euclid and a majority of the other Greek mathematicians considered 2 as prime. The medieval Islamic mathematicians largely followed the Greeks in viewing 1 as not being a number.
By the Middle Ages and Renaissance mathematicians began treating 1 as a number, and some of them included it as the first prime number. In the mid-18th century Christian Goldbach listed 1 as prime in his correspondence with Leonhard Euler; however, Euler himself did not consider 1 to be prime. In the 19th century many mathematicians still considered 1 to be prime, and lists of primes that included 1 continued to be published as recently as 1956.

If the definition of a prime number were changed to call 1 a prime, many statements involving prime numbers would need to be reworded in a more awkward way. For example, the fundamental theorem of arithmetic would need to be rephrased in terms of factorizations into primes greater than 1, because every number would have multiple factorizations with different numbers of copies of 1. Similarly, the sieve of Eratosthenes would not work correctly if it handled 1 as a prime, because it would eliminate all multiples of 1 (that is, all other numbers) and output only the single number 1. Some other more technical properties of prime numbers also do not hold for the number 1: for instance, the formulas for Euler's totient function or for the sum of divisors function are different for prime numbers than they are for 1. By the early 20th century, mathematicians began to agree that 1 should not be listed as prime, but rather in its own special category as a "unit".

Writing a number as a product of prime numbers is called a "prime factorization" of the number. For example:
The terms in the product are called prime factors. The same prime factor may occur more than once; this example has two copies of the prime factor formula_28.
When a prime occurs multiple times, exponentiation can be used to group together multiple copies of the same prime number: for instance, in the second way of writing the product above, formula_29 denotes the square or second power of formula_28.

The central importance of prime numbers to number theory and mathematics in general stems from the "fundamental theorem of arithmetic". This theorem states that every integer larger than 1 can be written as a product of one or more primes. More strongly, 
this product is unique in the sense that any two prime factorizations of the same number will have the same numbers of copies of the same primes,
although their ordering may differ. So, although there are many different ways of finding a factorization using an integer factorization algorithm, they all must produce the same result. Primes can thus be considered the "basic building blocks" of the natural numbers.

Some proofs of the uniqueness of prime factorizations are based on Euclid's lemma: If formula_20 is a prime number and formula_20 divides a product formula_33 of integers formula_34 and formula_35, then formula_20 divides formula_34 or formula_20 divides formula_35 (or both). Conversely, if a number formula_20 has the property that when it divides a product it always divides at least one factor of the product, then formula_20 must be prime.

There are infinitely many prime numbers. Another way of saying this is that the sequence
of prime numbers never ends. This statement is referred to as "Euclid's theorem" in honor of the ancient Greek mathematician Euclid, since the first known proof for this statement is attributed to him. Many more proofs of the infinitude of primes are known, including an analytical proof by Euler, Goldbach's proof based on Fermat numbers, Furstenberg's proof using general topology, and Kummer's elegant proof.

Euclid's proof shows that every finite list of primes is incomplete. The key idea is to multiply together the primes in any given list and add formula_42. If the list consists of the primes
formula_43, this gives the number
By the fundamental theorem, formula_45 has a prime factorization
with one or more prime factors. formula_45 is evenly divisible by each of these factors, but formula_45 has a remainder of one when divided by any of the prime numbers in the given list, so none of the prime factors of formula_45 can be in the given list. Because there is no finite list of all the primes, there must be infinitely many primes.

The numbers formed by adding one to the products of the smallest primes are called Euclid numbers. The first five of them are prime, but the sixth,
is a composite number.

There is no known efficient formula for primes. For example, there is no non-constant polynomial, even in several variables, that takes "only" prime values. However, there are numerous expressions that do encode all primes, or only primes. One possible formula is based on Wilson's theorem and generates the number 2 many times and all other primes exactly once. There is also a set of Diophantine equations in nine variables and one parameter with the following property: the parameter is prime if and only if the resulting system of equations has a solution over the natural numbers. This can be used to obtain a single formula with the property that all its "positive" values are prime.

Other examples of prime-generating formulas come from Mills' theorem and a theorem of Wright. These assert that there are real constants formula_51 and formula_52 such that
are prime for any natural number formula_1 in the first formula, and any number of exponents in the second formula. Here formula_55 represents the floor function, the largest integer less than or equal to the number in question. However, these are not useful for generating primes, as the primes must be generated first in order to compute the values of formula_56 or formula_52.

Many conjectures revolving about primes have been posed. Often having an elementary formulation, many of these conjectures have withstood proof for decades: all four of Landau's problems from 1912 are still unsolved. One of them is Goldbach's conjecture, which asserts that every even integer formula_1 greater than 2 can be written as a sum of two primes. , this conjecture has been verified for all numbers up to formula_59. Weaker statements than this have been proven, for example, Vinogradov's theorem says that every sufficiently large odd integer can be written as a sum of three primes. Chen's theorem says that every sufficiently large even number can be expressed as the sum of a prime and a semiprime, the product of two primes. Also, any even integer can be written as the sum of six primes. The branch of number theory studying such questions is called additive number theory.

Another type of problem concerns prime gaps, the differences between consecutive primes.
The existence of arbitrarily large prime gaps can be seen by noting that the sequence formula_60 consists of formula_61 composite numbers, for any natural number formula_1. However, large prime gaps occur much earlier than this argument shows. For example, the first prime gap of length 8 is between the primes 89 and 97, much smaller than formula_63. It is conjectured that there are infinitely many twin primes, pairs of primes with difference 2; this is the twin prime conjecture. Polignac's conjecture states more generally that for every positive integer formula_64, there are infinitely many pairs of consecutive primes that differ by formula_65.
Andrica's conjecture, Brocard's conjecture, Legendre's conjecture, and Oppermann's conjecture all suggest that the largest gaps between primes from formula_42 to formula_1 should be at most approximately formula_3, a result that is known to follow from the Riemann hypothesis,
while the much stronger Cramér conjecture sets the largest gap size at formula_69.
Prime gaps can be generalized to prime formula_64-tuples, patterns in the differences between more than two prime numbers. Their infinitude and density are the subject of the first Hardy–Littlewood conjecture, which can be motivated by the heuristic that the prime numbers behave similarly to a random sequence of numbers with density given by the prime number theorem.

Analytic number theory studies number theory through the lens of continuous functions, limits, infinite series, and the related mathematics of the infinite and infinitesimal.

This area of study began with Leonhard Euler and his first major result, the solution to the Basel problem.
The problem asked for the value of the infinite sum formula_71
which today can be recognized as the value formula_72 of the Riemann zeta function. This function is closely connected to the prime numbers and to one of the most significant unsolved problems in mathematics, the Riemann hypothesis. Euler showed that formula_73.
The reciprocal of this number, formula_74, is the limiting probability that two random numbers selected uniformly from a large range are relatively prime (have no factors in common).

The distribution of primes in the large, such as the question how many primes are smaller than a given, large threshold, is described by the prime number theorem, but no efficient formula for the formula_1-th prime is known.
Dirichlet's theorem on arithmetic progressions, in its basic form, asserts that linear polynomials
with relatively prime integers formula_34 and formula_35 take infinitely many prime values. Stronger forms of the theorem state that the sum of the reciprocals of these prime values diverges, and that different linear polynomials with the same formula_35 have approximately the same proportions of primes.
Although conjectures have been formulated about the proportions of primes in higher-degree polynomials, they remain unproven, and it is unknown whether there exists a quadratic polynomial that (for integer arguments) is prime infinitely often.

Euler's proof that there are infinitely many primes considers the sums of reciprocals of primes,

Euler showed that, for any arbitrary real number formula_22, there exists a prime formula_20 for which this sum is bigger than formula_22. This shows that there are infinitely many primes, because if there were finitely many primes the sum would reach its maximum value at the biggest prime rather than growing past every formula_22. 
The growth rate of this sum is described more precisely by Mertens' second theorem. For comparison, the sum

does not grow to infinity as formula_1 goes to infinity (see the Basel problem). In this sense, prime numbers occur more often than squares of natural numbers,
although both sets are infinite. Brun's theorem states that the sum of the reciprocals of twin primes,

is finite. Because of Brun's theorem, it is not possible to use Euler's method to solve the twin prime conjecture, that there exist infinitely many twin primes.

The prime counting function formula_88 is defined as the number of primes not greater than formula_1. For example, formula_90, since there are five primes less than or equal to 11. Methods such as the Meissel–Lehmer algorithm can compute exact values of formula_88 faster than it would be possible to list each prime up to formula_1. The prime number theorem states that formula_88 is asymptotic to formula_94, which is denoted as
and means that the ratio of formula_88 to the right-hand fraction approaches 1 as formula_1 grows to infinity. This implies that the likelihood that a randomly chosen number less than formula_1 is prime is (approximately) inversely proportional to the number of digits in formula_1.
It also implies that the formula_1th prime number is proportional to formula_101
and therefore that the average size of a prime gap is proportional to formula_102.
A more accurate estimate for formula_88 is given by the offset logarithmic integral

An arithmetic progression is a finite or infinite sequence of numbers such that consecutive numbers in the sequence all have the same difference. This difference is called the modulus of the progression. For example,
is an infinite arithmetic progression with modulus 9. In an arithmetic progression, all the numbers have the same remainder when divided by the modulus; in this example, the remainder is 3. Because both the modulus 9 and the remainder 3 are multiples of 3, so is every element in the sequence. Therefore, this progression contains only one prime number, 3 itself. In general, the infinite progression
can have more than one prime only when its remainder formula_34 and modulus formula_107 are relatively prime. If they are relatively prime, Dirichlet's theorem on arithmetic progressions asserts that the progression contains infinitely many primes.

The Green–Tao theorem shows that there are arbitrarily long finite arithmetic progressions consisting only of primes.

Euler noted that the function
yields prime numbers for formula_109, although composite numbers appear among its later values. The search for an explanation for this phenomenon led to the deep algebraic number theory of Heegner numbers and the class number problem. The Hardy-Littlewood conjecture F predicts the density of primes among the values of quadratic polynomials with integer coefficients
in terms of the logarithmic integral and the polynomial coefficients. No quadratic polynomial has been proven to take infinitely many prime values.

The Ulam spiral arranges the natural numbers in a two-dimensional grid, spiraling in concentric squares surrounding the origin with the prime numbers highlighted. Visually, the primes appear to cluster on certain diagonals and not others, suggesting that some quadratic polynomials take prime values more often than others.

One of the most famous unsolved questions in mathematics, dating from 1859, and one of the Millennium Prize Problems, is the Riemann hypothesis, which asks where the zeros of the Riemann zeta function formula_110 are located.
This function is an analytic function on the complex numbers. For complex numbers formula_111 with real part greater than one it equals both an infinite sum over all integers, and an infinite product over the prime numbers,
This equality between a sum and a product, discovered by Euler, is called an Euler product. The Euler product can be derived from the fundamental theorem of arithmetic, and shows the close connection between the zeta function and the prime numbers.
It leads to another proof that there are infinitely many primes: if there were only finitely many,
then the sum-product equality would also be valid at formula_113, but the sum would diverge (it is the harmonic series formula_114) while the product would be finite, a contradiction.

The Riemann hypothesis states that the zeros of the zeta-function are all either negative even numbers, or complex numbers with real part equal to 1/2. The original proof of the prime number theorem was based on a weak form of this hypothesis, that there are no zeros with real part equal to 1, although other more elementary proofs have been found.
The prime-counting function can be expressed by Riemann's explicit formula as a sum in which each term comes from one of the zeros of the zeta function; the main term of this sum is the logarithmic integral, and the remaining terms cause the sum to fluctuate above and below the main term.
In this sense, the zeros control how regularly the prime numbers are distributed. If the Riemann hypothesis is true, these fluctuations will be small, and the
asymptotic distribution of primes given by the prime number theorem will also hold over much shorter intervals (of length about the square root of formula_22 for intervals near a number formula_22).

Modular arithmetic modifies usual arithmetic by only using the numbers formula_117, for a natural number formula_1 called the modulus.
Any other natural number can be mapped into this system by replacing it by its remainder after division by formula_1.
Modular sums, differences and products are calculated by performing the same replacement by the remainder
on the result of the usual sum, difference, or product of integers. Equality of integers corresponds to "congruence" in modular arithmetic:
formula_22 and formula_121 are congruent (written formula_122 mod formula_1) when they have the same remainder after division by formula_1. However, in this system of numbers, division by all nonzero numbers is possible if and only if the modulus is prime. For instance, with the prime number formula_125 as modulus, division by formula_28 is possible: formula_127, because clearing denominators by multiplying both sides by formula_28 gives the valid formula formula_129. However, with the composite modulus formula_130, division by formula_28 is impossible. There is no valid solution to formula_132: clearing denominators by multiplying by formula_28 causes the left-hand side to become formula_134 while the right-hand side becomes either formula_135 or formula_28.
In the terminology of abstract algebra, the ability to perform division means that modular arithmetic modulo a prime number forms a field or, more specifically, a finite field, while other moduli only give a ring but not a field.

Several theorems about primes can be formulated using modular arithmetic. For instance, Fermat's little theorem states that if
formula_137 (mod formula_20), then formula_139 (mod formula_20).
Summing this over all choices of formula_34 gives the equation
valid whenever formula_20 is prime.
Giuga's conjecture says that this equation is also a sufficient condition for formula_20 to be prime.
Wilson's theorem says that an integer formula_145 is prime if and only if the factorial formula_146 is congruent to formula_147 mod formula_20. For a composite this cannot hold, since one of its factors divides both and formula_149, and so formula_150 is impossible.

The formula_20-adic order formula_152 of an integer formula_1 is the number of copies of formula_20 in the prime factorization of formula_1. The same concept can be extended from integers to rational numbers by defining the formula_20-adic order of a fraction formula_157 to be formula_158. The formula_20-adic absolute value formula_160 of any rational number formula_107 is then defined as
formula_162. Multiplying an integer by its formula_20-adic absolute value cancels out the factors of formula_20 in its factorization, leaving only the other primes. Just as the distance between two real numbers can be measured by the absolute value of their distance, the distance between two rational numbers can be measured by their formula_20-adic distance, the formula_20-adic absolute value of their difference. For this definition of distance, two numbers are close together (they have a small distance) when their difference is divisible by a high power of formula_20. In the same way that the real numbers can be formed from the rational numbers and their distances, by adding extra limiting values to form a complete field, the rational numbers with the formula_20-adic distance can be extended to a different complete field, the formula_20-adic numbers.

This picture of an order, absolute value, and complete field derived from them can be generalized to algebraic number fields and their valuations (certain mappings from the multiplicative group of the field to a totally ordered additive group, also called orders), absolute values (certain multiplicative mappings from the field to the real numbers, also called norms), and places (extensions to complete fields in which the given field is a dense set, also called completions). The extension from the rational numbers to the real numbers, for instance, is a place in which the distance between numbers is the usual absolute value of their difference. The corresponding mapping to an additive group would be the logarithm of the absolute value, although this does not meet all the requirements of a valuation. According to Ostrowski's theorem, up to a natural notion of equivalence, the real numbers and formula_20-adic numbers, with their orders and absolute values, are the only valuations, absolute values, and places on the rational numbers. The local-global principle allows certain problems over the rational numbers to be solved by piecing together solutions from each of their places, again underlining the importance of primes to number theory.

A commutative ring is an algebraic structure where addition, subtraction and multiplication are defined. The integers are a ring, and the prime numbers in the integers have been generalized to rings in two different ways, "prime elements" and "irreducible elements". An element formula_20 of a ring formula_172 is called prime if it is nonzero, has no multiplicative inverse (that is, it is not a unit), and satisfies the following requirement: whenever formula_20 divides the product formula_174 of two elements of formula_172, it also divides at least one of formula_22 or formula_121. An element is irreducible if it is neither a unit nor the product of two other non-unit elements. In the ring of integers, the prime and irreducible elements form the same set,
In an arbitrary ring, all prime elements are irreducible. The converse does not hold in general, but does hold for unique factorization domains.

The fundamental theorem of arithmetic continues to hold (by definition) in unique factorization domains. An example of such a domain is the Gaussian integers formula_179, the ring of complex numbers of the form formula_180 where formula_181 denotes the imaginary unit and formula_34 and formula_35 are arbitrary integers. Its prime elements are known as Gaussian primes. Not every number that is prime among the integers remains prime in the Gaussian integers; for instance, the number 2 can be written as a product of the two Gaussian primes formula_184 and formula_185. Rational primes (the prime elements in the integers) congruent to 3 mod 4 are Gaussian primes, but rational primes congruent to 1 mod 4 are not. This is a consequence of Fermat's theorem on sums of two squares,
which states that an odd prime formula_20 is expressible as the sum of two squares, formula_187, and therefore factorizable as formula_188, exactly when formula_20 is 1 mod 4.

Not every ring is a unique factorization domain. For instance, in the ring of numbers formula_190 (for integers formula_34 and formula_35) the number formula_193 has two factorizations formula_194, where neither of the four factors can be reduced any further, so it does not have a unique factorization. In order to extend unique factorization to a larger class of rings, the notion of a number can be replaced with that of an ideal, a subset of the elements of a ring that contains all sums of pairs of its elements, and all products of its elements with ring elements.
"Prime ideals", which generalize prime elements in the sense that the principal ideal generated by a prime element is a prime ideal, are an important tool and object of study in commutative algebra, algebraic number theory and algebraic geometry. The prime ideals of the ring of integers are the ideals (0), (2), (3), (5), (7), (11), … The fundamental theorem of arithmetic generalizes to the Lasker–Noether theorem, which expresses every ideal in a Noetherian commutative ring as an intersection of primary ideals, which are the appropriate generalizations of prime powers.

The spectrum of a ring is a geometric space whose points are the prime ideals of the ring. Arithmetic geometry also benefits from this notion, and many concepts exist in both geometry and number theory. For example, factorization or ramification of prime ideals when lifted to an extension field, a basic problem of algebraic number theory, bears some resemblance with ramification in geometry. These concepts can even assist with in number-theoretic questions solely concerned with integers. For example, prime ideals in the ring of integers of quadratic number fields can be used in proving quadratic reciprocity, a statement that concerns the existence of square roots modulo integer prime numbers.
Early attempts to prove Fermat's Last Theorem led to Kummer's introduction of regular primes, integer prime numbers connected with the failure of unique factorization in the cyclotomic integers.
The question of how many integer prime numbers factor into a product of multiple prime ideals in an algebraic number field is addressed by Chebotarev's density theorem, which (when applied to the cyclotomic integers) has Dirichlet's theorem on primes in arithmetic progressions as a special case.

In the theory of finite groups the Sylow theorems imply that, if a power of a prime number formula_195 divides the order of a group, then it has a subgroup of order formula_195. By Lagrange's theorem, any group of prime order is a cyclic group,
and by the Burnside theorem any group whose order is divisible by only two primes is solvable.

For a long time, number theory in general, and the study of prime numbers in particular, was seen as the canonical example of pure mathematics, with no applications outside of mathematics with the exception of use of prime numbered gear teeth to distribute wear evenly. In particular, number theorists such as British mathematician G. H. Hardy prided themselves on doing work that had absolutely no military significance.

This vision of the purity of number theory was shattered in the 1970s, when it was publicly announced that prime numbers could be used as the basis for the creation of public key cryptography algorithms.
These applications have led to significant study of algorithms for computing with prime numbers, and in particular of primality testing, methods for determining whether a given number is prime.
The most basic primality testing routine, trial division, is too slow to be useful for large numbers. One group of modern primality tests is applicable to arbitrary numbers, while more efficient tests are available for numbers of special types. Most primality tests only tell whether their argument is prime or not. Routines that also provide a prime factor of composite arguments (or all of its prime factors) are called factorization algorithms.
Prime numbers are also used in computing for checksums, hash tables, and pseudorandom number generators.

The most basic method of checking the primality of a given integer formula_1 is called "trial division". This method divides formula_1 by each integer from 2 up to the square root of formula_1. Any such integer dividing formula_1 evenly establishes formula_1 as composite; otherwise it is prime.
Integers larger than the square root do not need to be checked because, whenever formula_202, one of the two factors formula_34 and formula_35 is less than or equal to the square root of formula_1. Another optimization is to check only primes as factors in this range.
For instance, to check whether 37 is prime, this method divides it by the primes in the range from 2 to , which are 2, 3, and 5. Each division produces a nonzero remainder, so 37 is indeed prime.

Although this method is simple to describe, it is impractical for testing the primality of large integers, because the number of tests that it performs grows exponentially as a function of the number of digits of these integers. However, trial division is still used, with a smaller limit than the square root on the divisor size, to quickly discover composite numbers with small factors, before using more complicated methods on the numbers that pass this filter.

Before computers, mathematical tables listing all of the primes or prime factorizations up to a given limit were commonly printed. The oldest method for generating a list of primes is called the sieve of Eratosthenes. The animation shows an optimized variant of this method.
Another more efficient sieving method for the same problem is the sieve of Atkin. In advanced mathematics, sieve theory applies similar methods to other problems.

Some of the fastest modern tests for whether an arbitrary given number formula_1 is prime are probabilistic (or Monte Carlo) algorithms, meaning that they have a small random chance of producing an incorrect answer.
For instance the Solovay–Strassen primality test on a given number formula_20 chooses a number formula_34 randomly from formula_42 to formula_210 and uses modular exponentiation to check
whether formula_211 is divisible by formula_20. If so, it answers yes and otherwise it answers no. If formula_20 really is prime, it will always answer yes, but if formula_20 is composite then it answers yes with probability at most 1/2 and no with probability at least 1/2.
If this test is repeated formula_1 times on the same number,
the probability that a composite number could pass the test every time is at most formula_216. Because this decreases exponentially with the number of tests, it provides high confidence (although not certainty) that a number that passes the repeated test is prime. On the other hand, if the test ever fails, then the number is certainly composite.
A composite number that passes such a test is called a pseudoprime.

In contrast, some other algorithms guarantee that their answer will always be correct: primes will always be determined to be prime and composites will always be determined to be composite.
For instance, this is true of trial division.
The algorithms with guaranteed-correct output include both deterministic (non-random) algorithms, such as the AKS primality test,
and randomized Las Vegas algorithms where the random choices made by the algorithm do not affect its final answer, such as some variations of elliptic curve primality proving.
The elliptic curve primality test is the fastest in practice of the guaranteed-correct primality tests, but its runtime analysis is based on heuristic arguments rather than rigorous proofs. The AKS primality test has mathematically proven time complexity, but is slower than elliptic curve primality proving in practice. These methods can be used to generate large random prime numbers, by generating and testing random numbers until finding one that is prime;
when doing this, a faster probabilistic test can quickly eliminate most composite numbers before a guaranteed-correct algorithm is used to verify that the remaining numbers are prime.

The following table lists some of these tests. Their running time is given in terms of formula_1, the number to be tested and, for probabilistic algorithms, the number formula_64 of tests performed. Moreover, formula_219 is an arbitrarily small positive number, and log is the logarithm to an unspecified base. The big O notation means that each time bound should be multiplied by a constant factor to convert it from dimensionless units to units of time; this factor depends on implementation details such as the type of computer used to run the algorithm, but not on the input parameters formula_1 and formula_64.

In addition to the aforementioned tests that apply to any natural number, some numbers of a special form can be tested for primality more quickly.
For example, the Lucas–Lehmer primality test can determine whether a Mersenne number (one less than a power of two) is prime, deterministically,
in the same time as a single iteration of the Miller–Rabin test. This is why since 1992 () the largest "known" prime has always been a Mersenne prime.
It is conjectured that there are infinitely many Mersenne primes.

The following table gives the largest known primes of various types. Some of these primes have been found using distributed computing. In 2009, the Great Internet Mersenne Prime Search project was awarded a US$100,000 prize for first discovering a prime with at least 10 million digits. The Electronic Frontier Foundation also offers $150,000 and $250,000 for primes with at least 100 million digits and 1 billion digits, respectively.

Given a composite integer formula_1, the task of providing one (or all) prime factors is referred to as "factorization" of formula_1. It is significantly more difficult than primality testing, and although many factorization algorithms are known, they are slower than the fastest primality testing methods. Trial division and Pollard's rho algorithm can be used to find very small factors of formula_1, and elliptic curve factorization can be effective when formula_1 has factors of moderate size. Methods suitable for arbitrary large numbers that do not depend on the size of its factors include the quadratic sieve and general number field sieve. As with primality testing, there are also factorization algorithms that require their input to have a special form, including the special number field sieve. , the largest number known to have been factored by a general-purpose algorithm is RSA-768, which has 232 decimal digits (768 bits) and is the product of two large primes.

Shor's algorithm can factor any integer in a polynomial number of steps on a quantum computer. However current technology can only run this algorithm for very small numbers. the largest number that has been factored by a quantum computer running Shor's algorithm is 21.

Several public-key cryptography algorithms, such as RSA and the Diffie–Hellman key exchange, are based on large prime numbers (2048-bit primes are common). RSA relies on the assumption that it is much easier (that is, more efficient) to perform the multiplication of two (large) numbers formula_22 and formula_121 than to calculate formula_22 and formula_121 (assumed coprime) if only the product formula_174 is known. The Diffie–Hellman key exchange relies on the fact that there are efficient algorithms for modular exponentiation (computing formula_231), while the reverse operation (the discrete logarithm) is thought to be a hard problem.

Prime numbers are frequently used for hash tables. For instance the original method of Carter and Wegman for universal hashing was based on computing hash functions by choosing random linear functions modulo large prime numbers. Carter and Wegman generalized this method to formula_64-independent hashing by using higher-degree polynomials, again modulo large primes. As well as in the hash function, prime numbers are used for the hash table size in quadratic probing based hash tables to ensure that the probe sequence covers the whole table.

Some checksum methods are based on the mathematics of prime numbers. For instance the checksums used in International Standard Book Numbers are defined by taking the rest of the number modulo 11, a prime number. Because 11 is prime this method can detect both single-digit errors and transpositions of adjacent digits. Another checksum method, Adler-32, uses arithmetic modulo 65521, the largest prime number less than formula_233.
Prime numbers are also used in pseudorandom number generators including linear congruential generators and the Mersenne Twister.

Prime numbers are of central importance to number theory but also have many applications to other areas within mathematics, including abstract algebra and elementary geometry. For example, it is possible to place prime numbers of points in a two-dimensional grid so that no three are in a line, or so that every triangle formed by three of the points has large area. Another example is Eisenstein's criterion, a test for whether a polynomial is irreducible based on divisibility of its coefficients by a prime number and its square.
The concept of prime number is so important that it has been generalized in different ways in various branches of mathematics. Generally, "prime" indicates minimality or indecomposability, in an appropriate sense. For example, the prime field of a given field is its smallest subfield that contains both 0 and 1. It is either the field of rational numbers or a finite field with a prime number of elements, whence the name. Often a second, additional meaning is intended by using the word prime, namely that any object can be, essentially uniquely, decomposed into its prime components. For example, in knot theory, a prime knot is a knot that is indecomposable in the sense that it cannot be written as the connected sum of two nontrivial knots. Any knot can be uniquely expressed as a connected sum of prime knots. The prime decomposition of 3-manifolds is another example of this type.

Beyond mathematics and computing, prime numbers have potential connections to quantum mechanics, and have been used metaphorically in the arts and literature. They have also been used in evolutionary biology to explain the life cycles of cicadas.

Fermat primes are primes of the form
with formula_64 a natural number. They are named after Pierre de Fermat, who conjectured that all such numbers are prime. The first five of these numbers – 3, 5, 17, 257, and 65,537 – are prime, but formula_236 is composite and so are all other Fermat numbers that have been verified as of 2017. A regular formula_1-gon is constructible using straightedge and compass if and only if the odd prime factors of formula_1 (if any) are distinct Fermat primes. Likewise, a regular formula_1-gon may be constructed using straightedge, compass, and an angle trisector if and only if the prime factors of formula_1 are any number of copies of 2 or 3 together with a (possibly empty) set of distinct Pierpont primes, primes of the form formula_241.

It is possible to partition any convex polygon into formula_1 smaller convex polygons of equal area and equal perimeter, when formula_1 is a power of a prime number, but this is not known for other values of formula_1.

Beginning with the work of Hugh Montgomery and Freeman Dyson in the 1970s, mathematicians and physicists have speculated that the zeros of the Riemann zeta function are connected to the energy levels of quantum systems. Prime numbers are also significant in quantum information science, thanks to mathematical structures such as mutually unbiased bases and symmetric informationally complete positive-operator-valued measures.

The evolutionary strategy used by cicadas of the genus "Magicicada" makes use of prime numbers. These insects spend most of their lives as grubs underground. They only pupate and then emerge from their burrows after 7, 13 or 17 years, at which point they fly about, breed, and then die after a few weeks at most. Biologists theorize that these prime-numbered breeding cycle lengths have evolved in order to prevent predators from synchronizing with these cycles.
In contrast, the multi-year periods between flowering in bamboo plants are hypothesized to be smooth numbers, having only small prime numbers in their factorizations.

Prime numbers have influenced many artists and writers.
The French composer Olivier Messiaen used prime numbers to create ametrical music through "natural phenomena". In works such as "La Nativité du Seigneur" (1935) and "Quatre études de rythme" (1949–50), he simultaneously employs motifs with lengths given by different prime numbers to create unpredictable rhythms: the primes 41, 43, 47 and 53 appear in the third étude, "Neumes rythmiques". According to Messiaen this way of composing was "inspired by the movements of nature, movements of free and unequal durations".

In his science fiction novel "Contact", scientist Carl Sagan suggested that prime factorization could be used as a means of establishing two-dimensional image planes in communications with aliens, an idea that he had first developed informally with American astronomer Frank Drake in 1975. In the novel "The Curious Incident of the Dog in the Night-Time" by Mark Haddon, the narrator arranges the sections of the story by consecutive prime numbers as a way to convey the mental state of its main character, a mathematically gifted teen with Asperger syndrome. Prime numbers are used as a metaphor for loneliness and isolation in the Paolo Giordano novel "The Solitude of Prime Numbers", in which they are portrayed as "outsiders" among integers.




</doc>
<doc id="23669" url="https://en.wikipedia.org/wiki?curid=23669" title="Piers Anthony">
Piers Anthony

Piers Anthony Dillingham Jacob (born 6 August 1934 in Oxford, England) is an English American author in the science fiction and fantasy genres, publishing under the name Piers Anthony. He is most famous for his set in the fictional realm of Xanth.

Many of his books have appeared on "The New York Times" Best Seller list. He has stated that one of his greatest achievements has been to publish a book beginning with every letter of the alphabet, from "Anthonology" to "Zombie Lover".

Anthony's parents, Alfred and Norma Jacob, were Quaker pacifists studying at Oxford University who interrupted their studies in 1936 to undertake relief work on behalf of the Quakers during the Spanish Civil War, establishing a food kitchen for children in Barcelona. Piers and his sister were left in England in the care of their maternal grandparents and a nanny. Alfred Jacob, although a British citizen, had been born in America near Philadelphia, and in 1940, after being forced out of Spain and with the situation in Britain deteriorating, the family sailed to the United States. In 1941 the family settled in a rustic "back to the land" utopian community near Winhall, Vermont, where a young Piers made the acquaintance of radical author Scott Nearing, a neighbor. Both parents resumed their academic studies, and Alfred eventually became a professor of Romance languages, teaching at a number of colleges in the Philadelphia area.

Piers was moved around to a number of schools, eventually enrolling in Goddard College in Vermont where he graduated in 1956. On "This American Life" on 27 July 2012, Anthony revealed that his parents had divorced, he was bullied, and he had poor grades in school. Anthony referred to his high school as a "very fancy private school", and refuses to donate money to the school because as a student, he recalls being part of "the lower crust", and that no one paid attention to or cared about him. He said, "I didn't like being a member of the underclass, of the peons like that". He became a naturalized U.S. citizen while serving in the United States Army in 1958. After completing a two-year stint in military service, he briefly taught school at Admiral Farragut Academy in St. Petersburg, Florida before becoming a full-time writer.

Anthony met his future wife, Carol Marble, while both were attending college. They were married in 1956, the same year he graduated from Goddard College. After a series of odd jobs, Anthony decided to join the U.S. Army in 1957 for a steady source of income and medical coverage for his pregnant wife. He would stay in the Army until 1959 and became a US citizen during this time. While in the army, he became an editor and cartoonist for the battalion newspaper. After leaving the army, he spent a brief stint as a public school teacher before trying his hand at becoming a full-time writer.

Anthony and his wife made a deal: if he could sell a piece of writing within one year, she would continue to work to support him. But if he could not sell anything in that year, then he would forever give up his dream of being a writer. At the end of the year, he managed to get a short story published. He credits his wife as the person who made his writing career possible, and he advises aspiring writers that they need to have a source of income other than their writing in order to get through the early years of a writing career.

On multiple occasions Anthony has moved from one publisher to another (taking a profitable hit series with him) when he says he felt the editors were unduly tampering with his work. He has sued publishers for accounting malfeasance and won judgments in his favor. Anthony maintains an Internet Publishers Survey in the interest of helping aspiring writers. For this service, he won the 2003 "Friend of EPIC" award for service to the electronic publishing community. His website won the "Special Recognition for Service to Writers" award from Preditors and Editors, an author's guide to publishers and writing services.

Many of his popular novel series have been optioned for movies. His popular series Xanth inspired the DOS video game "Companions of Xanth", by Legend Entertainment. The series also spawned the board game "Xanth" by Mayfair Games.

Anthony's novels usually end with a chapter-long Author's Note, in which he talks about himself, his life, and his experiences as they related to the process of writing the novel. He often discusses correspondence with readers and any real-world issues that influenced the novel.

Since about 2000, Anthony has written his novels in a Linux environment.

Anthony's "Xanth" series was ranked No. 99 in a 2011 NPR readers' poll of best science fiction and fantasy books.

Act One of episode 470 of the radio program "This American Life" is an account of boyhood obsessions with Piers Anthony. The act is written and narrated by writer Logan Hill who, as a 12-year-old, was consumed with reading Anthony's novels. For a decade he felt he must have been Anthony's number one fan, until, when he was 22, he met "Andy" at a wedding and discovered their mutual interest in the writer. Andy is interviewed for the story and explains that, as a teenager, he had used escapist novels in order to cope with his alienating school and home life in Buffalo, New York. In 1987, at age 15, he decided to run away to Florida in order to try to live with Piers Anthony. The story includes Anthony's reflections on these events.

Naomi King, the daughter of writer Stephen King, enjoyed reading books by Piers Anthony, which included things like pixies, imps and fairies. After she told her father, "Dad, I just don't like those to be scared. Would you write something with dragons in it?", he wrote "The Eyes of the Dragon" which was originally published in 1984 and later in 1987 by Viking Press.

Early in Anthony's literary career, there was a dispute surrounding the original publication (1976) of "But What of Earth?". Editor Roger Elwood commissioned the novel for his nascent science-fiction line Laser Books. According to Anthony, he completed "But What of Earth?", and Elwood accepted and purchased it. Elwood then told Anthony that he wished to make several minor changes, and in order not to waste Anthony's time, he had hired copy editor (and author) Robert Coulson to retype the manuscript with the changes. Anthony described Coulson as a friend and was initially open to his contribution.

However, Elwood told Coulson he was to be a full collaborator, free to make revisions to Anthony's text in line with suggestions made by other copy editors. Elwood promised Coulson a 50-50 split with Anthony on all future royalties. According to Anthony, the published novel was very different from his version, with changes to characters and dialog, and with scenes added and removed. Anthony felt the changes worsened the novel. Laser's ultimate publication of "But What of Earth?" listed Anthony and Coulson together as collaborators. Publication rights were reverted to Anthony under threat of legal action. In 1989, Anthony (re)published his original "But What of Earth?" in an annotated edition through Tor Books. This edition contains an introduction and conclusion setting out the story of the novel's permutations and roughly 60 pages of notes by Anthony giving examples of changes to plot and characters, and describing some of the comments made by copy editors on his manuscript.

Anthony currently lives with his wife on a tree farm which he owns in Florida. He and his wife had two daughters, Penelope "Penny" Carolyn and Cheryl, and one grandchild, Logan. Regarding his religious beliefs, Anthony wrote in the October 2004 entry of his personal website, "I'm agnostic, which means I regard the case as unproven, but I'm much closer to the atheist position than to the theist one." In 2017 he stated, "I am more certain about God and the Afterlife: they don't exist." 

On 3 September 2009, their daughter Penny Jacob died from apparent respiratory paralysis following surgery for melanoma which had metastasized to her brain. She is survived by her husband and her daughter, Logan.

For autobiographies refer to autobiographical subsection.



</doc>
<doc id="23670" url="https://en.wikipedia.org/wiki?curid=23670" title="Perfect number">
Perfect number

In number theory, a perfect number is a positive integer that is equal to the sum of its proper positive divisors, that is, the sum of its positive divisors excluding the number itself (also known as its aliquot sum). Equivalently, a perfect number is a number that is half the sum of all of its positive divisors (including itself) i.e. "σ"("n") = 2"n".

This definition is ancient, appearing as early as Euclid's Elements (VII.22) where it is called ("perfect", "ideal", or "complete number"). Euclid also proved a formation rule (IX.36) whereby formula_1 is an even perfect number whenever formula_2 is a prime of the form formula_3 for prime formula_4—what is now called a Mersenne prime. Much later, Euler proved that all even perfect numbers are of this form. This is known as the Euclid–Euler theorem.

It is not known whether there are any odd perfect numbers, nor whether infinitely many perfect numbers exist.

The first perfect number is 6. Its proper divisors are 1, 2, and 3, and 1 + 2 + 3 = 6. Equivalently, the number 6 is equal to half the sum of all its positive divisors: ( 1 + 2 + 3 + 6 ) ÷ 2 = 6. The next perfect number is 28: 28 = 1 + 2 + 4 + 7 + 14. This is followed by the perfect numbers 496 and 8128 .

In about 300 BC Euclid showed that if 2 − 1 is prime then (2 − 1)2 is perfect.
The first four perfect numbers were the only ones known to early Greek mathematics, and the mathematician Nicomachus had noted 8128 as early as 100 AD. Philo of Alexandria in his first-century book "On the creation" mentions perfect numbers, claiming that the world was created in 6 days and the moon orbits in 28 days because 6 and 28 are perfect. Philo is followed by Origen, and by Didymus the Blind, who adds the observation that there are only four perfect numbers that are less than 10,000. (Commentary on Genesis 1. 14-19). St Augustine defines perfect numbers in City of God (Part XI, Chapter 30) in the early 5th century AD, repeating the claim that God created the world in 6 days because 6 is the smallest perfect number. The Egyptian mathematician Ismail ibn Fallūs (1194–1252) mentioned the next three perfect numbers (33,550,336; 8,589,869,056; and 137,438,691,328) and listed a few more which are now known to be incorrect. In a manuscript written between 1456 and 1461, an unknown mathematician recorded the earliest European reference to a fifth perfect number, with 33,550,336 being correctly identified for the first time. In 1588, the Italian mathematician Pietro Cataldi also identified the sixth (8,589,869,056) and the seventh (137,438,691,328) perfect numbers, and also proved that every perfect number obtained from Euclid's rule ends with a 6 or an 8.

Euclid proved that 2(2 − 1) is an even perfect number whenever 2 − 1 is prime (Euclid, Prop. IX.36).

For example, the first four perfect numbers are generated by the formula 2(2 − 1), with "p" a prime number, as follows:

Prime numbers of the form 2 − 1 are known as Mersenne primes, after the seventeenth-century monk Marin Mersenne, who studied number theory and perfect numbers. For 2 − 1 to be prime, it is necessary that "p" itself be prime. However, not all numbers of the form 2 − 1 with a prime "p" are prime; for example, 2 − 1 = 2047 = 23 × 89 is not a prime number. In fact, Mersenne primes are very rare—of the 2,270,720 prime numbers "p" up to 37,156,667, 
2 − 1 is prime for only 45 of them.

Nicomachus (60–120 AD) conjectured that every perfect number is of the form 2(2 − 1) where 2 − 1 is prime. Ibn al-Haytham (Alhazen) circa 1000 AD conjectured that every "even" perfect number is of that form. It was not until the 18th century that Leonhard Euler proved that the formula 2(2 − 1) will yield all the even perfect numbers. Thus, there is a one-to-one correspondence between even perfect numbers and Mersenne primes; each Mersenne prime generates one even perfect number, and vice versa. This result is often referred to as the Euclid–Euler theorem. , 50 Mersenne primes are known, and therefore 50 even perfect numbers (the largest of which is 2 × (2 − 1) with 46,498,850 digits).

An exhaustive search by the GIMPS distributed computing project has shown that the first 46 even perfect numbers are 2(2 − 1) for
Four higher perfect numbers have also been discovered, namely those for which "p" = 43112609, 57885161, 74207281, and 77232917, though there may be others within this range. It is not known whether there are infinitely many perfect numbers, nor whether there are infinitely many Mersenne primes.

As well as having the form 2(2 − 1), each even perfect number is the triangular number (and hence equal to the sum of the integers from 1 to ) and the hexagonal number. Furthermore, each even perfect number except for 6 is the centered nonagonal number and is equal to the sum of the first odd cubes:

Even perfect numbers (except 6) are of the form

with each resulting triangular number (after subtracting 1 from the perfect number and dividing the result by 9) ending in 3 or 5, the sequence starting with 3, 55, 903, 3727815, ... This can be reformulated as follows: adding the digits of any even perfect number (except 6), then adding the digits of the resulting number, and repeating this process until a single digit (called the digital root) is obtained, always produces the number 1. For example, the digital root of 8128 is 1, because 8 + 1 + 2 + 8 = 19, 1 + 9 = 10, and 1 + 0 = 1. This works with all perfect numbers 2(2 − 1) with odd prime "p" and, in fact, with all numbers of the form 2(2 − 1) for odd integer (not necessarily prime) "m".

Owing to their form, 2(2 − 1), every even perfect number is represented in binary as "p" ones followed by "p" − 1  zeros; for example,
and
Thus every even perfect number is a pernicious number.

Note that every even perfect number is also a practical number (c.f. Related concepts).

It is unknown whether there is any odd perfect number, though various results have been obtained. In 1496, Jacques Lefèvre stated that Euclid's rule gives all perfect numbers, thus implying that no odd perfect number exists. More recently, Carl Pomerance has presented a heuristic argument suggesting that indeed no odd perfect number should exist. All perfect numbers are also Ore's harmonic numbers, and it has been conjectured as well that there are no odd Ore's harmonic numbers other than 1.

Any odd perfect number "N" must satisfy the following conditions:

In 1888, Sylvester stated:
Euler stated: "Whether (...) there are any odd perfect numbers is a most difficult question".

All even perfect numbers have a very precise form; odd perfect numbers either do not exist or are rare. There are a number of results on perfect numbers that are actually quite easy to prove but nevertheless superficially impressive; some of them also come under Richard Guy's strong law of small numbers:

The sum of proper divisors gives various other kinds of numbers. Numbers where the sum is less than the number itself are called deficient, and where it is greater than the number, abundant. These terms, together with "perfect" itself, come from Greek numerology. A pair of numbers which are the sum of each other's proper divisors are called amicable, and larger cycles of numbers are called sociable. A positive integer such that every smaller positive integer is a sum of distinct divisors of it is a practical number.

By definition, a perfect number is a fixed point of the restricted divisor function , and the aliquot sequence associated with a perfect number is a constant sequence. All perfect numbers are also formula_16-perfect numbers, or Granville numbers.

A semiperfect number is a natural number that is equal to the sum of all or some of its proper divisors. A semiperfect number that is equal to the sum of all its proper divisors is a perfect number. Most abundant numbers are also semiperfect; abundant numbers which are not semiperfect are called weird numbers.





</doc>
<doc id="23672" url="https://en.wikipedia.org/wiki?curid=23672" title="Parthenon">
Parthenon

The Parthenon (; ; , "Parthenónas") is a former temple, on the Athenian Acropolis, Greece, dedicated to the goddess Athena, whom the people of Athens considered their patron. Construction began in 447 BC when the Athenian Empire was at the peak of its power. It was completed in 438 BC although decoration of the building continued until 432 BC. It is the most important surviving building of Classical Greece, generally considered the zenith of the Doric order. Its decorative sculptures are considered some of the high points of Greek art. The Parthenon is regarded as an enduring symbol of Ancient Greece, Athenian democracy and Western civilization, and one of the world's greatest cultural monuments. To the Athenians who built it, the Parthenon and other Periclean monuments of the Acropolis, were seen fundamentally as a celebration of Hellenic victory over the Persian invaders and as a thanksgiving to the gods for that victory. The Greek Ministry of Culture is currently carrying out a programme of selective restoration and reconstruction to ensure the stability of the partially ruined structure.

The Parthenon itself replaced an older temple of Athena, which historians call the Pre-Parthenon or Older Parthenon, that was destroyed in the Persian invasion of 480 BC. The temple is archaeoastronomically aligned to the Hyades. Like most Greek temples, the Parthenon served a practical purpose as the city treasury. For a time, it served as the treasury of the Delian League, which later became the Athenian Empire. In the final decade of the sixth century AD, the Parthenon was converted into a Christian church dedicated to the Virgin Mary.

After the Ottoman conquest, it was turned into a mosque in the early 1460s. On 26 September 1687, an Ottoman ammunition dump inside the building was ignited by Venetian bombardment. The resulting explosion severely damaged the Parthenon and its sculptures. From 1800 to 1803, Thomas Bruce, 7th Earl of Elgin removed some of the surviving sculptures with the alleged permission of the Ottoman Empire. These sculptures, now known as the Elgin Marbles or the Parthenon Marbles, were sold in 1816 to the British Museum in London, where they are now displayed. Since 1983 (on the initiative of Culture Minister Melina Mercouri), the Greek government has been committed to the return of the sculptures to Greece.

The origin of the Parthenon's name is from the Greek word παρθενών ("parthenon"), which referred to the "unmarried women's apartments" in a house and in the Parthenon's case seems to have been used at first only for a particular room of the temple; it is debated which room this is and how the room acquired its name. The Liddell–Scott–Jones "Greek–English Lexicon" states that this room was the western cella of the Parthenon, as does J. B. Bury. Jamauri D. Green holds that the parthenon was the room in which the peplos presented to Athena at the Panathenaic Festival was woven by the "arrephoroi", a group of four young girls chosen to serve Athena each year. Christopher Pelling asserts that Athena Parthenos may have constituted a discrete cult of Athena, intimately connected with, but not identical to, that of Athena Polias. According to this theory, the name of the Parthenon means the "temple of the virgin goddess" and refers to the cult of Athena Parthenos that was associated with the temple. The epithet "parthénos" () meant "maiden, girl", but also "virgin, unmarried woman" and was especially used for Artemis, the goddess of wild animals, the hunt, and vegetation, and for Athena, the goddess of strategy and tactics, handicraft, and practical reason. It has also been suggested that the name of the temple alludes to the maidens ("parthenoi"), whose supreme sacrifice guaranteed the safety of the city. "Parthénos" has also been applied to the Virgin Mary, Parthénos Maria, and the Parthenon had been converted to a Christian church dedicated to the Virgin Mary in the final decade of the sixth century.

The first instance in which "Parthenon" definitely refers to the entire building is found in the writings of the 4th century BC orator Demosthenes. In 5th-century building accounts, the structure is simply called "ho naos" ("the temple"). The architects Iktinos and Callicrates are said to have called the building "Hekatompedos" ("the hundred footer") in their lost treatise on Athenian architecture, and, in the 4th century and later, the building was referred to as the "Hekatompedos" or the "Hekatompedon" as well as the Parthenon; the 1st-century-AD writer Plutarch referred to the building as the "Hekatompedos Parthenon".

Because the Parthenon was dedicated to the Greek goddess Athena, it has sometimes been referred to as the Temple of Minerva, the Roman name for Athena, particularly during the 19th century.

Although the Parthenon is architecturally a temple and is usually called so, it is not really one in the conventional sense of the word. A small shrine has been excavated within the building, on the site of an older sanctuary probably dedicated to Athena as a way to get closer to the goddess, but the Parthenon never hosted the cult of Athena Polias, patron of Athens: the cult image, which was bathed in the sea and to which was presented the "peplos", was an olivewood "xoanon", located at an older altar on the northern side of the Acropolis.

The colossal statue of Athena by Phidias was not related to any cult and is not known to have inspired any religious fervour. It did not seem to have any priestess, altar or cult name.
According to Thucydides, Pericles once referred to the statue as a gold reserve, stressing that it "contained forty talents of pure gold and it was all removable".
The Athenian statesman thus implies that the metal, obtained from contemporary coinage, could be used again without any impiety.
The Parthenon should then be viewed as a grand setting for Phidias' votive statue rather than a cult site. It is said in many writings of the Greeks that there were many treasures stored inside the temple, such as Persian swords and small statue figures made of precious metals.

Archaeologist Joan Breton Connelly has recently argued for the coherency of the Parthenon’s sculptural programme in presenting a succession of genealogical narratives that track Athenian identity back through the ages: from the birth of Athena, through cosmic and epic battles, to the final great event of the Athenian Bronze Age, the war of Erechtheus and Eumolpos. She argues a pedagogical function for the Parthenon’s sculptured decoration, one that establishes and perpetuates Athenian foundation myth, memory, values and identity. While some classicists, including Mary Beard, Peter Green, and Garry Wills have doubted or rejected Connelly's thesis, an increasing number of historians, archaeologists, and classical scholars support her work. They include: J.J. Pollitt, Brunilde Ridgway, Nigel Spivey, Caroline Alexander, A.E. Stallings.

The first endeavour to build a sanctuary for on the site of the present Parthenon was begun shortly after the Battle of Marathon (c. 490–488 BC) upon a solid limestone foundation that extended and levelled the southern part of the Acropolis summit. This building replaced a "hekatompedon" (meaning "hundred-footer") and would have stood beside the archaic temple dedicated to "Athena Polias" ("of the city"). The Older or Pre-Parthenon, as it is frequently referred to, was still under construction when the Persians sacked the city in 480 BC and razed the Acropolis.

The existence of both the proto-Parthenon and its destruction were known from Herodotus, and the drums of its columns were plainly visible built into the curtain wall north of the Erechtheion. Further physical evidence of this structure was revealed with the excavations of Panagiotis Kavvadias of 1885–90. The findings of this dig allowed Wilhelm Dörpfeld, then director of the German Archaeological Institute, to assert that there existed a distinct substructure to the original Parthenon, called Parthenon I by Dörpfeld, not immediately below the present edifice as had been previously assumed. Dörpfeld's observation was that the three steps of the first Parthenon consisted of two steps of Poros limestone, the same as the foundations, and a top step of Karrha limestone that was covered by the lowest step of the Periclean Parthenon. This platform was smaller and slightly to the north of the final Parthenon, indicating that it was built for a wholly different building, now completely covered over. This picture was somewhat complicated by the publication of the final report on the 1885–90 excavations, indicating that the substructure was contemporary with the Kimonian walls, and implying a later date for the first temple.

If the original Parthenon was indeed destroyed in 480, it invites the question of why the site was left a ruin for thirty-three years. One argument involves the oath sworn by the Greek allies before the Battle of Plataea in 479 BC declaring that the sanctuaries destroyed by the Persians would not be rebuilt, an oath from which the Athenians were only absolved with the Peace of Callias in 450. The mundane fact of the cost of reconstructing Athens after the Persian sack is at least as likely a cause. However, the excavations of Bert Hodge Hill led him to propose the existence of a second Parthenon, begun in the period of Kimon after 468 BC. Hill claimed that the Karrha limestone step Dörpfeld thought was the highest of Parthenon I was in fact the lowest of the three steps of Parthenon II, whose stylobate dimensions Hill calculated at .

One difficulty in dating the proto-Parthenon is that at the time of the 1885 excavation the archaeological method of seriation was not fully developed; the careless digging and refilling of the site led to a loss of much valuable information. An attempt to discuss and make sense of the potsherds found on the Acropolis came with the two-volume study by Graef and Langlotz published in 1925–33. This inspired American archaeologist William Bell Dinsmoor to attempt to supply limiting dates for the temple platform and the five walls hidden under the re-terracing of the Acropolis. Dinsmoor concluded that the latest possible date for Parthenon I was no earlier than 495 BC, contradicting the early date given by Dörpfeld. Further, Dinsmoor denied that there were two proto-Parthenons, and held that the only pre-Periclean temple was what Dörpfeld referred to as Parthenon II. Dinsmoor and Dörpfeld exchanged views in the "American Journal of Archaeology" in 1935.

In the mid-5th century BC, when the Athenian Acropolis became the seat of the Delian League and Athens was the greatest cultural centre of its time, Pericles initiated an ambitious building project that lasted the entire second half of the century. The most important buildings visible on the Acropolis today — the Parthenon, the Propylaia, the Erechtheion and the temple of Athena Nike — were erected during this period. The Parthenon was built under the general supervision of the artist Phidias, who also had charge of the sculptural decoration. The architects Ictinos and Callicrates began their work in 447 BC, and the building was substantially completed by 432, but work on the decorations continued until at least 431.

The Parthenon is a peripteral octastyle Doric temple with Ionic architectural features. It stands on a platform or stylobate of three steps. In common with other Greek temples, it is of post and lintel construction and is surrounded by columns ("peripteral") carrying an entablature. There are eight columns at either end ("octastyle") and seventeen on the sides. There is a double row of columns at either end. The colonnade surrounds an inner masonry structure, the "cella", which is divided into two compartments. At either end of the building the gable is finished with a triangular pediment originally occupied by sculpted figures. The columns are of the Doric order, with simple capitals, fluted shafts and no bases. Above the architrave of the entablature is a frieze of carved pictorial panels (metopes), separated by formal architectural triglyphs, typical of the Doric order. Around the cella and across the lintels of the inner columns runs a continuous sculptured frieze in low relief. This element of the architecture is Ionic in style rather than Doric.

Measured at the stylobate, the dimensions of the base of the Parthenon are . The cella was 29.8 metres long by 19.2 metres wide (97.8 × 63.0 ft). On the exterior, the Doric columns measure in diameter and are high. The corner columns are slightly larger in diameter. The Parthenon had 46 outer columns and 23 inner columns in total, each column containing 20 flutes. (A flute is the concave shaft carved into the column form.) The roof was covered with large overlapping marble tiles known as imbrices and tegulae.

The Parthenon is regarded as the finest example of Greek architecture. The temple, wrote John Julius Cooper, "enjoys the reputation of being the most perfect Doric temple ever built. Even in antiquity, its architectural refinements were legendary, especially the subtle correspondence between the curvature of the stylobate, the taper of the naos walls and the "entasis" of the columns." "Entasis" refers to the slight swelling, of 1/8 inch, in the centre of the columns to counteract the appearance of columns having a waist, as the swelling makes them look straight from a distance. The stylobate is the platform on which the columns stand. As in many other classical Greek temples, it has a slight parabolic upward curvature intended to shed rainwater and reinforce the building against earthquakes. The columns might therefore be supposed to lean outwards, but they actually lean slightly inwards so that if they carried on, they would meet almost exactly a mile above the centre of the Parthenon; since they are all the same height, the curvature of the outer stylobate edge is transmitted to the architrave and roof above: "All follow the rule of being built to delicate curves", Gorham Stevens observed when pointing out that, in addition, the west front was built at a slightly higher level than that of the east front.

It is not universally agreed what the intended effect of these "optical refinements" was; they may serve as a sort of "reverse optical illusion". As the Greeks may have been aware, two parallel lines appear to bow, or curve outward, when intersected by converging lines. In this case, the ceiling and floor of the temple may seem to bow in the presence of the surrounding angles of the building. Striving for perfection, the designers may have added these curves, compensating for the illusion by creating their own curves, thus negating this effect and allowing the temple to be seen as they intended. It is also suggested that it was to enliven what might have appeared an inert mass in the case of a building without curves, but the comparison ought to be, according to Smithsonian historian Evan Hadingham, with the Parthenon's more obviously curved predecessors than with a notional rectilinear temple.

Some studies of the Acropolis, including the Parthenon, conclude that many of its proportions approximate the golden ratio. The Parthenon's façade as well as elements of its façade and elsewhere can be circumscribed by golden rectangles. This view that the golden ratio was employed in the design has been disputed in more recent studies.

The cella of the Parthenon housed the chryselephantine statue of Athena Parthenos sculpted by Phidias and dedicated in 439 or 438 BC. The appearance of this is known from other images. The decorative stonework was originally highly coloured. The temple was dedicated to Athena at that time, though construction continued until almost the beginning of the Peloponnesian War in 432. By the year 438, the sculptural decoration of the Doric metopes on the frieze above the exterior colonnade, and of the Ionic frieze around the upper portion of the walls of the cella, had been completed. In the "opisthodomus" (the back room of the cella) were stored the monetary contributions of the Delian League, of which Athens was the leading member.

Only a very small number of the sculptures remain "in situ"; most of the surviving sculptures are today (controversially) in the British Museum in London as the Elgin Marbles, and the Athens Acropolis Museum, but a few pieces are also in the Louvre, and museums in Rome, Vienna and Palermo.

The frieze of the Parthenon's entablature contained ninety-two metopes, fourteen each on the east and west sides, thirty-two each on the north and south sides. They were carved in high relief, a practice employed until then only in treasuries (buildings used to keep votive gifts to the gods). According to the building records, the metope sculptures date to the years 446–440 BC. The metopes of the east side of the Parthenon, above the main entrance, depict the Gigantomachy (the mythical battle between the Olympian gods and the Giants). The metopes of the west end show the Amazonomachy (the mythical battle of the Athenians against the Amazons). The metopes of the south side show the Thessalian Centauromachy (battle of the Lapiths aided by Theseus against the half-man, half-horse Centaurs). Metopes 13–21 are missing, but drawings from 1674 attributed to Jaques Carrey indicate a series of humans; these have been variously interpreted as scenes from the Lapith wedding, scenes from the early history of Athens and various myths. On the north side of the Parthenon, the metopes are poorly preserved, but the subject seems to be the sack of Troy.

The metopes present examples of the Severe Style in the anatomy of the figures' heads, in the limitation of the corporal movements to the contours and not to the muscles, and in the presence of pronounced veins in the figures of the Centauromachy. Several of the metopes still remain on the building, but, with the exception of those on the northern side, they are severely damaged. Some of them are located at the Acropolis Museum, others are in the British Museum, and one is at the Louvre museum.

In March 2011, archaeologists announced that they had discovered five metopes of the Parthenon in the south wall of the Acropolis, which had been extended when the Acropolis was used as a fortress. According to "Eleftherotypia" daily, the archaeologists claimed the metopes had been placed there in the 18th century when the Acropolis wall was being repaired. The experts discovered the metopes while processing 2,250 photos with modern photographic methods, as the white Pentelic marble they are made of differed from the other stone of the wall. It was previously presumed that the missing metopes were destroyed during the Morosini explosion of the Parthenon in 1687.

The most characteristic feature in the architecture and decoration of the temple is the Ionic frieze running around the exterior walls of the cella, which is the inside structure of the Parthenon. The bas-relief frieze was carved in situ; it is dated to 442 BC-438 BC.

One interpretation is that it depicts an idealized version of the Panathenaic procession from the Dipylon Gate in the Kerameikos to the Acropolis. In this procession held every year, with a special procession taking place every four years, Athenians and foreigners were participating to honour the goddess Athena, offering sacrifices and a new peplos (dress woven by selected noble Athenian girls called "ergastines").

Joan Breton Connelly offers a mythological interpretation for the frieze, one that is in harmony with the rest of the temple’s sculptural programme which shows Athenian genealogy through a series of succession myths set in the remote past. She identifies the central panel above the door of the Parthenon as the pre-battle sacrifice of the daughter of King Erechtheus, a sacrifice that ensured Athenian victory over Eumolpos and his Thracian army. The great procession marching toward the east end of the Parthenon shows the post-battle thanksgiving sacrifice of cattle and sheep, honey and water, followed by the triumphant army of Erechtheus returning from their victory. This represents the very first Panathenaia set in mythical times, the model on which historic Panathenaic processions was based.

The traveller Pausanias, when he visited the Acropolis at the end of the 2nd century AD, only mentioned briefly the sculptures of the pediments (gable ends) of the temple, reserving the majority of his description for the gold and ivory statue of the goddess inside.

The figures on the corners of the pediment depict the passage of time over the course of a full day. Tethrippa of Helios and Selene are located on the left and right corners of the pediment respectively. The horses of Helios's chariot are shown with livid expressions as they ascend into the sky at the start of the day; whereas the Selene's horses struggle to stay on the pediment scene as the day comes to an end.

The supporters of Athena are extensively illustrated at the back of the left chariot, while the defenders of Poseidon are shown trailing behind the right chariot. It is believed that the corners of the pediment are filled by Athenian water deities, such as Kephisos river, Ilissos river and nymph Callirhoe. This belief merges from the fluid character of the sculptures' body position which represents the effort of the artist to give the impression of a flowing river., Next to the left river god, there are the sculptures of the mythical king of Athens (Kekrops) with his daughters (Aglauros, Pandrosos, Herse). The statue of Poseidon was the largest sculpture in the pediment until it broke into pieces during Francesco Morosini's effort to remove it in 1688. The posterior piece of the torso was found by Lusieri in the groundwork of a Turkish house in 1801 and is currently held in British Museum. The anterior portion was revealed by Ross in 1835 and is now held in the Acropolis Museum of Athens.

Every statue in the west pediment has a fully completed back, which would have been impossible to see when the sculpture was on the temple; this indicates that the sculptors put great effort into accurately portraying the human body.

The only piece of sculpture from the Parthenon known to be from the hand of Phidias was the statue of Athena housed in the "naos". This massive chryselephantine sculpture is now lost and known only from copies, vase painting, gems, literary descriptions and coins.

A major fire broke out in the Parthenon shortly after the middle of the third century AD which destroyed the Parthenon's roof and much of the sanctuary's interior. Heruli pirates are also credited with sacking Athens in 276, and destroying most of the public buildings there, including the Parthenon. Repairs were made in the fourth century AD, possibly during the reign of Julian the Apostate. A new wooden roof overlaid with clay tiles was installed to cover the sanctuary. It sloped at a greater incline than the original roof and left the building's wings exposed.

The Parthenon survived as a temple dedicated to Athena for nearly one thousand years until Theodosius II, during the Persecution of pagans in the late Roman Empire, decreed in 435 AD that all pagan temples in the Eastern Roman Empire be closed. 
However, it is debated exactly when during the 5th-century that the closure of the Parthenon as a temple was actually put in practice. It is suggested to have occurred in c. 481-484, in the instructions against the remaining temples by order of Emperor Zeno, because the temple had been the focus of Pagan Hellenic opposition against Zeno in Athens in support of Illus, who had promised to restore Hellenic rites to the temples that were still standing. 

At some point in the fifth century, Athena's great cult image was looted by one of the emperors and taken to Constantinople, where it was later destroyed, possibly during the siege and sack of Constantinople during the Fourth Crusade in 1204 AD.

The Parthenon was converted into a Christian church in the final decade of the sixth century AD to become the Church of the Parthenos Maria (Virgin Mary), or the Church of the Theotokos (Mother of God). The orientation of the building was changed to face towards the east; the main entrance was placed at the building's western end, and the Christian altar and iconostasis were situated towards the building's eastern side adjacent to an apse built where the temple's pronaos was formerly located. A large central portal with surrounding side-doors was made in the wall dividing the cella, which became the church's nave, from the rear chamber, the church's narthex. The spaces between the columns of the "opisthodomus" and the peristyle were walled up, though a number of doorways still permitted access. Icons were painted on the walls and many Christian inscriptions were carved into the Parthenon's columns. These renovations inevitably led to the removal and dispersal of some of the sculptures. Those depicting gods were either possibly re-interpreted according to a Christian theme, or removed.

The Parthenon became the fourth most important Christian pilgrimage destination in the Eastern Roman Empire after Constantinople, Ephesos, and Thessalonica. In 1018, the emperor Basil II went on a pilgrimage to Athens directly after his final victory over the Bulgarians for the sole purpose of worshipping at the Parthenon. In medieval Greek accounts it is called the Temple of Theotokos Atheniotissa and often indirectly referred to as famous without explaining exactly which temple they were referring to, thus establishing that it was indeed well known.

At the time of the Latin occupation, it became for about 250 years a Roman Catholic church of Our Lady. During this period a tower, used either as a watchtower or bell tower and containing a spiral staircase, was constructed at the southwest corner of the cella, and vaulted tombs were built beneath the Parthenon's floor.

In 1456, Ottoman Turkish forces invaded Athens and laid siege to a Florentine army defending the Acropolis until June 1458, when it surrendered to the Turks. The Turks may have briefly restored the Parthenon to the Greek Orthodox Christians for continued use as a church. Some time before the close of the fifteenth century, the Parthenon became a mosque.

The precise circumstances under which the Turks appropriated it for use as a mosque are unclear; one account states that Mehmed II ordered its conversion as punishment for an Athenian plot against Ottoman rule. The apse became a mihrab, the tower previously constructed during the Roman Catholic occupation of the Parthenon was extended upwards to become a minaret, a minbar was installed, the Christian altar and iconostasis were removed, and the walls were whitewashed to cover icons of Christian saints and other Christian imagery.

Despite the alterations accompanying the Parthenon's conversion into a church and subsequently a mosque, its structure had remained basically intact. In 1667 the Turkish traveller Evliya Çelebi expressed marvel at the Parthenon's sculptures and figuratively described the building as "like some impregnable fortress not made by human agency". He composed a poetic supplication that it, as "a work less of human hands than of Heaven itself, should remain standing for all time". The French artist Jacques Carrey in 1674 visited the Acropolis and sketched the Parthenon's sculptural decorations. Early in 1687, an engineer named Plantier sketched the Parthenon for the Frenchman Graviers d’Ortières. These depictions, particularly those made by Carrey, provide important, and sometimes the only, evidence of the condition of the Parthenon and its various sculptures prior to the devastation it suffered in late 1687 and the subsequent looting of its art objects.

In 1687, the Parthenon was extensively damaged in the greatest catastrophe to befall it in its long history. As part of the Great Turkish War (1683–1699), the Venetians sent an expedition led by Francesco Morosini to attack Athens and capture the Acropolis. The Ottoman Turks fortified the Acropolis and used the Parthenon as a gunpowder magazine – despite having been forewarned of the dangers of this use by the 1656 explosion that severely damaged the Propylaea – and as a shelter for members of the local Turkish community. On 26 September a Venetian mortar round, fired from the Hill of Philopappus, blew up the magazine, and the building was partly destroyed. The explosion blew out the building's central portion and caused the cella's walls to crumble into rubble. Greek architect and archaeologist Kornilia Chatziaslani writes that "...three of the sanctuary’s four walls nearly collapsed and three-fifths of the sculptures from the frieze fell. Nothing of the roof apparently remained in place. Six columns from the south side fell, eight from the north, as well as whatever remained from eastern porch, except for one column. The columns brought down with them the enormous marble architraves, triglyphs and metopes." About three hundred people were killed in the explosion, which showered marble fragments over nearby Turkish defenders and caused large fires that burned until the following day and consumed many homes.

Accounts written at the time conflict over whether this destruction was deliberate or accidental; one such account, written by the German officer Sobievolski, states that a Turkish deserter revealed to Morosini the use to which the Turks had put the Parthenon; expecting that the Venetians would not target a building of such historic importance. Morosini was said to have responded by directing his artillery to aim at the Parthenon. Subsequently, Morosini sought to loot sculptures from the ruin and caused further damage in the process. Sculptures of Poseidon and Athena's horses fell to the ground and smashed as his soldiers tried to detach them from the building's west pediment.

The following year, the Venetians abandoned Athens to avoid a confrontation with a large force the Turks had assembled at Chalcis; at that time, the Venetians had considered blowing up what remained of the Parthenon along with the rest of the Acropolis to deny its further use as a fortification to the Turks, but that idea was not pursued.

After the Turks had recaptured the Acropolis they used some of the rubble produced by this explosion to erect a smaller mosque within the shell of the ruined Parthenon. For the next century and a half, portions of the remaining structure were looted for building material and any remaining objects of value.

The 18th century was a period of Ottoman stagnation; as a result, many more Europeans found access to Athens, and the picturesque ruins of the Parthenon were much drawn and painted, spurring a rise in philhellenism and helping to arouse sympathy in Britain and France for Greek independence. Amongst those early travellers and archaeologists were James Stuart and Nicholas Revett, who were commissioned by the Society of Dilettanti to survey the ruins of classical Athens. What they produced was the first measured drawings of the Parthenon published in 1787 in the second volume of "Antiquities of Athens Measured and Delineated". In 1801, the British Ambassador at Constantinople, the Earl of Elgin, obtained a questionable "firman" (edict) from the Sultan, whose existence or legitimacy has not been proved until today, to make casts and drawings of the antiquities on the Acropolis, to demolish recent buildings if this was necessary to view the antiquities, and to remove sculptures from them.

When independent Greece gained control of Athens in 1832, the visible section of the minaret was demolished; only its base and spiral staircase up to the level of the architrave remain intact. Soon all the medieval and Ottoman buildings on the Acropolis were destroyed. However, the image of the small mosque within the Parthenon's cella has been preserved in Joly de Lotbinière's photograph, published in Lerebours's "Excursions Daguerriennes" in 1842: the first photograph of the Acropolis. The area became a historical precinct controlled by the Greek government. Today it attracts millions of tourists every year, who travel up the path at the western end of the Acropolis, through the restored Propylaea, and up the Panathenaic Way to the Parthenon, which is surrounded by a low fence to prevent damage.

The dispute centres around the Parthenon Marbles removed by Thomas Bruce, 7th Earl of Elgin, from 1801 to 1803, which are in the British Museum. A few sculptures from the Parthenon are also in the Louvre in Paris, in Copenhagen, and elsewhere, but more than half are in the Acropolis Museum in Athens. A few can still be seen on the building itself. The Greek government has campaigned since 1983 for the British Museum to return the sculptures to Greece. The British Museum has steadfastly refused to return the sculptures, and successive British governments have been unwilling to force the Museum to do so (which would require legislation). Nevertheless, talks between senior representatives from Greek and British cultural ministries and their legal advisors took place in London on 4 May 2007. These were the first serious negotiations for several years, and there were hopes that the two sides may move a step closer to a resolution.

In 1975, the Greek government began a concerted effort to restore the Parthenon and other Acropolis structures. After some delay, a Committee for the Conservation of the Acropolis Monuments was established in 1983. The project later attracted funding and technical assistance from the European Union. An archaeological committee thoroughly documented every artifact remaining on the site, and architects assisted with computer models to determine their original locations. Particularly important and fragile sculptures were transferred to the Acropolis Museum. A crane was installed for moving marble blocks; the crane was designed to fold away beneath the roofline when not in use. In some cases, prior re-constructions were found to be incorrect. These were dismantled, and a careful process of restoration began. Originally, various blocks were held together by elongated iron H pins that were completely coated in lead, which protected the iron from corrosion. Stabilizing pins added in the 19th century were not so coated, and corroded. Since the corrosion product (rust) is expansive, the expansion caused further damage by cracking the marble.





</doc>
<doc id="23673" url="https://en.wikipedia.org/wiki?curid=23673" title="Pachomius the Great">
Pachomius the Great

Saint Pachomius (, ca. 292–348), also known as Pachome and Pakhomius (), is generally recognized as the founder of Christian cenobitic monasticism. Coptic churches celebrate his feast day on 9 May, and Eastern Orthodox and Roman Catholic churches mark his feast on 15 May or 28 May. In the Lutheran Church, the saint is remembered as a renewer of the church, along with his contemporary (and fellow desert saint), Anthony of Egypt on January 17.

Saint Pachomius was born in 292 in Thebes (Luxor, Egypt) to pagan parents. According to his hagiography, at age 21, Pachomius was swept up against his will in a Roman army recruitment drive, a common occurrence during this period of turmoil and civil war. With several other youths, he was put onto a ship that floated down the Nile and arrived at Thebes in the evening. Here he first encountered local Christians, who customarily brought food and comfort daily to the impressed troops. This made a lasting impression, and Pachomius vowed to investigate Christianity further when he got out. He was able to leave the army without ever having to fight, was converted and baptized (314).

Pachomius then came into contact with several well known ascetics and decided to pursue that path under the guidance of the hermit named Palaemon (317). One of his devotions, popular at the time, was praying with his arms stretched out in the form of a cross. After studying seven years with Palaemon, Pachomius set out to lead the life of a hermit near St. Anthony of Egypt, whose practices he imitated until Pachomius heard a voice in Tabennisi that told him to build a dwelling for the hermits to come to. An earlier ascetic named Macarius had created a number of proto-monasteries called lavra, or cells, where holy men who were physically or mentally unable to achieve the rigors of Anthony's solitary life would live in a community setting.

Pachomius established his first monastery between 318 and 323 at Tabennisi, Egypt. His elder brother John joined him, and soon more than 100 monks lived nearby. Pachomius set about organizing these cells into a formal organization. Until then, Christian asceticism had been solitary or "eremitic" with male or female monastics living in individual huts or caves and meeting only for occasional worship services. Pachomius created the community or "cenobitic" organization, in which male or female monastics lived together and held their property in common under the leadership of an abbot or abbess. Pachomius realized that some men, acquainted only with the eremitical life, might speedily become disgusted if the distracting cares of the cenobitical life were thrust too abruptly upon them. He therefore allowed them to devote their whole time to spiritual exercises, undertaking all the community's administrative tasks himself. The community hailed Pachomius as "Abba" ("father" in Hebrew), from which "Abbot" derives.
The monastery at Tabennisi, though enlarged several times, soon became too small and a second was founded at Pabau (Faou). After 336, Pachomius spent most of his time at Pabau. Though Pachomius sometimes acted as lector for nearby shepherds, neither he nor any of his monks became priests. St Athanasius visited and wished to ordain him in 333, but Pachomius fled from him. Athanasius' visit was probably a result of Pachomius' zealous defence of orthodoxy against Arianism. Basil of Caesarea visited, then took many of Pachomius' ideas, which he adapted and implemented in Caesarea. This ascetic rule, or Ascetica, is still used today by the Eastern Orthodox Church, comparable to that of the Rule of St. Benedict in the West.

Pachomius continued as abbot to the cenobites for some forty years. During an epidemic (probably plague), Pachomius called the monks, strengthened their faith, and appointed his successor. Pachomius then died on 14 Pashons, 64 A.M. (9 May 348 A.D.)

By the time Pachomius died (c. 345) eight monasteries and several hundred monks followed his guidance. Within a generation, cenobic practices spread from Egypt to Palestine and the Judean Desert, Syria, North Africa and eventually Western Europe. The number of monks, rather than the number of monasteries, may have reached 7000.

His reputation as a holy man has endured. As mentioned above, several liturgical calendars commemorate Pachomius. Among many miracles attributed to Pachomius, that though he had never learned the Greek or Latin tongues, he sometimes miraculously spoke them. Pachomius is also credited with being the first Christian to use and recommend use of a prayer rope.

Examples of purely Coptic literature are the works of Abba Antonius and Abba Pachomius, who spoke only Coptic, and the sermons and preachings of Abba Shenouda, who chose to write only in Coptic.

The Pachomian system tended to treat religious literature as mere written instructions.

The name of the saint is of Coptic origin: "pakhōm" from "akhōm" "eagle or falcon" ( "p"- at the beginning is the Coptic definite article). Into Greek it was adopted as Παχούμιος and Παχώμιος. By Greek folk etymology it was sometimes interpreted as "broad-shouldered" from παχύς "thick, large" and ὦμος "shoulder".


Further reading

Notes



</doc>
<doc id="23674" url="https://en.wikipedia.org/wiki?curid=23674" title="Philosophical Investigations">
Philosophical Investigations

Philosophical Investigations () is a work by the philosopher Ludwig Wittgenstein, first published, posthumously, in 1953, in which Wittgenstein discusses numerous problems and puzzles in the fields of semantics, logic, philosophy of mathematics, philosophy of psychology, philosophy of action, and philosophy of mind. He puts forth the view that conceptual confusions surrounding language use are at the root of most philosophical problems, contradicting or discarding much of what he argued in his earlier work, the "Tractatus Logico-Philosophicus" (1921).

He alleges that the problems are traceable to a set of related assumptions about the nature of language, which themselves presuppose a particular conception of the essence of language. This conception is considered and ultimately rejected for being too general; that is, as an essentialist account of the nature of language it is simply too narrow to be able to account for the variety of things we do with language. Wittgenstein begins the book with a quotation from St. Augustine, whom he cites as a proponent of the generalized and limited conception that he then summarizes:
The individual words in language name objects—sentences are combinations of such names. In this picture of language we find the roots of the following idea: Every word has a meaning. This meaning is correlated with the word. It is the object for which the word stands.
He then sets out throughout the rest of the book to demonstrate the limitations of this conception, including, he argues, with many traditional philosophical puzzles and confusions that arise as a result of this limited picture. "Philosophical Investigations" is highly influential. Within the analytic tradition, the book is considered by many as being one of the most important philosophical works of the 20th century, and it continues to influence contemporary philosophers, especially those studying mind and language.

"Philosophical Investigations" was not ready for publication when Wittgenstein died in 1951. G. E. M. Anscombe translated Wittgenstein's manuscript into English, and it was first published in 1953. There are multiple editions of "Philosophical Investigations" with the popular third edition and 50th anniversary edition having been edited by Anscombe: 

The text is divided into two parts, consisting of what Wittgenstein calls, in the preface, "Bemerkungen", translated by Anscombe as "remarks". In the first part, these remarks are rarely more than a paragraph long and are numbered sequentially. In the second part, the remarks are longer and numbered using Roman numerals. In the index, remarks from the first part are referenced by their number rather than page; however, references from the second part are cited by page number. The comparatively unusual nature of the second part is due to the fact that it comprises notes that Wittgenstein may have intended to re-incorporate into the first part. Subsequent to his death it was published as a "Part II" in the first, second and third editions. However, in light of continuing uncertainty about Wittgenstein's intentions regarding this material, the fourth edition (2009) re-titles "Part I" as "Philosophical Investigations" proper, and "Part II" as "Philosophy of Psychology – A Fragment."

In standard references, a small letter following a page, section, or proposition number indicates a paragraph.

"Philosophical Investigations" is unique in its approach to philosophy. A typical philosophical text presents a philosophical problem, summarizes and critiques various alternative approaches to solving it, presents its approach, and then argues in favour of that approach. In contrast, Wittgenstein's book treats philosophy as an activity, rather along the lines of Socrates's famous method of maieutics; he has the reader work through various problems, participating actively in the investigation. Rather than presenting a philosophical problem and its solution, Wittgenstein engages in a dialogue, where he provides a language-game (a more or less ordinary use of the words in question), that describes how one might be inclined to think about it, and then shows why that inclination suffers from conceptual confusion. The following is an excerpt from the first entry in the book that exemplifies this method:

...think of the following use of language: I send someone shopping. I give him a slip marked 'five red apples'. He takes the slip to the shopkeeper, who opens the drawer marked 'apples', then he looks up the word 'red' in a table and finds a colour sample opposite it; then he says the series of cardinal numbers—I assume that he knows them by heart—up to the word 'five' and for each number he takes an apple of the same colour as the sample out of the drawer.—It is in this and similar ways that one operates with words—"But how does he know where and how he is to look up the word 'red' and what he is to do with the word 'five'?" Well, I assume that he "acts" as I have described. Explanations come to an end somewhere.—But what is the meaning of the word 'five'? No such thing was in question here, only how the word 'five' is used.

This example is typical of the book's style. We can see each of the steps in Wittgenstein's method:


Through such language-games, Wittgenstein attempts to get the reader to come to certain difficult philosophical conclusions independently; he does not merely argue in favor of theories. 

The "Investigations" deals largely with the difficulties of language and meaning. Wittgenstein viewed the tools of language as being fundamentally simple, and he believed that philosophers had obscured this simplicity by misusing language and by asking meaningless questions. He attempted in the "Investigations" to make things clear: ""Der Fliege den Ausweg aus dem Fliegenglas zeigen""—to show the fly the way out of the fly bottle.

A common summary of his argument is that meaning is use—words are not defined by reference to the objects they designate, nor by the mental representations one might associate with them, but by how they are used. For example, this means there is no need to postulate that there is something called "good" that exists independently of any good deed. If this is an anthropological perspective, then it can be said that it contrasts with Platonic realism and with Gottlob Frege's notions of sense and reference. This argument has been labeled by some authors as "anthropological holism."

Section 43 in Wittgenstein's "Philosophical Investiations" reads: "For a large class of cases—though not for all—in which we employ the word "meaning" it can be defined thus: the meaning of a word is its use in the language." This suggests that in quite an early section of the book, Wittgenstein may think that meaning cannot be so easily glossed as how the word is used.

Wittgenstein rejects a variety of ways of thinking about what the meaning of a word is, or how meanings can be identified. He shows how, in each case, the "meaning" of the word presupposes our ability to use it. He first asks the reader to perform a thought experiment: to come up with a definition of the word "game". While this may at first seem a simple task, he then goes on to lead us through the problems with each of the possible definitions of the word "game". Any definition that focuses on amusement leaves us unsatisfied since the feelings experienced by a world class chess player are very different from those of a circle of children playing Duck Duck Goose. Any definition that focuses on competition will fail to explain the game of catch, or the game of solitaire. And a definition of the word "game" that focuses on rules will fall on similar difficulties.

The essential point of this exercise is often missed. Wittgenstein's point is not that it is impossible to define "game", but that "we don't have a definition, and we don't need one", because even without the definition, we "use" the word successfully. Everybody understands what we mean when we talk about playing a game, and we can even clearly identify and correct inaccurate uses of the word, all without reference to any definition that consists of necessary and sufficient conditions for the application of the concept of a game. The German word for "game", "Spiele/Spiel", has a different sense than in English; the meaning of "Spiele" also extends to the concept of "play" and "playing." This German sense of the word may help readers better understand Wittgenstein's context in the remarks regarding games.

Wittgenstein argues that definitions emerge from what he termed "forms of life", roughly the culture and society in which they are used. Wittgenstein stresses the social aspects of cognition; to see how language works for most cases, we have to see how it functions in a specific social situation. It is this emphasis on becoming attentive to the social backdrop against which language is rendered intelligible that explains Wittgenstein's elliptical comment that "If a lion could talk, we could not understand him." However, in proposing the thought experiment involving the fictional character, Robinson Crusoe, a captain shipwrecked on a desolate island with no other inhabitant, Wittgenstein shows that language is not in all cases a social phenomenon (although, they are for most cases); instead the criterion for a language is grounded in a set of interrelated normative activities: teaching, explanations, techniques and criteria of correctness. In short, it is essential that a language is shareable, but this does not imply that for a language to function that it is in fact already shared.

Wittgenstein rejects the idea that ostensive definitions can provide us with the meaning of a word. For Wittgenstein, the thing that the word stands for does "not" give the meaning of the word. Wittgenstein argues for this making a series of moves to show that to understand an ostensive definition presupposes an understanding of the way the word being defined is used. So, for instance, there is no difference between pointing to a piece of paper, to its colour, or to its shape; but understanding the difference is crucial to using the paper in an ostensive definition of a shape or of a colour.

Why is it that we are sure a particular activity — e.g. Olympic target shooting — is a game while a similar activity — e.g. military sharp shooting — is not? Wittgenstein's explanation is tied up with an important analogy. How do we recognize that two people we know are related to one another? We may see similar height, weight, eye color, hair, nose, mouth, patterns of speech, social or political views, mannerisms, body structure, last names, etc. If we see enough matches we say we've noticed a family resemblance. It is perhaps important to note that this is not always a conscious process — generally we don't catalog various similarities until we reach a certain threshold, we just intuitively "see" the resemblances. Wittgenstein suggests that the same is true of language. We are all familiar (i.e. socially) with enough things which "are games" and enough things which "are not games" that we can categorize new activities as either games or not.

This brings us back to Wittgenstein's reliance on indirect communication, and his reliance on thought-experiments. Some philosophical confusions come about because we aren't able to "see" family resemblances. We've made a mistake in understanding the vague and intuitive rules that language uses, and have thereby tied ourselves up in philosophical knots. He suggests that an attempt to untangle these knots requires more than simple deductive arguments pointing out the problems with some particular position. Instead, Wittgenstein's larger goal is to try to divert us from our philosophical problems long enough to become aware of our intuitive ability to "see" the family resemblances.

Wittgenstein develops this discussion of games into the key notion of a "language-game". Wittgenstein introduces the term using simple examples, but intends it to be used for the many ways in which we use language. The central component of language games is that they are uses of language, and language is used in multifarious ways. For example, in one language-game, a word might be used to stand for (or refer to) an object, but in another the same word might be used for giving orders, or for asking questions, and so on. The famous example is the meaning of the word "game". We speak of various kinds of games: board games, betting games, sports, "war games". These are all different uses of the word "games". Wittgenstein also gives the example of "Water!", which can be used as an exclamation, an order, a request, or as an answer to a question. The meaning of the word depends on the language-game within which it is being used. Another way Wittgenstein puts the point is that the word "water" has no meaning apart from its use within a language-game. One might use the word as an order to have someone else bring you a glass of water. But it can also be used to warn someone that the water has been poisoned. One might even use the word as code by members of a secret society.

Wittgenstein does not limit the application of his concept of language games to word-meaning. He also applies it to sentence-meaning. For example, the sentence "Moses did not exist" (§79) can mean various things. Wittgenstein argues that independently of use the sentence does not yet 'say' anything. It is 'meaningless' in the sense of not being significant for a particular purpose. It only acquires significance if we fix it within some context of use. Thus, it fails to say anything because the sentence as such does not yet determine some particular use. The sentence is only meaningful when it is used to say something. For instance, it can be used so as to say that no person or historical figure fits the set of descriptions attributed to the person that goes by the name of "Moses". But it can also mean that the leader of the Israelites was not called Moses. Or that there cannot have been anyone who accomplished all that the Bible relates of Moses, etc. What the sentence means thus depends on its context of use.

One general characteristic of games that Wittgenstein considers in detail is the way in which they consist in following rules. Rules constitute a family, rather than a class that can be explicitly defined. As a consequence, it is not possible to provide a definitive account of what it is to follow a rule. Indeed, he argues that "any" course of action can be made out to accord with some particular rule, and that therefore a rule cannot be used to explain an action. Rather, that one is following a rule or not is to be decided by looking to see if the actions conform to the expectations in the particular "form of life" in which one is involved. Following a rule is a social activity.

Wittgenstein also ponders the possibility of a language that talks about those things that are known only to the user, whose content is inherently private. The usual example is that of a language in which one names one's sensations and other subjective experiences, such that the meaning of the term is decided by the individual alone. For example, the individual names a particular sensation, on some occasion, 'S', and intends to use that word to refer to that sensation. Such a language Wittgenstein calls a "private language".

Wittgenstein presents several perspectives on the topic. One point he makes is that it is incoherent to talk of "knowing" that one is in some particular mental state. Whereas others can learn of my pain, for example, I simply "have" my own pain; it follows that one does not "know" of one's own pain, one simply "has" a pain. For Wittgenstein, this is a grammatical point, part of the way in which the language-game involving the word "pain" is played.

Although Wittgenstein certainly argues that the notion of private language is incoherent, because of the way in which the text is presented the exact nature of the argument is disputed. First, he argues that a private language is not really a language at all. This point is intimately connected with a variety of other themes in his later works, especially his investigations of "meaning". For Wittgenstein, there is no single, coherent "sample" or "object" that we can call "meaning". Rather, the supposition that there are such things is the source of many philosophical confusions. Meaning is a complicated phenomenon that is woven into the fabric of our lives. A good first approximation of Wittgenstein's point is that meaning is a "social" event; meaning happens "between" language users. As a consequence, it makes no sense to talk about a private language, with words that "mean" something in the absence of other users of the language.

Wittgenstein also argues that one couldn't possibly "use" the words of a private language. He invites the reader to consider a case in which someone decides that each time she has a particular sensation she will place a sign S in a diary. Wittgenstein points out that in such a case one could have no criteria for the correctness of one's use of S. Again, several examples are considered. One is that perhaps using S involves mentally consulting a table of sensations, to check that one has associated S correctly; but in this case, how could the mental table be checked for its correctness? It is "[a]s if someone were to buy several copies of the morning paper to assure himself that what it said was true", as Wittgenstein puts it. One common interpretation of the argument is that while one may have direct or privileged access to one's "current" mental states, there is no such infallible access to identifying previous mental states that one had in the past. That is, the only way to check to see if one has applied the symbol S correctly to a certain mental state is to introspect and determine whether the current sensation is identical to the sensation previously associated with S. And while identifying one's current mental state of remembering may be infallible, whether one remembered correctly is not infallible. Thus, for a language to be used at all it must have some public criterion of identity.

Often, what is widely regarded as a deep philosophical problem will vanish, argues Wittgenstein, and eventually be seen as a confusion about the significance of the words that philosophers use to frame such problems and questions. It is only in this way that it is interesting to talk about something like a "private language" — i.e., it is helpful to see how the "problem" results from a misunderstanding.

To sum up: Wittgenstein asserts that, if something is a language, it "cannot" be (logically) private; and if something "is" private, it is not (and cannot be) a language.

Another point that Wittgenstein makes against the possibility of a private language involves the beetle-in-a-box thought experiment. He asks the reader to imagine that each person has a box, inside of which is something that everyone intends to refer to with the word "beetle". Further, suppose that no one can look inside another's box, and each claims to know what a "beetle" is only by examining their own box. Wittgenstein suggests that, in such a situation, the word "beetle" could not be the name of a thing, because supposing that each person has something completely different in their boxes (or nothing at all) does not change the meaning of the word; the beetle as a private object "drops out of consideration as irrelevant". Thus, Wittgenstein argues, if we can talk about something, then it is not "private", in the sense considered. And, contrapositively, if we consider something to be indeed private, it follows that we "cannot talk about it".

The discussion of private languages was revitalized in 1982 with the publication of Saul Kripke's book "Wittgenstein on Rules and Private Language". In this work, Kripke uses Wittgenstein's text to develop a particular type of skepticism about rules that stresses the "communal" nature of language-use as grounding meaning. Critics of Kripke's version of Wittgenstein have facetiously referred to it as "Kripkenstein," scholars such as Gordon Baker, Peter Hacker, Colin McGinn, and John McDowell seeing it as a radical misinterpretation of Wittgenstein's text. Other philosophers – such as Martin Kusch – have defended Kripke's views.

Wittgenstein's investigations of language lead to several issues concerning the mind. His key target of criticism is any form of extreme mentalism that posits mental states that are entirely unconnected to the subject's environment. For Wittgenstein, thought is inevitably tied to language, which is inherently social; therefore, there is no 'inner' space in which thoughts can occur. Part of Wittgenstein's credo is captured in the following proclamation: "An 'inner process' stands in need of outward criteria." This follows primarily from his conclusions about private languages: similarly, a private mental state (a sensation of pain, for example) cannot be adequately discussed without public criteria for identifying it.

According to Wittgenstein, those who insist that consciousness (or any other apparently subjective mental state) is conceptually unconnected to the external world are mistaken. Wittgenstein explicitly criticizes so-called conceivability arguments: "Could one imagine a stone's having consciousness? And if anyone can do so—why should that not merely prove that such image-mongery is of no interest to us?" He considers and rejects the following reply as well:

"But if I suppose that someone is in pain, then I am simply supposing that he has just the same as I have so often had." — That gets us no further. It is as if I were to say: "You surely know what 'It is 5 o'clock here' means; so you also know what 'It's 5 o'clock on the sun' means. It means simply that it is just the same there as it is here when it is 5 o'clock." — The explanation by means of "identity" does not work here.

Thus, according to Wittgenstein, mental states are intimately connected to a subject's environment, especially their linguistic environment, and conceivability or imaginability. Arguments that claim otherwise are misguided. Wittgenstein has also said that "language is inherent and transcendental", which is also not difficult to understand, since we can only comprehend and explain transcendental affairs through language.

From his remarks on the importance of public, observable behavior (as opposed to private experiences), it may seem that Wittgenstein is simply a behaviorist—one who thinks that mental states are nothing over and above certain behavior. However, Wittgenstein resists such a characterization; he writes (considering what an objector might say):

"Are you not really a behaviourist in disguise? Aren't you at bottom really saying that everything except human behaviour is a fiction?" — If I do speak of a fiction, then it is of a "grammatical" fiction.

Clearly, Wittgenstein did not want to be a behaviorist, nor did he want to be a cognitivist or a phenomenologist. He is, of course, primarily concerned with facts of linguistic usage. However, some argue that Wittgenstein is basically a behaviorist because he considers facts about language use as all there is. Such a claim is controversial, since it is not explicitly endorsed in the "Investigations".

In addition to ambiguous sentences, Wittgenstein discussed figures that can be seen and understood in two different ways. Often one can see something in a straightforward way — seeing "that" it is a rabbit, perhaps. But, at other times, one notices a particular aspect — seeing it "as" something.

An example Wittgenstein uses is the "duckrabbit", an ambiguous image that can be "seen as" either a duck or a rabbit. When one looks at the duck-rabbit and sees a rabbit, one is not "interpreting" the picture as a rabbit, but rather "reporting" what one sees. One just sees the picture as a rabbit. But what occurs when one sees it first as a duck, then as a rabbit? As the gnomic remarks in the "Investigations" indicate, Wittgenstein isn't sure. However, he is sure that it could not be the case that the external world stays the same while an 'internal' cognitive change takes place.

According to the standard reading, in the "Philosophical Investigations" Wittgenstein repudiates many of his own earlier views, expressed in the "Tractatus Logico-Philosophicus". The "Tractatus", as Bertrand Russell saw it (though Wittgenstein took strong exception to Russell's reading), had been an attempt to set out a logically perfect language, building on Russell's own work. In the years between the two works Wittgenstein came to reject the idea that underpinned logical atomism, that there were ultimate "simples" from which a language should, or even could, be constructed.

In remark #23 of "Philosophical Investigations" he points out that the practice of human language is more complex than the simplified views of language that have been held by those who seek to explain or simulate human language by means of a formal system. It would be a disastrous mistake, according to Wittgenstein, to see language as being in any way analogous to formal logic.

Besides stressing the "Investigations"' opposition to the "Tractatus", there are critical approaches which have argued that there is much more continuity and similarity between the two works than supposed. One of these is the New Wittgenstein approach.

Norman Malcolm credits Piero Sraffa with providing Wittgenstein with the conceptual break that founded the "Philosophical Investigations", by means of a rude gesture on Sraffa's part:

"Wittgenstein was insisting that a proposition and that which it describes must have the same 'logical form', the same 'logical multiplicity', Sraffa made a gesture, familiar to Neapolitans as meaning something like disgust or contempt, of brushing the underneath of his chin with an outward sweep of the finger-tips of one hand. And he asked: 'What is the logical form of that?'"

The preface itself, dated January 1945, credits Sraffa for the "most consequential ideas" of the book.

Bertrand Russell made the following comment on the "Philosophical Investigations" in his book "My Philosophical Development":I have not found in Wittgenstein's Philosophical Investigations anything that seemed to me interesting and I do not understand why a whole school finds important wisdom in its pages. Psychologically this is surprising. The earlier Wittgenstein, whom I knew intimately, was a man addicted to passionately intense thinking, profoundly aware of difficult problems of which I, like him, felt the importance, and possessed (or at least so I thought) of true philosophical genius. The later Wittgenstein, on the contrary, seems to have grown tired of serious thinking and to have invented a doctrine which would make such an activity unnecessary. I do not for one moment believe that the doctrine which has these lazy consequences is true. I realize, however, that I have an overpoweringly strong bias against it, for, if it is true, philosophy is, at best, a slight help to lexicographers, and at worst, an idle tea-table amusement.Ernest Gellner wrote the book "Words and Things", in which he was fiercely critical of the work of Ludwig Wittgenstein, J. L. Austin, Gilbert Ryle, Antony Flew, P. F. Strawson and many others. Ryle refused to have the book reviewed in the philosophical journal "Mind" (which he edited), and Bertrand Russell (who had written an approving foreword) protested in a letter to "The Times". A response from Ryle and a lengthy correspondence ensued.


"Remarks in Part I of "Investigations" are preceded by the symbol ""§"". Remarks in Part II are referenced by their Roman numeral or their page number in the third edition.



</doc>
<doc id="23677" url="https://en.wikipedia.org/wiki?curid=23677" title="Poul Anderson">
Poul Anderson

Poul William Anderson (November 25, 1926 – July 31, 2001) was an American science fiction author who began his career in the 1940s and continued to write into the 21st century. Anderson authored several works of fantasy, historical novels, and short stories. His awards include seven Hugo Awards and three Nebula Awards.

Poul Anderson was born on November 25, 1926, in Bristol, Pennsylvania, of Scandinavian parents.
Shortly after his birth, his father, Anton Anderson, an engineer, moved the family to Texas, where they lived for over ten years. Following Anton Anderson's death, his widow took her children to Denmark. The family returned to the United States after the outbreak of World War II, settling eventually on a Minnesota farm. The frame story of his later novel "Three Hearts and Three Lions", before the fantasy part begins, is partly set in the Denmark which the young Anderson personally experienced.

While he was an undergraduate student at the University of Minnesota, Anderson's first stories were published by John W. Campbell in "Astounding Science Fiction": "Tomorrow's Children" by Anderson and F. N. Waldrop in March 1947 and a sequel, "Chain of Logic" by Anderson alone, in July. He earned his B.A. in physics with honors but made no serious attempt to work as a physicist; instead he became a free-lance writer after his graduation in 1948—and placed his third story in the December "Astounding".

Anderson married Karen Kruse in 1953 and moved with her to the San Francisco Bay area. Their daughter Astrid (now married to science fiction author Greg Bear) was born in 1954. They made their home in Orinda, California. Over the years Poul gave many readings at The Other Change of Hobbit bookstore in Berkeley, and his wife later donated his typewriter and desk to the store.

In 1965 Algis Budrys said that Anderson "has for some time been science fiction's best storyteller". He was a founding member of the Society for Creative Anachronism (SCA) in 1966 and of the Swordsmen and Sorcerers' Guild of America (SAGA), also in the mid-1960s. The latter was a loose-knit group of Heroic Fantasy authors led by Lin Carter, originally eight in number, with entry by credentials as a fantasy writer alone. Anderson was the sixth President of Science Fiction and Fantasy Writers of America, taking office in 1972.

Robert A. Heinlein dedicated his 1985 novel "The Cat Who Walks Through Walls" to Anderson and eight of the other members of the Citizens' Advisory Council on National Space Policy. The Science Fiction Writers of America made Anderson its 16th SFWA Grand Master in 1998 and the Science Fiction and Fantasy Hall of Fame inducted him in 2000, its fifth class of two deceased and two living writers. He died of cancer on July 31, 2001, after a month in the hospital. A few of his novels were first published posthumously.

Anderson is probably best known for adventure stories in which larger-than-life characters succeed gleefully or fail heroically. His characters were nonetheless thoughtful, often introspective, and well developed. His plot lines frequently involved the application of social and political issues in a speculative manner appropriate to the science fiction genre. He also wrote some quieter works, generally of shorter length, which appeared more often during the latter part of his career.

Much of his science fiction is thoroughly grounded in science (with the addition of unscientific but standard speculations such as faster-than-light travel). A specialty was imagining scientifically plausible non-Earthlike planets. Perhaps the best known was the planet of "The Man Who Counts"; Anderson adjusted its size and composition so that humans could live in the open air but flying intelligent aliens could evolve, and he explored consequences of these adjustments.

In many stories, Anderson commented on society and politics. Whatever other vicissitudes his views went through, he firmly retained his belief in the direct and inextricable connection between human liberty and expansion into space, for which reason he strongly cried out against any idea of space exploration being "a waste of money" or "unnecessary luxury".

The connection between space flight and freedom is clearly (as is stated explicitly in some of the stories) an extension of the nineteenth-century American concept of the Frontier, where malcontents can advance further and claim some new land, and pioneers either bring life to barren asteroids (as in "Tales of the Flying Mountains") or settle on Earth-like planets teeming with life, but not intelligent forms (such as New Europe in "Star Fox").

As he repeatedly expressed in his nonfiction essays, Anderson firmly held that going into space was not an unnecessary luxury but an existential need, and that abandoning space would doom humanity to "a society of brigands ruling over peasants".

This is graphically expressed in the chilling short story "Welcome". In it, humanity has abandoned space and is left with an overcrowded Earth where a small elite not only treats all the rest as chattel slaves, but also regularly practices cannibalism, their chefs preparing "roast suckling coolie" for their banquets.

Conversely, in the bleak Orwellian world of "The High Ones" where the Soviets have won the Third World War and gained control of the whole of Earth, the dissidents still have some hope, precisely because space flight has not been abandoned. By the end of the story, rebels have established themselves at another stellar system—where their descendants, the reader is told, would eventually build a liberating fleet and set out back to Earth.

While horrified by the prospect of the Soviets winning complete rule over the Earth, Anderson was not enthusiastic about having Americans in that role either. Several stories and books describing the aftermath of a total American victory in another world war, such as "Sam Hall" and its loose sequel "Three Worlds to Conquer" as well as "Shield", are scarcely less bleak than the above-mentioned depictions of a Soviet victory. Like Heinlein in "Solution Unsatisfactory", Anderson assumed that the imposition of an American military rule over the rest of the world would necessarily entail the destruction of American democracy and the imposition of a harsh tyrannical rule over the United States' own citizens.

Both Anderson's depiction of a Soviet-dominated world and that of an American-dominated one mention a rebellion breaking out in Brazil in the early 21st century, which is in both cases brutally put down by the dominant world power—the Brazilian rebels being characterized as "counter-revolutionaries" in the one case and as "communists" in the other.

In the early years of the Cold War—when he had been, as described by his later, more conservative self, a "flaming liberal"—Anderson pinned his hopes on the United Nations developing into a true world government. This is especially manifest in "Un-man", a future thriller where the Good Guys are agents of the UN Secretary General working to establish a world government while the Bad Guys are nationalists (especially American nationalists) who seek to preserve their respective nations' sovereignty at all costs. (The title has a double meaning: the hero is literally a UN man and has superhuman abilities which make his enemies fear him as an "un-man").

Anderson and his wife were among those who in 1968 signed a pro-Vietnam War advertisement in "Galaxy Science Fiction". By then Anderson had repudiated world government; a half-humorous remnant is the beginning of "Tau Zero": a future where the nations of the world entrusted Sweden with overseeing disarmament and found themselves living under the rule of the Swedish Empire. In "The Star Fox", he unfavorably depicts a future peace group called "World Militants for Peace". A more explicit expression of the same appears in the later "The Shield of Time" where a time-traveling young American woman from the 1990s pays a brief visit to a university campus of the 1960s and is not enthusiastic about what she sees there.

Anderson often returned to right-libertarianism and to the business leader as hero, most notably his character Nicholas van Rijn. Van Rijn is different from the archetype of a modern type of business executive, being more reflective of a Dutch Golden Age merchant of the 17th century. If he spends any time in boardrooms or plotting corporate takeovers, the reader remains ignorant of it, since nearly all his appearances are in the wilds of a space frontier.

Beginning in the 1970s, Anderson's historically grounded works were influenced by the theories of the historian John K. Hord, who argued that all empires follow the same broad cyclical pattern, into which the Terran Empire of the Dominic Flandry spy stories fit neatly.

The writer Sandra Miesel (1978) has argued that Anderson's overarching theme is the struggle against entropy and the heat death of the universe, a condition of perfect uniformity where nothing can happen.

In his numerous books and stories depicting conflict in science-fictional or fantasy settings, Anderson takes trouble to make both sides' points of view comprehensible. Even where there can be no doubt as to whose side the author is on, the antagonists are usually not depicted as villains but as honourable on their own terms. The reader is given access to their thoughts and feelings, and they often have a tragic dignity in defeat. Typical examples are "The Winter of the World" and "The People of the Wind".

A common theme in Anderson's works, and one with obvious origins in the Northern European legends, is that doing the "right" (wisest) thing often involves performing actions that, at face value, seem dishonorable, illegal, destructive, or downright evil. "The Man who Counts", Nicholas van Rijn is "The Man" because he is prepared to be tyrannical and callously manipulative so that he and his companions can survive. In "High Treason" the protagonist disobeys orders and betrays his subordinates to prevent a war crime that would bring severe retribution upon Humanity. In "A Knight of Ghosts and Shadows", Dominic Flandry first (effectively) lobotomizes his own son and then bombards the home planet of the Chereionite race in order to do his duty and prop up the Terran Empire. These actions affect their characters in different ways, and dealing with the repercussions of having done the "right" (but unpleasant) thing is often the major focus of his short stories. The general lesson seems to be that guilt is the penalty for action.

In "The Star Fox", a relationship of grudging respect is built up between the hero, space privateer Gunnar Heim, and his enemy Cynbe, an exceptionally gifted Alerione trained from a young age to understand his species' human enemies to the point of being alienated from his own kind. In the final scene, Cynbe challenges Heim to a space battle which only one of them would survive. Heim accepts, whereupon Cynbe says, "I thank you, my brother."

Anderson set much of his work in the past, often with the addition of magic, or in alternate or future worlds that resemble past eras. A specialty was his ancestral Scandinavia, as in his novel versions of the legends of Hrólf Kraki ("Hrolf Kraki's Saga") and Haddingus ("The War of the Gods"). Frequently he presented such worlds as superior to the dull, over-civilized present. Notable depictions of this superiority are the prehistoric world of "The Long Remembering", the quasi-medieval society of "No Truce with Kings", and the untamed Jupiter of "Call Me Joe" and "Three Worlds to Conquer". He handled the lure and power of atavism satirically in "Pact", critically in "The Queen of Air and Darkness" and "The Night Face", and tragically in "Goat Song".

His 1965 novel "The Corridors of Time" alternates between the European Stone Age and a repressive future. In this vision of tomorrow, almost everyone is either an agricultural serf or an industrial slave, but the rulers genuinely believe they are creating a better world. Set largely in Denmark, it treats the Neolithic society with knowledge and respect while not hiding its own faults. It is there that the protagonist, having access to literally all periods of the past and future, finally decides to settle down and finds a happy and satisfying life.

In many stories, a representative of a technologically advanced society underestimates "primitives" and pays a high price for it. In "The High Crusade", aliens who land in medieval England in the expectation of an easy conquest find that they are not immune to swords and arrows. In "The Only Game in Town", a Mongol warrior, while not knowing that the two "magicians" he meets are time travelers from the future, correctly guesses their intentions—and captures them with the help of the "magic" flashlight they had given him in an attempt to impress him. In another time-travel tale, "The Shield of Time", a "time policeman" from the Twentieth Century, equipped with information and technologies from much further in the future, is outwitted by a medieval knight and barely escapes with his life. Yet another story, "The Man Who Came Early", features a 20th-century United States Army soldier stationed in Iceland who is transported to the tenth century. Although he is full of ideas, his lack of practical knowledge of how to implement them and his total unfamiliarity with the technology and customs of the period lead to his downfall.

Anderson wrote "Uncleftish Beholding", an introduction to atomic theory, using only Germanic-rooted words. Fitting his love for olden years, this kind of learned writing has been named Ander-Saxon after him.

The story told in "The Shield of Time" is also an example of a tragic conflict, another common theme in Anderson's writing. The knight tries to do his best in terms of his own society and time, but his actions might bring about a horrible Twentieth Century (even more horrible than the one we know). Therefore, the Time Patrol protagonists, who like the young knight and wish him well (the female protagonist comes close to falling in love with him), have no choice but to fight and ultimately kill him.

In "The Sorrow of Odin the Goth" a time-travelling American anthropologist is assigned to study the culture of an ancient Gothic tribe by regular visits every few decades. Gradually he is drawn into close involvement, feeling protective towards the Goths (many of them his own descendants, following a brief and poignant liaison with a Gothic girl who died in childbirth), and they identify him as the god Odin/Wodan. Then he finds that he must cruelly betray his beloved Goths, since a ballad says that Odin did so; failure to fulfill his prescribed role might change history and bring the whole of the Twentieth Century as we know it crashing down. In the final scene he cries out in anguish: "Not even the gods can defy the Norns!"—giving a new twist to this central aspect of the Norse religion.

In "The Pirate", the hero is duty-bound to deny a band of people from societies blighted by poverty the chance for a new start on a new planet, because their settling the planet would eradicate the remnants of the artistic and articulate beings who lived there before. A similar theme but with much higher stakes appears in "Sister Planet": although terraforming Venus would provide new hope to starving people on the overcrowded Earth, it would exterminate Venus's just-discovered intelligent race, and the hero can avert that genocide only by murdering his best friends.

In "Delenda Est" the stakes are the highest imaginable. Time-travelling outlaws have created a new 20th Century—"not better or worse, just completely different". The hero can fight the outlaws and restore his (and our) familiar history, but only at the price of totally destroying the world which has taken its place. "Risking your neck in order to negate a world full of people like yourself" is how the hero describes what he eventually undertakes.


Philip K. Dick's story "Waterspider" features Poul Anderson as one of the main characters.

In the opening of S.M. Stirling's novel "In the Courts of the Crimson Kings", a group of science fiction authors, including Poul Anderson, watch first contact with the book's Martians while attending an SF convention. Poul supplies the beer.





</doc>
<doc id="23678" url="https://en.wikipedia.org/wiki?curid=23678" title="Panspermia">
Panspermia

Panspermia () is the hypothesis that life exists throughout the Universe, distributed by space dust, meteoroids, asteroids, comets, planetoids, and also by spacecraft carrying unintended contamination by microorganisms.

Panspermia hypotheses propose (for example) that microscopic life-forms that can survive the effects of space (such as extremophiles) can become trapped in debris ejected into space after collisions between planets and small Solar System bodies that harbor life. Some organisms may travel dormant for an extended amount of time before colliding randomly with other planets or intermingling with protoplanetary disks. Under certain ideal impact circumstances (into a body of water, for example), and ideal conditions on a new planet's surfaces, it is possible that the surviving organisms could become active and begin to colonize their new environment. Panspermia studies concentrate not on how life began, but on the methods that may cause its distribution in the Universe.

Pseudo-panspermia (sometimes called ""soft panspermia"" or ""molecular panspermia"") argues that the pre-biotic organic building-blocks of life originated in space, became incorporated in the solar nebula from which planets condensed, and were further—and continuously—distributed to planetary surfaces where life then emerged (abiogenesis). From the early 1970s it started to become evident that interstellar dust included a large component of organic molecules. Interstellar molecules are formed by chemical reactions within very sparse interstellar or circumstellar clouds of dust and gas. The dust plays a critical role in shielding the molecules from the ionizing effect of ultraviolet radiation emitted by stars.

The chemistry leading to life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the Universe was only 10 to 17 million years old. Though the presence of life is confirmed only on the Earth, some scientists think that extraterrestrial life is not only plausible, but probable or inevitable. Probes and instruments have started examining other planets and moons in the Solar System and in other planetary systems for evidence of having once supported simple life, and projects such as SETI attempt to detect radio transmissions from possible extra-terrestrial civilizations.

The first known mention of the term was in the writings of the 5th-century BC Greek philosopher Anaxagoras. Panspermia began to assume a more scientific form through the proposals of Jöns Jacob Berzelius (1834), Hermann E. Richter (1865), Kelvin (1871), Hermann von Helmholtz (1879) and finally reaching the level of a detailed scientific hypothesis through the efforts of the Swedish chemist Svante Arrhenius (1903).

Fred Hoyle (1915–2001) and Chandra Wickramasinghe (born 1939) were influential proponents of panspermia. In 1974 they proposed the hypothesis that some dust in interstellar space was largely organic (containing carbon), which Wickramasinghe later proved to be correct. Hoyle and Wickramasinghe further contended that life forms continue to enter the Earth's atmosphere, and may be responsible for epidemic outbreaks, new diseases, and the genetic novelty necessary for macroevolution.

In an Origins Symposium presentation on April 7, 2009, physicist Stephen Hawking stated his opinion about what humans may find when venturing into space, such as the possibility of alien life through the theory of panspermia: "Life could spread from planet to planet or from stellar system to stellar system, carried on meteors."

Three series of astrobiology experiments have been conducted outside the International Space Station between 2008 and 2015 (EXPOSE) where a wide variety of biomolecules, microorganisms, and their spores were exposed to the solar flux and vacuum of space for about 1.5 years. Some organisms survived in an inactive state for considerable lengths of time, and those samples sheltered by simulated meteorite material provide experimental evidence for the likelihood of the hypothetical scenario of lithopanspermia.

Several simulations in laboratories and in low Earth orbit suggest that ejection, entry and impact is survivable for some simple organisms. In 2015, remains of biotic material were found in 4.1 billion-year-old rocks in Western Australia, when the young Earth was about 400 million years old. According to one researcher, "If life arose relatively quickly on Earth … then it could be common in the universe."

In April 2018 a Russian team published a paper which disclosed that they found DNA on the exterior of the ISS from land and marine bacteria similar to those previously observed in superficial micro layers at the Barents and Kara seas' coastal zones. They conclude "The presence of the wild land and marine bacteria DNA on the ISS suggests their possible transfer from the stratosphere into the ionosphere with the ascending branch of the global atmospheric electrical circuit. Alternatively, the wild land and marine bacteria as well as the ISS bacteria may all have an ultimate space origin."

Panspermia can be said to be either interstellar (between star systems) or interplanetary (between planets in the same star system); its transport mechanisms may include comets, radiation pressure and lithopanspermia (microorganisms embedded in rocks). Interplanetary transfer of nonliving material is well documented, as evidenced by meteorites of Martian origin found on Earth. Space probes may also be a viable transport mechanism for interplanetary cross-pollination in the Solar System or even beyond. However, space agencies have implemented planetary protection procedures to reduce the risk of planetary contamination, although, as recently discovered, some microorganisms, such as Tersicoccus phoenicis, may be resistant to procedures used in spacecraft assembly clean room facilities. In 2012, mathematician Edward Belbruno and astronomers Amaya Moro-Martín and Renu Malhotra proposed that gravitational low-energy transfer of rocks among the young planets of stars in their birth cluster is commonplace, and not rare in the general galactic stellar population. Deliberate directed panspermia from space to seed Earth or sent from Earth to seed other planetary systems have also been proposed. One twist to the hypothesis by engineer Thomas Dehel (2006), proposes that plasmoid magnetic fields ejected from the magnetosphere may move the few spores lifted from the Earth's atmosphere with sufficient speed to cross interstellar space to other systems before the spores can be destroyed.

In 1903, Svante Arrhenius published in his article "The Distribution of Life in Space", the hypothesis now called radiopanspermia, that microscopic forms of life can be propagated in space, driven by the radiation pressure from stars. Arrhenius argued that particles at a critical size below 1.5 μm would be propagated at high speed by radiation pressure of the Sun. However, because its effectiveness decreases with increasing size of the particle, this mechanism holds for very tiny particles only, such as single bacterial spores. The main criticism of radiopanspermia hypothesis came from Iosif Shklovsky and Carl Sagan, who pointed out the proofs of the lethal action of space radiations (UV and X-rays) in the cosmos. Regardless of the evidence, Wallis and Wickramasinghe argued in 2004 that the transport of individual bacteria or clumps of bacteria, is overwhelmingly more important than lithopanspermia in terms of numbers of microbes transferred, even accounting for the death rate of unprotected bacteria in transit.

Then, data gathered by the orbital experiments ERA, BIOPAN, EXOSTACK and EXPOSE, determined that isolated spores, including those of "B. subtilis", were killed by several orders of magnitude if exposed to the full space environment for a mere few seconds, but if shielded against solar UV, the spores were capable of surviving in space for up to six years while embedded in clay or meteorite powder (artificial meteorites). Though minimal protection is required to shelter a spore against UV radiation, exposure to solar UV and cosmic ionizing radiation of unprotected DNA, break it up into its bases. Also, exposing DNA to the ultrahigh vacuum of space alone is sufficient to cause DNA damage, so the transport of unprotected DNA or RNA during interplanetary flights powered solely by light pressure is extremely unlikely. The feasibility of other means of transport for the more massive shielded spores into the outer Solar System – for example, through gravitational capture by comets – is at this time unknown.

Based on experimental data on radiation effects and DNA stability, it has been concluded that for such long travel times, boulder-sized rocks which are greater than or equal to 1 meter in diameter are required to effectively shield resistant microorganisms, such as bacterial spores against galactic cosmic radiation. These results clearly negate the radiopanspermia hypothesis, which requires single spores accelerated by the radiation pressure of the Sun, requiring many years to travel between the planets, and support the likelihood of interplanetary transfer of microorganisms within asteroids or comets, the so-called lithopanspermia hypothesis.

Lithopanspermia, the transfer of organisms in rocks from one planet to another either through interplanetary or interstellar space, remains speculative. Although there is no evidence that lithopanspermia has occurred in the Solar System, the various stages have become amenable to experimental testing.

Thomas Gold, a professor of astronomy, suggested in 1960 the hypothesis of "Cosmic Garbage", that life on Earth might have originated accidentally from a pile of waste products dumped on Earth long ago by extraterrestrial beings.

Directed panspermia concerns the deliberate transport of microorganisms in space, sent to Earth to start life here, or sent from Earth to seed new planetary systems with life by introduced species of microorganisms on lifeless planets. The Nobel prize winner Francis Crick, along with Leslie Orgel proposed that life may have been purposely spread by an advanced extraterrestrial civilization, but considering an early "RNA world" Crick noted later that life may have originated on Earth. It has been suggested that 'directed' panspermia was proposed in order to counteract various objections, including the argument that microbes would be inactivated by the space environment and cosmic radiation before they could make a chance encounter with Earth.

Conversely, active directed panspermia has been proposed to secure and expand life in space. This may be motivated by biotic ethics that values, and seeks to propagate, the basic patterns of our organic gene/protein life-form. The panbiotic program would seed new planetary systems nearby, and clusters of new stars in interstellar clouds. These young targets, where local life would not have formed yet, avoid any interference with local life.

For example, microbial payloads launched by solar sails at speeds up to 0.0001 "c" (30,000 m/s) would reach targets at 10 to 100 light-years in 0.1 million to 1 million years. Fleets of microbial capsules can be aimed at clusters of new stars in star-forming clouds, where they may land on planets or captured by asteroids and comets and later delivered to planets. Payloads may contain extremophiles for diverse environments and cyanobacteria similar to early microorganisms. Hardy multicellular organisms (rotifer cysts) may be included to induce higher evolution.

The probability of hitting the target zone can be calculated from formula_1 where "A"(target) is the cross-section of the target area, "dy" is the positional uncertainty at arrival; "a" – constant (depending on units), "r"(target) is the radius of the target area; "v" the velocity of the probe; (tp) the targeting precision (arcsec/yr); and "d" the distance to the target, guided by high-resolution astrometry of 1×10 arcsec/yr (all units in SIU). These calculations show that relatively near target stars(Alpha PsA, Beta Pictoris) can be seeded by milligrams of launched microbes; while seeding the Rho Ophiochus star-forming cloud requires hundreds of kilograms of dispersed capsules.

Directed panspermia to secure and expand life in space is becoming possible because of developments in solar sails, precise astrometry, extrasolar planets, extremophiles and microbial genetic engineering. After determining the composition of chosen meteorites, astroecologists performed laboratory experiments that suggest that many colonizing microorganisms and some plants could obtain many of their chemical nutrients from asteroid and cometary materials. However, the scientists noted that phosphate (PO) and nitrate (NO–N) critically limit nutrition to many terrestrial lifeforms. With such materials, and energy from long-lived stars, microscopic life planted by directed panspermia could find an immense future in the galaxy.

A number of publications since 1979 have proposed the idea that directed panspermia could be demonstrated to be the origin of all life on Earth if a distinctive 'signature' message were found, deliberately implanted into either the genome or the genetic code of the first microorganisms by our hypothetical progenitor.

In 2013 a team of physicists claimed that they had found mathematical and semiotic patterns in the genetic code which they think is evidence for such a signature. This claim has been refuted by biologist PZ Myers who said, writing in Pharyngula: In a later peer-reviewed article, the authors address the operation of natural law in an extensive statistical test, and draw the same conclusion as in the previous article. In special sections they also discuss methodological concerns raised by PZ Myers and some others.

Pseudo-panspermia (sometimes called soft panspermia, molecular panspermia or quasi-panspermia) proposes that the organic molecules used for life originated in space and were incorporated in the solar nebula, from which the planets condensed and were further —and continuously— distributed to planetary surfaces where life then emerged (abiogenesis). From the early 1970s it was becoming evident that interstellar dust consisted of a large component of organic molecules. The first suggestion came from Chandra Wickramasinghe, who proposed a polymeric composition based on the molecule formaldehyde (CHO). Interstellar molecules are formed by chemical reactions within very sparse interstellar or circumstellar clouds of dust and gas. Usually this occurs when a molecule becomes ionized, often as the result of an interaction with cosmic rays. This positively charged molecule then draws in a nearby reactant by electrostatic attraction of the neutral molecule's electrons. Molecules can also be generated by reactions between neutral atoms and molecules, although this process is generally slower. The dust plays a critical role of shielding the molecules from the ionizing effect of ultraviolet radiation emitted by stars.

A 2008 analysis of C/C isotopic ratios of organic compounds found in the Murchison meteorite indicates a non-terrestrial origin for these molecules rather than terrestrial contamination. Biologically relevant molecules identified so far include uracil, an RNA nucleobase, and xanthine. These results demonstrate that many organic compounds which are components of life on Earth were already present in the early Solar System and may have played a key role in life's origin.

In August 2009, NASA scientists identified one of the fundamental chemical building-blocks of life (the amino acid glycine) in a comet for the first time.

In August 2011, a report, based on NASA studies with meteorites found on Earth, was published suggesting building blocks of DNA (adenine, guanine and related organic molecules) may have been formed extraterrestrially in outer space. In October 2011, scientists reported that cosmic dust contains complex organic matter ("amorphous organic solids with a mixed aromatic-aliphatic structure") that could be created naturally, and rapidly, by stars. One of the scientists suggested that these complex organic compounds may have been related to the development of life on Earth and said that, "If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life."

In August 2012, and in a world first, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary "IRAS 16293-2422", which is located 400 light years from Earth. Glycolaldehyde is needed to form ribonucleic acid, or RNA, which is similar in function to DNA. This finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.

In September 2012, NASA scientists reported that polycyclic aromatic hydrocarbons (PAHs), subjected to interstellar medium (ISM) conditions, are transformed, through hydrogenation, oxygenation and hydroxylation, to more complex organics – "a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively". Further, as a result of these transformations, the PAHs lose their spectroscopic signature which could be one of the reasons "for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks."

In 2013, the Atacama Large Millimeter Array (ALMA Project) confirmed that researchers have discovered an important pair of prebiotic molecules in the icy particles in interstellar space (ISM). The chemicals, found in a giant cloud of gas about 25,000 light-years from Earth in ISM, may be a precursor to a key component of DNA and the other may have a role in the formation of an important amino acid. Researchers found a molecule called cyanomethanimine, which produces adenine, one of the four nucleobases that form the "rungs" in the ladder-like structure of DNA. The other molecule, called ethanamine, is thought to play a role in forming alanine, one of the twenty amino acids in the genetic code. Previously, scientists thought such processes took place in the very tenuous gas between the stars. The new discoveries, however, suggest that the chemical formation sequences for these molecules occurred not in gas, but on the surfaces of ice grains in interstellar space. NASA ALMA scientist Anthony Remijan stated that finding these molecules in an interstellar gas cloud means that important building blocks for DNA and amino acids can 'seed' newly formed planets with the chemical precursors for life.

In March 2013, a simulation experiment indicate that dipeptides (pairs of amino acids) that can be building blocks of proteins, can be created in interstellar dust.

In February 2014, NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.

In March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.

In May 2016, the Rosetta Mission team reported the presence of glycine, methylamine and ethylamine in the coma of 67P/Churyumov-Gerasimenko. This, plus the detection of phosphorus, is consistent with the hypothesis that comets played a crucial role in the emergence of life on Earth.

The chemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the Universe was only 10–17 million years old. According to the panspermia hypothesis, microscopic life—distributed by meteoroids, asteroids and other small Solar System bodies—may exist throughout the universe. Nonetheless, Earth is the only place in the universe known by humans to harbor life. The sheer number of planets in the Milky Way galaxy, however, may make it probable that life has arisen somewhere else in the galaxy and the universe. It is generally agreed that the conditions required for the evolution of intelligent life as we know it are probably exceedingly rare in the universe, while simultaneously noting that simple single-celled microorganisms may be more likely.

The extrasolar planet results from the Kepler mission estimate 100–400 billion exoplanets, with over 3,500 as candidates or confirmed exoplanets. On 4 November 2013, astronomers reported, based on Kepler space mission data, that there could be as many as 40 billion Earth-sized planets orbiting in the habitable zones of sun-like stars and red dwarf stars within the Milky Way Galaxy. 11 billion of these estimated planets may be orbiting sun-like stars. The nearest such planet may be 12 light-years away, according to the scientists.

It is estimated that space travel over cosmic distances would take an incredibly long time to an outside observer, and with vast amounts of energy required. However, there are reasons to hypothesize that faster-than-light interstellar space travel might be feasible. This has been explored by NASA scientists since at least 1995.

Hoyle and Wickramasinghe have speculated that several outbreaks of illnesses on Earth are of extraterrestrial origins, including the 1918 flu pandemic, and certain outbreaks of polio and mad cow disease. For the 1918 flu pandemic they hypothesized that cometary dust brought the virus to Earth simultaneously at multiple locations—a view almost universally dismissed by experts on this pandemic. Hoyle also speculated that HIV came from outer space. After Hoyle's death, "The Lancet" published a letter to the editor from Wickramasinghe and two of his colleagues, in which they hypothesized that the virus that causes severe acute respiratory syndrome (SARS) could be extraterrestrial in origin and not originated from chickens. "The Lancet" subsequently published three responses to this letter, showing that the hypothesis was not evidence-based, and casting doubts on the quality of the experiments referenced by Wickramasinghe in his letter. A 2008 encyclopedia notes that "Like other claims linking terrestrial disease to extraterrestrial pathogens, this proposal was rejected by the greater research community."

In April 2016, Jiangwen Qu of the Department of Infectious Disease Control in China presented a statistical study suggesting that "extremes of sunspot activity to within plus or minus 1  year may precipitate influenza pandemics." He discussed possible mechanisms of epidemic initiation and early spread, including speculation on primary causation by externally derived viral variants from space via cometary dust.




A separate fragment of the Orgueil meteorite (kept in a sealed glass jar since its discovery) was found in 1965 to have a seed capsule embedded in it, whilst the original glassy layer on the outside remained undisturbed. Despite great initial excitement, the seed was found to be that of a European Juncaceae or Rush plant that had been glued into the fragment and camouflaged using coal dust. The outer "fusion layer" was in fact glue. Whilst the perpetrator of this hoax is unknown, it is thought that they sought to influence the 19th century debate on spontaneous generation — rather than panspermia — by demonstrating the transformation of inorganic to biological matter.

Until the 1970s, life was thought to depend on its access to sunlight. Even life in the ocean depths, where sunlight cannot reach, was believed to obtain its nourishment either from consuming organic detritus rained down from the surface waters or from eating animals that did. However, in 1977, during an exploratory dive to the Galapagos Rift in the deep-sea exploration submersible "Alvin", scientists discovered colonies of assorted creatures clustered around undersea volcanic features known as black smokers. It was soon determined that the basis for this food chain is a form of bacterium that derives its energy from oxidation of reactive chemicals, such as hydrogen or hydrogen sulfide, that bubble up from the Earth's interior. This chemosynthesis revolutionized the study of biology by revealing that terrestrial life need not be Sun-dependent; it only requires water and an energy gradient in order to exist.

It is now known that extremophiles, microorganisms with extraordinary capability to thrive in the harshest environments on Earth, can specialize to thrive in the deep-sea, ice, boiling water, acid, the water core of nuclear reactors, salt crystals, toxic waste and in a range of other extreme habitats that were previously thought to be inhospitable for life. Living bacteria found in ice core samples retrieved from deep at Lake Vostok in Antarctica, have provided data for extrapolations to the likelihood of microorganisms surviving frozen in extraterrestrial habitats or during interplanetary transport. Also, bacteria have been discovered living within warm rock deep in the Earth's crust.

In order to test some these organisms' potential resilience in outer space, plant seeds and spores of bacteria, fungi and ferns have been exposed to the harsh space environment. Spores are produced as part of the normal life cycle of many plants, algae, fungi and some protozoans, and some bacteria produce endospores or cysts during times of stress. These structures may be highly resilient to ultraviolet and gamma radiation, desiccation, lysozyme, temperature, starvation and chemical disinfectants, while metabolically inactive. Spores germinate when favourable conditions are restored after exposure to conditions fatal to the parent organism.

Although computer models suggest that a captured meteoroid would typically take some tens of millions of years before collision with a planet, there are documented viable Earthly bacterial spores that are 40 million years old that are very resistant to radiation, and others able to resume life after being dormant for 25 million years, suggesting that lithopanspermia life-transfers are possible via meteorites exceeding 1 m in size.

The discovery of deep-sea ecosystems, along with advancements in the fields of astrobiology, observational astronomy and discovery of large varieties of extremophiles, opened up a new avenue in astrobiology by massively expanding the number of possible extraterrestrial habitats and possible transport of hardy microbial life through vast distances.

The question of whether certain microorganisms can survive in the harsh environment of outer space has intrigued biologists since the beginning of spaceflight, and opportunities were provided to expose samples to space. The first American tests were made in 1966, during the Gemini IX and XII missions, when samples of bacteriophage T1 and spores of "Penicillium roqueforti" were exposed to outer space for 16.8 h and 6.5 h, respectively. Other basic life sciences research in low Earth orbit started in 1966 with the Soviet biosatellite program Bion and the U.S. Biosatellite program. Thus, the plausibility of panspermia can be evaluated by examining life forms on Earth for their capacity to survive in space. The following experiments carried on low Earth orbit specifically tested some aspects of panspermia or lithopanspermia:

The Exobiology Radiation Assembly (ERA) was a 1992 experiment on board the European Retrievable Carrier (EURECA) on the biological effects of space radiation. EURECA was an unmanned 4.5 tonne satellite with a payload of 15 experiments. It was an astrobiology mission developed by the European Space Agency (ESA). Spores of different strains of "Bacillus subtilis" and the "Escherichia coli" plasmid pUC19 were exposed to selected conditions of space (space vacuum and/or defined wavebands and intensities of solar ultraviolet radiation). After the approximately 11-month mission, their responses were studied in terms of survival, mutagenesis in the "his" ("B. subtilis") or "lac" locus (pUC19), induction of DNA strand breaks, efficiency of DNA repair systems, and the role of external protective agents. The data were compared with those of a simultaneously running ground control experiment:

BIOPAN is a multi-user experimental facility installed on the external surface of the Russian Foton descent capsule. Experiments developed for BIOPAN are designed to investigate the effect of the space environment on biological material after exposure between 13 and 17 days. The experiments in BIOPAN are exposed to solar and cosmic radiation, the space vacuum and weightlessness, or a selection thereof. Of the 6 missions flown so far on BIOPAN between 1992 and 2007, dozens of experiments were conducted, and some analyzed the likelihood of panspermia. Some bacteria, lichens ("Xanthoria elegans", "Rhizocarpon geographicum" and their mycobiont cultures, the black Antarctic microfungi "Cryomyces minteri" and "Cryomyces antarcticus"), spores, and even one animal (tardigrades) were found to have survived the harsh outer space environment and cosmic radiation.

The German EXOSTACK experiment was deployed on 7 April 1984 on board the Long Duration Exposure Facility statellite. 30% of "Bacillus subtilis" spores survived the nearly 6 years exposure when embedded in salt crystals, whereas 80% survived in the presence of glucose, which stabilize the structure of the cellular macromolecules, especially during vacuum-induced dehydration.

If shielded against solar UV, spores of "B. subtilis" were capable of surviving in space for up to 6 years, especially if embedded in clay or meteorite powder (artificial meteorites). The data support the likelihood of interplanetary transfer of microorganisms within meteorites, the so-called lithopanspermia hypothesis.

EXPOSE is a multi-user facility mounted outside the International Space Station dedicated to astrobiology experiments. There have been three EXPOSE experiments flown between 2008 and 2015: EXPOSE-E, EXPOSE-R and EXPOSE-R2. 
Results from the orbital missions, especially the experiments "SEEDS" and "LiFE", concluded that after an 18-month exposure, some seeds and lichens ("Stichococcus sp." and "Acarospora sp"., a lichenized fungal genus) may be capable to survive interplanetary travel if sheltered inside comets or rocks from cosmic radiation and UV radiation. The "LIFE", "SPORES", and "SEEDS" parts of the experiments provided information about the likelihood of lithopanspermia. These studies will provide experimental data to the lithopanspermia hypothesis, and they will provide basic data to planetary protection issues.

The Tanpopo mission is an orbital astrobiology experiment by Japan that is currently investigating the possible interplanetary transfer of life, organic compounds, and possible terrestrial particles in low Earth orbit. The Tanpopo experiment is taking place at the Exposed Facility located on the exterior of Kibo module of the International Space Station. The mission will collect cosmic dusts and other particles for three years by using an ultra-low density silica gel called aerogel. The purpose is to assess the panspermia hypothesis and the possibility of natural interplanetary transport of life and its precursors. Some of these aerogels will be replaced every one or two years through 2018. Sample collection began in May 2015, and the first samples were be returned to Earth in mid-2016.

Panspermia is often criticized because it does not answer the question of the origin of life but merely places it on another celestial body. It was also criticized because it was thought it could not be tested experimentally.

Wallis and Wickramasinghe argued in 2004 that the transport of individual bacteria or clumps of bacteria, is overwhelmingly more important than lithopanspermia in terms of numbers of microbes transferred, even accounting for the death rate of unprotected bacteria in transit. Then it was found that isolated spores of "B. subtilis" were killed by several orders of magnitude if exposed to the full space environment for a mere few seconds. These results clearly negate the original panspermia hypothesis, which requires single spores as space travelers accelerated by the radiation pressure of the Sun, requiring many years to travel between the planets. However, if shielded against solar UV, spores of "Bacillus subtilis" were capable of surviving in space for up to 6 years, especially if embedded in clay or meteorite powder (artificial meteorites). The data support the likelihood of interplanetary transfer of microorganisms within meteorites, the so-called lithopanspermia hypothesis.




</doc>
<doc id="23680" url="https://en.wikipedia.org/wiki?curid=23680" title="There's Plenty of Room at the Bottom">
There's Plenty of Room at the Bottom

"There's Plenty of Room at the Bottom: An Invitation to Enter a New Field of Physics" was a lecture given by physicist Richard Feynman at the annual American Physical Society meeting at Caltech on December 29, 1959. Feynman considered the possibility of direct manipulation of individual atoms as a more powerful form of synthetic chemistry than those used at the time. Versions of the talk were printed in a few popular magazines within a year and newspapers covered the winning of the presented challenges in 1960 and again in 1985. The talk went unnoticed and did not inspire the conceptual beginnings of the field, but nanotechnology research advocates began citing it in the 1990s to establish the scientific credibility of their work.

Feynman considered a number of interesting ramifications of a general ability to manipulate matter on an atomic scale. He was particularly interested in the possibilities of denser computer circuitry, and microscopes that could see things much smaller than is possible with scanning electron microscopes. These ideas were later realized by the use of the scanning tunneling microscope, the atomic force microscope and other examples of scanning probe microscopy and storage systems such as Millipede, created by researchers at IBM.

Feynman also suggested that it should be possible, in principle, to make nanoscale machines that "arrange the atoms the way we want", and do chemical synthesis by mechanical manipulation.

He also presented the possibility of "swallowing the doctor", an idea that he credited in the essay to his friend and graduate student Albert Hibbs. This concept involved building a tiny, swallowable surgical robot.

As a thought experiment he proposed developing a set of one-quarter-scale manipulator hands slaved to the operator's hands to build one-quarter scale machine tools analogous to those found in any machine shop. This set of small tools would then be used by the small hands to build and operate ten sets of one-sixteenth-scale hands and tools, and so forth, culminating in perhaps a billion tiny factories to achieve massively parallel operations. He uses the analogy of a pantograph as a way of scaling down items. This idea was anticipated in part, down to the microscale, by science fiction author Robert A. Heinlein in his 1942 story "Waldo".
As the sizes got smaller, one would have to redesign some tools, because the relative strength of various forces would change. Although gravity would become unimportant, surface tension would become more important, Van der Waals attraction would become important, etc. Feynman mentioned these scaling issues during his talk. Nobody has yet attempted to implement this thought experiment, although it has been noted that some types of biological enzymes and enzyme complexes (especially ribosomes) function chemically in a way close to Feynman's vision. Feynman also mentioned in his lecture that it might be better eventually to use glass or plastic because their greater uniformity would avoid problems in the very small scale (metals and crystals are separated into domains where the lattice structure prevails). This could be a good reason to make machines and also electronics out of glass and plastic. At the present time, there are electronic components made of both materials. In glass, there are optical fiber cables that amplify the light pulses at regular intervals, using glass doped with the rare-earth element erbium. The doped glass is spliced into the fiber and pumped by a laser operating at a different frequency. In plastic, field effect transistors are being made with polythiophene, a plastic invented by Alan J. Heeger et al. that becomes an electrical conductor when oxidized. At this time, a factor of just 20 in electron mobility separates plastic from silicon.

At the meeting Feynman concluded his talk with two challenges, and he offered a prize of $1000 for the first individuals to solve each one. The first challenge involved the construction of a tiny motor, which, to Feynman's surprise, was achieved by November 1960 by Caltech graduate, William McLellan, a meticulous craftsman, using conventional tools. The motor met the conditions, but did not advance the art. The second challenge involved the possibility of scaling down letters small enough so as to be able to fit the entire "Encyclopædia Britannica" on the head of a pin, by writing the information from a book page on a surface 1/25,000 smaller in linear scale. In 1985, Tom Newman, a Stanford graduate student, successfully reduced the first paragraph of "A Tale of Two Cities" by 1/25,000, and collected the second Feynman prize. Newman's thesis adviser, R. Fabian Pease, had read the paper back in 1966; but it was another grad student in the lab, Ken Polasko, who had recently read it who suggested attempting the challenge. Newman was looking for some arbitrary random pattern for demonstrating their technology. Newman said, "Text was ideal because it has so many different shapes."

"The New Scientist" reported "the scientific audience was captivated." Feynman had "spun the idea off the top of his mind" without even "notes from beforehand". There were no copies of the speech for those asking for copies. A "foresighted admirer" brought a tape recorder and an edited transcript, without Feynman's jokes, was made for publication by Caltech. In February 1960, Caltech's "Engineering and Science" published the speech. In addition to excerpts in "The New Scientist", versions were printed in "The Saturday Review" and "Popular Science". Newspapers announced the winning of the first challenge. It was included as the final chapter in the 1961 book, "Miniaturization".

K. Eric Drexler later took the Feynman concept of a billion tiny factories and added the idea that they could make more copies of themselves, via computer control instead of control by a human operator, in his 1986 book "Engines of Creation: The Coming Era of Nanotechnology".

After Feynman's death, scholars studying the historical development of nanotechnology have concluded that his actual role in catalyzing nanotechnology research was limited based on recollections from many of the people active in the nascent field in the 1980s and 1990s. Chris Toumey, a cultural anthropologist at the University of South Carolina, has reconstructed the history of the publication and republication of Feynman’s talk, along with the record of citations to “Plenty of Room” in the scientific literature. In Toumey's 2008 article, "Reading Feynman into Nanotechnology", he found 11 versions of the publication of “Plenty of Room", plus two instances of a closely related talk by Feynman, “Infinitesimal Machinery,” which Feynman called “Plenty of Room, Revisited.” Also in Toumey’s references are videotapes of that second talk.

Toumey found that the published versions of Feynman’s talk had a negligible influence in the twenty years after it was first published, as measured by citations in the scientific literature, and not much more influence in the decade after the Scanning Tunneling Microscope was invented in 1981. Subsequently, interest in “Plenty of Room” in the scientific literature greatly increased in the early 1990s. This is probably because the term “nanotechnology” gained serious attention just before that time, following its use by Drexler in his 1986 book, "Engines of Creation: The Coming Era of Nanotechnology", which cited Feynman, and in a cover article headlined "Nanotechnology", published later that year in a mass-circulation science-oriented magazine, "OMNI". The journal "Nanotechnology" was launched in 1989; the famous Eigler-Schweizer experiment, precisely manipulating 35 xenon atoms, was published in "Nature" in April 1990; and "Science" had a special issue on nanotechnology in November 1991. These and other developments hint that the retroactive rediscovery of Feynman’s “Plenty of Room” gave nanotechnology a packaged history that provided an early date of December 1959, plus a connection to the charisma and genius of Richard Feynman.

Toumey’s analysis also includes comments from distinguished scientists in nanotechnology who say that “Plenty of Room” did not influence their early work, and in fact most of them had not read it until a later date.

Feynman's stature as a Nobel laureate and as an iconic figure in 20th century science surely helped advocates of nanotechnology and provided a valuable intellectual link to the past. More concretely, his stature and concept of atomically precise fabrication played a role in securing funding for nanotechnology research, illustrated by President Clinton's January 2000 speech calling for a Federal program:

While the version of the Nanotechnology Research and Development Act that was passed by the House in May 2003 called for a study of the technical feasibility of molecular manufacturing, this study was removed to safeguard funding of less controversial research before the Act was passed by the Senate and finally signed into law by President Bush on December 3, 2003.





</doc>
<doc id="23681" url="https://en.wikipedia.org/wiki?curid=23681" title="Philately">
Philately

Philately (; ) is the study of stamps and postal history and other related items. It also refers to the collection, appreciation and research activities on stamps and other philatelic products. Philately involves more than just stamp collecting, which does not necessarily involve the study of stamps. It is possible to be a philatelist without owning any stamps. For instance, the stamps being studied may be very rare, or reside only in museums.

The word "philately" is the English version of the French word "philatélie", coined by Georges Herpin in 1864. Herpin stated that stamps had been collected and studied for the previous six or seven years and a better name was required for the new hobby than "timbromanie", which was disliked. He took the Greek root word φιλ(ο)- "phil(o)-", meaning "an attraction or affinity for something", and ἀτέλεια "ateleia", meaning "exempt from duties and taxes" to form "philatelie". The introduction of postage stamps meant that the receipt of letters was now free of charge, whereas before stamps it was normal for postal charges to be paid by the recipient of a letter.

The alternative terms "timbromania", "timbrophily" and "timbrology" gradually fell out of use as "philately" gained acceptance during the 1860s.

The origins of philately lie in the observation that in a number of apparently similar stamps, closer examination may reveal differences in the printed design, paper, watermark, colour, perforations and other areas of the stamp. Comparison with the records of postal authorities may or may not show that the variations were intentional, which leads to further inquiry as to how the changes could have happened, and why. To make things more interesting, thousands of forgeries have been produced over the years, some of them very good, and only a thorough knowledge of philately gives any hope of detecting the fakes.

Traditional philately is the study of the technical aspects of stamp production and stamp identification, including:


Philately uses a number of tools, including stamp tongs (a specialized form of tweezers) to safely handle the stamps, a strong magnifying glass and a perforation gauge (odontometer) to measure the perforation gauge of the stamp.

The identification of watermarks is important and may be done with the naked eye by turning the stamp over or holding it up to the light. If this fails then "watermark fluid" may be used, which "wets" the stamp to reveal the mark.

Other common tools include stamp catalogues, stamp stock books and stamp hinges.

Philatelic organisations sprang up soon after people started collecting and studying stamps. They include local, national and international clubs and societies where collectors come together to share the various aspects of their hobby.





</doc>
<doc id="23682" url="https://en.wikipedia.org/wiki?curid=23682" title="Puget Sound">
Puget Sound

Puget Sound is a sound along the northwestern coast of the U.S. state of Washington, an inlet of the Pacific Ocean, and part of the Salish Sea. It is a complex estuarine system of interconnected marine waterways and basins, with one major and two minor connections to the open Pacific Ocean via the Strait of Juan de Fuca—Admiralty Inlet being the major connection and Deception Pass and Swinomish Channel being the minor.

Water flow through Deception Pass is approximately equal to 2% of the total tidal exchange between Puget Sound and the Strait of Juan de Fuca. Puget Sound extends approximately from Deception Pass in the north to Olympia, Washington in the south. Its average depth is and its maximum depth, off Jefferson Point between Indianola and Kingston, is . The depth of the main basin, between the southern tip of Whidbey Island and Tacoma, Washington, is approximately .

Since 2009, the term Salish Sea has been established by the United States Board on Geographic Names as the collective waters of Puget Sound, the Strait of Juan de Fuca, and the Strait of Georgia. Sometimes the terms "Puget Sound" and "Puget Sound and adjacent waters" are used for not only Puget Sound proper but also for waters to the north, such as Bellingham Bay and the San Juan Islands region.

The term "Puget Sound" is used not just for the body of water but also the Puget Sound region centered on the sound. Major cities on the sound include Seattle, Tacoma, Olympia, and Everett, Washington. Puget Sound is also the third largest estuary in the United States, after Chesapeake Bay in Maryland and Virginia, and San Francisco Bay in northern California.

In 1792 George Vancouver gave the name "Puget's Sound" to the waters south of the Tacoma Narrows, in honor of Peter Puget, a Huguenot lieutenant accompanying him on the Vancouver Expedition. This name later came to be used for the waters north of Tacoma Narrows as well.

A different term for Puget Sound, used by a number of Native Americans and environmental groups, is "Whulge" (or "Whulj"), an anglicization of the Lushootseed name "x̌ʷə́lč", which means "sea, salt water, ocean, or sound".

The USGS defines Puget Sound as all the waters south of three entrances from the Strait of Juan de Fuca. The main entrance at Admiralty Inlet is defined as a line between Point Wilson on the Olympic Peninsula, and Point Partridge on Whidbey Island. The second entrance is at Deception Pass along a line from West Point on Whidbey Island, to Deception Island, then to Rosario Head on Fidalgo Island. The third entrance is at the south end of the Swinomish Channel, which connects Skagit Bay and Padilla Bay. Under this definition, Puget Sound includes the waters of Hood Canal, Admiralty Inlet, Possession Sound, Saratoga Passage, and others. It does not include Bellingham Bay, Padilla Bay, the waters of the San Juan Islands or anything farther north.

Another definition, given by NOAA, subdivides Puget Sound into five basins or regions. Four of these (including South Puget Sound) correspond to areas within the USGS definition, but the fifth one, called "Northern Puget Sound" includes a large additional region. It is defined as bounded to the north by the international boundary with Canada, and to the west by a line running north from the mouth of the Sekiu River on the Olympic Peninsula. Under this definition significant parts of the Strait of Juan de Fuca and the Strait of Georgia are included in Puget Sound, with the international boundary marking an abrupt and hydrologically arbitrary limit.

According to Arthur Kruckeberg, the term "Puget Sound" is sometimes used for waters north of Admiralty Inlet and Deception Pass, especially for areas along the north coast of Washington and the San Juan Islands, essentially equivalent to NOAA's "Northern Puget Sound" subdivision described above. Kruckeberg uses the term "Puget Sound and adjacent waters".

Continental ice sheets have repeatedly advanced and retreated from the Puget Sound region. The most recent glacial period, called the Fraser Glaciation, had three phases, or stades. During the third, or Vashon Glaciation, a lobe of the Cordilleran Ice Sheet, called the Puget Lobe, spread south about 15,000 years ago, covering the Puget Sound region with an ice sheet about thick near Seattle, and nearly at the present Canada-U.S. border. Since each new advance and retreat of ice erodes away much of the evidence of previous ice ages, the most recent Vashon phase has left the clearest imprint on the land. At its maximum extent the Vashon ice sheet extended south of Olympia to near Tenino, and covered the lowlands between the Olympic and Cascade mountain ranges. About 14,000 years ago the ice began to retreat. By 11,000 years ago it survived only north of the Canada–US border.

The melting retreat of the Vashon Glaciation eroded the land, creating a drumlin field of hundreds of aligned drumlin hills. Lake Washington and Lake Sammamish (which are ribbon lakes), Hood Canal, and the main Puget Sound basin were altered by glacial forces. These glacial forces are not specifically "carving", as in cutting into the landscape via the mechanics of ice/glaciers, but rather eroding the landscape from melt water of the Vashon Glacier creating the drumlin field. As the ice retreated, vast amounts of glacial till were deposited throughout the Puget Sound region. The soils of the region, less than ten thousand years old, are still characterized as immature.

As the Vashon glacier receded a series of proglacial lakes formed, filling the main trough of Puget Sound and inundating the southern lowlands. Glacial Lake Russell was the first such large recessional lake. From the vicinity of Seattle in the north the lake extended south to the Black Hills, where it drained south into the Chehalis River. Sediments from Lake Russell form the blue-gray clay identified as the Lawton Clay. The second major recessional lake was Glacial Lake Bretz. It also drained to the Chehalis River until the Chimacum Valley, in the northeast Olympic Peninsula, melted, allowing the lake's water to rapidly drain north into the marine waters of the Strait of Juan de Fuca, which was rising as the ice sheet retreated.

As icebergs calved off the toe of the glacier, their embedded gravels and boulders were deposited in the chaotic mix of unsorted till geologists call "glaciomarine drift." Many beaches about the Sound display glacial erratics, rendered more prominent than those in coastal woodland solely by their exposed position; submerged glacial erratics sometimes cause hazards to navigation. The sheer weight of glacial-age ice depressed the landforms, which experienced post-glacial rebound after the ice sheets had retreated. Because the rate of rebound was not synchronous with the post-ice age rise in sea levels, the bed of what is Puget Sound, filled alternately with fresh and with sea water. The upper level of the lake-sediment Lawton Clay now lies about above sea level.

The Puget Sound system consists of four deep basins connected by shallower sills. The four basins are Hood Canal, west of the Kitsap Peninsula, Whidbey Basin, east of Whidbey Island, South Sound, south of the Tacoma Narrows, and the Main Basin, which is further subdivided into Admiralty Inlet and the Central Basin. Puget Sound's sills, a kind of submarine terminal moraine, separate the basins from one another, and Puget Sound from the Strait of Juan de Fuca. Three sills are particularly significant—the one at Admiralty Inlet which checks the flow of water between the Strait of Juan de Fuca and Puget Sound, the one at the entrance to Hood Canal (about below the surface), and the one at the Tacoma Narrows (about ). Other sills that present less of a barrier include the ones at Blake Island, Agate Pass, Rich Passage, and Hammersley Inlet.

The depth of the basins is a result of the Sound being part of the Cascadia subduction zone, where the terranes accreted at the edge of the Juan de Fuca Plate are being subducted under the North American Plate. There has not been a major subduction zone earthquake here since the magnitude nine Cascadia earthquake; according to Japanese records, it occurred 26 January 1700. Lesser Puget Sound earthquakes with shallow epicenters, caused by the fracturing of stressed oceanic rocks as they are subducted, still cause great damage. The Seattle Fault cuts across Puget Sound, crossing the southern tip of Bainbridge Island and under Elliott Bay. To the south, the existence of a second fault, the Tacoma Fault, has buckled the intervening strata in the Seattle Uplift.

Typical Puget Sound profiles of dense glacial till overlying permeable glacial outwash of gravels above an impermeable bed of silty clay may become unstable after periods of unusually wet weather and slump in landslides.

The United States Geological Survey (USGS) defines Puget Sound as a bay with numerous channels and branches; more specifically, it is a fjord system of flooded glacial valleys. Puget Sound is part of a larger physiographic structure termed the Puget Trough, which is a physiographic section of the larger Pacific Border province, which in turn is part of the larger Pacific Mountain System.

Puget Sound is a large salt water estuary, or system of many estuaries, fed by highly seasonal freshwater from the Olympic and Cascade Mountain watersheds. The mean annual river discharge into Puget Sound is , with a monthly average maximum of about and minimum of about . Puget Sound's shoreline is long, encompassing a water area of and a total volume of at mean high water. The average volume of water flowing in and out of Puget Sound during each tide is . The maximum tidal currents, in the range of 9 to 10 knots, occurs at Deception Pass.
The size of Puget Sound's watershed is . "Northern Puget Sound" is frequently considered part of the Puget Sound watershed, which enlarges its size to . The USGS uses the name "Puget Sound" for its hydrologic unit subregion 1711, which includes areas draining to Puget Sound proper as well as the Strait of Juan de Fuca, the Strait of Georgia, and the Fraser River. Significant rivers that drain to "Northern Puget Sound" include the Nooksack, Dungeness, and Elwha Rivers. The Nooksack empties into Bellingham Bay, the Dungeness and Elwha into the Strait of Juan de Fuca. The Chilliwack River flows north to the Fraser River in Canada.

Tides in Puget Sound are of the mixed type with two high and two low tides each tidal day. These are called Higher High Water (HHW), Lower Low Water (LLW), Lower High Water (LHW), and Higher Low Water (HLW). The configuration of basins, sills, and interconnections cause the tidal range to increase within Puget Sound. The difference in height between the Higher High Water and the Lower Low Water averages about at Port Townsend on Admiralty Inlet, but increases to about at Olympia, the southern end of Puget Sound.

Puget Sound is generally accepted as the start of the Inside Passage.

Important marine flora of Puget Sound include eelgrass ("Zostera marina") and kelp, especially bull kelp ("Nereocystis luetkeana").

Among the marine mammals species found in Puget Sound are harbor seals ("Phoca vitulina"). Orca ("Orcinus orca") are famous throughout the Sound, and are a large tourist attraction. Although orca are sometimes seen in Puget Sound proper they are far more prevalent around the San Juan Islands north of Puget Sound.

Many fish species occur in Puget Sound. The various salmonid species, including salmon, trout, and char are particularly well-known and studied. Salmonid species of Puget Sound include chinook salmon ("Oncorhynchus tshawytscha"), chum salmon ("O. keta"), coho salmon ("O. kisutch"), pink salmon ("O. gorbuscha"), sockeye salmon ("O. nerka"), sea-run coastal cutthroat trout ("O. clarki clarki"), steelhead ("O. mykiss irideus"), sea-run bull trout ("Salvelinus confluentus"), and Dolly Varden trout ("Salvelinus malma malma").

Common forage fishes found in Puget Sound include Pacific herring ("Clupea pallasii"), surf smelt ("Hypomesus pretiosus"), and Pacific sand lance ("Ammodytes hexapterus"). Important benthopelagic fish of Puget Sound include North Pacific hake ("Merluccius productus"), Pacific cod ("Gadus macrocelhalus"), walleye pollock ("Theragra chalcogramma"), and the spiny dogfish ("Squalus acanthias"). There are about 28 species of Sebastidae (rockfish), of many types, found in Puget Sound. Among those of special interest are copper rockfish ("Sebastes caurinus"), quillback rockfish ("S. maliger"), black rockfish ("S. melanops"), yelloweye rockfish ("S. ruberrimus"), bocaccio rockfish ("S. paucispinis"), canary rockfish ("S. pinniger"), and Puget Sound rockfish ("S. emphaeus").

Many other fish species occur in Puget Sound, such as sturgeons, lampreys, various sharks, rays, and skates.

Puget Sound is home to numerous species of marine invertebrates, including sponges, sea anemones, chitons, clams, sea snails, limpets crabs, barnacles starfish, sea urchins, and sand dollars. Dungeness crabs ("Metacarcinus magister") occur throughout Washington waters, including Puget Sound. Many bivalves occur in Puget Sound, such as Pacific oysters ("Crassostrea gigas") and geoduck clams ("Panopea generosa"). The Olympia oyster ("Ostreola conchaphila"), once common in Puget Sound, was depleted by human activities during the 20th century. There are ongoing efforts to restore Olympia oysters in Puget Sound.
There are many seabird species of Puget Sound. Among these are grebes such as the western grebe ("Aechmophorus occidentalis"); loons such as the common loon ("Gavia immer"); auks such as the pigeon guillemot ("Cepphus columba"), rhinoceros auklet ("Cerorhinca monocerata"), common murre ("Uria aalge"), and marbled murrelet ("Brachyramphus marmoratus"); the brant goose ("Branta bernicla"); seaducks such as the long-tailed duck ("Clangula hyemalis"), harlequin duck ("Histrionicus histrionicus"), and surf scoter ("Melanitta perspicillata"); and cormorants such as the double-crested cormorant ("Phalacrocorax auritus"). Puget Sound is home to a non-migratory and marine-oriented subspecies of great blue herons ("Ardea herodias fannini"). Bald eagles ("Haliaeetus leucocephalus") occur in relative high densities in the Puget Sound region.

It is estimated that more than 100 million geoducks (pronounced "gooey ducks") are packed into Puget Sound's sediments. Also known as "king clam", geoducks are considered to be a delicacy in Asian countries.

George Vancouver explored Puget Sound in 1792. Vancouver claimed it for Great Britain on 4 June 1792, naming it for one of his officers, Lieutenant Peter Puget.

After 1818 Britain and the United States, which both claimed the Oregon Country, agreed to "joint occupancy", deferring resolution of the Oregon boundary dispute until the 1846 Oregon Treaty. Puget Sound was part of the disputed region until 1846, after which it became US territory.

American maritime fur traders visited Puget Sound in the early 19th century.

The first European settlement in the Puget Sound area was Fort Nisqually, a fur trade post of the Hudson's Bay Company (HBC) built in 1833. Fort Nisqually was part of the HBC's Columbia District, headquartered at Fort Vancouver. The Puget Sound Agricultural Company, a subsidiary of the HBC, established farms and ranches near Fort Nisqually. British ships such as the "Beaver", exported foodstuffs and provisions from Fort Nisqually.

The first American settlement on Puget Sound was Tumwater. It was founded in 1845 by Americans who had come via the Oregon Trail. The decision to settle north of the Columbia River was made in part because one of the settlers, George Washington Bush, was considered black and the Provisional Government of Oregon banned the residency of mulattoes but did not actively enforce the restriction north of the river.

In 1853 Washington Territory was formed from part of Oregon Territory. In 1888 the Northern Pacific railroad line reached Puget Sound, linking the region to eastern states.

A unique state-run ferry system, the Washington State Ferries, connects the larger islands to the Washington mainland, as well as both sides of the sound, with vessels capable of carrying passengers and automobile traffic. The system carries 24 million passengers annually and is the largest ferry operator in the United States.

In the past 30 years there has been a large recession in the populations of the species which inhabit Puget Sound. The decrease has been seen in the populations of: forage fish, salmonids, bottom fish, marine birds, harbor porpoise and orcas. This decline is attributed to the various environmental issues in Puget Sound. Because of this population decline, there have been changes to the fishery practices, and an increase in petitioning to add species to the Endangered Species Act. There has also been an increase in recovery and management plans for many different area species.

The causes of these environmental issues are toxic contamination, eutrophication (low oxygen due to excess nutrients), and near shore habitat changes.





</doc>
<doc id="23688" url="https://en.wikipedia.org/wiki?curid=23688" title="Perjury">
Perjury

Perjury is the intentional act of swearing a false oath or falsifying an affirmation to tell the truth, whether spoken or in writing, concerning matters material to an official proceeding. In some jurisdictions, contrary to popular misconception, no crime has occurred when a false statement is (intentionally or unintentionally) made while under oath or subject to penalty. 
The charge wording is imperative, with proceedings famously failing in the matter of Roche v Stephens.
Instead, criminal culpability attaches only at the instant the declarant falsely asserts the truth of statements (made or to be made) that are material to the outcome of the proceeding. For example, it is not perjury to lie about one's age except if age is a fact material to influencing the legal result, such as eligibility for old age retirement benefits or whether a person was of an age to have legal capacity.

Perjury is considered a serious offense, as it can be used to usurp the power of the courts, resulting in miscarriages of justice. In the United States, for example, the general perjury statute under federal law classifies perjury as a felony and provides for a prison sentence of up to five years. The California Penal Code allows for perjury to be a capital offense in cases causing wrongful execution. Perjury which caused the wrongful execution of another or in the pursuit of causing the wrongful execution of another is respectively construed as murder or attempted murder, and is normally itself punishable by execution in countries that retain the death penalty. Perjury is considered a felony in most U.S. states as well as most Australian states. In Queensland, under Section 124 of the Queensland Criminal Code Act 1899, perjury is punishable by up to life in prison if it is committed to procure an innocent person for a crime that is punishable by life in prison. However, prosecutions for perjury are rare. In some countries such as France and Italy, suspects cannot be heard under oath or affirmation and so cannot commit perjury, regardless of what they say during their trial.

The rules for perjury also apply when a person has made a statement "under penalty of perjury" even if the person has not been sworn or affirmed as a witness before an appropriate official. An example is the US income tax return, which, by law, must be signed as true and correct under penalty of perjury (see ). Federal tax law provides criminal penalties of up to three years in prison for violation of the tax return perjury statute. See: 

Statements that entail an "interpretation" of fact are not perjury because people often draw inaccurate conclusions unwittingly or make honest mistakes without the intent to deceive. Individuals may have honest but mistaken beliefs about certain facts or their recollection may be inaccurate, or may have a different perception of what is the accurate way to state the truth. Like most other crimes in the common law system, to be convicted of perjury one must have had the "intention" ("mens rea") to commit the act and to have "actually committed" the act ("actus reus"). Further, statements that "are facts" cannot be considered perjury, even if they might arguably constitute an omission, and it is not perjury to lie about matters that are immaterial to the legal proceeding.

In the United States, Kenya, and Scotland, subornation of perjury, which is attempting to induce another person to commit perjury, is itself a crime.

The offence of perjury is codified by section 132 of the Criminal Code. It is defined by section 131, which provides:

As to corroboration, see section 133.

Mode of trial and sentence

Every one who commits perjury is guilty of an indictable offence and liable to imprisonment for a term not exceeding fourteen years.

A person who, before the Court of Justice of the European Union, swears anything which he knows to be false or does not believe to be true is, whatever his nationality, guilty of perjury. Proceedings for this offence may be taken in any place in the State and the offence may for all incidental purposes be treated as having been committed in that place.

Perjury is a statutory offence in England and Wales. It is created by section 1(1) of the Perjury Act 1911. Section 1 of that Act reads:

The words omitted from section 1(1) were repealed by section 1(2) of the Criminal Justice Act 1948.

A person guilty of an offence under section 11(1) of the European Communities Act 1972 may be proceeded against and punished in England and Wales as for an offence under section 1(1).

Section 1(4) has effect in relation to proceedings in the Court of Justice of the European Communities as it has effect in relation to a judicial proceeding in a tribunal of a foreign state.

Section 1(4) applies in relation to proceedings before a relevant convention court under the European Patent Convention as it applies to a judicial proceeding in a tribunal of a foreign state.

A statement made on oath by a witness outside the United Kingdom and given in evidence through a live television link by virtue of section 32 of the Criminal Justice Act 1988 must be treated for the purposes of section 1 as having been made in the proceedings in which it is given in evidence.

Section 1 applies in relation to a person acting as an intermediary as it applies in relation to a person lawfully sworn as an interpreter in a judicial proceeding; and for this purpose, where a person acts as an intermediary in any proceeding which is not a judicial proceeding for the purposes of section 1, that proceeding must be taken to be part of the judicial proceeding in which the witness’s evidence is given.

Where any statement made by a person on oath in any proceeding which is not a judicial proceeding for the purposes of section 1 is received in evidence in pursuance of a special measures direction, that proceeding must be taken for the purposes of section 1 to be part of the judicial proceeding in which the statement is so received in evidence.

Judicial proceeding

The definition in section 1(2) is not "comprehensive".

The book "Archbold" said that it appears to be immaterial whether the court, before which the statement is made, has jurisdiction in the particular cause in which the statement is made, because there is no express requirement in the Act that the court be one of "competent jurisdiction" and because the definition in section 1(2) does not appear to require this by implication either.

Actus reus

The actus reus of perjury might be considered to be the making of a statement, whether true or false, on oath in a judicial proceeding, where the person knows the statement to be false or believes it to be false.

Perjury is a conduct crime.

Mode of trial

Perjury is triable only on indictment.

Sentence

A person convicted of perjury is liable to imprisonment for a term not exceeding seven years, or to a fine, or to both.

The following cases are relevant:

See also the Crown Prosecution Service sentencing manual.

In Anglo-Saxon legal procedure, the offence of perjury could only be committed by both jurors and by compurgators. With time witnesses began to appear in court they were not so treated despite the fact that their functions were akin to that of modern witnesses. This was due to the fact that their role were not yet differentiated from those of the juror – hence false evidence or perjury by witnesses was not made a crime. Even in the fourteenth century when witnesses started appearing before the jury to testify, perjury by them was not made a punishable offence. The maxim then was that every witness’s evidence on oath was true. Perjury by witnesses began to be punished before the end of fifteenth century by the Star Chamber. The immunity enjoyed by witnesses began also to be whittled down or interfered with by the parliament in England in 1540 with subornation of perjury and, in 1562, with perjury proper. The punishment for the offence then was in the nature of monetary penalty, recoverable in a civil action and, not by penal sanction. In 1613, the Star Chamber declared perjury by a witness to be a punishable offence at common law.

See section 3 of the Maintenance and Embracery Act 1540, the 5 Eliz 1 c 9 (An Act for the Punyshement of suche persones as shall procure or comit any wyllful Perjurye) and the Perjury Act 1728.

Materiality

The requirement that the statement be material can be traced back to, and has been credited to, Coke. He said:

Perjury is a statutory offence in Northern Ireland. It is created by article 3(1) of the Perjury (Northern Ireland) Order 1979 (S.I. 1979/1714 (N.I. 19)). This replaces the Perjury Act (Northern Ireland) 1946 (c. 13) (N.I.).

Perjury operates in American law as an inherited principle of the common law of England, which defined the act as the "willful and corrupt giving, upon a lawful oath, or in any form allowed by law to be substituted for an oath, in a judicial proceeding or course of justice, of a false testimony material to the issue or matter of inquiry." William Blackstone touched on the subject in his Commentaries on the Laws of England, establishing perjury as "a crime committed when a lawful oath is administered, in some judicial proceeding, to a person who swears willfully, absolutely, and falsely, in a matter material to the issue or point in question." The punishment for perjury under the common law has varied from death to banishment and has included such grotesque penalties as severing the tongue of the perjurer. The definitional structure of perjury provides an important framework for legal proceedings, as the component parts of this definition have permeated jurisdictional lines, finding a home in American legal constructs. As such, the main tenets of perjury, including mens rea, a lawful oath, occurring during a judicial proceeding, a false testimony have remained necessary pieces of perjury’s definition in the United States.

Perjury’s current position in the American legal system takes the form of state and federal statutes. Most notably, the United States Code prohibits perjury, which is defined in two senses for federal purposes as someone who:

The above statute provides for a fine and/or up to five years in prison as punishment. Within federal jurisdiction, statements made in two broad categories of judicial proceedings may qualify as perjurious: 1) Federal official proceedings, and 2) Federal Court or Grand Jury proceedings. A third type of perjury entails the procurement of perjurious statements from another person. More generally, the statement must occur in the "course of justice," but this definition leaves room open for interpretation. One particularly precarious aspect of this phrasing is that it entails knowledge of the accused person’s perception of the truthful nature of events and not necessarily the actual truth of those events. It is important to note the distinction here, between giving a false statement under oath and merely misstating a fact accidentally, though this distinction can be especially difficult to discern in court of law.

The development of perjury law in the United States centers on United States v. Dunnigan, a seminal case that set out the parameters of perjury within United States law. The court uses the Dunnigan-based legal standard to determine if an accused person, "[T]estifying under oath or affirmation violates this section if she gives false testimony concerning a material matter with the willful intent to provide false testimony, rather than as a result of confusion, mistake, or faulty memory." However, a defendant shown to be willfully ignorant may in fact be eligible for perjury prosecution. The Dunnigan distinction manifests its importance with regard to the relation between two component parts of perjury’s definition: in willfully giving a false statement, a person must understand that she is giving a false statement to be considered a perjurer under the Dunnigan framework. Deliberation on the part of the defendant is required for a statement to constitute perjury. Jurisprudential developments in the American law of perjury have revolved around the facilitation of "perjury prosecutions and thereby enhance the reliability of testimony before federal courts and grand juries." With this goal in mind, Congress has sometimes expanded the grounds on which an individual may be prosecuted for perjury, with section 1623 of the United States Code recognizing the utterance of two mutually incompatible statements as grounds for perjury indictment even if neither can unequivocally be proven false. However, the two statements must be so mutually incompatible that at least one must necessarily be false; it is irrelevant whether the false statement can be specifically identified from among the two. It thus falls on the government to show that a defendant (a) knowingly made a (b) false (c) material statement (d) under oath (e) in a legal proceeding. These proceedings can be ancillary to normal court proceedings, and thus, even such menial interactions as bail hearings can qualify as protected proceedings under this statute.

Wilfulness is an element of the offense. The mere existence of two mutually exclusive factual statements is not sufficient to prove perjury; the prosecutor nonetheless has the duty to plead and prove statement was willfully made. Mere contradiction will not sustain the charge; there must be strong corroborative evidence of the contradiction.

One significant legal distinction lies in the specific realm of knowledge necessarily possessed by a defendant for her statements to be properly called perjury. Though the defendant must knowingly render a false statement in a legal proceeding or under federal jurisdiction, the defendant need not know that they are speaking under such conditions for the statement to constitute perjury. All tenets of perjury qualification persist–the “knowingly” aspect of telling the false statement simply does not apply to the defendant’s knowledge about the person she intends to deceive.

Perjury law’s evolution in the United States has experienced the most debate with regards to the materiality requirement. Fundamentally, statements that are literally true cannot provide the basis for a perjury charge (as they do not meet the falsehood requirement) just as answers to truly ambiguous statements cannot constitute perjury. However, such fundamental truths of perjury law become muddled when discerning the materiality of a given statement and the way in which it was material to the given case. In "United States v. Brown", the court defined material statements as those with "a natural tendency to influence, or is capable of influencing, the decision of the decision-making body to be addressed," such as a jury or grand jury. While courts have specifically made clear certain instances which have succeeded or failed to meet the nebulous threshold for materiality, the topic remains unresolved in large part, except in certain legal areas where intent manifests itself in an abundantly clear fashion, such as with the so-called perjury trap, a specific situation in which a prosecutor calls a person to testify before a grand jury with the intent of drawing a perjurious statement from the person being questioned.

Despite a tendency of American perjury law toward broad prosecutory power under perjury statutes, American perjury law has afforded potential defendants a new form of defense not found in the British Common Law. This defense requires that an individual admit to making a perjurious statement during that same proceeding and recanting the statement. Though this defensive loophole slightly narrows the types of cases which may be prosecuted for perjury, the effect of this statutory defense is to promote a truthful retelling of facts by witnesses, thus helping to ensure the reliability of American court proceedings just as broadened perjury statutes aimed to do.

Subornation of perjury stands as a subset of American perjury laws and prohibits an individual from inducing another to commit perjury. Subornation of perjury entails equivalent possible punishments as perjury on the federal level. This crime requires an extra level of satisfactory proof, as prosecutors must show not only that perjury occurred, but also that the defendant positively induced said perjury. Furthermore, the inducing defendant must know that the suborned statement is a false, perjurious statement.


Notable people who have been accused of perjury include:



</doc>
<doc id="23689" url="https://en.wikipedia.org/wiki?curid=23689" title="Phoenix">
Phoenix

Phoenix most often refers to:


Phoenix may also refer to:




























Lists

Individual vessels





</doc>
<doc id="23690" url="https://en.wikipedia.org/wiki?curid=23690" title="Phosphate">
Phosphate

A phosphate is chemical derivative of phosphoric acid. The phosphate ion, (), is an inorganic chemical, the conjugate base that can form many different salts. In organic chemistry, a phosphate, or organophosphate, is an ester of phosphoric acid. Of the various phosphoric acids and phosphates, organic phosphates are important in biochemistry and biogeochemistry (and, consequently, in ecology), and inorganic phosphates are mined to obtain phosphorus for use in agriculture and industry. At elevated temperatures in the solid state, phosphates can condense to form pyrophosphates.

In biology, adding phosphates to—and removing them from—proteins in cells are both pivotal in the regulation of metabolic processes. Referred to as phosphorylation and dephosphorylation, respectively, they are important ways that energy is stored and released in living systems.

The phosphate ion is a polyatomic ion with the empirical formula and a molar mass of 94.97 g/mol. It consists of one central phosphorus atom surrounded by four oxygen atoms in a tetrahedral arrangement. The phosphate ion carries a −3 formal charge and is the conjugate base of the hydrogen phosphate ion, , which is the conjugate base of , the dihydrogen phosphate ion, which in turn is the conjugate base of , phosphoric acid. A phosphate salt forms when a positively charged ion attaches to the negatively charged oxygen atoms of the ion, forming an ionic compound.

Many phosphates are not soluble in water at standard temperature and pressure. The sodium, potassium, rubidium, caesium, and ammonium phosphates are all water-soluble. Most other phosphates are only slightly soluble or are insoluble in water. As a rule, the hydrogen and dihydrogen phosphates are slightly more soluble than the corresponding phosphates. The pyrophosphates are mostly water-soluble. Aqueous phosphate exists in four forms:

More precisely, considering these three equilibrium reactions:

the corresponding constants at 25 °C (in mol/L) are (see phosphoric acid):

The speciation diagram obtained using these p"K" values shows three distinct regions. In effect, , and behave as separate weak acids because the successive p"K" values differ by more than 4. For each acid, the pH at half-neutralization is equal to the p"K" value of the acid. The region in which the acid is in equilibrium with its conjugate base is defined by . Thus, the three pH regions are approximately 0–4, 5–9 and 10–14. This is a simplified model, assuming a constant ionic strength. It will not hold in reality at very low and very high pH values.

For a neutral pH, as in the cytosol, pH = 7.0

so that only and ions are present in significant amounts (62% , 38% . Note that in the extracellular fluid (pH = 7.4), this proportion is inverted (61% , 39% ).

Phosphate can form many polymeric ions such as pyrophosphate), , and triphosphate, . The various metaphosphate ions (which are usually long linear polymers) have an empirical formula of and are found in many compounds.

In biological systems, phosphorus is found as a free phosphate ion in solution and is called inorganic phosphate, to distinguish it from phosphates bound in various phosphate esters. Inorganic phosphate is generally denoted P and at physiological (homeostatic) pH primarily consists of a mixture of and ions.

Inorganic phosphate can be created by the hydrolysis of pyrophosphate, denoted PP:

However, phosphates are most commonly found in the form of adenosine phosphates (AMP, ADP, and ATP) and in DNA and RNA. It can be released by the hydrolysis of ATP or ADP. Similar reactions exist for the other nucleoside diphosphates and triphosphates. Phosphoanhydride bonds in ADP and ATP, or other nucleoside diphosphates and triphosphates, can release high amounts of energy when hydrolyzed which give them their vital role in all living organisms. They are generally referred to as high-energy phosphate, as are the phosphagens in muscle tissue. Compounds such as substituted phosphines have uses in organic chemistry, but do not seem to have any natural counterparts.

The addition and removal of phosphate from proteins in all cells is a pivotal strategy in the regulation of metabolic processes. Phosphorylation and dephosphorylation are important ways that energy is stored and released in living systems. Cells use ATP for this.

Phosphate is useful in animal cells as a buffering agent. Phosphate salts that are commonly used for preparing buffer solutions at cell pHs include NaHPO, NaHPO, and the corresponding potassium salts.

An important occurrence of phosphates in biological systems is as the structural material of bone and teeth. These structures are made of crystalline calcium phosphate in the form of hydroxyapatite. The hard dense enamel of mammalian teeth consists of fluoroapatite, a hydroxy calcium phosphate where some of the hydroxyl groups have been replaced by fluoride ions.

Plants take up phosphorus through several pathways: the arbuscular mycorrhizal pathway and the direct uptake pathway.

Phosphates are the naturally occurring form of the element phosphorus, found in many phosphate minerals. In mineralogy and geology, phosphate refers to a rock or ore containing phosphate ions. Inorganic phosphates are mined to obtain phosphorus for use in agriculture and industry.

The largest global producer and exporter of phosphates is Morocco. Within North America, the largest deposits lie in the Bone Valley region of central Florida, the Soda Springs region of southeastern Idaho, and the coast of North Carolina. Smaller deposits are located in Montana, Tennessee, Georgia, and South Carolina. The small island nation of Nauru and its neighbor Banaba Island, which used to have massive phosphate deposits of the best quality, have been mined excessively. Rock phosphate can also be found in Egypt, Israel, Western Sahara, Navassa Island, Tunisia, Togo, and Jordan, countries that have large phosphate-mining industries.

Phosphorite mines are primarily found in:


In 2007, at the current rate of consumption, the supply of phosphorus was estimated to run out in 345 years. However, some scientists thought that a "peak phosphorus" will occur in 30 years and Dana Cordell from Institute for Sustainable Futures said that at "current rates, reserves will be depleted in the next 50 to 100 years". Reserves refer to the amount assumed recoverable at current market prices, and, in 2012, the USGS estimated 71 billion tons of world reserves, while 0.19 billion tons were mined globally in 2011. Phosphorus comprises 0.1% by mass of the average rock (while, for perspective, its typical concentration in vegetation is 0.03% to 0.2%), and consequently there are quadrillions of tons of phosphorus in Earth's 3 * 10 ton crust, albeit at predominantly lower concentration than the deposits counted as reserves from being inventoried and cheaper to extract; if it is assumed that the phosphate minerals in phosphate rock are hydroxyapatite and fluoroapatite, phosphate minerals contain roughly 18.5% phosphorus by weight and if phosphate rock contains around 20% of these minerals, the average phosphate rock has roughly 3.7% phosphorus by weight.

Some phosphate rock deposits are notable for their inclusion of significant quantities of radioactive uranium isotopes. This syndrome is noteworthy because radioactivity can be released into surface waters in the process of application of the resultant phosphate fertilizer (e.g. in many tobacco farming operations in the southeast US).

In December 2012, Cominco Resources announced an updated JORC compliant resource of their Hinda project in Congo-Brazzaville of 531 Mt, making it the largest measured and indicated phosphate deposit in the world.

The three principal phosphate producer countries (China, Morocco and the United States) account for about 70% of world production.

In ecological terms, because of its important role in biological systems, phosphate is a highly sought after resource. Once used, it is often a limiting nutrient in environments, and its availability may govern the rate of growth of organisms. This is generally true of freshwater environments, whereas nitrogen is more often the limiting nutrient in marine (seawater) environments. Addition of high levels of phosphate to environments and to micro-environments in which it is typically rare can have significant ecological consequences. For example, blooms in the populations of some organisms at the expense of others, and the collapse of populations deprived of resources such as oxygen (see eutrophication) can occur. In the context of pollution, phosphates are one component of total dissolved solids, a major indicator of water quality, but not all phosphorus is in a molecular form that algae can break down and consume.

Calcium hydroxyapatite and calcite precipitates can be found around bacteria in alluvial topsoil. As clay minerals promote biomineralization, the presence of bacteria and clay minerals resulted in calcium hydroxyapatite and calcite precipitates.

Phosphate deposits can contain significant amounts of naturally occurring heavy metals. Mining operations processing phosphate rock can leave tailings piles containing elevated levels of cadmium, lead, nickel, copper, chromium, and uranium. Unless carefully managed, these waste products can leach heavy metals into groundwater or nearby estuaries. Uptake of these substances by plants and marine life can lead to concentration of toxic heavy metals in food products.



</doc>
<doc id="23692" url="https://en.wikipedia.org/wiki?curid=23692" title="Prime number theorem">
Prime number theorem

In number theory, the prime number theorem (PNT) describes the asymptotic distribution of the prime numbers among the positive integers. It formalizes the intuitive idea that primes become less common as they become larger by precisely quantifying the rate at which this occurs. The theorem was proved independently by Jacques Hadamard and Charles Jean de la Vallée Poussin in 1896 using ideas introduced by Bernhard Riemann (in particular, the Riemann zeta function).

The first such distribution found is , where is the prime-counting function and is the natural logarithm of . This means that for large enough , the probability that a random integer not greater than is prime is very close to . Consequently, a random integer with at most digits (for large enough ) is about half as likely to be prime as a random integer with at most digits. For example, among the positive integers of at most 1000 digits, about one in 2300 is prime (), whereas among positive integers of at most 2000 digits, about one in 4600 is prime (). In other words, the average gap between consecutive prime numbers among the first integers is roughly .

Let be the prime-counting function that gives the number of primes less than or equal to , for any real number . For example, because there are four prime numbers (2, 3, 5 and 7) less than or equal to 10. The prime number theorem then states that is a good approximation to , in the sense that the limit of the "quotient" of the two functions and as increases without bound is 1:

known as the asymptotic law of distribution of prime numbers. Using asymptotic notation this result can be restated as

This notation (and the theorem) does "not" say anything about the limit of the "difference" of the two functions as increases without bound. Instead, the theorem states that approximates in the sense that the relative error of this approximation approaches 0 as increases without bound.

The prime number theorem is equivalent to the statement that the th prime number satisfies

the asymptotic notation meaning, again, that the relative error of this approximation approaches 0 as increases without bound. For example, the th prime number is , and ()log() rounds to , a relative error of about 6.4%.

The prime number theorem is also equivalent to
where and are the first and the second Chebyshev functions respectively.

Based on the tables by Anton Felkel and Jurij Vega, Adrien-Marie Legendre conjectured in 1797 or 1798 that is approximated by the function , where and are unspecified constants. In the second edition of his book on number theory (1808) he then made a more precise conjecture, with and . Carl Friedrich Gauss considered the same question at age 15 or 16 "in the year 1792 or 1793", according to his own recollection in 1849. In 1838 Peter Gustav Lejeune Dirichlet came up with his own approximating function, the logarithmic integral (under the slightly different form of a series, which he communicated to Gauss). Both Legendre's and Dirichlet's formulas imply the same conjectured asymptotic equivalence of and stated above, although it turned out that Dirichlet's approximation is considerably better if one considers the differences instead of quotients.

In two papers from 1848 and 1850, the Russian mathematician Pafnuty Chebyshev attempted to prove the asymptotic law of distribution of prime numbers. His work is notable for the use of the zeta function , for real values of the argument "", as in works of Leonhard Euler, as early as 1737. Chebyshev's papers predated Riemann's celebrated memoir of 1859, and he succeeded in proving a slightly weaker form of the asymptotic law, namely, that if the limit of as goes to infinity exists at all, then it is necessarily equal to one. He was able to prove unconditionally that this ratio is bounded above and below by two explicitly given constants near 1, for all sufficiently large . Although Chebyshev's paper did not prove the Prime Number Theorem, his estimates for were strong enough for him to prove Bertrand's postulate that there exists a prime number between and for any integer .

An important paper concerning the distribution of prime numbers was Riemann's 1859 memoir "On the Number of Primes Less Than a Given Magnitude", the only paper he ever wrote on the subject. Riemann introduced new ideas into the subject, the chief of them being that the distribution of prime numbers is intimately connected with the zeros of the analytically extended Riemann zeta function of a complex variable. In particular, it is in this paper of Riemann that the idea to apply methods of complex analysis to the study of the real function originates. Extending the ideas of Riemann, two proofs of the asymptotic law of the distribution of prime numbers were obtained independently by Jacques Hadamard and Charles Jean de la Vallée Poussin and appeared in the same year (1896). Both proofs used methods from complex analysis, establishing as a main step of the proof that the Riemann zeta function is non-zero for all complex values of the variable that have the form with .

During the 20th century, the theorem of Hadamard and de la Vallée Poussin also became known as the Prime Number Theorem. Several different proofs of it were found, including the "elementary" proofs of Atle Selberg and Paul Erdős (1949). While the original proofs of Hadamard and de la Vallée Poussin are long and elaborate, later proofs introduced various simplifications through the use of Tauberian theorems but remained difficult to digest. A short proof was discovered in 1980 by American mathematician Donald J. Newman. Newman's proof is arguably the simplest known proof of the theorem, although it is non-elementary in the sense that it uses Cauchy's integral theorem from complex analysis.

Here is a sketch of the proof referred to in one of Terence Tao's lectures. Like most proofs of the PNT, it starts out by reformulating the problem in terms of a less intuitive, but better-behaved, prime-counting function. The idea is to count the primes (or a related set such as the set of prime powers) with "weights" to arrive at a function with smoother asymptotic behavior. The most common such generalized counting function is the Chebyshev function , defined by

This is sometimes written as

where is the von Mangoldt function, namely

It is now relatively easy to check that the PNT is equivalent to the claim that
Indeed, this follows from the easy estimates
and (using big notation) for any ,

The next step is to find a useful representation for . Let be the Riemann zeta function. It can be shown that is related to the von Mangoldt function , and hence to , via the relation

A delicate analysis of this equation and related properties of the zeta function, using the Mellin transform and Perron's formula, shows that for non-integer the equation

holds, where the sum is over all zeros (trivial and nontrivial) of the zeta function. This striking formula is one of the so-called explicit formulas of number theory, and is already suggestive of the result we wish to prove, since the term (claimed to be the correct asymptotic order of ) appears on the right-hand side, followed by (presumably) lower-order asymptotic terms.

The next step in the proof involves a study of the zeros of the zeta function. The trivial zeros −2, −4, −6, −8, ... can be handled separately:
which vanishes for a large . The nontrivial zeros, namely those on the critical strip , can potentially be of an asymptotic order comparable to the main term if , so we need to show that all zeros have real part strictly less than 1.

To do this, we take for granted that is meromorphic in the half-plane , and is analytic there except for a simple pole at , and that there is a product formula
for . This product formula follows from the existence of unique prime factorization of integers, and shows that is never zero in this region, so that its logarithm is defined there and

Write ; then

Now observe the identity
so that

for all . Suppose now that . Certainly is not zero, since has a simple pole at . Suppose that and let tend to 1 from above. Since formula_19 has a simple pole at and stays analytic, the left hand side in the previous inequality tends to 0, a contradiction.

Finally, we can conclude that the PNT is heuristically true. To rigorously complete the proof there are still serious technicalities to overcome, due to the fact that the summation over zeta zeros in the explicit formula for does not converge absolutely but only conditionally and in a "principal value" sense. There are several ways around this problem but many of them require rather delicate complex-analytic estimates that are beyond the scope of this paper. Edwards's book provides the details. Another method is to use Ikehara's Tauberian theorem, though this theorem is itself quite hard to prove. D. J. Newman observed that the full strength of Ikehara's theorem is not needed for the prime number theorem, and one can get away with a special case that is much easier to prove.

In a handwritten note on a reprint of his 1838 paper "", which he mailed to Gauss, Dirichlet conjectured (under a slightly different form appealing to a series rather than an integral) that an even better approximation to is given by the offset logarithmic integral function , defined by

Indeed, this integral is strongly suggestive of the notion that the "density" of primes around should be . This function is related to the logarithm by the asymptotic expansion

So, the prime number theorem can also be written as . In fact, in another paper in 1899 de la Vallée Poussin proved that

for some positive constant , where is the big notation. This has been improved to

Because of the connection between the Riemann zeta function and , the Riemann hypothesis has considerable importance in number theory: if established, it would yield a far better estimate of the error involved in the prime number theorem than is available today. More specifically, Helge von Koch showed in 1901 that, if and only if the Riemann hypothesis is true, the error term in the above relation can be improved to

The constant involved in the big notation was estimated in 1976 by Lowell Schoenfeld: assuming the Riemann hypothesis,

for all . He also derived a similar bound for the Chebyshev prime-counting function :

for all . This latter bound has been shown to express a variance to mean power law (when regarded as a random function over the integers), noise and to also correspond to the Tweedie compound Poisson distribution. Parenthetically, the Tweedie distributions represent a family of scale invariant distributions that serve as foci of convergence for a generalization of the central limit theorem.

The logarithmic integral is larger than for "small" values of . This is because it is (in some sense) counting not primes, but prime powers, where a power of a prime is counted as of a prime. This suggests that should usually be larger than by roughly , and in particular should always be larger than . However, in 1914, J. E. Littlewood proved that this is not the case. The first value of where exceeds is probably around ; see the article on Skewes' number for more details. (On the other hand, the offset logarithmic integral is smaller than already for ; indeed, , while .)

In the first half of the twentieth century, some mathematicians (notably G. H. Hardy) believed that there exists a hierarchy of proof methods in mathematics depending on what sorts of numbers (integers, reals, complex) a proof requires, and that the prime number theorem (PNT) is a "deep" theorem by virtue of requiring complex analysis. This belief was somewhat shaken by a proof of the PNT based on Wiener's tauberian theorem, though this could be set aside if Wiener's theorem were deemed to have a "depth" equivalent to that of complex variable methods.

In March 1948, Atle Selberg established, by "elementary" means, the asymptotic formula

where

for primes . By July of that year, Selberg and Paul Erdős had each obtained elementary proofs of the PNT, both using Selberg's asymptotic formula as a starting point. These proofs effectively laid to rest the notion that the PNT was "deep", and showed that technically "elementary" methods were more powerful than had been believed to be the case. On the history of the elementary proofs of the PNT, including the Erdős–Selberg priority dispute, see an article by Dorian Goldfeld.

There is some debate about the significance of Erdős and Selberg's result. There is no rigorous and widely accepted definition of the notion of elementary proof in number theory, so it is not clear exactly in what sense their proof is "elementary". Although it does not use complex analysis, it is in fact much more technical than the standard proof of PNT. One possible definition of an "elementary" proof is "one that can be carried out in first order Peano arithmetic." There are number-theoretic statements (for example, the Paris–Harrington theorem) provable using second order but not first order methods, but such theorems are rare to date. Erdős and Selberg's proof can certainly be formalized in Peano arithmetic, and in 1994, Charalambos Cornaros and Costas Dimitracopoulos proved that their proof can be formalized in a very weak fragment of PA, namely , However, this does not address the question of whether or not the standard proof of PNT can be formalized in PA.

In 2005, Avigad "et al." employed the Isabelle theorem prover to devise a computer-verified variant of the Erdős–Selberg proof of the PNT. This was the first machine-verified proof of the PNT. Avigad chose to formalize the Erdős–Selberg proof rather than an analytic one because while Isabelle's library at the time could implement the notions of limit, derivative, and transcendental function, it had almost no theory of integration to speak of.

In 2009, John Harrison employed HOL Light to formalize a proof employing complex analysis. By developing the necessary analytic machinery, including the Cauchy integral formula, Harrison was able to formalize "a direct, modern and elegant proof instead of the more involved 'elementary' Erdős–Selberg argument".

Let denote the number of primes in the arithmetic progression less than . Dirichlet and Legendre conjectured, and de la Vallée Poussin proved, that, if and are coprime, then

where is Euler's totient function. In other words, the primes are distributed evenly among the residue classes modulo with 1. This is stronger than Dirichlet's theorem on arithmetic progressions (which only states that there is an infinity of primes in each class) and can be proved using similar methods used by Newman for his proof of the prime number theorem.

The Siegel–Walfisz theorem gives a good estimate for the distribution of primes in residue classes.

Although we have in particular

empirically the primes congruent to 3 are more numerous and are nearly always ahead in this "prime number race"; the first reversal occurs at . However Littlewood showed in 1914 that there are infinitely many sign changes for the function

so the lead in the race switches back and forth infinitely many times. The phenomenon that is ahead most of the time is called Chebyshev's bias. The prime number race generalizes to other moduli and is the subject of much research; Pál Turán asked whether it is always the case that and change places when and are coprime to . Granville and Martin give a thorough exposition and survey.

The prime number theorem is an "asymptotic" result. It gives an ineffective bound on as a direct consequence of the definition of the limit: for all , there is an such that for all ,

However, better bounds on are known, for instance Pierre Dusart's
The first inequality holds for all and the second one for .

A weaker but sometimes useful bound for is

In Pierre Dusart's thesis there are stronger versions of this type of inequality that are valid for larger . Later in 2010, Dusart proved:

The proof by de la Vallée Poussin implies the following.
For every , there is an such that for all ,

As a consequence of the prime number theorem, one gets an asymptotic expression for the th prime number, denoted by :
A better approximation is
Again considering the th prime number , this gives an estimate of ; the first 5 digits match and relative error is about 0.00005%.

Rosser's theorem states that 
This can be improved by the following pair of bounds:

The table compares exact values of to the two approximations and . The last column, , is the average prime gap below .

The value for was originally computed assuming the Riemann hypothesis; it has since been verified unconditionally.

There is an analogue of the prime number theorem that describes the "distribution" of irreducible polynomials over a finite field; the form it takes is strikingly similar to the case of the classical prime number theorem.

To state it precisely, let be the finite field with elements, for some fixed , and let be the number of monic "irreducible" polynomials over whose degree is equal to . That is, we are looking at polynomials with coefficients chosen from , which cannot be written as products of polynomials of smaller degree. In this setting, these polynomials play the role of the prime numbers, since all other monic polynomials are built up of products of them. One can then prove that
If we make the substitution , then the right hand side is just
which makes the analogy clearer. Since there are precisely monic polynomials of degree (including the reducible ones), this can be rephrased as follows: if a monic polynomial of degree is selected randomly, then the probability of it being irreducible is about .

One can even prove an analogue of the Riemann hypothesis, namely that

The proofs of these statements are far simpler than in the classical case. It involves a short combinatorial argument, summarised as follows: every element of the degree extension of is a root of some irreducible polynomial whose degree divides ; by counting these roots in two different ways one establishes that
where the sum is over all divisors of . Möbius inversion then yields
where is the Möbius function. (This formula was known to Gauss.) The main term occurs for , and it is not difficult to bound the remaining terms. The "Riemann hypothesis" statement depends on the fact that the largest proper divisor of can be no larger than .





</doc>
<doc id="23693" url="https://en.wikipedia.org/wiki?curid=23693" title="Conflict of laws">
Conflict of laws

Conflict of laws concerns relations across different legal jurisdictions between natural persons, companies, corporations and other legal entities, their legal obligations and the appropriate forum and procedure for resolving disputes between them. Conflict of laws especially affects private international law, but may also affect domestic legal disputes e.g. determination of which state law applies in the United States, or where a contract makes incompatible reference to more than one legal framework.

Courts faced with a choice of law issue have a two-stage process:


In divorce cases, when a court is attempting to distribute marital property, if the divorcing couple is local and the property is local, then the court applies its domestic law "lex fori". The case becomes more complicated if foreign elements are thrown into the mix, such as when the place of marriage is different from the territory where divorce was filed; when the parties' nationalities and residences do not match; when there is property in a foreign jurisdiction; or when the parties have changed residence several times during the marriage.

Whereas commercial agreements or prenuptial agreements generally do not require legal formalities to be observed, when married couples enter a property agreement (agreement for the division of property at the termination of the marriage), stringent requirements are imposed, including notarization, witnesses, special acknowledgment forms. In some countries, these must be filed (or docketed) with a domestic court, and the terms must be "so ordered" by a judge. This is done in order to ensure that no undue influence or oppression has been exerted by one spouse against the other. Upon presenting a property agreement between spouses to a court of divorce, that court will generally assure itself of the following factors: signatures, legal formalities, intent, later intent, free will, lack of oppression, reasonableness and fairness, consideration, performance, reliance, later repudiation in writing or by conduct, and whichever other concepts of contractual bargaining apply in the context.

Unlike marriage which has an international recognised legal status, there are no international treaties on recognition of unmarried couple's legal status. If an unmarried couple change residence to different countries, then the local law on where the couple is last domiciled is applied to them. This covers legal status of the relationship, rights, obligations, and all worldwide movable and immovable property. To otherwise interpret the law would mean if the unmarried couple had assets in several different countries, they would then need separate legal cases in each country to resolve all their movable and immovable property.

In the absence of a valid and enforceable agreement for an unmarried couple, here’s how the conflict of law rules work:


Many contracts and other forms of legally binding agreement include a jurisdiction or arbitration clause specifying the parties' choice of venue for any litigation (called a forum selection clause). In England and the EU, this is governed by the Rome I Regulation. Choice of law clauses may specify which laws the court or tribunal should apply to each aspect of the dispute. This matches the substantive policy of freedom of contract and will be determined by the law of the state where the choice of law clause confers its competence. Oxford Professor Adrian Briggs suggests that this is doctrinally problematic as it is emblematic of 'pulling oneself up by the bootstraps'. Judges have accepted that the principle of party autonomy allows the parties to select the law most appropriate to their transaction. This judicial acceptance of subjective intent excludes the traditional reliance on objective connecting factors; it also harms consumers as vendors often impose one-sided contractual terms selecting a venue far from the buyer's home or workplace. Contractual clauses relating to consumers, employees, and insurance beneficiaries are regulated under additional terms set out in Rome I, which may modify the contractual terms imposed by vendors.

To apply one national legal system as against another may never be an entirely satisfactory approach. The parties' interests may always be better protected by applying a law conceived with international realities in mind. The Hague Conference on Private International Law is a treaty organization that oversees conventions designed to develop a uniform system. The deliberations of the conference have recently been the subject of controversy over the extent of cross-border jurisdiction on electronic commerce and defamation issues. There is a general recognition that there is a need for an international law of contracts: for example, many nations have ratified the "Vienna Convention on the International Sale of Goods", the "Rome Convention on the Law Applicable to Contractual Obligations" offers less specialized uniformity, and there is support for the "UNIDROIT Principles of International Commercial Contracts", a private restatement, all of which represent continuing efforts to produce international standards as the internet and other technologies encourage ever more interstate commerce. But other branches of the law are less well served and the dominant trend remains the role of the forum law rather than a supranational system for conflict purposes. Even the EU, which has institutions capable of creating uniform rules with direct effect, has failed to produce a universal system for the common market. Nevertheless, the Treaty of Amsterdam does confer authority on the community's institutions to legislate by Council Regulation in this area with supranational effect. Article 177 would give the Court of Justice jurisdiction to interpret and apply their principles so, if the political will arises, uniformity may gradually emerge in letter. Whether the domestic courts of the Member States would be consistent in applying those letters is speculative.





</doc>
<doc id="23696" url="https://en.wikipedia.org/wiki?curid=23696" title="Timeline of programming languages">
Timeline of programming languages

This is a record of historically important programming languages, by decade.




</doc>
<doc id="23698" url="https://en.wikipedia.org/wiki?curid=23698" title="International Fixed Calendar">
International Fixed Calendar

The International Fixed Calendar (also known as the Cotsworth plan, the Eastman plan, the 13 Month calendar or the Equal Month calendar) is a solar calendar proposal for calendar reform designed by Moses B. Cotsworth, who presented it in 1902. It divides the solar year into 13 months of 28 days each. It is therefore a perennial calendar, with every date fixed to the same weekday every year. Though it was never officially adopted in any country, entrepreneur George Eastman adopted it for use in his Eastman Kodak Company, where it was used from 1928 to 1989.

The calendar year has 13 months with 28 days each, divided into exactly 4 weeks (13 × 28 = 364). An extra day added as a holiday at the end of the year (after December 28, i.e. equal December 31 Gregorian), sometimes called "Year Day", does not belong to any week and brings the total to 365 days. Each year coincides with the corresponding Gregorian year, so January 1 in the Cotsworth calendar always falls on Gregorian January 1. Twelve months are named and ordered the same as those of the Gregorian calendar, except that the extra month is inserted between June and July, and called "Sol". Situated in mid-summer (from the point of view of its Northern Hemisphere authors) and including the mid-year "solstice", the name of the new month was chosen in homage to the sun.

Leap year in the International Fixed Calendar contains 366 days, and its occurrence follows the Gregorian rule. There is a leap year in every year whose number is divisible by 4, but not if the year number is divisible by 100, unless it is also divisible by 400. So although the year 2000 was a leap year, the years 1700, 1800, and 1900 were common years. The International Fixed Calendar inserts the extra day in leap year as June 29 - between Saturday June 28 and Sunday Sol 1.

Each month begins on a Sunday, and ends on a Saturday; consequently, every year begins on Sunday. Neither Year Day nor Leap Day are considered to be part of any week; they are preceded by a Saturday and are followed by a Sunday.

All the months look like this:

The following shows how the 13 months and extra days of the International Fixed Calendar occur in relation to the dates of the Gregorian calendar:

<nowiki>*</nowiki>These Gregorian dates between March and June are a day earlier in a Gregorian leap year. March in the Fixed Calendar always has a fixed number of days (28), and includes the Gregorian 29 February (on Gregorian leap years).

Lunisolar calendars, with fixed weekdays, existed in many ancient cultures, with certain holidays always falling on the same dates of the month and days of the week. 

The simple idea of a 13-month perennial calendar has been around since at least the middle of the 18th century. Versions of the idea differ mainly on how the months are named, and the treatment of the extra day in leap year.

The "Georgian calendar" was proposed in 1745 by an American colonist from Maryland writing under the pen name Hirossa Ap-Iccim, the Rev. Hugh Jones. The author named the plan, and the thirteenth month, after King George II of Great Britain. The 365th day each year was to be set aside as Christmas. The treatment of leap year varied from the Gregorian rule, however; and the year would begin closer to the winter solstice. In a later version of the plan, published in 1753, the 13 months were all renamed for Christian saints.

In 1849 the French philosopher Auguste Comte (1798–1857) proposed the 13-month "Positivist Calendar", naming the months: Moses, Homer, Aristotle, Archimedes, Caesar, St. Paul, Charlemagne, Dante, Gutenberg, Shakespeare, Descartes, Frederic and Bichat. The days of the year were likewise dedicated to "saints" in the Positivist Religion of Humanity. Positivist weeks, months, and years begin with Monday instead of Sunday. Comte also reset the year number, beginning the era of his calendar (year 1) with the Gregorian year 1789. For the extra days of the year not belonging to any week or month, Comte followed the pattern of Ap-Iccim (Jones), ending each year with a festival on the 365th day, followed by a subsequent feast day occurring only in leap years.

Whether Moses Cotsworth was familiar with the 13-month plans that preceded his International Fixed Calendar is not known. He did follow Ap-Iccim (Jones) in designating the 365th day of the year as Christmas. His suggestion was that this last day of the year should be designated a Sunday, and hence, because the following day would be New Year's Day and a Sunday also, he called it a Double Sunday. Since Cotsworth's goal was a simplified, more "rational" calendar for business and industry, he would carry over all the features of the Gregorian calendar consistent with this goal, including the traditional month names, the week beginning on Sunday (still traditionally used in US, but uncommon in most other countries and in the ISO week standard, starting their weeks on Monday), and the Gregorian leap-year rule.

To promote Cotsworth's calendar reform the International Fixed Calendar League was founded in 1923, just after the plan was selected by the League of Nations as the best of 130 calendar proposals put forward. Sir Sandford Fleming, the inventor and driving force behind worldwide adoption of standard time, became the first president of the IFCL. The League opened offices in London and later in Rochester, New York. George Eastman, of the Eastman Kodak Company, became a fervent supporter of the IFC, and instituted its use at Kodak. The International Fixed Calendar League ceased operations shortly after the calendar plan failed to win final approval of the League of Nations in 1937.

The several advantages of the International Fixed Calendar are mainly related to its organization.






</doc>
<doc id="23703" url="https://en.wikipedia.org/wiki?curid=23703" title="Potential energy">
Potential energy

In physics, potential energy is the energy possessed by an object because of its position relative to other objects, stresses within itself, its electric charge, or other factors.

Common types of potential energy include the gravitational potential energy of an object that depends on its mass and its distance from the center of mass of another object, the elastic potential energy of an extended spring, and the electric potential energy of an electric charge in an electric field. The unit for energy in the International System of Units (SI) is the joule, which has the symbol J.

The term "potential energy" was introduced by the 19th century Scottish engineer and physicist William Rankine, although it has links to Greek philosopher Aristotle's concept of potentiality.
Potential energy is associated with forces that act on a body in a way that the total work done by these forces on the body depends only on the initial and final positions of the body in space. These forces, that are called "conservative forces", can be represented at every point in space by vectors expressed as gradients of a certain scalar function called "potential".

Since the work of potential forces acting on a body that moves from a start to an end position is determined only by these two positions, and does not depend on the trajectory of the body, there is a function known as "potential" or "potential energy" that can be evaluated at the two positions to determine this work.

There are various types of potential energy, each associated with a particular type of force. For example, the work of an elastic force is called elastic potential energy; work of the gravitational force is called gravitational potential energy; work of the Coulomb force is called electric potential energy; work of the strong nuclear force or weak nuclear force acting on the baryon charge is called nuclear potential energy; work of intermolecular forces is called intermolecular potential energy. Chemical potential energy, such as the energy stored in fossil fuels, is the work of the Coulomb force during rearrangement of mutual positions of electrons and nuclei in atoms and molecules. Thermal energy usually has two components: the kinetic energy of random motions of particles and the potential energy of their mutual positions.

Forces derivable from a potential are also called conservative forces. The work done by a conservative force is
where formula_2 is the change in the potential energy associated with the force. The negative sign provides the convention that work done against a force field increases potential energy, while work done by the force field decreases potential energy. Common notations for potential energy are "PE", "U", "V", and "E".

Potential energy is the energy by virtue of an object's position relative to other objects. Potential energy is often associated with restoring forces such as a spring or the force of gravity. The action of stretching a spring or lifting a mass is performed by an external force that works against the force field of the potential. This work is stored in the force field, which is said to be stored as potential energy. If the external force is removed the force field acts on the body to perform the work as it moves the body back to the initial position, reducing the stretch of the spring or causing a body to fall.

Consider a ball whose mass is m and whose height is h. The acceleration g of free fall is approximately constant, so the weight force of the ball mg is constant. Force × displacement gives the work done, which is equal to the gravitational potential energy, thus

The more formal definition is that potential energy is the energy difference between the energy of an object in a given position and its energy at a reference position.

Potential energy is closely linked with forces. If the work done by a force on a body that moves from "A" to "B" does not depend on the path between these points (if the work is done by a conservative force), then the work of this force measured from "A" assigns a scalar value to every other point in space and defines a scalar potential field. In this case, the force can be defined as the negative of the vector gradient of the potential field.
If the work for an applied force is independent of the path, then the work done by the force is evaluated at the start and end of the trajectory of the point of application. This means that there is a function "U"(x), called a "potential," that can be evaluated at the two points x and x to obtain the work over any trajectory between these two points. It is tradition to define this function with a negative sign so that positive work is a reduction in the potential, that is 
where "C" is the trajectory taken from A to B. Because the work done is independent of the path taken, then this expression is true for any trajectory, "C", from A to B.

The function "U"(x) is called the potential energy associated with the applied force. Examples of forces that have potential energies are gravity and spring forces.

In this section the relationship between work and potential energy is presented in more detail. The line integral that defines work along curve "C" takes a special form if the force F is related to a scalar field φ(x) so that
In this case, work along the curve is given by
which can be evaluated using the gradient theorem to obtain
This shows that when forces are derivable from a scalar field, the work of those forces along a curve "C" is computed by evaluating the scalar field at the start point "A" and the end point "B" of the curve. This means the work integral does not depend on the path between "A" and "B" and is said to be independent of the path.

Potential energy "U"=-φ(x) is traditionally defined as the negative of this scalar field so that work by the force field decreases potential energy, that is

In this case, the application of the del operator to the work function yields,
and the force F is said to be "derivable from a potential." This also necessarily implies that F must be a conservative vector field. The potential "U" defines a force F at every point x in space, so the set of forces is called a force field.

Given a force field F(x), evaluation of the work integral using the gradient theorem can be used to find the scalar function associated with potential energy. This is done by introducing a parameterized curve γ(t)=r(t) from γ(a)=A to γ(b)=B, and computing,

For the force field F, let v= dr/dt, then the gradient theorem yields,

The power applied to a body by a force field is obtained from the gradient of the work, or potential, in the direction of the velocity v of the point of application, that is

Examples of work that can be computed from potential functions are gravity and spring forces.

For small height changes, gravitational potential energy can be computed using

where m is the mass in kg, g is the local gravitational field (9.8 metres per second squared on earth) and h is the height above a reference level in metres.

In classical physics, gravity exerts a constant downward force F=(0, 0, "F") on the center of mass of a body moving near the surface of the Earth. The work of gravity on a body moving along a trajectory r(t) = ("x"(t), "y"(t), "z"(t)), such as the track of a roller coaster is calculated using its velocity, v=("v", "v", "v"), to obtain
where the integral of the vertical component of velocity is the vertical distance. Notice that the work of gravity depends only on the vertical movement of the curve r(t).

A horizontal spring exerts a force F = (−"kx", 0, 0) that is proportional to its deformation in the axial or "x" direction. The work of this spring on a body moving along the space curve s("t") = ("x"("t"), "y"("t"), "z"("t")), is calculated using its velocity, v = ("v", "v", "v"), to obtain
For convenience, consider contact with the spring occurs at "t" = 0, then the integral of the product of the distance "x" and the "x"-velocity, "xv", is "x"/2.

The function 
is called the potential energy of a linear spring.

Elastic potential energy is the potential energy of an elastic object (for example a bow or a catapult) that is deformed under tension or compression (or stressed in formal terminology). It arises as a consequence of a force that tries to restore the object to its original shape, which is most often the electromagnetic force between the atoms and molecules that constitute the object. If the stretch is released, the energy is transformed into kinetic energy.

The gravitational potential function, also known as gravitational potential energy, is:

The negative sign follows the convention that work is gained from a loss of potential energy.

The gravitational force between two bodies of mass "M" and "m" separated by a distance "r" is given by

where formula_19 is a vector of length 1 pointing from "M" to "m" and "G" is the gravitational constant.

Let the mass "m" move at the velocity v then the work of gravity on this mass as it moves from position r(t) to r(t) is given by
Notice that the position and velocity of the mass "m" are given by

where e and e are the radial and tangential unit vectors directed relative to the vector from "M" to "m". Use this to simplify the formula for work of gravity to,

This calculation uses the fact that

The electrostatic force exerted by a charge "Q" on another charge "q" separated by a distance "r" is given by Coulomb's Law

where formula_19 is a vector of length 1 pointing from "Q" to "q" and "ε" is the vacuum permittivity. This may also be written using Coulomb's constant .

The work "W" required to move "q" from "A" to any point "B" in the electrostatic force field is given by the potential function

The potential energy is a function of the state a system is in, and is defined relative to that for a particular state. This reference state is not always a real state; it may also be a limit, such as with the distances between all bodies tending to infinity, provided that the energy involved in tending to that limit is finite, such as in the case of inverse-square law forces. Any arbitrary reference state could be used; therefore it can be chosen based on convenience.

Typically the potential energy of a system depends on the "relative" positions of its components only, so the reference state can also be expressed in terms of relative positions.

Gravitational energy is the potential energy associated with gravitational force, as work is required to elevate objects against Earth's gravity. The potential energy due to elevated positions is called gravitational potential energy, and is evidenced by water in an elevated reservoir or kept behind a dam. If an object falls from one point to another point inside a gravitational field, the force of gravity will do positive work on the object, and the gravitational potential energy will decrease by the same amount.

Consider a book placed on top of a table. As the book is raised from the floor, to the table, some external force works against the gravitational force. If the book falls back to the floor, the "falling" energy the book receives is provided by the gravitational force. Thus, if the book falls off the table, this potential energy goes to accelerate the mass of the book and is converted into kinetic energy. When the book hits the floor this kinetic energy is converted into heat, deformation and sound by the impact.

The factors that affect an object's gravitational potential energy are its height relative to some reference point, its mass, and the strength of the gravitational field it is in. Thus, a book lying on a table has less gravitational potential energy than the same book on top of a taller cupboard, and less gravitational potential energy than a heavier book lying on the same table. An object at a certain height above the Moon's surface has less gravitational potential energy than at the same height above the Earth's surface because the Moon's gravity is weaker. Note that "height" in the common sense of the term cannot be used for gravitational potential energy calculations when gravity is not assumed to be a constant. The following sections provide more detail.

The strength of a gravitational field varies with location. However, when the change of distance is small in relation to the distances from the center of the source of the gravitational field, this variation in field strength is negligible and we can assume that the force of gravity on a particular object is constant. Near the surface of the Earth, for example, we assume that the acceleration due to gravity is a constant ("standard gravity"). In this case, a simple expression for gravitational potential energy can be derived using the "W" = "Fd" equation for work, and the equation

The amount of gravitational potential energy possessed by an elevated object is equal to the work done against gravity in lifting it. The work done equals the force required to move it upward multiplied with the vertical distance it is moved (remember "W = Fd"). The upward force required while moving at a constant velocity is equal to the weight, "mg", of an object, so the work done in lifting it through a height "h" is the product "mgh". Thus, when accounting only for mass, gravity, and altitude, the equation is:
where "U" is the potential energy of the object relative to its being on the Earth's surface, "m" is the mass of the object, "g" is the acceleration due to gravity, and "h" is the altitude of the object. If "m" is expressed in kilograms, "g" in m/s and "h" in metres then "U" will be calculated in joules.

Hence, the potential difference is

However, over large variations in distance, the approximation that "g" is constant is no longer valid, and we have to use calculus and the general mathematical definition of work to determine gravitational potential energy. For the computation of the potential energy we can integrate the gravitational force, whose magnitude is given by Newton's law of gravitation, with respect to the distance "r" between the two bodies. Using that definition, the gravitational potential energy of a system of masses "m" and "M" at a distance "r" using gravitational constant "G" is

where "K" is an arbitrary constant dependent on the choice of datum from which potential is measured. Choosing the convention that "K"=0 (i.e. in relation to a point at infinity) makes calculations simpler, albeit at the cost of making "U" negative; for why this is physically reasonable, see below.

Given this formula for "U", the total potential energy of a system of "n" bodies is found by summing, for all formula_31 pairs of two bodies, the potential energy of the system of those two bodies.

Considering the system of bodies as the combined set of small particles the bodies consist of, and applying the previous on the particle level we get the negative gravitational binding energy. This potential energy is more strongly negative than the total potential energy of the system of bodies as such since it also includes the negative gravitational binding energy of each body. The potential energy of the system of bodies as such is the negative of the energy needed to separate the bodies from each other to infinity, while the gravitational binding energy is the energy needed to separate all particles from each other to infinity.
therefore,

As with all potential energies, only differences in gravitational potential energy matter for most physical purposes, and the choice of zero point is arbitrary. Given that there is no reasonable criterion for preferring one particular finite "r" over another, there seem to be only two reasonable choices for the distance at which "U" becomes zero: formula_34 and formula_35. The choice of formula_36 at infinity may seem peculiar, and the consequence that gravitational energy is always negative may seem counterintuitive, but this choice allows gravitational potential energy values to be finite, albeit negative.

The singularity at formula_34 in the formula for gravitational potential energy means that the only other apparently reasonable alternative choice of convention, with formula_36 for formula_34, would result in potential energy being positive, but infinitely large for all nonzero values of "r", and would make calculations involving sums or differences of potential energies beyond what is possible with the real number system. Since physicists abhor infinities in their calculations, and "r" is always non-zero in practice, the choice of formula_36 at infinity is by far the more preferable choice, even if the idea of negative energy in a gravity well appears to be peculiar at first.

The negative value for gravitational energy also has deeper implications that make it seem more reasonable in cosmological calculations where the total energy of the universe can meaningfully be considered; see inflation theory for more on this.

Gravitational potential energy has a number of practical uses, notably the generation of pumped-storage hydroelectricity. For example, in Dinorwig, Wales, there are two lakes, one at a higher elevation than the other. At times when surplus electricity is not required (and so is comparatively cheap), water is pumped up to the higher lake, thus converting the electrical energy (running the pump) to gravitational potential energy. At times of peak demand for electricity, the water flows back down through electrical generator turbines, converting the potential energy into kinetic energy and then back into electricity. The process is not completely efficient and some of the original energy from the surplus electricity is in fact lost to friction.

Gravitational potential energy is also used to power clocks in which falling weights operate the mechanism. It's also used by counterweights for lifting up an elevator, crane, or sash window.
Roller coasters are an entertaining way to utilize potential energy - chains are used to move a car up an incline (building up gravitational potential energy), to then have that energy converted into kinetic energy as it falls.

Another practical use is utilizing gravitational potential energy to descend (perhaps coast) downhill in transportation such as the descent of an automobile, truck, railroad train, bicycle, airplane, or fluid in a pipeline. In some cases the kinetic energy obtained from potential energy of descent may be used to start ascending the next grade such as what happens when a road is undulating and has frequent dips. The commercialization of stored energy (in the form of rail cars raised to higher elevations) that is then converted to electrical energy when needed by an electrical grid, is being undertaken in the United States in a system called Advanced Rail Energy Storage (ARES).

Chemical potential energy is a form of potential energy related to the structural arrangement of atoms or molecules. This arrangement may be the result of chemical bonds within a molecule or otherwise. Chemical energy of a chemical substance can be transformed to other forms of energy by a chemical reaction. As an example, when a fuel is burned the chemical energy is converted to heat, same is the case with digestion of food metabolized in a biological organism. Green plants transform solar energy to chemical energy through the process known as photosynthesis, and electrical energy can be converted to chemical energy through electrochemical reactions.

The similar term chemical potential is used to indicate the potential of a substance to undergo a change of configuration, be it in the form of a chemical reaction, spatial transport, particle exchange with a reservoir, etc.

An object can have potential energy by virtue of its electric charge and several forces related to their presence. There are two main types of this kind of potential energy: electrostatic potential energy, electrodynamic potential energy (also sometimes called magnetic potential energy).

Electrostatic potential energy between two bodies in space is obtained from the force exerted by a charge "Q" on another charge "q" which is given by

where formula_19 is a vector of length 1 pointing from "Q" to "q" and "ε" is the vacuum permittivity. This may also be written using Coulomb's constant .

If the electric charge of an object can be assumed to be at rest, then it has potential energy due to its position relative to other charged objects. The electrostatic potential energy is the energy of an electrically charged particle (at rest) in an electric field. It is defined as the work that must be done to move it from an infinite distance away to its present location, adjusted for non-electrical forces on the object. This energy will generally be non-zero if there is another electrically charged object nearby.

The work "W" required to move "q" from "A" to any point "B" in the electrostatic force field is given by
typically given in "J" for Joules. A related quantity called "electric potential" (commonly denoted with a "V" for voltage) is equal to the electric potential energy per unit charge.

The energy of a magnetic moment formula_44 in an externally produced magnetic B-field has potential energy

The magnetization in a field is

where the integral can be over all space or, equivalently, where is nonzero.
Magnetic potential energy is the form of energy related not only to the distance between magnetic materials, but also to the orientation, or alignment, of those materials within the field. For example, the needle of a compass has the lowest magnetic potential energy when it is aligned with the north and south poles of the Earth's magnetic field. If the needle is moved by an outside force, torque is exerted on the magnetic dipole of the needle by the Earth's magnetic field, causing it to move back into alignment. The magnetic potential energy of the needle is highest when its field is in the same direction as the Earth's magnetic field. Two magnets will have potential energy in relation to each other and the distance between them, but this also depends on their orientation. If the opposite poles are held apart, the potential energy will be higher the further they are apart and lower the closer they are. Conversely, like poles will have the highest potential energy when forced together, and the lowest when they spring apart.

Nuclear potential energy is the potential energy of the particles inside an atomic nucleus. The nuclear particles are bound together by the strong nuclear force. Weak nuclear forces provide the potential energy for certain kinds of radioactive decay, such as beta decay.

Nuclear particles like protons and neutrons are not destroyed in fission and fusion processes, but collections of them can have less mass than if they were individually free, in which case this mass difference can be liberated as heat and radiation in nuclear reactions (the heat and radiation have the missing mass, but it often escapes from the system, where it is not measured). The energy from the Sun is an example of this form of energy conversion. In the Sun, the process of hydrogen fusion converts about 4 million tonnes of solar matter per second into electromagnetic energy, which is radiated into space.

Potential energy is closely linked with forces. If the work done by a force on a body that moves from "A" to "B" does not depend on the path between these points, then the work of this force measured from "A" assigns a scalar value to every other point in space and defines a scalar potential field. In this case, the force can be defined as the negative of the vector gradient of the potential field.

For example, gravity is a conservative force. The associated potential is the gravitational potential, often denoted by formula_47 or formula_48, corresponding to the energy per unit mass as a function of position. The gravitational potential energy of two particles of mass "M" and "m" separated by a distance "r" is
The gravitational potential (specific energy) of the two bodies is
where formula_51 is the reduced mass.

The work done against gravity by moving an infinitesimal mass from point A with formula_52 to point B with formula_53 is formula_54 and the work done going back the other way is formula_55 so that the total work done in moving from A to B and returning to A is
If the potential is redefined at A to be formula_57 and the potential at B to be formula_58, where formula_59 is a constant (i.e. formula_59 can be any number, positive or negative, but it must be the same at A as it is at B) then the work done going from A to B is
as before.

In practical terms, this means that one can set the zero of formula_62 and formula_47 anywhere one likes. One may set it to be zero at the surface of the Earth, or may find it more convenient to set zero at infinity (as in the expressions given earlier in this section).
A conservative force can be expressed in the language of differential geometry as a closed form. As Euclidean space is contractible, its de Rham cohomology vanishes, so every closed form is also an exact form, and can be expressed as the gradient of a scalar field. This gives a mathematical justification of the fact that all conservative forces are gradients of a potential field.




</doc>
<doc id="23704" url="https://en.wikipedia.org/wiki?curid=23704" title="Pyramid">
Pyramid

A pyramid (from "") is a structure whose outer surfaces are triangular and converge to a single point at the top, making the shape roughly a pyramid in the geometric sense. The base of a pyramid can be trilateral, quadrilateral, or of any polygon shape. As such, a pyramid has at least three outer triangular surfaces (at least four faces including the base). The square pyramid, with a square base and four triangular outer surfaces, is a common version.

A pyramid's design, with the majority of the weight closer to the ground, and with the pyramidion on top, means that less material higher up on the pyramid will be pushing down from above. This distribution of weight allowed early civilizations to create stable monumental structures.

Civilizations in many parts of the world have built pyramids. The largest pyramid by volume is the Great Pyramid of Cholula, in the Mexican state of Puebla. For thousands of years, the largest structures on Earth were pyramids—first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt - the latter is the only one of the Seven Wonders of the Ancient World still remaining.

Khufu's Pyramid, built mainly of limestone (with large red granite blocks used in some interior chambers), is considered an architectural masterpiece. It contains over 2,000,000 blocks ranging in weight from to and is built on a square base with sides measuring about 230 m (755 ft), covering 13 acres. Its four sides face the four cardinal points precisely and it has an angle of 52 degrees. The original height of the pyramid was 146.5 m (488 ft), but today it is only 137 m (455 ft) high as 9 m (33 ft) has gone missing due to theft of the fine quality white Tura limestone covering, or casing stones, for construction in Cairo. It remains the tallest pyramid.

The Mesopotamians built the earliest pyramidal structures, called "ziggurats". In ancient times, these were brightly painted in gold/bronze. Since they were constructed of sun-dried mud-brick, little remains of them. Ziggurats were built by the Sumerians, Babylonians, Elamites, Akkadians, and Assyrians for local religions. Each ziggurat was part of a temple complex which included other buildings. The precursors of the ziggurat were raised platforms that date from the Ubaid period during the fourth millennium BC. The earliest ziggurats began near the end of the Early Dynastic Period. The latest Mesopotamian ziggurats date from the 6th century BC.

Built in receding tiers upon a rectangular, oval, or square platform, the ziggurat was a pyramidal structure with a flat top. Sun-baked bricks made up the core of the ziggurat with facings of fired bricks on the outside. The facings were often glazed in different colors and may have had astrological significance. Kings sometimes had their names engraved on these glazed bricks. The number of tiers ranged from two to seven. It is assumed that they had shrines at the top, but there is no archaeological evidence for this and the only textual evidence is from Herodotus. Access to the shrine would have been by a series of ramps on one side of the ziggurat or by a spiral ramp from base to summit.

The most famous pyramids are the Egyptian — huge structures built of brick or stone, some of which are among the world's largest constructions. They are shaped as a reference to the rays of the sun. Most pyramids had a polished, highly reflective white limestone surface, to give them a shining appearance when viewed from a distance. The capstone was usually made of hard stone – granite or basalt – and could be plated with gold, silver, or electrum and would also be highly reflective. After 2700 BC, the ancient Egyptians began building pyramids, until around 1700 BC. The first pyramid was erected during the Third Dynasty by the Pharaoh Djoser and his architect Imhotep. This step pyramid consisted of six stacked mastabas. The largest Egyptian pyramids are those at the Giza pyramid complex. The Egyptian sun god Ra, considered the father of all pharaohs, was said to have created himself from a pyramid-shaped mound of earth before creating all other gods.

The age of the pyramids reached its zenith at Giza in 2575–2150 BC. Ancient Egyptian pyramids were in most cases placed west of the river Nile because the divine pharaoh's soul was meant to join with the sun during its descent before continuing with the sun in its eternal round. As of 2008, some 135 pyramids have been discovered in Egypt. The Great Pyramid of Giza is the largest in Egypt and one of the largest in the world. It was the tallest building in the world until Lincoln Cathedral was finished in 1311 AD. The base is over in area. While pyramids are associated with Egypt, the nation of Sudan has 220 extant pyramids, the most numerous in the world. The Great Pyramid of Giza is one of the Seven Wonders of the Ancient World. It is the only one to survive into modern times. The Ancient Egyptians covered the faces of pyramids with polished white limestone, containing great quantities of fossilized seashells. Many of the facing stones have fallen or have been removed and used for construction in Cairo.
Most pyramids are located near Cairo, with only one royal pyramid being located south of Cairo, at the Abydos temple complex. The pyramid at Abydos, Egypt were commissioned by Ahmose I who founded the 18th Dynasty and the New Kingdom. The building of pyramids began in the Third Dynasty with the reign of King Djoser. Early kings such as Snefru built several pyramids, with subsequent kings adding to the number of pyramids until the end of the Middle Kingdom.

The last king to build royal pyramids was Ahmose, with later kings hiding their tombs in the hills, such as those in the Valley of the Kings in Luxor's West Bank. In Medinat Habu, or Deir el-Medina, smaller pyramids were built by individuals. Smaller pyramids were also built by the Nubians who ruled Egypt in the Late Period, though their pyramids had steeper sides.

Nubian pyramids were constructed (roughly 240 of them) at three sites in Sudan to serve as tombs for the kings and queens of Napata and Meroë. The pyramids of Kush, also known as Nubian Pyramids, have different characteristics than the pyramids of Egypt. The Nubian pyramids were constructed at a steeper angle than Egyptian ones. Pyramids were still being built in Sudan as late as 200 AD.

One of the unique structures of Igbo culture was the Nsude Pyramids, at the Nigerian town of Nsude, northern Igboland. Ten pyramidal structures were built of clay/mud. The first base section was 60 ft. in circumference and 3 ft. in height. The next stack was 45 ft. in circumference. Circular stacks continued, till it reached the top. The structures were temples for the god Ala/Uto, who was believed to reside at the top. A stick was placed at the top to represent the god's residence. The structures were laid in groups of five parallel to each other. Because it was built of clay/mud like the Deffufa of Nubia, time has taken its toll requiring periodic reconstruction.

Pausanias (2nd century AD) mentions two buildings resembling pyramids, one, 19 kilometres (12 mi) southwest of the still standing structure at Hellenikon, a common tomb for soldiers who died in a legendary struggle for the throne of Argos and another which he was told was the tomb of Argives killed in a battle around 669/8 BC. Neither of these still survive and there is no evidence that they resembled Egyptian pyramids.
There are also at least two surviving pyramid-like structures still available to study, one at Hellenikon and the other at Ligourio/Ligurio, a village near the ancient theatre Epidaurus. These buildings were not constructed in the same manner as the pyramids in Egypt. They do have inwardly sloping walls but other than those there is no obvious resemblance to Egyptian pyramids. They had large central rooms (unlike Egyptian pyramids) and the Hellenikon structure is rectangular rather than square, which means that the sides could not have met at a point. The stone used to build these structures was limestone quarried locally and was cut to fit, not into freestanding blocks like the Great Pyramid of Giza.

The dating of these structures has been made from the pot shards excavated from the floor and on the grounds. The latest dates available from scientific dating have been estimated around the 5th and 4th centuries. Normally this technique is used for dating pottery, but here researchers have used it to try to date stone flakes from the walls of the structures. This has created some debate about whether or not these structures are actually older than Egypt, which is part of the Black Athena controversy.

Mary Lefkowitz has criticised this research. She suggests that some of the research was done not to determine the reliability of the dating method, as was suggested, but to back up an assumption of age and to make certain points about pyramids and Greek civilization. She notes that not only are the results not very precise, but that other structures mentioned in the research are not in fact pyramids, e.g. a tomb alleged to be the tomb of Amphion and Zethus near Thebes, a structure at Stylidha (Thessaly) which is just a long wall, etc. She also notes the possibility that the stones that were dated might have been recycled from earlier constructions. She also notes that earlier research from the 1930s, confirmed in the 1980s by Fracchia was ignored. She argues that they undertook their research using a novel and previously untested methodology in order to confirm a predetermined theory about the age of these structures.

Liritzis responded in a journal article published in 2011, stating that Lefkowitz failed to understand and misinterpreted the methodology.

The Pyramids of Güímar refer to six rectangular pyramid-shaped, terraced structures, built from lava stone without the use of mortar. They are located in the district of Chacona, part of the town of Güímar on the island of Tenerife in the Canary Islands. The structures have been dated to the 19th century and their original function explained as a byproduct of contemporary agricultural techniques.

Autochthonous Guanche traditions as well as surviving images indicate that similar structures (also known as, "Morras", "Majanos", "Molleros", or "Paredones") could once have been found in many locations on the island. However, over time they have been dismantled and used as a cheap building material. In Güímar itself there were nine pyramids, only six of which survive.

There are many square flat-topped mound tombs in China. The First Emperor Qin Shi Huang (circa 221 BC, who unified the 7 pre-Imperial Kingdoms) was buried under a large mound outside modern day Xi'an. In the following centuries about a dozen more Han Dynasty royals were also buried under flat-topped pyramidal earthworks.

A number of Mesoamerican cultures also built pyramid-shaped structures. Mesoamerican pyramids were usually stepped, with temples on top, more similar to the Mesopotamian ziggurat than the Egyptian pyramid.

The largest pyramid by volume is the Great Pyramid of Cholula, in the Mexican state of Puebla. Constructed from the 3rd century BC to the 9th century AD, this pyramid is considered the largest monument ever constructed anywhere in the world, and is still being excavated. The third largest pyramid in the world, the Pyramid of the Sun, at Teotihuacan is also located in Mexico. There is an unusual pyramid with a circular plan at the site of Cuicuilco, now inside Mexico City and mostly covered with lava from an eruption of the Xitle Volcano in the 1st century BC. There are several circular stepped pyramids called Guachimontones in Teuchitlán, Jalisco as well.

Pyramids in Mexico were often used as places of human sacrifice. For the re-consecration of Great Pyramid of Tenochtitlan in 1487, Where, according to Michael Harner, "one source states 20,000, another 72,344, and several give 80,400".

Many pre-Columbian Native American societies of ancient North America built large pyramidal earth structures known as platform mounds. Among the largest and best-known of these structures is Monks Mound at the site of Cahokia in what became Illinois, completed around 1100 AD, which has a base larger than that of the Great Pyramid at Giza. Many of the mounds underwent multiple episodes of mound construction at periodic intervals, some becoming quite large. They are believed to have played a central role in the mound-building peoples' religious life and documented uses include semi-public chief's house platforms, public temple platforms, mortuary platforms, charnel house platforms, earth lodge/town house platforms, residence platforms, square ground and rotunda platforms, and dance platforms. Cultures who built substructure mounds include the Troyville culture, Coles Creek culture, Plaquemine culture and Mississippian cultures.

The 27-metre-high Pyramid of Cestius was built by the end of the 1st century BC and still exists today, close to the Porta San Paolo. Another one, named "Meta Romuli", standing in the "Ager Vaticanus" (today's Borgo), was destroyed at the end of the 15th century.

Pyramids have occasionally been used in Christian architecture of the feudal era, e.g. as the tower of Oviedo's Gothic Cathedral of San Salvador.

Many giant granite temple pyramids were made in South India during the Chola Empire, many of which are still in religious use today. Examples of such pyramid temples include Brihadisvara Temple at Thanjavur, the Temple of Gangaikondacholapuram and the Airavatesvara Temple at Darasuram. However the largest temple pyramid in the area is Sri Rangam in Srirangam, Tamil Nadu. The Thanjavur temple was built by Raja raja Chola in the 11th century. The Brihadisvara Temple was declared by UNESCO as a World Heritage Site in 1987; the Temple of Gangaikondacholapuram and the Airavatesvara Temple at Darasuram were added as extensions to the site in 2004.

Next to menhir, stone table, and stone statue; Austronesian megalithic culture in Indonesia also featured earth and stone step pyramid structures called "punden berundak" as discovered in Pangguyangan site near Cisolok and in Cipari near Kuningan. The construction of stone pyramids is based on the native beliefs that mountains and high places are the abode for the spirit of the ancestors.

The step pyramid is the basic design of 8th century Borobudur Buddhist monument in Central Java. However the later temples built in Java were influenced by Indian Hindu architecture, as displayed by the towering spires of Prambanan temple. In the 15th century Java during late Majapahit period saw the revival of Austronesian indigenous elements as displayed by Sukuh temple that somewhat resemble Mesoamerican pyramid, and also stepped pyramids of Mount Penanggungan.

Andean cultures had used pyramids in various architectural structures such as the ones in Caral, Túcume and Chavín de Huantar.




</doc>
<doc id="23705" url="https://en.wikipedia.org/wiki?curid=23705" title="Predestination">
Predestination

Predestination, in theology, is the doctrine that all events have been willed by God, usually with reference to the eventual fate of the individual soul. Explanations of predestination often seek to address the "paradox of free will", whereby God's omniscience seems incompatible with human free will. In this usage, predestination can be regarded as a form of religious determinism; and usually predeterminism.

There is some disagreement among scholars regarding the views on predestination of first-century AD Judaism, out of which Christianity came. Josephus wrote during the first century that the three main Jewish sects differed on this question. He argued that the Essenes and Pharisees argued that God's providence orders all human events, but the Pharisees still maintained that people are able to choose between right and wrong. He wrote that the Sadducees did not have a doctrine of providence.
The biblical scholar N. T. Wright argues that Josephus's portrayal of these groups is incorrect, and that the Jewish debates referenced by Josephus should be seen as having to do with God's work to liberate Israel rather than philosophical questions about predestination. Wright asserts that Essenes were content to wait for God to liberate Israel while Pharisees believed Jews needed to act in cooperation with God. John Barclay responded that Josephus's description was an over-simplification and there were likely to be complex differences between these groups which may have been similar to those described by Josephus. Francis Watson has also argued on the basis of 4 Ezra, a document dated to the first century AD, that Jewish beliefs in predestination are primarily concerned with God's choice to save some individual Jews.

In the New Testament, Romans 8–11 presents a statement on predestination. In Romans 8:28–30, Paul writes, 
Biblical scholars have interpreted this passage in several ways. Many say this only has to do with service, those he chose of service and is not about salvation. The Catholic biblical commentator Brendan Byrne wrote that the predestination mentioned in this passage should be interpreted as applied to the Christian community corporately rather than individuals. Another Catholic commentator, Joseph Fitzmyer, wrote that this passage teaches that God has predestined the salvation of all humans. Douglas Moo, a Protestant biblical interpreter, reads the passage as teaching that God has predestined a certain set of people to salvation. Similarly, Wright's interpretation is that in this passage Paul teaches that God will save those whom he has chosen, but Wright also emphasizes that Paul does not intend to suggest that God has eliminated human free will or responsibility. Instead, Wright asserts that God's will works through that of humans to accomplish salvation.

Origen, writing in the third century, taught that God's providence extends to every individual. He believed God's predestination was based on God's foreknowledge of every individual's merits, whether in their current life or a previous life.

Later in the fourth and fifth centuries, Augustine of Hippo (354–430) also taught that God orders all things while preserving human freedom. Prior to 396, Augustine believed that predestination was based on God's foreknowledge of whether individuals would believe, that God's grace was "a reward for human assent". Later, in response to Pelagius, Augustine said that the sin of pride consists in assuming that "we are the ones who choose God or that God chooses us (in his foreknowledge) because of something worthy in us", and argued that God's grace causes individual act of faith. Scholars are divided over whether Augustine's teaching implies double predestination, or the belief that God chooses some people for damnation as well as some for salvation. Catholic scholars tend to deny that he held such a view while some Protestants and secular scholars have held that Augustine did believe in double predestination.

Augustine's position raised objections. Julian of Eclanum expressed the view that Augustine was bringing Manichean thoughts into the church. For Vincent of Lérins, this was a disturbing innovation. This new tension eventually became obvious with the confrontation between Augustine and Pelagius culminating in condemnation of Pelagianism (as interpreted by Augustine) at the Council of Ephesus in 431. Pelagius denied Augustine's view of predestination in order to affirm that salvation is achieved by an act of free will.

The Council of Arles in the late fifth century condemned the position "that some have been condemned to death, others have been predestined to life", though this may seem to follow from Augustine's teaching. The Second Council of Orange in 529 also condemned the position that "some have been truly predestined to evil by divine power".

In the eighth century, John of Damascus emphasized the freedom of the human will in his doctrine of predestination, and argued that acts arising from peoples' wills are not part of God's providence at all. Damascene teaches that people's good actions are done in cooperation with God, but are not caused by him.

Gottschalk of Orbais, a ninth-century Saxon monk, argued that God predestines some people to hell as well as predestining some to heaven, a view known as double predestination. He was condemned by several synods, but his views remained popular. Irish theologian John Scottus Eriugena wrote a refutation of Gottschalk. Eriugena abandoned Augustine's teaching on predestination. He wrote that God's predestination should be equated with his foreknowledge of people's choices.

In the twelfth century, Thomas Aquinas taught that God predestines certain people to the beatific vision based solely on his own goodness rather than that of creatures. Aquinas also believed people are free in their choices, fully cause their own sin, and are solely responsible for it. Aquinas distinguished between several ways in which God wills actions. He directly wills the good, indirectly wills evil consequences of good things, and only permits evil. Aquinas held that in permitting evil, God does not will it to be done or not to be done.

In the thirteenth century, William of Ockham taught that God does not cause human choices and equated predestination with divine foreknowledge. Though Ockham taught that God predestines based on people's foreseen works, he maintained that God's will was not constrained to do this.

John Calvin rejected the idea that God permits rather than actively decrees the damnation of sinners, as well as other evil. Calvin did not believe God to be guilty of sin, but he considered it an unfathomable mystery that God seems to simultaneously will sin and to also not will sin. Though he maintained God's predestination applies to damnation as well as salvation, he taught that the damnation of the damned is caused by their sin, but that the salvation of the saved is solely caused by God. Other Protestant Reformers, including Martin Luther and Huldrych Zwingli, also held double predestinarian views.

The Eastern Orthodox view was summarized by Bishop Theophan the Recluse in response to the question, "What is the relationship between the Divine provision and our free will?"

Catholicism teaches the doctrine of predestination, while rejecting the classical Calvinist view known as "double predestination". This means that while it is held that those whom God has elected to eternal life will infallibly attain it, and are therefore said to be predestined to salvation by God, those who perish are not predestined to damnation. But Catholicism has been generally discouraging to human attempts to guess or predict the Divine Will. The "Catholic Encyclopedia" entry on predestination says:

Pope John Paul II wrote:
The Catholic Catechism says, "To God, all moments of time are present in their immediacy. When therefore he establishes his eternal plan of "predestination", he includes in it each person's free response to his grace." According to the Catholic Church, God predestines no one to go to hell, for this, a willful turning away from God (a mortal sin) is necessary, and persistence in it until the end."

Catholics do not believe that any hints or evidence of the predestined status of individuals is available to humans, and predestination generally plays little or no part in Catholic teaching to the faithful, being a topic addressed in a professional theological context only.

Augustine of Hippo laid the foundation for much of the later Catholic teaching on predestination. His teachings on grace and free will were largely adopted by the Second Council of Orange (529), whose decrees were directed against the Semipelagians. Augustine wrote, 
Augustine also teaches that people have free will. For example, in "On Grace and Free Will", (see especially chapters II–IV) Augustine states that "He [God] has revealed to us, through His Holy Scriptures, that there is in man a free choice of will," and that "God's precepts themselves would be of no use to a man unless he had free choice of will, so that by performing them he might obtain the promised rewards." (chap. II)

Thomas Aquinas' views concerning predestination are largely in agreement with Augustine and can be summarized by many of his writings in his "Summa Theologiæ":

This table summarizes the classical views of three different Protestant beliefs.

Lutherans do not believe that there are certain people that are predestined to salvation, but salvation is predestined for those who seek God. Lutherans believe Christians should be assured that they are among the predestined. However, they disagree with those who make predestination the source of salvation rather than Christ's suffering, death, and resurrection. Unlike some Calvinists, Lutherans do not believe in a predestination to damnation. Instead, Lutherans teach eternal damnation is a result of the unbeliever's sins, rejection of the forgiveness of sins, and unbelief.

Martin Luther's attitude towards predestination is set out in his "On the Bondage of the Will", published in 1525. This publication by Luther was in response to the published treatise by Desiderius Erasmus in 1524 known as "On Free Will". Luther based his views on Ephesians 2:8–10, which says:

The Belgic Confession of 1561 affirmed that God "delivers and preserves" from perdition "all whom he, in his eternal and unchangeable council, of mere goodness hath elected in Christ Jesus our Lord, without respect to their works" (Article XVI).
Calvinists believe that God picked those who he will save and bring with him to Heaven before the world was created. They also believe that those people God does not save will go to Hell. John Calvin thought people who were saved could never lose their salvation and the "elect" (those God saved) would know they were saved because of their actions.

In this common, loose sense of the term, to affirm or to deny predestination has particular reference to the Calvinist doctrine of unconditional election. In the Calvinist interpretation of the Bible, this doctrine normally has only pastoral value related to the assurance of salvation and the absolution of salvation by grace alone. However, the philosophical implications of the doctrine of election and predestination are sometimes discussed beyond these systematic bounds. Under the topic of the doctrine of God (theology proper), the predestinating decision of God cannot be contingent upon anything outside of himself, because all other things are dependent upon him for existence and meaning. Under the topic of the doctrines of salvation (soteriology), the predestinating decision of God is made from God's knowledge of his own will (Romans 9:15), and is therefore not contingent upon human decisions (rather, free human decisions are outworkings of the decision of God, which sets the total reality within which those decisions are made in exhaustive detail: that is, nothing left to chance). Calvinists do not pretend to understand how this works; but they are insistent that the Scriptures teach both the sovereign control of God and the responsibility and freedom of human decisions.

Calvinist groups use the term Hyper-Calvinism to describe Calvinistic systems that assert without qualification that God's intention to destroy some is equal to his intention to save others. Some forms of Hyper-Calvinism have racial implications, as when Dutch Calvinist theologian Franciscus Gomarus however argued that Jews, because of their refusal to worship Jesus Christ, were members of the non-elect.Some Dutch settlers in South Africa argued that black people were sons of Ham, whom Noah had cursed to be slaves, according to Genesis 9:18-19, or drew analogies between them and the Canaanites, suggesting a "chosen people" ideology similar to that espoused by proponents of the Jewish nation. This justified racial hierarchy on earth, as well as racial segregation of congregations, but did not exclude blacks from being part of the elect. Other Calvinists vigorously objected to these arguments (see Afrikaner Calvinism). 

Expressed sympathetically, the Calvinist doctrine is that God has mercy or withholds it, with particular consciousness of who are to be the recipients of mercy in Christ. Therefore, the particular persons are chosen, out of the total number of human beings, who will be rescued from enslavement to sin and the fear of death, and from punishment due to sin, to dwell forever in his presence. Those who are being saved are assured through the gifts of faith, the sacraments, and communion with God through prayer and increase of good works, that their reconciliation with him through Christ is settled by the sovereign determination of God's will. God also has particular consciousness of those who are passed over by his selection, who are without excuse for their rebellion against him, and will be judged for their sins.

Calvinists typically divide on the issue of predestination into infralapsarians (sometimes called 'sublapsarians') and supralapsarians. Infralapsarians interpret the biblical election of God to highlight his love (1 John 4:8; Ephesians 1:4b-5a) and chose his elect considering the situation after the Fall, while supralapsarians interpret biblical election to highlight God's sovereignty (Romans 9:16) and that the Fall was ordained by God's decree of election. In infralapsarianism, election is God's response to the Fall, while in supralapsarianism the Fall is part of God's plan for election. In spite of the division, many Calvinist theologians would consider the debate surrounding the infra- and supralapsarian positions one in which scant Scriptural evidence can be mustered in either direction, and that, at any rate, has little effect on the overall doctrine.

Some Calvinists decline from describing the eternal decree of God in terms of a sequence of events or thoughts, and many caution against the simplifications involved in describing any action of God in speculative terms. Most make distinctions between the positive manner in which God chooses some to be recipients of grace, and the manner in which grace is consciously withheld so that some are destined for everlasting punishments.

Debate concerning predestination according to the common usage, concerns the destiny of the damned, whether God is just if that destiny is settled prior to the existence of any actual volition of the individual, and whether the individual is in any meaningful sense responsible for his destiny if it is settled by the eternal action of God.

Arminians hold that God does not predetermine, but instead infallibly knows who will believe and perseveringly be saved. This view is known as conditional election, because it states that election is conditional on the one who wills to have faith in God for salvation. Although God knows from the beginning of the world who will go where, the choice is still with the individual. The Dutch Calvinist theologian Franciscus Gomarus strongly opposed the views of Jacobus Arminius with his doctrine of supralapsarian predestination.

The Church of Jesus Christ of Latter-day Saints (LDS Church) rejects the doctrine of predestination, but does believe in foreordination. Foreordination, an important doctrine of the LDS Church, teaches that during the pre-mortal existence, God selected ("foreordained") particular people to fulfill certain missions ("callings") during their mortal lives. For example, prophets were foreordained to be the Lord's servants (see Jeremiah 1:5), all who receive the priesthood were foreordained to that calling, and Jesus was foreordained to enact the atonement.

The LDS Church teaches the doctrine of moral agency, the ability to choose and act for ourselves, and decide whether to accept Christ's atonement.

Conditional election is the belief that God chooses for eternal salvation those whom he foresees will have faith in Christ. This belief emphasizes the importance of a person's free will. The counter-view is known as unconditional election, and is the belief that God chooses whomever he will, based solely on his purposes and apart from an individual's free will. It has long been an issue in Calvinist–Arminian debate. An alternative viewpoint is Corporate election, which distinguishes God's election and predestination for corporate entities such as the community "in Christ," and individuals who can benefit from that community's election and predestination so long as they continue belonging to that community.

Infralapsarianism (also called sublapsarianism) holds that predestination logically coincides with the preordination of Man's fall into sin. That is, God predestined sinful men for salvation. Therefore, according to this view, God is the ultimate cause, but not the proximate source or "author" of sin. Infralapsarians often emphasize a difference between God's decree (which is inviolable and inscrutable), and his revealed will (against which man is disobedient). Proponents also typically emphasize the grace and mercy of God toward all men, although teaching also that only some are predestined for salvation.

In common English parlance, the doctrine of predestination often has particular reference to the doctrines of Calvinism. The version of predestination espoused by John Calvin, after whom Calvinism is named, is sometimes referred to as "double predestination" because in it God predestines some people for salvation (i.e. unconditional election) and some for condemnation (i.e. Reprobation) which results by allowing the individual's own sins to condemn them. Calvin himself defines predestination as "the eternal decree of God, by which he determined with himself whatever he wished to happen with regard to every man. Not all are created on equal terms, but some are preordained to eternal life, others to eternal damnation; and, accordingly, as each has been created for one or other of these ends, we say that he has been predestined to life or to death."

On the spectrum of beliefs concerning predestination, Calvinism is the strongest form among Christians. It teaches that God's predestining decision is based on the knowledge of his own will rather than foreknowledge, concerning every particular person and event; and, God continually acts with entire freedom, in order to bring about his will in completeness, but in such a way that the freedom of the creature is not violated, "but rather, established".

Calvinists who hold the infralapsarian view of predestination usually prefer that term to "sublapsarianism," perhaps with the intent of blocking the inference that they believe predestination is on the basis of foreknowledge ("sublapsarian" meaning, assuming the fall into sin). The different terminology has the benefit of distinguishing the Calvinist double predestination version of infralapsarianism from Lutheranism's view that predestination is a mystery, which forbids the unprofitable intrusion of prying minds since God only reveals partial knowledge to the human race.

Supralapsarianism is the doctrine that God's decree of predestination for salvation and reprobation logically precedes his preordination of the human race's fall into sin. That is, God decided to save, and to damn; he then determined the means by which that would be made possible. It is a matter of controversy whether or not Calvin himself held this view, but most scholars link him with the infralapsarian position. It is known, however, that Calvin's successor in Geneva, Theodore Beza, held to the supralapsarian view.

Double predestination, or the double decree, is the doctrine that God actively reprobates, or decrees damnation of some, as well as salvation for those whom he has elected. Augustine made statements that on their own seem to teach such a doctrine, but in the context of his other writings it is not clear whether he held it. Augustine's doctrine of predestination does seem to imply a double predestinarian view. Gottschalk of Orbais taught it more explicitly in the ninth century, and Gregory of Rimini in the fourteenth. During the Protestant Reformation John Calvin also held double predestinarian views. John Calvin states: "By predestination we mean the eternal decree of God, by which he determined with himself whatever he wished to happen with regard to every man. All are not created on equal terms, but some are preordained to eternal life, others to eternal damnation; and, accordingly, as each has been created for one or other of these ends, we say that he has been predestinated to life or to death."

Open theism advocates the non-traditional Arminian view of election that predestination is corporate. In corporate election, God does not choose which individuals he will save prior to creation, but rather God chooses the church as a whole. Or put differently, God chooses what type of individuals he will save. Another way the New Testament puts this is to say that God chose the church in Christ (Eph. 1:4). In other words, God chose from all eternity to save all those who would be found in Christ, by faith in God. This choosing is not primarily about salvation from eternal destruction either but is about God's chosen agency in the world. Thus individuals have full freedom in terms of whether they become members of the church or not. Corporate election is thus consistent with the open view's position on God's omniscience, which states that God's foreknowledge does not determine the outcomes of individual free will.

"Qadar" (, transliterated "qadar", meaning "fate", "divine fore-ordainment", "predestination") is the concept of divine destiny in Islam. It is one of Islam's six pillars of faith, along with belief in the Oneness of God, the Revealed Books, the Prophets of Islam, the Day of Resurrection and Angels.

In Islam, "predestination" is the usual English language rendering of a belief that Muslims call "al-qada wa al-qadar" in Arabic. The phrase means "the divine decree and the predestination". In Islam, God has predetermined, known, ordained, and is constantly creating every event that takes place in the world. This is entailed by his being omnipotent and omniscient. Sunni scholars hold that there is no contradiction in people's deeds (and naturally their choices) being created and predetermined by the creator, since they define free will to be the antonym of compulsion and coercion. People – in the Sunni perspective – do acknowledge that they are free, since they do not see anybody or anything forcing them to do whatever they chose to do. This, however, does not contradict that everything they do, including the choices they make, are predestined and predetermined by God. Consequently, people are already predestined to either heaven or hell at birth, as Sunnis believe; however, they will have no argument on the day of judgment since they never knew in advance what their fate would be, and they do acknowledge that they have choice; which is what moral responsibility comes with.

The concept of human will being predetermined by God's will is stated clearly in the Quran:
"Verily this (The Holy Quran) is no less than a Message to (all) the Worlds; (With profit) to whoever among you wills to go straight, "but ye shall not will except as God wills;" the Cherisher of the Worlds."

Predestination is rejected in Shiaism.

In Rabbinic literature, there is much discussion as to the apparent contradiction between God's omniscience and free will. The representative view is that "Everything is foreseen; yet free will is given" (Rabbi Akiva, "Pirkei Avoth" 3:15). Based on this understanding, the problem is formally described as a paradox, perhaps beyond our understanding.

Hasdai Crescas resolved this dialectical tension by taking the position that free will doesn't exist. All of a person's actions are predetermined by the moment of their birth, and thus their judgment in the eyes of God (so to speak) is effectively preordained. In this scheme this is not a result of God's predetermining one's fate, but rather that the universe is deterministic. Crescas's views on this topic were rejected by Judaism at large. In later centuries this idea independently developed among some in the Chabad (Lubavitch) movement of Hasidic Judaism. Many individuals within Chabad take this view seriously, and hence effectively deny the existence of free will.

However, many Chabad (Lubavitch) Jews attempt to hold both views. They affirm as infallible their rebbe's teachings that God knows and controls the fate of all, yet at the same time affirm the classical Jewish belief in free will. The inherent contradiction between the two results in their belief that such contradictions are only "apparent", due to man's inherent lack of ability to understand greater truths and due to the fact that Creator and Created exist in different realities. The same idea is strongly repeated by Rambam (Mishneh Torah, Laws of Repentance, Chapter 5).

Many other Jews (Orthodox, Conservative, Reform and secular) affirm that since free will exists, then by definition one's fate is not preordained. It is held as a tenet of faith that whether God is omniscient or not, nothing interferes with mankind's free will. Some Jewish theologians, both during the medieval era and today, have attempted to formulate a philosophy in which free will is preserved, while also affirming that God has knowledge of what decisions people will make in the future. Whether or not these two ideas are mutually compatible, or whether there is a contradiction between the two, is still a matter of great study and interest in philosophy today.

Predestination is rejected in Zoroastrian teaching. Humans bear responsibility for all situations they are in, and in the way they act toward one another. Reward, punishment, happiness, and grief all depend on how individuals live their lives.






</doc>
<doc id="23706" url="https://en.wikipedia.org/wiki?curid=23706" title="Primitive notion">
Primitive notion

In mathematics, logic, and formal systems, a primitive notion is an undefined concept. In particular, a primitive notion is not defined in terms of previously defined concepts, but is only motivated informally, usually by an appeal to intuition and everyday experience. In an axiomatic theory or other formal system, the role of a primitive notion is analogous to that of axiom. In axiomatic theories, the primitive notions are sometimes said to be "defined" by one or more axioms, but this can be misleading. Formal theories cannot dispense with primitive notions, under pain of infinite regress.

Alfred Tarski explained the role of primitive notions as follows:

An inevitable regress to primitive notions in the theory of knowledge was explained by Gilbert de B. Robinson:

The necessity for primitive notions is illustrated in several axiomatic foundations in mathematics:



</doc>
